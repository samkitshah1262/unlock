[
  {
    "id": "ai-gpu-architecture-streaming-multiprocessors-sms-1",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Fundamental Architectural Components",
    "title": "Streaming Multiprocessors (SMs)",
    "subtitle": "Fundamental Architectural Components",
    "contentHtml": "<ul>\n  <li>\n    <p>The SM is the fundamental execution unit in an NVIDIA GPU. Each SM contains:</p>\n\n    <ul>\n      <li>Multiple CUDA cores (integer and floating-point ALUs)</li>\n      <li>Special Function Units (SFUs) for transcendental math</li>\n      <li>Tensor Cores for matrix-heavy operations (deep learning, HPC)</li>\n      <li>Load/store units for memory access</li>\n      <li>Warp schedulers for issuing instructions to warps</li>\n    </ul>\n  </li>\n</ul>\n<p>The SM is the fundamental execution unit in an NVIDIA GPU. Each SM contains:</p>\n<ul>\n      <li>Multiple CUDA cores (integer and floating-point ALUs)</li>\n      <li>Special Function Units (SFUs) for transcendental math</li>\n      <li>Tensor Cores for matrix-heavy operations (deep learning, HPC)</li>\n      <li>Load/store units for memory access</li>\n      <li>Warp schedulers for issuing instructions to warps</li>\n    </ul>",
    "contentMarkdown": "*   The SM is the fundamental execution unit in an NVIDIA GPU. Each SM contains:\n    \n    *   Multiple CUDA cores (integer and floating-point ALUs)\n    *   Special Function Units (SFUs) for transcendental math\n    *   Tensor Cores for matrix-heavy operations (deep learning, HPC)\n    *   Load/store units for memory access\n    *   Warp schedulers for issuing instructions to warps\n\nThe SM is the fundamental execution unit in an NVIDIA GPU. Each SM contains:\n\n*   Multiple CUDA cores (integer and floating-point ALUs)\n*   Special Function Units (SFUs) for transcendental math\n*   Tensor Cores for matrix-heavy operations (deep learning, HPC)\n*   Load/store units for memory access\n*   Warp schedulers for issuing instructions to warps",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous",
      "deep learning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 107,
      "contentLength": 892
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#streaming-multiprocessors-(sms)",
    "scrapedAt": "2025-12-28T11:56:41.454Z"
  },
  {
    "id": "ai-gpu-architecture-cuda-cores-2",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Fundamental Architectural Components",
    "title": "CUDA Cores",
    "subtitle": "Fundamental Architectural Components",
    "contentHtml": "<ul>\n  <li>Handle general-purpose floating-point and integer operations.</li>\n  <li>Organized into warps of 32 threads.</li>\n  <li>Designed for SIMD-like execution, where all threads in a warp execute the same instruction.</li>\n</ul>",
    "contentMarkdown": "*   Handle general-purpose floating-point and integer operations.\n*   Organized into warps of 32 threads.\n*   Designed for SIMD-like execution, where all threads in a warp execute the same instruction.",
    "order": 2,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 29,
      "contentLength": 233
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#cuda-cores",
    "scrapedAt": "2025-12-28T11:56:41.454Z"
  },
  {
    "id": "ai-gpu-architecture-tensor-cores-3",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Fundamental Architectural Components",
    "title": "Tensor Cores",
    "subtitle": "Fundamental Architectural Components",
    "contentHtml": "<ul>\n  <li>Specialized hardware for matrix multiply-and-accumulate (MMA) operations.</li>\n  <li>Key for AI/ML workloads (e.g., FP16, BF16, INT8 precision).</li>\n  <li>Introduced in Volta, improved in each subsequent architecture.</li>\n</ul>",
    "contentMarkdown": "*   Specialized hardware for matrix multiply-and-accumulate (MMA) operations.\n*   Key for AI/ML workloads (e.g., FP16, BF16, INT8 precision).\n*   Introduced in Volta, improved in each subsequent architecture.",
    "order": 3,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 27,
      "contentLength": 240
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#tensor-cores",
    "scrapedAt": "2025-12-28T11:56:41.454Z"
  },
  {
    "id": "ai-gpu-architecture-ray-tracing-rt-cores-4",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Fundamental Architectural Components",
    "title": "Ray Tracing (RT) Cores",
    "subtitle": "Fundamental Architectural Components",
    "contentHtml": "<ul>\n  <li>Accelerate BVH traversal and ray-triangle intersection tests for real-time ray tracing.</li>\n  <li>First introduced in Turing architecture.</li>\n</ul>",
    "contentMarkdown": "*   Accelerate BVH traversal and ray-triangle intersection tests for real-time ray tracing.\n*   First introduced in Turing architecture.",
    "order": 4,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 18,
      "contentLength": 161
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#ray-tracing-(rt)-cores",
    "scrapedAt": "2025-12-28T11:56:41.454Z"
  },
  {
    "id": "ai-gpu-architecture-memory-hierarchy-5",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Fundamental Architectural Components",
    "title": "Memory Hierarchy",
    "subtitle": "Fundamental Architectural Components",
    "contentHtml": "<ul>\n  <li>\n    <p>NVIDIA GPUs employ a multi-tier memory system:</p>\n\n    <ul>\n      <li><strong>Registers</strong> (per thread, lowest latency)</li>\n      <li><strong>Shared Memory / L1 Cache</strong> (per SM, low latency, user-controlled or cache mode)</li>\n      <li><strong>L2 Cache</strong> (shared across all SMs, higher capacity)</li>\n      <li><strong>Global Memory (VRAM)</strong> (off-chip, high latency, GDDR6 or HBM)</li>\n      <li><strong>Texture and constant caches</strong> (specialized caching units)</li>\n    </ul>\n  </li>\n</ul>\n<p>NVIDIA GPUs employ a multi-tier memory system:</p>\n<ul>\n      <li><strong>Registers</strong> (per thread, lowest latency)</li>\n      <li><strong>Shared Memory / L1 Cache</strong> (per SM, low latency, user-controlled or cache mode)</li>\n      <li><strong>L2 Cache</strong> (shared across all SMs, higher capacity)</li>\n      <li><strong>Global Memory (VRAM)</strong> (off-chip, high latency, GDDR6 or HBM)</li>\n      <li><strong>Texture and constant caches</strong> (specialized caching units)</li>\n    </ul>",
    "contentMarkdown": "*   NVIDIA GPUs employ a multi-tier memory system:\n    \n    *   **Registers** (per thread, lowest latency)\n    *   **Shared Memory / L1 Cache** (per SM, low latency, user-controlled or cache mode)\n    *   **L2 Cache** (shared across all SMs, higher capacity)\n    *   **Global Memory (VRAM)** (off-chip, high latency, GDDR6 or HBM)\n    *   **Texture and constant caches** (specialized caching units)\n\nNVIDIA GPUs employ a multi-tier memory system:\n\n*   **Registers** (per thread, lowest latency)\n*   **Shared Memory / L1 Cache** (per SM, low latency, user-controlled or cache mode)\n*   **L2 Cache** (shared across all SMs, higher capacity)\n*   **Global Memory (VRAM)** (off-chip, high latency, GDDR6 or HBM)\n*   **Texture and constant caches** (specialized caching units)",
    "order": 5,
    "orderInChapter": 5,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 109,
      "contentLength": 1058
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#memory-hierarchy",
    "scrapedAt": "2025-12-28T11:56:41.454Z"
  },
  {
    "id": "ai-gpu-architecture-interconnects-6",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Fundamental Architectural Components",
    "title": "Interconnects",
    "subtitle": "Fundamental Architectural Components",
    "contentHtml": "<ul>\n  <li><strong>NVLink</strong>: High-bandwidth, low-latency GPU-to-GPU and GPU-to-CPU interconnect.</li>\n  <li><strong>PCIe</strong>: Standard system interconnect, slower than NVLink.</li>\n</ul>",
    "contentMarkdown": "*   **NVLink**: High-bandwidth, low-latency GPU-to-GPU and GPU-to-CPU interconnect.\n*   **PCIe**: Standard system interconnect, slower than NVLink.",
    "order": 6,
    "orderInChapter": 6,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 16,
      "contentLength": 198
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#interconnects",
    "scrapedAt": "2025-12-28T11:56:41.454Z"
  },
  {
    "id": "ai-gpu-architecture-key-architectural-design-goals-7",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Fundamental Architectural Components",
    "title": "Key Architectural Design Goals",
    "subtitle": "Fundamental Architectural Components",
    "contentHtml": "<ol>\n  <li><strong>High Parallelism</strong> – Scaling SM count and CUDA cores.</li>\n  <li><strong>Specialization</strong> – Adding RT and Tensor Cores for dedicated workloads.</li>\n  <li><strong>Memory Bandwidth</strong> – Using faster VRAM and wider buses.</li>\n  <li><strong>Energy Efficiency</strong> – Improved performance per watt through better fabrication processes and architectural optimizations.</li>\n  <li><strong>Scalability</strong> – Supporting multi-GPU setups with coherent memory models.</li>\n</ol>",
    "contentMarkdown": "1.  **High Parallelism** – Scaling SM count and CUDA cores.\n2.  **Specialization** – Adding RT and Tensor Cores for dedicated workloads.\n3.  **Memory Bandwidth** – Using faster VRAM and wider buses.\n4.  **Energy Efficiency** – Improved performance per watt through better fabrication processes and architectural optimizations.\n5.  **Scalability** – Supporting multi-GPU setups with coherent memory models.",
    "order": 7,
    "orderInChapter": 7,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous",
      "optimization"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 56,
      "contentLength": 516
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#key-architectural-design-goals",
    "scrapedAt": "2025-12-28T11:56:41.454Z"
  },
  {
    "id": "ai-gpu-architecture-thread-and-warp-model-8",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Execution Paradigms in NVIDIA GPUs",
    "title": "Thread and Warp Model",
    "subtitle": "Execution Paradigms in NVIDIA GPUs",
    "contentHtml": "<ul>\n  <li><strong>Thread</strong>:\n    <ul>\n      <li>The smallest unit of execution. Each thread has its own registers, program counter, and can execute independently, but in practice, threads are grouped for efficiency.</li>\n    </ul>\n  </li>\n  <li><strong>Warp</strong>:\n    <ul>\n      <li>A warp is a group of <strong>32 threads</strong> that execute the same instruction in lockstep on a single SM.\n        <ul>\n          <li>If threads within a warp diverge (take different branches), execution is serialized for each path until they reconverge (warp divergence penalty).</li>\n          <li>Warps are the main scheduling unit in NVIDIA GPUs.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>The smallest unit of execution. Each thread has its own registers, program counter, and can execute independently, but in practice, threads are grouped for efficiency.</li>\n    </ul>\n<ul>\n      <li>A warp is a group of <strong>32 threads</strong> that execute the same instruction in lockstep on a single SM.\n        <ul>\n          <li>If threads within a warp diverge (take different branches), execution is serialized for each path until they reconverge (warp divergence penalty).</li>\n          <li>Warps are the main scheduling unit in NVIDIA GPUs.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>If threads within a warp diverge (take different branches), execution is serialized for each path until they reconverge (warp divergence penalty).</li>\n          <li>Warps are the main scheduling unit in NVIDIA GPUs.</li>\n        </ul>",
    "contentMarkdown": "*   **Thread**:\n    *   The smallest unit of execution. Each thread has its own registers, program counter, and can execute independently, but in practice, threads are grouped for efficiency.\n*   **Warp**:\n    *   A warp is a group of **32 threads** that execute the same instruction in lockstep on a single SM.\n        *   If threads within a warp diverge (take different branches), execution is serialized for each path until they reconverge (warp divergence penalty).\n        *   Warps are the main scheduling unit in NVIDIA GPUs.\n\n*   The smallest unit of execution. Each thread has its own registers, program counter, and can execute independently, but in practice, threads are grouped for efficiency.\n\n*   A warp is a group of **32 threads** that execute the same instruction in lockstep on a single SM.\n    *   If threads within a warp diverge (take different branches), execution is serialized for each path until they reconverge (warp divergence penalty).\n    *   Warps are the main scheduling unit in NVIDIA GPUs.\n\n*   If threads within a warp diverge (take different branches), execution is serialized for each path until they reconverge (warp divergence penalty).\n*   Warps are the main scheduling unit in NVIDIA GPUs.",
    "order": 8,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 192,
      "contentLength": 1567
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#thread-and-warp-model",
    "scrapedAt": "2025-12-28T11:56:41.454Z"
  },
  {
    "id": "ai-gpu-architecture-thread-blocks-and-grids-9",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Execution Paradigms in NVIDIA GPUs",
    "title": "Thread Blocks and Grids",
    "subtitle": "Execution Paradigms in NVIDIA GPUs",
    "contentHtml": "<ul>\n  <li><strong>Thread Block (also known as Cooperative Thread Array)</strong>:\n    <ul>\n      <li>A thread block, also called a Cooperative Thread Array (CTA), is a group of threads (typically up to 1024) organized in 1D, 2D, or 3D layouts that work together, sharing data via shared memory and synchronizing their execution.\n        <ul>\n          <li>Threads in a block share <strong>SM-local resources</strong> such as shared memory and L1 cache.</li>\n          <li>A block is always executed on a single SM (cannot span multiple SMs).</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Grid</strong>:\n    <ul>\n      <li>A grid is a set of thread blocks launched for a kernel execution.\n        <ul>\n          <li>Can have 1D, 2D, or 3D configuration.</li>\n          <li>Enables scaling up to millions of threads.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>A thread block, also called a Cooperative Thread Array (CTA), is a group of threads (typically up to 1024) organized in 1D, 2D, or 3D layouts that work together, sharing data via shared memory and synchronizing their execution.\n        <ul>\n          <li>Threads in a block share <strong>SM-local resources</strong> such as shared memory and L1 cache.</li>\n          <li>A block is always executed on a single SM (cannot span multiple SMs).</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Threads in a block share <strong>SM-local resources</strong> such as shared memory and L1 cache.</li>\n          <li>A block is always executed on a single SM (cannot span multiple SMs).</li>\n        </ul>\n<ul>\n      <li>A grid is a set of thread blocks launched for a kernel execution.\n        <ul>\n          <li>Can have 1D, 2D, or 3D configuration.</li>\n          <li>Enables scaling up to millions of threads.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Can have 1D, 2D, or 3D configuration.</li>\n          <li>Enables scaling up to millions of threads.</li>\n        </ul>",
    "contentMarkdown": "*   **Thread Block (also known as Cooperative Thread Array)**:\n    *   A thread block, also called a Cooperative Thread Array (CTA), is a group of threads (typically up to 1024) organized in 1D, 2D, or 3D layouts that work together, sharing data via shared memory and synchronizing their execution.\n        *   Threads in a block share **SM-local resources** such as shared memory and L1 cache.\n        *   A block is always executed on a single SM (cannot span multiple SMs).\n*   **Grid**:\n    *   A grid is a set of thread blocks launched for a kernel execution.\n        *   Can have 1D, 2D, or 3D configuration.\n        *   Enables scaling up to millions of threads.\n\n*   A thread block, also called a Cooperative Thread Array (CTA), is a group of threads (typically up to 1024) organized in 1D, 2D, or 3D layouts that work together, sharing data via shared memory and synchronizing their execution.\n    *   Threads in a block share **SM-local resources** such as shared memory and L1 cache.\n    *   A block is always executed on a single SM (cannot span multiple SMs).\n\n*   Threads in a block share **SM-local resources** such as shared memory and L1 cache.\n*   A block is always executed on a single SM (cannot span multiple SMs).\n\n*   A grid is a set of thread blocks launched for a kernel execution.\n    *   Can have 1D, 2D, or 3D configuration.\n    *   Enables scaling up to millions of threads.\n\n*   Can have 1D, 2D, or 3D configuration.\n*   Enables scaling up to millions of threads.",
    "order": 9,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 252,
      "contentLength": 1995
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#thread-blocks-and-grids",
    "scrapedAt": "2025-12-28T11:56:41.454Z"
  },
  {
    "id": "ai-gpu-architecture-simt-execution-model-10",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Execution Paradigms in NVIDIA GPUs",
    "title": "SIMT Execution Model",
    "subtitle": "Execution Paradigms in NVIDIA GPUs",
    "contentHtml": "<ul>\n  <li>SIMT is similar to SIMD but with more flexibility—each thread has its own instruction state, yet warps execute in a vector-like fashion.</li>\n  <li>Hardware warp schedulers select ready warps each cycle to hide memory latency.</li>\n  <li>The SIMT model allows massive parallelism while tolerating high memory access latencies.</li>\n</ul>",
    "contentMarkdown": "*   SIMT is similar to SIMD but with more flexibility—each thread has its own instruction state, yet warps execute in a vector-like fashion.\n*   Hardware warp schedulers select ready warps each cycle to hide memory latency.\n*   The SIMT model allows massive parallelism while tolerating high memory access latencies.",
    "order": 10,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 49,
      "contentLength": 348
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#simt-execution-model",
    "scrapedAt": "2025-12-28T11:56:41.454Z"
  },
  {
    "id": "ai-gpu-architecture-warp-scheduling-11",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Execution Paradigms in NVIDIA GPUs",
    "title": "Warp Scheduling",
    "subtitle": "Execution Paradigms in NVIDIA GPUs",
    "contentHtml": "<ul>\n  <li>Each SM contains <strong>warp schedulers</strong> (number varies by architecture).</li>\n  <li>The scheduler chooses one or more ready warps per cycle, issuing instructions to execution units.</li>\n  <li>\n    <p>Techniques to maximize utilization:</p>\n\n    <ul>\n      <li><strong>Latency hiding</strong>: Switching to another ready warp when one stalls.</li>\n      <li><strong>Dual-issue</strong>: Issuing two independent instructions in the same cycle.</li>\n      <li><strong>Specialized pipelines</strong>: Routing instructions to CUDA cores, Tensor Cores, or SFUs.</li>\n    </ul>\n  </li>\n</ul>\n<p>Techniques to maximize utilization:</p>\n<ul>\n      <li><strong>Latency hiding</strong>: Switching to another ready warp when one stalls.</li>\n      <li><strong>Dual-issue</strong>: Issuing two independent instructions in the same cycle.</li>\n      <li><strong>Specialized pipelines</strong>: Routing instructions to CUDA cores, Tensor Cores, or SFUs.</li>\n    </ul>",
    "contentMarkdown": "*   Each SM contains **warp schedulers** (number varies by architecture).\n*   The scheduler chooses one or more ready warps per cycle, issuing instructions to execution units.\n*   Techniques to maximize utilization:\n    \n    *   **Latency hiding**: Switching to another ready warp when one stalls.\n    *   **Dual-issue**: Issuing two independent instructions in the same cycle.\n    *   **Specialized pipelines**: Routing instructions to CUDA cores, Tensor Cores, or SFUs.\n\nTechniques to maximize utilization:\n\n*   **Latency hiding**: Switching to another ready warp when one stalls.\n*   **Dual-issue**: Issuing two independent instructions in the same cycle.\n*   **Specialized pipelines**: Routing instructions to CUDA cores, Tensor Cores, or SFUs.",
    "order": 11,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 101,
      "contentLength": 975
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#warp-scheduling",
    "scrapedAt": "2025-12-28T11:56:41.454Z"
  },
  {
    "id": "ai-gpu-architecture-synchronization-and-communication-12",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Execution Paradigms in NVIDIA GPUs",
    "title": "Synchronization and Communication",
    "subtitle": "Execution Paradigms in NVIDIA GPUs",
    "contentHtml": "<ul>\n  <li><strong>Intra-warp</strong>: Implicit, as all threads execute in lockstep.</li>\n  <li><strong>Intra-block</strong>: Achieved via <code class=\"language-plaintext highlighter-rouge\">__syncthreads()</code> barrier and shared memory.</li>\n  <li><strong>Inter-block</strong>: No built-in sync; requires multiple kernel launches or cooperative groups.</li>\n</ul>",
    "contentMarkdown": "*   **Intra-warp**: Implicit, as all threads execute in lockstep.\n*   **Intra-block**: Achieved via `__syncthreads()` barrier and shared memory.\n*   **Inter-block**: No built-in sync; requires multiple kernel launches or cooperative groups.",
    "order": 12,
    "orderInChapter": 5,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 30,
      "contentLength": 367
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#synchronization-and-communication",
    "scrapedAt": "2025-12-28T11:56:41.454Z"
  },
  {
    "id": "ai-gpu-architecture-model-definition-and-training-cpugpu-13",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
    "title": "Model Definition and Training (CPU/GPU)",
    "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
    "contentHtml": "<ul>\n  <li>Models are typically defined in Python using high-level APIs in PyTorch (<code class=\"language-plaintext highlighter-rouge\">nn.Module</code>) or TensorFlow (<code class=\"language-plaintext highlighter-rouge\">tf.keras.Model</code>). During training, operations like matrix multiplications, convolutions, and non-linearities are recorded dynamically (in PyTorch’s eager mode) or statically (in TensorFlow’s graph mode).</li>\n  <li>If a GPU is available and selected (<code class=\"language-plaintext highlighter-rouge\">device=\"cuda\"</code> or <code class=\"language-plaintext highlighter-rouge\">tf.device('/GPU:0')</code>), the framework schedules these computations for GPU execution.</li>\n</ul>",
    "contentMarkdown": "*   Models are typically defined in Python using high-level APIs in PyTorch (`nn.Module`) or TensorFlow (`tf.keras.Model`). During training, operations like matrix multiplications, convolutions, and non-linearities are recorded dynamically (in PyTorch’s eager mode) or statically (in TensorFlow’s graph mode).\n*   If a GPU is available and selected (`device=\"cuda\"` or `tf.device('/GPU:0')`), the framework schedules these computations for GPU execution.",
    "order": 13,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous",
      "convolution"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 57,
      "contentLength": 703
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#model-definition-and-training-(cpu/gpu)",
    "scrapedAt": "2025-12-28T11:56:41.454Z"
  },
  {
    "id": "ai-gpu-architecture-operator-dispatch-and-kernel-mapping-14",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
    "title": "Operator Dispatch and Kernel Mapping",
    "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
    "contentHtml": "<ul>\n  <li>\n    <p>Each tensor operation (e.g., <code class=\"language-plaintext highlighter-rouge\">matmul</code>, <code class=\"language-plaintext highlighter-rouge\">ReLU</code>, <code class=\"language-plaintext highlighter-rouge\">conv2d</code>) corresponds to one or more GPU kernels. The deep learning framework uses backend libraries like:</p>\n\n    <ul>\n      <li><strong>cuDNN</strong> (for deep neural networks: convolutions, activation, normalization)</li>\n      <li><strong>cuBLAS</strong> (for matrix algebra)</li>\n      <li><strong>cuFFT</strong>, <strong>cuSPARSE</strong>, etc. (for specialized ops)</li>\n    </ul>\n  </li>\n  <li>\n    <p>These libraries provide pre-optimized CUDA kernels tailored for various tensor shapes and precisions (e.g., FP32, FP16, BF16, INT8). When a model executes, the framework dispatches operator calls to these GPU-optimized kernels via these libraries.</p>\n  </li>\n</ul>\n<p>Each tensor operation (e.g., <code class=\"language-plaintext highlighter-rouge\">matmul</code>, <code class=\"language-plaintext highlighter-rouge\">ReLU</code>, <code class=\"language-plaintext highlighter-rouge\">conv2d</code>) corresponds to one or more GPU kernels. The deep learning framework uses backend libraries like:</p>\n<ul>\n      <li><strong>cuDNN</strong> (for deep neural networks: convolutions, activation, normalization)</li>\n      <li><strong>cuBLAS</strong> (for matrix algebra)</li>\n      <li><strong>cuFFT</strong>, <strong>cuSPARSE</strong>, etc. (for specialized ops)</li>\n    </ul>\n<p>These libraries provide pre-optimized CUDA kernels tailored for various tensor shapes and precisions (e.g., FP32, FP16, BF16, INT8). When a model executes, the framework dispatches operator calls to these GPU-optimized kernels via these libraries.</p>",
    "contentMarkdown": "*   Each tensor operation (e.g., `matmul`, `ReLU`, `conv2d`) corresponds to one or more GPU kernels. The deep learning framework uses backend libraries like:\n    \n    *   **cuDNN** (for deep neural networks: convolutions, activation, normalization)\n    *   **cuBLAS** (for matrix algebra)\n    *   **cuFFT**, **cuSPARSE**, etc. (for specialized ops)\n*   These libraries provide pre-optimized CUDA kernels tailored for various tensor shapes and precisions (e.g., FP32, FP16, BF16, INT8). When a model executes, the framework dispatches operator calls to these GPU-optimized kernels via these libraries.\n    \n\nEach tensor operation (e.g., `matmul`, `ReLU`, `conv2d`) corresponds to one or more GPU kernels. The deep learning framework uses backend libraries like:\n\n*   **cuDNN** (for deep neural networks: convolutions, activation, normalization)\n*   **cuBLAS** (for matrix algebra)\n*   **cuFFT**, **cuSPARSE**, etc. (for specialized ops)\n\nThese libraries provide pre-optimized CUDA kernels tailored for various tensor shapes and precisions (e.g., FP32, FP16, BF16, INT8). When a model executes, the framework dispatches operator calls to these GPU-optimized kernels via these libraries.",
    "order": 14,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous",
      "neural network",
      "deep learning",
      "convolution",
      "activation"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 156,
      "contentLength": 1769
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#operator-dispatch-and-kernel-mapping",
    "scrapedAt": "2025-12-28T11:56:41.454Z"
  },
  {
    "id": "ai-gpu-architecture-memory-management-and-transfer-15",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
    "title": "Memory Management and Transfer",
    "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
    "contentHtml": "<ul>\n  <li>\n    <p>Before execution, model parameters and inputs are moved to GPU memory (VRAM). PyTorch and TensorFlow both manage memory allocation, deallocation, and device transfers:</p>\n\n    <ul>\n      <li>CPU to GPU: <code class=\"language-plaintext highlighter-rouge\">tensor.to(\"cuda\")</code> (PyTorch) or <code class=\"language-plaintext highlighter-rouge\">.gpu()</code> context (TensorFlow)</li>\n      <li>GPU memory is allocated dynamically and reused to reduce fragmentation</li>\n      <li>Intermediate results during forward/backward passes are cached in GPU RAM to avoid excessive recomputation</li>\n    </ul>\n  </li>\n  <li>\n    <p>Efficient memory use is critical, especially for large models and high-resolution inputs.</p>\n  </li>\n</ul>\n<p>Before execution, model parameters and inputs are moved to GPU memory (VRAM). PyTorch and TensorFlow both manage memory allocation, deallocation, and device transfers:</p>\n<ul>\n      <li>CPU to GPU: <code class=\"language-plaintext highlighter-rouge\">tensor.to(\"cuda\")</code> (PyTorch) or <code class=\"language-plaintext highlighter-rouge\">.gpu()</code> context (TensorFlow)</li>\n      <li>GPU memory is allocated dynamically and reused to reduce fragmentation</li>\n      <li>Intermediate results during forward/backward passes are cached in GPU RAM to avoid excessive recomputation</li>\n    </ul>\n<p>Efficient memory use is critical, especially for large models and high-resolution inputs.</p>",
    "contentMarkdown": "*   Before execution, model parameters and inputs are moved to GPU memory (VRAM). PyTorch and TensorFlow both manage memory allocation, deallocation, and device transfers:\n    \n    *   CPU to GPU: `tensor.to(\"cuda\")` (PyTorch) or `.gpu()` context (TensorFlow)\n    *   GPU memory is allocated dynamically and reused to reduce fragmentation\n    *   Intermediate results during forward/backward passes are cached in GPU RAM to avoid excessive recomputation\n*   Efficient memory use is critical, especially for large models and high-resolution inputs.\n    \n\nBefore execution, model parameters and inputs are moved to GPU memory (VRAM). PyTorch and TensorFlow both manage memory allocation, deallocation, and device transfers:\n\n*   CPU to GPU: `tensor.to(\"cuda\")` (PyTorch) or `.gpu()` context (TensorFlow)\n*   GPU memory is allocated dynamically and reused to reduce fragmentation\n*   Intermediate results during forward/backward passes are cached in GPU RAM to avoid excessive recomputation\n\nEfficient memory use is critical, especially for large models and high-resolution inputs.",
    "order": 15,
    "orderInChapter": 3,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 144,
      "contentLength": 1447
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#memory-management-and-transfer",
    "scrapedAt": "2025-12-28T11:56:41.454Z"
  },
  {
    "id": "ai-gpu-architecture-kernel-launch-and-execution-on-sms-16",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
    "title": "Kernel Launch and Execution on SMs",
    "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
    "contentHtml": "<ul>\n  <li>\n    <p>Once data and operations are prepared, each operator triggers a <strong>CUDA kernel launch</strong>. This is where NVIDIA’s execution model comes into play:</p>\n\n    <ul>\n      <li>Kernels are executed by <strong>Streaming Multiprocessors (SMs)</strong>, leveraging thousands of <strong>CUDA cores</strong></li>\n      <li>Work is parallelized into <strong>warps</strong> (32 threads), grouped into <strong>thread blocks</strong></li>\n      <li>Tensor Cores accelerate operations like GEMMs and convolutions in mixed precision (FP16, BF16, TF32)</li>\n    </ul>\n  </li>\n  <li>\n    <p>The GPU’s warp schedulers manage kernel instruction dispatch, hide memory latency, and maximize throughput by overlapping compute and memory-bound operations.</p>\n  </li>\n</ul>\n<p>Once data and operations are prepared, each operator triggers a <strong>CUDA kernel launch</strong>. This is where NVIDIA’s execution model comes into play:</p>\n<ul>\n      <li>Kernels are executed by <strong>Streaming Multiprocessors (SMs)</strong>, leveraging thousands of <strong>CUDA cores</strong></li>\n      <li>Work is parallelized into <strong>warps</strong> (32 threads), grouped into <strong>thread blocks</strong></li>\n      <li>Tensor Cores accelerate operations like GEMMs and convolutions in mixed precision (FP16, BF16, TF32)</li>\n    </ul>\n<p>The GPU’s warp schedulers manage kernel instruction dispatch, hide memory latency, and maximize throughput by overlapping compute and memory-bound operations.</p>",
    "contentMarkdown": "*   Once data and operations are prepared, each operator triggers a **CUDA kernel launch**. This is where NVIDIA’s execution model comes into play:\n    \n    *   Kernels are executed by **Streaming Multiprocessors (SMs)**, leveraging thousands of **CUDA cores**\n    *   Work is parallelized into **warps** (32 threads), grouped into **thread blocks**\n    *   Tensor Cores accelerate operations like GEMMs and convolutions in mixed precision (FP16, BF16, TF32)\n*   The GPU’s warp schedulers manage kernel instruction dispatch, hide memory latency, and maximize throughput by overlapping compute and memory-bound operations.\n    \n\nOnce data and operations are prepared, each operator triggers a **CUDA kernel launch**. This is where NVIDIA’s execution model comes into play:\n\n*   Kernels are executed by **Streaming Multiprocessors (SMs)**, leveraging thousands of **CUDA cores**\n*   Work is parallelized into **warps** (32 threads), grouped into **thread blocks**\n*   Tensor Cores accelerate operations like GEMMs and convolutions in mixed precision (FP16, BF16, TF32)\n\nThe GPU’s warp schedulers manage kernel instruction dispatch, hide memory latency, and maximize throughput by overlapping compute and memory-bound operations.",
    "order": 16,
    "orderInChapter": 4,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous",
      "convolution"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 166,
      "contentLength": 1501
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#kernel-launch-and-execution-on-sms",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-forward-and-backward-passes-training-17",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
    "title": "Forward and Backward Passes (Training)",
    "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
    "contentHtml": "<ul>\n  <li><strong>Forward pass</strong>: Model computations are executed layer by layer, and activations are stored.</li>\n  <li><strong>Backward pass</strong>: Gradients are computed using automatic differentiation, with additional kernels launched for each operation’s derivative.</li>\n  <li>\n    <p><strong>Gradient updates</strong>: Optimizer steps (e.g., Adam, SGD) are also dispatched to GPU kernels when tensors reside in GPU memory.</p>\n  </li>\n  <li>All of this happens in the context of a training loop, with heavy reliance on GPU compute resources and memory bandwidth.</li>\n</ul>\n<p><strong>Gradient updates</strong>: Optimizer steps (e.g., Adam, SGD) are also dispatched to GPU kernels when tensors reside in GPU memory.</p>",
    "contentMarkdown": "*   **Forward pass**: Model computations are executed layer by layer, and activations are stored.\n*   **Backward pass**: Gradients are computed using automatic differentiation, with additional kernels launched for each operation’s derivative.\n*   **Gradient updates**: Optimizer steps (e.g., Adam, SGD) are also dispatched to GPU kernels when tensors reside in GPU memory.\n    \n*   All of this happens in the context of a training loop, with heavy reliance on GPU compute resources and memory bandwidth.\n\n**Gradient updates**: Optimizer steps (e.g., Adam, SGD) are also dispatched to GPU kernels when tensors reside in GPU memory.",
    "order": 17,
    "orderInChapter": 5,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous",
      "activation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 92,
      "contentLength": 737
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#forward-and-backward-passes-(training)",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-inference-deployment-18",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
    "title": "Inference Deployment",
    "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
    "contentHtml": "<ul>\n  <li>\n    <p>Once trained, the model is often exported for inference:</p>\n\n    <ul>\n      <li><strong>TensorFlow:</strong> SavedModel, TF Lite, or ONNX format</li>\n      <li><strong>PyTorch:</strong> TorchScript, ONNX, or native PyTorch weights</li>\n    </ul>\n  </li>\n  <li>\n    <p>During inference, only the forward pass is executed. Frameworks and inference engines (like TensorRT, TorchServe, or TF Serving) strip unnecessary training components, optimize kernel ordering, and preload weights into GPU memory to reduce latency and maximize throughput.</p>\n  </li>\n</ul>\n<p>Once trained, the model is often exported for inference:</p>\n<ul>\n      <li><strong>TensorFlow:</strong> SavedModel, TF Lite, or ONNX format</li>\n      <li><strong>PyTorch:</strong> TorchScript, ONNX, or native PyTorch weights</li>\n    </ul>\n<p>During inference, only the forward pass is executed. Frameworks and inference engines (like TensorRT, TorchServe, or TF Serving) strip unnecessary training components, optimize kernel ordering, and preload weights into GPU memory to reduce latency and maximize throughput.</p>",
    "contentMarkdown": "*   Once trained, the model is often exported for inference:\n    \n    *   **TensorFlow:** SavedModel, TF Lite, or ONNX format\n    *   **PyTorch:** TorchScript, ONNX, or native PyTorch weights\n*   During inference, only the forward pass is executed. Frameworks and inference engines (like TensorRT, TorchServe, or TF Serving) strip unnecessary training components, optimize kernel ordering, and preload weights into GPU memory to reduce latency and maximize throughput.\n    \n\nOnce trained, the model is often exported for inference:\n\n*   **TensorFlow:** SavedModel, TF Lite, or ONNX format\n*   **PyTorch:** TorchScript, ONNX, or native PyTorch weights\n\nDuring inference, only the forward pass is executed. Frameworks and inference engines (like TensorRT, TorchServe, or TF Serving) strip unnecessary training components, optimize kernel ordering, and preload weights into GPU memory to reduce latency and maximize throughput.",
    "order": 18,
    "orderInChapter": 6,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 126,
      "contentLength": 1103
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#inference-deployment",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-precision-optimization-19",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
    "title": "Precision Optimization",
    "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
    "contentHtml": "<ul>\n  <li>\n    <p>To reduce GPU memory usage and increase performance, models often use:</p>\n\n    <ul>\n      <li><strong>Mixed precision training</strong> (e.g., <code class=\"language-plaintext highlighter-rouge\">float16</code> for activations, <code class=\"language-plaintext highlighter-rouge\">float32</code> for loss and gradients)</li>\n      <li><strong>Quantized inference</strong> (e.g., <code class=\"language-plaintext highlighter-rouge\">int8</code> for weights and activations)\nThese techniques rely on the GPU’s Tensor Cores and require calibration or loss-scaling strategies for training stability.</li>\n    </ul>\n  </li>\n</ul>\n<p>To reduce GPU memory usage and increase performance, models often use:</p>\n<ul>\n      <li><strong>Mixed precision training</strong> (e.g., <code class=\"language-plaintext highlighter-rouge\">float16</code> for activations, <code class=\"language-plaintext highlighter-rouge\">float32</code> for loss and gradients)</li>\n      <li><strong>Quantized inference</strong> (e.g., <code class=\"language-plaintext highlighter-rouge\">int8</code> for weights and activations)\nThese techniques rely on the GPU’s Tensor Cores and require calibration or loss-scaling strategies for training stability.</li>\n    </ul>",
    "contentMarkdown": "*   To reduce GPU memory usage and increase performance, models often use:\n    \n    *   **Mixed precision training** (e.g., `float16` for activations, `float32` for loss and gradients)\n    *   **Quantized inference** (e.g., `int8` for weights and activations) These techniques rely on the GPU’s Tensor Cores and require calibration or loss-scaling strategies for training stability.\n\nTo reduce GPU memory usage and increase performance, models often use:\n\n*   **Mixed precision training** (e.g., `float16` for activations, `float32` for loss and gradients)\n*   **Quantized inference** (e.g., `int8` for weights and activations) These techniques rely on the GPU’s Tensor Cores and require calibration or loss-scaling strategies for training stability.",
    "order": 19,
    "orderInChapter": 7,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous",
      "activation"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 101,
      "contentLength": 1242
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#precision-optimization",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-runtime-tools-and-profiling-20",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
    "title": "Runtime Tools and Profiling",
    "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
    "contentHtml": "<ul>\n  <li>\n    <p>Execution can be monitored and optimized using:</p>\n\n    <ul>\n      <li><strong>Nsight Compute/Systems</strong>: Low-level GPU profiling</li>\n      <li><strong>TensorBoard</strong>: Graph inspection and timing (TensorFlow)</li>\n      <li><strong>PyTorch Profiler</strong>: Operator-level breakdown and bottleneck tracing</li>\n      <li><strong>NVIDIA’s CUPTI</strong>: Provides hooks for collecting runtime metrics</li>\n    </ul>\n  </li>\n</ul>\n<p>Execution can be monitored and optimized using:</p>\n<ul>\n      <li><strong>Nsight Compute/Systems</strong>: Low-level GPU profiling</li>\n      <li><strong>TensorBoard</strong>: Graph inspection and timing (TensorFlow)</li>\n      <li><strong>PyTorch Profiler</strong>: Operator-level breakdown and bottleneck tracing</li>\n      <li><strong>NVIDIA’s CUPTI</strong>: Provides hooks for collecting runtime metrics</li>\n    </ul>",
    "contentMarkdown": "*   Execution can be monitored and optimized using:\n    \n    *   **Nsight Compute/Systems**: Low-level GPU profiling\n    *   **TensorBoard**: Graph inspection and timing (TensorFlow)\n    *   **PyTorch Profiler**: Operator-level breakdown and bottleneck tracing\n    *   **NVIDIA’s CUPTI**: Provides hooks for collecting runtime metrics\n\nExecution can be monitored and optimized using:\n\n*   **Nsight Compute/Systems**: Low-level GPU profiling\n*   **TensorBoard**: Graph inspection and timing (TensorFlow)\n*   **PyTorch Profiler**: Operator-level breakdown and bottleneck tracing\n*   **NVIDIA’s CUPTI**: Provides hooks for collecting runtime metrics",
    "order": 20,
    "orderInChapter": 8,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 75,
      "contentLength": 890
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#runtime-tools-and-profiling",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Compute Architecture Evolution",
    "title": "Streaming Multiprocessors (SM) Evolution",
    "subtitle": "Compute Architecture Evolution",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Ampere (2020)</strong>:</p>\n\n    <ul>\n      <li>SMs housed 128 CUDA cores (vs. 64 in Turing).</li>\n      <li>Warp schedulers improved to issue instructions to mixed precision units simultaneously.</li>\n      <li>Dual datapath per CUDA core allowed FP32 + INT32 execution concurrently.</li>\n      <li>FP64 throughput doubled in data center models (A100).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hopper (2022)</strong>:</p>\n\n    <ul>\n      <li>Redesigned SMs for higher clock speeds and more instruction-level parallelism.</li>\n      <li>Added <strong>DPX instructions</strong> for dynamic programming acceleration (bioinformatics, optimization).</li>\n      <li>Warp specializations for matrix workloads to better feed Tensor Cores.</li>\n      <li>Increased register file size for HPC workloads.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Ada Lovelace (2022)</strong>:</p>\n\n    <ul>\n      <li>Focused on gaming and creative workloads.</li>\n      <li>Higher boost frequencies per SM.</li>\n      <li>Improved power gating for efficiency.</li>\n      <li>Enhanced scheduling for real-time ray tracing workloads.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Blackwell (2024)</strong>:</p>\n\n    <ul>\n      <li>Next-generation SMs with <em>thread block clusters</em> for improved multi-SM cooperation.</li>\n      <li>Improved simultaneous multi-kernel execution.</li>\n      <li>Expanded warp schedulers to reduce instruction stalls in large AI inference.</li>\n      <li>Further increased FP8, BF16, and mixed-precision throughput in SM datapaths.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Ampere (2020)</strong>:</p>\n<ul>\n      <li>SMs housed 128 CUDA cores (vs. 64 in Turing).</li>\n      <li>Warp schedulers improved to issue instructions to mixed precision units simultaneously.</li>\n      <li>Dual datapath per CUDA core allowed FP32 + INT32 execution concurrently.</li>\n      <li>FP64 throughput doubled in data center models (A100).</li>\n    </ul>\n<p><strong>Hopper (2022)</strong>:</p>\n<ul>\n      <li>Redesigned SMs for higher clock speeds and more instruction-level parallelism.</li>\n      <li>Added <strong>DPX instructions</strong> for dynamic programming acceleration (bioinformatics, optimization).</li>\n      <li>Warp specializations for matrix workloads to better feed Tensor Cores.</li>\n      <li>Increased register file size for HPC workloads.</li>\n    </ul>\n<p><strong>Ada Lovelace (2022)</strong>:</p>\n<ul>\n      <li>Focused on gaming and creative workloads.</li>\n      <li>Higher boost frequencies per SM.</li>\n      <li>Improved power gating for efficiency.</li>\n      <li>Enhanced scheduling for real-time ray tracing workloads.</li>\n    </ul>\n<p><strong>Blackwell (2024)</strong>:</p>\n<ul>\n      <li>Next-generation SMs with <em>thread block clusters</em> for improved multi-SM cooperation.</li>\n      <li>Improved simultaneous multi-kernel execution.</li>\n      <li>Expanded warp schedulers to reduce instruction stalls in large AI inference.</li>\n      <li>Further increased FP8, BF16, and mixed-precision throughput in SM datapaths.</li>\n    </ul>",
    "contentMarkdown": "*   **Ampere (2020)**:\n    \n    *   SMs housed 128 CUDA cores (vs. 64 in Turing).\n    *   Warp schedulers improved to issue instructions to mixed precision units simultaneously.\n    *   Dual datapath per CUDA core allowed FP32 + INT32 execution concurrently.\n    *   FP64 throughput doubled in data center models (A100).\n*   **Hopper (2022)**:\n    \n    *   Redesigned SMs for higher clock speeds and more instruction-level parallelism.\n    *   Added **DPX instructions** for dynamic programming acceleration (bioinformatics, optimization).\n    *   Warp specializations for matrix workloads to better feed Tensor Cores.\n    *   Increased register file size for HPC workloads.\n*   **Ada Lovelace (2022)**:\n    \n    *   Focused on gaming and creative workloads.\n    *   Higher boost frequencies per SM.\n    *   Improved power gating for efficiency.\n    *   Enhanced scheduling for real-time ray tracing workloads.\n*   **Blackwell (2024)**:\n    \n    *   Next-generation SMs with _thread block clusters_ for improved multi-SM cooperation.\n    *   Improved simultaneous multi-kernel execution.\n    *   Expanded warp schedulers to reduce instruction stalls in large AI inference.\n    *   Further increased FP8, BF16, and mixed-precision throughput in SM datapaths.\n\n**Ampere (2020)**:\n\n*   SMs housed 128 CUDA cores (vs. 64 in Turing).\n*   Warp schedulers improved to issue instructions to mixed precision units simultaneously.\n*   Dual datapath per CUDA core allowed FP32 + INT32 execution concurrently.\n*   FP64 throughput doubled in data center models (A100).\n\n**Hopper (2022)**:\n\n*   Redesigned SMs for higher clock speeds and more instruction-level parallelism.\n*   Added **DPX instructions** for dynamic programming acceleration (bioinformatics, optimization).\n*   Warp specializations for matrix workloads to better feed Tensor Cores.\n*   Increased register file size for HPC workloads.\n\n**Ada Lovelace (2022)**:\n\n*   Focused on gaming and creative workloads.\n*   Higher boost frequencies per SM.\n*   Improved power gating for efficiency.\n*   Enhanced scheduling for real-time ray tracing workloads.\n\n**Blackwell (2024)**:\n\n*   Next-generation SMs with _thread block clusters_ for improved multi-SM cooperation.\n*   Improved simultaneous multi-kernel execution.\n*   Expanded warp schedulers to reduce instruction stalls in large AI inference.\n*   Further increased FP8, BF16, and mixed-precision throughput in SM datapaths.",
    "order": 21,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "miscellaneous",
      "optimization"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 320,
      "contentLength": 3094
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#streaming-multiprocessors-(sm)-evolution",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-cuda-core-advancements-22",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Compute Architecture Evolution",
    "title": "CUDA Core Advancements",
    "subtitle": "Compute Architecture Evolution",
    "contentHtml": "<ul>\n  <li><strong>Ampere:</strong> 128 CUDA cores/SM, concurrent FP32 and INT32 execution per clock.</li>\n  <li><strong>Hopper:</strong> Improved dual-issue scheduling, better cache locality for CUDA workloads.</li>\n  <li><strong>Ada:</strong> Boosted per-core clock frequency; targeted gaming rasterization and shading throughput.</li>\n  <li><strong>Blackwell:</strong> Higher per-core IPC, deeper pipelines for AI workloads.</li>\n</ul>",
    "contentMarkdown": "*   **Ampere:** 128 CUDA cores/SM, concurrent FP32 and INT32 execution per clock.\n*   **Hopper:** Improved dual-issue scheduling, better cache locality for CUDA workloads.\n*   **Ada:** Boosted per-core clock frequency; targeted gaming rasterization and shading throughput.\n*   **Blackwell:** Higher per-core IPC, deeper pipelines for AI workloads.",
    "order": 22,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 45,
      "contentLength": 438
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#cuda-core-advancements",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-tensor-core-evolution-23",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Compute Architecture Evolution",
    "title": "Tensor Core Evolution",
    "subtitle": "Compute Architecture Evolution",
    "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Generation</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Precision Support</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Notable Features</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ampere</td>\n<td class=\"tg-tleft-valign-first\"><code>float16</code>, <code>bfloat16</code>, <code>TensorFloat-32\n</code>, <code>int8</code>, <code>int4</code></td>\n<td class=\"tg-tleft-valign-second\">TF32 introduced for AI training; structured sparsity (2:4 pattern).</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Hopper</td>\n<td class=\"tg-tleft-valign-first\">Adds <code>float8</code>, improved <code>bfloat16</code>/<code>float16</code></td>\n<td class=\"tg-tleft-valign-second\">Transformer Engine for mixed-precision AI acceleration.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ada</td>\n<td class=\"tg-tleft-valign-first\">Similar to Hopper for consumer SKUs</td>\n<td class=\"tg-tleft-valign-second\">AI super resolution for DLSS 3.x.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Blackwell</td>\n<td class=\"tg-tleft-valign-first\"><code>float4</code>, expanded <code>float8</code></td>\n<td class=\"tg-tleft-valign-second\">Enhanced Transformer Engine for LLMs, better sparsity handling.</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Generation</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Precision Support</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Notable Features</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ampere</td>\n<td class=\"tg-tleft-valign-first\"><code>float16</code>, <code>bfloat16</code>, <code>TensorFloat-32\n</code>, <code>int8</code>, <code>int4</code></td>\n<td class=\"tg-tleft-valign-second\">TF32 introduced for AI training; structured sparsity (2:4 pattern).</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Hopper</td>\n<td class=\"tg-tleft-valign-first\">Adds <code>float8</code>, improved <code>bfloat16</code>/<code>float16</code></td>\n<td class=\"tg-tleft-valign-second\">Transformer Engine for mixed-precision AI acceleration.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ada</td>\n<td class=\"tg-tleft-valign-first\">Similar to Hopper for consumer SKUs</td>\n<td class=\"tg-tleft-valign-second\">AI super resolution for DLSS 3.x.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Blackwell</td>\n<td class=\"tg-tleft-valign-first\"><code>float4</code>, expanded <code>float8</code></td>\n<td class=\"tg-tleft-valign-second\">Enhanced Transformer Engine for LLMs, better sparsity handling.</td>\n</tr>\n</tbody>\n</table>",
    "contentMarkdown": "**Generation**\n\n**Precision Support**\n\n**Notable Features**\n\nAmpere\n\n`float16`, `bfloat16`, `TensorFloat-32` , `int8`, `int4`\n\nTF32 introduced for AI training; structured sparsity (2:4 pattern).\n\nHopper\n\nAdds `float8`, improved `bfloat16`/`float16`\n\nTransformer Engine for mixed-precision AI acceleration.\n\nAda\n\nSimilar to Hopper for consumer SKUs\n\nAI super resolution for DLSS 3.x.\n\nBlackwell\n\n`float4`, expanded `float8`\n\nEnhanced Transformer Engine for LLMs, better sparsity handling.\n\n**Generation**\n\n**Precision Support**\n\n**Notable Features**\n\nAmpere\n\n`float16`, `bfloat16`, `TensorFloat-32` , `int8`, `int4`\n\nTF32 introduced for AI training; structured sparsity (2:4 pattern).\n\nHopper\n\nAdds `float8`, improved `bfloat16`/`float16`\n\nTransformer Engine for mixed-precision AI acceleration.\n\nAda\n\nSimilar to Hopper for consumer SKUs\n\nAI super resolution for DLSS 3.x.\n\nBlackwell\n\n`float4`, expanded `float8`\n\nEnhanced Transformer Engine for LLMs, better sparsity handling.",
    "order": 23,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous",
      "transformer",
      "llm"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 114,
      "contentLength": 2699
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#tensor-core-evolution",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-ray-tracing-core-evolution-24",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Compute Architecture Evolution",
    "title": "Ray Tracing Core Evolution",
    "subtitle": "Compute Architecture Evolution",
    "contentHtml": "<ul>\n  <li><strong>Ampere:</strong> 2nd-gen RT Cores, hardware triangle intersection, motion blur support.</li>\n  <li><strong>Hopper:</strong> Primarily HPC focus; RT improvements not the priority.</li>\n  <li><strong>Ada:</strong> 3rd-gen RT Cores, Opacity Micromaps, Displaced Micro-Meshes.</li>\n  <li><strong>Blackwell:</strong> AI-assisted ray traversal prediction, further pipeline efficiency gains.</li>\n</ul>",
    "contentMarkdown": "*   **Ampere:** 2nd-gen RT Cores, hardware triangle intersection, motion blur support.\n*   **Hopper:** Primarily HPC focus; RT improvements not the priority.\n*   **Ada:** 3rd-gen RT Cores, Opacity Micromaps, Displaced Micro-Meshes.\n*   **Blackwell:** AI-assisted ray traversal prediction, further pipeline efficiency gains.",
    "order": 24,
    "orderInChapter": 4,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 40,
      "contentLength": 414
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#ray-tracing-core-evolution",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-overview-of-precision-types-25",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Floating-Point Precision Performance Evolution",
    "title": "Overview of Precision Types",
    "subtitle": "Floating-Point Precision Performance Evolution",
    "contentHtml": "<ul>\n  <li>\n    <p>NVIDIA GPUs handle multiple floating-point formats for different workloads:</p>\n\n    <ul>\n      <li><strong>FP64 (Double Precision):</strong> 64-bit IEEE format; essential for scientific and HPC workloads.</li>\n      <li><strong>FP32 (Single Precision):</strong> 32-bit IEEE format; common for graphics, simulations, and ML training.</li>\n      <li><strong>TF32 (Tensor Float 32):</strong> 19-bit precision (8-bit exponent, 10-bit mantissa) introduced in Ampere for AI workloads—maintains FP32 range with reduced precision for faster computation.</li>\n      <li><strong>FP16 (Half Precision):</strong> 16-bit IEEE format; used for mixed-precision deep learning training.</li>\n      <li><strong>BF16 (Brain Floating Point 16):</strong> 16-bit format with FP32’s exponent range but reduced mantissa precision; popular in AI training for stability.</li>\n      <li><strong>FP8:</strong> 8-bit floating point (two formats E4M3 and E5M2); optimized for AI inference.</li>\n      <li><strong>FP4:</strong> 4-bit floating point; introduced in Blackwell for ultra-low precision inference.</li>\n    </ul>\n  </li>\n</ul>\n<p>NVIDIA GPUs handle multiple floating-point formats for different workloads:</p>\n<ul>\n      <li><strong>FP64 (Double Precision):</strong> 64-bit IEEE format; essential for scientific and HPC workloads.</li>\n      <li><strong>FP32 (Single Precision):</strong> 32-bit IEEE format; common for graphics, simulations, and ML training.</li>\n      <li><strong>TF32 (Tensor Float 32):</strong> 19-bit precision (8-bit exponent, 10-bit mantissa) introduced in Ampere for AI workloads—maintains FP32 range with reduced precision for faster computation.</li>\n      <li><strong>FP16 (Half Precision):</strong> 16-bit IEEE format; used for mixed-precision deep learning training.</li>\n      <li><strong>BF16 (Brain Floating Point 16):</strong> 16-bit format with FP32’s exponent range but reduced mantissa precision; popular in AI training for stability.</li>\n      <li><strong>FP8:</strong> 8-bit floating point (two formats E4M3 and E5M2); optimized for AI inference.</li>\n      <li><strong>FP4:</strong> 4-bit floating point; introduced in Blackwell for ultra-low precision inference.</li>\n    </ul>",
    "contentMarkdown": "*   NVIDIA GPUs handle multiple floating-point formats for different workloads:\n    \n    *   **FP64 (Double Precision):** 64-bit IEEE format; essential for scientific and HPC workloads.\n    *   **FP32 (Single Precision):** 32-bit IEEE format; common for graphics, simulations, and ML training.\n    *   **TF32 (Tensor Float 32):** 19-bit precision (8-bit exponent, 10-bit mantissa) introduced in Ampere for AI workloads—maintains FP32 range with reduced precision for faster computation.\n    *   **FP16 (Half Precision):** 16-bit IEEE format; used for mixed-precision deep learning training.\n    *   **BF16 (Brain Floating Point 16):** 16-bit format with FP32’s exponent range but reduced mantissa precision; popular in AI training for stability.\n    *   **FP8:** 8-bit floating point (two formats E4M3 and E5M2); optimized for AI inference.\n    *   **FP4:** 4-bit floating point; introduced in Blackwell for ultra-low precision inference.\n\nNVIDIA GPUs handle multiple floating-point formats for different workloads:\n\n*   **FP64 (Double Precision):** 64-bit IEEE format; essential for scientific and HPC workloads.\n*   **FP32 (Single Precision):** 32-bit IEEE format; common for graphics, simulations, and ML training.\n*   **TF32 (Tensor Float 32):** 19-bit precision (8-bit exponent, 10-bit mantissa) introduced in Ampere for AI workloads—maintains FP32 range with reduced precision for faster computation.\n*   **FP16 (Half Precision):** 16-bit IEEE format; used for mixed-precision deep learning training.\n*   **BF16 (Brain Floating Point 16):** 16-bit format with FP32’s exponent range but reduced mantissa precision; popular in AI training for stability.\n*   **FP8:** 8-bit floating point (two formats E4M3 and E5M2); optimized for AI inference.\n*   **FP4:** 4-bit floating point; introduced in Blackwell for ultra-low precision inference.",
    "order": 25,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "miscellaneous",
      "deep learning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 245,
      "contentLength": 2218
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#overview-of-precision-types",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-per-generation-precision-support-and-performance-26",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Floating-Point Precision Performance Evolution",
    "title": "Per-Generation Precision Support and Performance",
    "subtitle": "Floating-Point Precision Performance Evolution",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Ampere (2020)</strong>:</p>\n\n    <ul>\n      <li>FP64: 1/2 rate of FP32 in A100 (HPC models), 1/64 in consumer cards.</li>\n      <li>FP32: 2× throughput vs Turing; dual datapath allowed FP32 + INT32 execution.</li>\n      <li>TF32: Enabled on Tensor Cores; up to 20× faster AI training vs pure FP32.</li>\n      <li>FP16/BF16: Tensor Core acceleration, ~312 TFLOPS on A100.</li>\n      <li>INT8/INT4: Supported with sparsity acceleration.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hopper (2022)</strong>:</p>\n\n    <ul>\n      <li>FP64: Maintained 1/2 rate in HPC variants (H100).</li>\n      <li>FP32: Slight IPC and clock improvements.</li>\n      <li>FP16/BF16: Transformer Engine dynamically chose precision (FP16, BF16, FP8) per layer to optimize throughput.</li>\n      <li>FP8: Introduced with up to 4× throughput improvement over FP16 for inference.</li>\n      <li>TF32: Same as Ampere but with efficiency gains via scheduling.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Ada Lovelace (2022)</strong>:</p>\n\n    <ul>\n      <li>FP64: Mostly absent in consumer models (used in some pro variants at reduced rate).</li>\n      <li>FP32: Highest per-core clock speeds to date for rasterization.</li>\n      <li>FP16/BF16: Similar to Hopper in supported formats but tuned for DLSS and AI upscaling.</li>\n      <li>FP8: Present in professional Ada GPUs for AI workloads.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Blackwell (2024)</strong>:</p>\n\n    <ul>\n      <li>FP64: Maintains HPC capability at 1/2 FP32 rate in B100/B200.</li>\n      <li>FP32: Further IPC gains; optimized for large-scale AI as well as HPC.</li>\n      <li>FP16/BF16: Higher throughput than Hopper with enhanced Transformer Engine.</li>\n      <li>FP8: Doubled throughput over Hopper for inference.</li>\n      <li>FP4: New ultra-low precision mode; allows extremely high throughput for LLM inference with minimal accuracy loss using quantization-aware training.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Ampere (2020)</strong>:</p>\n<ul>\n      <li>FP64: 1/2 rate of FP32 in A100 (HPC models), 1/64 in consumer cards.</li>\n      <li>FP32: 2× throughput vs Turing; dual datapath allowed FP32 + INT32 execution.</li>\n      <li>TF32: Enabled on Tensor Cores; up to 20× faster AI training vs pure FP32.</li>\n      <li>FP16/BF16: Tensor Core acceleration, ~312 TFLOPS on A100.</li>\n      <li>INT8/INT4: Supported with sparsity acceleration.</li>\n    </ul>\n<p><strong>Hopper (2022)</strong>:</p>\n<ul>\n      <li>FP64: Maintained 1/2 rate in HPC variants (H100).</li>\n      <li>FP32: Slight IPC and clock improvements.</li>\n      <li>FP16/BF16: Transformer Engine dynamically chose precision (FP16, BF16, FP8) per layer to optimize throughput.</li>\n      <li>FP8: Introduced with up to 4× throughput improvement over FP16 for inference.</li>\n      <li>TF32: Same as Ampere but with efficiency gains via scheduling.</li>\n    </ul>\n<p><strong>Ada Lovelace (2022)</strong>:</p>\n<ul>\n      <li>FP64: Mostly absent in consumer models (used in some pro variants at reduced rate).</li>\n      <li>FP32: Highest per-core clock speeds to date for rasterization.</li>\n      <li>FP16/BF16: Similar to Hopper in supported formats but tuned for DLSS and AI upscaling.</li>\n      <li>FP8: Present in professional Ada GPUs for AI workloads.</li>\n    </ul>\n<p><strong>Blackwell (2024)</strong>:</p>\n<ul>\n      <li>FP64: Maintains HPC capability at 1/2 FP32 rate in B100/B200.</li>\n      <li>FP32: Further IPC gains; optimized for large-scale AI as well as HPC.</li>\n      <li>FP16/BF16: Higher throughput than Hopper with enhanced Transformer Engine.</li>\n      <li>FP8: Doubled throughput over Hopper for inference.</li>\n      <li>FP4: New ultra-low precision mode; allows extremely high throughput for LLM inference with minimal accuracy loss using quantization-aware training.</li>\n    </ul>",
    "contentMarkdown": "*   **Ampere (2020)**:\n    \n    *   FP64: 1/2 rate of FP32 in A100 (HPC models), 1/64 in consumer cards.\n    *   FP32: 2× throughput vs Turing; dual datapath allowed FP32 + INT32 execution.\n    *   TF32: Enabled on Tensor Cores; up to 20× faster AI training vs pure FP32.\n    *   FP16/BF16: Tensor Core acceleration, ~312 TFLOPS on A100.\n    *   INT8/INT4: Supported with sparsity acceleration.\n*   **Hopper (2022)**:\n    \n    *   FP64: Maintained 1/2 rate in HPC variants (H100).\n    *   FP32: Slight IPC and clock improvements.\n    *   FP16/BF16: Transformer Engine dynamically chose precision (FP16, BF16, FP8) per layer to optimize throughput.\n    *   FP8: Introduced with up to 4× throughput improvement over FP16 for inference.\n    *   TF32: Same as Ampere but with efficiency gains via scheduling.\n*   **Ada Lovelace (2022)**:\n    \n    *   FP64: Mostly absent in consumer models (used in some pro variants at reduced rate).\n    *   FP32: Highest per-core clock speeds to date for rasterization.\n    *   FP16/BF16: Similar to Hopper in supported formats but tuned for DLSS and AI upscaling.\n    *   FP8: Present in professional Ada GPUs for AI workloads.\n*   **Blackwell (2024)**:\n    \n    *   FP64: Maintains HPC capability at 1/2 FP32 rate in B100/B200.\n    *   FP32: Further IPC gains; optimized for large-scale AI as well as HPC.\n    *   FP16/BF16: Higher throughput than Hopper with enhanced Transformer Engine.\n    *   FP8: Doubled throughput over Hopper for inference.\n    *   FP4: New ultra-low precision mode; allows extremely high throughput for LLM inference with minimal accuracy loss using quantization-aware training.\n\n**Ampere (2020)**:\n\n*   FP64: 1/2 rate of FP32 in A100 (HPC models), 1/64 in consumer cards.\n*   FP32: 2× throughput vs Turing; dual datapath allowed FP32 + INT32 execution.\n*   TF32: Enabled on Tensor Cores; up to 20× faster AI training vs pure FP32.\n*   FP16/BF16: Tensor Core acceleration, ~312 TFLOPS on A100.\n*   INT8/INT4: Supported with sparsity acceleration.\n\n**Hopper (2022)**:\n\n*   FP64: Maintained 1/2 rate in HPC variants (H100).\n*   FP32: Slight IPC and clock improvements.\n*   FP16/BF16: Transformer Engine dynamically chose precision (FP16, BF16, FP8) per layer to optimize throughput.\n*   FP8: Introduced with up to 4× throughput improvement over FP16 for inference.\n*   TF32: Same as Ampere but with efficiency gains via scheduling.\n\n**Ada Lovelace (2022)**:\n\n*   FP64: Mostly absent in consumer models (used in some pro variants at reduced rate).\n*   FP32: Highest per-core clock speeds to date for rasterization.\n*   FP16/BF16: Similar to Hopper in supported formats but tuned for DLSS and AI upscaling.\n*   FP8: Present in professional Ada GPUs for AI workloads.\n\n**Blackwell (2024)**:\n\n*   FP64: Maintains HPC capability at 1/2 FP32 rate in B100/B200.\n*   FP32: Further IPC gains; optimized for large-scale AI as well as HPC.\n*   FP16/BF16: Higher throughput than Hopper with enhanced Transformer Engine.\n*   FP8: Doubled throughput over Hopper for inference.\n*   FP4: New ultra-low precision mode; allows extremely high throughput for LLM inference with minimal accuracy loss using quantization-aware training.",
    "order": 26,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 3,
    "tags": [
      "miscellaneous",
      "transformer",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 470,
      "contentLength": 3856
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#per-generation-precision-support-and-performance",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-performance-analysis-27",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Floating-Point Precision Performance Evolution",
    "title": "Performance Analysis",
    "subtitle": "Floating-Point Precision Performance Evolution",
    "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Generation</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP64 (HPC)</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP32</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TF32</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP16/BF16</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP8</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>FP4</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ampere</td>\n<td class=\"tg-tleft-valign-first\">1/2 FP32 rate (A100)</td>\n<td class=\"tg-tleft-valign-first\">Dual datapath, +2× Turing</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Tensor Core, ~312 TFLOPS</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Hopper</td>\n<td class=\"tg-tleft-valign-first\">1/2 FP32 rate (H100)</td>\n<td class=\"tg-tleft-valign-first\">IPC + clock boost</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Transformer Engine</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ada</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-first\">High clocks</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Consumer AI workloads</td>\n<td class=\"tg-tleft-valign-first\">Some pro models</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Blackwell</td>\n<td class=\"tg-tleft-valign-first\">1/2 FP32 rate (B100/B200)</td>\n<td class=\"tg-tleft-valign-first\">Higher IPC</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Enhanced Transformer Engine</td>\n<td class=\"tg-tleft-valign-first\">Yes, 2× Hopper</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Generation</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP64 (HPC)</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP32</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TF32</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP16/BF16</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP8</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>FP4</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ampere</td>\n<td class=\"tg-tleft-valign-first\">1/2 FP32 rate (A100)</td>\n<td class=\"tg-tleft-valign-first\">Dual datapath, +2× Turing</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Tensor Core, ~312 TFLOPS</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Hopper</td>\n<td class=\"tg-tleft-valign-first\">1/2 FP32 rate (H100)</td>\n<td class=\"tg-tleft-valign-first\">IPC + clock boost</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Transformer Engine</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ada</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-first\">High clocks</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Consumer AI workloads</td>\n<td class=\"tg-tleft-valign-first\">Some pro models</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Blackwell</td>\n<td class=\"tg-tleft-valign-first\">1/2 FP32 rate (B100/B200)</td>\n<td class=\"tg-tleft-valign-first\">Higher IPC</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Enhanced Transformer Engine</td>\n<td class=\"tg-tleft-valign-first\">Yes, 2× Hopper</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n</tbody>\n</table>",
    "contentMarkdown": "**Generation**\n\n**FP64 (HPC)**\n\n**FP32**\n\n**TF32**\n\n**FP16/BF16**\n\n**FP8**\n\n**FP4**\n\nAmpere\n\n1/2 FP32 rate (A100)\n\nDual datapath, +2× Turing\n\nYes\n\nTensor Core, ~312 TFLOPS\n\nNo\n\nNo\n\nHopper\n\n1/2 FP32 rate (H100)\n\nIPC + clock boost\n\nYes\n\nTransformer Engine\n\nYes\n\nNo\n\nAda\n\nLimited\n\nHigh clocks\n\nYes\n\nConsumer AI workloads\n\nSome pro models\n\nNo\n\nBlackwell\n\n1/2 FP32 rate (B100/B200)\n\nHigher IPC\n\nYes\n\nEnhanced Transformer Engine\n\nYes, 2× Hopper\n\nYes\n\n**Generation**\n\n**FP64 (HPC)**\n\n**FP32**\n\n**TF32**\n\n**FP16/BF16**\n\n**FP8**\n\n**FP4**\n\nAmpere\n\n1/2 FP32 rate (A100)\n\nDual datapath, +2× Turing\n\nYes\n\nTensor Core, ~312 TFLOPS\n\nNo\n\nNo\n\nHopper\n\n1/2 FP32 rate (H100)\n\nIPC + clock boost\n\nYes\n\nTransformer Engine\n\nYes\n\nNo\n\nAda\n\nLimited\n\nHigh clocks\n\nYes\n\nConsumer AI workloads\n\nSome pro models\n\nNo\n\nBlackwell\n\n1/2 FP32 rate (B100/B200)\n\nHigher IPC\n\nYes\n\nEnhanced Transformer Engine\n\nYes, 2× Hopper\n\nYes",
    "order": 27,
    "orderInChapter": 3,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous",
      "transformer"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 130,
      "contentLength": 4031
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#performance-analysis",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-vram-technologies-and-bandwidth-28",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Memory Architecture Evolution",
    "title": "VRAM Technologies and Bandwidth",
    "subtitle": "Memory Architecture Evolution",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Ampere (2020)</strong>:</p>\n\n    <ul>\n      <li><strong>Data center (A100):</strong> Used <strong>HBM2e</strong> with up to 1.6 TB/s bandwidth.</li>\n      <li><strong>Consumer (RTX 30-series):</strong> Used <strong>GDDR6X</strong> (developed with Micron) on higher-end cards for &gt;900 GB/s bandwidth (RTX 3090).</li>\n      <li><strong>Bus widths:</strong> Up to 384-bit on flagship consumer GPUs.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hopper (2022)</strong>:</p>\n\n    <ul>\n      <li><strong>H100:</strong> Used <strong>HBM3</strong> in some SKUs, offering up to 3 TB/s bandwidth.</li>\n      <li>Support for larger memory capacities per GPU package (up to 80 GB HBM3).</li>\n      <li>Designed for massive AI model parameter storage in-memory.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Ada Lovelace (2022)</strong>:</p>\n\n    <ul>\n      <li><strong>Consumer focus:</strong> GDDR6X on high-end cards, GDDR6 on midrange.</li>\n      <li>Bandwidth efficiency improved with <strong>L2 cache enlargement</strong>, reducing VRAM fetch pressure.</li>\n      <li>Top models (RTX 4090) reached 1 TB/s effective bandwidth.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Blackwell (2024)</strong>:</p>\n\n    <ul>\n      <li><strong>Data center (B100/B200):</strong> HBM3e with &gt;4 TB/s bandwidth in top configurations.</li>\n      <li><strong>Consumer:</strong> Higher-speed GDDR7 for &gt;1.2 TB/s bandwidth on enthusiast cards.</li>\n      <li>Improved memory controllers for lower latency in AI workloads.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Ampere (2020)</strong>:</p>\n<ul>\n      <li><strong>Data center (A100):</strong> Used <strong>HBM2e</strong> with up to 1.6 TB/s bandwidth.</li>\n      <li><strong>Consumer (RTX 30-series):</strong> Used <strong>GDDR6X</strong> (developed with Micron) on higher-end cards for &gt;900 GB/s bandwidth (RTX 3090).</li>\n      <li><strong>Bus widths:</strong> Up to 384-bit on flagship consumer GPUs.</li>\n    </ul>\n<p><strong>Hopper (2022)</strong>:</p>\n<ul>\n      <li><strong>H100:</strong> Used <strong>HBM3</strong> in some SKUs, offering up to 3 TB/s bandwidth.</li>\n      <li>Support for larger memory capacities per GPU package (up to 80 GB HBM3).</li>\n      <li>Designed for massive AI model parameter storage in-memory.</li>\n    </ul>\n<p><strong>Ada Lovelace (2022)</strong>:</p>\n<ul>\n      <li><strong>Consumer focus:</strong> GDDR6X on high-end cards, GDDR6 on midrange.</li>\n      <li>Bandwidth efficiency improved with <strong>L2 cache enlargement</strong>, reducing VRAM fetch pressure.</li>\n      <li>Top models (RTX 4090) reached 1 TB/s effective bandwidth.</li>\n    </ul>\n<p><strong>Blackwell (2024)</strong>:</p>\n<ul>\n      <li><strong>Data center (B100/B200):</strong> HBM3e with &gt;4 TB/s bandwidth in top configurations.</li>\n      <li><strong>Consumer:</strong> Higher-speed GDDR7 for &gt;1.2 TB/s bandwidth on enthusiast cards.</li>\n      <li>Improved memory controllers for lower latency in AI workloads.</li>\n    </ul>",
    "contentMarkdown": "*   **Ampere (2020)**:\n    \n    *   **Data center (A100):** Used **HBM2e** with up to 1.6 TB/s bandwidth.\n    *   **Consumer (RTX 30-series):** Used **GDDR6X** (developed with Micron) on higher-end cards for >900 GB/s bandwidth (RTX 3090).\n    *   **Bus widths:** Up to 384-bit on flagship consumer GPUs.\n*   **Hopper (2022)**:\n    \n    *   **H100:** Used **HBM3** in some SKUs, offering up to 3 TB/s bandwidth.\n    *   Support for larger memory capacities per GPU package (up to 80 GB HBM3).\n    *   Designed for massive AI model parameter storage in-memory.\n*   **Ada Lovelace (2022)**:\n    \n    *   **Consumer focus:** GDDR6X on high-end cards, GDDR6 on midrange.\n    *   Bandwidth efficiency improved with **L2 cache enlargement**, reducing VRAM fetch pressure.\n    *   Top models (RTX 4090) reached 1 TB/s effective bandwidth.\n*   **Blackwell (2024)**:\n    \n    *   **Data center (B100/B200):** HBM3e with >4 TB/s bandwidth in top configurations.\n    *   **Consumer:** Higher-speed GDDR7 for >1.2 TB/s bandwidth on enthusiast cards.\n    *   Improved memory controllers for lower latency in AI workloads.\n\n**Ampere (2020)**:\n\n*   **Data center (A100):** Used **HBM2e** with up to 1.6 TB/s bandwidth.\n*   **Consumer (RTX 30-series):** Used **GDDR6X** (developed with Micron) on higher-end cards for >900 GB/s bandwidth (RTX 3090).\n*   **Bus widths:** Up to 384-bit on flagship consumer GPUs.\n\n**Hopper (2022)**:\n\n*   **H100:** Used **HBM3** in some SKUs, offering up to 3 TB/s bandwidth.\n*   Support for larger memory capacities per GPU package (up to 80 GB HBM3).\n*   Designed for massive AI model parameter storage in-memory.\n\n**Ada Lovelace (2022)**:\n\n*   **Consumer focus:** GDDR6X on high-end cards, GDDR6 on midrange.\n*   Bandwidth efficiency improved with **L2 cache enlargement**, reducing VRAM fetch pressure.\n*   Top models (RTX 4090) reached 1 TB/s effective bandwidth.\n\n**Blackwell (2024)**:\n\n*   **Data center (B100/B200):** HBM3e with >4 TB/s bandwidth in top configurations.\n*   **Consumer:** Higher-speed GDDR7 for >1.2 TB/s bandwidth on enthusiast cards.\n*   Improved memory controllers for lower latency in AI workloads.",
    "order": 28,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 304,
      "contentLength": 3004
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#vram-technologies-and-bandwidth",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-cache-hierarchy-changes-29",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Memory Architecture Evolution",
    "title": "Cache Hierarchy Changes",
    "subtitle": "Memory Architecture Evolution",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Ampere</strong>:</p>\n\n    <ul>\n      <li>L2 cache sizes: up to 40 MB on A100, smaller on consumer (~6 MB).</li>\n      <li>Configurable L1/shared memory up to 192 KB per SM.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hopper</strong>:</p>\n\n    <ul>\n      <li>L2 cache: 50 MB on H100.</li>\n      <li>L1/shared memory bandwidth doubled compared to Ampere.</li>\n      <li>Improved cache coherence across NVLink-connected GPUs.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Ada</strong>:</p>\n\n    <ul>\n      <li>Significantly larger L2 cache for consumer GPUs (72 MB on RTX 4090).</li>\n      <li>Reduced VRAM dependence for gaming workloads.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Blackwell</strong>:</p>\n\n    <ul>\n      <li>Unified large L2 (100+ MB) for data center GPUs.</li>\n      <li>Faster shared memory with AI-aware prefetching.</li>\n      <li>AI-managed cache policies to keep transformer weights resident.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Ampere</strong>:</p>\n<ul>\n      <li>L2 cache sizes: up to 40 MB on A100, smaller on consumer (~6 MB).</li>\n      <li>Configurable L1/shared memory up to 192 KB per SM.</li>\n    </ul>\n<p><strong>Hopper</strong>:</p>\n<ul>\n      <li>L2 cache: 50 MB on H100.</li>\n      <li>L1/shared memory bandwidth doubled compared to Ampere.</li>\n      <li>Improved cache coherence across NVLink-connected GPUs.</li>\n    </ul>\n<p><strong>Ada</strong>:</p>\n<ul>\n      <li>Significantly larger L2 cache for consumer GPUs (72 MB on RTX 4090).</li>\n      <li>Reduced VRAM dependence for gaming workloads.</li>\n    </ul>\n<p><strong>Blackwell</strong>:</p>\n<ul>\n      <li>Unified large L2 (100+ MB) for data center GPUs.</li>\n      <li>Faster shared memory with AI-aware prefetching.</li>\n      <li>AI-managed cache policies to keep transformer weights resident.</li>\n    </ul>",
    "contentMarkdown": "*   **Ampere**:\n    \n    *   L2 cache sizes: up to 40 MB on A100, smaller on consumer (~6 MB).\n    *   Configurable L1/shared memory up to 192 KB per SM.\n*   **Hopper**:\n    \n    *   L2 cache: 50 MB on H100.\n    *   L1/shared memory bandwidth doubled compared to Ampere.\n    *   Improved cache coherence across NVLink-connected GPUs.\n*   **Ada**:\n    \n    *   Significantly larger L2 cache for consumer GPUs (72 MB on RTX 4090).\n    *   Reduced VRAM dependence for gaming workloads.\n*   **Blackwell**:\n    \n    *   Unified large L2 (100+ MB) for data center GPUs.\n    *   Faster shared memory with AI-aware prefetching.\n    *   AI-managed cache policies to keep transformer weights resident.\n\n**Ampere**:\n\n*   L2 cache sizes: up to 40 MB on A100, smaller on consumer (~6 MB).\n*   Configurable L1/shared memory up to 192 KB per SM.\n\n**Hopper**:\n\n*   L2 cache: 50 MB on H100.\n*   L1/shared memory bandwidth doubled compared to Ampere.\n*   Improved cache coherence across NVLink-connected GPUs.\n\n**Ada**:\n\n*   Significantly larger L2 cache for consumer GPUs (72 MB on RTX 4090).\n*   Reduced VRAM dependence for gaming workloads.\n\n**Blackwell**:\n\n*   Unified large L2 (100+ MB) for data center GPUs.\n*   Faster shared memory with AI-aware prefetching.\n*   AI-managed cache policies to keep transformer weights resident.",
    "order": 29,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous",
      "transformer"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 198,
      "contentLength": 1838
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#cache-hierarchy-changes",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-memory-latency-and-efficiency-improvements-30",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Memory Architecture Evolution",
    "title": "Memory Latency and Efficiency Improvements",
    "subtitle": "Memory Architecture Evolution",
    "contentHtml": "<ul>\n  <li><strong>Ampere:</strong> Introduced <strong>structured sparsity</strong> support in Tensor Cores, reducing VRAM traffic.</li>\n  <li><strong>Hopper:</strong> Added <strong>Transformer Engine</strong>, dynamically choosing precision to reduce memory footprint.</li>\n  <li><strong>Ada:</strong> Focused on L2 cache expansion to mask VRAM latency.</li>\n  <li><strong>Blackwell:</strong> Integrated <strong>streaming memory partitioning</strong> for large model inference; data preloaded into SM-local caches before execution.</li>\n</ul>",
    "contentMarkdown": "*   **Ampere:** Introduced **structured sparsity** support in Tensor Cores, reducing VRAM traffic.\n*   **Hopper:** Added **Transformer Engine**, dynamically choosing precision to reduce memory footprint.\n*   **Ada:** Focused on L2 cache expansion to mask VRAM latency.\n*   **Blackwell:** Integrated **streaming memory partitioning** for large model inference; data preloaded into SM-local caches before execution.",
    "order": 30,
    "orderInChapter": 3,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous",
      "transformer"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 52,
      "contentLength": 543
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#memory-latency-and-efficiency-improvements",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-multi-gpu-memory-coherency-31",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "Memory Architecture Evolution",
    "title": "Multi-GPU Memory Coherency",
    "subtitle": "Memory Architecture Evolution",
    "contentHtml": "<ul>\n  <li><strong>Ampere:</strong> NVLink 3.0 with 600 GB/s bidirectional bandwidth, partial memory coherency.</li>\n  <li><strong>Hopper:</strong> NVLink 4.0 with 900 GB/s bandwidth, full memory coherency across up to 256 GPUs in NVSwitch topologies.</li>\n  <li><strong>Ada:</strong> NVLink limited or absent in consumer cards.</li>\n  <li><strong>Blackwell:</strong> NVLink 5.0, &gt;1 TB/s, improved for multi-node AI training with direct GPU-to-GPU streaming.</li>\n</ul>",
    "contentMarkdown": "*   **Ampere:** NVLink 3.0 with 600 GB/s bidirectional bandwidth, partial memory coherency.\n*   **Hopper:** NVLink 4.0 with 900 GB/s bandwidth, full memory coherency across up to 256 GPUs in NVSwitch topologies.\n*   **Ada:** NVLink limited or absent in consumer cards.\n*   **Blackwell:** NVLink 5.0, >1 TB/s, improved for multi-node AI training with direct GPU-to-GPU streaming.",
    "order": 31,
    "orderInChapter": 4,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 55,
      "contentLength": 472
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#multi-gpu-memory-coherency",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-fundamental-architectural-components-amd-32",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "AMD GPU Architecture Overview and Comparison with NVIDIA",
    "title": "Fundamental Architectural Components (AMD)",
    "subtitle": "AMD GPU Architecture Overview and Comparison with NVIDIA",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Compute Units (CUs)</strong>:</p>\n\n    <ul>\n      <li>AMD’s equivalent of NVIDIA’s Streaming Multiprocessors (SMs).</li>\n      <li>\n        <p>Each CU contains:</p>\n\n        <ul>\n          <li><strong>64 Stream Processors</strong> (SPs, roughly equivalent to CUDA cores)</li>\n          <li><strong>Vector ALUs</strong> for FP32/INT32</li>\n          <li><strong>Scalar Units</strong> for FP32/INT32 operations shared across the CU</li>\n          <li><strong>Matrix Cores (AI Matrix Accelerators)</strong> in newer RDNA/CDNA architectures for mixed-precision AI workloads.</li>\n        </ul>\n      </li>\n      <li>CUs are grouped into Shader Arrays and Shader Engines.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Specialized Hardware</strong>:</p>\n\n    <ul>\n      <li><strong>Ray Accelerators</strong> (since RDNA 2) — analogous to NVIDIA’s RT Cores.</li>\n      <li><strong>Matrix Cores</strong> (CDNA and RDNA 3) — compete with NVIDIA’s Tensor Cores for AI/HPC workloads.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Memory Hierarchy</strong>:</p>\n\n    <ul>\n      <li>Registers per wavefront (equivalent to warp)</li>\n      <li>Local Data Share (LDS) — similar to NVIDIA’s shared memory, software-managed.</li>\n      <li>L1 Cache and scalar caches.</li>\n      <li>L2 Cache shared across CUs.</li>\n      <li>VRAM (GDDR6, GDDR6X, or HBM depending on SKU).</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Compute Units (CUs)</strong>:</p>\n<ul>\n      <li>AMD’s equivalent of NVIDIA’s Streaming Multiprocessors (SMs).</li>\n      <li>\n        <p>Each CU contains:</p>\n\n        <ul>\n          <li><strong>64 Stream Processors</strong> (SPs, roughly equivalent to CUDA cores)</li>\n          <li><strong>Vector ALUs</strong> for FP32/INT32</li>\n          <li><strong>Scalar Units</strong> for FP32/INT32 operations shared across the CU</li>\n          <li><strong>Matrix Cores (AI Matrix Accelerators)</strong> in newer RDNA/CDNA architectures for mixed-precision AI workloads.</li>\n        </ul>\n      </li>\n      <li>CUs are grouped into Shader Arrays and Shader Engines.</li>\n    </ul>\n<p>Each CU contains:</p>\n<ul>\n          <li><strong>64 Stream Processors</strong> (SPs, roughly equivalent to CUDA cores)</li>\n          <li><strong>Vector ALUs</strong> for FP32/INT32</li>\n          <li><strong>Scalar Units</strong> for FP32/INT32 operations shared across the CU</li>\n          <li><strong>Matrix Cores (AI Matrix Accelerators)</strong> in newer RDNA/CDNA architectures for mixed-precision AI workloads.</li>\n        </ul>\n<p><strong>Specialized Hardware</strong>:</p>\n<ul>\n      <li><strong>Ray Accelerators</strong> (since RDNA 2) — analogous to NVIDIA’s RT Cores.</li>\n      <li><strong>Matrix Cores</strong> (CDNA and RDNA 3) — compete with NVIDIA’s Tensor Cores for AI/HPC workloads.</li>\n    </ul>\n<p><strong>Memory Hierarchy</strong>:</p>\n<ul>\n      <li>Registers per wavefront (equivalent to warp)</li>\n      <li>Local Data Share (LDS) — similar to NVIDIA’s shared memory, software-managed.</li>\n      <li>L1 Cache and scalar caches.</li>\n      <li>L2 Cache shared across CUs.</li>\n      <li>VRAM (GDDR6, GDDR6X, or HBM depending on SKU).</li>\n    </ul>",
    "contentMarkdown": "*   **Compute Units (CUs)**:\n    \n    *   AMD’s equivalent of NVIDIA’s Streaming Multiprocessors (SMs).\n    *   Each CU contains:\n        \n        *   **64 Stream Processors** (SPs, roughly equivalent to CUDA cores)\n        *   **Vector ALUs** for FP32/INT32\n        *   **Scalar Units** for FP32/INT32 operations shared across the CU\n        *   **Matrix Cores (AI Matrix Accelerators)** in newer RDNA/CDNA architectures for mixed-precision AI workloads.\n    *   CUs are grouped into Shader Arrays and Shader Engines.\n*   **Specialized Hardware**:\n    \n    *   **Ray Accelerators** (since RDNA 2) — analogous to NVIDIA’s RT Cores.\n    *   **Matrix Cores** (CDNA and RDNA 3) — compete with NVIDIA’s Tensor Cores for AI/HPC workloads.\n*   **Memory Hierarchy**:\n    \n    *   Registers per wavefront (equivalent to warp)\n    *   Local Data Share (LDS) — similar to NVIDIA’s shared memory, software-managed.\n    *   L1 Cache and scalar caches.\n    *   L2 Cache shared across CUs.\n    *   VRAM (GDDR6, GDDR6X, or HBM depending on SKU).\n\n**Compute Units (CUs)**:\n\n*   AMD’s equivalent of NVIDIA’s Streaming Multiprocessors (SMs).\n*   Each CU contains:\n    \n    *   **64 Stream Processors** (SPs, roughly equivalent to CUDA cores)\n    *   **Vector ALUs** for FP32/INT32\n    *   **Scalar Units** for FP32/INT32 operations shared across the CU\n    *   **Matrix Cores (AI Matrix Accelerators)** in newer RDNA/CDNA architectures for mixed-precision AI workloads.\n*   CUs are grouped into Shader Arrays and Shader Engines.\n\nEach CU contains:\n\n*   **64 Stream Processors** (SPs, roughly equivalent to CUDA cores)\n*   **Vector ALUs** for FP32/INT32\n*   **Scalar Units** for FP32/INT32 operations shared across the CU\n*   **Matrix Cores (AI Matrix Accelerators)** in newer RDNA/CDNA architectures for mixed-precision AI workloads.\n\n**Specialized Hardware**:\n\n*   **Ray Accelerators** (since RDNA 2) — analogous to NVIDIA’s RT Cores.\n*   **Matrix Cores** (CDNA and RDNA 3) — compete with NVIDIA’s Tensor Cores for AI/HPC workloads.\n\n**Memory Hierarchy**:\n\n*   Registers per wavefront (equivalent to warp)\n*   Local Data Share (LDS) — similar to NVIDIA’s shared memory, software-managed.\n*   L1 Cache and scalar caches.\n*   L2 Cache shared across CUs.\n*   VRAM (GDDR6, GDDR6X, or HBM depending on SKU).",
    "order": 32,
    "orderInChapter": 1,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 317,
      "contentLength": 3177
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#fundamental-architectural-components-(amd)",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-execution-paradigms-amd-33",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "AMD GPU Architecture Overview and Comparison with NVIDIA",
    "title": "Execution Paradigms (AMD)",
    "subtitle": "AMD GPU Architecture Overview and Comparison with NVIDIA",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>Wavefronts</strong>:</p>\n\n    <ul>\n      <li>AMD’s equivalent to NVIDIA’s warps — <strong>fixed at 64 threads</strong> (vs. NVIDIA’s 32).</li>\n      <li>Like warps, wavefronts execute in lockstep using SIMD lanes.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Workgroups</strong>:</p>\n\n    <ul>\n      <li>Equivalent to NVIDIA’s CTAs (Cooperative Thread Arrays).</li>\n      <li>Workgroups contain one or more wavefronts and share LDS memory.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>SIMD Execution</strong>:</p>\n\n    <ul>\n      <li>Each CU contains multiple SIMD units (typically 4×16-wide for wavefront64).</li>\n      <li>Supports predication and divergence handling similar to NVIDIA’s warp divergence model.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Scheduling</strong>:</p>\n\n    <ul>\n      <li>Hardware schedulers issue wavefront instructions to SIMD units.</li>\n      <li>AMD uses <strong>asynchronous compute</strong> extensively — multiple compute queues can be scheduled across the GPU concurrently.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Wavefronts</strong>:</p>\n<ul>\n      <li>AMD’s equivalent to NVIDIA’s warps — <strong>fixed at 64 threads</strong> (vs. NVIDIA’s 32).</li>\n      <li>Like warps, wavefronts execute in lockstep using SIMD lanes.</li>\n    </ul>\n<p><strong>Workgroups</strong>:</p>\n<ul>\n      <li>Equivalent to NVIDIA’s CTAs (Cooperative Thread Arrays).</li>\n      <li>Workgroups contain one or more wavefronts and share LDS memory.</li>\n    </ul>\n<p><strong>SIMD Execution</strong>:</p>\n<ul>\n      <li>Each CU contains multiple SIMD units (typically 4×16-wide for wavefront64).</li>\n      <li>Supports predication and divergence handling similar to NVIDIA’s warp divergence model.</li>\n    </ul>\n<p><strong>Scheduling</strong>:</p>\n<ul>\n      <li>Hardware schedulers issue wavefront instructions to SIMD units.</li>\n      <li>AMD uses <strong>asynchronous compute</strong> extensively — multiple compute queues can be scheduled across the GPU concurrently.</li>\n    </ul>",
    "contentMarkdown": "*   **Wavefronts**:\n    \n    *   AMD’s equivalent to NVIDIA’s warps — **fixed at 64 threads** (vs. NVIDIA’s 32).\n    *   Like warps, wavefronts execute in lockstep using SIMD lanes.\n*   **Workgroups**:\n    \n    *   Equivalent to NVIDIA’s CTAs (Cooperative Thread Arrays).\n    *   Workgroups contain one or more wavefronts and share LDS memory.\n*   **SIMD Execution**:\n    \n    *   Each CU contains multiple SIMD units (typically 4×16-wide for wavefront64).\n    *   Supports predication and divergence handling similar to NVIDIA’s warp divergence model.\n*   **Scheduling**:\n    \n    *   Hardware schedulers issue wavefront instructions to SIMD units.\n    *   AMD uses **asynchronous compute** extensively — multiple compute queues can be scheduled across the GPU concurrently.\n\n**Wavefronts**:\n\n*   AMD’s equivalent to NVIDIA’s warps — **fixed at 64 threads** (vs. NVIDIA’s 32).\n*   Like warps, wavefronts execute in lockstep using SIMD lanes.\n\n**Workgroups**:\n\n*   Equivalent to NVIDIA’s CTAs (Cooperative Thread Arrays).\n*   Workgroups contain one or more wavefronts and share LDS memory.\n\n**SIMD Execution**:\n\n*   Each CU contains multiple SIMD units (typically 4×16-wide for wavefront64).\n*   Supports predication and divergence handling similar to NVIDIA’s warp divergence model.\n\n**Scheduling**:\n\n*   Hardware schedulers issue wavefront instructions to SIMD units.\n*   AMD uses **asynchronous compute** extensively — multiple compute queues can be scheduled across the GPU concurrently.",
    "order": 33,
    "orderInChapter": 2,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 198,
      "contentLength": 2030
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#execution-paradigms-(amd)",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-compute-architecture-evolution-amd-cdna-rdna-34",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "AMD GPU Architecture Overview and Comparison with NVIDIA",
    "title": "Compute Architecture Evolution (AMD CDNA & RDNA)",
    "subtitle": "AMD GPU Architecture Overview and Comparison with NVIDIA",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>CDNA (Data Center, HPC)</strong>:</p>\n\n    <ul>\n      <li>Focused on FP64, large HBM bandwidth, Infinity Fabric interconnect.</li>\n      <li>MI100, MI200, MI300 accelerators target AI training/inference and HPC workloads.</li>\n      <li>Incorporates AI Matrix Cores for FP16/BF16/FP8 acceleration.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>RDNA (Gaming &amp; Consumer)</strong>:</p>\n\n    <ul>\n      <li>RDNA 2 (2020) — introduced hardware Ray Accelerators.</li>\n      <li>RDNA 3 (2022) — chiplet-based design, higher clock speeds, improved efficiency, larger L0/L1 caches.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Trends</strong>:</p>\n\n    <ul>\n      <li>Gradual convergence with NVIDIA’s philosophy on specialized cores for AI and ray tracing.</li>\n      <li>CDNA increasingly mirrors NVIDIA’s HPC-first approach, while RDNA focuses on gaming + mixed AI features.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>CDNA (Data Center, HPC)</strong>:</p>\n<ul>\n      <li>Focused on FP64, large HBM bandwidth, Infinity Fabric interconnect.</li>\n      <li>MI100, MI200, MI300 accelerators target AI training/inference and HPC workloads.</li>\n      <li>Incorporates AI Matrix Cores for FP16/BF16/FP8 acceleration.</li>\n    </ul>\n<p><strong>RDNA (Gaming &amp; Consumer)</strong>:</p>\n<ul>\n      <li>RDNA 2 (2020) — introduced hardware Ray Accelerators.</li>\n      <li>RDNA 3 (2022) — chiplet-based design, higher clock speeds, improved efficiency, larger L0/L1 caches.</li>\n    </ul>\n<p><strong>Trends</strong>:</p>\n<ul>\n      <li>Gradual convergence with NVIDIA’s philosophy on specialized cores for AI and ray tracing.</li>\n      <li>CDNA increasingly mirrors NVIDIA’s HPC-first approach, while RDNA focuses on gaming + mixed AI features.</li>\n    </ul>",
    "contentMarkdown": "*   **CDNA (Data Center, HPC)**:\n    \n    *   Focused on FP64, large HBM bandwidth, Infinity Fabric interconnect.\n    *   MI100, MI200, MI300 accelerators target AI training/inference and HPC workloads.\n    *   Incorporates AI Matrix Cores for FP16/BF16/FP8 acceleration.\n*   **RDNA (Gaming & Consumer)**:\n    \n    *   RDNA 2 (2020) — introduced hardware Ray Accelerators.\n    *   RDNA 3 (2022) — chiplet-based design, higher clock speeds, improved efficiency, larger L0/L1 caches.\n*   **Trends**:\n    \n    *   Gradual convergence with NVIDIA’s philosophy on specialized cores for AI and ray tracing.\n    *   CDNA increasingly mirrors NVIDIA’s HPC-first approach, while RDNA focuses on gaming + mixed AI features.\n\n**CDNA (Data Center, HPC)**:\n\n*   Focused on FP64, large HBM bandwidth, Infinity Fabric interconnect.\n*   MI100, MI200, MI300 accelerators target AI training/inference and HPC workloads.\n*   Incorporates AI Matrix Cores for FP16/BF16/FP8 acceleration.\n\n**RDNA (Gaming & Consumer)**:\n\n*   RDNA 2 (2020) — introduced hardware Ray Accelerators.\n*   RDNA 3 (2022) — chiplet-based design, higher clock speeds, improved efficiency, larger L0/L1 caches.\n\n**Trends**:\n\n*   Gradual convergence with NVIDIA’s philosophy on specialized cores for AI and ray tracing.\n*   CDNA increasingly mirrors NVIDIA’s HPC-first approach, while RDNA focuses on gaming + mixed AI features.",
    "order": 34,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 187,
      "contentLength": 1772
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#compute-architecture-evolution-(amd-cdna-&-rdna)",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-floating-point-precision-performance-evolution-35",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "AMD GPU Architecture Overview and Comparison with NVIDIA",
    "title": "Floating-Point Precision Performance Evolution",
    "subtitle": "AMD GPU Architecture Overview and Comparison with NVIDIA",
    "contentHtml": "<ul>\n  <li>\n    <p>AMD’s HPC GPUs (CDNA) support a similar spread of precisions:</p>\n\n    <ul>\n      <li><strong>FP64:</strong> Full-rate or half-rate in CDNA for HPC.</li>\n      <li><strong>FP32:</strong> High throughput in all architectures; RDNA targets gaming efficiency.</li>\n      <li><strong>FP16/BF16:</strong> Supported in Matrix Cores for AI workloads.</li>\n      <li><strong>FP8:</strong> Introduced in MI300 for AI inference acceleration.</li>\n      <li><strong>No FP4 yet</strong> in shipping AMD hardware (as of 2025).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>NVIDIA Comparison:</strong></p>\n\n    <ul>\n      <li>AMD lagged behind in adopting sub-16-bit formats but has now matched FP8 capabilities in data center parts.</li>\n      <li>NVIDIA maintains an edge in software-optimized mixed-precision scheduling (Transformer Engine, FP4).</li>\n    </ul>\n  </li>\n</ul>\n<p>AMD’s HPC GPUs (CDNA) support a similar spread of precisions:</p>\n<ul>\n      <li><strong>FP64:</strong> Full-rate or half-rate in CDNA for HPC.</li>\n      <li><strong>FP32:</strong> High throughput in all architectures; RDNA targets gaming efficiency.</li>\n      <li><strong>FP16/BF16:</strong> Supported in Matrix Cores for AI workloads.</li>\n      <li><strong>FP8:</strong> Introduced in MI300 for AI inference acceleration.</li>\n      <li><strong>No FP4 yet</strong> in shipping AMD hardware (as of 2025).</li>\n    </ul>\n<p><strong>NVIDIA Comparison:</strong></p>\n<ul>\n      <li>AMD lagged behind in adopting sub-16-bit formats but has now matched FP8 capabilities in data center parts.</li>\n      <li>NVIDIA maintains an edge in software-optimized mixed-precision scheduling (Transformer Engine, FP4).</li>\n    </ul>",
    "contentMarkdown": "*   AMD’s HPC GPUs (CDNA) support a similar spread of precisions:\n    \n    *   **FP64:** Full-rate or half-rate in CDNA for HPC.\n    *   **FP32:** High throughput in all architectures; RDNA targets gaming efficiency.\n    *   **FP16/BF16:** Supported in Matrix Cores for AI workloads.\n    *   **FP8:** Introduced in MI300 for AI inference acceleration.\n    *   **No FP4 yet** in shipping AMD hardware (as of 2025).\n*   **NVIDIA Comparison:**\n    \n    *   AMD lagged behind in adopting sub-16-bit formats but has now matched FP8 capabilities in data center parts.\n    *   NVIDIA maintains an edge in software-optimized mixed-precision scheduling (Transformer Engine, FP4).\n\nAMD’s HPC GPUs (CDNA) support a similar spread of precisions:\n\n*   **FP64:** Full-rate or half-rate in CDNA for HPC.\n*   **FP32:** High throughput in all architectures; RDNA targets gaming efficiency.\n*   **FP16/BF16:** Supported in Matrix Cores for AI workloads.\n*   **FP8:** Introduced in MI300 for AI inference acceleration.\n*   **No FP4 yet** in shipping AMD hardware (as of 2025).\n\n**NVIDIA Comparison:**\n\n*   AMD lagged behind in adopting sub-16-bit formats but has now matched FP8 capabilities in data center parts.\n*   NVIDIA maintains an edge in software-optimized mixed-precision scheduling (Transformer Engine, FP4).",
    "order": 35,
    "orderInChapter": 4,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous",
      "transformer"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 184,
      "contentLength": 1706
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#floating-point-precision-performance-evolution",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-memory-architecture-evolution-36",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "AMD GPU Architecture Overview and Comparison with NVIDIA",
    "title": "Memory Architecture Evolution",
    "subtitle": "AMD GPU Architecture Overview and Comparison with NVIDIA",
    "contentHtml": "<ul>\n  <li>\n    <p><strong>VRAM Types</strong></p>\n\n    <ul>\n      <li><strong>RDNA:</strong> Primarily GDDR6 (with Infinity Cache to offset bandwidth needs).</li>\n      <li><strong>CDNA:</strong> HBM2/HBM2e (MI100), HBM3 (MI250), HBM3e (MI300).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cache Strategies</strong></p>\n\n    <ul>\n      <li><strong>Infinity Cache</strong> (RDNA 2 &amp; 3) — very large L3 cache (up to 128 MB) to minimize VRAM accesses in gaming workloads.</li>\n      <li>L1/L2 sizes generally smaller than NVIDIA’s in absolute terms, but Infinity Cache changes bandwidth behavior.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Interconnects</strong></p>\n\n    <ul>\n      <li><strong>Infinity Fabric</strong> — scalable, high-bandwidth interconnect across dies or GPUs.</li>\n      <li>Competes with NVIDIA NVLink; excels in multi-GPU HPC configurations.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>NVIDIA Comparison:</strong></p>\n\n    <ul>\n      <li>NVIDIA tends to rely on increasing L2 cache size and raw VRAM bandwidth;</li>\n      <li>AMD focuses on large on-die caches (Infinity Cache) to improve effective bandwidth, especially in gaming SKUs.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>VRAM Types</strong></p>\n<ul>\n      <li><strong>RDNA:</strong> Primarily GDDR6 (with Infinity Cache to offset bandwidth needs).</li>\n      <li><strong>CDNA:</strong> HBM2/HBM2e (MI100), HBM3 (MI250), HBM3e (MI300).</li>\n    </ul>\n<p><strong>Cache Strategies</strong></p>\n<ul>\n      <li><strong>Infinity Cache</strong> (RDNA 2 &amp; 3) — very large L3 cache (up to 128 MB) to minimize VRAM accesses in gaming workloads.</li>\n      <li>L1/L2 sizes generally smaller than NVIDIA’s in absolute terms, but Infinity Cache changes bandwidth behavior.</li>\n    </ul>\n<p><strong>Interconnects</strong></p>\n<ul>\n      <li><strong>Infinity Fabric</strong> — scalable, high-bandwidth interconnect across dies or GPUs.</li>\n      <li>Competes with NVIDIA NVLink; excels in multi-GPU HPC configurations.</li>\n    </ul>\n<p><strong>NVIDIA Comparison:</strong></p>\n<ul>\n      <li>NVIDIA tends to rely on increasing L2 cache size and raw VRAM bandwidth;</li>\n      <li>AMD focuses on large on-die caches (Infinity Cache) to improve effective bandwidth, especially in gaming SKUs.</li>\n    </ul>",
    "contentMarkdown": "*   **VRAM Types**\n    \n    *   **RDNA:** Primarily GDDR6 (with Infinity Cache to offset bandwidth needs).\n    *   **CDNA:** HBM2/HBM2e (MI100), HBM3 (MI250), HBM3e (MI300).\n*   **Cache Strategies**\n    \n    *   **Infinity Cache** (RDNA 2 & 3) — very large L3 cache (up to 128 MB) to minimize VRAM accesses in gaming workloads.\n    *   L1/L2 sizes generally smaller than NVIDIA’s in absolute terms, but Infinity Cache changes bandwidth behavior.\n*   **Interconnects**\n    \n    *   **Infinity Fabric** — scalable, high-bandwidth interconnect across dies or GPUs.\n    *   Competes with NVIDIA NVLink; excels in multi-GPU HPC configurations.\n*   **NVIDIA Comparison:**\n    \n    *   NVIDIA tends to rely on increasing L2 cache size and raw VRAM bandwidth;\n    *   AMD focuses on large on-die caches (Infinity Cache) to improve effective bandwidth, especially in gaming SKUs.\n\n**VRAM Types**\n\n*   **RDNA:** Primarily GDDR6 (with Infinity Cache to offset bandwidth needs).\n*   **CDNA:** HBM2/HBM2e (MI100), HBM3 (MI250), HBM3e (MI300).\n\n**Cache Strategies**\n\n*   **Infinity Cache** (RDNA 2 & 3) — very large L3 cache (up to 128 MB) to minimize VRAM accesses in gaming workloads.\n*   L1/L2 sizes generally smaller than NVIDIA’s in absolute terms, but Infinity Cache changes bandwidth behavior.\n\n**Interconnects**\n\n*   **Infinity Fabric** — scalable, high-bandwidth interconnect across dies or GPUs.\n*   Competes with NVIDIA NVLink; excels in multi-GPU HPC configurations.\n\n**NVIDIA Comparison:**\n\n*   NVIDIA tends to rely on increasing L2 cache size and raw VRAM bandwidth;\n*   AMD focuses on large on-die caches (Infinity Cache) to improve effective bandwidth, especially in gaming SKUs.",
    "order": 36,
    "orderInChapter": 5,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "miscellaneous"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 238,
      "contentLength": 2280
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#memory-architecture-evolution",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-comparative-analysis-amd-vs-nvidia-architectures-37",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "AMD GPU Architecture Overview and Comparison with NVIDIA",
    "title": "Comparative Analysis: AMD vs. NVIDIA Architectures",
    "subtitle": "AMD GPU Architecture Overview and Comparison with NVIDIA",
    "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Aspect</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>AMD Approach</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>NVIDIA Approach</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Execution Unit</td>\n<td class=\"tg-tleft-valign-first\">Compute Units (64 SPs)</td>\n<td class=\"tg-tleft-valign-second\">Streaming Multiprocessors (128 CUDA cores)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Warp/Wavefront</td>\n<td class=\"tg-tleft-valign-first\">Wavefront64</td>\n<td class=\"tg-tleft-valign-second\">Warp32</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Specialized Cores</td>\n<td class=\"tg-tleft-valign-first\">Ray Accelerators, Matrix Cores</td>\n<td class=\"tg-tleft-valign-second\">RT Cores, Tensor Cores</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">AI Acceleration</td>\n<td class=\"tg-tleft-valign-first\">Matrix Cores (FP16/BF16/FP8)</td>\n<td class=\"tg-tleft-valign-second\">Tensor Cores (FP16/BF16/FP8/FP4, Transformer Engine)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">VRAM Strategy</td>\n<td class=\"tg-tleft-valign-first\">Infinity Cache + VRAM</td>\n<td class=\"tg-tleft-valign-second\">Large L2 + VRAM</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">HPC Interconnect</td>\n<td class=\"tg-tleft-valign-first\">Infinity Fabric</td>\n<td class=\"tg-tleft-valign-second\">NVLink/NVSwitch</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Sub-16-bit Formats</td>\n<td class=\"tg-tleft-valign-first\">FP8 (CDNA MI300)</td>\n<td class=\"tg-tleft-valign-second\">FP8, FP4</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Software Ecosystem</td>\n<td class=\"tg-tleft-valign-first\">ROCm, HIP</td>\n<td class=\"tg-tleft-valign-second\">CUDA, cuDNN, TensorRT</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Consumer Focus</td>\n<td class=\"tg-tleft-valign-first\">RDNA for gaming</td>\n<td class=\"tg-tleft-valign-second\">Ada for gaming</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">HPC Focus</td>\n<td class=\"tg-tleft-valign-first\">CDNA for data center</td>\n<td class=\"tg-tleft-valign-second\">Hopper/Blackwell for data center</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Aspect</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>AMD Approach</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>NVIDIA Approach</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Execution Unit</td>\n<td class=\"tg-tleft-valign-first\">Compute Units (64 SPs)</td>\n<td class=\"tg-tleft-valign-second\">Streaming Multiprocessors (128 CUDA cores)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Warp/Wavefront</td>\n<td class=\"tg-tleft-valign-first\">Wavefront64</td>\n<td class=\"tg-tleft-valign-second\">Warp32</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Specialized Cores</td>\n<td class=\"tg-tleft-valign-first\">Ray Accelerators, Matrix Cores</td>\n<td class=\"tg-tleft-valign-second\">RT Cores, Tensor Cores</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">AI Acceleration</td>\n<td class=\"tg-tleft-valign-first\">Matrix Cores (FP16/BF16/FP8)</td>\n<td class=\"tg-tleft-valign-second\">Tensor Cores (FP16/BF16/FP8/FP4, Transformer Engine)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">VRAM Strategy</td>\n<td class=\"tg-tleft-valign-first\">Infinity Cache + VRAM</td>\n<td class=\"tg-tleft-valign-second\">Large L2 + VRAM</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">HPC Interconnect</td>\n<td class=\"tg-tleft-valign-first\">Infinity Fabric</td>\n<td class=\"tg-tleft-valign-second\">NVLink/NVSwitch</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Sub-16-bit Formats</td>\n<td class=\"tg-tleft-valign-first\">FP8 (CDNA MI300)</td>\n<td class=\"tg-tleft-valign-second\">FP8, FP4</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Software Ecosystem</td>\n<td class=\"tg-tleft-valign-first\">ROCm, HIP</td>\n<td class=\"tg-tleft-valign-second\">CUDA, cuDNN, TensorRT</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Consumer Focus</td>\n<td class=\"tg-tleft-valign-first\">RDNA for gaming</td>\n<td class=\"tg-tleft-valign-second\">Ada for gaming</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">HPC Focus</td>\n<td class=\"tg-tleft-valign-first\">CDNA for data center</td>\n<td class=\"tg-tleft-valign-second\">Hopper/Blackwell for data center</td>\n</tr>\n</tbody>\n</table>",
    "contentMarkdown": "**Aspect**\n\n**AMD Approach**\n\n**NVIDIA Approach**\n\nExecution Unit\n\nCompute Units (64 SPs)\n\nStreaming Multiprocessors (128 CUDA cores)\n\nWarp/Wavefront\n\nWavefront64\n\nWarp32\n\nSpecialized Cores\n\nRay Accelerators, Matrix Cores\n\nRT Cores, Tensor Cores\n\nAI Acceleration\n\nMatrix Cores (FP16/BF16/FP8)\n\nTensor Cores (FP16/BF16/FP8/FP4, Transformer Engine)\n\nVRAM Strategy\n\nInfinity Cache + VRAM\n\nLarge L2 + VRAM\n\nHPC Interconnect\n\nInfinity Fabric\n\nNVLink/NVSwitch\n\nSub-16-bit Formats\n\nFP8 (CDNA MI300)\n\nFP8, FP4\n\nSoftware Ecosystem\n\nROCm, HIP\n\nCUDA, cuDNN, TensorRT\n\nConsumer Focus\n\nRDNA for gaming\n\nAda for gaming\n\nHPC Focus\n\nCDNA for data center\n\nHopper/Blackwell for data center\n\n**Aspect**\n\n**AMD Approach**\n\n**NVIDIA Approach**\n\nExecution Unit\n\nCompute Units (64 SPs)\n\nStreaming Multiprocessors (128 CUDA cores)\n\nWarp/Wavefront\n\nWavefront64\n\nWarp32\n\nSpecialized Cores\n\nRay Accelerators, Matrix Cores\n\nRT Cores, Tensor Cores\n\nAI Acceleration\n\nMatrix Cores (FP16/BF16/FP8)\n\nTensor Cores (FP16/BF16/FP8/FP4, Transformer Engine)\n\nVRAM Strategy\n\nInfinity Cache + VRAM\n\nLarge L2 + VRAM\n\nHPC Interconnect\n\nInfinity Fabric\n\nNVLink/NVSwitch\n\nSub-16-bit Formats\n\nFP8 (CDNA MI300)\n\nFP8, FP4\n\nSoftware Ecosystem\n\nROCm, HIP\n\nCUDA, cuDNN, TensorRT\n\nConsumer Focus\n\nRDNA for gaming\n\nAda for gaming\n\nHPC Focus\n\nCDNA for data center\n\nHopper/Blackwell for data center",
    "order": 37,
    "orderInChapter": 6,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous",
      "transformer"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 172,
      "contentLength": 4359
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#comparative-analysis:-amd-vs.-nvidia-architectures",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  },
  {
    "id": "ai-gpu-architecture-key-takeaways-38",
    "domain": "ai_primers",
    "category": "Miscellaneous",
    "article": "GPU Architecture",
    "articleSlug": "gpu-architecture",
    "chapter": "AMD GPU Architecture Overview and Comparison with NVIDIA",
    "title": "Key Takeaways",
    "subtitle": "AMD GPU Architecture Overview and Comparison with NVIDIA",
    "contentHtml": "<ol>\n  <li><strong>Warp/Wavefront Size Difference:</strong> NVIDIA’s 32-thread warps offer finer granularity, while AMD’s 64-thread wavefronts can be more efficient in highly parallel workloads but risk more idle lanes under divergence.</li>\n  <li><strong>Memory Philosophy:</strong> NVIDIA favors enlarging L2 cache and maximizing bandwidth; AMD offsets narrower VRAM interfaces with large on-die Infinity Cache.</li>\n  <li><strong>AI Focus:</strong> NVIDIA’s Transformer Engine and <code class=\"language-plaintext highlighter-rouge\">float4</code> give it a precision flexibility edge for LLM workloads, while AMD is catching up rapidly in FP8 inference.</li>\n  <li><strong>Interconnect Strategy:</strong> Both offer high-bandwidth interconnects (Infinity Fabric vs. NVLink), but NVIDIA currently scales to larger GPU counts in a single coherent memory space.</li>\n  <li><strong>Software Ecosystem:</strong> NVIDIA’s CUDA ecosystem remains more mature, but AMD’s ROCm stack has made major strides in HPC adoption.</li>\n</ol>",
    "contentMarkdown": "1.  **Warp/Wavefront Size Difference:** NVIDIA’s 32-thread warps offer finer granularity, while AMD’s 64-thread wavefronts can be more efficient in highly parallel workloads but risk more idle lanes under divergence.\n2.  **Memory Philosophy:** NVIDIA favors enlarging L2 cache and maximizing bandwidth; AMD offsets narrower VRAM interfaces with large on-die Infinity Cache.\n3.  **AI Focus:** NVIDIA’s Transformer Engine and `float4` give it a precision flexibility edge for LLM workloads, while AMD is catching up rapidly in FP8 inference.\n4.  **Interconnect Strategy:** Both offer high-bandwidth interconnects (Infinity Fabric vs. NVLink), but NVIDIA currently scales to larger GPU counts in a single coherent memory space.\n5.  **Software Ecosystem:** NVIDIA’s CUDA ecosystem remains more mature, but AMD’s ROCm stack has made major strides in HPC adoption.",
    "order": 38,
    "orderInChapter": 7,
    "difficulty": 5,
    "estimatedMinutes": 1,
    "tags": [
      "miscellaneous",
      "transformer",
      "llm"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 121,
      "contentLength": 1025
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#key-takeaways",
    "scrapedAt": "2025-12-28T11:56:41.455Z"
  }
]