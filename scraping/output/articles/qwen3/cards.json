[
  {
    "id": "ai-qwen3-moe-configuration-1",
    "domain": "ai_primers",
    "category": "Models",
    "article": "Qwen 3",
    "articleSlug": "qwen3",
    "chapter": "Architectural Advances",
    "title": "MoE Configuration",
    "subtitle": "Architectural Advances",
    "contentHtml": "<ul>\n  <li><strong>Expert Count:</strong> Qwen3 MoE models have <strong>128 total experts</strong> with <strong>8 activated experts per token</strong>.</li>\n  <li><strong>Architecture:</strong> The MoE design specifically <strong>excludes shared experts</strong>, unlike some other MoE architectures.</li>\n  <li><strong>Load Balancing:</strong> The model adopts the <strong>global-batch load balancing loss</strong> to encourage expert specialization.</li>\n  <li><strong>Flagship Specs:</strong> The Qwen3-235B-A22B model has <strong>94 layers</strong> and utilizes 64 Query heads and 4 Key/Value heads (64 / 4).</li>\n</ul>",
    "contentMarkdown": "*   **Expert Count:** Qwen3 MoE models have **128 total experts** with **8 activated experts per token**.\n*   **Architecture:** The MoE design specifically **excludes shared experts**, unlike some other MoE architectures.\n*   **Load Balancing:** The model adopts the **global-batch load balancing loss** to encourage expert specialization.\n*   **Flagship Specs:** The Qwen3-235B-A22B model has **94 layers** and utilizes 64 Query heads and 4 Key/Value heads (64 / 4).",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 66,
      "contentLength": 623
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/qwen3/#moe-configuration",
    "scrapedAt": "2025-12-28T11:52:13.873Z"
  },
  {
    "id": "ai-qwen3-attention-and-context-scaling-2",
    "domain": "ai_primers",
    "category": "Models",
    "article": "Qwen 3",
    "articleSlug": "qwen3",
    "chapter": "Architectural Advances",
    "title": "Attention and Context Scaling",
    "subtitle": "Architectural Advances",
    "contentHtml": "<ul>\n  <li><strong>Attention Stabilization (QK-Norm):</strong> Qwen3 introduces <strong>QK-Norm</strong> to normalize queries and keys independently before the dot-product attention. This addresses instability at long sequence lengths, reducing perplexity growth by <strong>20–30%</strong> relative to baseline configurations during <strong>128K context extrapolation</strong>.</li>\n  <li><strong>Efficiency:</strong> <strong>Grouped Query Attention (GQA)</strong> is implemented, sharing Key-Value pairs across query heads to yield <strong>linear scaling with a reduced memory footprint</strong>.</li>\n  <li><strong>Positional Embedding:</strong> <strong>Rotary Position Embeddings (RoPE)</strong> are enhanced with <strong>Adaptive Basis Function (ABF)</strong>. The base frequency of RoPE is increased from $10,000$ to $1,000,000$ using ABF to maximize effective sequence utilization during context extension.</li>\n  <li><strong>Long-Sequence Inference:</strong> <strong>Dual Chunked Attention (DCA)</strong> is introduced to optimize long-sequence inference by chunking sequences into overlapping windows, achieving a throughput improvement of <strong>1.6$\\times$ at 128K tokens</strong> without accuracy loss. Context length is extended to <strong>128K</strong> using the <strong>YaRN</strong> method.</li>\n  <li><strong>Tokenizer:</strong> Qwen’s tokenizer is utilized, implementing byte-level byte-pair encoding (BBPE) with a vocabulary size of <strong>151,669</strong>.</li>\n</ul>",
    "contentMarkdown": "*   **Attention Stabilization (QK-Norm):** Qwen3 introduces **QK-Norm** to normalize queries and keys independently before the dot-product attention. This addresses instability at long sequence lengths, reducing perplexity growth by **20–30%** relative to baseline configurations during **128K context extrapolation**.\n*   **Efficiency:** **Grouped Query Attention (GQA)** is implemented, sharing Key-Value pairs across query heads to yield **linear scaling with a reduced memory footprint**.\n*   **Positional Embedding:** **Rotary Position Embeddings (RoPE)** are enhanced with **Adaptive Basis Function (ABF)**. The base frequency of RoPE is increased from $10,000$ to $1,000,000$ using ABF to maximize effective sequence utilization during context extension.\n*   **Long-Sequence Inference:** **Dual Chunked Attention (DCA)** is introduced to optimize long-sequence inference by chunking sequences into overlapping windows, achieving a throughput improvement of **1.6$\\\\times$ at 128K tokens** without accuracy loss. Context length is extended to **128K** using the **YaRN** method.\n*   **Tokenizer:** Qwen’s tokenizer is utilized, implementing byte-level byte-pair encoding (BBPE) with a vocabulary size of **151,669**.",
    "order": 2,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models",
      "attention",
      "embedding"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 153,
      "contentLength": 1488
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/qwen3/#attention-and-context-scaling",
    "scrapedAt": "2025-12-28T11:52:13.873Z"
  },
  {
    "id": "ai-qwen3-corpus-and-data-synthesis-3",
    "domain": "ai_primers",
    "category": "Models",
    "article": "Qwen 3",
    "articleSlug": "qwen3",
    "chapter": "Pre-Training Innovations",
    "title": "Corpus and Data Synthesis",
    "subtitle": "Pre-Training Innovations",
    "contentHtml": "<ul>\n  <li><strong>Data Sources:</strong> The dataset covers <strong>119 languages</strong> and includes high-quality content in coding, STEM, reasoning tasks, and synthesized data.</li>\n  <li><strong>Synthesis Methods:</strong> Synthetic data (trillions of tokens) is generated using specialized models: <strong>Qwen2.5-VL</strong> (for PDF text recognition), <strong>Qwen2.5-Math</strong> (for mathematical content), and <strong>Qwen2.5-Coder</strong> (for code snippets). Code datasets are automatically validated against execution environments.</li>\n  <li><strong>Instance-Level Optimization:</strong> Unlike previous studies, Qwen3 optimizes the data mixture at the <strong>instance-level</strong> using fine-grained data labels and ablation experiments on small proxy models.</li>\n</ul>",
    "contentMarkdown": "*   **Data Sources:** The dataset covers **119 languages** and includes high-quality content in coding, STEM, reasoning tasks, and synthesized data.\n*   **Synthesis Methods:** Synthetic data (trillions of tokens) is generated using specialized models: **Qwen2.5-VL** (for PDF text recognition), **Qwen2.5-Math** (for mathematical content), and **Qwen2.5-Coder** (for code snippets). Code datasets are automatically validated against execution environments.\n*   **Instance-Level Optimization:** Unlike previous studies, Qwen3 optimizes the data mixture at the **instance-level** using fine-grained data labels and ablation experiments on small proxy models.",
    "order": 3,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models",
      "optimization"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 80,
      "contentLength": 792
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/qwen3/#corpus-and-data-synthesis",
    "scrapedAt": "2025-12-28T11:52:13.873Z"
  },
  {
    "id": "ai-qwen3-three-stage-pre-training-process-4",
    "domain": "ai_primers",
    "category": "Models",
    "article": "Qwen 3",
    "articleSlug": "qwen3",
    "chapter": "Pre-Training Innovations",
    "title": "Three-Stage Pre-Training Process",
    "subtitle": "Pre-Training Innovations",
    "contentHtml": "<ol>\n  <li><strong>General Stage (S1):</strong> Training on <strong>over 30 trillion tokens</strong> at a sequence length of 4,096 tokens to build a strong foundation of general knowledge.</li>\n  <li><strong>Reasoning Stage (S2):</strong> Training on <strong>about 5T higher-quality tokens</strong> with an increased proportion of STEM, coding, and reasoning data.</li>\n  <li><strong>Long Context Stage (S3):</strong> Training on <strong>hundreds of billions of tokens</strong> at a sequence length of <strong>32,768 tokens</strong> using YaRN and DCA. The corpus for this stage includes <strong>75% of text between 16,384 and 32,768 tokens</strong> in length.</li>\n</ol>",
    "contentMarkdown": "1.  **General Stage (S1):** Training on **over 30 trillion tokens** at a sequence length of 4,096 tokens to build a strong foundation of general knowledge.\n2.  **Reasoning Stage (S2):** Training on **about 5T higher-quality tokens** with an increased proportion of STEM, coding, and reasoning data.\n3.  **Long Context Stage (S3):** Training on **hundreds of billions of tokens** at a sequence length of **32,768 tokens** using YaRN and DCA. The corpus for this stage includes **75% of text between 16,384 and 32,768 tokens** in length.",
    "order": 4,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 84,
      "contentLength": 671
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/qwen3/#three-stage-pre-training-process",
    "scrapedAt": "2025-12-28T11:52:13.873Z"
  },
  {
    "id": "ai-qwen3-four-stage-post-training-pipeline-flagship-models-5",
    "domain": "ai_primers",
    "category": "Models",
    "article": "Qwen 3",
    "articleSlug": "qwen3",
    "chapter": "Post-Training and Thinking Control",
    "title": "Four-Stage Post-Training Pipeline (Flagship Models)",
    "subtitle": "Post-Training and Thinking Control",
    "contentHtml": "<ol>\n  <li><strong>Long-CoT Cold Start (S1):</strong> Supervised fine-tuning (SFT) is performed on curated Chain-of-Thought (CoT) examples that <strong>require deeper reasoning</strong>, excluding queries solvable without CoT.</li>\n  <li><strong>Reasoning RL (S2):</strong> Reinforcement learning is applied using <strong>GRPO</strong> (Group Relative Policy Optimization) on <strong>3,995 challenging query-verifier pairs</strong> not used in the cold-start phase. This stage resulted in the AIME’24 score increasing from <strong>70.1 to 85.1</strong> over 170 RL training steps for Qwen3-235B-A22B.</li>\n  <li><strong>Thinking Mode Fusion (S3):</strong> Continual SFT integrates <strong>“thinking” and “non-thinking” modes</strong> using a specific chat template.\n    <ul>\n      <li><strong>Explicit Control:</strong> The flags <code class=\"language-plaintext highlighter-rouge\">/think</code> and <code class=\"language-plaintext highlighter-rouge\">/no think</code> are introduced in the user query or system message. The non-thinking mode response retains an <strong>empty thinking block</strong> (<code class=\"language-plaintext highlighter-rouge\">&lt;think&gt;&lt;/think&gt;</code>) to ensure internal format consistency.</li>\n      <li><strong>Dynamic Budget:</strong> The model naturally develops the ability to handle <strong>intermediate cases</strong> where reasoning is halted at a user-defined threshold (<strong>thinking budget</strong>), inserting a stop instruction. Performance scales <strong>smoothly and consistently</strong> with the allocated thinking budget.</li>\n    </ul>\n  </li>\n  <li><strong>General RL (S4):</strong> Aims for broad capability enhancement across <strong>over 20 distinct tasks</strong>.\n    <ul>\n      <li><strong>Reward System:</strong> Utilizes three distinct reward types: <strong>Rule-based Reward</strong> (for format adherence), <strong>Model-based Reward with Reference Answer</strong> (using Qwen2.5-72B-Instruct to score output), and <strong>Model-based Reward without Reference Answer</strong> (trained on human preference data).</li>\n      <li><strong>Agent Ability:</strong> The model is trained to correctly invoke tools via designated interfaces, performing <strong>complete multi-turn interaction cycles with real environment execution feedback</strong>.</li>\n    </ul>\n  </li>\n</ol>\n<ul>\n      <li><strong>Explicit Control:</strong> The flags <code class=\"language-plaintext highlighter-rouge\">/think</code> and <code class=\"language-plaintext highlighter-rouge\">/no think</code> are introduced in the user query or system message. The non-thinking mode response retains an <strong>empty thinking block</strong> (<code class=\"language-plaintext highlighter-rouge\">&lt;think&gt;&lt;/think&gt;</code>) to ensure internal format consistency.</li>\n      <li><strong>Dynamic Budget:</strong> The model naturally develops the ability to handle <strong>intermediate cases</strong> where reasoning is halted at a user-defined threshold (<strong>thinking budget</strong>), inserting a stop instruction. Performance scales <strong>smoothly and consistently</strong> with the allocated thinking budget.</li>\n    </ul>\n<ul>\n      <li><strong>Reward System:</strong> Utilizes three distinct reward types: <strong>Rule-based Reward</strong> (for format adherence), <strong>Model-based Reward with Reference Answer</strong> (using Qwen2.5-72B-Instruct to score output), and <strong>Model-based Reward without Reference Answer</strong> (trained on human preference data).</li>\n      <li><strong>Agent Ability:</strong> The model is trained to correctly invoke tools via designated interfaces, performing <strong>complete multi-turn interaction cycles with real environment execution feedback</strong>.</li>\n    </ul>",
    "contentMarkdown": "1.  **Long-CoT Cold Start (S1):** Supervised fine-tuning (SFT) is performed on curated Chain-of-Thought (CoT) examples that **require deeper reasoning**, excluding queries solvable without CoT.\n2.  **Reasoning RL (S2):** Reinforcement learning is applied using **GRPO** (Group Relative Policy Optimization) on **3,995 challenging query-verifier pairs** not used in the cold-start phase. This stage resulted in the AIME’24 score increasing from **70.1 to 85.1** over 170 RL training steps for Qwen3-235B-A22B.\n3.  **Thinking Mode Fusion (S3):** Continual SFT integrates **“thinking” and “non-thinking” modes** using a specific chat template.\n    *   **Explicit Control:** The flags `/think` and `/no think` are introduced in the user query or system message. The non-thinking mode response retains an **empty thinking block** (`<think></think>`) to ensure internal format consistency.\n    *   **Dynamic Budget:** The model naturally develops the ability to handle **intermediate cases** where reasoning is halted at a user-defined threshold (**thinking budget**), inserting a stop instruction. Performance scales **smoothly and consistently** with the allocated thinking budget.\n4.  **General RL (S4):** Aims for broad capability enhancement across **over 20 distinct tasks**.\n    *   **Reward System:** Utilizes three distinct reward types: **Rule-based Reward** (for format adherence), **Model-based Reward with Reference Answer** (using Qwen2.5-72B-Instruct to score output), and **Model-based Reward without Reference Answer** (trained on human preference data).\n    *   **Agent Ability:** The model is trained to correctly invoke tools via designated interfaces, performing **complete multi-turn interaction cycles with real environment execution feedback**.\n\n*   **Explicit Control:** The flags `/think` and `/no think` are introduced in the user query or system message. The non-thinking mode response retains an **empty thinking block** (`<think></think>`) to ensure internal format consistency.\n*   **Dynamic Budget:** The model naturally develops the ability to handle **intermediate cases** where reasoning is halted at a user-defined threshold (**thinking budget**), inserting a stop instruction. Performance scales **smoothly and consistently** with the allocated thinking budget.\n\n*   **Reward System:** Utilizes three distinct reward types: **Rule-based Reward** (for format adherence), **Model-based Reward with Reference Answer** (using Qwen2.5-72B-Instruct to score output), and **Model-based Reward without Reference Answer** (trained on human preference data).\n*   **Agent Ability:** The model is trained to correctly invoke tools via designated interfaces, performing **complete multi-turn interaction cycles with real environment execution feedback**.",
    "order": 5,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "models",
      "reinforcement learning",
      "optimization",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 355,
      "contentLength": 3757
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/qwen3/#four-stage-post-training-pipeline-(flagship-models)",
    "scrapedAt": "2025-12-28T11:52:13.873Z"
  },
  {
    "id": "ai-qwen3-strong-to-weak-distillation-6",
    "domain": "ai_primers",
    "category": "Models",
    "article": "Qwen 3",
    "articleSlug": "qwen3",
    "chapter": "Post-Training and Thinking Control",
    "title": "Strong-to-Weak Distillation",
    "subtitle": "Post-Training and Thinking Control",
    "contentHtml": "<p>This method is used for lightweight models (0.6B to 30B-A3B) to enhance performance and impart mode-switching capabilities with high efficiency.</p>\n<ul>\n  <li><strong>Efficiency:</strong> Distillation achieves significantly better performance than direct RL while requiring approximately <strong>1/10 of the GPU hours</strong>.</li>\n  <li><strong>Process:</strong> Includes two phases: <strong>Off-policy Distillation</strong> (using combined /think and /no think teacher outputs) and <strong>On-policy Distillation</strong> (where the student generates sequences and is fine-tuned by aligning its logits with the teacher model to minimize KL divergence).</li>\n</ul>",
    "contentMarkdown": "This method is used for lightweight models (0.6B to 30B-A3B) to enhance performance and impart mode-switching capabilities with high efficiency.\n\n*   **Efficiency:** Distillation achieves significantly better performance than direct RL while requiring approximately **1/10 of the GPU hours**.\n*   **Process:** Includes two phases: **Off-policy Distillation** (using combined /think and /no think teacher outputs) and **On-policy Distillation** (where the student generates sequences and is fine-tuned by aligning its logits with the teacher model to minimize KL divergence).",
    "order": 6,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 76,
      "contentLength": 670
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/qwen3/#strong-to-weak-distillation",
    "scrapedAt": "2025-12-28T11:52:13.873Z"
  },
  {
    "id": "ai-qwen3-competitive-benchmarking-qwen3-235b-a22b-base-7",
    "domain": "ai_primers",
    "category": "Models",
    "article": "Qwen 3",
    "articleSlug": "qwen3",
    "chapter": "Evaluation and Performance Metrics",
    "title": "Competitive Benchmarking (Qwen3-235B-A22B-Base)",
    "subtitle": "Evaluation and Performance Metrics",
    "contentHtml": "<p>The base model version outperforms key open-source rivals despite being significantly smaller.</p>\n<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: left\">Baseline Comparison</th>\n      <th style=\"text-align: left\">Qwen3-235B-A22B-Base (235B/22B)</th>\n      <th style=\"text-align: left\">DeepSeek-V3 Base (671B/37B)</th>\n      <th style=\"text-align: left\">Rationale</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: left\"><strong>Total Params Ratio</strong></td>\n      <td style=\"text-align: left\">1.0</td>\n      <td style=\"text-align: left\">$\\sim 3.0\\times$ Larger</td>\n      <td style=\"text-align: left\">Qwen3 outperforms DeepSeek-V3 on <strong>14 out of 15 benchmarks</strong>.</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\"><strong>Activated Params Ratio</strong></td>\n      <td style=\"text-align: left\">1.0</td>\n      <td style=\"text-align: left\">$\\sim 1.7\\times$ Larger</td>\n      <td style=\"text-align: left\">Qwen3 uses $\\sim 2/3$ activated parameters.</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\"><strong>MMLU-Redux</strong></td>\n      <td style=\"text-align: left\"><strong>87.40</strong></td>\n      <td style=\"text-align: left\">86.14</td>\n      <td style=\"text-align: left\">Highest score among competitors.</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\"><strong>MATH</strong></td>\n      <td style=\"text-align: left\"><strong>71.84</strong></td>\n      <td style=\"text-align: left\">62.62</td>\n      <td style=\"text-align: left\">Significant performance lead.</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\"><strong>EvalPlus (Coding)</strong></td>\n      <td style=\"text-align: left\"><strong>77.60</strong></td>\n      <td style=\"text-align: left\">63.75</td>\n      <td style=\"text-align: left\">Strong coding superiority.</td>\n    </tr>\n  </tbody>\n</table>",
    "contentMarkdown": "The base model version outperforms key open-source rivals despite being significantly smaller.\n\nBaseline Comparison\n\nQwen3-235B-A22B-Base (235B/22B)\n\nDeepSeek-V3 Base (671B/37B)\n\nRationale\n\n**Total Params Ratio**\n\n1.0\n\n$\\\\sim 3.0\\\\times$ Larger\n\nQwen3 outperforms DeepSeek-V3 on **14 out of 15 benchmarks**.\n\n**Activated Params Ratio**\n\n1.0\n\n$\\\\sim 1.7\\\\times$ Larger\n\nQwen3 uses $\\\\sim 2/3$ activated parameters.\n\n**MMLU-Redux**\n\n**87.40**\n\n86.14\n\nHighest score among competitors.\n\n**MATH**\n\n**71.84**\n\n62.62\n\nSignificant performance lead.\n\n**EvalPlus (Coding)**\n\n**77.60**\n\n63.75\n\nStrong coding superiority.",
    "order": 7,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 69,
      "contentLength": 1837
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/qwen3/#competitive-benchmarking-(qwen3-235b-a22b-base)",
    "scrapedAt": "2025-12-28T11:52:13.873Z"
  },
  {
    "id": "ai-qwen3-flagship-performance-qwen3-235b-a22b-post-trained-8",
    "domain": "ai_primers",
    "category": "Models",
    "article": "Qwen 3",
    "articleSlug": "qwen3",
    "chapter": "Evaluation and Performance Metrics",
    "title": "Flagship Performance (Qwen3-235B-A22B, Post-Trained)",
    "subtitle": "Evaluation and Performance Metrics",
    "contentHtml": "<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: left\">Benchmark</th>\n      <th style=\"text-align: left\">Thinking Mode Score</th>\n      <th style=\"text-align: left\">Non-Thinking Mode Score</th>\n      <th style=\"text-align: left\">Comparative Detail</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: left\"><strong>MMLU-Redux</strong></td>\n      <td style=\"text-align: left\"><strong>92.7</strong></td>\n      <td style=\"text-align: left\">89.2</td>\n      <td style=\"text-align: left\">Thinking mode is competitive with GPT-4o, Gemini 2.5 Pro.</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\"><strong>GPQA-Diamond</strong></td>\n      <td style=\"text-align: left\">71.1</td>\n      <td style=\"text-align: left\">62.9</td>\n      <td style=\"text-align: left\">Lower than Gemini 2.5 Pro (84.0) but strong for open source.</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\"><strong>AIME’25</strong> (Math)</td>\n      <td style=\"text-align: left\"><strong>81.5</strong></td>\n      <td style=\"text-align: left\">24.7</td>\n      <td style=\"text-align: left\">Demonstrates effectiveness of “Thinking Mode” for complex math.</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\"><strong>LiveCodeBench v5</strong></td>\n      <td style=\"text-align: left\">70.7</td>\n      <td style=\"text-align: left\">35.3</td>\n      <td style=\"text-align: left\">High performance in the agent/coding domain.</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\"><strong>CodeForces (Rating)</strong></td>\n      <td style=\"text-align: left\"><strong>2056 / 98.2%</strong></td>\n      <td style=\"text-align: left\">1387 / 75.7%</td>\n      <td style=\"text-align: left\">SOTA CodeForces rating among reasoning models, surpassing DeepSeek-R1 and Gemini 2.5 Pro.</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\"><strong>Arena-Hard</strong> (Alignment)</td>\n      <td style=\"text-align: left\"><strong>95.6</strong></td>\n      <td style=\"text-align: left\"><strong>96.1</strong></td>\n      <td style=\"text-align: left\">Highest score among models listed, surpassing GPT-4o (85.3).</td>\n    </tr>\n  </tbody>\n</table>",
    "contentMarkdown": "Benchmark\n\nThinking Mode Score\n\nNon-Thinking Mode Score\n\nComparative Detail\n\n**MMLU-Redux**\n\n**92.7**\n\n89.2\n\nThinking mode is competitive with GPT-4o, Gemini 2.5 Pro.\n\n**GPQA-Diamond**\n\n71.1\n\n62.9\n\nLower than Gemini 2.5 Pro (84.0) but strong for open source.\n\n**AIME’25** (Math)\n\n**81.5**\n\n24.7\n\nDemonstrates effectiveness of “Thinking Mode” for complex math.\n\n**LiveCodeBench v5**\n\n70.7\n\n35.3\n\nHigh performance in the agent/coding domain.\n\n**CodeForces (Rating)**\n\n**2056 / 98.2%**\n\n1387 / 75.7%\n\nSOTA CodeForces rating among reasoning models, surpassing DeepSeek-R1 and Gemini 2.5 Pro.\n\n**Arena-Hard** (Alignment)\n\n**95.6**\n\n**96.1**\n\nHighest score among models listed, surpassing GPT-4o (85.3).",
    "order": 8,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models",
      "gpt"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 89,
      "contentLength": 2125
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/qwen3/#flagship-performance-(qwen3-235b-a22b,-post-trained)",
    "scrapedAt": "2025-12-28T11:52:13.873Z"
  },
  {
    "id": "ai-qwen3-multilingual-expertise-9",
    "domain": "ai_primers",
    "category": "Models",
    "article": "Qwen 3",
    "articleSlug": "qwen3",
    "chapter": "Evaluation and Performance Metrics",
    "title": "Multilingual Expertise",
    "subtitle": "Evaluation and Performance Metrics",
    "contentHtml": "<ul>\n  <li>Qwen3 achieves competitive performance across all evaluated benchmarks covering <strong>119 languages</strong>.</li>\n  <li>Multilingual evaluations (Multi-IF, MT-AIME2024, PolyMath) show Qwen3 consistently outperforms DeepSeek-V3 and Gemini Flash in non-English STEM and reasoning tasks (e.g., MT-AIME2024: Qwen3-235B-A22B Thinking <strong>80.8</strong> vs. Gemini 2.5 Pro <strong>76.9</strong>).</li>\n</ul>",
    "contentMarkdown": "*   Qwen3 achieves competitive performance across all evaluated benchmarks covering **119 languages**.\n*   Multilingual evaluations (Multi-IF, MT-AIME2024, PolyMath) show Qwen3 consistently outperforms DeepSeek-V3 and Gemini Flash in non-English STEM and reasoning tasks (e.g., MT-AIME2024: Qwen3-235B-A22B Thinking **80.8** vs. Gemini 2.5 Pro **76.9**).",
    "order": 9,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 42,
      "contentLength": 418
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/qwen3/#multilingual-expertise",
    "scrapedAt": "2025-12-28T11:52:13.873Z"
  }
]