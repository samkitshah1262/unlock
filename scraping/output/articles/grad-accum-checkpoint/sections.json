[
  {
    "id": "ai-grad-accum-checkpoint-related-fsdp-and-qlora-to-train-a-70b-llm-at-home-1",
    "articleSlug": "grad-accum-checkpoint",
    "articleTitle": "Gradient Accumulation and Checkpointing",
    "category": "Data/Training",
    "chapter": "Gradient Checkpointing",
    "title": "Related: FSDP and QLoRA to Train a 70b LLM at Home!",
    "order": 1,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li><a href=\"https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html\">Answer.AI</a> details an open source system, based on FSDP and QLoRA, that can train a 70b model on two consumer 24GB gaming GPUs.</li>\n  <li>In collaboration with Tim Dettmers (University of Washington) and Hugging Face’s Titus von Koeller and Sourab Mangrulkar, Answer.AI has released an open-source system that enables the training of a 70 billion parameter language model on a standard gaming PC.</li>\n  <li><strong>Democratizing AI with Gaming GPUs</strong>\n    <ul>\n      <li>The limiting factor in training large language models on consumer-grade GPUs is the amount of memory available on these cards. While gaming GPUs like the RTX 3090 or 4090 offer impressive computational power, they typically have a maximum of 24GB of RAM. This is significantly less than the memory found on data center-class GPUs, such as the A100 or H100, which can have up to 80GB of RAM.</li>\n      <li>The memory limitation becomes a bottleneck when training large models, as the entire model, along with activations, gradients, and optimization states, needs to fit within the GPU’s memory.</li>\n      <li>This constraint has made it challenging to train state-of-the-art models with billions of parameters on consumer hardware, as the model size alone can exceed the available memory. Consequently, the limited memory capacity of gaming GPUs has been the primary obstacle in making large model training accessible to a wider audience.</li>\n    </ul>\n  </li>\n  <li>This innovation makes large model training more accessible by leveraging the power of gaming GPUs like the RTX 3090 or 4090. The cost-effectiveness and accessibility of this approach have the potential to revolutionize the AI landscape.</li>\n  <li><strong>The Technology Behind the Scenes: FSDP and QLoRA</strong>\n    <ul>\n      <li>The system combines two innovative technologies:\n        <ol>\n          <li><strong>“Fully Sharded Data Parallel (FSDP)”:</strong> Allows efficient model training across multiple GPUs.</li>\n          <li><strong>“Quantization and Low-Rank Adaptation (QLoRA)”:</strong> Overcomes memory limitations of gaming GPUs.</li>\n        </ol>\n      </li>\n    </ul>\n  </li>\n  <li>Together, FSDP and QLoRA enable small labs and individuals to train large models locally, without the need for expensive specialized hardware.</li>\n  <li><strong>Empowering the Open Source Community</strong>\n    <ul>\n      <li>This development has the potential to accelerate AI innovation by making state-of-the-art models more accessible to researchers, startups, and enthusiasts. Teknium, the creator of the popular OpenHermes models and datasets, stated, “With this capability, we can take huge models to new heights locally, and gigantic, hundreds of billions of parameter models are now accessible by small labs.”</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>The limiting factor in training large language models on consumer-grade GPUs is the amount of memory available on these cards. While gaming GPUs like the RTX 3090 or 4090 offer impressive computational power, they typically have a maximum of 24GB of RAM. This is significantly less than the memory found on data center-class GPUs, such as the A100 or H100, which can have up to 80GB of RAM.</li>\n      <li>The memory limitation becomes a bottleneck when training large models, as the entire model, along with activations, gradients, and optimization states, needs to fit within the GPU’s memory.</li>\n      <li>This constraint has made it challenging to train state-of-the-art models with billions of parameters on consumer hardware, as the model size alone can exceed the available memory. Consequently, the limited memory capacity of gaming GPUs has been the primary obstacle in making large model training accessible to a wider audience.</li>\n    </ul>\n<ul>\n      <li>The system combines two innovative technologies:\n        <ol>\n          <li><strong>“Fully Sharded Data Parallel (FSDP)”:</strong> Allows efficient model training across multiple GPUs.</li>\n          <li><strong>“Quantization and Low-Rank Adaptation (QLoRA)”:</strong> Overcomes memory limitations of gaming GPUs.</li>\n        </ol>\n      </li>\n    </ul>\n<ol>\n          <li><strong>“Fully Sharded Data Parallel (FSDP)”:</strong> Allows efficient model training across multiple GPUs.</li>\n          <li><strong>“Quantization and Low-Rank Adaptation (QLoRA)”:</strong> Overcomes memory limitations of gaming GPUs.</li>\n        </ol>\n<ul>\n      <li>This development has the potential to accelerate AI innovation by making state-of-the-art models more accessible to researchers, startups, and enthusiasts. Teknium, the creator of the popular OpenHermes models and datasets, stated, “With this capability, we can take huge models to new heights locally, and gigantic, hundreds of billions of parameter models are now accessible by small labs.”</li>\n    </ul>\n<p><a href=\"https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html\"><img src=\"../../../images/read/FSDP_QLoRA.jpg\" alt=\"\"></a></p>",
    "contentMarkdown": "*   [Answer.AI](https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html) details an open source system, based on FSDP and QLoRA, that can train a 70b model on two consumer 24GB gaming GPUs.\n*   In collaboration with Tim Dettmers (University of Washington) and Hugging Face’s Titus von Koeller and Sourab Mangrulkar, Answer.AI has released an open-source system that enables the training of a 70 billion parameter language model on a standard gaming PC.\n*   **Democratizing AI with Gaming GPUs**\n    *   The limiting factor in training large language models on consumer-grade GPUs is the amount of memory available on these cards. While gaming GPUs like the RTX 3090 or 4090 offer impressive computational power, they typically have a maximum of 24GB of RAM. This is significantly less than the memory found on data center-class GPUs, such as the A100 or H100, which can have up to 80GB of RAM.\n    *   The memory limitation becomes a bottleneck when training large models, as the entire model, along with activations, gradients, and optimization states, needs to fit within the GPU’s memory.\n    *   This constraint has made it challenging to train state-of-the-art models with billions of parameters on consumer hardware, as the model size alone can exceed the available memory. Consequently, the limited memory capacity of gaming GPUs has been the primary obstacle in making large model training accessible to a wider audience.\n*   This innovation makes large model training more accessible by leveraging the power of gaming GPUs like the RTX 3090 or 4090. The cost-effectiveness and accessibility of this approach have the potential to revolutionize the AI landscape.\n*   **The Technology Behind the Scenes: FSDP and QLoRA**\n    *   The system combines two innovative technologies:\n        1.  **“Fully Sharded Data Parallel (FSDP)”:** Allows efficient model training across multiple GPUs.\n        2.  **“Quantization and Low-Rank Adaptation (QLoRA)”:** Overcomes memory limitations of gaming GPUs.\n*   Together, FSDP and QLoRA enable small labs and individuals to train large models locally, without the need for expensive specialized hardware.\n*   **Empowering the Open Source Community**\n    *   This development has the potential to accelerate AI innovation by making state-of-the-art models more accessible to researchers, startups, and enthusiasts. Teknium, the creator of the popular OpenHermes models and datasets, stated, “With this capability, we can take huge models to new heights locally, and gigantic, hundreds of billions of parameter models are now accessible by small labs.”\n\n*   The limiting factor in training large language models on consumer-grade GPUs is the amount of memory available on these cards. While gaming GPUs like the RTX 3090 or 4090 offer impressive computational power, they typically have a maximum of 24GB of RAM. This is significantly less than the memory found on data center-class GPUs, such as the A100 or H100, which can have up to 80GB of RAM.\n*   The memory limitation becomes a bottleneck when training large models, as the entire model, along with activations, gradients, and optimization states, needs to fit within the GPU’s memory.\n*   This constraint has made it challenging to train state-of-the-art models with billions of parameters on consumer hardware, as the model size alone can exceed the available memory. Consequently, the limited memory capacity of gaming GPUs has been the primary obstacle in making large model training accessible to a wider audience.\n\n*   The system combines two innovative technologies:\n    1.  **“Fully Sharded Data Parallel (FSDP)”:** Allows efficient model training across multiple GPUs.\n    2.  **“Quantization and Low-Rank Adaptation (QLoRA)”:** Overcomes memory limitations of gaming GPUs.\n\n1.  **“Fully Sharded Data Parallel (FSDP)”:** Allows efficient model training across multiple GPUs.\n2.  **“Quantization and Low-Rank Adaptation (QLoRA)”:** Overcomes memory limitations of gaming GPUs.\n\n*   This development has the potential to accelerate AI innovation by making state-of-the-art models more accessible to researchers, startups, and enthusiasts. Teknium, the creator of the popular OpenHermes models and datasets, stated, “With this capability, we can take huge models to new heights locally, and gigantic, hundreds of billions of parameter models are now accessible by small labs.”\n\n[![](../../../images/read/FSDP_QLoRA.jpg)](https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html)",
    "contentLength": 5049,
    "wordCount": 648,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/grad-accum-checkpoint/#related:-fsdp-and-qlora-to-train-a-70b-llm-at-home!"
  }
]