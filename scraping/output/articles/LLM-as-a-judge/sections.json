[
  {
    "id": "ai-LLM-as-a-judge-limitations-of-traditional-evaluation-metrics-1",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Motivation: Why LLM-as-a-Judge is Needed Beyond Traditional Metrics",
    "title": "Limitations of Traditional Evaluation Metrics",
    "order": 1,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>Traditional evaluation metrics were designed for narrow, well-defined tasks with deterministic or near-deterministic outputs. Examples include BLEU for machine translation, ROUGE for summarization, Exact Match (EM) for question answering, and accuracy or F1 for classification. While these metrics are easy to compute, fast, and reproducible, they fail to capture many properties that matter for modern LLM outputs.</p>\n  </li>\n  <li>\n    <p>Key limitations include:</p>\n\n    <ul>\n      <li><strong>Surface-form dependence</strong>: Metrics like BLEU and ROUGE rely on n-gram overlap, penalizing valid paraphrases and rewarding shallow lexical similarity. This was extensively analyzed in <a href=\"https://aclanthology.org/W17-2602/\">On the Limitations of Automatic Metrics for Evaluating Natural Language Generation</a> by Novikova et al. (2017).</li>\n      <li><strong>Inability to measure reasoning quality</strong>: Exact match and token overlap metrics cannot distinguish between correct reasoning and lucky guessing, or between flawed reasoning and correct final answers.</li>\n      <li><strong>Poor alignment with human judgment</strong>: Numerous studies show weak correlation between traditional metrics and human preferences for tasks like summarization and dialogue, e.g., <a href=\"https://aclanthology.org/D19-1389/\">Re-evaluating Automatic Metrics for Natural Language Generation</a> by Reiter (2019).</li>\n      <li><strong>Single-reference bias</strong>: Many benchmarks rely on one or a few reference outputs, even though generative tasks are inherently one-to-many.</li>\n      <li><strong>Task brittleness</strong>: Metrics must be redesigned for each task, making them difficult to generalize across domains such as reasoning, safety, instruction following, or creativity.</li>\n    </ul>\n  </li>\n  <li>\n    <p>As LLMs began to outperform reference-based baselines while producing diverse and high-quality outputs, these weaknesses became critical bottlenecks to progress.</p>\n  </li>\n</ul>\n<p>Traditional evaluation metrics were designed for narrow, well-defined tasks with deterministic or near-deterministic outputs. Examples include BLEU for machine translation, ROUGE for summarization, Exact Match (EM) for question answering, and accuracy or F1 for classification. While these metrics are easy to compute, fast, and reproducible, they fail to capture many properties that matter for modern LLM outputs.</p>\n<p>Key limitations include:</p>\n<ul>\n      <li><strong>Surface-form dependence</strong>: Metrics like BLEU and ROUGE rely on n-gram overlap, penalizing valid paraphrases and rewarding shallow lexical similarity. This was extensively analyzed in <a href=\"https://aclanthology.org/W17-2602/\">On the Limitations of Automatic Metrics for Evaluating Natural Language Generation</a> by Novikova et al. (2017).</li>\n      <li><strong>Inability to measure reasoning quality</strong>: Exact match and token overlap metrics cannot distinguish between correct reasoning and lucky guessing, or between flawed reasoning and correct final answers.</li>\n      <li><strong>Poor alignment with human judgment</strong>: Numerous studies show weak correlation between traditional metrics and human preferences for tasks like summarization and dialogue, e.g., <a href=\"https://aclanthology.org/D19-1389/\">Re-evaluating Automatic Metrics for Natural Language Generation</a> by Reiter (2019).</li>\n      <li><strong>Single-reference bias</strong>: Many benchmarks rely on one or a few reference outputs, even though generative tasks are inherently one-to-many.</li>\n      <li><strong>Task brittleness</strong>: Metrics must be redesigned for each task, making them difficult to generalize across domains such as reasoning, safety, instruction following, or creativity.</li>\n    </ul>\n<p>As LLMs began to outperform reference-based baselines while producing diverse and high-quality outputs, these weaknesses became critical bottlenecks to progress.</p>",
    "contentMarkdown": "*   Traditional evaluation metrics were designed for narrow, well-defined tasks with deterministic or near-deterministic outputs. Examples include BLEU for machine translation, ROUGE for summarization, Exact Match (EM) for question answering, and accuracy or F1 for classification. While these metrics are easy to compute, fast, and reproducible, they fail to capture many properties that matter for modern LLM outputs.\n    \n*   Key limitations include:\n    \n    *   **Surface-form dependence**: Metrics like BLEU and ROUGE rely on n-gram overlap, penalizing valid paraphrases and rewarding shallow lexical similarity. This was extensively analyzed in [On the Limitations of Automatic Metrics for Evaluating Natural Language Generation](https://aclanthology.org/W17-2602/) by Novikova et al. (2017).\n    *   **Inability to measure reasoning quality**: Exact match and token overlap metrics cannot distinguish between correct reasoning and lucky guessing, or between flawed reasoning and correct final answers.\n    *   **Poor alignment with human judgment**: Numerous studies show weak correlation between traditional metrics and human preferences for tasks like summarization and dialogue, e.g., [Re-evaluating Automatic Metrics for Natural Language Generation](https://aclanthology.org/D19-1389/) by Reiter (2019).\n    *   **Single-reference bias**: Many benchmarks rely on one or a few reference outputs, even though generative tasks are inherently one-to-many.\n    *   **Task brittleness**: Metrics must be redesigned for each task, making them difficult to generalize across domains such as reasoning, safety, instruction following, or creativity.\n*   As LLMs began to outperform reference-based baselines while producing diverse and high-quality outputs, these weaknesses became critical bottlenecks to progress.\n    \n\nTraditional evaluation metrics were designed for narrow, well-defined tasks with deterministic or near-deterministic outputs. Examples include BLEU for machine translation, ROUGE for summarization, Exact Match (EM) for question answering, and accuracy or F1 for classification. While these metrics are easy to compute, fast, and reproducible, they fail to capture many properties that matter for modern LLM outputs.\n\nKey limitations include:\n\n*   **Surface-form dependence**: Metrics like BLEU and ROUGE rely on n-gram overlap, penalizing valid paraphrases and rewarding shallow lexical similarity. This was extensively analyzed in [On the Limitations of Automatic Metrics for Evaluating Natural Language Generation](https://aclanthology.org/W17-2602/) by Novikova et al. (2017).\n*   **Inability to measure reasoning quality**: Exact match and token overlap metrics cannot distinguish between correct reasoning and lucky guessing, or between flawed reasoning and correct final answers.\n*   **Poor alignment with human judgment**: Numerous studies show weak correlation between traditional metrics and human preferences for tasks like summarization and dialogue, e.g., [Re-evaluating Automatic Metrics for Natural Language Generation](https://aclanthology.org/D19-1389/) by Reiter (2019).\n*   **Single-reference bias**: Many benchmarks rely on one or a few reference outputs, even though generative tasks are inherently one-to-many.\n*   **Task brittleness**: Metrics must be redesigned for each task, making them difficult to generalize across domains such as reasoning, safety, instruction following, or creativity.\n\nAs LLMs began to outperform reference-based baselines while producing diverse and high-quality outputs, these weaknesses became critical bottlenecks to progress.",
    "contentLength": 3980,
    "wordCount": 459,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#limitations-of-traditional-evaluation-metrics"
  },
  {
    "id": "ai-LLM-as-a-judge-the-rise-of-human-evaluationand-its-costs-2",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Motivation: Why LLM-as-a-Judge is Needed Beyond Traditional Metrics",
    "title": "The Rise of Human Evaluation—and Its Costs",
    "order": 2,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>To address these shortcomings, the community increasingly relied on human evaluation. Human judges can assess:</p>\n\n    <ul>\n      <li>Semantic correctness</li>\n      <li>Factuality</li>\n      <li>Coherence and clarity</li>\n      <li>Helpfulness and safety</li>\n      <li>Preference between multiple outputs</li>\n    </ul>\n  </li>\n  <li>\n    <p>However, human evaluation introduces its own challenges:</p>\n\n    <ul>\n      <li><strong>Cost and latency</strong>: Large-scale human evaluation is expensive and slow.</li>\n      <li><strong>Inconsistency</strong>: Inter-annotator agreement can be low, especially for subjective criteria.</li>\n      <li><strong>Limited scalability</strong>: Continuous evaluation during training or deployment is impractical.</li>\n      <li><strong>Reproducibility issues</strong>: Results depend heavily on annotator pools and instructions.</li>\n    </ul>\n  </li>\n  <li>\n    <p>These issues are discussed in detail in <a href=\"https://arxiv.org/abs/2307.03109\">How to Evaluate Language Models: A Survey</a> by Chang et al. (2023).</p>\n  </li>\n</ul>\n<p>To address these shortcomings, the community increasingly relied on human evaluation. Human judges can assess:</p>\n<ul>\n      <li>Semantic correctness</li>\n      <li>Factuality</li>\n      <li>Coherence and clarity</li>\n      <li>Helpfulness and safety</li>\n      <li>Preference between multiple outputs</li>\n    </ul>\n<p>However, human evaluation introduces its own challenges:</p>\n<ul>\n      <li><strong>Cost and latency</strong>: Large-scale human evaluation is expensive and slow.</li>\n      <li><strong>Inconsistency</strong>: Inter-annotator agreement can be low, especially for subjective criteria.</li>\n      <li><strong>Limited scalability</strong>: Continuous evaluation during training or deployment is impractical.</li>\n      <li><strong>Reproducibility issues</strong>: Results depend heavily on annotator pools and instructions.</li>\n    </ul>\n<p>These issues are discussed in detail in <a href=\"https://arxiv.org/abs/2307.03109\">How to Evaluate Language Models: A Survey</a> by Chang et al. (2023).</p>",
    "contentMarkdown": "*   To address these shortcomings, the community increasingly relied on human evaluation. Human judges can assess:\n    \n    *   Semantic correctness\n    *   Factuality\n    *   Coherence and clarity\n    *   Helpfulness and safety\n    *   Preference between multiple outputs\n*   However, human evaluation introduces its own challenges:\n    \n    *   **Cost and latency**: Large-scale human evaluation is expensive and slow.\n    *   **Inconsistency**: Inter-annotator agreement can be low, especially for subjective criteria.\n    *   **Limited scalability**: Continuous evaluation during training or deployment is impractical.\n    *   **Reproducibility issues**: Results depend heavily on annotator pools and instructions.\n*   These issues are discussed in detail in [How to Evaluate Language Models: A Survey](https://arxiv.org/abs/2307.03109) by Chang et al. (2023).\n    \n\nTo address these shortcomings, the community increasingly relied on human evaluation. Human judges can assess:\n\n*   Semantic correctness\n*   Factuality\n*   Coherence and clarity\n*   Helpfulness and safety\n*   Preference between multiple outputs\n\nHowever, human evaluation introduces its own challenges:\n\n*   **Cost and latency**: Large-scale human evaluation is expensive and slow.\n*   **Inconsistency**: Inter-annotator agreement can be low, especially for subjective criteria.\n*   **Limited scalability**: Continuous evaluation during training or deployment is impractical.\n*   **Reproducibility issues**: Results depend heavily on annotator pools and instructions.\n\nThese issues are discussed in detail in [How to Evaluate Language Models: A Survey](https://arxiv.org/abs/2307.03109) by Chang et al. (2023).",
    "contentLength": 2117,
    "wordCount": 209,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#the-rise-of-human-evaluation—and-its-costs"
  },
  {
    "id": "ai-LLM-as-a-judge-llm-as-a-judge-as-a-scalable-approximation-to-huma-3",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Motivation: Why LLM-as-a-Judge is Needed Beyond Traditional Metrics",
    "title": "LLM-as-a-Judge As a Scalable Approximation to Human Judgment",
    "order": 3,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>LLM-as-a-Judge emerged as a pragmatic compromise between brittle automatic metrics and expensive human evaluation. The core insight is that strong LLMs already encode many of the same linguistic and semantic priors that humans use when judging outputs.</p>\n  </li>\n  <li>\n    <p>Empirical evidence supporting this idea includes:</p>\n\n    <ul>\n      <li>High agreement between GPT-4 judges and human annotators on dialogue quality and reasoning tasks, shown in <a href=\"https://arxiv.org/abs/2306.05685\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> by Zheng et al. (2023).</li>\n      <li>Strong correlation between AI preference models and human feedback in Reinforcement Learning (RL) pipelines, as demonstrated in <a href=\"https://arxiv.org/abs/2203.02155\">Training language models to follow instructions with human feedback</a> by Ouyang et al. (2022).</li>\n      <li>Effective use of AI-generated feedback in place of human annotations in <a href=\"https://www.anthropic.com/news/constitutional-ai\">Constitutional AI</a>.</li>\n    </ul>\n  </li>\n  <li>\n    <p>LLM-as-a-Judge offers several advantages:</p>\n\n    <ul>\n      <li><strong>Semantic sensitivity</strong>: Judges reason about meaning, not surface form.</li>\n      <li><strong>Task flexibility</strong>: New tasks can be evaluated by changing prompts rather than metrics.</li>\n      <li><strong>Low marginal cost</strong>: Once deployed, evaluations scale cheaply.</li>\n      <li><strong>Structured output</strong>: Judges can emit scores, rankings, and rationales.</li>\n      <li><strong>Fast iteration</strong>: Enables rapid offline evaluation during model development.</li>\n    </ul>\n  </li>\n</ul>\n<p>LLM-as-a-Judge emerged as a pragmatic compromise between brittle automatic metrics and expensive human evaluation. The core insight is that strong LLMs already encode many of the same linguistic and semantic priors that humans use when judging outputs.</p>\n<p>Empirical evidence supporting this idea includes:</p>\n<ul>\n      <li>High agreement between GPT-4 judges and human annotators on dialogue quality and reasoning tasks, shown in <a href=\"https://arxiv.org/abs/2306.05685\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> by Zheng et al. (2023).</li>\n      <li>Strong correlation between AI preference models and human feedback in Reinforcement Learning (RL) pipelines, as demonstrated in <a href=\"https://arxiv.org/abs/2203.02155\">Training language models to follow instructions with human feedback</a> by Ouyang et al. (2022).</li>\n      <li>Effective use of AI-generated feedback in place of human annotations in <a href=\"https://www.anthropic.com/news/constitutional-ai\">Constitutional AI</a>.</li>\n    </ul>\n<p>LLM-as-a-Judge offers several advantages:</p>\n<ul>\n      <li><strong>Semantic sensitivity</strong>: Judges reason about meaning, not surface form.</li>\n      <li><strong>Task flexibility</strong>: New tasks can be evaluated by changing prompts rather than metrics.</li>\n      <li><strong>Low marginal cost</strong>: Once deployed, evaluations scale cheaply.</li>\n      <li><strong>Structured output</strong>: Judges can emit scores, rankings, and rationales.</li>\n      <li><strong>Fast iteration</strong>: Enables rapid offline evaluation during model development.</li>\n    </ul>",
    "contentMarkdown": "*   LLM-as-a-Judge emerged as a pragmatic compromise between brittle automatic metrics and expensive human evaluation. The core insight is that strong LLMs already encode many of the same linguistic and semantic priors that humans use when judging outputs.\n    \n*   Empirical evidence supporting this idea includes:\n    \n    *   High agreement between GPT-4 judges and human annotators on dialogue quality and reasoning tasks, shown in [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) by Zheng et al. (2023).\n    *   Strong correlation between AI preference models and human feedback in Reinforcement Learning (RL) pipelines, as demonstrated in [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) by Ouyang et al. (2022).\n    *   Effective use of AI-generated feedback in place of human annotations in [Constitutional AI](https://www.anthropic.com/news/constitutional-ai).\n*   LLM-as-a-Judge offers several advantages:\n    \n    *   **Semantic sensitivity**: Judges reason about meaning, not surface form.\n    *   **Task flexibility**: New tasks can be evaluated by changing prompts rather than metrics.\n    *   **Low marginal cost**: Once deployed, evaluations scale cheaply.\n    *   **Structured output**: Judges can emit scores, rankings, and rationales.\n    *   **Fast iteration**: Enables rapid offline evaluation during model development.\n\nLLM-as-a-Judge emerged as a pragmatic compromise between brittle automatic metrics and expensive human evaluation. The core insight is that strong LLMs already encode many of the same linguistic and semantic priors that humans use when judging outputs.\n\nEmpirical evidence supporting this idea includes:\n\n*   High agreement between GPT-4 judges and human annotators on dialogue quality and reasoning tasks, shown in [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) by Zheng et al. (2023).\n*   Strong correlation between AI preference models and human feedback in Reinforcement Learning (RL) pipelines, as demonstrated in [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) by Ouyang et al. (2022).\n*   Effective use of AI-generated feedback in place of human annotations in [Constitutional AI](https://www.anthropic.com/news/constitutional-ai).\n\nLLM-as-a-Judge offers several advantages:\n\n*   **Semantic sensitivity**: Judges reason about meaning, not surface form.\n*   **Task flexibility**: New tasks can be evaluated by changing prompts rather than metrics.\n*   **Low marginal cost**: Once deployed, evaluations scale cheaply.\n*   **Structured output**: Judges can emit scores, rankings, and rationales.\n*   **Fast iteration**: Enables rapid offline evaluation during model development.",
    "contentLength": 3309,
    "wordCount": 353,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#llm-as-a-judge-as-a-scalable-approximation-to-human-judgment"
  },
  {
    "id": "ai-LLM-as-a-judge-why-this-matters-for-modern-llm-tasks-4",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Motivation: Why LLM-as-a-Judge is Needed Beyond Traditional Metrics",
    "title": "Why This Matters for Modern LLM Tasks",
    "order": 4,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>\n    <p>Modern LLM applications—reasoning, tool use, code generation, RAG, safety alignment—often lack clear ground truth. Evaluation becomes inherently subjective or context-dependent.</p>\n  </li>\n  <li>\n    <p>Examples where traditional metrics fail but LLM-as-a-Judge succeeds include:</p>\n\n    <ul>\n      <li><strong>Chain-of-thought reasoning</strong>: Evaluating whether reasoning is valid, not just whether the final answer matches.</li>\n      <li><strong>Summarization faithfulness</strong>: Detecting subtle hallucinations that ROUGE cannot capture, as shown in <a href=\"https://arxiv.org/abs/2008.01415\">Evaluating the Factual Consistency of Summaries</a> by Kryściński et al. (2020).</li>\n      <li><strong>Instruction following</strong>: Determining whether constraints were followed, even when outputs differ lexically.</li>\n      <li><strong>Safety and policy compliance</strong>: Judging nuanced violations where keyword matching fails.</li>\n    </ul>\n  </li>\n  <li>\n    <p>As a result, LLM-as-a-Judge has become a foundational component of modern evaluation pipelines.</p>\n  </li>\n</ul>\n<p>Modern LLM applications—reasoning, tool use, code generation, RAG, safety alignment—often lack clear ground truth. Evaluation becomes inherently subjective or context-dependent.</p>\n<p>Examples where traditional metrics fail but LLM-as-a-Judge succeeds include:</p>\n<ul>\n      <li><strong>Chain-of-thought reasoning</strong>: Evaluating whether reasoning is valid, not just whether the final answer matches.</li>\n      <li><strong>Summarization faithfulness</strong>: Detecting subtle hallucinations that ROUGE cannot capture, as shown in <a href=\"https://arxiv.org/abs/2008.01415\">Evaluating the Factual Consistency of Summaries</a> by Kryściński et al. (2020).</li>\n      <li><strong>Instruction following</strong>: Determining whether constraints were followed, even when outputs differ lexically.</li>\n      <li><strong>Safety and policy compliance</strong>: Judging nuanced violations where keyword matching fails.</li>\n    </ul>\n<p>As a result, LLM-as-a-Judge has become a foundational component of modern evaluation pipelines.</p>",
    "contentMarkdown": "*   Modern LLM applications—reasoning, tool use, code generation, RAG, safety alignment—often lack clear ground truth. Evaluation becomes inherently subjective or context-dependent.\n    \n*   Examples where traditional metrics fail but LLM-as-a-Judge succeeds include:\n    \n    *   **Chain-of-thought reasoning**: Evaluating whether reasoning is valid, not just whether the final answer matches.\n    *   **Summarization faithfulness**: Detecting subtle hallucinations that ROUGE cannot capture, as shown in [Evaluating the Factual Consistency of Summaries](https://arxiv.org/abs/2008.01415) by Kryściński et al. (2020).\n    *   **Instruction following**: Determining whether constraints were followed, even when outputs differ lexically.\n    *   **Safety and policy compliance**: Judging nuanced violations where keyword matching fails.\n*   As a result, LLM-as-a-Judge has become a foundational component of modern evaluation pipelines.\n    \n\nModern LLM applications—reasoning, tool use, code generation, RAG, safety alignment—often lack clear ground truth. Evaluation becomes inherently subjective or context-dependent.\n\nExamples where traditional metrics fail but LLM-as-a-Judge succeeds include:\n\n*   **Chain-of-thought reasoning**: Evaluating whether reasoning is valid, not just whether the final answer matches.\n*   **Summarization faithfulness**: Detecting subtle hallucinations that ROUGE cannot capture, as shown in [Evaluating the Factual Consistency of Summaries](https://arxiv.org/abs/2008.01415) by Kryściński et al. (2020).\n*   **Instruction following**: Determining whether constraints were followed, even when outputs differ lexically.\n*   **Safety and policy compliance**: Judging nuanced violations where keyword matching fails.\n\nAs a result, LLM-as-a-Judge has become a foundational component of modern evaluation pipelines.",
    "contentLength": 2154,
    "wordCount": 215,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#why-this-matters-for-modern-llm-tasks"
  },
  {
    "id": "ai-LLM-as-a-judge-from-evaluation-to-ranking-and-learning-5",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Motivation: Why LLM-as-a-Judge is Needed Beyond Traditional Metrics",
    "title": "From Evaluation to Ranking and Learning",
    "order": 5,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>\n    <p>Crucially, LLM-as-a-Judge is not limited to scalar scoring. It naturally connects to <strong>Learning-to-Rank (LTR)</strong> formulations:</p>\n\n    <ul>\n      <li>Scoring an output independently (pointwise)</li>\n      <li>Comparing two outputs relatively (pairwise)</li>\n      <li>Ranking lists of outputs holistically (listwise)</li>\n    </ul>\n  </li>\n  <li>\n    <p><a href=\"#background-learning-to-rank-ltr-paradigms\">Background: Learning-to-Rank (LTR) Paradigms</a> offers a detailed discourse on this topic, covering each paradigm in detail.</p>\n  </li>\n  <li>\n    <p>This connection enables downstream use cases such as:</p>\n\n    <ul>\n      <li>Model selection and benchmarking</li>\n      <li>Dataset filtering and curriculum construction</li>\n      <li>Reward modeling for RL</li>\n      <li>Reranking in retrieval and generation pipelines</li>\n    </ul>\n  </li>\n  <li>\n    <p>In the next section, we will formally introduce <strong>Learning-to-Rank paradigms (pointwise, pairwise, listwise)</strong> and explain how LLM-as-a-Judge fits naturally into these frameworks.</p>\n  </li>\n</ul>\n<p>Crucially, LLM-as-a-Judge is not limited to scalar scoring. It naturally connects to <strong>Learning-to-Rank (LTR)</strong> formulations:</p>\n<ul>\n      <li>Scoring an output independently (pointwise)</li>\n      <li>Comparing two outputs relatively (pairwise)</li>\n      <li>Ranking lists of outputs holistically (listwise)</li>\n    </ul>\n<p><a href=\"#background-learning-to-rank-ltr-paradigms\">Background: Learning-to-Rank (LTR) Paradigms</a> offers a detailed discourse on this topic, covering each paradigm in detail.</p>\n<p>This connection enables downstream use cases such as:</p>\n<ul>\n      <li>Model selection and benchmarking</li>\n      <li>Dataset filtering and curriculum construction</li>\n      <li>Reward modeling for RL</li>\n      <li>Reranking in retrieval and generation pipelines</li>\n    </ul>\n<p>In the next section, we will formally introduce <strong>Learning-to-Rank paradigms (pointwise, pairwise, listwise)</strong> and explain how LLM-as-a-Judge fits naturally into these frameworks.</p>",
    "contentMarkdown": "*   Crucially, LLM-as-a-Judge is not limited to scalar scoring. It naturally connects to **Learning-to-Rank (LTR)** formulations:\n    \n    *   Scoring an output independently (pointwise)\n    *   Comparing two outputs relatively (pairwise)\n    *   Ranking lists of outputs holistically (listwise)\n*   [Background: Learning-to-Rank (LTR) Paradigms](#background-learning-to-rank-ltr-paradigms) offers a detailed discourse on this topic, covering each paradigm in detail.\n    \n*   This connection enables downstream use cases such as:\n    \n    *   Model selection and benchmarking\n    *   Dataset filtering and curriculum construction\n    *   Reward modeling for RL\n    *   Reranking in retrieval and generation pipelines\n*   In the next section, we will formally introduce **Learning-to-Rank paradigms (pointwise, pairwise, listwise)** and explain how LLM-as-a-Judge fits naturally into these frameworks.\n    \n\nCrucially, LLM-as-a-Judge is not limited to scalar scoring. It naturally connects to **Learning-to-Rank (LTR)** formulations:\n\n*   Scoring an output independently (pointwise)\n*   Comparing two outputs relatively (pairwise)\n*   Ranking lists of outputs holistically (listwise)\n\n[Background: Learning-to-Rank (LTR) Paradigms](#background-learning-to-rank-ltr-paradigms) offers a detailed discourse on this topic, covering each paradigm in detail.\n\nThis connection enables downstream use cases such as:\n\n*   Model selection and benchmarking\n*   Dataset filtering and curriculum construction\n*   Reward modeling for RL\n*   Reranking in retrieval and generation pipelines\n\nIn the next section, we will formally introduce **Learning-to-Rank paradigms (pointwise, pairwise, listwise)** and explain how LLM-as-a-Judge fits naturally into these frameworks.",
    "contentLength": 2126,
    "wordCount": 210,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#from-evaluation-to-ranking-and-learning"
  },
  {
    "id": "ai-LLM-as-a-judge-pointwise-ranking-6",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Background: Learning-to-Rank (LTR) Paradigms",
    "title": "Pointwise Ranking",
    "order": 6,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>In pointwise LTR, each candidate is scored independently (i.e., in an absolute manner) with respect to the query. The model learns a function:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 6.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.159em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.16em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-5\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-6\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-7\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-8\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-9\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-10\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-11\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">→</span><span class=\"msubsup\" id=\"MathJax-Span-12\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-13\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-14\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">→</mo><msub><mi>s</mi><mi>i</mi></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-1\">f(q, x_i) \\rightarrow s_i</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-15\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-16\"><span class=\"mi\" id=\"MathJax-Span-17\" style=\"font-family: STIXGeneral-Italic;\">q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">q</script> is the query (or task), <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-18\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-19\"><span class=\"msubsup\" id=\"MathJax-Span-20\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-21\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-22\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">x_i</script> is a candidate output, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-23\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-24\"><span class=\"msubsup\" id=\"MathJax-Span-25\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-27\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">s_i</script> is an absolute relevance or quality score.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Classic examples include regression or classification approaches, where the model predicts relevance labels or probabilities. Early neural formulations include <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf\">RankNet</a> by Burges et al. (2010) and later transformer-based rankers such as <a href=\"https://arxiv.org/abs/1910.14424\">monoBERT</a> by Nogueira et al. (2019).</p>\n  </li>\n  <li>\n    <p>In the context of LLM-as-a-Judge:</p>\n\n    <ul>\n      <li>The judge scores each output independently</li>\n      <li>Scores may be binary, ordinal, or continuous</li>\n      <li>No explicit comparison between outputs is required</li>\n    </ul>\n  </li>\n  <li>\n    <p>This is the <strong>most common formulation</strong> used in practice, because it is simple, parallelizable, and easy to operationalize.</p>\n  </li>\n</ul>\n<p>In pointwise LTR, each candidate is scored independently (i.e., in an absolute manner) with respect to the query. The model learns a function:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-15\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-16\"><span class=\"mi\" id=\"MathJax-Span-17\" style=\"font-family: STIXGeneral-Italic;\">q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">q</script> is the query (or task), <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-18\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-19\"><span class=\"msubsup\" id=\"MathJax-Span-20\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-21\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-22\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">x_i</script> is a candidate output, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-23\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-24\"><span class=\"msubsup\" id=\"MathJax-Span-25\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-27\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">s_i</script> is an absolute relevance or quality score.</li>\n    </ul>\n<p>Classic examples include regression or classification approaches, where the model predicts relevance labels or probabilities. Early neural formulations include <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf\">RankNet</a> by Burges et al. (2010) and later transformer-based rankers such as <a href=\"https://arxiv.org/abs/1910.14424\">monoBERT</a> by Nogueira et al. (2019).</p>\n<p>In the context of LLM-as-a-Judge:</p>\n<ul>\n      <li>The judge scores each output independently</li>\n      <li>Scores may be binary, ordinal, or continuous</li>\n      <li>No explicit comparison between outputs is required</li>\n    </ul>\n<p>This is the <strong>most common formulation</strong> used in practice, because it is simple, parallelizable, and easy to operationalize.</p>",
    "contentMarkdown": "*   In pointwise LTR, each candidate is scored independently (i.e., in an absolute manner) with respect to the query. The model learns a function:\n    \n    f(q,xi)→sif(q,xi)→si\n    \n    f(q, x\\_i) \\\\rightarrow s\\_i\n    *   where qqq is the query (or task), xixix\\_i is a candidate output, and sisis\\_i is an absolute relevance or quality score.\n*   Classic examples include regression or classification approaches, where the model predicts relevance labels or probabilities. Early neural formulations include [RankNet](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) by Burges et al. (2010) and later transformer-based rankers such as [monoBERT](https://arxiv.org/abs/1910.14424) by Nogueira et al. (2019).\n    \n*   In the context of LLM-as-a-Judge:\n    \n    *   The judge scores each output independently\n    *   Scores may be binary, ordinal, or continuous\n    *   No explicit comparison between outputs is required\n*   This is the **most common formulation** used in practice, because it is simple, parallelizable, and easy to operationalize.\n    \n\nIn pointwise LTR, each candidate is scored independently (i.e., in an absolute manner) with respect to the query. The model learns a function:\n\n*   where qqq is the query (or task), xixix\\_i is a candidate output, and sisis\\_i is an absolute relevance or quality score.\n\nClassic examples include regression or classification approaches, where the model predicts relevance labels or probabilities. Early neural formulations include [RankNet](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) by Burges et al. (2010) and later transformer-based rankers such as [monoBERT](https://arxiv.org/abs/1910.14424) by Nogueira et al. (2019).\n\nIn the context of LLM-as-a-Judge:\n\n*   The judge scores each output independently\n*   Scores may be binary, ordinal, or continuous\n*   No explicit comparison between outputs is required\n\nThis is the **most common formulation** used in practice, because it is simple, parallelizable, and easy to operationalize.",
    "contentLength": 15817,
    "wordCount": 265,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#pointwise-ranking"
  },
  {
    "id": "ai-LLM-as-a-judge-pairwise-ranking-7",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Background: Learning-to-Rank (LTR) Paradigms",
    "title": "Pairwise Ranking",
    "order": 7,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Pairwise LTR models learn relative preferences between two candidates at a time. Instead of predicting absolute scores, the model predicts which of two candidates is better:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>&amp;#x227B;</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-28\" style=\"width: 11.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.586em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1009.53em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-29\"><span class=\"mi\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-31\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-32\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-34\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-35\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-38\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-39\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-41\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-42\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">→</span><span class=\"mi\" id=\"MathJax-Span-43\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">P</span><span class=\"mo\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-45\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-47\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≻</span><span class=\"msubsup\" id=\"MathJax-Span-49\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-50\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-51\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-52\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">→</mo><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>≻</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>\n    <p>Training data consists of ordered pairs <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-53\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.55em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-54\"><span class=\"mo\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-56\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-58\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-60\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-62\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">(x_i, x_j)</script> labeled according to preference. Optimization typically minimizes a pairwise loss such as logistic loss:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>pairwise</mtext></mrow></msub><mo>=</mo><mo>&amp;#x2212;</mo><mi>log</mi><mo>&amp;#x2061;</mo><mi>&amp;#x03C3;</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>&amp;#x2212;</mo><msub><mi>s</mi><mi>j</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-64\" style=\"width: 12.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1010.32em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-65\"><span class=\"msubsup\" id=\"MathJax-Span-66\"><span style=\"display: inline-block; position: relative; width: 3.18em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-67\"><span class=\"mrow\" id=\"MathJax-Span-68\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-70\"><span class=\"mrow\" id=\"MathJax-Span-71\"><span class=\"mtext\" id=\"MathJax-Span-72\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">pairwise</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-73\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-74\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"mi\" id=\"MathJax-Span-75\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-76\"></span><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-78\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-79\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-80\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-81\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-83\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-84\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-85\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-86\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>pairwise</mtext></mrow></msub><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>−</mo><msub><mi>s</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-7\">\\mathcal{L}_{\\text{pairwise}} = - \\log \\sigma(s_i - s_j)</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-87\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-88\"><span class=\"mi\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>σ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">\\sigma</script> is the sigmoid function.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Examples include <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf\">RankNet</a> by Burges et al. (2010) and pairwise extensions of transformer models such as <a href=\"https://arxiv.org/abs/1910.14424\">duoBERT</a> by Nogueira et al. (2019).</p>\n  </li>\n  <li>\n    <p>In LLM-as-a-Judge systems, pairwise evaluation appears when:</p>\n\n    <ul>\n      <li>The judge is asked “Which output is better?”</li>\n      <li>Human-like preference judgments are required</li>\n      <li>Absolute scoring is difficult or ill-defined</li>\n    </ul>\n  </li>\n  <li>\n    <p>Pairwise judgments often exhibit <strong>higher inter-annotator agreement</strong> than absolute ratings, a phenomenon discussed in <a href=\"https://arxiv.org/abs/2305.17926\">A Large-Scale Analysis of Evaluation Biases in LLMs</a> by Wang et al. (2023).</p>\n  </li>\n</ul>\n<p>Training data consists of ordered pairs <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-53\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.55em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-54\"><span class=\"mo\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-56\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-58\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-60\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-62\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">(x_i, x_j)</script> labeled according to preference. Optimization typically minimizes a pairwise loss such as logistic loss:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-87\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-88\"><span class=\"mi\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>σ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">\\sigma</script> is the sigmoid function.</li>\n    </ul>\n<p>Examples include <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf\">RankNet</a> by Burges et al. (2010) and pairwise extensions of transformer models such as <a href=\"https://arxiv.org/abs/1910.14424\">duoBERT</a> by Nogueira et al. (2019).</p>\n<p>In LLM-as-a-Judge systems, pairwise evaluation appears when:</p>\n<ul>\n      <li>The judge is asked “Which output is better?”</li>\n      <li>Human-like preference judgments are required</li>\n      <li>Absolute scoring is difficult or ill-defined</li>\n    </ul>\n<p>Pairwise judgments often exhibit <strong>higher inter-annotator agreement</strong> than absolute ratings, a phenomenon discussed in <a href=\"https://arxiv.org/abs/2305.17926\">A Large-Scale Analysis of Evaluation Biases in LLMs</a> by Wang et al. (2023).</p>",
    "contentMarkdown": "*   Pairwise LTR models learn relative preferences between two candidates at a time. Instead of predicting absolute scores, the model predicts which of two candidates is better:\n\nf(q,xi,xj)→P(xi≻xj)f(q,xi,xj)→P(xi≻xj)\n\n*   Training data consists of ordered pairs (xi,xj)(xi,xj)(x\\_i, x\\_j) labeled according to preference. Optimization typically minimizes a pairwise loss such as logistic loss:\n    \n    pairwise\\=−logσ(si−sj)Lpairwise\\=−log⁡σ(si−sj)\n    \n    \\\\mathcal{L}\\_{\\\\text{pairwise}} = - \\\\log \\\\sigma(s\\_i - s\\_j)\n    *   where σσ\\\\sigma is the sigmoid function.\n*   Examples include [RankNet](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) by Burges et al. (2010) and pairwise extensions of transformer models such as [duoBERT](https://arxiv.org/abs/1910.14424) by Nogueira et al. (2019).\n    \n*   In LLM-as-a-Judge systems, pairwise evaluation appears when:\n    \n    *   The judge is asked “Which output is better?”\n    *   Human-like preference judgments are required\n    *   Absolute scoring is difficult or ill-defined\n*   Pairwise judgments often exhibit **higher inter-annotator agreement** than absolute ratings, a phenomenon discussed in [A Large-Scale Analysis of Evaluation Biases in LLMs](https://arxiv.org/abs/2305.17926) by Wang et al. (2023).\n    \n\nTraining data consists of ordered pairs (xi,xj)(xi,xj)(x\\_i, x\\_j) labeled according to preference. Optimization typically minimizes a pairwise loss such as logistic loss:\n\n*   where σσ\\\\sigma is the sigmoid function.\n\nExamples include [RankNet](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) by Burges et al. (2010) and pairwise extensions of transformer models such as [duoBERT](https://arxiv.org/abs/1910.14424) by Nogueira et al. (2019).\n\nIn LLM-as-a-Judge systems, pairwise evaluation appears when:\n\n*   The judge is asked “Which output is better?”\n*   Human-like preference judgments are required\n*   Absolute scoring is difficult or ill-defined\n\nPairwise judgments often exhibit **higher inter-annotator agreement** than absolute ratings, a phenomenon discussed in [A Large-Scale Analysis of Evaluation Biases in LLMs](https://arxiv.org/abs/2305.17926) by Wang et al. (2023).",
    "contentLength": 23018,
    "wordCount": 254,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#pairwise-ranking"
  },
  {
    "id": "ai-LLM-as-a-judge-listwise-ranking-8",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Background: Learning-to-Rank (LTR) Paradigms",
    "title": "Listwise Ranking",
    "order": 8,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>Listwise LTR methods consider the entire candidate set jointly and optimize a loss defined over permutations or ranked lists:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo><mi>&amp;#x03C0;</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-90\" style=\"width: 9.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.284em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1008.28em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-91\"><span class=\"mi\" id=\"MathJax-Span-92\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-96\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-97\"><span class=\"msubsup\" id=\"MathJax-Span-98\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-100\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-103\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-104\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-105\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-108\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">→</span><span class=\"mi\" id=\"MathJax-Span-109\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">→</mo><mi>π</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-9\">f(q, {x_1, \\dots, x_n}) \\rightarrow \\pi</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C0;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-110\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-111\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>π</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">\\pi</script> is an ordering of the candidates.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Listwise approaches directly optimize ranking metrics such as NDCG or MAP using differentiable approximations. Well-known listwise losses include ListMLE proposed in <a href=\"https://icml.cc/Conferences/2008/papers/167.pdf\">Listwise Approach to Learning to Rank - Theory and Algorithm</a> by Xia et al. (2008) and softmax cross-entropy over permutations, introduced in <a href=\"https://dl.acm.org/doi/10.1145/1277741.1277800\">Listwise Approach to Learning to Rank</a> by Cao et al. (2007).</p>\n  </li>\n  <li>\n    <p>More recent transformer-based listwise models include:</p>\n\n    <ul>\n      <li><a href=\"https://arxiv.org/abs/2206.15198\">ListBERT</a> by Kumar et al. (2022)</li>\n      <li><a href=\"https://arxiv.org/abs/2402.15838\">ListT5</a> by Yoon et al. (2024)</li>\n      <li>Agentic rerankers such as <a href=\"https://arxiv.org/abs/2505.20046\">REARANK</a> by Zhang et al. (2025)</li>\n    </ul>\n  </li>\n  <li>\n    <p>In LLM-as-a-Judge settings, listwise evaluation arises when:</p>\n\n    <ul>\n      <li>The judge must rank multiple outputs at once</li>\n      <li>Relative ordering matters more than absolute scores</li>\n      <li>Global consistency across outputs is important</li>\n    </ul>\n  </li>\n</ul>\n<p>Listwise LTR methods consider the entire candidate set jointly and optimize a loss defined over permutations or ranked lists:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C0;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-110\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-111\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>π</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">\\pi</script> is an ordering of the candidates.</li>\n    </ul>\n<p>Listwise approaches directly optimize ranking metrics such as NDCG or MAP using differentiable approximations. Well-known listwise losses include ListMLE proposed in <a href=\"https://icml.cc/Conferences/2008/papers/167.pdf\">Listwise Approach to Learning to Rank - Theory and Algorithm</a> by Xia et al. (2008) and softmax cross-entropy over permutations, introduced in <a href=\"https://dl.acm.org/doi/10.1145/1277741.1277800\">Listwise Approach to Learning to Rank</a> by Cao et al. (2007).</p>\n<p>More recent transformer-based listwise models include:</p>\n<ul>\n      <li><a href=\"https://arxiv.org/abs/2206.15198\">ListBERT</a> by Kumar et al. (2022)</li>\n      <li><a href=\"https://arxiv.org/abs/2402.15838\">ListT5</a> by Yoon et al. (2024)</li>\n      <li>Agentic rerankers such as <a href=\"https://arxiv.org/abs/2505.20046\">REARANK</a> by Zhang et al. (2025)</li>\n    </ul>\n<p>In LLM-as-a-Judge settings, listwise evaluation arises when:</p>\n<ul>\n      <li>The judge must rank multiple outputs at once</li>\n      <li>Relative ordering matters more than absolute scores</li>\n      <li>Global consistency across outputs is important</li>\n    </ul>",
    "contentMarkdown": "*   Listwise LTR methods consider the entire candidate set jointly and optimize a loss defined over permutations or ranked lists:\n    \n    f(q,x1,…,xn)→πf(q,x1,…,xn)→π\n    \n    f(q, {x\\_1, \\\\dots, x\\_n}) \\\\rightarrow \\\\pi\n    *   where ππ\\\\pi is an ordering of the candidates.\n*   Listwise approaches directly optimize ranking metrics such as NDCG or MAP using differentiable approximations. Well-known listwise losses include ListMLE proposed in [Listwise Approach to Learning to Rank - Theory and Algorithm](https://icml.cc/Conferences/2008/papers/167.pdf) by Xia et al. (2008) and softmax cross-entropy over permutations, introduced in [Listwise Approach to Learning to Rank](https://dl.acm.org/doi/10.1145/1277741.1277800) by Cao et al. (2007).\n    \n*   More recent transformer-based listwise models include:\n    \n    *   [ListBERT](https://arxiv.org/abs/2206.15198) by Kumar et al. (2022)\n    *   [ListT5](https://arxiv.org/abs/2402.15838) by Yoon et al. (2024)\n    *   Agentic rerankers such as [REARANK](https://arxiv.org/abs/2505.20046) by Zhang et al. (2025)\n*   In LLM-as-a-Judge settings, listwise evaluation arises when:\n    \n    *   The judge must rank multiple outputs at once\n    *   Relative ordering matters more than absolute scores\n    *   Global consistency across outputs is important\n\nListwise LTR methods consider the entire candidate set jointly and optimize a loss defined over permutations or ranked lists:\n\n*   where ππ\\\\pi is an ordering of the candidates.\n\nListwise approaches directly optimize ranking metrics such as NDCG or MAP using differentiable approximations. Well-known listwise losses include ListMLE proposed in [Listwise Approach to Learning to Rank - Theory and Algorithm](https://icml.cc/Conferences/2008/papers/167.pdf) by Xia et al. (2008) and softmax cross-entropy over permutations, introduced in [Listwise Approach to Learning to Rank](https://dl.acm.org/doi/10.1145/1277741.1277800) by Cao et al. (2007).\n\nMore recent transformer-based listwise models include:\n\n*   [ListBERT](https://arxiv.org/abs/2206.15198) by Kumar et al. (2022)\n*   [ListT5](https://arxiv.org/abs/2402.15838) by Yoon et al. (2024)\n*   Agentic rerankers such as [REARANK](https://arxiv.org/abs/2505.20046) by Zhang et al. (2025)\n\nIn LLM-as-a-Judge settings, listwise evaluation arises when:\n\n*   The judge must rank multiple outputs at once\n*   Relative ordering matters more than absolute scores\n*   Global consistency across outputs is important",
    "contentLength": 10074,
    "wordCount": 299,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#listwise-ranking"
  },
  {
    "id": "ai-LLM-as-a-judge-how-llm-as-a-judge-fits-into-ltr-9",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Pointwise LLM-as-a-Judge in Practice",
    "title": "How LLM-as-a-Judge Fits Into LTR",
    "order": 9,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>LLM-as-a-Judge can be viewed as an <strong>instantiation of LTR</strong>, where the judge model implicitly implements an LTR paradigm (pointwise, pairwise, and listwise) via prompting or fine-tuning.</p>\n  </li>\n  <li>\n    <p>Crucially:</p>\n  </li>\n</ul>\n<p>LLM-as-a-Judge can be viewed as an <strong>instantiation of LTR</strong>, where the judge model implicitly implements an LTR paradigm (pointwise, pairwise, and listwise) via prompting or fine-tuning.</p>\n<p>Crucially:</p>\n<blockquote>\n  <p>LLM-as-a-judge models most commonly perform <strong>pointwise</strong> evaluation (scoring each output independently against a rubric), though pairwise or listwise ranking is also used in some settings.</p>\n</blockquote>\n<p>LLM-as-a-judge models most commonly perform <strong>pointwise</strong> evaluation (scoring each output independently against a rubric), though pairwise or listwise ranking is also used in some settings.</p>\n<ul>\n  <li>\n    <p>This design choice reflects practical trade-offs:</p>\n\n    <ul>\n      <li>Pointwise judging is simpler and cheaper</li>\n      <li>Pairwise judging reduces calibration issues</li>\n      <li>Listwise judging captures global consistency but is more expensive</li>\n    </ul>\n  </li>\n  <li>\n    <p>In the next section, we will how an LLM-as-a-Judge operationalizes <strong>pointwise LTR</strong>, introduce the judge prompt, and discuss design principles for reliable judge prompts before moving on to fine-tuning encoder and decoder models for LTR.</p>\n  </li>\n</ul>\n<p>This design choice reflects practical trade-offs:</p>\n<ul>\n      <li>Pointwise judging is simpler and cheaper</li>\n      <li>Pairwise judging reduces calibration issues</li>\n      <li>Listwise judging captures global consistency but is more expensive</li>\n    </ul>\n<p>In the next section, we will how an LLM-as-a-Judge operationalizes <strong>pointwise LTR</strong>, introduce the judge prompt, and discuss design principles for reliable judge prompts before moving on to fine-tuning encoder and decoder models for LTR.</p>",
    "contentMarkdown": "*   LLM-as-a-Judge can be viewed as an **instantiation of LTR**, where the judge model implicitly implements an LTR paradigm (pointwise, pairwise, and listwise) via prompting or fine-tuning.\n    \n*   Crucially:\n    \n\nLLM-as-a-Judge can be viewed as an **instantiation of LTR**, where the judge model implicitly implements an LTR paradigm (pointwise, pairwise, and listwise) via prompting or fine-tuning.\n\nCrucially:\n\n> LLM-as-a-judge models most commonly perform **pointwise** evaluation (scoring each output independently against a rubric), though pairwise or listwise ranking is also used in some settings.\n\nLLM-as-a-judge models most commonly perform **pointwise** evaluation (scoring each output independently against a rubric), though pairwise or listwise ranking is also used in some settings.\n\n*   This design choice reflects practical trade-offs:\n    \n    *   Pointwise judging is simpler and cheaper\n    *   Pairwise judging reduces calibration issues\n    *   Listwise judging captures global consistency but is more expensive\n*   In the next section, we will how an LLM-as-a-Judge operationalizes **pointwise LTR**, introduce the judge prompt, and discuss design principles for reliable judge prompts before moving on to fine-tuning encoder and decoder models for LTR.\n    \n\nThis design choice reflects practical trade-offs:\n\n*   Pointwise judging is simpler and cheaper\n*   Pairwise judging reduces calibration issues\n*   Listwise judging captures global consistency but is more expensive\n\nIn the next section, we will how an LLM-as-a-Judge operationalizes **pointwise LTR**, introduce the judge prompt, and discuss design principles for reliable judge prompts before moving on to fine-tuning encoder and decoder models for LTR.",
    "contentLength": 2057,
    "wordCount": 237,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#how-llm-as-a-judge-fits-into-ltr"
  },
  {
    "id": "ai-LLM-as-a-judge-pointwise-ltr-framing-for-llm-as-a-judge-10",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Pointwise LLM-as-a-Judge in Practice",
    "title": "Pointwise LTR Framing for LLM-as-a-Judge",
    "order": 10,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>Under a pointwise formulation, the judge estimates an absolute quality score for each candidate output <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-113\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-114\"><span class=\"msubsup\" id=\"MathJax-Span-115\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-117\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">x_i</script>, conditioned on the task <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-118\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-119\"><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Italic;\">q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">q</script>:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><msub><mi>f</mi><mi>&amp;#x03B8;</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-121\" style=\"width: 5.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.898em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.85em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-122\"><span class=\"msubsup\" id=\"MathJax-Span-123\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-125\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-126\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-127\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-128\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-129\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-130\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-132\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-133\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-134\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-135\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-136\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-13\">s_i = f_\\theta(q, x_i)</script>\n\n    <ul>\n      <li>\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-137\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-138\"><span class=\"mi\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Italic;\">q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">q</script> encodes the task description and evaluation criteria</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-140\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-141\"><span class=\"msubsup\" id=\"MathJax-Span-142\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">x_i</script> is a single model-generated output</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-145\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-146\"><span class=\"msubsup\" id=\"MathJax-Span-147\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-149\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">s_i</script> may be binary, ordinal, or scalar</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mi>&amp;#x03B8;</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-150\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.68em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-151\"><span class=\"msubsup\" id=\"MathJax-Span-152\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-154\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mi>θ</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">f_\\theta</script> is implemented via an LLM prompted as a judge</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>Unlike traditional neural rankers, the scoring function is not hard-coded or trained from scratch; instead, it is <strong>induced via natural language instructions</strong>. This makes prompt design the critical interface between evaluation intent and model behavior.</p>\n  </li>\n  <li>\n    <p>Pointwise LLM-as-a-Judge is especially well-suited for:</p>\n\n    <ul>\n      <li>Offline evaluation of generated outputs</li>\n      <li>Reward modeling bootstrapping (in RL-based policy/preference optimization pipelines)</li>\n      <li>Dataset filtering and quality control</li>\n      <li>Continuous evaluation in CI pipelines</li>\n    </ul>\n  </li>\n  <li>\n    <p>Empirically, pointwise judging has been shown to correlate strongly with human ratings when prompts are carefully structured, as demonstrated in <a href=\"https://arxiv.org/abs/2306.05685\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> by Zheng et al. (2023).</p>\n  </li>\n</ul>\n<p>Under a pointwise formulation, the judge estimates an absolute quality score for each candidate output <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-113\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-114\"><span class=\"msubsup\" id=\"MathJax-Span-115\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-117\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">x_i</script>, conditioned on the task <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-118\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-119\"><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Italic;\">q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">q</script>:</p>\n<ul>\n      <li>\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-137\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-138\"><span class=\"mi\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Italic;\">q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">q</script> encodes the task description and evaluation criteria</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-140\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-141\"><span class=\"msubsup\" id=\"MathJax-Span-142\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">x_i</script> is a single model-generated output</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-145\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-146\"><span class=\"msubsup\" id=\"MathJax-Span-147\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-149\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">s_i</script> may be binary, ordinal, or scalar</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mi>&amp;#x03B8;</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-150\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.68em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-151\"><span class=\"msubsup\" id=\"MathJax-Span-152\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-154\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mi>θ</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">f_\\theta</script> is implemented via an LLM prompted as a judge</li>\n        </ul>\n      </li>\n    </ul>\n<p>where:</p>\n<ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-137\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-138\"><span class=\"mi\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Italic;\">q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">q</script> encodes the task description and evaluation criteria</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-140\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-141\"><span class=\"msubsup\" id=\"MathJax-Span-142\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">x_i</script> is a single model-generated output</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-145\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-146\"><span class=\"msubsup\" id=\"MathJax-Span-147\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-149\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">s_i</script> may be binary, ordinal, or scalar</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mi>&amp;#x03B8;</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-150\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.68em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-151\"><span class=\"msubsup\" id=\"MathJax-Span-152\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-154\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mi>θ</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">f_\\theta</script> is implemented via an LLM prompted as a judge</li>\n        </ul>\n<p>Unlike traditional neural rankers, the scoring function is not hard-coded or trained from scratch; instead, it is <strong>induced via natural language instructions</strong>. This makes prompt design the critical interface between evaluation intent and model behavior.</p>\n<p>Pointwise LLM-as-a-Judge is especially well-suited for:</p>\n<ul>\n      <li>Offline evaluation of generated outputs</li>\n      <li>Reward modeling bootstrapping (in RL-based policy/preference optimization pipelines)</li>\n      <li>Dataset filtering and quality control</li>\n      <li>Continuous evaluation in CI pipelines</li>\n    </ul>\n<p>Empirically, pointwise judging has been shown to correlate strongly with human ratings when prompts are carefully structured, as demonstrated in <a href=\"https://arxiv.org/abs/2306.05685\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> by Zheng et al. (2023).</p>",
    "contentMarkdown": "*   Under a pointwise formulation, the judge estimates an absolute quality score for each candidate output xixix\\_i, conditioned on the task qqq:\n    \n    si\\=fθ(q,xi)si\\=fθ(q,xi)\n    \n    s\\_i = f\\_\\\\theta(q, x\\_i)\n    *   where:\n        \n        *   qqq encodes the task description and evaluation criteria\n        *   xixix\\_i is a single model-generated output\n        *   sisis\\_i may be binary, ordinal, or scalar\n        *   fθfθf\\_\\\\theta is implemented via an LLM prompted as a judge\n*   Unlike traditional neural rankers, the scoring function is not hard-coded or trained from scratch; instead, it is **induced via natural language instructions**. This makes prompt design the critical interface between evaluation intent and model behavior.\n    \n*   Pointwise LLM-as-a-Judge is especially well-suited for:\n    \n    *   Offline evaluation of generated outputs\n    *   Reward modeling bootstrapping (in RL-based policy/preference optimization pipelines)\n    *   Dataset filtering and quality control\n    *   Continuous evaluation in CI pipelines\n*   Empirically, pointwise judging has been shown to correlate strongly with human ratings when prompts are carefully structured, as demonstrated in [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) by Zheng et al. (2023).\n    \n\nUnder a pointwise formulation, the judge estimates an absolute quality score for each candidate output xixix\\_i, conditioned on the task qqq:\n\n*   where:\n    \n    *   qqq encodes the task description and evaluation criteria\n    *   xixix\\_i is a single model-generated output\n    *   sisis\\_i may be binary, ordinal, or scalar\n    *   fθfθf\\_\\\\theta is implemented via an LLM prompted as a judge\n\nwhere:\n\n*   qqq encodes the task description and evaluation criteria\n*   xixix\\_i is a single model-generated output\n*   sisis\\_i may be binary, ordinal, or scalar\n*   fθfθf\\_\\\\theta is implemented via an LLM prompted as a judge\n\nUnlike traditional neural rankers, the scoring function is not hard-coded or trained from scratch; instead, it is **induced via natural language instructions**. This makes prompt design the critical interface between evaluation intent and model behavior.\n\nPointwise LLM-as-a-Judge is especially well-suited for:\n\n*   Offline evaluation of generated outputs\n*   Reward modeling bootstrapping (in RL-based policy/preference optimization pipelines)\n*   Dataset filtering and quality control\n*   Continuous evaluation in CI pipelines\n\nEmpirically, pointwise judging has been shown to correlate strongly with human ratings when prompts are carefully structured, as demonstrated in [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) by Zheng et al. (2023).",
    "contentLength": 34474,
    "wordCount": 361,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#pointwise-ltr-framing-for-llm-as-a-judge"
  },
  {
    "id": "ai-LLM-as-a-judge-design-principles-for-reliable-judge-prompts-11",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Pointwise LLM-as-a-Judge in Practice",
    "title": "Design Principles for Reliable Judge Prompts",
    "order": 11,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>Several best practices have emerged for robust pointwise judge prompts:</p>\n\n    <ol>\n      <li>\n        <p><strong>Explicit role definition</strong>: The judge should be instructed to behave as an impartial evaluator, not a helper or teacher.</p>\n      </li>\n      <li>\n        <p><strong>Clear separation of task and evaluation</strong>: The prompt must distinguish between what the model was asked to do and how it should be judged.</p>\n      </li>\n      <li>\n        <p><strong>Structured criteria with explicit scales</strong>: Binary, ternary, ordinal, and Likert-type scales should be clearly labeled to reduce ambiguity.</p>\n      </li>\n      <li>\n        <p><strong>Anchored examples</strong>: Providing example inputs for each score level stabilizes calibration and reduces variance, similar to rater training in human evaluation.</p>\n      </li>\n      <li>\n        <p><strong>Schema-constrained outputs</strong>: Enforcing JSON or schema-based outputs reduces parsing errors and improves reproducibility.</p>\n      </li>\n      <li>\n        <p><strong>Omission of improvement suggestions</strong>: Judges should evaluate, not coach, unless explicitly instructed.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p>These principles are echoed in industry evaluation frameworks such as <a href=\"https://openai.com/blog/evals\">OpenAI Evals</a> and preference modeling pipelines described in <a href=\"https://arxiv.org/abs/2203.02155\">Training language models to follow instructions with human feedback</a> by Ouyang et al. (2022).</p>\n  </li>\n</ul>\n<p>Several best practices have emerged for robust pointwise judge prompts:</p>\n<ol>\n      <li>\n        <p><strong>Explicit role definition</strong>: The judge should be instructed to behave as an impartial evaluator, not a helper or teacher.</p>\n      </li>\n      <li>\n        <p><strong>Clear separation of task and evaluation</strong>: The prompt must distinguish between what the model was asked to do and how it should be judged.</p>\n      </li>\n      <li>\n        <p><strong>Structured criteria with explicit scales</strong>: Binary, ternary, ordinal, and Likert-type scales should be clearly labeled to reduce ambiguity.</p>\n      </li>\n      <li>\n        <p><strong>Anchored examples</strong>: Providing example inputs for each score level stabilizes calibration and reduces variance, similar to rater training in human evaluation.</p>\n      </li>\n      <li>\n        <p><strong>Schema-constrained outputs</strong>: Enforcing JSON or schema-based outputs reduces parsing errors and improves reproducibility.</p>\n      </li>\n      <li>\n        <p><strong>Omission of improvement suggestions</strong>: Judges should evaluate, not coach, unless explicitly instructed.</p>\n      </li>\n    </ol>\n<p><strong>Explicit role definition</strong>: The judge should be instructed to behave as an impartial evaluator, not a helper or teacher.</p>\n<p><strong>Clear separation of task and evaluation</strong>: The prompt must distinguish between what the model was asked to do and how it should be judged.</p>\n<p><strong>Structured criteria with explicit scales</strong>: Binary, ternary, ordinal, and Likert-type scales should be clearly labeled to reduce ambiguity.</p>\n<p><strong>Anchored examples</strong>: Providing example inputs for each score level stabilizes calibration and reduces variance, similar to rater training in human evaluation.</p>\n<p><strong>Schema-constrained outputs</strong>: Enforcing JSON or schema-based outputs reduces parsing errors and improves reproducibility.</p>\n<p><strong>Omission of improvement suggestions</strong>: Judges should evaluate, not coach, unless explicitly instructed.</p>\n<p>These principles are echoed in industry evaluation frameworks such as <a href=\"https://openai.com/blog/evals\">OpenAI Evals</a> and preference modeling pipelines described in <a href=\"https://arxiv.org/abs/2203.02155\">Training language models to follow instructions with human feedback</a> by Ouyang et al. (2022).</p>",
    "contentMarkdown": "*   Several best practices have emerged for robust pointwise judge prompts:\n    \n    1.  **Explicit role definition**: The judge should be instructed to behave as an impartial evaluator, not a helper or teacher.\n        \n    2.  **Clear separation of task and evaluation**: The prompt must distinguish between what the model was asked to do and how it should be judged.\n        \n    3.  **Structured criteria with explicit scales**: Binary, ternary, ordinal, and Likert-type scales should be clearly labeled to reduce ambiguity.\n        \n    4.  **Anchored examples**: Providing example inputs for each score level stabilizes calibration and reduces variance, similar to rater training in human evaluation.\n        \n    5.  **Schema-constrained outputs**: Enforcing JSON or schema-based outputs reduces parsing errors and improves reproducibility.\n        \n    6.  **Omission of improvement suggestions**: Judges should evaluate, not coach, unless explicitly instructed.\n        \n*   These principles are echoed in industry evaluation frameworks such as [OpenAI Evals](https://openai.com/blog/evals) and preference modeling pipelines described in [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) by Ouyang et al. (2022).\n    \n\nSeveral best practices have emerged for robust pointwise judge prompts:\n\n1.  **Explicit role definition**: The judge should be instructed to behave as an impartial evaluator, not a helper or teacher.\n    \n2.  **Clear separation of task and evaluation**: The prompt must distinguish between what the model was asked to do and how it should be judged.\n    \n3.  **Structured criteria with explicit scales**: Binary, ternary, ordinal, and Likert-type scales should be clearly labeled to reduce ambiguity.\n    \n4.  **Anchored examples**: Providing example inputs for each score level stabilizes calibration and reduces variance, similar to rater training in human evaluation.\n    \n5.  **Schema-constrained outputs**: Enforcing JSON or schema-based outputs reduces parsing errors and improves reproducibility.\n    \n6.  **Omission of improvement suggestions**: Judges should evaluate, not coach, unless explicitly instructed.\n    \n\n**Explicit role definition**: The judge should be instructed to behave as an impartial evaluator, not a helper or teacher.\n\n**Clear separation of task and evaluation**: The prompt must distinguish between what the model was asked to do and how it should be judged.\n\n**Structured criteria with explicit scales**: Binary, ternary, ordinal, and Likert-type scales should be clearly labeled to reduce ambiguity.\n\n**Anchored examples**: Providing example inputs for each score level stabilizes calibration and reduces variance, similar to rater training in human evaluation.\n\n**Schema-constrained outputs**: Enforcing JSON or schema-based outputs reduces parsing errors and improves reproducibility.\n\n**Omission of improvement suggestions**: Judges should evaluate, not coach, unless explicitly instructed.\n\nThese principles are echoed in industry evaluation frameworks such as [OpenAI Evals](https://openai.com/blog/evals) and preference modeling pipelines described in [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) by Ouyang et al. (2022).",
    "contentLength": 4006,
    "wordCount": 419,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#design-principles-for-reliable-judge-prompts"
  },
  {
    "id": "ai-LLM-as-a-judge-example-pointwise-llm-as-a-judge-prompt-12",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Pointwise LLM-as-a-Judge in Practice",
    "title": "Example Pointwise LLM-as-a-Judge Prompt",
    "order": 12,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>Below is an <strong>example pointwise LLM-as-a-Judge prompt</strong> for evaluating a summarization task, implementing the principles above. This prompt instantiates a mixed-scale, rubric-driven evaluator and corresponds directly to a pointwise LTR model that scores each output independently.</li>\n  <li>Importantly, the prompt includes <strong>anchored example inputs for each evaluation criterion and each rubric level</strong>, which serve as calibration references for the judge. Multiple such examples can be provided per rubric level, enabling <strong>few-shot learning</strong> that improves scale consistency, reduces ambiguity between adjacent scores, and stabilizes judgments across different criteria.</li>\n  <li>Example prompt for an LLM-as-Judge for evaluating a summarization task (mixed scales, JSON output):</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\"># Role Definition\n\nYou are an impartial, highly rigorous evaluator acting as a judge for assessing model-generated summaries of technical articles. \nYour role is to assess whether a given model-generated summary successfully fulfills the task of producing a concise, accurate synthesis of a technical article, capturing its main claims, key supporting points, and overall conclusion while remaining faithful to the original content. \nAll evaluations must be conducted strictly according to the specified evaluation criteria and scoring rubrics, with the expectation that the target summary length is 120–150 words.\n\n# Evaluation Criteria and Scales\n\n* Accuracy (binary yes/no)\n* Coverage (ordinal scale: 0–3)\n* Faithfulness (ternary ordinal scale: 0–2)\n* Clarity (Likert-type ordinal scale: 1–5)\n* Conciseness (ternary ordinal scale: 0–2)\n\n# Scoring Rubrics with Anchored Example Inputs\n\n## Accuracy (yes/no)\n\nWhether the summary contains any factual errors relative to the source.\n\n* yes: No factual errors are present.  \n  Example input that should be rated yes:  \n  “The paper evaluates a transformer-based model on three benchmarks and reports consistent performance improvements across all of them,” when the source article states exactly this.\n\n* no: One or more factual errors are present.  \n  Example input that should be rated no:  \n  “The authors conducted a large-scale human trial to validate the approach,” when the source explicitly states that no human experiments were performed.\n\n## Coverage (0–3)\n\nThe extent to which major points of the source are included.\n\n* 3: All major points are included.  \n  Example input that should be rated 3:  \n  A summary that describes the problem motivation, the proposed method, the experimental setup, the main results, and the stated limitations.\n\n* 2: Most major points are included with one minor omission.  \n  Example input that should be rated 2:  \n  A summary that explains the method and results but omits a short discussion of future work mentioned at the end of the article.\n\n* 1: Some major points are missing.  \n  Example input that should be rated 1:  \n  A summary that reports numerical results but does not explain what method or model produced them.\n\n* 0: The summary is largely incomplete.  \n  Example input that should be rated 0:  \n  A summary that only provides background context and never mentions the method, results, or conclusions.\n\n## Faithfulness (0–2)\n\nWhether the summary introduces unsupported information or interpretations.\n\n* 2: All statements are directly supported by the source.  \n  Example input that should be rated 2:  \n  A summary that paraphrases the article’s claims without adding interpretations or conclusions beyond what is stated.\n\n* 1: The summary contains a minor unsubstantiated inference.  \n  Example input that should be rated 1:  \n  A summary that claims the method is “likely to generalize to all domains,” when the paper only reports results in a limited setting.\n\n* 0: The summary is unfaithful.  \n  Example input that should be rated 0:  \n  A summary that introduces a recommendation, application, or claim that does not appear anywhere in the source article.\n\n## Clarity (Likert-type ordinal scale: 1–5)\n\nThe organization, readability, and coherence of the summary.\n\n* 5: Exceptionally clear and well-structured.  \n  Example input that should be rated 5:  \n  A summary with a clear logical flow from motivation to method to results, written in precise and unambiguous language.\n\n* 4: Mostly clear with minor issues.  \n  Example input that should be rated 4:  \n  A summary that is easy to understand overall but contains one awkward transition or slightly unclear sentence.\n\n* 3: Adequately clear but uneven.  \n  Example input that should be rated 3:  \n  A summary that is generally understandable but includes several vague phrases or mildly confusing sentences.\n\n* 2: Hard to follow.  \n  Example input that should be rated 2:  \n  A summary that jumps between ideas without clear transitions, making the structure difficult to follow.\n\n* 1: Unclear or incoherent.  \n  Example input that should be rated 1:  \n  A summary with disorganized sentences, unclear references, and no apparent structure.\n\n## Conciseness (0–2)\n\nAdherence to the target length and avoidance of unnecessary detail.\n\n* 2: Fully concise.  \n  Example input that should be rated 2:  \n  A summary within 120–150 words that avoids repetition and includes only essential information.\n\n* 1: Minor conciseness issues.  \n  Example input that should be rated 1:  \n  A summary that slightly exceeds the word limit or repeats one idea unnecessarily.\n\n* 0: Not concise.  \n  Example input that should be rated 0:  \n  A summary that is substantially longer than the target length or includes extensive irrelevant detail.\n\n# Model Outputs to Evaluate\n\nYou will be given a single model-generated summary of a source article. Evaluate this summary according to the evaluation criteria.\n\n# Evaluation Instructions\n\n1. Read the role definition, evaluation criteria, and scoring rubrics carefully.  \n2. For the given model-generated summary, assign a score for each criterion using the defined scales.  \n3. For each assigned score, provide a clear, evidence-based rationale explaining why the summary merits that score according to the rubric.  \n4. Judge only what is present in the summary.  \n5. Base decisions strictly on the rubric descriptions, using the example inputs only as anchors.  \n6. Apply scales consistently across all evaluated summaries.\n\n# Required Output Format (Strict JSON)\n\nFor each model-generated summary, output a single JSON object with the following structure and keys exactly as specified:\n\n{\n  \"accuracy\": {\n    \"score\": \"yes | no\",\n    \"justification\": \"&lt;string&gt;\"\n  },\n  \"coverage\": {\n    \"score\": &lt;integer 0–3&gt;,\n    \"justification\": \"&lt;string&gt;\"\n  },\n  \"faithfulness\": {\n    \"score\": &lt;integer 0–2&gt;,\n    \"justification\": \"&lt;string&gt;\"\n  },\n  \"clarity\": {\n    \"score\": &lt;integer 1–5&gt;,\n    \"justification\": \"&lt;string&gt;\"\n  },\n  \"conciseness\": {\n    \"score\": &lt;integer 0–2&gt;,\n    \"justification\": \"&lt;string&gt;\"\n  },\n\n  // OPTIONAL BLOCK: include only if critical errors or violations are present\n  \"errors_or_violations\": [\n    \"&lt;string&gt;\",\n    \"&lt;string&gt;\"\n  ]\n}\n\n# Additional Requirements\n\n* If no errors or violations are identified, omit the errors_or_violations field entirely.\n* If included, errors_or_violations must be a non-empty array of concise, concrete descriptions.\n* Each justification should be concise and evidence-based, typically 1–2 sentences.\n* Do not include placeholder text.\n* Do not include any keys not explicitly specified above.\n* Do not include any text outside the JSON object.\n\n# Tone and Constraints\n\n* Maintain a neutral, professional, and analytical tone.\n* Do not suggest improvements.\n* Do not include any content outside the required JSON structure.\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\"># Role Definition\n\nYou are an impartial, highly rigorous evaluator acting as a judge for assessing model-generated summaries of technical articles. \nYour role is to assess whether a given model-generated summary successfully fulfills the task of producing a concise, accurate synthesis of a technical article, capturing its main claims, key supporting points, and overall conclusion while remaining faithful to the original content. \nAll evaluations must be conducted strictly according to the specified evaluation criteria and scoring rubrics, with the expectation that the target summary length is 120–150 words.\n\n# Evaluation Criteria and Scales\n\n* Accuracy (binary yes/no)\n* Coverage (ordinal scale: 0–3)\n* Faithfulness (ternary ordinal scale: 0–2)\n* Clarity (Likert-type ordinal scale: 1–5)\n* Conciseness (ternary ordinal scale: 0–2)\n\n# Scoring Rubrics with Anchored Example Inputs\n\n## Accuracy (yes/no)\n\nWhether the summary contains any factual errors relative to the source.\n\n* yes: No factual errors are present.  \n  Example input that should be rated yes:  \n  “The paper evaluates a transformer-based model on three benchmarks and reports consistent performance improvements across all of them,” when the source article states exactly this.\n\n* no: One or more factual errors are present.  \n  Example input that should be rated no:  \n  “The authors conducted a large-scale human trial to validate the approach,” when the source explicitly states that no human experiments were performed.\n\n## Coverage (0–3)\n\nThe extent to which major points of the source are included.\n\n* 3: All major points are included.  \n  Example input that should be rated 3:  \n  A summary that describes the problem motivation, the proposed method, the experimental setup, the main results, and the stated limitations.\n\n* 2: Most major points are included with one minor omission.  \n  Example input that should be rated 2:  \n  A summary that explains the method and results but omits a short discussion of future work mentioned at the end of the article.\n\n* 1: Some major points are missing.  \n  Example input that should be rated 1:  \n  A summary that reports numerical results but does not explain what method or model produced them.\n\n* 0: The summary is largely incomplete.  \n  Example input that should be rated 0:  \n  A summary that only provides background context and never mentions the method, results, or conclusions.\n\n## Faithfulness (0–2)\n\nWhether the summary introduces unsupported information or interpretations.\n\n* 2: All statements are directly supported by the source.  \n  Example input that should be rated 2:  \n  A summary that paraphrases the article’s claims without adding interpretations or conclusions beyond what is stated.\n\n* 1: The summary contains a minor unsubstantiated inference.  \n  Example input that should be rated 1:  \n  A summary that claims the method is “likely to generalize to all domains,” when the paper only reports results in a limited setting.\n\n* 0: The summary is unfaithful.  \n  Example input that should be rated 0:  \n  A summary that introduces a recommendation, application, or claim that does not appear anywhere in the source article.\n\n## Clarity (Likert-type ordinal scale: 1–5)\n\nThe organization, readability, and coherence of the summary.\n\n* 5: Exceptionally clear and well-structured.  \n  Example input that should be rated 5:  \n  A summary with a clear logical flow from motivation to method to results, written in precise and unambiguous language.\n\n* 4: Mostly clear with minor issues.  \n  Example input that should be rated 4:  \n  A summary that is easy to understand overall but contains one awkward transition or slightly unclear sentence.\n\n* 3: Adequately clear but uneven.  \n  Example input that should be rated 3:  \n  A summary that is generally understandable but includes several vague phrases or mildly confusing sentences.\n\n* 2: Hard to follow.  \n  Example input that should be rated 2:  \n  A summary that jumps between ideas without clear transitions, making the structure difficult to follow.\n\n* 1: Unclear or incoherent.  \n  Example input that should be rated 1:  \n  A summary with disorganized sentences, unclear references, and no apparent structure.\n\n## Conciseness (0–2)\n\nAdherence to the target length and avoidance of unnecessary detail.\n\n* 2: Fully concise.  \n  Example input that should be rated 2:  \n  A summary within 120–150 words that avoids repetition and includes only essential information.\n\n* 1: Minor conciseness issues.  \n  Example input that should be rated 1:  \n  A summary that slightly exceeds the word limit or repeats one idea unnecessarily.\n\n* 0: Not concise.  \n  Example input that should be rated 0:  \n  A summary that is substantially longer than the target length or includes extensive irrelevant detail.\n\n# Model Outputs to Evaluate\n\nYou will be given a single model-generated summary of a source article. Evaluate this summary according to the evaluation criteria.\n\n# Evaluation Instructions\n\n1. Read the role definition, evaluation criteria, and scoring rubrics carefully.  \n2. For the given model-generated summary, assign a score for each criterion using the defined scales.  \n3. For each assigned score, provide a clear, evidence-based rationale explaining why the summary merits that score according to the rubric.  \n4. Judge only what is present in the summary.  \n5. Base decisions strictly on the rubric descriptions, using the example inputs only as anchors.  \n6. Apply scales consistently across all evaluated summaries.\n\n# Required Output Format (Strict JSON)\n\nFor each model-generated summary, output a single JSON object with the following structure and keys exactly as specified:\n\n{\n  \"accuracy\": {\n    \"score\": \"yes | no\",\n    \"justification\": \"&lt;string&gt;\"\n  },\n  \"coverage\": {\n    \"score\": &lt;integer 0–3&gt;,\n    \"justification\": \"&lt;string&gt;\"\n  },\n  \"faithfulness\": {\n    \"score\": &lt;integer 0–2&gt;,\n    \"justification\": \"&lt;string&gt;\"\n  },\n  \"clarity\": {\n    \"score\": &lt;integer 1–5&gt;,\n    \"justification\": \"&lt;string&gt;\"\n  },\n  \"conciseness\": {\n    \"score\": &lt;integer 0–2&gt;,\n    \"justification\": \"&lt;string&gt;\"\n  },\n\n  // OPTIONAL BLOCK: include only if critical errors or violations are present\n  \"errors_or_violations\": [\n    \"&lt;string&gt;\",\n    \"&lt;string&gt;\"\n  ]\n}\n\n# Additional Requirements\n\n* If no errors or violations are identified, omit the errors_or_violations field entirely.\n* If included, errors_or_violations must be a non-empty array of concise, concrete descriptions.\n* Each justification should be concise and evidence-based, typically 1–2 sentences.\n* Do not include placeholder text.\n* Do not include any keys not explicitly specified above.\n* Do not include any text outside the JSON object.\n\n# Tone and Constraints\n\n* Maintain a neutral, professional, and analytical tone.\n* Do not suggest improvements.\n* Do not include any content outside the required JSON structure.\n</code></pre>\n<h4 id=\"why-this-is-pointwise-ltr\">Why This is Pointwise LTR</h4>\n<ul>\n  <li>\n    <p>This prompt implements pointwise ranking because:</p>\n\n    <ul>\n      <li>Each output is scored independently</li>\n      <li>No cross-output comparisons are required</li>\n      <li>Scores can be aggregated or thresholded downstream</li>\n      <li>The judge function approximates <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-155\" style=\"width: 3.44em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.867em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.82em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-156\"><span class=\"mi\" id=\"MathJax-Span-157\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-158\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-159\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-160\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-161\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-162\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-163\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-164\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">f(q, x_i)</script> directly</li>\n    </ul>\n  </li>\n  <li>\n    <p>This formulation enables simple extensions such as:</p>\n\n    <ul>\n      <li>Ranking outputs by weighted sums of criteria</li>\n      <li>Filtering low-quality outputs</li>\n      <li>Training reward models via supervised regression</li>\n    </ul>\n  </li>\n  <li>\n    <p>In the next section, we move beyond prompting and examine <strong>fine-tuning encoder-only and encoder–decoder models for LTR</strong>, covering pointwise, pairwise, and listwise objectives with architectures, loss functions, and concrete input–output examples.</p>\n  </li>\n</ul>\n<p>This prompt implements pointwise ranking because:</p>\n<ul>\n      <li>Each output is scored independently</li>\n      <li>No cross-output comparisons are required</li>\n      <li>Scores can be aggregated or thresholded downstream</li>\n      <li>The judge function approximates <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-155\" style=\"width: 3.44em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.867em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.82em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-156\"><span class=\"mi\" id=\"MathJax-Span-157\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-158\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-159\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-160\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-161\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-162\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-163\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-164\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">f(q, x_i)</script> directly</li>\n    </ul>\n<p>This formulation enables simple extensions such as:</p>\n<ul>\n      <li>Ranking outputs by weighted sums of criteria</li>\n      <li>Filtering low-quality outputs</li>\n      <li>Training reward models via supervised regression</li>\n    </ul>\n<p>In the next section, we move beyond prompting and examine <strong>fine-tuning encoder-only and encoder–decoder models for LTR</strong>, covering pointwise, pairwise, and listwise objectives with architectures, loss functions, and concrete input–output examples.</p>",
    "contentMarkdown": "*   Below is an **example pointwise LLM-as-a-Judge prompt** for evaluating a summarization task, implementing the principles above. This prompt instantiates a mixed-scale, rubric-driven evaluator and corresponds directly to a pointwise LTR model that scores each output independently.\n*   Importantly, the prompt includes **anchored example inputs for each evaluation criterion and each rubric level**, which serve as calibration references for the judge. Multiple such examples can be provided per rubric level, enabling **few-shot learning** that improves scale consistency, reduces ambiguity between adjacent scores, and stabilizes judgments across different criteria.\n*   Example prompt for an LLM-as-Judge for evaluating a summarization task (mixed scales, JSON output):\n\n![](https://aman.ai/images/copy.png)\n\n`# Role Definition  You are an impartial, highly rigorous evaluator acting as a judge for assessing model-generated summaries of technical articles.  Your role is to assess whether a given model-generated summary successfully fulfills the task of producing a concise, accurate synthesis of a technical article, capturing its main claims, key supporting points, and overall conclusion while remaining faithful to the original content.  All evaluations must be conducted strictly according to the specified evaluation criteria and scoring rubrics, with the expectation that the target summary length is 120–150 words.  # Evaluation Criteria and Scales  * Accuracy (binary yes/no) * Coverage (ordinal scale: 0–3) * Faithfulness (ternary ordinal scale: 0–2) * Clarity (Likert-type ordinal scale: 1–5) * Conciseness (ternary ordinal scale: 0–2)  # Scoring Rubrics with Anchored Example Inputs  ## Accuracy (yes/no)  Whether the summary contains any factual errors relative to the source.  * yes: No factual errors are present.     Example input that should be rated yes:     “The paper evaluates a transformer-based model on three benchmarks and reports consistent performance improvements across all of them,” when the source article states exactly this.  * no: One or more factual errors are present.     Example input that should be rated no:     “The authors conducted a large-scale human trial to validate the approach,” when the source explicitly states that no human experiments were performed.  ## Coverage (0–3)  The extent to which major points of the source are included.  * 3: All major points are included.     Example input that should be rated 3:     A summary that describes the problem motivation, the proposed method, the experimental setup, the main results, and the stated limitations.  * 2: Most major points are included with one minor omission.     Example input that should be rated 2:     A summary that explains the method and results but omits a short discussion of future work mentioned at the end of the article.  * 1: Some major points are missing.     Example input that should be rated 1:     A summary that reports numerical results but does not explain what method or model produced them.  * 0: The summary is largely incomplete.     Example input that should be rated 0:     A summary that only provides background context and never mentions the method, results, or conclusions.  ## Faithfulness (0–2)  Whether the summary introduces unsupported information or interpretations.  * 2: All statements are directly supported by the source.     Example input that should be rated 2:     A summary that paraphrases the article’s claims without adding interpretations or conclusions beyond what is stated.  * 1: The summary contains a minor unsubstantiated inference.     Example input that should be rated 1:     A summary that claims the method is “likely to generalize to all domains,” when the paper only reports results in a limited setting.  * 0: The summary is unfaithful.     Example input that should be rated 0:     A summary that introduces a recommendation, application, or claim that does not appear anywhere in the source article.  ## Clarity (Likert-type ordinal scale: 1–5)  The organization, readability, and coherence of the summary.  * 5: Exceptionally clear and well-structured.     Example input that should be rated 5:     A summary with a clear logical flow from motivation to method to results, written in precise and unambiguous language.  * 4: Mostly clear with minor issues.     Example input that should be rated 4:     A summary that is easy to understand overall but contains one awkward transition or slightly unclear sentence.  * 3: Adequately clear but uneven.     Example input that should be rated 3:     A summary that is generally understandable but includes several vague phrases or mildly confusing sentences.  * 2: Hard to follow.     Example input that should be rated 2:     A summary that jumps between ideas without clear transitions, making the structure difficult to follow.  * 1: Unclear or incoherent.     Example input that should be rated 1:     A summary with disorganized sentences, unclear references, and no apparent structure.  ## Conciseness (0–2)  Adherence to the target length and avoidance of unnecessary detail.  * 2: Fully concise.     Example input that should be rated 2:     A summary within 120–150 words that avoids repetition and includes only essential information.  * 1: Minor conciseness issues.     Example input that should be rated 1:     A summary that slightly exceeds the word limit or repeats one idea unnecessarily.  * 0: Not concise.     Example input that should be rated 0:     A summary that is substantially longer than the target length or includes extensive irrelevant detail.  # Model Outputs to Evaluate  You will be given a single model-generated summary of a source article. Evaluate this summary according to the evaluation criteria.  # Evaluation Instructions  1. Read the role definition, evaluation criteria, and scoring rubrics carefully.   2. For the given model-generated summary, assign a score for each criterion using the defined scales.   3. For each assigned score, provide a clear, evidence-based rationale explaining why the summary merits that score according to the rubric.   4. Judge only what is present in the summary.   5. Base decisions strictly on the rubric descriptions, using the example inputs only as anchors.   6. Apply scales consistently across all evaluated summaries.  # Required Output Format (Strict JSON)  For each model-generated summary, output a single JSON object with the following structure and keys exactly as specified:  {   \"accuracy\": {     \"score\": \"yes | no\",     \"justification\": \"<string>\"   },   \"coverage\": {     \"score\": <integer 0–3>,     \"justification\": \"<string>\"   },   \"faithfulness\": {     \"score\": <integer 0–2>,     \"justification\": \"<string>\"   },   \"clarity\": {     \"score\": <integer 1–5>,     \"justification\": \"<string>\"   },   \"conciseness\": {     \"score\": <integer 0–2>,     \"justification\": \"<string>\"   },    // OPTIONAL BLOCK: include only if critical errors or violations are present   \"errors_or_violations\": [     \"<string>\",     \"<string>\"   ] }  # Additional Requirements  * If no errors or violations are identified, omit the errors_or_violations field entirely. * If included, errors_or_violations must be a non-empty array of concise, concrete descriptions. * Each justification should be concise and evidence-based, typically 1–2 sentences. * Do not include placeholder text. * Do not include any keys not explicitly specified above. * Do not include any text outside the JSON object.  # Tone and Constraints  * Maintain a neutral, professional, and analytical tone. * Do not suggest improvements. * Do not include any content outside the required JSON structure.`\n\n![](https://aman.ai/images/copy.png)\n\n`# Role Definition  You are an impartial, highly rigorous evaluator acting as a judge for assessing model-generated summaries of technical articles.  Your role is to assess whether a given model-generated summary successfully fulfills the task of producing a concise, accurate synthesis of a technical article, capturing its main claims, key supporting points, and overall conclusion while remaining faithful to the original content.  All evaluations must be conducted strictly according to the specified evaluation criteria and scoring rubrics, with the expectation that the target summary length is 120–150 words.  # Evaluation Criteria and Scales  * Accuracy (binary yes/no) * Coverage (ordinal scale: 0–3) * Faithfulness (ternary ordinal scale: 0–2) * Clarity (Likert-type ordinal scale: 1–5) * Conciseness (ternary ordinal scale: 0–2)  # Scoring Rubrics with Anchored Example Inputs  ## Accuracy (yes/no)  Whether the summary contains any factual errors relative to the source.  * yes: No factual errors are present.     Example input that should be rated yes:     “The paper evaluates a transformer-based model on three benchmarks and reports consistent performance improvements across all of them,” when the source article states exactly this.  * no: One or more factual errors are present.     Example input that should be rated no:     “The authors conducted a large-scale human trial to validate the approach,” when the source explicitly states that no human experiments were performed.  ## Coverage (0–3)  The extent to which major points of the source are included.  * 3: All major points are included.     Example input that should be rated 3:     A summary that describes the problem motivation, the proposed method, the experimental setup, the main results, and the stated limitations.  * 2: Most major points are included with one minor omission.     Example input that should be rated 2:     A summary that explains the method and results but omits a short discussion of future work mentioned at the end of the article.  * 1: Some major points are missing.     Example input that should be rated 1:     A summary that reports numerical results but does not explain what method or model produced them.  * 0: The summary is largely incomplete.     Example input that should be rated 0:     A summary that only provides background context and never mentions the method, results, or conclusions.  ## Faithfulness (0–2)  Whether the summary introduces unsupported information or interpretations.  * 2: All statements are directly supported by the source.     Example input that should be rated 2:     A summary that paraphrases the article’s claims without adding interpretations or conclusions beyond what is stated.  * 1: The summary contains a minor unsubstantiated inference.     Example input that should be rated 1:     A summary that claims the method is “likely to generalize to all domains,” when the paper only reports results in a limited setting.  * 0: The summary is unfaithful.     Example input that should be rated 0:     A summary that introduces a recommendation, application, or claim that does not appear anywhere in the source article.  ## Clarity (Likert-type ordinal scale: 1–5)  The organization, readability, and coherence of the summary.  * 5: Exceptionally clear and well-structured.     Example input that should be rated 5:     A summary with a clear logical flow from motivation to method to results, written in precise and unambiguous language.  * 4: Mostly clear with minor issues.     Example input that should be rated 4:     A summary that is easy to understand overall but contains one awkward transition or slightly unclear sentence.  * 3: Adequately clear but uneven.     Example input that should be rated 3:     A summary that is generally understandable but includes several vague phrases or mildly confusing sentences.  * 2: Hard to follow.     Example input that should be rated 2:     A summary that jumps between ideas without clear transitions, making the structure difficult to follow.  * 1: Unclear or incoherent.     Example input that should be rated 1:     A summary with disorganized sentences, unclear references, and no apparent structure.  ## Conciseness (0–2)  Adherence to the target length and avoidance of unnecessary detail.  * 2: Fully concise.     Example input that should be rated 2:     A summary within 120–150 words that avoids repetition and includes only essential information.  * 1: Minor conciseness issues.     Example input that should be rated 1:     A summary that slightly exceeds the word limit or repeats one idea unnecessarily.  * 0: Not concise.     Example input that should be rated 0:     A summary that is substantially longer than the target length or includes extensive irrelevant detail.  # Model Outputs to Evaluate  You will be given a single model-generated summary of a source article. Evaluate this summary according to the evaluation criteria.  # Evaluation Instructions  1. Read the role definition, evaluation criteria, and scoring rubrics carefully.   2. For the given model-generated summary, assign a score for each criterion using the defined scales.   3. For each assigned score, provide a clear, evidence-based rationale explaining why the summary merits that score according to the rubric.   4. Judge only what is present in the summary.   5. Base decisions strictly on the rubric descriptions, using the example inputs only as anchors.   6. Apply scales consistently across all evaluated summaries.  # Required Output Format (Strict JSON)  For each model-generated summary, output a single JSON object with the following structure and keys exactly as specified:  {   \"accuracy\": {     \"score\": \"yes | no\",     \"justification\": \"<string>\"   },   \"coverage\": {     \"score\": <integer 0–3>,     \"justification\": \"<string>\"   },   \"faithfulness\": {     \"score\": <integer 0–2>,     \"justification\": \"<string>\"   },   \"clarity\": {     \"score\": <integer 1–5>,     \"justification\": \"<string>\"   },   \"conciseness\": {     \"score\": <integer 0–2>,     \"justification\": \"<string>\"   },    // OPTIONAL BLOCK: include only if critical errors or violations are present   \"errors_or_violations\": [     \"<string>\",     \"<string>\"   ] }  # Additional Requirements  * If no errors or violations are identified, omit the errors_or_violations field entirely. * If included, errors_or_violations must be a non-empty array of concise, concrete descriptions. * Each justification should be concise and evidence-based, typically 1–2 sentences. * Do not include placeholder text. * Do not include any keys not explicitly specified above. * Do not include any text outside the JSON object.  # Tone and Constraints  * Maintain a neutral, professional, and analytical tone. * Do not suggest improvements. * Do not include any content outside the required JSON structure.`\n\n#### Why This is Pointwise LTR\n\n*   This prompt implements pointwise ranking because:\n    \n    *   Each output is scored independently\n    *   No cross-output comparisons are required\n    *   Scores can be aggregated or thresholded downstream\n    *   The judge function approximates f(q,xi)f(q,xi)f(q, x\\_i) directly\n*   This formulation enables simple extensions such as:\n    \n    *   Ranking outputs by weighted sums of criteria\n    *   Filtering low-quality outputs\n    *   Training reward models via supervised regression\n*   In the next section, we move beyond prompting and examine **fine-tuning encoder-only and encoder–decoder models for LTR**, covering pointwise, pairwise, and listwise objectives with architectures, loss functions, and concrete input–output examples.\n    \n\nThis prompt implements pointwise ranking because:\n\n*   Each output is scored independently\n*   No cross-output comparisons are required\n*   Scores can be aggregated or thresholded downstream\n*   The judge function approximates f(q,xi)f(q,xi)f(q, x\\_i) directly\n\nThis formulation enables simple extensions such as:\n\n*   Ranking outputs by weighted sums of criteria\n*   Filtering low-quality outputs\n*   Training reward models via supervised regression\n\nIn the next section, we move beyond prompting and examine **fine-tuning encoder-only and encoder–decoder models for LTR**, covering pointwise, pairwise, and listwise objectives with architectures, loss functions, and concrete input–output examples.",
    "contentLength": 22443,
    "wordCount": 2314,
    "hasCode": true,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#example-pointwise-llm-as-a-judge-prompt"
  },
  {
    "id": "ai-LLM-as-a-judge-why-fine-tune-models-for-ltr-13",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Fine-Tuning Encoder, Encoder–Decoder, and Decoder-Only Models for LTR",
    "title": "Why Fine-Tune Models for LTR?",
    "order": 13,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Fine-tuning ranking models is motivated by fundamental limitations of purely prompt-based or heuristic evaluation approaches.</li>\n</ul>\n<h4 id=\"scalability-and-latency-constraints\">Scalability and Latency Constraints</h4>\n<ul>\n  <li>\n    <p>Prompt-based LLM judges are expensive at inference time and do not scale well to scenarios involving:</p>\n\n    <ul>\n      <li>Millions of query–candidate comparisons</li>\n      <li>Low-latency retrieval and reranking pipelines</li>\n      <li>Online serving with strict SLA requirements</li>\n    </ul>\n  </li>\n  <li>\n    <p>In contrast, fine-tuned rankers—especially encoder-only models—can score thousands of candidates per second on commodity hardware. This trade-off is extensively discussed in <a href=\"https://arxiv.org/abs/1707.08690\">Neural Information Retrieval: A Literature Review</a> by Mitra et al. (2018).</p>\n  </li>\n</ul>\n<p>Prompt-based LLM judges are expensive at inference time and do not scale well to scenarios involving:</p>\n<ul>\n      <li>Millions of query–candidate comparisons</li>\n      <li>Low-latency retrieval and reranking pipelines</li>\n      <li>Online serving with strict SLA requirements</li>\n    </ul>\n<p>In contrast, fine-tuned rankers—especially encoder-only models—can score thousands of candidates per second on commodity hardware. This trade-off is extensively discussed in <a href=\"https://arxiv.org/abs/1707.08690\">Neural Information Retrieval: A Literature Review</a> by Mitra et al. (2018).</p>\n<h4 id=\"direct-optimization-of-ranking-metrics\">Direct Optimization of Ranking Metrics</h4>\n<ul>\n  <li>\n    <p>Traditional prompt-based evaluation yields <strong>scores or preferences</strong>, but does not directly optimize ranking metrics such as NDCG, MAP, or MRR.</p>\n  </li>\n  <li>\n    <p>Fine-tuned LTR models enable:</p>\n\n    <ul>\n      <li>Approximate or direct optimization of ranking metrics</li>\n      <li>Use of differentiable surrogate losses</li>\n      <li>Consistent behavior across training and inference</li>\n    </ul>\n  </li>\n  <li>\n    <p>The importance of metric-aware optimization was established in early LTR work such as <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2005-82.pdf\">Learning to Rank Using Gradient Descent</a> by Burges et al. (2005).</p>\n  </li>\n</ul>\n<p>Traditional prompt-based evaluation yields <strong>scores or preferences</strong>, but does not directly optimize ranking metrics such as NDCG, MAP, or MRR.</p>\n<p>Fine-tuned LTR models enable:</p>\n<ul>\n      <li>Approximate or direct optimization of ranking metrics</li>\n      <li>Use of differentiable surrogate losses</li>\n      <li>Consistent behavior across training and inference</li>\n    </ul>\n<p>The importance of metric-aware optimization was established in early LTR work such as <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2005-82.pdf\">Learning to Rank Using Gradient Descent</a> by Burges et al. (2005).</p>\n<h4 id=\"stability-calibration-and-reproducibility\">Stability, Calibration, and Reproducibility</h4>\n<ul>\n  <li>\n    <p>Prompt-based LLM judges can exhibit:</p>\n\n    <ul>\n      <li>Sensitivity to prompt wording</li>\n      <li>Drift across model versions</li>\n      <li>Stochastic variance in outputs</li>\n    </ul>\n  </li>\n  <li>\n    <p>Fine-tuned ranking models provide:</p>\n\n    <ul>\n      <li>Fixed decision boundaries</li>\n      <li>Calibrated score distributions</li>\n      <li>Reproducible evaluation results</li>\n    </ul>\n  </li>\n  <li>\n    <p>This is critical for production evaluation pipelines and benchmarking, as noted in <a href=\"https://arxiv.org/abs/2307.03109\">How to Evaluate Language Models: A Survey</a> by Chang et al. (2023).</p>\n  </li>\n</ul>\n<p>Prompt-based LLM judges can exhibit:</p>\n<ul>\n      <li>Sensitivity to prompt wording</li>\n      <li>Drift across model versions</li>\n      <li>Stochastic variance in outputs</li>\n    </ul>\n<p>Fine-tuned ranking models provide:</p>\n<ul>\n      <li>Fixed decision boundaries</li>\n      <li>Calibrated score distributions</li>\n      <li>Reproducible evaluation results</li>\n    </ul>\n<p>This is critical for production evaluation pipelines and benchmarking, as noted in <a href=\"https://arxiv.org/abs/2307.03109\">How to Evaluate Language Models: A Survey</a> by Chang et al. (2023).</p>\n<h4 id=\"integration-into-retrieval-and-rag-pipelines\">Integration Into Retrieval and RAG Pipelines</h4>\n<ul>\n  <li>\n    <p>Ranking models are core components of:</p>\n\n    <ul>\n      <li>Multi-stage retrieval pipelines</li>\n      <li>Reranking for dense and hybrid search</li>\n      <li>Retrieval-augmented generation (RAG)</li>\n    </ul>\n  </li>\n  <li>\n    <p>Fine-tuned rankers can be tightly integrated into these systems, unlike prompt-based judges that operate externally. Practical examples are detailed in <a href=\"https://arxiv.org/abs/2004.04906\">Dense Passage Retrieval for Open-Domain Question Answering</a> by Karpukhin et al. (2020) and <a href=\"https://arxiv.org/abs/2005.11401\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a> by Lewis et al. (2020).</p>\n  </li>\n</ul>\n<p>Ranking models are core components of:</p>\n<ul>\n      <li>Multi-stage retrieval pipelines</li>\n      <li>Reranking for dense and hybrid search</li>\n      <li>Retrieval-augmented generation (RAG)</li>\n    </ul>\n<p>Fine-tuned rankers can be tightly integrated into these systems, unlike prompt-based judges that operate externally. Practical examples are detailed in <a href=\"https://arxiv.org/abs/2004.04906\">Dense Passage Retrieval for Open-Domain Question Answering</a> by Karpukhin et al. (2020) and <a href=\"https://arxiv.org/abs/2005.11401\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a> by Lewis et al. (2020).</p>",
    "contentMarkdown": "*   Fine-tuning ranking models is motivated by fundamental limitations of purely prompt-based or heuristic evaluation approaches.\n\n#### Scalability and Latency Constraints\n\n*   Prompt-based LLM judges are expensive at inference time and do not scale well to scenarios involving:\n    \n    *   Millions of query–candidate comparisons\n    *   Low-latency retrieval and reranking pipelines\n    *   Online serving with strict SLA requirements\n*   In contrast, fine-tuned rankers—especially encoder-only models—can score thousands of candidates per second on commodity hardware. This trade-off is extensively discussed in [Neural Information Retrieval: A Literature Review](https://arxiv.org/abs/1707.08690) by Mitra et al. (2018).\n    \n\nPrompt-based LLM judges are expensive at inference time and do not scale well to scenarios involving:\n\n*   Millions of query–candidate comparisons\n*   Low-latency retrieval and reranking pipelines\n*   Online serving with strict SLA requirements\n\nIn contrast, fine-tuned rankers—especially encoder-only models—can score thousands of candidates per second on commodity hardware. This trade-off is extensively discussed in [Neural Information Retrieval: A Literature Review](https://arxiv.org/abs/1707.08690) by Mitra et al. (2018).\n\n#### Direct Optimization of Ranking Metrics\n\n*   Traditional prompt-based evaluation yields **scores or preferences**, but does not directly optimize ranking metrics such as NDCG, MAP, or MRR.\n    \n*   Fine-tuned LTR models enable:\n    \n    *   Approximate or direct optimization of ranking metrics\n    *   Use of differentiable surrogate losses\n    *   Consistent behavior across training and inference\n*   The importance of metric-aware optimization was established in early LTR work such as [Learning to Rank Using Gradient Descent](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2005-82.pdf) by Burges et al. (2005).\n    \n\nTraditional prompt-based evaluation yields **scores or preferences**, but does not directly optimize ranking metrics such as NDCG, MAP, or MRR.\n\nFine-tuned LTR models enable:\n\n*   Approximate or direct optimization of ranking metrics\n*   Use of differentiable surrogate losses\n*   Consistent behavior across training and inference\n\nThe importance of metric-aware optimization was established in early LTR work such as [Learning to Rank Using Gradient Descent](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2005-82.pdf) by Burges et al. (2005).\n\n#### Stability, Calibration, and Reproducibility\n\n*   Prompt-based LLM judges can exhibit:\n    \n    *   Sensitivity to prompt wording\n    *   Drift across model versions\n    *   Stochastic variance in outputs\n*   Fine-tuned ranking models provide:\n    \n    *   Fixed decision boundaries\n    *   Calibrated score distributions\n    *   Reproducible evaluation results\n*   This is critical for production evaluation pipelines and benchmarking, as noted in [How to Evaluate Language Models: A Survey](https://arxiv.org/abs/2307.03109) by Chang et al. (2023).\n    \n\nPrompt-based LLM judges can exhibit:\n\n*   Sensitivity to prompt wording\n*   Drift across model versions\n*   Stochastic variance in outputs\n\nFine-tuned ranking models provide:\n\n*   Fixed decision boundaries\n*   Calibrated score distributions\n*   Reproducible evaluation results\n\nThis is critical for production evaluation pipelines and benchmarking, as noted in [How to Evaluate Language Models: A Survey](https://arxiv.org/abs/2307.03109) by Chang et al. (2023).\n\n#### Integration Into Retrieval and RAG Pipelines\n\n*   Ranking models are core components of:\n    \n    *   Multi-stage retrieval pipelines\n    *   Reranking for dense and hybrid search\n    *   Retrieval-augmented generation (RAG)\n*   Fine-tuned rankers can be tightly integrated into these systems, unlike prompt-based judges that operate externally. Practical examples are detailed in [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by Karpukhin et al. (2020) and [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Lewis et al. (2020).\n    \n\nRanking models are core components of:\n\n*   Multi-stage retrieval pipelines\n*   Reranking for dense and hybrid search\n*   Retrieval-augmented generation (RAG)\n\nFine-tuned rankers can be tightly integrated into these systems, unlike prompt-based judges that operate externally. Practical examples are detailed in [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by Karpukhin et al. (2020) and [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Lewis et al. (2020).",
    "contentLength": 5780,
    "wordCount": 569,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#why-fine-tune-models-for-ltr?"
  },
  {
    "id": "ai-LLM-as-a-judge-how-ltr-fine-tuning-works-a-unified-view-14",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Fine-Tuning Encoder, Encoder–Decoder, and Decoder-Only Models for LTR",
    "title": "How LTR Fine-Tuning Works: a Unified View",
    "order": 14,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>Across architectures, LTR fine-tuning follows a common conceptual pattern:</p>\n\n    <ol>\n      <li>\n        <p><strong>Define a query</strong>: The query may be a user query, a task description, or an evaluation prompt.</p>\n      </li>\n      <li>\n        <p><strong>Define candidates</strong>: Candidates may be documents, passages, model-generated outputs, or tool responses.</p>\n      </li>\n      <li>\n        <p><strong>Define a relevance signal</strong>: Relevance may come from:</p>\n\n        <ul>\n          <li>Human annotations</li>\n          <li>LLM-as-a-Judge outputs</li>\n          <li>Implicit feedback (clicks, dwell time)</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Optimize a ranking objective</strong>: Using pointwise, pairwise, or listwise loss functions.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p>This abstraction allows LTR techniques to generalize across search, evaluation, and generation tasks.</p>\n  </li>\n</ul>\n<p>Across architectures, LTR fine-tuning follows a common conceptual pattern:</p>\n<ol>\n      <li>\n        <p><strong>Define a query</strong>: The query may be a user query, a task description, or an evaluation prompt.</p>\n      </li>\n      <li>\n        <p><strong>Define candidates</strong>: Candidates may be documents, passages, model-generated outputs, or tool responses.</p>\n      </li>\n      <li>\n        <p><strong>Define a relevance signal</strong>: Relevance may come from:</p>\n\n        <ul>\n          <li>Human annotations</li>\n          <li>LLM-as-a-Judge outputs</li>\n          <li>Implicit feedback (clicks, dwell time)</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Optimize a ranking objective</strong>: Using pointwise, pairwise, or listwise loss functions.</p>\n      </li>\n    </ol>\n<p><strong>Define a query</strong>: The query may be a user query, a task description, or an evaluation prompt.</p>\n<p><strong>Define candidates</strong>: Candidates may be documents, passages, model-generated outputs, or tool responses.</p>\n<p><strong>Define a relevance signal</strong>: Relevance may come from:</p>\n<ul>\n          <li>Human annotations</li>\n          <li>LLM-as-a-Judge outputs</li>\n          <li>Implicit feedback (clicks, dwell time)</li>\n        </ul>\n<p><strong>Optimize a ranking objective</strong>: Using pointwise, pairwise, or listwise loss functions.</p>\n<p>This abstraction allows LTR techniques to generalize across search, evaluation, and generation tasks.</p>",
    "contentMarkdown": "*   Across architectures, LTR fine-tuning follows a common conceptual pattern:\n    \n    1.  **Define a query**: The query may be a user query, a task description, or an evaluation prompt.\n        \n    2.  **Define candidates**: Candidates may be documents, passages, model-generated outputs, or tool responses.\n        \n    3.  **Define a relevance signal**: Relevance may come from:\n        \n        *   Human annotations\n        *   LLM-as-a-Judge outputs\n        *   Implicit feedback (clicks, dwell time)\n    4.  **Optimize a ranking objective**: Using pointwise, pairwise, or listwise loss functions.\n        \n*   This abstraction allows LTR techniques to generalize across search, evaluation, and generation tasks.\n    \n\nAcross architectures, LTR fine-tuning follows a common conceptual pattern:\n\n1.  **Define a query**: The query may be a user query, a task description, or an evaluation prompt.\n    \n2.  **Define candidates**: Candidates may be documents, passages, model-generated outputs, or tool responses.\n    \n3.  **Define a relevance signal**: Relevance may come from:\n    \n    *   Human annotations\n    *   LLM-as-a-Judge outputs\n    *   Implicit feedback (clicks, dwell time)\n4.  **Optimize a ranking objective**: Using pointwise, pairwise, or listwise loss functions.\n    \n\n**Define a query**: The query may be a user query, a task description, or an evaluation prompt.\n\n**Define candidates**: Candidates may be documents, passages, model-generated outputs, or tool responses.\n\n**Define a relevance signal**: Relevance may come from:\n\n*   Human annotations\n*   LLM-as-a-Judge outputs\n*   Implicit feedback (clicks, dwell time)\n\n**Optimize a ranking objective**: Using pointwise, pairwise, or listwise loss functions.\n\nThis abstraction allows LTR techniques to generalize across search, evaluation, and generation tasks.",
    "contentLength": 2480,
    "wordCount": 234,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#how-ltr-fine-tuning-works:-a-unified-view"
  },
  {
    "id": "ai-LLM-as-a-judge-architectural-choices-and-their-implications-15",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Fine-Tuning Encoder, Encoder–Decoder, and Decoder-Only Models for LTR",
    "title": "Architectural Choices and Their Implications",
    "order": 15,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>Different transformer architectures support LTR in different ways.</li>\n</ul>\n<h4 id=\"encoder-only-models\">Encoder-Only Models</h4>\n<ul>\n  <li>\n    <p>Encoder-only models (e.g., BERT-style transformers) map inputs to contextual embeddings and produce scalar scores.</p>\n\n    <ul>\n      <li>Best suited for <strong>pointwise and pairwise ranking</strong></li>\n      <li>High precision due to joint encoding</li>\n      <li>Computationally expensive for large candidate sets</li>\n    </ul>\n  </li>\n  <li>\n    <p>These models dominate <strong>late-stage reranking</strong>, as exemplified by monoBERT and duoBERT (<a href=\"https://arxiv.org/abs/1910.14424\">Multi-Stage Document Ranking with BERT</a> by Nogueira et al. (2019)).</p>\n  </li>\n</ul>\n<p>Encoder-only models (e.g., BERT-style transformers) map inputs to contextual embeddings and produce scalar scores.</p>\n<ul>\n      <li>Best suited for <strong>pointwise and pairwise ranking</strong></li>\n      <li>High precision due to joint encoding</li>\n      <li>Computationally expensive for large candidate sets</li>\n    </ul>\n<p>These models dominate <strong>late-stage reranking</strong>, as exemplified by monoBERT and duoBERT (<a href=\"https://arxiv.org/abs/1910.14424\">Multi-Stage Document Ranking with BERT</a> by Nogueira et al. (2019)).</p>\n<h4 id=\"encoderdecoder-models\">Encoder–Decoder Models</h4>\n<ul>\n  <li>\n    <p>Encoder–decoder models enable <strong>list-level reasoning</strong> by separating representation and aggregation.</p>\n\n    <ul>\n      <li>Encoder processes each candidate independently</li>\n      <li>Decoder jointly attends over all candidates</li>\n      <li>Supports listwise objectives naturally</li>\n    </ul>\n  </li>\n  <li>\n    <p>This design underpins architectures such as FiD (<a href=\"https://arxiv.org/abs/2007.01282\">Fusion-in-Decoder for Open-Domain Question Answering</a> by Izacard et al. (2021)) and ListT5 (<a href=\"https://arxiv.org/abs/2402.15838\">ListT5: Listwise Re-ranking with Fusion-in-Decoder Improves Zero-shot Retrieval</a> by Yoon et al. (2024)).</p>\n  </li>\n</ul>\n<p>Encoder–decoder models enable <strong>list-level reasoning</strong> by separating representation and aggregation.</p>\n<ul>\n      <li>Encoder processes each candidate independently</li>\n      <li>Decoder jointly attends over all candidates</li>\n      <li>Supports listwise objectives naturally</li>\n    </ul>\n<p>This design underpins architectures such as FiD (<a href=\"https://arxiv.org/abs/2007.01282\">Fusion-in-Decoder for Open-Domain Question Answering</a> by Izacard et al. (2021)) and ListT5 (<a href=\"https://arxiv.org/abs/2402.15838\">ListT5: Listwise Re-ranking with Fusion-in-Decoder Improves Zero-shot Retrieval</a> by Yoon et al. (2024)).</p>\n<h4 id=\"decoder-only-models\">Decoder-Only Models</h4>\n<ul>\n  <li>\n    <p>Decoder-only models perform ranking via <strong>generation or classification over text</strong>.</p>\n\n    <ul>\n      <li>Can be prompted for pointwise, pairwise, or listwise judgments</li>\n      <li>Support reasoning-intensive ranking</li>\n      <li>Often used without fine-tuning, but can also be trained</li>\n    </ul>\n  </li>\n  <li>\n    <p>Prompt-based ranking with decoder-only models is explored in <a href=\"https://arxiv.org/abs/2304.09542\">RankGPT: A Prompt-Based Pairwise Ranking Framework for LLMs</a> by Sun et al. (2023) and large-scale evaluation studies such as <a href=\"https://arxiv.org/abs/2306.05685\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> by Zheng et al. (2023).</p>\n  </li>\n</ul>\n<p>Decoder-only models perform ranking via <strong>generation or classification over text</strong>.</p>\n<ul>\n      <li>Can be prompted for pointwise, pairwise, or listwise judgments</li>\n      <li>Support reasoning-intensive ranking</li>\n      <li>Often used without fine-tuning, but can also be trained</li>\n    </ul>\n<p>Prompt-based ranking with decoder-only models is explored in <a href=\"https://arxiv.org/abs/2304.09542\">RankGPT: A Prompt-Based Pairwise Ranking Framework for LLMs</a> by Sun et al. (2023) and large-scale evaluation studies such as <a href=\"https://arxiv.org/abs/2306.05685\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> by Zheng et al. (2023).</p>",
    "contentMarkdown": "*   Different transformer architectures support LTR in different ways.\n\n#### Encoder-Only Models\n\n*   Encoder-only models (e.g., BERT-style transformers) map inputs to contextual embeddings and produce scalar scores.\n    \n    *   Best suited for **pointwise and pairwise ranking**\n    *   High precision due to joint encoding\n    *   Computationally expensive for large candidate sets\n*   These models dominate **late-stage reranking**, as exemplified by monoBERT and duoBERT ([Multi-Stage Document Ranking with BERT](https://arxiv.org/abs/1910.14424) by Nogueira et al. (2019)).\n    \n\nEncoder-only models (e.g., BERT-style transformers) map inputs to contextual embeddings and produce scalar scores.\n\n*   Best suited for **pointwise and pairwise ranking**\n*   High precision due to joint encoding\n*   Computationally expensive for large candidate sets\n\nThese models dominate **late-stage reranking**, as exemplified by monoBERT and duoBERT ([Multi-Stage Document Ranking with BERT](https://arxiv.org/abs/1910.14424) by Nogueira et al. (2019)).\n\n#### Encoder–Decoder Models\n\n*   Encoder–decoder models enable **list-level reasoning** by separating representation and aggregation.\n    \n    *   Encoder processes each candidate independently\n    *   Decoder jointly attends over all candidates\n    *   Supports listwise objectives naturally\n*   This design underpins architectures such as FiD ([Fusion-in-Decoder for Open-Domain Question Answering](https://arxiv.org/abs/2007.01282) by Izacard et al. (2021)) and ListT5 ([ListT5: Listwise Re-ranking with Fusion-in-Decoder Improves Zero-shot Retrieval](https://arxiv.org/abs/2402.15838) by Yoon et al. (2024)).\n    \n\nEncoder–decoder models enable **list-level reasoning** by separating representation and aggregation.\n\n*   Encoder processes each candidate independently\n*   Decoder jointly attends over all candidates\n*   Supports listwise objectives naturally\n\nThis design underpins architectures such as FiD ([Fusion-in-Decoder for Open-Domain Question Answering](https://arxiv.org/abs/2007.01282) by Izacard et al. (2021)) and ListT5 ([ListT5: Listwise Re-ranking with Fusion-in-Decoder Improves Zero-shot Retrieval](https://arxiv.org/abs/2402.15838) by Yoon et al. (2024)).\n\n#### Decoder-Only Models\n\n*   Decoder-only models perform ranking via **generation or classification over text**.\n    \n    *   Can be prompted for pointwise, pairwise, or listwise judgments\n    *   Support reasoning-intensive ranking\n    *   Often used without fine-tuning, but can also be trained\n*   Prompt-based ranking with decoder-only models is explored in [RankGPT: A Prompt-Based Pairwise Ranking Framework for LLMs](https://arxiv.org/abs/2304.09542) by Sun et al. (2023) and large-scale evaluation studies such as [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) by Zheng et al. (2023).\n    \n\nDecoder-only models perform ranking via **generation or classification over text**.\n\n*   Can be prompted for pointwise, pairwise, or listwise judgments\n*   Support reasoning-intensive ranking\n*   Often used without fine-tuning, but can also be trained\n\nPrompt-based ranking with decoder-only models is explored in [RankGPT: A Prompt-Based Pairwise Ranking Framework for LLMs](https://arxiv.org/abs/2304.09542) by Sun et al. (2023) and large-scale evaluation studies such as [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) by Zheng et al. (2023).",
    "contentLength": 4209,
    "wordCount": 404,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#architectural-choices-and-their-implications"
  },
  {
    "id": "ai-LLM-as-a-judge-the-role-of-llm-as-a-judge-in-ltr-fine-tuning-16",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Fine-Tuning Encoder, Encoder–Decoder, and Decoder-Only Models for LTR",
    "title": "The Role of LLM-as-a-Judge in LTR Fine-Tuning",
    "order": 16,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>\n    <p>LLM-as-a-Judge acts as a <strong>scalable supervision source</strong> across architectures:</p>\n\n    <ul>\n      <li>Pointwise scores <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-165\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-166\"><span class=\"mo\" id=\"MathJax-Span-167\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">\\rightarrow</script> regression targets</li>\n      <li>Pairwise preferences <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-168\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-169\"><span class=\"mo\" id=\"MathJax-Span-170\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">\\rightarrow</script> ranking constraints</li>\n      <li>Listwise rankings <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-171\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-172\"><span class=\"mo\" id=\"MathJax-Span-173\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">\\rightarrow</script> permutation supervision</li>\n    </ul>\n  </li>\n  <li>\n    <p>This enables <strong>weakly supervised LTR</strong>, reducing reliance on expensive human labels, as demonstrated in <a href=\"https://arxiv.org/abs/2302.08582\">Training Language Models from AI Feedback</a> by Bai et al. (2023).</p>\n  </li>\n</ul>\n<p>LLM-as-a-Judge acts as a <strong>scalable supervision source</strong> across architectures:</p>\n<ul>\n      <li>Pointwise scores <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-165\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-166\"><span class=\"mo\" id=\"MathJax-Span-167\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">\\rightarrow</script> regression targets</li>\n      <li>Pairwise preferences <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-168\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-169\"><span class=\"mo\" id=\"MathJax-Span-170\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">\\rightarrow</script> ranking constraints</li>\n      <li>Listwise rankings <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-171\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-172\"><span class=\"mo\" id=\"MathJax-Span-173\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">\\rightarrow</script> permutation supervision</li>\n    </ul>\n<p>This enables <strong>weakly supervised LTR</strong>, reducing reliance on expensive human labels, as demonstrated in <a href=\"https://arxiv.org/abs/2302.08582\">Training Language Models from AI Feedback</a> by Bai et al. (2023).</p>",
    "contentMarkdown": "*   LLM-as-a-Judge acts as a **scalable supervision source** across architectures:\n    \n    *   Pointwise scores →→\\\\rightarrow regression targets\n    *   Pairwise preferences →→\\\\rightarrow ranking constraints\n    *   Listwise rankings →→\\\\rightarrow permutation supervision\n*   This enables **weakly supervised LTR**, reducing reliance on expensive human labels, as demonstrated in [Training Language Models from AI Feedback](https://arxiv.org/abs/2302.08582) by Bai et al. (2023).\n    \n\nLLM-as-a-Judge acts as a **scalable supervision source** across architectures:\n\n*   Pointwise scores →→\\\\rightarrow regression targets\n*   Pairwise preferences →→\\\\rightarrow ranking constraints\n*   Listwise rankings →→\\\\rightarrow permutation supervision\n\nThis enables **weakly supervised LTR**, reducing reliance on expensive human labels, as demonstrated in [Training Language Models from AI Feedback](https://arxiv.org/abs/2302.08582) by Bai et al. (2023).",
    "contentLength": 8549,
    "wordCount": 106,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#the-role-of-llm-as-a-judge-in-ltr-fine-tuning"
  },
  {
    "id": "ai-LLM-as-a-judge-encoder-only-models-for-ltr-17",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Fine-Tuning Encoder, Encoder–Decoder, and Decoder-Only Models for LTR",
    "title": "Encoder-Only Models for LTR",
    "order": 17,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>\n    <p>Encoder-only transformer models form the backbone of most <strong>high-precision neural ranking systems</strong> used today. These models take structured inputs (queries, documents, or generated outputs) and map them to contextual representations that are then converted into relevance scores. Their strength lies in <strong>token-level interaction modeling</strong>, which makes them especially effective for reranking small candidate sets.</p>\n  </li>\n  <li>\n    <p>Canonical encoder-only architectures are based on transformers such as <a href=\"https://arxiv.org/abs/1810.04805\">BERT</a> by Devlin et al. (2018), <a href=\"https://arxiv.org/abs/1907.11692\">RoBERTa</a> by Liu et al. (2019), and <a href=\"https://arxiv.org/abs/2003.10555\">ELECTRA</a> by Clark et al. (2020).</p>\n  </li>\n  <li>\n    <p>Encoder-only rankers are most commonly implemented as <strong>cross-encoders</strong>, where the query and candidate are jointly encoded, enabling full self-attention across tokens.</p>\n  </li>\n</ul>\n<p>Encoder-only transformer models form the backbone of most <strong>high-precision neural ranking systems</strong> used today. These models take structured inputs (queries, documents, or generated outputs) and map them to contextual representations that are then converted into relevance scores. Their strength lies in <strong>token-level interaction modeling</strong>, which makes them especially effective for reranking small candidate sets.</p>\n<p>Canonical encoder-only architectures are based on transformers such as <a href=\"https://arxiv.org/abs/1810.04805\">BERT</a> by Devlin et al. (2018), <a href=\"https://arxiv.org/abs/1907.11692\">RoBERTa</a> by Liu et al. (2019), and <a href=\"https://arxiv.org/abs/2003.10555\">ELECTRA</a> by Clark et al. (2020).</p>\n<p>Encoder-only rankers are most commonly implemented as <strong>cross-encoders</strong>, where the query and candidate are jointly encoded, enabling full self-attention across tokens.</p>\n<h4 id=\"pointwise-encoder-only-ranking\">Pointwise Encoder-Only Ranking</h4>\n<h5 id=\"architecture\">Architecture</h5>\n<ul>\n  <li>\n    <p>In pointwise ranking, the model independently scores each query–candidate pair.</p>\n  </li>\n  <li><strong>Architecture:</strong> <strong>Cross-encoder</strong></li>\n  <li>\n    <p><strong>Input format:</strong></p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext mathvariant=&quot;monospace&quot;>[CLS] q [SEP] d_i [SEP]</mtext></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-174\" style=\"width: 13.909em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.565em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1011.46em, 2.867em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-175\"><span class=\"texatom\" id=\"MathJax-Span-176\"><span class=\"mrow\" id=\"MathJax-Span-177\"><span class=\"mtext\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Regular;\">[𝙲𝙻𝚂]<span style=\"font-family: STIXVariants; font-style: normal; font-weight: normal;\"> <span style=\"margin-left: 0.263em;\"></span></span>𝚚<span style=\"font-family: STIXVariants; font-style: normal; font-weight: normal;\"> <span style=\"margin-left: 0.263em;\"></span></span>[𝚂𝙴𝙿]<span style=\"font-family: STIXVariants; font-style: normal; font-weight: normal;\"> <span style=\"margin-left: 0.263em;\"></span></span>𝚍_𝚒<span style=\"font-family: STIXVariants; font-style: normal; font-weight: normal;\"> <span style=\"margin-left: 0.263em;\"></span></span>[𝚂𝙴𝙿]</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext mathvariant=\"monospace\">[CLS] q [SEP] d_i [SEP]</mtext></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-22\">\\texttt{[CLS] q [SEP] d_i [SEP]}</script>\n  </li>\n  <li>\n    <p><strong>Output:</strong> scalar relevance score derived from the final <code class=\"language-plaintext highlighter-rouge\">[CLS]</code> embedding</p>\n  </li>\n  <li>This joint encoding allows the model to capture fine-grained semantic interactions such as negation, entity alignment, and discourse structure, which are inaccessible to bi-encoders.</li>\n</ul>\n<p>In pointwise ranking, the model independently scores each query–candidate pair.</p>\n<p><strong>Input format:</strong></p>\n<p><strong>Output:</strong> scalar relevance score derived from the final <code class=\"language-plaintext highlighter-rouge\">[CLS]</code> embedding</p>\n<h5 id=\"example-monobert\">Example: MonoBERT</h5>\n<ul>\n  <li>\n    <p><a href=\"https://arxiv.org/abs/1910.14424\">Multi-Stage Document Ranking with BERT</a> by Nogueira et al. (2019)</p>\n  </li>\n  <li>\n    <p>monoBERT estimates an absolute relevance probability:</p>\n  </li>\n</ul>\n<p><a href=\"https://arxiv.org/abs/1910.14424\">Multi-Stage Document Ranking with BERT</a> by Nogueira et al. (2019)</p>\n<p>monoBERT estimates an absolute relevance probability:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mtext>relevant</mtext><mo>&amp;#x2223;</mo><mi>q</mi><mo>,</mo><msub><mi>d</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-179\" style=\"width: 10.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.961em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1008.91em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-180\"><span class=\"msubsup\" id=\"MathJax-Span-181\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-182\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-183\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-184\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-185\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">P</span><span class=\"mo\" id=\"MathJax-Span-186\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mtext\" id=\"MathJax-Span-187\" style=\"font-family: STIXGeneral-Regular;\">relevant</span><span class=\"mo\" id=\"MathJax-Span-188\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-189\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">q</span><span class=\"mo\" id=\"MathJax-Span-190\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-191\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-192\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-193\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-194\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mi>P</mi><mo stretchy=\"false\">(</mo><mtext>relevant</mtext><mo>∣</mo><mi>q</mi><mo>,</mo><msub><mi>d</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>Although originally designed for document retrieval, monoBERT-style models are now widely used to rank <strong>generated outputs</strong>, including summaries, answers, and tool responses.</li>\n</ul>\n<h5 id=\"loss-function\">Loss Function</h5>\n<ul>\n  <li>\n    <p>Pointwise models are typically trained using <strong>binary cross-entropy</strong> or regression losses:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>pointwise</mtext></mrow></msub><mo>=</mo><mo>&amp;#x2212;</mo><mrow><mo>[</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mi>log</mi><mo>&amp;#x2061;</mo><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo><mi>log</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></mrow><mo>]</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-195\" style=\"width: 21.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 17.919em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.878em, 1017.76em, 3.44em, -999.997em); top: -2.914em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-196\"><span class=\"msubsup\" id=\"MathJax-Span-197\"><span style=\"display: inline-block; position: relative; width: 3.544em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-198\"><span class=\"mrow\" id=\"MathJax-Span-199\"><span class=\"mi\" id=\"MathJax-Span-200\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-201\"><span class=\"mrow\" id=\"MathJax-Span-202\"><span class=\"mtext\" id=\"MathJax-Span-203\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">pointwise</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-204\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-205\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"mrow\" id=\"MathJax-Span-206\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-207\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">[</span></span><span class=\"mrow\" id=\"MathJax-Span-208\"><span class=\"msubsup\" id=\"MathJax-Span-209\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-210\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-211\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-212\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-213\"></span><span class=\"msubsup\" id=\"MathJax-Span-214\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-215\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-216\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-217\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mo\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">(</span><span class=\"mn\" id=\"MathJax-Span-219\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-220\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-221\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-222\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-223\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-224\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mi\" id=\"MathJax-Span-225\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-226\"></span><span class=\"mo\" id=\"MathJax-Span-227\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-228\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-229\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-230\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-231\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-232\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-233\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span class=\"mo\" id=\"MathJax-Span-234\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">]</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.919em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>pointwise</mtext></mrow></msub><mo>=</mo><mo>−</mo><mrow><mo>[</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mi>log</mi><mo>⁡</mo><msub><mi>s</mi><mi>i</mi></msub><mo>+</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo>]</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-24\">\\mathcal{L}_{\\text{pointwise}} =\n  -\\left[y_i \\log s_i + (1 - y_i)\\log(1 - s_i)\\right]</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mi>i</mi></msub><mo>&amp;#x2208;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>0</mn><mo>,</mo><mn>1</mn></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-235\" style=\"width: 4.065em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.388em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.28em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-236\"><span class=\"msubsup\" id=\"MathJax-Span-237\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-238\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-239\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-240\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"texatom\" id=\"MathJax-Span-241\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-242\"><span class=\"mn\" id=\"MathJax-Span-243\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-245\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">1</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><mrow class=\"MJX-TeXAtom-ORD\"><mn>0</mn><mo>,</mo><mn>1</mn></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">y_i \\in {0,1}</script> is a relevance label.</li>\n    </ul>\n  </li>\n  <li>\n    <p>When supervision comes from LLM-as-a-Judge, labels may be:</p>\n\n    <ul>\n      <li>Binary (acceptable / unacceptable)</li>\n      <li>Ordinal (Likert-style scores)</li>\n      <li>Continuous (normalized quality scores)</li>\n    </ul>\n  </li>\n</ul>\n<p>Pointwise models are typically trained using <strong>binary cross-entropy</strong> or regression losses:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mi>i</mi></msub><mo>&amp;#x2208;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>0</mn><mo>,</mo><mn>1</mn></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-235\" style=\"width: 4.065em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.388em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.28em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-236\"><span class=\"msubsup\" id=\"MathJax-Span-237\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-238\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-239\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-240\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"texatom\" id=\"MathJax-Span-241\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-242\"><span class=\"mn\" id=\"MathJax-Span-243\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-245\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">1</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><mrow class=\"MJX-TeXAtom-ORD\"><mn>0</mn><mo>,</mo><mn>1</mn></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">y_i \\in {0,1}</script> is a relevance label.</li>\n    </ul>\n<p>When supervision comes from LLM-as-a-Judge, labels may be:</p>\n<ul>\n      <li>Binary (acceptable / unacceptable)</li>\n      <li>Ordinal (Likert-style scores)</li>\n      <li>Continuous (normalized quality scores)</li>\n    </ul>\n<h5 id=\"inputoutput-example\">Input–Output Example</h5>\n<ul>\n  <li><strong>Input:</strong>\n    <ul>\n      <li><strong>Query:</strong> “Summarize the paper”</li>\n      <li><strong>Candidate:</strong> model-generated summary</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Output:</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mn>0.91</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-246\" style=\"width: 4.378em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.648em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.54em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-247\"><span class=\"msubsup\" id=\"MathJax-Span-248\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-249\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-250\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-251\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-252\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0.91</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mn>0.91</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">s_i = 0.91</script></p>\n  </li>\n  <li>This setup directly mirrors a <strong>pointwise LLM-as-a-Judge</strong>, where each output is scored independently.</li>\n</ul>\n<ul>\n      <li><strong>Query:</strong> “Summarize the paper”</li>\n      <li><strong>Candidate:</strong> model-generated summary</li>\n    </ul>\n<p><strong>Output:</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mn>0.91</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-246\" style=\"width: 4.378em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.648em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.54em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-247\"><span class=\"msubsup\" id=\"MathJax-Span-248\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-249\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-250\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-251\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-252\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0.91</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mn>0.91</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">s_i = 0.91</script></p>\n<h4 id=\"pairwise-encoder-only-ranking\">Pairwise Encoder-Only Ranking</h4>\n<ul>\n  <li>Pairwise ranking reframes relevance as a <strong>relative preference</strong> between two candidates under the same query.</li>\n</ul>\n<h5 id=\"architecture-1\">Architecture</h5>\n<ul>\n  <li>Cross-encoder</li>\n  <li>\n    <p>Input format:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext mathvariant=&quot;monospace&quot;>[CLS] q [SEP] d_i [SEP] d_j [SEP]</mtext></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-253\" style=\"width: 20.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 16.669em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1016.57em, 2.867em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-254\"><span class=\"texatom\" id=\"MathJax-Span-255\"><span class=\"mrow\" id=\"MathJax-Span-256\"><span class=\"mtext\" id=\"MathJax-Span-257\" style=\"font-family: STIXGeneral-Regular;\">[𝙲𝙻𝚂]<span style=\"font-family: STIXVariants; font-style: normal; font-weight: normal;\"> <span style=\"margin-left: 0.263em;\"></span></span>𝚚<span style=\"font-family: STIXVariants; font-style: normal; font-weight: normal;\"> <span style=\"margin-left: 0.263em;\"></span></span>[𝚂𝙴𝙿]<span style=\"font-family: STIXVariants; font-style: normal; font-weight: normal;\"> <span style=\"margin-left: 0.263em;\"></span></span>𝚍_𝚒<span style=\"font-family: STIXVariants; font-style: normal; font-weight: normal;\"> <span style=\"margin-left: 0.263em;\"></span></span>[𝚂𝙴𝙿]<span style=\"font-family: STIXVariants; font-style: normal; font-weight: normal;\"> <span style=\"margin-left: 0.263em;\"></span></span>𝚍_𝚓<span style=\"font-family: STIXVariants; font-style: normal; font-weight: normal;\"> <span style=\"margin-left: 0.263em;\"></span></span>[𝚂𝙴𝙿]</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext mathvariant=\"monospace\">[CLS] q [SEP] d_i [SEP] d_j [SEP]</mtext></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-27\">\\texttt{[CLS] q [SEP] d_i [SEP] d_j [SEP]}</script>\n  </li>\n  <li>The model processes both candidates jointly and produces a score indicating which is preferred.</li>\n</ul>\n<p>Input format:</p>\n<h5 id=\"example-duobert\">Example: DuoBERT</h5>\n<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/1910.14424\">Multi-Stage Document Ranking with BERT</a> by Nogueira et al. (2019), duoBERT estimates:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>d</mi><mi>i</mi></msub><mo>&amp;#x227B;</mo><msub><mi>d</mi><mi>j</mi></msub><mo>&amp;#x2223;</mo><mi>q</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-258\" style=\"width: 9.273em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1007.66em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-259\"><span class=\"msubsup\" id=\"MathJax-Span-260\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-261\" style=\"font-family: STIXGeneral-Italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-262\"><span class=\"mrow\" id=\"MathJax-Span-263\"><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-265\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-266\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-267\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-268\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">P</span><span class=\"mo\" id=\"MathJax-Span-269\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-270\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-271\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-272\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-273\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≻</span><span class=\"msubsup\" id=\"MathJax-Span-274\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-275\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-276\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-277\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-278\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">q</span><span class=\"mo\" id=\"MathJax-Span-279\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mi>i</mi></msub><mo>≻</mo><msub><mi>d</mi><mi>j</mi></msub><mo>∣</mo><mi>q</mi><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>Unlike pointwise models, pairwise models are invariant to absolute score calibration, which often leads to more stable training.</li>\n</ul>\n<h5 id=\"loss-function-1\">Loss Function</h5>\n<ul>\n  <li>A common choice is <strong>pairwise logistic loss</strong> (introduced in <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf\">RankNet</a> by Burges et al. (2010)):</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>pairwise</mtext></mrow></msub><mo>=</mo><mo>&amp;#x2212;</mo><mi>log</mi><mo>&amp;#x2061;</mo><mi>&amp;#x03C3;</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>&amp;#x2212;</mo><msub><mi>s</mi><mi>j</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-280\" style=\"width: 12.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1010.32em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-281\"><span class=\"msubsup\" id=\"MathJax-Span-282\"><span style=\"display: inline-block; position: relative; width: 3.18em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-283\"><span class=\"mrow\" id=\"MathJax-Span-284\"><span class=\"mi\" id=\"MathJax-Span-285\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-286\"><span class=\"mrow\" id=\"MathJax-Span-287\"><span class=\"mtext\" id=\"MathJax-Span-288\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">pairwise</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-289\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-290\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"mi\" id=\"MathJax-Span-291\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-292\"></span><span class=\"mi\" id=\"MathJax-Span-293\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-294\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-295\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-296\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-297\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-298\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-299\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-300\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-301\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-302\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>pairwise</mtext></mrow></msub><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>−</mo><msub><mi>s</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>This loss encourages the score of the preferred candidate to exceed that of the non-preferred one.</li>\n</ul>\n<h5 id=\"inputoutput-example-1\">Input–Output Example</h5>\n<ul>\n  <li><strong>Input:</strong> Query + Summary A + Summary B</li>\n  <li>\n    <p><strong>Output:</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><mo>&amp;#x227B;</mo><mi>B</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0.78</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-303\" style=\"width: 8.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.72em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-304\"><span class=\"mi\" id=\"MathJax-Span-305\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-307\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-308\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≻</span><span class=\"mi\" id=\"MathJax-Span-309\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">B</span><span class=\"mo\" id=\"MathJax-Span-310\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-311\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-312\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0.78</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi><mo stretchy=\"false\">(</mo><mi>A</mi><mo>≻</mo><mi>B</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mn>0.78</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">P(A \\succ B) = 0.78</script></p>\n  </li>\n  <li>\n    <p>Pairwise supervision aligns closely with <strong>pairwise LLM-as-a-Judge prompts</strong> (“Which output is better?”), which have been shown to yield higher agreement than absolute ratings, as discussed in <a href=\"https://arxiv.org/abs/2305.17926\">A Large-Scale Analysis of Evaluation Biases in LLMs</a> by Wang et al. (2023).</p>\n  </li>\n  <li>The following figure (<a href=\"https://arxiv.org/abs/1910.14424\">source</a>) shows an illustration of a multi-stage ranking architecture involving BM25 as the first stage, monoBERT as the second stage, and duoBERT as the third stage. In the first stage <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>H</mi><mn>0</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-313\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-314\"><span class=\"msubsup\" id=\"MathJax-Span-315\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-316\" style=\"font-family: STIXGeneral-Italic;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mn\" id=\"MathJax-Span-317\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>H</mi><mn>0</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">H_0</script>, given a query <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-318\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-319\"><span class=\"mi\" id=\"MathJax-Span-320\" style=\"font-family: STIXGeneral-Italic;\">q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">q</script>, the top-<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>k</mi><mn>0</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-321\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.89em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-322\"><span class=\"msubsup\" id=\"MathJax-Span-323\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-324\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-325\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>k</mi><mn>0</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">k_0</script> (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>k</mi><mn>0</mn></msub><mo>=</mo><mn>5</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-326\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.55em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-327\"><span class=\"msubsup\" id=\"MathJax-Span-328\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-329\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-330\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-331\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-332\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">5</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>k</mi><mn>0</mn></msub><mo>=</mo><mn>5</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">k_0 = 5</script> in the figure) candidate documents <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>R</mi><mn>0</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-333\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.04em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-334\"><span class=\"msubsup\" id=\"MathJax-Span-335\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-336\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mn\" id=\"MathJax-Span-337\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>R</mi><mn>0</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">R_0</script> are retrieved using BM25. In the second stage <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>H</mi><mn>1</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-338\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-339\"><span class=\"msubsup\" id=\"MathJax-Span-340\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-341\" style=\"font-family: STIXGeneral-Italic;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mn\" id=\"MathJax-Span-342\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>H</mi><mn>1</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">H_1</script>, monoBERT produces a relevance score <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-343\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-344\"><span class=\"msubsup\" id=\"MathJax-Span-345\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-346\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-347\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">s_i</script> for each pair of query <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-348\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-349\"><span class=\"mi\" id=\"MathJax-Span-350\" style=\"font-family: STIXGeneral-Italic;\">q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">q</script> and candidate <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mi>i</mi></msub><mo>&amp;#x2208;</mo><msub><mi>R</mi><mn>0</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-351\" style=\"width: 3.648em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.023em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.02em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-352\"><span class=\"msubsup\" id=\"MathJax-Span-353\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-354\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-355\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-356\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-357\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-358\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mn\" id=\"MathJax-Span-359\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mi>i</mi></msub><mo>∈</mo><msub><mi>R</mi><mn>0</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">d_i \\in R_0</script>. The top-<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>k</mi><mn>1</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-360\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.89em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-361\"><span class=\"msubsup\" id=\"MathJax-Span-362\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-363\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-364\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>k</mi><mn>1</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">k_1</script> (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>k</mi><mn>1</mn></msub><mo>=</mo><mn>3</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-365\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.55em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-366\"><span class=\"msubsup\" id=\"MathJax-Span-367\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-368\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-369\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-370\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-371\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">3</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>k</mi><mn>1</mn></msub><mo>=</mo><mn>3</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-41\">k_1 = 3</script> in the figure) candidates with respect to these relevance scores are passed to the last stage <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>H</mi><mn>2</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-372\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-373\"><span class=\"msubsup\" id=\"MathJax-Span-374\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-375\" style=\"font-family: STIXGeneral-Italic;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mn\" id=\"MathJax-Span-376\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>H</mi><mn>2</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-42\">H_2</script>, in which duoBERT computes a relevance score <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-377\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.15em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-378\"><span class=\"msubsup\" id=\"MathJax-Span-379\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-380\" style=\"font-family: STIXGeneral-Italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-381\"><span class=\"mrow\" id=\"MathJax-Span-382\"><span class=\"mi\" id=\"MathJax-Span-383\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-384\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-385\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-43\">p_{i,j}</script> for each triple <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><msub><mi>d</mi><mi>i</mi></msub><mo>,</mo><msub><mi>d</mi><mi>j</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-386\" style=\"width: 4.378em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.648em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.6em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-387\"><span class=\"mo\" id=\"MathJax-Span-388\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-389\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-390\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-391\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-392\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-393\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-394\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-395\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-396\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-397\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-398\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><msub><mi>d</mi><mi>i</mi></msub><mo>,</mo><msub><mi>d</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">(q, d_i, d_j)</script>. The final list of candidates <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>R</mi><mn>2</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-399\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.04em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-400\"><span class=\"msubsup\" id=\"MathJax-Span-401\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-402\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mn\" id=\"MathJax-Span-403\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>R</mi><mn>2</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-45\">R_2</script> is formed by re-ranking the candidates according to these scores (see Section 3.3 for a description of how these pairwise scores are aggregated).</li>\n</ul>\n<p><strong>Output:</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><mo>&amp;#x227B;</mo><mi>B</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0.78</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-303\" style=\"width: 8.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.72em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-304\"><span class=\"mi\" id=\"MathJax-Span-305\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-307\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-308\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≻</span><span class=\"mi\" id=\"MathJax-Span-309\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">B</span><span class=\"mo\" id=\"MathJax-Span-310\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-311\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-312\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0.78</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi><mo stretchy=\"false\">(</mo><mi>A</mi><mo>≻</mo><mi>B</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mn>0.78</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">P(A \\succ B) = 0.78</script></p>\n<p>Pairwise supervision aligns closely with <strong>pairwise LLM-as-a-Judge prompts</strong> (“Which output is better?”), which have been shown to yield higher agreement than absolute ratings, as discussed in <a href=\"https://arxiv.org/abs/2305.17926\">A Large-Scale Analysis of Evaluation Biases in LLMs</a> by Wang et al. (2023).</p>\n<p><img src=\"/primers/ai/assets/LLM-as-a-judge/monoBERT-duoBERT.jpg\" alt=\"\"></p>\n<h4 id=\"listwise-encoder-only-ranking\">Listwise Encoder-Only Ranking</h4>\n<ul>\n  <li>Listwise ranking considers an entire candidate set jointly and optimizes ranking quality at the list level.</li>\n</ul>\n<h5 id=\"architecture-2\">Architecture</h5>\n<ul>\n  <li>Extended cross-encoder</li>\n  <li>Input includes multiple candidates (often truncated for efficiency)</li>\n  <li>\n    <p>Output is a list of scores or a permutation distribution</p>\n  </li>\n  <li>Listwise methods aim to optimize ranking metrics directly rather than approximating them via pairwise comparisons.</li>\n</ul>\n<p>Output is a list of scores or a permutation distribution</p>\n<h5 id=\"example-listbert\">Example: ListBERT</h5>\n<ul>\n  <li>\n    <p><a href=\"https://arxiv.org/abs/2206.15198\">ListBERT: Learning to Rank E-commerce Products with Listwise BERT</a> by Kumar et al. (2022)</p>\n  </li>\n  <li>\n    <p>The following figure (<a href=\"https://arxiv.org/abs/2206.15198\">source</a>) shows the ListBERT architecture.</p>\n  </li>\n</ul>\n<p><a href=\"https://arxiv.org/abs/2206.15198\">ListBERT: Learning to Rank E-commerce Products with Listwise BERT</a> by Kumar et al. (2022)</p>\n<p>The following figure (<a href=\"https://arxiv.org/abs/2206.15198\">source</a>) shows the ListBERT architecture.</p>\n<p><img src=\"/primers/ai/assets/LLM-as-a-judge/ListBERT.jpg\" alt=\"\"></p>\n<h4 id=\"direct-and-approximate-optimization-of-ndcg\">Direct and Approximate Optimization of NDCG</h4>\n<ul>\n  <li>A central motivation for listwise ranking is <strong>directly optimizing ranking metrics</strong> such as Normalized Discounted Cumulative Gain (NDCG).</li>\n</ul>\n<h5 id=\"ndcg-definition\">NDCG Definition</h5>\n<ul>\n  <li>Given a ranked list, DCG is defined as:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>DCG</mtext><mo>=</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><mfrac><mrow><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mi>e</mi><msub><mi>l</mi><mi>i</mi></msub></mrow></msup><mo>&amp;#x2212;</mo><mn>1</mn></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-404\" style=\"width: 11.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.534em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.419em, 1009.53em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-405\"><span class=\"mtext\" id=\"MathJax-Span-406\" style=\"font-family: STIXGeneral-Regular;\">DCG</span><span class=\"mo\" id=\"MathJax-Span-407\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-408\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-409\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-410\"><span class=\"mrow\" id=\"MathJax-Span-411\"><span class=\"mi\" id=\"MathJax-Span-412\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-413\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-414\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-415\"><span class=\"mrow\" id=\"MathJax-Span-416\"><span class=\"mi\" id=\"MathJax-Span-417\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mfrac\" id=\"MathJax-Span-418\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 4.482em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(2.971em, 1003.18em, 4.326em, -999.997em); top: -4.685em; left: 50%; margin-left: -1.612em;\"><span class=\"mrow\" id=\"MathJax-Span-419\"><span class=\"msubsup\" id=\"MathJax-Span-420\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-421\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-422\"><span class=\"mrow\" id=\"MathJax-Span-423\"><span class=\"mi\" id=\"MathJax-Span-424\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-425\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"msubsup\" id=\"MathJax-Span-426\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-427\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.211em;\"><span class=\"mi\" id=\"MathJax-Span-428\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-429\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mn\" id=\"MathJax-Span-430\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1004.33em, 4.43em, -999.997em); top: -3.331em; left: 50%; margin-left: -2.185em;\"><span class=\"mrow\" id=\"MathJax-Span-431\"><span class=\"msubsup\" id=\"MathJax-Span-432\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-433\" style=\"font-family: STIXGeneral-Regular;\">log</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 1.305em;\"><span class=\"mn\" id=\"MathJax-Span-434\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-435\"></span><span class=\"mo\" id=\"MathJax-Span-436\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-437\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-438\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mn\" id=\"MathJax-Span-439\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span><span class=\"mo\" id=\"MathJax-Span-440\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1004.48em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 4.482em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>DCG</mtext><mo>=</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><mfrac><mrow><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mi>e</mi><msub><mi>l</mi><mi>i</mi></msub></mrow></msup><mo>−</mo><mn>1</mn></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mfrac></math></span></span></div>\n<ul>\n  <li>\n    <p>… and NDCG is:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>NDCG</mtext><mo>=</mo><mfrac><mtext>DCG</mtext><mtext>IDCG</mtext></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-441\" style=\"width: 8.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1006.77em, 3.023em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-442\"><span class=\"mtext\" id=\"MathJax-Span-443\" style=\"font-family: STIXGeneral-Regular;\">NDCG</span><span class=\"mo\" id=\"MathJax-Span-444\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-445\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1002.09em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -1.039em;\"><span class=\"mtext\" id=\"MathJax-Span-446\" style=\"font-family: STIXGeneral-Regular;\">DCG</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1002.45em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.195em;\"><span class=\"mtext\" id=\"MathJax-Span-447\" style=\"font-family: STIXGeneral-Regular;\">IDCG</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.55em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.555em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>NDCG</mtext><mo>=</mo><mfrac><mtext>DCG</mtext><mtext>IDCG</mtext></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-47\">\\text{NDCG} = \\frac{\\text{DCG}}{\\text{IDCG}}</script>\n\n    <ul>\n      <li>where IDCG is the DCG of the ideal ranking.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Because NDCG is non-differentiable, encoder-only models rely on <strong>surrogate losses</strong>.</p>\n  </li>\n</ul>\n<p>… and NDCG is:</p>\n<ul>\n      <li>where IDCG is the DCG of the ideal ranking.</li>\n    </ul>\n<p>Because NDCG is non-differentiable, encoder-only models rely on <strong>surrogate losses</strong>.</p>\n<h5 id=\"common-listwise-losses\">Common Listwise Losses</h5>\n<ul>\n  <li><strong>ListMLE</strong> (<a href=\"https://dl.acm.org/doi/10.1145/1277741.1277800\">Listwise Approach to Learning to Rank</a> by Cao et al. (2007)):</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>ListMLE</mtext></mrow></msub><mo>=</mo><mo>&amp;#x2212;</mo><mi>log</mi><mo>&amp;#x2061;</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>&amp;#x03C0;</mi><mo>&amp;#x2217;</mo></msup><mo>&amp;#x2223;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>d</mi><mn>1</mn></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>d</mi><mi>n</mi></msub></mrow><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-448\" style=\"width: 16.826em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.013em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1013.96em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-449\"><span class=\"msubsup\" id=\"MathJax-Span-450\"><span style=\"display: inline-block; position: relative; width: 3.388em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-451\"><span class=\"mrow\" id=\"MathJax-Span-452\"><span class=\"mi\" id=\"MathJax-Span-453\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-454\"><span class=\"mrow\" id=\"MathJax-Span-455\"><span class=\"mtext\" id=\"MathJax-Span-456\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">ListMLE</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-457\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-458\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"mi\" id=\"MathJax-Span-459\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-460\"></span><span class=\"mi\" id=\"MathJax-Span-461\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">P</span><span class=\"mo\" id=\"MathJax-Span-462\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-463\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-464\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-465\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-466\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"texatom\" id=\"MathJax-Span-467\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-468\"><span class=\"msubsup\" id=\"MathJax-Span-469\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-470\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-471\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-472\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-473\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-474\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-475\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-476\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-477\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-478\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>ListMLE</mtext></mrow></msub><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy=\"false\">(</mo><msup><mi>π</mi><mo>∗</mo></msup><mo>∣</mo><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>d</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>d</mi><mi>n</mi></msub></mrow><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li><strong>Softmax Cross-Entropy over relevance labels</strong></li>\n  <li>\n    <p><strong>LambdaRank / LambdaLoss</strong>, which approximate gradients of NDCG (<a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2005-82.pdf\">Learning to Rank Using Gradient Descent</a> by Burges et al. (2005))</p>\n  </li>\n  <li>These methods weight gradient updates by estimated changes in NDCG, allowing encoder-only models to optimize ranking quality more directly.</li>\n</ul>\n<p><strong>LambdaRank / LambdaLoss</strong>, which approximate gradients of NDCG (<a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2005-82.pdf\">Learning to Rank Using Gradient Descent</a> by Burges et al. (2005))</p>\n<h4 id=\"relationship-to-llm-as-a-judge-supervision\">Relationship to LLM-as-a-Judge Supervision</h4>\n<ul>\n  <li>\n    <p>Encoder-only rankers are often trained using supervision derived from LLM-as-a-Judge:</p>\n\n    <ul>\n      <li>Pointwise judge scores <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-479\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-480\"><span class=\"mo\" id=\"MathJax-Span-481\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-49\">\\rightarrow</script> regression or classification labels</li>\n      <li>Pairwise judge preferences <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-482\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-483\"><span class=\"mo\" id=\"MathJax-Span-484\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">\\rightarrow</script> ordered pairs</li>\n      <li>Aggregated judge rankings <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-485\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-486\"><span class=\"mo\" id=\"MathJax-Span-487\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-51\">\\rightarrow</script> listwise permutations</li>\n    </ul>\n  </li>\n  <li>\n    <p>This weak supervision strategy is increasingly common in large-scale systems where human labels are scarce, and is conceptually aligned with RLAIF pipelines such as <a href=\"https://arxiv.org/abs/2302.08582\">Training Language Models from AI Feedback</a> by Bai et al. (2023).</p>\n  </li>\n</ul>\n<p>Encoder-only rankers are often trained using supervision derived from LLM-as-a-Judge:</p>\n<ul>\n      <li>Pointwise judge scores <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-479\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-480\"><span class=\"mo\" id=\"MathJax-Span-481\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-49\">\\rightarrow</script> regression or classification labels</li>\n      <li>Pairwise judge preferences <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-482\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-483\"><span class=\"mo\" id=\"MathJax-Span-484\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">\\rightarrow</script> ordered pairs</li>\n      <li>Aggregated judge rankings <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-485\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-486\"><span class=\"mo\" id=\"MathJax-Span-487\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-51\">\\rightarrow</script> listwise permutations</li>\n    </ul>\n<p>This weak supervision strategy is increasingly common in large-scale systems where human labels are scarce, and is conceptually aligned with RLAIF pipelines such as <a href=\"https://arxiv.org/abs/2302.08582\">Training Language Models from AI Feedback</a> by Bai et al. (2023).</p>\n<h4 id=\"takeaways\">Takeaways</h4>\n<ul>\n  <li>\n    <p><strong>Cross-encoder architectures enable high-precision ranking</strong> by jointly encoding the query and candidate output, allowing full token-level interaction and capturing fine-grained relevance signals that simpler models miss.</p>\n  </li>\n  <li>\n    <p><strong>All major LTR paradigms are supported</strong>—pointwise (absolute scoring), pairwise (relative preferences), and listwise (global ordering)—with different trade-offs in supervision complexity, stability, and alignment with ranking objectives.</p>\n  </li>\n  <li>\n    <p><strong>Listwise training methods enable approximate or direct optimization of NDCG</strong>, using surrogate losses such as ListMLE or gradient-based approaches like LambdaRank to overcome the non-differentiability of ranking metrics.</p>\n  </li>\n  <li>\n    <p><strong>LLM-as-a-Judge provides scalable, high-quality supervision signals</strong> that can replace or augment human annotations, enabling efficient fine-tuning of encoder-based LTR models across pointwise, pairwise, and listwise settings.</p>\n  </li>\n</ul>\n<p><strong>Cross-encoder architectures enable high-precision ranking</strong> by jointly encoding the query and candidate output, allowing full token-level interaction and capturing fine-grained relevance signals that simpler models miss.</p>\n<p><strong>All major LTR paradigms are supported</strong>—pointwise (absolute scoring), pairwise (relative preferences), and listwise (global ordering)—with different trade-offs in supervision complexity, stability, and alignment with ranking objectives.</p>\n<p><strong>Listwise training methods enable approximate or direct optimization of NDCG</strong>, using surrogate losses such as ListMLE or gradient-based approaches like LambdaRank to overcome the non-differentiability of ranking metrics.</p>\n<p><strong>LLM-as-a-Judge provides scalable, high-quality supervision signals</strong> that can replace or augment human annotations, enabling efficient fine-tuning of encoder-based LTR models across pointwise, pairwise, and listwise settings.</p>",
    "contentMarkdown": "*   Encoder-only transformer models form the backbone of most **high-precision neural ranking systems** used today. These models take structured inputs (queries, documents, or generated outputs) and map them to contextual representations that are then converted into relevance scores. Their strength lies in **token-level interaction modeling**, which makes them especially effective for reranking small candidate sets.\n    \n*   Canonical encoder-only architectures are based on transformers such as [BERT](https://arxiv.org/abs/1810.04805) by Devlin et al. (2018), [RoBERTa](https://arxiv.org/abs/1907.11692) by Liu et al. (2019), and [ELECTRA](https://arxiv.org/abs/2003.10555) by Clark et al. (2020).\n    \n*   Encoder-only rankers are most commonly implemented as **cross-encoders**, where the query and candidate are jointly encoded, enabling full self-attention across tokens.\n    \n\nEncoder-only transformer models form the backbone of most **high-precision neural ranking systems** used today. These models take structured inputs (queries, documents, or generated outputs) and map them to contextual representations that are then converted into relevance scores. Their strength lies in **token-level interaction modeling**, which makes them especially effective for reranking small candidate sets.\n\nCanonical encoder-only architectures are based on transformers such as [BERT](https://arxiv.org/abs/1810.04805) by Devlin et al. (2018), [RoBERTa](https://arxiv.org/abs/1907.11692) by Liu et al. (2019), and [ELECTRA](https://arxiv.org/abs/2003.10555) by Clark et al. (2020).\n\nEncoder-only rankers are most commonly implemented as **cross-encoders**, where the query and candidate are jointly encoded, enabling full self-attention across tokens.\n\n#### Pointwise Encoder-Only Ranking\n\n##### Architecture\n\n*   In pointwise ranking, the model independently scores each query–candidate pair.\n    \n*   **Architecture:** **Cross-encoder**\n*   **Input format:**\n    \n    \\[𝙲𝙻𝚂\\] 𝚚 \\[𝚂𝙴𝙿\\] 𝚍\\_𝚒 \\[𝚂𝙴𝙿\\]\\[CLS\\] q \\[SEP\\] d\\_i \\[SEP\\]\n    \n    \\\\texttt{\\[CLS\\] q \\[SEP\\] d\\_i \\[SEP\\]}\n*   **Output:** scalar relevance score derived from the final `[CLS]` embedding\n    \n*   This joint encoding allows the model to capture fine-grained semantic interactions such as negation, entity alignment, and discourse structure, which are inaccessible to bi-encoders.\n\nIn pointwise ranking, the model independently scores each query–candidate pair.\n\n**Input format:**\n\n**Output:** scalar relevance score derived from the final `[CLS]` embedding\n\n##### Example: MonoBERT\n\n*   [Multi-Stage Document Ranking with BERT](https://arxiv.org/abs/1910.14424) by Nogueira et al. (2019)\n    \n*   monoBERT estimates an absolute relevance probability:\n    \n\n[Multi-Stage Document Ranking with BERT](https://arxiv.org/abs/1910.14424) by Nogueira et al. (2019)\n\nmonoBERT estimates an absolute relevance probability:\n\nsi\\=P(relevant∣q,di)si\\=P(relevant∣q,di)\n\n*   Although originally designed for document retrieval, monoBERT-style models are now widely used to rank **generated outputs**, including summaries, answers, and tool responses.\n\n##### Loss Function\n\n*   Pointwise models are typically trained using **binary cross-entropy** or regression losses:\n    \n    pointwise\\=−\\[yilogsi+(1−yi)log(1−si)\\]Lpointwise\\=−\\[yilog⁡si+(1−yi)log⁡(1−si)\\]\n    \n    \\\\mathcal{L}\\_{\\\\text{pointwise}} = -\\\\left\\[y\\_i \\\\log s\\_i + (1 - y\\_i)\\\\log(1 - s\\_i)\\\\right\\]\n    *   where yi∈0,1yi∈0,1y\\_i \\\\in {0,1} is a relevance label.\n*   When supervision comes from LLM-as-a-Judge, labels may be:\n    \n    *   Binary (acceptable / unacceptable)\n    *   Ordinal (Likert-style scores)\n    *   Continuous (normalized quality scores)\n\nPointwise models are typically trained using **binary cross-entropy** or regression losses:\n\n*   where yi∈0,1yi∈0,1y\\_i \\\\in {0,1} is a relevance label.\n\nWhen supervision comes from LLM-as-a-Judge, labels may be:\n\n*   Binary (acceptable / unacceptable)\n*   Ordinal (Likert-style scores)\n*   Continuous (normalized quality scores)\n\n##### Input–Output Example\n\n*   **Input:**\n    *   **Query:** “Summarize the paper”\n    *   **Candidate:** model-generated summary\n*   **Output:** si\\=0.91si\\=0.91s\\_i = 0.91\n    \n*   This setup directly mirrors a **pointwise LLM-as-a-Judge**, where each output is scored independently.\n\n*   **Query:** “Summarize the paper”\n*   **Candidate:** model-generated summary\n\n**Output:** si\\=0.91si\\=0.91s\\_i = 0.91\n\n#### Pairwise Encoder-Only Ranking\n\n*   Pairwise ranking reframes relevance as a **relative preference** between two candidates under the same query.\n\n##### Architecture\n\n*   Cross-encoder\n*   Input format:\n    \n    \\[𝙲𝙻𝚂\\] 𝚚 \\[𝚂𝙴𝙿\\] 𝚍\\_𝚒 \\[𝚂𝙴𝙿\\] 𝚍\\_𝚓 \\[𝚂𝙴𝙿\\]\\[CLS\\] q \\[SEP\\] d\\_i \\[SEP\\] d\\_j \\[SEP\\]\n    \n    \\\\texttt{\\[CLS\\] q \\[SEP\\] d\\_i \\[SEP\\] d\\_j \\[SEP\\]}\n*   The model processes both candidates jointly and produces a score indicating which is preferred.\n\nInput format:\n\n##### Example: DuoBERT\n\n*   Proposed in [Multi-Stage Document Ranking with BERT](https://arxiv.org/abs/1910.14424) by Nogueira et al. (2019), duoBERT estimates:\n\npi,j\\=P(di≻dj∣q)pi,j\\=P(di≻dj∣q)\n\n*   Unlike pointwise models, pairwise models are invariant to absolute score calibration, which often leads to more stable training.\n\n##### Loss Function\n\n*   A common choice is **pairwise logistic loss** (introduced in [RankNet](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) by Burges et al. (2010)):\n\npairwise\\=−logσ(si−sj)Lpairwise\\=−log⁡σ(si−sj)\n\n*   This loss encourages the score of the preferred candidate to exceed that of the non-preferred one.\n\n##### Input–Output Example\n\n*   **Input:** Query + Summary A + Summary B\n*   **Output:** P(A≻B)\\=0.78P(A≻B)\\=0.78P(A \\\\succ B) = 0.78\n    \n*   Pairwise supervision aligns closely with **pairwise LLM-as-a-Judge prompts** (“Which output is better?”), which have been shown to yield higher agreement than absolute ratings, as discussed in [A Large-Scale Analysis of Evaluation Biases in LLMs](https://arxiv.org/abs/2305.17926) by Wang et al. (2023).\n    \n*   The following figure ([source](https://arxiv.org/abs/1910.14424)) shows an illustration of a multi-stage ranking architecture involving BM25 as the first stage, monoBERT as the second stage, and duoBERT as the third stage. In the first stage H0H0H\\_0, given a query qqq, the top-k0k0k\\_0 (k0\\=5k0\\=5k\\_0 = 5 in the figure) candidate documents R0R0R\\_0 are retrieved using BM25. In the second stage H1H1H\\_1, monoBERT produces a relevance score sisis\\_i for each pair of query qqq and candidate di∈R0di∈R0d\\_i \\\\in R\\_0. The top-k1k1k\\_1 (k1\\=3k1\\=3k\\_1 = 3 in the figure) candidates with respect to these relevance scores are passed to the last stage H2H2H\\_2, in which duoBERT computes a relevance score pi,jpi,jp\\_{i,j} for each triple (q,di,dj)(q,di,dj)(q, d\\_i, d\\_j). The final list of candidates R2R2R\\_2 is formed by re-ranking the candidates according to these scores (see Section 3.3 for a description of how these pairwise scores are aggregated).\n\n**Output:** P(A≻B)\\=0.78P(A≻B)\\=0.78P(A \\\\succ B) = 0.78\n\nPairwise supervision aligns closely with **pairwise LLM-as-a-Judge prompts** (“Which output is better?”), which have been shown to yield higher agreement than absolute ratings, as discussed in [A Large-Scale Analysis of Evaluation Biases in LLMs](https://arxiv.org/abs/2305.17926) by Wang et al. (2023).\n\n![](/primers/ai/assets/LLM-as-a-judge/monoBERT-duoBERT.jpg)\n\n#### Listwise Encoder-Only Ranking\n\n*   Listwise ranking considers an entire candidate set jointly and optimizes ranking quality at the list level.\n\n##### Architecture\n\n*   Extended cross-encoder\n*   Input includes multiple candidates (often truncated for efficiency)\n*   Output is a list of scores or a permutation distribution\n    \n*   Listwise methods aim to optimize ranking metrics directly rather than approximating them via pairwise comparisons.\n\nOutput is a list of scores or a permutation distribution\n\n##### Example: ListBERT\n\n*   [ListBERT: Learning to Rank E-commerce Products with Listwise BERT](https://arxiv.org/abs/2206.15198) by Kumar et al. (2022)\n    \n*   The following figure ([source](https://arxiv.org/abs/2206.15198)) shows the ListBERT architecture.\n    \n\n[ListBERT: Learning to Rank E-commerce Products with Listwise BERT](https://arxiv.org/abs/2206.15198) by Kumar et al. (2022)\n\nThe following figure ([source](https://arxiv.org/abs/2206.15198)) shows the ListBERT architecture.\n\n![](/primers/ai/assets/LLM-as-a-judge/ListBERT.jpg)\n\n#### Direct and Approximate Optimization of NDCG\n\n*   A central motivation for listwise ranking is **directly optimizing ranking metrics** such as Normalized Discounted Cumulative Gain (NDCG).\n\n##### NDCG Definition\n\n*   Given a ranked list, DCG is defined as:\n\nDCG\\=∑i\\=1n2reli−1log2(i+1)DCG\\=∑i\\=1n2reli−1log2⁡(i+1)\n\n*   … and NDCG is:\n    \n    NDCG\\=DCGIDCGNDCG\\=DCGIDCG\n    \n    \\\\text{NDCG} = \\\\frac{\\\\text{DCG}}{\\\\text{IDCG}}\n    *   where IDCG is the DCG of the ideal ranking.\n*   Because NDCG is non-differentiable, encoder-only models rely on **surrogate losses**.\n    \n\n… and NDCG is:\n\n*   where IDCG is the DCG of the ideal ranking.\n\nBecause NDCG is non-differentiable, encoder-only models rely on **surrogate losses**.\n\n##### Common Listwise Losses\n\n*   **ListMLE** ([Listwise Approach to Learning to Rank](https://dl.acm.org/doi/10.1145/1277741.1277800) by Cao et al. (2007)):\n\nListMLE\\=−logP(π∗∣d1,…,dn)LListMLE\\=−log⁡P(π∗∣d1,…,dn)\n\n*   **Softmax Cross-Entropy over relevance labels**\n*   **LambdaRank / LambdaLoss**, which approximate gradients of NDCG ([Learning to Rank Using Gradient Descent](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2005-82.pdf) by Burges et al. (2005))\n    \n*   These methods weight gradient updates by estimated changes in NDCG, allowing encoder-only models to optimize ranking quality more directly.\n\n**LambdaRank / LambdaLoss**, which approximate gradients of NDCG ([Learning to Rank Using Gradient Descent](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2005-82.pdf) by Burges et al. (2005))\n\n#### Relationship to LLM-as-a-Judge Supervision\n\n*   Encoder-only rankers are often trained using supervision derived from LLM-as-a-Judge:\n    \n    *   Pointwise judge scores →→\\\\rightarrow regression or classification labels\n    *   Pairwise judge preferences →→\\\\rightarrow ordered pairs\n    *   Aggregated judge rankings →→\\\\rightarrow listwise permutations\n*   This weak supervision strategy is increasingly common in large-scale systems where human labels are scarce, and is conceptually aligned with RLAIF pipelines such as [Training Language Models from AI Feedback](https://arxiv.org/abs/2302.08582) by Bai et al. (2023).\n    \n\nEncoder-only rankers are often trained using supervision derived from LLM-as-a-Judge:\n\n*   Pointwise judge scores →→\\\\rightarrow regression or classification labels\n*   Pairwise judge preferences →→\\\\rightarrow ordered pairs\n*   Aggregated judge rankings →→\\\\rightarrow listwise permutations\n\nThis weak supervision strategy is increasingly common in large-scale systems where human labels are scarce, and is conceptually aligned with RLAIF pipelines such as [Training Language Models from AI Feedback](https://arxiv.org/abs/2302.08582) by Bai et al. (2023).\n\n#### Takeaways\n\n*   **Cross-encoder architectures enable high-precision ranking** by jointly encoding the query and candidate output, allowing full token-level interaction and capturing fine-grained relevance signals that simpler models miss.\n    \n*   **All major LTR paradigms are supported**—pointwise (absolute scoring), pairwise (relative preferences), and listwise (global ordering)—with different trade-offs in supervision complexity, stability, and alignment with ranking objectives.\n    \n*   **Listwise training methods enable approximate or direct optimization of NDCG**, using surrogate losses such as ListMLE or gradient-based approaches like LambdaRank to overcome the non-differentiability of ranking metrics.\n    \n*   **LLM-as-a-Judge provides scalable, high-quality supervision signals** that can replace or augment human annotations, enabling efficient fine-tuning of encoder-based LTR models across pointwise, pairwise, and listwise settings.\n    \n\n**Cross-encoder architectures enable high-precision ranking** by jointly encoding the query and candidate output, allowing full token-level interaction and capturing fine-grained relevance signals that simpler models miss.\n\n**All major LTR paradigms are supported**—pointwise (absolute scoring), pairwise (relative preferences), and listwise (global ordering)—with different trade-offs in supervision complexity, stability, and alignment with ranking objectives.\n\n**Listwise training methods enable approximate or direct optimization of NDCG**, using surrogate losses such as ListMLE or gradient-based approaches like LambdaRank to overcome the non-differentiability of ranking metrics.\n\n**LLM-as-a-Judge provides scalable, high-quality supervision signals** that can replace or augment human annotations, enabling efficient fine-tuning of encoder-based LTR models across pointwise, pairwise, and listwise settings.",
    "contentLength": 110600,
    "wordCount": 1556,
    "hasCode": true,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#encoder-only-models-for-ltr"
  },
  {
    "id": "ai-LLM-as-a-judge-encoderdecoder-models-for-learning-to-rank-ltr-lis-18",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Fine-Tuning Encoder, Encoder–Decoder, and Decoder-Only Models for LTR",
    "title": "Encoder–Decoder Models for Learning-to-Rank (LTR): Listwise Reasoning at Decode Time",
    "order": 18,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>\n    <p>Encoder–decoder architectures extend encoder-only rankers by enabling <strong>list-level reasoning during decoding</strong>, rather than relying solely on encoder-side interactions. This shift is especially powerful for <strong>listwise ranking</strong>, where the objective depends on the <em>entire ordering</em> of candidates (e.g., NDCG), and where global trade-offs across candidates matter.</p>\n  </li>\n  <li>\n    <p>Foundational encoder–decoder models include <a href=\"https://arxiv.org/abs/1910.10683\">T5</a> by Raffel et al. (2019) and <a href=\"https://arxiv.org/abs/1910.13461\">BART</a> by Lewis et al. (2019). In ranking settings, these models are adapted to ingest multiple candidates and produce <strong>ranking-aware outputs</strong>—scores, permutations, or ordered tokens—via the decoder.</p>\n  </li>\n</ul>\n<p>Encoder–decoder architectures extend encoder-only rankers by enabling <strong>list-level reasoning during decoding</strong>, rather than relying solely on encoder-side interactions. This shift is especially powerful for <strong>listwise ranking</strong>, where the objective depends on the <em>entire ordering</em> of candidates (e.g., NDCG), and where global trade-offs across candidates matter.</p>\n<p>Foundational encoder–decoder models include <a href=\"https://arxiv.org/abs/1910.10683\">T5</a> by Raffel et al. (2019) and <a href=\"https://arxiv.org/abs/1910.13461\">BART</a> by Lewis et al. (2019). In ranking settings, these models are adapted to ingest multiple candidates and produce <strong>ranking-aware outputs</strong>—scores, permutations, or ordered tokens—via the decoder.</p>\n<h4 id=\"why-encoderdecoder-for-ranking\">Why Encoder–Decoder for Ranking?</h4>\n<ul>\n  <li>\n    <p>Encoder-only listwise rankers face two core limitations:</p>\n\n    <ol>\n      <li><strong>Quadratic encoder cost</strong> when jointly encoding many candidates.</li>\n      <li><strong>Weak global reasoning</strong>, since most listwise losses still rely on per-item scores.</li>\n    </ol>\n  </li>\n  <li>\n    <p>Encoder–decoder models address both by:</p>\n\n    <ul>\n      <li>Encoding each candidate independently (linear in list size).</li>\n      <li>Performing <strong>global interaction in the decoder</strong>, which attends over <em>all</em> candidate encodings.</li>\n    </ul>\n  </li>\n  <li>\n    <p>This design enables richer list-level dependencies while keeping computation tractable.</p>\n  </li>\n</ul>\n<p>Encoder-only listwise rankers face two core limitations:</p>\n<ol>\n      <li><strong>Quadratic encoder cost</strong> when jointly encoding many candidates.</li>\n      <li><strong>Weak global reasoning</strong>, since most listwise losses still rely on per-item scores.</li>\n    </ol>\n<p>Encoder–decoder models address both by:</p>\n<ul>\n      <li>Encoding each candidate independently (linear in list size).</li>\n      <li>Performing <strong>global interaction in the decoder</strong>, which attends over <em>all</em> candidate encodings.</li>\n    </ul>\n<p>This design enables richer list-level dependencies while keeping computation tractable.</p>\n<h4 id=\"fusion-in-decoder-fid-the-core-pattern\">Fusion-in-Decoder (FiD): the Core Pattern</h4>\n<ul>\n  <li>The canonical architecture for encoder–decoder ranking is <strong>Fusion-in-Decoder (FiD)</strong>, introduced in\n<a href=\"https://arxiv.org/abs/2007.01282\">Fusion-in-Decoder for Open-Domain Question Answering</a> by Izacard et al. (2021).</li>\n</ul>\n<h5 id=\"architecture-3\">Architecture</h5>\n<ul>\n  <li><strong>Encoder</strong>: independently encodes each (query, candidate) pair</li>\n  <li><strong>Decoder</strong>: attends jointly over all encoded representations</li>\n  <li>\n    <p><strong>Output</strong>: sequence conditioned on the <em>entire candidate set</em></p>\n  </li>\n  <li>The following figure (<a href=\"https://arxiv.org/abs/2007.01282\">source</a>) shows the architecture of the FiD architecture, where multiple embedded documents are fused at decoding time.</li>\n</ul>\n<p><strong>Output</strong>: sequence conditioned on the <em>entire candidate set</em></p>\n<p><img src=\"/primers/ai/assets/LLM-as-a-judge/FID.jpg\" alt=\"\"></p>\n<ul>\n  <li>Formally, each candidate <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-488\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.78em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-489\"><span class=\"msubsup\" id=\"MathJax-Span-490\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-491\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-492\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">d_i</script> is encoded as:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>h</mi><mi>i</mi></msub><mo>=</mo><mtext>Enc</mtext><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><msub><mi>d</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-493\" style=\"width: 7.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.89em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-494\"><span class=\"msubsup\" id=\"MathJax-Span-495\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-496\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-497\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-498\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mtext\" id=\"MathJax-Span-499\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">Enc</span><span class=\"mo\" id=\"MathJax-Span-500\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-501\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-502\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-503\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-504\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-505\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-506\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>h</mi><mi>i</mi></msub><mo>=</mo><mtext>Enc</mtext><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><msub><mi>d</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>The decoder then conditions on the set <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>h</mi><mn>1</mn></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>h</mi><mi>n</mi></msub></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-507\" style=\"width: 4.846em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.013em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.01em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-508\"><span class=\"texatom\" id=\"MathJax-Span-509\"><span class=\"mrow\" id=\"MathJax-Span-510\"><span class=\"msubsup\" id=\"MathJax-Span-511\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-512\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-513\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-514\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-515\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-516\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-517\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-518\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-519\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>h</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>h</mi><mi>n</mi></msub></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-54\">{h_1, \\dots, h_n}</script> to produce outputs.</li>\n</ul>\n<h4 id=\"listt5-encoderdecoder-listwise-re-ranking\">ListT5: Encoder–Decoder Listwise Re-Ranking</h4>\n<ul>\n  <li>\n    <p>A representative ranking system built on FiD is <strong>ListT5</strong>.</p>\n  </li>\n  <li>\n    <p><a href=\"https://arxiv.org/abs/2402.15838\">ListT5: Listwise Re-ranking with Fusion-in-Decoder Improves Zero-shot Retrieval</a> by Yoon et al. (2024)</p>\n  </li>\n</ul>\n<p>A representative ranking system built on FiD is <strong>ListT5</strong>.</p>\n<p><a href=\"https://arxiv.org/abs/2402.15838\">ListT5: Listwise Re-ranking with Fusion-in-Decoder Improves Zero-shot Retrieval</a> by Yoon et al. (2024)</p>\n<h5 id=\"architecture-4\">Architecture</h5>\n<ul>\n  <li><strong>Backbone:</strong> T5-style encoder–decoder</li>\n  <li><strong>Encoder input:</strong> multiple (query, document) pairs</li>\n  <li>\n    <p><strong>Decoder output:</strong> <strong>ranking tokens</strong> representing an ordering</p>\n  </li>\n  <li>Unlike encoder-only models that emit scalar scores, ListT5 generates a <strong>sequence that encodes the ranking itself</strong>.</li>\n</ul>\n<p><strong>Decoder output:</strong> <strong>ranking tokens</strong> representing an ordering</p>\n<h5 id=\"output-representation\">Output Representation</h5>\n<ul>\n  <li>\n    <p>For a list of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-520\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-521\"><span class=\"mi\" id=\"MathJax-Span-522\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-55\">k</script> candidates, the decoder emits a sequence such as:</p>\n\n    <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\">  3 &gt; 7 &gt; 1 &gt; 5 &gt; ...\n</code></pre></div>    </div>\n  </li>\n  <li>\n    <p>This sequence represents a permutation <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C0;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-523\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-524\"><span class=\"mi\" id=\"MathJax-Span-525\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>π</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">\\pi</script> over candidates.</p>\n  </li>\n</ul>\n<p>For a list of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-520\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-521\"><span class=\"mi\" id=\"MathJax-Span-522\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-55\">k</script> candidates, the decoder emits a sequence such as:</p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\">  3 &gt; 7 &gt; 1 &gt; 5 &gt; ...\n</code></pre>\n<p>This sequence represents a permutation <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C0;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-523\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-524\"><span class=\"mi\" id=\"MathJax-Span-525\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>π</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">\\pi</script> over candidates.</p>\n<h4 id=\"training-objectives-for-encoderdecoder-ranking\">Training Objectives for Encoder–Decoder Ranking</h4>\n<ul>\n  <li>Encoder–decoder rankers are naturally trained with <strong>sequence-level objectives</strong>, which align well with listwise metrics.</li>\n</ul>\n<h5 id=\"sequence-cross-entropy-loss\">Sequence Cross-Entropy Loss</h5>\n<ul>\n  <li>Given a target permutation <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>&amp;#x03C0;</mi><mo>&amp;#x2217;</mo></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-526\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.04em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-527\"><span class=\"msubsup\" id=\"MathJax-Span-528\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-529\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-530\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>π</mi><mo>∗</mo></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">\\pi^*</script>, training minimizes:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>listwise</mtext></mrow></msub><mo>=</mo><mo>&amp;#x2212;</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi></mrow></munderover><mi>log</mi><mo>&amp;#x2061;</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>&amp;#x03C0;</mi><mi>t</mi><mo>&amp;#x2217;</mo></msubsup><mo>&amp;#x2223;</mo><mi>q</mi><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>d</mi><mi>i</mi></msub></mrow><mo>,</mo><msubsup><mi>&amp;#x03C0;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;lt;</mo><mi>t</mi></mrow><mo>&amp;#x2217;</mo></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-531\" style=\"width: 17.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.482em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.315em, 1014.43em, 3.648em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-532\"><span class=\"msubsup\" id=\"MathJax-Span-533\"><span style=\"display: inline-block; position: relative; width: 2.919em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-534\"><span class=\"mrow\" id=\"MathJax-Span-535\"><span class=\"mi\" id=\"MathJax-Span-536\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-537\"><span class=\"mrow\" id=\"MathJax-Span-538\"><span class=\"mtext\" id=\"MathJax-Span-539\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">listwise</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-540\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-541\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"munderover\" id=\"MathJax-Span-542\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-543\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.99em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-544\"><span class=\"mrow\" id=\"MathJax-Span-545\"><span class=\"mi\" id=\"MathJax-Span-546\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-547\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-548\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-549\"><span class=\"mrow\" id=\"MathJax-Span-550\"><span class=\"mi\" id=\"MathJax-Span-551\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-552\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-553\"></span><span class=\"mi\" id=\"MathJax-Span-554\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">P</span><span class=\"mo\" id=\"MathJax-Span-555\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-556\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-557\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.47em, 4.169em, -999.997em); top: -4.32em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-558\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -3.799em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-559\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-560\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-561\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">q</span><span class=\"mo\" id=\"MathJax-Span-562\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-563\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-564\"><span class=\"msubsup\" id=\"MathJax-Span-565\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-566\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-567\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-568\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-569\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-570\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.47em, 4.169em, -999.997em); top: -4.32em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-571\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.78em, 4.169em, -999.997em); top: -3.799em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-572\"><span class=\"mrow\" id=\"MathJax-Span-573\"><span class=\"mo\" id=\"MathJax-Span-574\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&lt;</span><span class=\"mi\" id=\"MathJax-Span-575\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-576\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>listwise</mtext></mrow></msub><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi></mrow></munderover><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy=\"false\">(</mo><msubsup><mi>π</mi><mi>t</mi><mo>∗</mo></msubsup><mo>∣</mo><mi>q</mi><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>d</mi><mi>i</mi></msub></mrow><mo>,</mo><msubsup><mi>π</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>&lt;</mo><mi>t</mi></mrow><mo>∗</mo></msubsup><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>This loss encourages the model to generate the correct ranking order token by token.</li>\n</ul>\n<h5 id=\"connection-to-ndcg-optimization\">Connection to NDCG Optimization</h5>\n<ul>\n  <li>\n    <p>While NDCG is non-differentiable, encoder–decoder models approximate it by:</p>\n\n    <ul>\n      <li>Training on permutations sorted by relevance labels</li>\n      <li>Emphasizing early positions in the sequence (higher DCG weight)</li>\n      <li>Using curriculum strategies that prioritize top-ranked correctness</li>\n    </ul>\n  </li>\n  <li>\n    <p>Some systems additionally <strong>reweight token-level losses</strong> by DCG discounts:</p>\n  </li>\n</ul>\n<p>While NDCG is non-differentiable, encoder–decoder models approximate it by:</p>\n<ul>\n      <li>Training on permutations sorted by relevance labels</li>\n      <li>Emphasizing early positions in the sequence (higher DCG weight)</li>\n      <li>Using curriculum strategies that prioritize top-ranked correctness</li>\n    </ul>\n<p>Some systems additionally <strong>reweight token-level losses</strong> by DCG discounts:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-59-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>w</mi><mi>t</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-577\" style=\"width: 8.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1006.83em, 3.284em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-578\"><span class=\"msubsup\" id=\"MathJax-Span-579\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-580\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-581\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-582\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-583\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 4.482em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-584\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1004.33em, 4.43em, -999.997em); top: -3.331em; left: 50%; margin-left: -2.185em;\"><span class=\"mrow\" id=\"MathJax-Span-585\"><span class=\"msubsup\" id=\"MathJax-Span-586\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-587\" style=\"font-family: STIXGeneral-Regular;\">log</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 1.305em;\"><span class=\"mn\" id=\"MathJax-Span-588\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-589\"></span><span class=\"mo\" id=\"MathJax-Span-590\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-591\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-592\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mn\" id=\"MathJax-Span-593\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span><span class=\"mo\" id=\"MathJax-Span-594\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1004.48em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 4.482em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.184em; border-left: 0px solid; width: 0px; height: 2.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>w</mi><mi>t</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mfrac></math></span></span></div>\n<ul>\n  <li>so that early ranking errors incur larger penalties, approximating NDCG optimization.</li>\n</ul>\n<h4 id=\"advantages-over-encoder-only-listwise-models\">Advantages Over Encoder-Only Listwise Models</h4>\n<ul>\n  <li>\n    <p>Encoder–decoder rankers offer several advantages:</p>\n\n    <ul>\n      <li><strong>True listwise reasoning</strong> at decode time</li>\n      <li><strong>No need for score calibration</strong></li>\n      <li><strong>Direct permutation generation</strong></li>\n      <li><strong>Better alignment with ranking metrics</strong></li>\n    </ul>\n  </li>\n  <li>\n    <p>Empirically, ListT5 shows strong performance in <strong>zero-shot and low-supervision</strong> settings, outperforming encoder-only baselines on retrieval benchmarks, as reported in <a href=\"https://arxiv.org/abs/2402.15838\">ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval</a> Yoon et al. (2024).</p>\n  </li>\n</ul>\n<p>Encoder–decoder rankers offer several advantages:</p>\n<ul>\n      <li><strong>True listwise reasoning</strong> at decode time</li>\n      <li><strong>No need for score calibration</strong></li>\n      <li><strong>Direct permutation generation</strong></li>\n      <li><strong>Better alignment with ranking metrics</strong></li>\n    </ul>\n<p>Empirically, ListT5 shows strong performance in <strong>zero-shot and low-supervision</strong> settings, outperforming encoder-only baselines on retrieval benchmarks, as reported in <a href=\"https://arxiv.org/abs/2402.15838\">ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval</a> Yoon et al. (2024).</p>\n<h4 id=\"using-llm-as-a-judge-for-encoderdecoder-supervision\">Using LLM-as-a-Judge for Encoder–Decoder Supervision</h4>\n<ul>\n  <li>\n    <p>Encoder–decoder rankers are particularly well-suited to supervision from LLM-as-a-Judge:</p>\n\n    <ul>\n      <li><strong>Pointwise judges</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-60-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-595\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-596\"><span class=\"mo\" id=\"MathJax-Span-597\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-60\">\\rightarrow</script> relevance labels used to construct permutations</li>\n      <li><strong>Pairwise judges</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-598\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-599\"><span class=\"mo\" id=\"MathJax-Span-600\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">\\rightarrow</script> partial order constraints</li>\n      <li><strong>Listwise judges</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-62-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-601\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-602\"><span class=\"mo\" id=\"MathJax-Span-603\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-62\">\\rightarrow</script> direct target sequences</li>\n    </ul>\n  </li>\n  <li>\n    <p>This pipeline mirrors weak-supervision strategies used in <a href=\"https://arxiv.org/abs/2302.08582\">Training Language Models from AI Feedback</a> by Bai et al. (2023), where AI-generated preferences replace or augment human labels.</p>\n  </li>\n</ul>\n<p>Encoder–decoder rankers are particularly well-suited to supervision from LLM-as-a-Judge:</p>\n<ul>\n      <li><strong>Pointwise judges</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-60-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-595\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-596\"><span class=\"mo\" id=\"MathJax-Span-597\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-60\">\\rightarrow</script> relevance labels used to construct permutations</li>\n      <li><strong>Pairwise judges</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-598\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-599\"><span class=\"mo\" id=\"MathJax-Span-600\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">\\rightarrow</script> partial order constraints</li>\n      <li><strong>Listwise judges</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-62-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-601\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-602\"><span class=\"mo\" id=\"MathJax-Span-603\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-62\">\\rightarrow</script> direct target sequences</li>\n    </ul>\n<p>This pipeline mirrors weak-supervision strategies used in <a href=\"https://arxiv.org/abs/2302.08582\">Training Language Models from AI Feedback</a> by Bai et al. (2023), where AI-generated preferences replace or augment human labels.</p>\n<h4 id=\"when-to-use-encoderdecoder-rankers\">When to Use Encoder–Decoder Rankers</h4>\n<ul>\n  <li>\n    <p>Encoder–decoder ranking is most appropriate when:</p>\n\n    <ul>\n      <li>Ranking quality matters more than latency</li>\n      <li>Candidate sets are moderate in size (e.g., 10–100)</li>\n      <li>List-level consistency is critical</li>\n      <li>Outputs must be globally coherent</li>\n    </ul>\n  </li>\n  <li>\n    <p>These models are increasingly used in <strong>RAG pipelines</strong>, <strong>evaluation benchmarks</strong>, and <strong>zero-shot reranking</strong> scenarios.</p>\n  </li>\n</ul>\n<p>Encoder–decoder ranking is most appropriate when:</p>\n<ul>\n      <li>Ranking quality matters more than latency</li>\n      <li>Candidate sets are moderate in size (e.g., 10–100)</li>\n      <li>List-level consistency is critical</li>\n      <li>Outputs must be globally coherent</li>\n    </ul>\n<p>These models are increasingly used in <strong>RAG pipelines</strong>, <strong>evaluation benchmarks</strong>, and <strong>zero-shot reranking</strong> scenarios.</p>\n<h4 id=\"takeaways-1\">Takeaways</h4>\n<ul>\n  <li>\n    <p><strong>Encoder–decoder architectures enable listwise reasoning during decoding</strong>, allowing global comparison across all candidates rather than independent scoring.</p>\n  </li>\n  <li>\n    <p><strong>Fusion-in-Decoder (FiD) is the dominant architectural pattern for listwise LTR</strong>, as it encodes each query–document pair independently and fuses all representations in the decoder. This avoids quadratic encoder costs while preserving the ability to compare and weigh all candidates jointly during ranking.</p>\n  </li>\n  <li>\n    <p><strong>ListT5 demonstrates strong listwise ranking performance in low- and zero-shot settings</strong> by adapting the T5 encoder–decoder architecture with FiD-style decoding and listwise objectives. Its results show that encoder–decoder rankers can generalize effectively even with limited supervised ranking data.</p>\n  </li>\n  <li>\n    <p><strong>Sequence-level losses act as practical surrogates for direct NDCG optimization</strong> because they encourage correct global orderings over candidate lists. Although NDCG itself is non-differentiable, sequence cross-entropy and permutation-based losses approximate its behavior by penalizing incorrect relative positions throughout the ranked list.</p>\n  </li>\n  <li>\n    <p><strong>LLM-as-a-Judge enables scalable supervision</strong>, supplying listwise training signals without extensive human annotation.</p>\n  </li>\n</ul>\n<p><strong>Encoder–decoder architectures enable listwise reasoning during decoding</strong>, allowing global comparison across all candidates rather than independent scoring.</p>\n<p><strong>Fusion-in-Decoder (FiD) is the dominant architectural pattern for listwise LTR</strong>, as it encodes each query–document pair independently and fuses all representations in the decoder. This avoids quadratic encoder costs while preserving the ability to compare and weigh all candidates jointly during ranking.</p>\n<p><strong>ListT5 demonstrates strong listwise ranking performance in low- and zero-shot settings</strong> by adapting the T5 encoder–decoder architecture with FiD-style decoding and listwise objectives. Its results show that encoder–decoder rankers can generalize effectively even with limited supervised ranking data.</p>\n<p><strong>Sequence-level losses act as practical surrogates for direct NDCG optimization</strong> because they encourage correct global orderings over candidate lists. Although NDCG itself is non-differentiable, sequence cross-entropy and permutation-based losses approximate its behavior by penalizing incorrect relative positions throughout the ranked list.</p>\n<p><strong>LLM-as-a-Judge enables scalable supervision</strong>, supplying listwise training signals without extensive human annotation.</p>",
    "contentMarkdown": "*   Encoder–decoder architectures extend encoder-only rankers by enabling **list-level reasoning during decoding**, rather than relying solely on encoder-side interactions. This shift is especially powerful for **listwise ranking**, where the objective depends on the _entire ordering_ of candidates (e.g., NDCG), and where global trade-offs across candidates matter.\n    \n*   Foundational encoder–decoder models include [T5](https://arxiv.org/abs/1910.10683) by Raffel et al. (2019) and [BART](https://arxiv.org/abs/1910.13461) by Lewis et al. (2019). In ranking settings, these models are adapted to ingest multiple candidates and produce **ranking-aware outputs**—scores, permutations, or ordered tokens—via the decoder.\n    \n\nEncoder–decoder architectures extend encoder-only rankers by enabling **list-level reasoning during decoding**, rather than relying solely on encoder-side interactions. This shift is especially powerful for **listwise ranking**, where the objective depends on the _entire ordering_ of candidates (e.g., NDCG), and where global trade-offs across candidates matter.\n\nFoundational encoder–decoder models include [T5](https://arxiv.org/abs/1910.10683) by Raffel et al. (2019) and [BART](https://arxiv.org/abs/1910.13461) by Lewis et al. (2019). In ranking settings, these models are adapted to ingest multiple candidates and produce **ranking-aware outputs**—scores, permutations, or ordered tokens—via the decoder.\n\n#### Why Encoder–Decoder for Ranking?\n\n*   Encoder-only listwise rankers face two core limitations:\n    \n    1.  **Quadratic encoder cost** when jointly encoding many candidates.\n    2.  **Weak global reasoning**, since most listwise losses still rely on per-item scores.\n*   Encoder–decoder models address both by:\n    \n    *   Encoding each candidate independently (linear in list size).\n    *   Performing **global interaction in the decoder**, which attends over _all_ candidate encodings.\n*   This design enables richer list-level dependencies while keeping computation tractable.\n    \n\nEncoder-only listwise rankers face two core limitations:\n\n1.  **Quadratic encoder cost** when jointly encoding many candidates.\n2.  **Weak global reasoning**, since most listwise losses still rely on per-item scores.\n\nEncoder–decoder models address both by:\n\n*   Encoding each candidate independently (linear in list size).\n*   Performing **global interaction in the decoder**, which attends over _all_ candidate encodings.\n\nThis design enables richer list-level dependencies while keeping computation tractable.\n\n#### Fusion-in-Decoder (FiD): the Core Pattern\n\n*   The canonical architecture for encoder–decoder ranking is **Fusion-in-Decoder (FiD)**, introduced in [Fusion-in-Decoder for Open-Domain Question Answering](https://arxiv.org/abs/2007.01282) by Izacard et al. (2021).\n\n##### Architecture\n\n*   **Encoder**: independently encodes each (query, candidate) pair\n*   **Decoder**: attends jointly over all encoded representations\n*   **Output**: sequence conditioned on the _entire candidate set_\n    \n*   The following figure ([source](https://arxiv.org/abs/2007.01282)) shows the architecture of the FiD architecture, where multiple embedded documents are fused at decoding time.\n\n**Output**: sequence conditioned on the _entire candidate set_\n\n![](/primers/ai/assets/LLM-as-a-judge/FID.jpg)\n\n*   Formally, each candidate didid\\_i is encoded as:\n\nhi\\=Enc(q,di)hi\\=Enc(q,di)\n\n*   The decoder then conditions on the set h1,…,hnh1,…,hn{h\\_1, \\\\dots, h\\_n} to produce outputs.\n\n#### ListT5: Encoder–Decoder Listwise Re-Ranking\n\n*   A representative ranking system built on FiD is **ListT5**.\n    \n*   [ListT5: Listwise Re-ranking with Fusion-in-Decoder Improves Zero-shot Retrieval](https://arxiv.org/abs/2402.15838) by Yoon et al. (2024)\n    \n\nA representative ranking system built on FiD is **ListT5**.\n\n[ListT5: Listwise Re-ranking with Fusion-in-Decoder Improves Zero-shot Retrieval](https://arxiv.org/abs/2402.15838) by Yoon et al. (2024)\n\n##### Architecture\n\n*   **Backbone:** T5-style encoder–decoder\n*   **Encoder input:** multiple (query, document) pairs\n*   **Decoder output:** **ranking tokens** representing an ordering\n    \n*   Unlike encoder-only models that emit scalar scores, ListT5 generates a **sequence that encodes the ranking itself**.\n\n**Decoder output:** **ranking tokens** representing an ordering\n\n##### Output Representation\n\n*   For a list of kkk candidates, the decoder emits a sequence such as:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `3 > 7 > 1 > 5 > ...`\n    \n*   This sequence represents a permutation ππ\\\\pi over candidates.\n    \n\nFor a list of kkk candidates, the decoder emits a sequence such as:\n\n![](https://aman.ai/images/copy.png)\n\n  `3 > 7 > 1 > 5 > ...`\n\nThis sequence represents a permutation ππ\\\\pi over candidates.\n\n#### Training Objectives for Encoder–Decoder Ranking\n\n*   Encoder–decoder rankers are naturally trained with **sequence-level objectives**, which align well with listwise metrics.\n\n##### Sequence Cross-Entropy Loss\n\n*   Given a target permutation π∗π∗\\\\pi^\\*, training minimizes:\n\nlistwise\\=−∑t\\=1klogP(π∗t∣q,di,π∗<t)Llistwise\\=−∑t\\=1klog⁡P(πt∗∣q,di,π<t∗)\n\n*   This loss encourages the model to generate the correct ranking order token by token.\n\n##### Connection to NDCG Optimization\n\n*   While NDCG is non-differentiable, encoder–decoder models approximate it by:\n    \n    *   Training on permutations sorted by relevance labels\n    *   Emphasizing early positions in the sequence (higher DCG weight)\n    *   Using curriculum strategies that prioritize top-ranked correctness\n*   Some systems additionally **reweight token-level losses** by DCG discounts:\n    \n\nWhile NDCG is non-differentiable, encoder–decoder models approximate it by:\n\n*   Training on permutations sorted by relevance labels\n*   Emphasizing early positions in the sequence (higher DCG weight)\n*   Using curriculum strategies that prioritize top-ranked correctness\n\nSome systems additionally **reweight token-level losses** by DCG discounts:\n\nwt\\=1log2(t+1)wt\\=1log2⁡(t+1)\n\n*   so that early ranking errors incur larger penalties, approximating NDCG optimization.\n\n#### Advantages Over Encoder-Only Listwise Models\n\n*   Encoder–decoder rankers offer several advantages:\n    \n    *   **True listwise reasoning** at decode time\n    *   **No need for score calibration**\n    *   **Direct permutation generation**\n    *   **Better alignment with ranking metrics**\n*   Empirically, ListT5 shows strong performance in **zero-shot and low-supervision** settings, outperforming encoder-only baselines on retrieval benchmarks, as reported in [ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval](https://arxiv.org/abs/2402.15838) Yoon et al. (2024).\n    \n\nEncoder–decoder rankers offer several advantages:\n\n*   **True listwise reasoning** at decode time\n*   **No need for score calibration**\n*   **Direct permutation generation**\n*   **Better alignment with ranking metrics**\n\nEmpirically, ListT5 shows strong performance in **zero-shot and low-supervision** settings, outperforming encoder-only baselines on retrieval benchmarks, as reported in [ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval](https://arxiv.org/abs/2402.15838) Yoon et al. (2024).\n\n#### Using LLM-as-a-Judge for Encoder–Decoder Supervision\n\n*   Encoder–decoder rankers are particularly well-suited to supervision from LLM-as-a-Judge:\n    \n    *   **Pointwise judges** →→\\\\rightarrow relevance labels used to construct permutations\n    *   **Pairwise judges** →→\\\\rightarrow partial order constraints\n    *   **Listwise judges** →→\\\\rightarrow direct target sequences\n*   This pipeline mirrors weak-supervision strategies used in [Training Language Models from AI Feedback](https://arxiv.org/abs/2302.08582) by Bai et al. (2023), where AI-generated preferences replace or augment human labels.\n    \n\nEncoder–decoder rankers are particularly well-suited to supervision from LLM-as-a-Judge:\n\n*   **Pointwise judges** →→\\\\rightarrow relevance labels used to construct permutations\n*   **Pairwise judges** →→\\\\rightarrow partial order constraints\n*   **Listwise judges** →→\\\\rightarrow direct target sequences\n\nThis pipeline mirrors weak-supervision strategies used in [Training Language Models from AI Feedback](https://arxiv.org/abs/2302.08582) by Bai et al. (2023), where AI-generated preferences replace or augment human labels.\n\n#### When to Use Encoder–Decoder Rankers\n\n*   Encoder–decoder ranking is most appropriate when:\n    \n    *   Ranking quality matters more than latency\n    *   Candidate sets are moderate in size (e.g., 10–100)\n    *   List-level consistency is critical\n    *   Outputs must be globally coherent\n*   These models are increasingly used in **RAG pipelines**, **evaluation benchmarks**, and **zero-shot reranking** scenarios.\n    \n\nEncoder–decoder ranking is most appropriate when:\n\n*   Ranking quality matters more than latency\n*   Candidate sets are moderate in size (e.g., 10–100)\n*   List-level consistency is critical\n*   Outputs must be globally coherent\n\nThese models are increasingly used in **RAG pipelines**, **evaluation benchmarks**, and **zero-shot reranking** scenarios.\n\n#### Takeaways\n\n*   **Encoder–decoder architectures enable listwise reasoning during decoding**, allowing global comparison across all candidates rather than independent scoring.\n    \n*   **Fusion-in-Decoder (FiD) is the dominant architectural pattern for listwise LTR**, as it encodes each query–document pair independently and fuses all representations in the decoder. This avoids quadratic encoder costs while preserving the ability to compare and weigh all candidates jointly during ranking.\n    \n*   **ListT5 demonstrates strong listwise ranking performance in low- and zero-shot settings** by adapting the T5 encoder–decoder architecture with FiD-style decoding and listwise objectives. Its results show that encoder–decoder rankers can generalize effectively even with limited supervised ranking data.\n    \n*   **Sequence-level losses act as practical surrogates for direct NDCG optimization** because they encourage correct global orderings over candidate lists. Although NDCG itself is non-differentiable, sequence cross-entropy and permutation-based losses approximate its behavior by penalizing incorrect relative positions throughout the ranked list.\n    \n*   **LLM-as-a-Judge enables scalable supervision**, supplying listwise training signals without extensive human annotation.\n    \n\n**Encoder–decoder architectures enable listwise reasoning during decoding**, allowing global comparison across all candidates rather than independent scoring.\n\n**Fusion-in-Decoder (FiD) is the dominant architectural pattern for listwise LTR**, as it encodes each query–document pair independently and fuses all representations in the decoder. This avoids quadratic encoder costs while preserving the ability to compare and weigh all candidates jointly during ranking.\n\n**ListT5 demonstrates strong listwise ranking performance in low- and zero-shot settings** by adapting the T5 encoder–decoder architecture with FiD-style decoding and listwise objectives. Its results show that encoder–decoder rankers can generalize effectively even with limited supervised ranking data.\n\n**Sequence-level losses act as practical surrogates for direct NDCG optimization** because they encourage correct global orderings over candidate lists. Although NDCG itself is non-differentiable, sequence cross-entropy and permutation-based losses approximate its behavior by penalizing incorrect relative positions throughout the ranked list.\n\n**LLM-as-a-Judge enables scalable supervision**, supplying listwise training signals without extensive human annotation.",
    "contentLength": 52662,
    "wordCount": 1397,
    "hasCode": true,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#encoder–decoder-models-for-learning-to-rank-(ltr):-listwise-reasoning-at-decode-time"
  },
  {
    "id": "ai-LLM-as-a-judge-decoder-based-llm-as-a-judge-for-ltr-19",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Fine-Tuning Encoder, Encoder–Decoder, and Decoder-Only Models for LTR",
    "title": "Decoder-Based LLM-as-a-Judge for LTR",
    "order": 19,
    "orderInChapter": 7,
    "contentHtml": "<ul>\n  <li>\n    <p>Decoder-only LLMs, such as GPT-style architectures, can act directly as ranking judges through prompting, without explicit parameter fine-tuning. In these setups, ranking behavior is induced at inference time via natural language instructions rather than learned scoring heads. Prompt-based LLM-as-a-Judge systems naturally support pointwise, pairwise, and listwise LTR formulations.</p>\n  </li>\n  <li>\n    <p>This section describes how each paradigm is realized with decoder-only judges and explains, in depth, how listwise losses and NDCG-based objectives arise when listwise judgments are used for optimization or supervision.</p>\n  </li>\n  <li>\n    <p>Recent work demonstrates that decoder-based judges can be specialized into evaluator LLMs that closely match human and proprietary-LLM judgments under both pointwise and pairwise formulations. Notably, Prometheus-style evaluators formalize LLM-as-a-Judge as rubric-conditioned scoring within a decoder-only architecture (<a href=\"https://arxiv.org/abs/2310.08491\">Prometheus: Inducing Fine-grained Evaluation Capability in Language Models</a> by Kim et al. (2023); <a href=\"https://arxiv.org/abs/2405.01535\">Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models</a> by Kim et al. (2024)).</p>\n  </li>\n  <li>\n    <p>These systems treat evaluation as a conditional generation problem over instructions, candidate outputs, optional reference answers, and explicit evaluation criteria, grounding ranking behavior in structured supervision rather than implicit prompting alone.</p>\n  </li>\n</ul>\n<p>Decoder-only LLMs, such as GPT-style architectures, can act directly as ranking judges through prompting, without explicit parameter fine-tuning. In these setups, ranking behavior is induced at inference time via natural language instructions rather than learned scoring heads. Prompt-based LLM-as-a-Judge systems naturally support pointwise, pairwise, and listwise LTR formulations.</p>\n<p>This section describes how each paradigm is realized with decoder-only judges and explains, in depth, how listwise losses and NDCG-based objectives arise when listwise judgments are used for optimization or supervision.</p>\n<p>Recent work demonstrates that decoder-based judges can be specialized into evaluator LLMs that closely match human and proprietary-LLM judgments under both pointwise and pairwise formulations. Notably, Prometheus-style evaluators formalize LLM-as-a-Judge as rubric-conditioned scoring within a decoder-only architecture (<a href=\"https://arxiv.org/abs/2310.08491\">Prometheus: Inducing Fine-grained Evaluation Capability in Language Models</a> by Kim et al. (2023); <a href=\"https://arxiv.org/abs/2405.01535\">Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models</a> by Kim et al. (2024)).</p>\n<p>These systems treat evaluation as a conditional generation problem over instructions, candidate outputs, optional reference answers, and explicit evaluation criteria, grounding ranking behavior in structured supervision rather than implicit prompting alone.</p>\n<h4 id=\"pointwise-prompt-based-llm-as-a-judge\">Pointwise Prompt-Based LLM-as-a-Judge</h4>\n<ul>\n  <li>In pointwise evaluation, each candidate output is judged independently against a rubric.</li>\n</ul>\n<h5 id=\"prompt-pattern\">Prompt Pattern</h5>\n<ul>\n  <li>\n    <p>A typical pointwise judge prompt instructs the LLM to:</p>\n\n    <ul>\n      <li>Read the task and evaluation criteria</li>\n      <li>Evaluate a single candidate output</li>\n      <li>Return a discrete label or numeric score</li>\n    </ul>\n  </li>\n  <li>\n    <p>Example:</p>\n\n    <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\">  You are a judge. Based on the rubric below, assign a score from 1–5.\n  Task: &lt;task description&gt;\n  Output: &lt;candidate&gt;\n  Rubric: &lt;criteria&gt;\n  Score:\n</code></pre></div>    </div>\n  </li>\n</ul>\n<p>A typical pointwise judge prompt instructs the LLM to:</p>\n<ul>\n      <li>Read the task and evaluation criteria</li>\n      <li>Evaluate a single candidate output</li>\n      <li>Return a discrete label or numeric score</li>\n    </ul>\n<p>Example:</p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\">  You are a judge. Based on the rubric below, assign a score from 1–5.\n  Task: &lt;task description&gt;\n  Output: &lt;candidate&gt;\n  Rubric: &lt;criteria&gt;\n  Score:\n</code></pre>\n<h5 id=\"interpretation-as-pointwise-ltr\">Interpretation As Pointwise LTR</h5>\n<ul>\n  <li>\n    <p>This corresponds to estimating an absolute relevance or quality score:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-63-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-604\" style=\"width: 5.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.742em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.69em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-605\"><span class=\"msubsup\" id=\"MathJax-Span-606\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-607\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-608\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-609\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-610\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-611\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-612\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-613\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-614\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-615\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-616\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-617\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-63\">s_i = f(q, x_i)</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-64-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-618\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-619\"><span class=\"mi\" id=\"MathJax-Span-620\" style=\"font-family: STIXGeneral-Italic;\">q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-64\">q</script> is the task specification and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-65-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-621\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-622\"><span class=\"msubsup\" id=\"MathJax-Span-623\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-624\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-625\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-65\">x_i</script> is a single output.</li>\n    </ul>\n  </li>\n  <li>\n    <p>This formulation underlies many practical LLM-as-a-Judge systems, including MT-Bench-style evaluations described in <a href=\"https://arxiv.org/abs/2306.05685\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> by Zheng et al. (2023).</p>\n  </li>\n  <li>\n    <p>Beyond pure prompting, pointwise judging has been explicitly modeled via supervised evaluator LLMs trained to emit both natural-language rationales and scalar scores conditioned on user-defined rubrics and reference answers, as demonstrated in <a href=\"https://arxiv.org/abs/2310.08491\">Prometheus: Inducing Fine-grained Evaluation Capability in Language Models</a> by Kim et al. (2023).</p>\n  </li>\n  <li>\n    <p>In this formulation, the scoring function is extended to include reference material and evaluation criteria:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-66-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>direct</mtext></mrow></msub><mo>:</mo><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><mi>a</mi><mo>,</mo><mi>e</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>r</mi><mi>i</mi></msub><mo>,</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-626\" style=\"width: 13.44em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1011.15em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-627\"><span class=\"msubsup\" id=\"MathJax-Span-628\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-629\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-630\"><span class=\"mrow\" id=\"MathJax-Span-631\"><span class=\"mtext\" id=\"MathJax-Span-632\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">direct</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-633\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">:</span><span class=\"mo\" id=\"MathJax-Span-634\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mi\" id=\"MathJax-Span-635\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-636\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-637\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-638\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-639\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-640\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-641\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">a</span><span class=\"mo\" id=\"MathJax-Span-642\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-643\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">e</span><span class=\"mo\" id=\"MathJax-Span-644\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-645\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">→</span><span class=\"mo\" id=\"MathJax-Span-646\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-647\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-648\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-649\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-650\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-651\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-652\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-653\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-654\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>direct</mtext></mrow></msub><mo>:</mo><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><mi>a</mi><mo>,</mo><mi>e</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">→</mo><mo stretchy=\"false\">(</mo><msub><mi>r</mi><mi>i</mi></msub><mo>,</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-66\">f_{\\text{direct}} : (q, x_i, a, e) \\rightarrow (r_i, s_i)</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-67-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>a</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-655\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-656\"><span class=\"mi\" id=\"MathJax-Span-657\" style=\"font-family: STIXGeneral-Italic;\">a</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>a</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-67\">a</script> denotes a reference answer, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-68-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>e</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-658\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.42em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-659\"><span class=\"mi\" id=\"MathJax-Span-660\" style=\"font-family: STIXGeneral-Italic;\">e</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>e</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-68\">e</script> a structured score rubric, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-69-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>r</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-661\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-662\"><span class=\"msubsup\" id=\"MathJax-Span-663\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-664\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-665\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>r</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-69\">r_i</script> a verbal justification, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-70-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub><mo>&amp;#x2208;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo>,</mo><mn>5</mn></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-666\" style=\"width: 7.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.2em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-667\"><span class=\"msubsup\" id=\"MathJax-Span-668\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-669\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-670\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-671\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"texatom\" id=\"MathJax-Span-672\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-673\"><span class=\"mn\" id=\"MathJax-Span-674\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-675\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-676\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">2</span><span class=\"mo\" id=\"MathJax-Span-677\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-678\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">3</span><span class=\"mo\" id=\"MathJax-Span-679\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-680\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">4</span><span class=\"mo\" id=\"MathJax-Span-681\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-682\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">5</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub><mo>∈</mo><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo>,</mo><mn>5</mn></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-70\">s_i \\in {1,2,3,4,5}</script> the pointwise relevance score, enabling fine-grained, rubric-aligned supervision (cf. <a href=\"https://arxiv.org/abs/2310.08491\">Prometheus</a> by Kim et al. (2023)).</li>\n    </ul>\n  </li>\n  <li>\n    <p>Empirically, <a href=\"https://arxiv.org/abs/2310.08491\">Prometheus: Inducing Fine-grained Evaluation Capability in Language Models</a> by Kim et al. (2023) shows that rubric-conditioned pointwise judgments substantially improve correlation with human evaluators, achieving Pearson correlations comparable to GPT-4 on MT-Bench and Vicuna Bench.</p>\n  </li>\n  <li>\n    <p>The following figure (<a href=\"https://arxiv.org/abs/2310.08491\">source</a>) shows that compared to conventional, coarse-grained LLM evaluation, a fine-grained approach that takes user-defined score rubrics as input.</p>\n  </li>\n</ul>\n<p>This corresponds to estimating an absolute relevance or quality score:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-64-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-618\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-619\"><span class=\"mi\" id=\"MathJax-Span-620\" style=\"font-family: STIXGeneral-Italic;\">q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-64\">q</script> is the task specification and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-65-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-621\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-622\"><span class=\"msubsup\" id=\"MathJax-Span-623\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-624\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-625\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-65\">x_i</script> is a single output.</li>\n    </ul>\n<p>This formulation underlies many practical LLM-as-a-Judge systems, including MT-Bench-style evaluations described in <a href=\"https://arxiv.org/abs/2306.05685\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> by Zheng et al. (2023).</p>\n<p>Beyond pure prompting, pointwise judging has been explicitly modeled via supervised evaluator LLMs trained to emit both natural-language rationales and scalar scores conditioned on user-defined rubrics and reference answers, as demonstrated in <a href=\"https://arxiv.org/abs/2310.08491\">Prometheus: Inducing Fine-grained Evaluation Capability in Language Models</a> by Kim et al. (2023).</p>\n<p>In this formulation, the scoring function is extended to include reference material and evaluation criteria:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-67-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>a</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-655\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-656\"><span class=\"mi\" id=\"MathJax-Span-657\" style=\"font-family: STIXGeneral-Italic;\">a</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>a</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-67\">a</script> denotes a reference answer, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-68-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>e</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-658\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.42em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-659\"><span class=\"mi\" id=\"MathJax-Span-660\" style=\"font-family: STIXGeneral-Italic;\">e</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>e</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-68\">e</script> a structured score rubric, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-69-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>r</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-661\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-662\"><span class=\"msubsup\" id=\"MathJax-Span-663\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-664\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-665\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>r</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-69\">r_i</script> a verbal justification, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-70-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub><mo>&amp;#x2208;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo>,</mo><mn>5</mn></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-666\" style=\"width: 7.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.2em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-667\"><span class=\"msubsup\" id=\"MathJax-Span-668\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-669\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-670\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-671\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"texatom\" id=\"MathJax-Span-672\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-673\"><span class=\"mn\" id=\"MathJax-Span-674\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-675\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-676\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">2</span><span class=\"mo\" id=\"MathJax-Span-677\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-678\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">3</span><span class=\"mo\" id=\"MathJax-Span-679\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-680\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">4</span><span class=\"mo\" id=\"MathJax-Span-681\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-682\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">5</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub><mo>∈</mo><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo>,</mo><mn>5</mn></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-70\">s_i \\in {1,2,3,4,5}</script> the pointwise relevance score, enabling fine-grained, rubric-aligned supervision (cf. <a href=\"https://arxiv.org/abs/2310.08491\">Prometheus</a> by Kim et al. (2023)).</li>\n    </ul>\n<p>Empirically, <a href=\"https://arxiv.org/abs/2310.08491\">Prometheus: Inducing Fine-grained Evaluation Capability in Language Models</a> by Kim et al. (2023) shows that rubric-conditioned pointwise judgments substantially improve correlation with human evaluators, achieving Pearson correlations comparable to GPT-4 on MT-Bench and Vicuna Bench.</p>\n<p>The following figure (<a href=\"https://arxiv.org/abs/2310.08491\">source</a>) shows that compared to conventional, coarse-grained LLM evaluation, a fine-grained approach that takes user-defined score rubrics as input.</p>\n<p><img src=\"/primers/ai/assets/LLM-as-a-judge/Prometheus_1.jpg\" alt=\"\"></p>\n<h5 id=\"implicit-losses\">Implicit Losses</h5>\n<ul>\n  <li>\n    <p>While prompt-based judging does not itself involve optimization, pointwise judge outputs are commonly reused as supervision signals when training downstream evaluators or reward models.</p>\n  </li>\n  <li>\n    <p>In practice, scalar scores produced by judges are treated as regression targets and optimized using mean squared error:</p>\n  </li>\n</ul>\n<p>While prompt-based judging does not itself involve optimization, pointwise judge outputs are commonly reused as supervision signals when training downstream evaluators or reward models.</p>\n<p>In practice, scalar scores produced by judges are treated as regression targets and optimized using mean squared error:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-71-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>MSE</mtext></mrow></msub><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>&amp;#x2212;</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>s</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mi>i</mi></msub><msup><mo stretchy=&quot;false&quot;>)</mo><mn>2</mn></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-683\" style=\"width: 8.388em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.982em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1006.98em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-684\"><span class=\"msubsup\" id=\"MathJax-Span-685\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-686\"><span class=\"mrow\" id=\"MathJax-Span-687\"><span class=\"mi\" id=\"MathJax-Span-688\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-689\"><span class=\"mrow\" id=\"MathJax-Span-690\"><span class=\"mtext\" id=\"MathJax-Span-691\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">MSE</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-692\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-693\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-694\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-695\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-696\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-697\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-698\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.128em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-699\"><span class=\"mrow\" id=\"MathJax-Span-700\"><span class=\"munderover\" id=\"MathJax-Span-701\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-702\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.055em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-703\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-704\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-705\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-706\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.315em;\"><span class=\"mn\" id=\"MathJax-Span-707\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>MSE</mtext></mrow></msub><mo>=</mo><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>−</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>s</mi><mo stretchy=\"false\">^</mo></mover></mrow><mi>i</mi></msub><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup></math></span></span></div>\n<ul>\n  <li>\n    <p>When scores are discretized into ordinal buckets (e.g., 1–5), cross-entropy over score classes is also sometimes used, though regression losses are more common in evaluator training.</p>\n  </li>\n  <li>\n    <p>Evaluator LLMs such as Prometheus are trained directly on rubric-conditioned pointwise supervision by minimizing the expected squared error between predicted and reference scores:</p>\n  </li>\n</ul>\n<p>When scores are discretized into ordinal buckets (e.g., 1–5), cross-entropy over score classes is also sometimes used, though regression losses are more common in evaluator training.</p>\n<p>Evaluator LLMs such as Prometheus are trained directly on rubric-conditioned pointwise supervision by minimizing the expected squared error between predicted and reference scores:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-72-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>direct</mtext></mrow></msub><mo>=</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>E</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>e</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msub><mrow><mo>[</mo><mrow><mo stretchy=&quot;false&quot;>(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>&amp;#x2212;</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>s</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mi>i</mi></msub><msup><mo stretchy=&quot;false&quot;>)</mo><mn>2</mn></msup></mrow><mo>]</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-708\" style=\"width: 13.388em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.878em, 1010.99em, 3.44em, -999.997em); top: -2.914em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-709\"><span class=\"msubsup\" id=\"MathJax-Span-710\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-711\"><span class=\"mrow\" id=\"MathJax-Span-712\"><span class=\"mi\" id=\"MathJax-Span-713\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-714\"><span class=\"mrow\" id=\"MathJax-Span-715\"><span class=\"mtext\" id=\"MathJax-Span-716\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">direct</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-717\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-718\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.023em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-719\"><span class=\"mrow\" id=\"MathJax-Span-720\"><span class=\"mi\" id=\"MathJax-Span-721\" style=\"font-family: STIXGeneral-Regular;\">𝔼</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-722\"><span class=\"mrow\" id=\"MathJax-Span-723\"><span class=\"mo\" id=\"MathJax-Span-724\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-725\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-726\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-727\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-728\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-729\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-730\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-731\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mo\" id=\"MathJax-Span-732\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mrow\" id=\"MathJax-Span-733\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-734\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">[</span></span><span class=\"mrow\" id=\"MathJax-Span-735\"><span class=\"mo\" id=\"MathJax-Span-736\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-737\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-738\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-739\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-740\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-741\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.128em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-742\"><span class=\"mrow\" id=\"MathJax-Span-743\"><span class=\"munderover\" id=\"MathJax-Span-744\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-745\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.055em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-746\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-747\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-748\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-749\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.315em;\"><span class=\"mn\" id=\"MathJax-Span-750\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-751\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">]</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.919em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>direct</mtext></mrow></msub><mo>=</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">E</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>e</mi><mo stretchy=\"false\">)</mo></mrow></msub><mrow><mo>[</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>−</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>s</mi><mo stretchy=\"false\">^</mo></mover></mrow><mi>i</mi></msub><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup></mrow><mo>]</mo></mrow></math></span></span></div>\n<ul>\n  <li>Importantly, Prometheus jointly models rationale generation and score prediction in a single autoregressive decoding pass, coupling interpretability with scalar accuracy rather than treating explanation and scoring as separate objectives (<a href=\"https://arxiv.org/abs/2310.08491\">Prometheus</a> by Kim et al. (2023)).</li>\n</ul>\n<h4 id=\"pairwise-prompt-based-llm-as-a-judge\">Pairwise Prompt-Based LLM-as-a-Judge</h4>\n<ul>\n  <li>In pairwise evaluation, the judge compares two candidates and expresses a preference.</li>\n</ul>\n<h5 id=\"prompt-pattern-1\">Prompt Pattern</h5>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\">You are a judge. Which output is better for the task?\nTask: &lt;task&gt;\nOutput A: &lt;candidate A&gt;\nOutput B: &lt;candidate B&gt;\nAnswer (A or B):\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\">You are a judge. Which output is better for the task?\nTask: &lt;task&gt;\nOutput A: &lt;candidate A&gt;\nOutput B: &lt;candidate B&gt;\nAnswer (A or B):\n</code></pre>\n<ul>\n  <li>This formulation mirrors pairwise preference modeling used in RLHF and ranking systems.</li>\n</ul>\n<h5 id=\"pairwise-ltr-interpretation\">Pairwise LTR Interpretation</h5>\n<ul>\n  <li>The judge estimates:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-73-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>&amp;#x227B;</mo><msub><mi>x</mi><mi>j</mi></msub><mo>&amp;#x2223;</mo><mi>q</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-752\" style=\"width: 6.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1005.21em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-753\"><span class=\"mi\" id=\"MathJax-Span-754\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-755\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-756\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-757\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-758\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-759\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≻</span><span class=\"msubsup\" id=\"MathJax-Span-760\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-761\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-762\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-763\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-764\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">q</span><span class=\"mo\" id=\"MathJax-Span-765\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>≻</mo><msub><mi>x</mi><mi>j</mi></msub><mo>∣</mo><mi>q</mi><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>\n    <p>This aligns with pairwise ranking frameworks such as RankNet and duoBERT, and is directly studied in <a href=\"https://arxiv.org/abs/2304.09542\">RankGPT: A Prompt-Based Pairwise Ranking Framework for LLMs</a> by Sun et al. (2023).</p>\n  </li>\n  <li>\n    <p>Pairwise prompting has been extended to supervised evaluator LLMs capable of ranking outputs under explicit, user-defined criteria. <a href=\"https://arxiv.org/abs/2405.01535\">Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models</a> by Kim et al. (2024) introduces a unified formulation conditioning on the task, two candidates, optional reference answers, and an evaluation criterion:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-74-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>pair</mtext></mrow></msub><mo>:</mo><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo>,</mo><mi>a</mi><mo>,</mo><mi>e</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-766\" style=\"width: 14.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1012.24em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-767\"><span class=\"msubsup\" id=\"MathJax-Span-768\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-769\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-770\"><span class=\"mrow\" id=\"MathJax-Span-771\"><span class=\"mtext\" id=\"MathJax-Span-772\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">pair</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-773\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">:</span><span class=\"mo\" id=\"MathJax-Span-774\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mi\" id=\"MathJax-Span-775\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-776\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-777\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-778\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-779\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-780\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-781\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-782\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-783\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-784\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-785\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">a</span><span class=\"mo\" id=\"MathJax-Span-786\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-787\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">e</span><span class=\"mo\" id=\"MathJax-Span-788\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-789\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">→</span><span class=\"mo\" id=\"MathJax-Span-790\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-791\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-792\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-793\"><span class=\"mrow\" id=\"MathJax-Span-794\"><span class=\"mi\" id=\"MathJax-Span-795\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-796\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-797\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-798\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-799\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-800\"><span class=\"mrow\" id=\"MathJax-Span-801\"><span class=\"mi\" id=\"MathJax-Span-802\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-803\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-804\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>pair</mtext></mrow></msub><mo>:</mo><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo>,</mo><mi>a</mi><mo>,</mo><mi>e</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">→</mo><mo stretchy=\"false\">(</mo><msub><mi>r</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>j</mi></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-74\">f_{\\text{pair}} : (q, x_i, x_j, a, e) \\rightarrow (r_{ij}, y_{ij})</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-75-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>j</mi></mrow></msub><mo>&amp;#x2208;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>,</mo><mi>j</mi></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-805\" style=\"width: 3.857em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.18em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.18em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-806\"><span class=\"msubsup\" id=\"MathJax-Span-807\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-808\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-809\"><span class=\"mrow\" id=\"MathJax-Span-810\"><span class=\"mi\" id=\"MathJax-Span-811\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-812\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-813\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"texatom\" id=\"MathJax-Span-814\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-815\"><span class=\"mi\" id=\"MathJax-Span-816\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-817\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-818\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>j</mi></mrow></msub><mo>∈</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>,</mo><mi>j</mi></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-75\">y_{ij} \\in {i,j}</script> denotes the preferred candidate and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-76-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>j</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-819\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-820\"><span class=\"msubsup\" id=\"MathJax-Span-821\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-822\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-823\"><span class=\"mrow\" id=\"MathJax-Span-824\"><span class=\"mi\" id=\"MathJax-Span-825\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-826\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>r</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>j</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-76\">r_{ij}</script> is a comparative rationale highlighting criterion-specific differences.</li>\n    </ul>\n  </li>\n  <li>\n    <p>The following figure shows a comparison of direct assessment and pairwise ranking. Both responses could be considered decent under the umbrella of ‘helpfulness’. However, the scoring decision might change based on a specific evaluation criterion.</p>\n  </li>\n</ul>\n<p>This aligns with pairwise ranking frameworks such as RankNet and duoBERT, and is directly studied in <a href=\"https://arxiv.org/abs/2304.09542\">RankGPT: A Prompt-Based Pairwise Ranking Framework for LLMs</a> by Sun et al. (2023).</p>\n<p>Pairwise prompting has been extended to supervised evaluator LLMs capable of ranking outputs under explicit, user-defined criteria. <a href=\"https://arxiv.org/abs/2405.01535\">Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models</a> by Kim et al. (2024) introduces a unified formulation conditioning on the task, two candidates, optional reference answers, and an evaluation criterion:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-75-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>j</mi></mrow></msub><mo>&amp;#x2208;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>,</mo><mi>j</mi></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-805\" style=\"width: 3.857em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.18em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.18em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-806\"><span class=\"msubsup\" id=\"MathJax-Span-807\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-808\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-809\"><span class=\"mrow\" id=\"MathJax-Span-810\"><span class=\"mi\" id=\"MathJax-Span-811\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-812\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-813\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"texatom\" id=\"MathJax-Span-814\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-815\"><span class=\"mi\" id=\"MathJax-Span-816\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-817\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-818\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>j</mi></mrow></msub><mo>∈</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>,</mo><mi>j</mi></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-75\">y_{ij} \\in {i,j}</script> denotes the preferred candidate and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-76-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>j</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-819\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-820\"><span class=\"msubsup\" id=\"MathJax-Span-821\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-822\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-823\"><span class=\"mrow\" id=\"MathJax-Span-824\"><span class=\"mi\" id=\"MathJax-Span-825\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-826\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>r</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>j</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-76\">r_{ij}</script> is a comparative rationale highlighting criterion-specific differences.</li>\n    </ul>\n<p>The following figure shows a comparison of direct assessment and pairwise ranking. Both responses could be considered decent under the umbrella of ‘helpfulness’. However, the scoring decision might change based on a specific evaluation criterion.</p>\n<p><img src=\"/primers/ai/assets/LLM-as-a-judge/Prometheus_2.jpg\" alt=\"\"></p>\n<ul>\n  <li><a href=\"https://arxiv.org/abs/2405.01535\">Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models</a> by Kim et al. (2024) demonstrates that pointwise and pairwise judges can be unified within a single decoder-only evaluator via weight merging, enabling strong performance across both LTR paradigms.</li>\n</ul>\n<h5 id=\"associated-loss-when-used-for-training\">Associated Loss (When Used for Training)</h5>\n<ul>\n  <li>Pairwise judgments are commonly converted into a logistic loss:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-77-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>pairwise</mtext></mrow></msub><mo>=</mo><mo>&amp;#x2212;</mo><mi>log</mi><mo>&amp;#x2061;</mo><mi>&amp;#x03C3;</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>&amp;#x2212;</mo><msub><mi>s</mi><mi>j</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-827\" style=\"width: 12.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1010.32em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-828\"><span class=\"msubsup\" id=\"MathJax-Span-829\"><span style=\"display: inline-block; position: relative; width: 3.18em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-830\"><span class=\"mrow\" id=\"MathJax-Span-831\"><span class=\"mi\" id=\"MathJax-Span-832\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-833\"><span class=\"mrow\" id=\"MathJax-Span-834\"><span class=\"mtext\" id=\"MathJax-Span-835\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">pairwise</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-836\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-837\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"mi\" id=\"MathJax-Span-838\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-839\"></span><span class=\"mi\" id=\"MathJax-Span-840\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-841\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-842\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-843\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-844\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-845\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-846\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-847\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-848\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-849\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>pairwise</mtext></mrow></msub><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>−</mo><msub><mi>s</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>\n    <p>Pairwise prompting is known to reduce calibration variance relative to absolute scoring, a phenomenon also discussed in\n<a href=\"https://arxiv.org/abs/2203.02155\">Training Language Models to Follow Instructions with Human Feedback</a> by Ouyang et al. (2022).</p>\n  </li>\n  <li>\n    <p>Prometheus 2 further shows that training separate pointwise and pairwise evaluators and merging their parameters yields higher agreement with human rankings than joint multi-task training, providing empirical justification for treating LLM-as-a-Judge systems as flexible LTR models whose optimization objective can shift between pointwise, pairwise, and listwise regimes (<a href=\"https://arxiv.org/abs/2405.01535\">Prometheus 2</a> by Kim et al. (2024)).</p>\n  </li>\n</ul>\n<p>Pairwise prompting is known to reduce calibration variance relative to absolute scoring, a phenomenon also discussed in\n<a href=\"https://arxiv.org/abs/2203.02155\">Training Language Models to Follow Instructions with Human Feedback</a> by Ouyang et al. (2022).</p>\n<p>Prometheus 2 further shows that training separate pointwise and pairwise evaluators and merging their parameters yields higher agreement with human rankings than joint multi-task training, providing empirical justification for treating LLM-as-a-Judge systems as flexible LTR models whose optimization objective can shift between pointwise, pairwise, and listwise regimes (<a href=\"https://arxiv.org/abs/2405.01535\">Prometheus 2</a> by Kim et al. (2024)).</p>\n<h4 id=\"listwise-prompt-based-llm-as-a-judge\">Listwise Prompt-Based LLM-as-a-Judge</h4>\n<ul>\n  <li>Listwise evaluation asks the LLM to reason jointly over an entire set of candidates and produce a ranked ordering or structured list-level judgment.</li>\n</ul>\n<h5 id=\"prompt-patterns\">Prompt Patterns</h5>\n<ul>\n  <li>\n    <p>A common listwise prompt is:</p>\n\n    <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\">  Rank the following outputs from best to worst according to the rubric:\n  1. &lt;candidate 1&gt;\n  2. &lt;candidate 2&gt;\n  3. &lt;candidate 3&gt;\n  Return the ranked order:\n</code></pre></div>    </div>\n  </li>\n  <li>\n    <p>Alternatively, the judge may be asked to assign scores jointly and produce a sorted list.</p>\n  </li>\n</ul>\n<p>A common listwise prompt is:</p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\">  Rank the following outputs from best to worst according to the rubric:\n  1. &lt;candidate 1&gt;\n  2. &lt;candidate 2&gt;\n  3. &lt;candidate 3&gt;\n  Return the ranked order:\n</code></pre>\n<p>Alternatively, the judge may be asked to assign scores jointly and produce a sorted list.</p>\n<h5 id=\"listwise-ltr-interpretation\">Listwise LTR Interpretation</h5>\n<ul>\n  <li>Listwise ranking models a permutation over candidates:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-78-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>&amp;#x03C0;</mi><mo>=</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-850\" style=\"width: 9.378em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.815em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1007.76em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-851\"><span class=\"mi\" id=\"MathJax-Span-852\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-853\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-854\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-855\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-856\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-857\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-858\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-859\"><span class=\"msubsup\" id=\"MathJax-Span-860\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-861\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-862\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-863\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-864\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-865\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-866\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-867\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-868\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-869\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>π</mi><mo>=</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>\n    <p>Unlike pointwise or pairwise methods, listwise approaches optimize ranking quality <strong>at the list level</strong>, capturing interactions between candidates.</p>\n  </li>\n  <li>\n    <p>Prompt-based listwise judging has been explored in recent work such as <a href=\"https://arxiv.org/abs/2505.14432\">Rank-K: Test-Time Reasoning for Listwise Reranking</a> by Yang et al. (2025), where decoder models explicitly reason about entire candidate sets.</p>\n  </li>\n</ul>\n<p>Unlike pointwise or pairwise methods, listwise approaches optimize ranking quality <strong>at the list level</strong>, capturing interactions between candidates.</p>\n<p>Prompt-based listwise judging has been explored in recent work such as <a href=\"https://arxiv.org/abs/2505.14432\">Rank-K: Test-Time Reasoning for Listwise Reranking</a> by Yang et al. (2025), where decoder models explicitly reason about entire candidate sets.</p>\n<h4 id=\"listwise-loss-functions-and-ndcg-optimization\">Listwise Loss Functions and NDCG Optimization</h4>\n<ul>\n  <li>When listwise judgments from an LLM-as-a-Judge are used to train or evaluate ranking models, they are typically connected to <strong>listwise loss functions</strong>. These losses aim to optimize ranking quality <strong>at the list level</strong>, rather than independent scores, and are designed to align with ranking metrics such as NDCG.</li>\n</ul>\n<h5 id=\"normalized-discounted-cumulative-gain-ndcg\">Normalized Discounted Cumulative Gain (NDCG)</h5>\n<ul>\n  <li>\n    <p>One of the most widely used listwise ranking metrics is <strong>NDCG</strong>, introduced in <a href=\"https://dl.acm.org/doi/10.1145/582415.582418\">Cumulated Gain-based Evaluation of IR Techniques</a> by Järvelin and Kekäläinen (2002).</p>\n  </li>\n  <li>\n    <p>For a ranked list of length <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-79-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-870\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-871\"><span class=\"mi\" id=\"MathJax-Span-872\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-79\">k</script>, Discounted Cumulative Gain (DCG) is defined as:</p>\n  </li>\n</ul>\n<p>One of the most widely used listwise ranking metrics is <strong>NDCG</strong>, introduced in <a href=\"https://dl.acm.org/doi/10.1145/582415.582418\">Cumulated Gain-based Evaluation of IR Techniques</a> by Järvelin and Kekäläinen (2002).</p>\n<p>For a ranked list of length <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-79-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-870\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-871\"><span class=\"mi\" id=\"MathJax-Span-872\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-79\">k</script>, Discounted Cumulative Gain (DCG) is defined as:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-80-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>DCG</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>@</mo></mrow><mi>k</mi><mo>=</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi></mrow></munderover><mfrac><mrow><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mi>e</mi><msub><mi>l</mi><mi>i</mi></msub></mrow></msup><mo>&amp;#x2212;</mo><mn>1</mn></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-873\" style=\"width: 13.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.263em, 1010.94em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-874\"><span class=\"mtext\" id=\"MathJax-Span-875\" style=\"font-family: STIXGeneral-Regular;\">DCG</span><span class=\"texatom\" id=\"MathJax-Span-876\"><span class=\"mrow\" id=\"MathJax-Span-877\"><span class=\"mo\" id=\"MathJax-Span-878\" style=\"font-family: STIXGeneral-Regular;\">@</span></span></span><span class=\"mi\" id=\"MathJax-Span-879\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-880\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-881\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-882\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-883\"><span class=\"mrow\" id=\"MathJax-Span-884\"><span class=\"mi\" id=\"MathJax-Span-885\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-886\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-887\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-888\"><span class=\"mrow\" id=\"MathJax-Span-889\"><span class=\"mi\" id=\"MathJax-Span-890\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mfrac\" id=\"MathJax-Span-891\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 4.482em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(2.971em, 1003.18em, 4.326em, -999.997em); top: -4.685em; left: 50%; margin-left: -1.612em;\"><span class=\"mrow\" id=\"MathJax-Span-892\"><span class=\"msubsup\" id=\"MathJax-Span-893\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-894\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-895\"><span class=\"mrow\" id=\"MathJax-Span-896\"><span class=\"mi\" id=\"MathJax-Span-897\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-898\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"msubsup\" id=\"MathJax-Span-899\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-900\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.211em;\"><span class=\"mi\" id=\"MathJax-Span-901\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-902\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mn\" id=\"MathJax-Span-903\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1004.33em, 4.43em, -999.997em); top: -3.331em; left: 50%; margin-left: -2.185em;\"><span class=\"mrow\" id=\"MathJax-Span-904\"><span class=\"msubsup\" id=\"MathJax-Span-905\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-906\" style=\"font-family: STIXGeneral-Regular;\">log</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 1.305em;\"><span class=\"mn\" id=\"MathJax-Span-907\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-908\"></span><span class=\"mo\" id=\"MathJax-Span-909\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-910\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-911\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mn\" id=\"MathJax-Span-912\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span><span class=\"mo\" id=\"MathJax-Span-913\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1004.48em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 4.482em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>DCG</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mo>@</mo></mrow><mi>k</mi><mo>=</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi></mrow></munderover><mfrac><mrow><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mi>e</mi><msub><mi>l</mi><mi>i</mi></msub></mrow></msup><mo>−</mo><mn>1</mn></mrow><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mfrac></math></span></span></div>\n<ul>\n  <li>\n    <p>where:</p>\n\n    <ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-81-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mi>e</mi><msub><mi>l</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-914\" style=\"width: 1.669em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.36em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-915\"><span class=\"mi\" id=\"MathJax-Span-916\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-917\" style=\"font-family: STIXGeneral-Italic;\">e</span><span class=\"msubsup\" id=\"MathJax-Span-918\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-919\" style=\"font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-920\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mi>e</mi><msub><mi>l</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-81\">rel_i</script> is the relevance grade of the item at rank position <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-82-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-921\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-922\"><span class=\"mi\" id=\"MathJax-Span-923\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-82\">i</script></li>\n    </ul>\n  </li>\n  <li>\n    <p>NDCG normalizes DCG by the ideal ranking:</p>\n  </li>\n</ul>\n<p>where:</p>\n<ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-81-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mi>e</mi><msub><mi>l</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-914\" style=\"width: 1.669em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.36em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-915\"><span class=\"mi\" id=\"MathJax-Span-916\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-917\" style=\"font-family: STIXGeneral-Italic;\">e</span><span class=\"msubsup\" id=\"MathJax-Span-918\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-919\" style=\"font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-920\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mi>e</mi><msub><mi>l</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-81\">rel_i</script> is the relevance grade of the item at rank position <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-82-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-921\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-922\"><span class=\"mi\" id=\"MathJax-Span-923\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-82\">i</script></li>\n    </ul>\n<p>NDCG normalizes DCG by the ideal ranking:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-83-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>NDCG</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>@</mo></mrow><mi>k</mi><mo>=</mo><mfrac><mrow><mtext>DCG</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>@</mo></mrow><mi>k</mi></mrow><mrow><mtext>IDCG</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>@</mo></mrow><mi>k</mi></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-924\" style=\"width: 11.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.638em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1009.64em, 3.023em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-925\"><span class=\"mtext\" id=\"MathJax-Span-926\" style=\"font-family: STIXGeneral-Regular;\">NDCG</span><span class=\"texatom\" id=\"MathJax-Span-927\"><span class=\"mrow\" id=\"MathJax-Span-928\"><span class=\"mo\" id=\"MathJax-Span-929\" style=\"font-family: STIXGeneral-Regular;\">@</span></span></span><span class=\"mi\" id=\"MathJax-Span-930\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-931\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-932\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1003.49em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -1.768em;\"><span class=\"mrow\" id=\"MathJax-Span-933\"><span class=\"mtext\" id=\"MathJax-Span-934\" style=\"font-family: STIXGeneral-Regular;\">DCG</span><span class=\"texatom\" id=\"MathJax-Span-935\"><span class=\"mrow\" id=\"MathJax-Span-936\"><span class=\"mo\" id=\"MathJax-Span-937\" style=\"font-family: STIXGeneral-Regular;\">@</span></span></span><span class=\"mi\" id=\"MathJax-Span-938\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.8em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.924em;\"><span class=\"mrow\" id=\"MathJax-Span-939\"><span class=\"mtext\" id=\"MathJax-Span-940\" style=\"font-family: STIXGeneral-Regular;\">IDCG</span><span class=\"texatom\" id=\"MathJax-Span-941\"><span class=\"mrow\" id=\"MathJax-Span-942\"><span class=\"mo\" id=\"MathJax-Span-943\" style=\"font-family: STIXGeneral-Regular;\">@</span></span></span><span class=\"mi\" id=\"MathJax-Span-944\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.96em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.961em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>NDCG</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mo>@</mo></mrow><mi>k</mi><mo>=</mo><mfrac><mrow><mtext>DCG</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mo>@</mo></mrow><mi>k</mi></mrow><mrow><mtext>IDCG</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mo>@</mo></mrow><mi>k</mi></mrow></mfrac></math></span></span></div>\n<ul>\n  <li>This normalization allows ranking quality to be compared across queries with different relevance distributions.</li>\n</ul>\n<h5 id=\"optimizing-ndcg\">Optimizing NDCG</h5>\n<ul>\n  <li>NDCG is <strong>non-differentiable</strong> because it depends on discrete rank positions. As a result, it cannot be optimized directly with gradient descent. Instead, several surrogate strategies are used.</li>\n</ul>\n<h6 id=\"listwise-surrogate-losses\">Listwise Surrogate Losses</h6>\n<ul>\n  <li>\n    <p>A common approach is to optimize a surrogate likelihood over permutations.</p>\n  </li>\n  <li>\n    <p><strong>ListMLE</strong>, introduced in <a href=\"https://dl.acm.org/doi/10.1145/1277741.1277800\">Listwise Approach to Learning to Rank</a> by Cao et al. (2007), models the probability of the ground-truth permutation (\\pi^*):</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-84-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>ListMLE</mtext></mrow></msub><mo>=</mo><mo>&amp;#x2212;</mo><mi>log</mi><mo>&amp;#x2061;</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>&amp;#x03C0;</mi><mo>&amp;#x2217;</mo></msup><mo>&amp;#x2223;</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>s</mi><mi>n</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-945\" style=\"width: 16.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1013.75em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-946\"><span class=\"msubsup\" id=\"MathJax-Span-947\"><span style=\"display: inline-block; position: relative; width: 3.388em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-948\"><span class=\"mrow\" id=\"MathJax-Span-949\"><span class=\"mi\" id=\"MathJax-Span-950\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-951\"><span class=\"mrow\" id=\"MathJax-Span-952\"><span class=\"mtext\" id=\"MathJax-Span-953\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">ListMLE</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-954\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-955\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"mi\" id=\"MathJax-Span-956\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-957\"></span><span class=\"mi\" id=\"MathJax-Span-958\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">P</span><span class=\"mo\" id=\"MathJax-Span-959\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-960\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-961\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-962\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-963\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-964\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-965\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mn\" id=\"MathJax-Span-966\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-967\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-968\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-969\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-970\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-971\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-972\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-973\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>ListMLE</mtext></mrow></msub><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy=\"false\">(</mo><msup><mi>π</mi><mo>∗</mo></msup><mo>∣</mo><msub><mi>s</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>s</mi><mi>n</mi></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-84\">\\mathcal{L}_{\\text{ListMLE}} = -\\log P(\\pi^* \\mid s_1, \\dots, s_n)</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-85-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-974\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-975\"><span class=\"msubsup\" id=\"MathJax-Span-976\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-977\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-978\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-85\">s_i</script> are model scores. This loss encourages the model to assign higher scores to items appearing earlier in the target ranking.</li>\n    </ul>\n  </li>\n</ul>\n<p>A common approach is to optimize a surrogate likelihood over permutations.</p>\n<p><strong>ListMLE</strong>, introduced in <a href=\"https://dl.acm.org/doi/10.1145/1277741.1277800\">Listwise Approach to Learning to Rank</a> by Cao et al. (2007), models the probability of the ground-truth permutation (\\pi^*):</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-85-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-974\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-975\"><span class=\"msubsup\" id=\"MathJax-Span-976\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-977\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-978\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-85\">s_i</script> are model scores. This loss encourages the model to assign higher scores to items appearing earlier in the target ranking.</li>\n    </ul>\n<h6 id=\"gradient-based-approximations-to-ndcg\">Gradient-Based Approximations to NDCG</h6>\n<ul>\n  <li>\n    <p>Another family of methods directly approximates the <strong>gradient of NDCG</strong>.</p>\n  </li>\n  <li>\n    <p><strong>LambdaRank</strong> and <strong>LambdaMART</strong>, described in <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf\">Learning to Rank using Gradient Descent</a> by Burges et al. (2010), compute pairwise gradients weighted by the change in NDCG caused by swapping two items:</p>\n  </li>\n</ul>\n<p>Another family of methods directly approximates the <strong>gradient of NDCG</strong>.</p>\n<p><strong>LambdaRank</strong> and <strong>LambdaMART</strong>, described in <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf\">Learning to Rank using Gradient Descent</a> by Burges et al. (2010), compute pairwise gradients weighted by the change in NDCG caused by swapping two items:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-86-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>&amp;#x03BB;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>&amp;#x221D;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><msub><mtext>NDCG</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-979\" style=\"width: 8.544em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1006.98em, 2.971em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-980\"><span class=\"msubsup\" id=\"MathJax-Span-981\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-982\" style=\"font-family: STIXGeneral-Italic;\">λ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-983\"><span class=\"mrow\" id=\"MathJax-Span-984\"><span class=\"mi\" id=\"MathJax-Span-985\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-986\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-987\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-988\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∝</span><span class=\"texatom\" id=\"MathJax-Span-989\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-990\"><span class=\"mo\" id=\"MathJax-Span-991\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-992\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"msubsup\" id=\"MathJax-Span-993\"><span style=\"display: inline-block; position: relative; width: 3.492em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1002.82em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mtext\" id=\"MathJax-Span-994\" style=\"font-family: STIXGeneral-Regular;\">NDCG</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 2.815em;\"><span class=\"texatom\" id=\"MathJax-Span-995\"><span class=\"mrow\" id=\"MathJax-Span-996\"><span class=\"mi\" id=\"MathJax-Span-997\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-998\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-999\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-1000\"><span class=\"mrow\" id=\"MathJax-Span-1001\"><span class=\"mo\" id=\"MathJax-Span-1002\" style=\"font-family: STIXVariants;\">|</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>λ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>∝</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi mathvariant=\"normal\">Δ</mi><msub><mtext>NDCG</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow></math></span></span></div>\n<ul>\n  <li>This approach preserves the pairwise structure of optimization while explicitly targeting listwise ranking quality.</li>\n</ul>\n<h6 id=\"using-llm-judges-to-approximate-ndcg\">Using LLM Judges to Approximate NDCG</h6>\n<ul>\n  <li>\n    <p>In LLM-as-a-Judge pipelines:</p>\n\n    <ul>\n      <li>The judge produces relevance grades or ranked lists</li>\n      <li>These induce a pseudo ground-truth permutation (\\pi^*)</li>\n      <li>NDCG can be computed directly on judge-derived rankings</li>\n      <li>Or used indirectly to supervise ranking models via listwise losses</li>\n    </ul>\n  </li>\n  <li>\n    <p>This allows <strong>metric-aligned optimization without human annotations</strong>, which is especially valuable in large-scale or continuously evolving systems.</p>\n  </li>\n</ul>\n<p>In LLM-as-a-Judge pipelines:</p>\n<ul>\n      <li>The judge produces relevance grades or ranked lists</li>\n      <li>These induce a pseudo ground-truth permutation (\\pi^*)</li>\n      <li>NDCG can be computed directly on judge-derived rankings</li>\n      <li>Or used indirectly to supervise ranking models via listwise losses</li>\n    </ul>\n<p>This allows <strong>metric-aligned optimization without human annotations</strong>, which is especially valuable in large-scale or continuously evolving systems.</p>\n<h5 id=\"can-categorical-cross-entropy-be-used-for-listwise-ranking\">Can Categorical Cross-Entropy be Used for Listwise Ranking?</h5>\n<ul>\n  <li><strong>Categorical Cross-Entropy (CCE) can be used for listwise ranking</strong>, but only under <strong>specific formulations</strong>, and it is important to understand its limitations.</li>\n</ul>\n<h6 id=\"when-categorical-cross-entropy-applies\">When Categorical Cross-Entropy Applies</h6>\n<ul>\n  <li>Categorical cross-entropy can be used when <strong>listwise ranking is reformulated as a classification problem</strong>, typically in one of the following ways:</li>\n</ul>\n<ol>\n  <li><strong>Top-1 (or Top-k) Prediction</strong>:\n    <ul>\n      <li>\n        <p>The model predicts which item should appear at rank 1 (or within the top <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-87-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1003\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1004\"><span class=\"mi\" id=\"MathJax-Span-1005\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-87\">k</script>).</p>\n      </li>\n      <li>\n        <p>If a probability distribution over candidates is produced:</p>\n      </li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-88-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>i</mi><mo>&amp;#x2223;</mo><mi>q</mi><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1006\" style=\"width: 8.701em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1007.19em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1007\"><span class=\"mi\" id=\"MathJax-Span-1008\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-1009\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1010\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1011\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-1012\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">q</span><span class=\"mo\" id=\"MathJax-Span-1013\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-1014\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-1015\"><span class=\"msubsup\" id=\"MathJax-Span-1016\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1017\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-1018\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1019\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-1020\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-1021\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1022\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1023\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1024\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1025\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>P</mi><mo stretchy=\"false\">(</mo><mi>i</mi><mo>∣</mo><mi>q</mi><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-88\">P(i \\mid q, {x_1,\\dots,x_n})</script>\n\n    <ul>\n      <li>\n        <p>… then categorical cross-entropy applies:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-89-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>CCE</mtext></mrow></msub><mo>=</mo><mo>&amp;#x2212;</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msub><mi>y</mi><mi>i</mi></msub><mi>log</mi><mo>&amp;#x2061;</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>i</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1026\" style=\"width: 11.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.534em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.419em, 1009.48em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1027\"><span class=\"msubsup\" id=\"MathJax-Span-1028\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1029\"><span class=\"mrow\" id=\"MathJax-Span-1030\"><span class=\"mi\" id=\"MathJax-Span-1031\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1032\"><span class=\"mrow\" id=\"MathJax-Span-1033\"><span class=\"mtext\" id=\"MathJax-Span-1034\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">CCE</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1035\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-1036\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"munderover\" id=\"MathJax-Span-1037\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1038\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-1039\"><span class=\"mrow\" id=\"MathJax-Span-1040\"><span class=\"mi\" id=\"MathJax-Span-1041\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1042\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-1043\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1044\"><span class=\"mrow\" id=\"MathJax-Span-1045\"><span class=\"mi\" id=\"MathJax-Span-1046\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1047\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1048\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1049\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-1050\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-1051\"></span><span class=\"mi\" id=\"MathJax-Span-1052\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">P</span><span class=\"mo\" id=\"MathJax-Span-1053\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1054\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1055\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>CCE</mtext></mrow></msub><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msub><mi>y</mi><mi>i</mi></msub><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-89\">\\mathcal{L}_{\\text{CCE}} = -\\sum_{i=1}^{n} y_i \\log P(i)</script>\n\n        <ul>\n          <li>where (y_i) is a one-hot indicator of the correct top-ranked item.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Position-Wise Classification</strong>:\n    <ul>\n      <li>Some models decompose ranking into multiple classification steps, predicting which item belongs at each rank position. Each step uses categorical cross-entropy over remaining candidates.</li>\n    </ul>\n  </li>\n  <li><strong>Softmax-Based Listwise Losses</strong>:\n    <ul>\n      <li>\n        <p>Certain listwise objectives (including simplified versions of ListNet) can be interpreted as categorical cross-entropy between:</p>\n\n        <ul>\n          <li>A target distribution derived from relevance labels</li>\n          <li>A predicted softmax distribution over scores</li>\n        </ul>\n      </li>\n      <li>\n        <p>This perspective is discussed in <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2007-40.pdf\">Learning to Rank: From Pairwise Approach to Listwise Approach</a> by Cao et al. (2007).</p>\n      </li>\n    </ul>\n  </li>\n</ol>\n<ul>\n      <li>\n        <p>The model predicts which item should appear at rank 1 (or within the top <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-87-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1003\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1004\"><span class=\"mi\" id=\"MathJax-Span-1005\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-87\">k</script>).</p>\n      </li>\n      <li>\n        <p>If a probability distribution over candidates is produced:</p>\n      </li>\n    </ul>\n<p>The model predicts which item should appear at rank 1 (or within the top <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-87-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1003\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1004\"><span class=\"mi\" id=\"MathJax-Span-1005\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-87\">k</script>).</p>\n<p>If a probability distribution over candidates is produced:</p>\n<ul>\n      <li>\n        <p>… then categorical cross-entropy applies:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-89-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>CCE</mtext></mrow></msub><mo>=</mo><mo>&amp;#x2212;</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msub><mi>y</mi><mi>i</mi></msub><mi>log</mi><mo>&amp;#x2061;</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>i</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1026\" style=\"width: 11.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.534em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.419em, 1009.48em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1027\"><span class=\"msubsup\" id=\"MathJax-Span-1028\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1029\"><span class=\"mrow\" id=\"MathJax-Span-1030\"><span class=\"mi\" id=\"MathJax-Span-1031\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1032\"><span class=\"mrow\" id=\"MathJax-Span-1033\"><span class=\"mtext\" id=\"MathJax-Span-1034\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">CCE</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1035\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-1036\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"munderover\" id=\"MathJax-Span-1037\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1038\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-1039\"><span class=\"mrow\" id=\"MathJax-Span-1040\"><span class=\"mi\" id=\"MathJax-Span-1041\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1042\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-1043\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1044\"><span class=\"mrow\" id=\"MathJax-Span-1045\"><span class=\"mi\" id=\"MathJax-Span-1046\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1047\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1048\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1049\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-1050\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-1051\"></span><span class=\"mi\" id=\"MathJax-Span-1052\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">P</span><span class=\"mo\" id=\"MathJax-Span-1053\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1054\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1055\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>CCE</mtext></mrow></msub><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msub><mi>y</mi><mi>i</mi></msub><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-89\">\\mathcal{L}_{\\text{CCE}} = -\\sum_{i=1}^{n} y_i \\log P(i)</script>\n\n        <ul>\n          <li>where (y_i) is a one-hot indicator of the correct top-ranked item.</li>\n        </ul>\n      </li>\n    </ul>\n<p>… then categorical cross-entropy applies:</p>\n<ul>\n          <li>where (y_i) is a one-hot indicator of the correct top-ranked item.</li>\n        </ul>\n<ul>\n      <li>Some models decompose ranking into multiple classification steps, predicting which item belongs at each rank position. Each step uses categorical cross-entropy over remaining candidates.</li>\n    </ul>\n<ul>\n      <li>\n        <p>Certain listwise objectives (including simplified versions of ListNet) can be interpreted as categorical cross-entropy between:</p>\n\n        <ul>\n          <li>A target distribution derived from relevance labels</li>\n          <li>A predicted softmax distribution over scores</li>\n        </ul>\n      </li>\n      <li>\n        <p>This perspective is discussed in <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2007-40.pdf\">Learning to Rank: From Pairwise Approach to Listwise Approach</a> by Cao et al. (2007).</p>\n      </li>\n    </ul>\n<p>Certain listwise objectives (including simplified versions of ListNet) can be interpreted as categorical cross-entropy between:</p>\n<ul>\n          <li>A target distribution derived from relevance labels</li>\n          <li>A predicted softmax distribution over scores</li>\n        </ul>\n<p>This perspective is discussed in <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2007-40.pdf\">Learning to Rank: From Pairwise Approach to Listwise Approach</a> by Cao et al. (2007).</p>\n<h6 id=\"limitations-of-categorical-cross-entropy-for-ranking\">Limitations of Categorical Cross-Entropy for Ranking</h6>\n<ul>\n  <li>\n    <p>While usable, categorical cross-entropy has important limitations in ranking contexts:</p>\n\n    <ul>\n      <li>It does <strong>not directly model permutations</strong>, only class probabilities</li>\n      <li>It does <strong>not encode rank position sensitivity</strong> (e.g., top-heavy emphasis)</li>\n      <li>It does <strong>not naturally align with NDCG</strong>, which discounts lower ranks</li>\n      <li>It assumes a single correct label or distribution, which may not reflect graded relevance</li>\n    </ul>\n  </li>\n  <li>\n    <p>As a result, categorical cross-entropy is generally <strong>inferior to NDCG-aware losses</strong> (e.g., ListMLE, LambdaRank) when the goal is high-quality ranking across the entire list.</p>\n  </li>\n</ul>\n<p>While usable, categorical cross-entropy has important limitations in ranking contexts:</p>\n<ul>\n      <li>It does <strong>not directly model permutations</strong>, only class probabilities</li>\n      <li>It does <strong>not encode rank position sensitivity</strong> (e.g., top-heavy emphasis)</li>\n      <li>It does <strong>not naturally align with NDCG</strong>, which discounts lower ranks</li>\n      <li>It assumes a single correct label or distribution, which may not reflect graded relevance</li>\n    </ul>\n<p>As a result, categorical cross-entropy is generally <strong>inferior to NDCG-aware losses</strong> (e.g., ListMLE, LambdaRank) when the goal is high-quality ranking across the entire list.</p>\n<h6 id=\"relationship-to-ndcg\">Relationship to NDCG</h6>\n<ul>\n  <li>\n    <p>Categorical cross-entropy is <strong>not an NDCG-consistent loss</strong>. Optimizing CCE does not guarantee improvements in NDCG, except in restricted cases (e.g., when only top-1 accuracy matters).</p>\n  </li>\n  <li>\n    <p>Therefore, in practice:</p>\n\n    <ul>\n      <li><strong>CCE is acceptable for coarse listwise supervision</strong></li>\n      <li><strong>NDCG-driven or permutation-based losses are preferred for ranking quality</strong></li>\n      <li><strong>LLM-as-a-Judge outputs are often better consumed via NDCG-aligned objectives</strong></li>\n    </ul>\n  </li>\n</ul>\n<p>Categorical cross-entropy is <strong>not an NDCG-consistent loss</strong>. Optimizing CCE does not guarantee improvements in NDCG, except in restricted cases (e.g., when only top-1 accuracy matters).</p>\n<p>Therefore, in practice:</p>\n<ul>\n      <li><strong>CCE is acceptable for coarse listwise supervision</strong></li>\n      <li><strong>NDCG-driven or permutation-based losses are preferred for ranking quality</strong></li>\n      <li><strong>LLM-as-a-Judge outputs are often better consumed via NDCG-aligned objectives</strong></li>\n    </ul>\n<h5 id=\"inputoutput-example-listwise\">Input–Output Example (Listwise)</h5>\n<ul>\n  <li><strong>Input</strong>: Task + 5 candidate summaries</li>\n  <li><strong>LLM Judge Output</strong>: <code class=\"language-plaintext highlighter-rouge\">3 &gt; 1 &gt; 5 &gt; 2 &gt; 4</code></li>\n  <li>\n    <p><strong>Derived Supervision</strong>:</p>\n\n    <ul>\n      <li>Permutation <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-90-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>&amp;#x03C0;</mi><mo>&amp;#x2217;</mo></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1056\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.04em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1057\"><span class=\"msubsup\" id=\"MathJax-Span-1058\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1059\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-1060\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>π</mi><mo>∗</mo></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-90\">\\pi^*</script></li>\n      <li>Optional relevance grades inferred from positions</li>\n      <li>NDCG computed against downstream model outputs</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Derived Supervision</strong>:</p>\n<ul>\n      <li>Permutation <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-90-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>&amp;#x03C0;</mi><mo>&amp;#x2217;</mo></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1056\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.04em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1057\"><span class=\"msubsup\" id=\"MathJax-Span-1058\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1059\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-1060\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>π</mi><mo>∗</mo></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-90\">\\pi^*</script></li>\n      <li>Optional relevance grades inferred from positions</li>\n      <li>NDCG computed against downstream model outputs</li>\n    </ul>\n<h4 id=\"takeaways-2\">Takeaways</h4>\n<ul>\n  <li><strong>Pointwise judging</strong> estimates absolute quality scores for individual outputs</li>\n  <li><strong>Pairwise judging</strong> models relative preferences between two outputs</li>\n  <li><strong>Listwise judging</strong> produces global rankings over entire candidate sets</li>\n  <li><strong>NDCG</strong> is the dominant evaluation metric for listwise ranking quality</li>\n  <li><strong>NDCG is non-differentiable</strong> and therefore requires surrogate optimization methods</li>\n  <li><strong>Listwise losses such as ListMLE and LambdaRank</strong> effectively approximate NDCG during training</li>\n  <li><strong>Categorical cross-entropy (CCE)</strong> can be used in constrained listwise setups (e.g., top-1 or position-wise classification)</li>\n  <li><strong>CCE is generally weaker than NDCG-aware losses</strong> for full-list ranking optimization</li>\n  <li><strong>LLM-as-a-Judge</strong> enables scalable, metric-aligned supervision and generation of listwise training signals</li>\n</ul>",
    "contentMarkdown": "*   Decoder-only LLMs, such as GPT-style architectures, can act directly as ranking judges through prompting, without explicit parameter fine-tuning. In these setups, ranking behavior is induced at inference time via natural language instructions rather than learned scoring heads. Prompt-based LLM-as-a-Judge systems naturally support pointwise, pairwise, and listwise LTR formulations.\n    \n*   This section describes how each paradigm is realized with decoder-only judges and explains, in depth, how listwise losses and NDCG-based objectives arise when listwise judgments are used for optimization or supervision.\n    \n*   Recent work demonstrates that decoder-based judges can be specialized into evaluator LLMs that closely match human and proprietary-LLM judgments under both pointwise and pairwise formulations. Notably, Prometheus-style evaluators formalize LLM-as-a-Judge as rubric-conditioned scoring within a decoder-only architecture ([Prometheus: Inducing Fine-grained Evaluation Capability in Language Models](https://arxiv.org/abs/2310.08491) by Kim et al. (2023); [Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://arxiv.org/abs/2405.01535) by Kim et al. (2024)).\n    \n*   These systems treat evaluation as a conditional generation problem over instructions, candidate outputs, optional reference answers, and explicit evaluation criteria, grounding ranking behavior in structured supervision rather than implicit prompting alone.\n    \n\nDecoder-only LLMs, such as GPT-style architectures, can act directly as ranking judges through prompting, without explicit parameter fine-tuning. In these setups, ranking behavior is induced at inference time via natural language instructions rather than learned scoring heads. Prompt-based LLM-as-a-Judge systems naturally support pointwise, pairwise, and listwise LTR formulations.\n\nThis section describes how each paradigm is realized with decoder-only judges and explains, in depth, how listwise losses and NDCG-based objectives arise when listwise judgments are used for optimization or supervision.\n\nRecent work demonstrates that decoder-based judges can be specialized into evaluator LLMs that closely match human and proprietary-LLM judgments under both pointwise and pairwise formulations. Notably, Prometheus-style evaluators formalize LLM-as-a-Judge as rubric-conditioned scoring within a decoder-only architecture ([Prometheus: Inducing Fine-grained Evaluation Capability in Language Models](https://arxiv.org/abs/2310.08491) by Kim et al. (2023); [Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://arxiv.org/abs/2405.01535) by Kim et al. (2024)).\n\nThese systems treat evaluation as a conditional generation problem over instructions, candidate outputs, optional reference answers, and explicit evaluation criteria, grounding ranking behavior in structured supervision rather than implicit prompting alone.\n\n#### Pointwise Prompt-Based LLM-as-a-Judge\n\n*   In pointwise evaluation, each candidate output is judged independently against a rubric.\n\n##### Prompt Pattern\n\n*   A typical pointwise judge prompt instructs the LLM to:\n    \n    *   Read the task and evaluation criteria\n    *   Evaluate a single candidate output\n    *   Return a discrete label or numeric score\n*   Example:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `You are a judge. Based on the rubric below, assign a score from 1–5.   Task: <task description>   Output: <candidate>   Rubric: <criteria>   Score:`\n    \n\nA typical pointwise judge prompt instructs the LLM to:\n\n*   Read the task and evaluation criteria\n*   Evaluate a single candidate output\n*   Return a discrete label or numeric score\n\nExample:\n\n![](https://aman.ai/images/copy.png)\n\n  `You are a judge. Based on the rubric below, assign a score from 1–5.   Task: <task description>   Output: <candidate>   Rubric: <criteria>   Score:`\n\n##### Interpretation As Pointwise LTR\n\n*   This corresponds to estimating an absolute relevance or quality score:\n    \n    si\\=f(q,xi)si\\=f(q,xi)\n    \n    s\\_i = f(q, x\\_i)\n    *   where qqq is the task specification and xixix\\_i is a single output.\n*   This formulation underlies many practical LLM-as-a-Judge systems, including MT-Bench-style evaluations described in [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) by Zheng et al. (2023).\n    \n*   Beyond pure prompting, pointwise judging has been explicitly modeled via supervised evaluator LLMs trained to emit both natural-language rationales and scalar scores conditioned on user-defined rubrics and reference answers, as demonstrated in [Prometheus: Inducing Fine-grained Evaluation Capability in Language Models](https://arxiv.org/abs/2310.08491) by Kim et al. (2023).\n    \n*   In this formulation, the scoring function is extended to include reference material and evaluation criteria:\n    \n    fdirect:(q,xi,a,e)→(ri,si)fdirect:(q,xi,a,e)→(ri,si)\n    \n    f\\_{\\\\text{direct}} : (q, x\\_i, a, e) \\\\rightarrow (r\\_i, s\\_i)\n    *   where aaa denotes a reference answer, eee a structured score rubric, ririr\\_i a verbal justification, and si∈1,2,3,4,5si∈1,2,3,4,5s\\_i \\\\in {1,2,3,4,5} the pointwise relevance score, enabling fine-grained, rubric-aligned supervision (cf. [Prometheus](https://arxiv.org/abs/2310.08491) by Kim et al. (2023)).\n*   Empirically, [Prometheus: Inducing Fine-grained Evaluation Capability in Language Models](https://arxiv.org/abs/2310.08491) by Kim et al. (2023) shows that rubric-conditioned pointwise judgments substantially improve correlation with human evaluators, achieving Pearson correlations comparable to GPT-4 on MT-Bench and Vicuna Bench.\n    \n*   The following figure ([source](https://arxiv.org/abs/2310.08491)) shows that compared to conventional, coarse-grained LLM evaluation, a fine-grained approach that takes user-defined score rubrics as input.\n    \n\nThis corresponds to estimating an absolute relevance or quality score:\n\n*   where qqq is the task specification and xixix\\_i is a single output.\n\nThis formulation underlies many practical LLM-as-a-Judge systems, including MT-Bench-style evaluations described in [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) by Zheng et al. (2023).\n\nBeyond pure prompting, pointwise judging has been explicitly modeled via supervised evaluator LLMs trained to emit both natural-language rationales and scalar scores conditioned on user-defined rubrics and reference answers, as demonstrated in [Prometheus: Inducing Fine-grained Evaluation Capability in Language Models](https://arxiv.org/abs/2310.08491) by Kim et al. (2023).\n\nIn this formulation, the scoring function is extended to include reference material and evaluation criteria:\n\n*   where aaa denotes a reference answer, eee a structured score rubric, ririr\\_i a verbal justification, and si∈1,2,3,4,5si∈1,2,3,4,5s\\_i \\\\in {1,2,3,4,5} the pointwise relevance score, enabling fine-grained, rubric-aligned supervision (cf. [Prometheus](https://arxiv.org/abs/2310.08491) by Kim et al. (2023)).\n\nEmpirically, [Prometheus: Inducing Fine-grained Evaluation Capability in Language Models](https://arxiv.org/abs/2310.08491) by Kim et al. (2023) shows that rubric-conditioned pointwise judgments substantially improve correlation with human evaluators, achieving Pearson correlations comparable to GPT-4 on MT-Bench and Vicuna Bench.\n\nThe following figure ([source](https://arxiv.org/abs/2310.08491)) shows that compared to conventional, coarse-grained LLM evaluation, a fine-grained approach that takes user-defined score rubrics as input.\n\n![](/primers/ai/assets/LLM-as-a-judge/Prometheus_1.jpg)\n\n##### Implicit Losses\n\n*   While prompt-based judging does not itself involve optimization, pointwise judge outputs are commonly reused as supervision signals when training downstream evaluators or reward models.\n    \n*   In practice, scalar scores produced by judges are treated as regression targets and optimized using mean squared error:\n    \n\nWhile prompt-based judging does not itself involve optimization, pointwise judge outputs are commonly reused as supervision signals when training downstream evaluators or reward models.\n\nIn practice, scalar scores produced by judges are treated as regression targets and optimized using mean squared error:\n\nMSE\\=(si−ŝ i)2LMSE\\=(si−s^i)2\n\n*   When scores are discretized into ordinal buckets (e.g., 1–5), cross-entropy over score classes is also sometimes used, though regression losses are more common in evaluator training.\n    \n*   Evaluator LLMs such as Prometheus are trained directly on rubric-conditioned pointwise supervision by minimizing the expected squared error between predicted and reference scores:\n    \n\nWhen scores are discretized into ordinal buckets (e.g., 1–5), cross-entropy over score classes is also sometimes used, though regression losses are more common in evaluator training.\n\nEvaluator LLMs such as Prometheus are trained directly on rubric-conditioned pointwise supervision by minimizing the expected squared error between predicted and reference scores:\n\ndirect\\=𝔼(q,x,a,e)\\[(si−ŝ i)2\\]Ldirect\\=E(q,x,a,e)\\[(si−s^i)2\\]\n\n*   Importantly, Prometheus jointly models rationale generation and score prediction in a single autoregressive decoding pass, coupling interpretability with scalar accuracy rather than treating explanation and scoring as separate objectives ([Prometheus](https://arxiv.org/abs/2310.08491) by Kim et al. (2023)).\n\n#### Pairwise Prompt-Based LLM-as-a-Judge\n\n*   In pairwise evaluation, the judge compares two candidates and expresses a preference.\n\n##### Prompt Pattern\n\n![](https://aman.ai/images/copy.png)\n\n`You are a judge. Which output is better for the task? Task: <task> Output A: <candidate A> Output B: <candidate B> Answer (A or B):`\n\n![](https://aman.ai/images/copy.png)\n\n`You are a judge. Which output is better for the task? Task: <task> Output A: <candidate A> Output B: <candidate B> Answer (A or B):`\n\n*   This formulation mirrors pairwise preference modeling used in RLHF and ranking systems.\n\n##### Pairwise LTR Interpretation\n\n*   The judge estimates:\n\nP(xi≻xj∣q)P(xi≻xj∣q)\n\n*   This aligns with pairwise ranking frameworks such as RankNet and duoBERT, and is directly studied in [RankGPT: A Prompt-Based Pairwise Ranking Framework for LLMs](https://arxiv.org/abs/2304.09542) by Sun et al. (2023).\n    \n*   Pairwise prompting has been extended to supervised evaluator LLMs capable of ranking outputs under explicit, user-defined criteria. [Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://arxiv.org/abs/2405.01535) by Kim et al. (2024) introduces a unified formulation conditioning on the task, two candidates, optional reference answers, and an evaluation criterion:\n    \n    fpair:(q,xi,xj,a,e)→(rij,yij)fpair:(q,xi,xj,a,e)→(rij,yij)\n    \n    f\\_{\\\\text{pair}} : (q, x\\_i, x\\_j, a, e) \\\\rightarrow (r\\_{ij}, y\\_{ij})\n    *   where yij∈i,jyij∈i,jy\\_{ij} \\\\in {i,j} denotes the preferred candidate and rijrijr\\_{ij} is a comparative rationale highlighting criterion-specific differences.\n*   The following figure shows a comparison of direct assessment and pairwise ranking. Both responses could be considered decent under the umbrella of ‘helpfulness’. However, the scoring decision might change based on a specific evaluation criterion.\n    \n\nThis aligns with pairwise ranking frameworks such as RankNet and duoBERT, and is directly studied in [RankGPT: A Prompt-Based Pairwise Ranking Framework for LLMs](https://arxiv.org/abs/2304.09542) by Sun et al. (2023).\n\nPairwise prompting has been extended to supervised evaluator LLMs capable of ranking outputs under explicit, user-defined criteria. [Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://arxiv.org/abs/2405.01535) by Kim et al. (2024) introduces a unified formulation conditioning on the task, two candidates, optional reference answers, and an evaluation criterion:\n\n*   where yij∈i,jyij∈i,jy\\_{ij} \\\\in {i,j} denotes the preferred candidate and rijrijr\\_{ij} is a comparative rationale highlighting criterion-specific differences.\n\nThe following figure shows a comparison of direct assessment and pairwise ranking. Both responses could be considered decent under the umbrella of ‘helpfulness’. However, the scoring decision might change based on a specific evaluation criterion.\n\n![](/primers/ai/assets/LLM-as-a-judge/Prometheus_2.jpg)\n\n*   [Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://arxiv.org/abs/2405.01535) by Kim et al. (2024) demonstrates that pointwise and pairwise judges can be unified within a single decoder-only evaluator via weight merging, enabling strong performance across both LTR paradigms.\n\n##### Associated Loss (When Used for Training)\n\n*   Pairwise judgments are commonly converted into a logistic loss:\n\npairwise\\=−logσ(si−sj)Lpairwise\\=−log⁡σ(si−sj)\n\n*   Pairwise prompting is known to reduce calibration variance relative to absolute scoring, a phenomenon also discussed in [Training Language Models to Follow Instructions with Human Feedback](https://arxiv.org/abs/2203.02155) by Ouyang et al. (2022).\n    \n*   Prometheus 2 further shows that training separate pointwise and pairwise evaluators and merging their parameters yields higher agreement with human rankings than joint multi-task training, providing empirical justification for treating LLM-as-a-Judge systems as flexible LTR models whose optimization objective can shift between pointwise, pairwise, and listwise regimes ([Prometheus 2](https://arxiv.org/abs/2405.01535) by Kim et al. (2024)).\n    \n\nPairwise prompting is known to reduce calibration variance relative to absolute scoring, a phenomenon also discussed in [Training Language Models to Follow Instructions with Human Feedback](https://arxiv.org/abs/2203.02155) by Ouyang et al. (2022).\n\nPrometheus 2 further shows that training separate pointwise and pairwise evaluators and merging their parameters yields higher agreement with human rankings than joint multi-task training, providing empirical justification for treating LLM-as-a-Judge systems as flexible LTR models whose optimization objective can shift between pointwise, pairwise, and listwise regimes ([Prometheus 2](https://arxiv.org/abs/2405.01535) by Kim et al. (2024)).\n\n#### Listwise Prompt-Based LLM-as-a-Judge\n\n*   Listwise evaluation asks the LLM to reason jointly over an entire set of candidates and produce a ranked ordering or structured list-level judgment.\n\n##### Prompt Patterns\n\n*   A common listwise prompt is:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `Rank the following outputs from best to worst according to the rubric:   1. <candidate 1>   2. <candidate 2>   3. <candidate 3>   Return the ranked order:`\n    \n*   Alternatively, the judge may be asked to assign scores jointly and produce a sorted list.\n    \n\nA common listwise prompt is:\n\n![](https://aman.ai/images/copy.png)\n\n  `Rank the following outputs from best to worst according to the rubric:   1. <candidate 1>   2. <candidate 2>   3. <candidate 3>   Return the ranked order:`\n\nAlternatively, the judge may be asked to assign scores jointly and produce a sorted list.\n\n##### Listwise LTR Interpretation\n\n*   Listwise ranking models a permutation over candidates:\n\nπ\\=f(q,x1,…,xn)π\\=f(q,x1,…,xn)\n\n*   Unlike pointwise or pairwise methods, listwise approaches optimize ranking quality **at the list level**, capturing interactions between candidates.\n    \n*   Prompt-based listwise judging has been explored in recent work such as [Rank-K: Test-Time Reasoning for Listwise Reranking](https://arxiv.org/abs/2505.14432) by Yang et al. (2025), where decoder models explicitly reason about entire candidate sets.\n    \n\nUnlike pointwise or pairwise methods, listwise approaches optimize ranking quality **at the list level**, capturing interactions between candidates.\n\nPrompt-based listwise judging has been explored in recent work such as [Rank-K: Test-Time Reasoning for Listwise Reranking](https://arxiv.org/abs/2505.14432) by Yang et al. (2025), where decoder models explicitly reason about entire candidate sets.\n\n#### Listwise Loss Functions and NDCG Optimization\n\n*   When listwise judgments from an LLM-as-a-Judge are used to train or evaluate ranking models, they are typically connected to **listwise loss functions**. These losses aim to optimize ranking quality **at the list level**, rather than independent scores, and are designed to align with ranking metrics such as NDCG.\n\n##### Normalized Discounted Cumulative Gain (NDCG)\n\n*   One of the most widely used listwise ranking metrics is **NDCG**, introduced in [Cumulated Gain-based Evaluation of IR Techniques](https://dl.acm.org/doi/10.1145/582415.582418) by Järvelin and Kekäläinen (2002).\n    \n*   For a ranked list of length kkk, Discounted Cumulative Gain (DCG) is defined as:\n    \n\nOne of the most widely used listwise ranking metrics is **NDCG**, introduced in [Cumulated Gain-based Evaluation of IR Techniques](https://dl.acm.org/doi/10.1145/582415.582418) by Järvelin and Kekäläinen (2002).\n\nFor a ranked list of length kkk, Discounted Cumulative Gain (DCG) is defined as:\n\nDCG@k\\=∑i\\=1k2reli−1log2(i+1)DCG@k\\=∑i\\=1k2reli−1log2⁡(i+1)\n\n*   where:\n    \n    *   relirelirel\\_i is the relevance grade of the item at rank position iii\n*   NDCG normalizes DCG by the ideal ranking:\n    \n\nwhere:\n\n*   relirelirel\\_i is the relevance grade of the item at rank position iii\n\nNDCG normalizes DCG by the ideal ranking:\n\nNDCG@k\\=DCG@kIDCG@kNDCG@k\\=DCG@kIDCG@k\n\n*   This normalization allows ranking quality to be compared across queries with different relevance distributions.\n\n##### Optimizing NDCG\n\n*   NDCG is **non-differentiable** because it depends on discrete rank positions. As a result, it cannot be optimized directly with gradient descent. Instead, several surrogate strategies are used.\n\n###### Listwise Surrogate Losses\n\n*   A common approach is to optimize a surrogate likelihood over permutations.\n    \n*   **ListMLE**, introduced in [Listwise Approach to Learning to Rank](https://dl.acm.org/doi/10.1145/1277741.1277800) by Cao et al. (2007), models the probability of the ground-truth permutation (\\\\pi^\\*):\n    \n    ListMLE\\=−logP(π∗∣s1,…,sn)LListMLE\\=−log⁡P(π∗∣s1,…,sn)\n    \n    \\\\mathcal{L}\\_{\\\\text{ListMLE}} = -\\\\log P(\\\\pi^\\* \\\\mid s\\_1, \\\\dots, s\\_n)\n    *   where sisis\\_i are model scores. This loss encourages the model to assign higher scores to items appearing earlier in the target ranking.\n\nA common approach is to optimize a surrogate likelihood over permutations.\n\n**ListMLE**, introduced in [Listwise Approach to Learning to Rank](https://dl.acm.org/doi/10.1145/1277741.1277800) by Cao et al. (2007), models the probability of the ground-truth permutation (\\\\pi^\\*):\n\n*   where sisis\\_i are model scores. This loss encourages the model to assign higher scores to items appearing earlier in the target ranking.\n\n###### Gradient-Based Approximations to NDCG\n\n*   Another family of methods directly approximates the **gradient of NDCG**.\n    \n*   **LambdaRank** and **LambdaMART**, described in [Learning to Rank using Gradient Descent](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) by Burges et al. (2010), compute pairwise gradients weighted by the change in NDCG caused by swapping two items:\n    \n\nAnother family of methods directly approximates the **gradient of NDCG**.\n\n**LambdaRank** and **LambdaMART**, described in [Learning to Rank using Gradient Descent](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf) by Burges et al. (2010), compute pairwise gradients weighted by the change in NDCG caused by swapping two items:\n\nλi,j∝|ΔNDCGi,j|λi,j∝|ΔNDCGi,j|\n\n*   This approach preserves the pairwise structure of optimization while explicitly targeting listwise ranking quality.\n\n###### Using LLM Judges to Approximate NDCG\n\n*   In LLM-as-a-Judge pipelines:\n    \n    *   The judge produces relevance grades or ranked lists\n    *   These induce a pseudo ground-truth permutation (\\\\pi^\\*)\n    *   NDCG can be computed directly on judge-derived rankings\n    *   Or used indirectly to supervise ranking models via listwise losses\n*   This allows **metric-aligned optimization without human annotations**, which is especially valuable in large-scale or continuously evolving systems.\n    \n\nIn LLM-as-a-Judge pipelines:\n\n*   The judge produces relevance grades or ranked lists\n*   These induce a pseudo ground-truth permutation (\\\\pi^\\*)\n*   NDCG can be computed directly on judge-derived rankings\n*   Or used indirectly to supervise ranking models via listwise losses\n\nThis allows **metric-aligned optimization without human annotations**, which is especially valuable in large-scale or continuously evolving systems.\n\n##### Can Categorical Cross-Entropy be Used for Listwise Ranking?\n\n*   **Categorical Cross-Entropy (CCE) can be used for listwise ranking**, but only under **specific formulations**, and it is important to understand its limitations.\n\n###### When Categorical Cross-Entropy Applies\n\n*   Categorical cross-entropy can be used when **listwise ranking is reformulated as a classification problem**, typically in one of the following ways:\n\n1.  **Top-1 (or Top-k) Prediction**:\n    \n    *   The model predicts which item should appear at rank 1 (or within the top kkk).\n        \n    *   If a probability distribution over candidates is produced:\n        \n    \n    P(i∣q,x1,…,xn)P(i∣q,x1,…,xn)\n    \n    P(i \\\\mid q, {x\\_1,\\\\dots,x\\_n})\n    *   … then categorical cross-entropy applies:\n        \n        CCE\\=−∑i\\=1nyilogP(i)LCCE\\=−∑i\\=1nyilog⁡P(i)\n        \n        \\\\mathcal{L}\\_{\\\\text{CCE}} = -\\\\sum\\_{i=1}^{n} y\\_i \\\\log P(i)\n        *   where (y\\_i) is a one-hot indicator of the correct top-ranked item.\n2.  **Position-Wise Classification**:\n    *   Some models decompose ranking into multiple classification steps, predicting which item belongs at each rank position. Each step uses categorical cross-entropy over remaining candidates.\n3.  **Softmax-Based Listwise Losses**:\n    *   Certain listwise objectives (including simplified versions of ListNet) can be interpreted as categorical cross-entropy between:\n        \n        *   A target distribution derived from relevance labels\n        *   A predicted softmax distribution over scores\n    *   This perspective is discussed in [Learning to Rank: From Pairwise Approach to Listwise Approach](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2007-40.pdf) by Cao et al. (2007).\n        \n\n*   The model predicts which item should appear at rank 1 (or within the top kkk).\n    \n*   If a probability distribution over candidates is produced:\n    \n\nThe model predicts which item should appear at rank 1 (or within the top kkk).\n\nIf a probability distribution over candidates is produced:\n\n*   … then categorical cross-entropy applies:\n    \n    CCE\\=−∑i\\=1nyilogP(i)LCCE\\=−∑i\\=1nyilog⁡P(i)\n    \n    \\\\mathcal{L}\\_{\\\\text{CCE}} = -\\\\sum\\_{i=1}^{n} y\\_i \\\\log P(i)\n    *   where (y\\_i) is a one-hot indicator of the correct top-ranked item.\n\n… then categorical cross-entropy applies:\n\n*   where (y\\_i) is a one-hot indicator of the correct top-ranked item.\n\n*   Some models decompose ranking into multiple classification steps, predicting which item belongs at each rank position. Each step uses categorical cross-entropy over remaining candidates.\n\n*   Certain listwise objectives (including simplified versions of ListNet) can be interpreted as categorical cross-entropy between:\n    \n    *   A target distribution derived from relevance labels\n    *   A predicted softmax distribution over scores\n*   This perspective is discussed in [Learning to Rank: From Pairwise Approach to Listwise Approach](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2007-40.pdf) by Cao et al. (2007).\n    \n\nCertain listwise objectives (including simplified versions of ListNet) can be interpreted as categorical cross-entropy between:\n\n*   A target distribution derived from relevance labels\n*   A predicted softmax distribution over scores\n\nThis perspective is discussed in [Learning to Rank: From Pairwise Approach to Listwise Approach](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2007-40.pdf) by Cao et al. (2007).\n\n###### Limitations of Categorical Cross-Entropy for Ranking\n\n*   While usable, categorical cross-entropy has important limitations in ranking contexts:\n    \n    *   It does **not directly model permutations**, only class probabilities\n    *   It does **not encode rank position sensitivity** (e.g., top-heavy emphasis)\n    *   It does **not naturally align with NDCG**, which discounts lower ranks\n    *   It assumes a single correct label or distribution, which may not reflect graded relevance\n*   As a result, categorical cross-entropy is generally **inferior to NDCG-aware losses** (e.g., ListMLE, LambdaRank) when the goal is high-quality ranking across the entire list.\n    \n\nWhile usable, categorical cross-entropy has important limitations in ranking contexts:\n\n*   It does **not directly model permutations**, only class probabilities\n*   It does **not encode rank position sensitivity** (e.g., top-heavy emphasis)\n*   It does **not naturally align with NDCG**, which discounts lower ranks\n*   It assumes a single correct label or distribution, which may not reflect graded relevance\n\nAs a result, categorical cross-entropy is generally **inferior to NDCG-aware losses** (e.g., ListMLE, LambdaRank) when the goal is high-quality ranking across the entire list.\n\n###### Relationship to NDCG\n\n*   Categorical cross-entropy is **not an NDCG-consistent loss**. Optimizing CCE does not guarantee improvements in NDCG, except in restricted cases (e.g., when only top-1 accuracy matters).\n    \n*   Therefore, in practice:\n    \n    *   **CCE is acceptable for coarse listwise supervision**\n    *   **NDCG-driven or permutation-based losses are preferred for ranking quality**\n    *   **LLM-as-a-Judge outputs are often better consumed via NDCG-aligned objectives**\n\nCategorical cross-entropy is **not an NDCG-consistent loss**. Optimizing CCE does not guarantee improvements in NDCG, except in restricted cases (e.g., when only top-1 accuracy matters).\n\nTherefore, in practice:\n\n*   **CCE is acceptable for coarse listwise supervision**\n*   **NDCG-driven or permutation-based losses are preferred for ranking quality**\n*   **LLM-as-a-Judge outputs are often better consumed via NDCG-aligned objectives**\n\n##### Input–Output Example (Listwise)\n\n*   **Input**: Task + 5 candidate summaries\n*   **LLM Judge Output**: `3 > 1 > 5 > 2 > 4`\n*   **Derived Supervision**:\n    \n    *   Permutation π∗π∗\\\\pi^\\*\n    *   Optional relevance grades inferred from positions\n    *   NDCG computed against downstream model outputs\n\n**Derived Supervision**:\n\n*   Permutation π∗π∗\\\\pi^\\*\n*   Optional relevance grades inferred from positions\n*   NDCG computed against downstream model outputs\n\n#### Takeaways\n\n*   **Pointwise judging** estimates absolute quality scores for individual outputs\n*   **Pairwise judging** models relative preferences between two outputs\n*   **Listwise judging** produces global rankings over entire candidate sets\n*   **NDCG** is the dominant evaluation metric for listwise ranking quality\n*   **NDCG is non-differentiable** and therefore requires surrogate optimization methods\n*   **Listwise losses such as ListMLE and LambdaRank** effectively approximate NDCG during training\n*   **Categorical cross-entropy (CCE)** can be used in constrained listwise setups (e.g., top-1 or position-wise classification)\n*   **CCE is generally weaker than NDCG-aware losses** for full-list ranking optimization\n*   **LLM-as-a-Judge** enables scalable, metric-aligned supervision and generation of listwise training signals",
    "contentLength": 174094,
    "wordCount": 3455,
    "hasCode": true,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#decoder-based-llm-as-a-judge-for-ltr"
  },
  {
    "id": "ai-LLM-as-a-judge-end-to-end-evaluation-pipelines-with-llm-as-a-judg-20",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Putting It All Together: LLM-as-a-Judge in Modern Evaluation and RAG Pipelines",
    "title": "End-to-End Evaluation Pipelines with LLM-as-a-Judge",
    "order": 20,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>A canonical modern evaluation pipeline looks like:</p>\n\n    <ol>\n      <li>\n        <p><strong>Generation stage</strong>: A base model (or multiple candidate models) produces outputs for a task.</p>\n      </li>\n      <li>\n        <p><strong>Judging stage (LLM-as-a-Judge)</strong>: A judge model evaluates each output using a structured prompt (often pointwise).</p>\n      </li>\n      <li>\n        <p><strong>Aggregation / ranking stage</strong>: Scores are aggregated, thresholded, or ranked to:</p>\n\n        <ul>\n          <li>Select the best output</li>\n          <li>Filter low-quality data</li>\n          <li>Produce training signals</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Optional learning stage</strong>: Judge outputs are used to fine-tune ranking or reward models.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p>This pattern appears in many systems, including OpenAI’s internal evaluation tooling (<a href=\"https://openai.com/blog/evals\">Introducing OpenAI Evals</a>) and Anthropic’s RLAIF pipelines (<a href=\"https://arxiv.org/abs/2302.08582\">Training Language Models from AI Feedback</a> by Bai et al. (2023)).</p>\n  </li>\n</ul>\n<p>A canonical modern evaluation pipeline looks like:</p>\n<ol>\n      <li>\n        <p><strong>Generation stage</strong>: A base model (or multiple candidate models) produces outputs for a task.</p>\n      </li>\n      <li>\n        <p><strong>Judging stage (LLM-as-a-Judge)</strong>: A judge model evaluates each output using a structured prompt (often pointwise).</p>\n      </li>\n      <li>\n        <p><strong>Aggregation / ranking stage</strong>: Scores are aggregated, thresholded, or ranked to:</p>\n\n        <ul>\n          <li>Select the best output</li>\n          <li>Filter low-quality data</li>\n          <li>Produce training signals</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Optional learning stage</strong>: Judge outputs are used to fine-tune ranking or reward models.</p>\n      </li>\n    </ol>\n<p><strong>Generation stage</strong>: A base model (or multiple candidate models) produces outputs for a task.</p>\n<p><strong>Judging stage (LLM-as-a-Judge)</strong>: A judge model evaluates each output using a structured prompt (often pointwise).</p>\n<p><strong>Aggregation / ranking stage</strong>: Scores are aggregated, thresholded, or ranked to:</p>\n<ul>\n          <li>Select the best output</li>\n          <li>Filter low-quality data</li>\n          <li>Produce training signals</li>\n        </ul>\n<p><strong>Optional learning stage</strong>: Judge outputs are used to fine-tune ranking or reward models.</p>\n<p>This pattern appears in many systems, including OpenAI’s internal evaluation tooling (<a href=\"https://openai.com/blog/evals\">Introducing OpenAI Evals</a>) and Anthropic’s RLAIF pipelines (<a href=\"https://arxiv.org/abs/2302.08582\">Training Language Models from AI Feedback</a> by Bai et al. (2023)).</p>",
    "contentMarkdown": "*   A canonical modern evaluation pipeline looks like:\n    \n    1.  **Generation stage**: A base model (or multiple candidate models) produces outputs for a task.\n        \n    2.  **Judging stage (LLM-as-a-Judge)**: A judge model evaluates each output using a structured prompt (often pointwise).\n        \n    3.  **Aggregation / ranking stage**: Scores are aggregated, thresholded, or ranked to:\n        \n        *   Select the best output\n        *   Filter low-quality data\n        *   Produce training signals\n    4.  **Optional learning stage**: Judge outputs are used to fine-tune ranking or reward models.\n        \n*   This pattern appears in many systems, including OpenAI’s internal evaluation tooling ([Introducing OpenAI Evals](https://openai.com/blog/evals)) and Anthropic’s RLAIF pipelines ([Training Language Models from AI Feedback](https://arxiv.org/abs/2302.08582) by Bai et al. (2023)).\n    \n\nA canonical modern evaluation pipeline looks like:\n\n1.  **Generation stage**: A base model (or multiple candidate models) produces outputs for a task.\n    \n2.  **Judging stage (LLM-as-a-Judge)**: A judge model evaluates each output using a structured prompt (often pointwise).\n    \n3.  **Aggregation / ranking stage**: Scores are aggregated, thresholded, or ranked to:\n    \n    *   Select the best output\n    *   Filter low-quality data\n    *   Produce training signals\n4.  **Optional learning stage**: Judge outputs are used to fine-tune ranking or reward models.\n    \n\n**Generation stage**: A base model (or multiple candidate models) produces outputs for a task.\n\n**Judging stage (LLM-as-a-Judge)**: A judge model evaluates each output using a structured prompt (often pointwise).\n\n**Aggregation / ranking stage**: Scores are aggregated, thresholded, or ranked to:\n\n*   Select the best output\n*   Filter low-quality data\n*   Produce training signals\n\n**Optional learning stage**: Judge outputs are used to fine-tune ranking or reward models.\n\nThis pattern appears in many systems, including OpenAI’s internal evaluation tooling ([Introducing OpenAI Evals](https://openai.com/blog/evals)) and Anthropic’s RLAIF pipelines ([Training Language Models from AI Feedback](https://arxiv.org/abs/2302.08582) by Bai et al. (2023)).",
    "contentLength": 2923,
    "wordCount": 280,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#end-to-end-evaluation-pipelines-with-llm-as-a-judge"
  },
  {
    "id": "ai-LLM-as-a-judge-llm-as-a-judge-inside-retrieval-augmented-generati-21",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Putting It All Together: LLM-as-a-Judge in Modern Evaluation and RAG Pipelines",
    "title": "LLM-as-a-Judge Inside Retrieval-Augmented Generation (RAG)",
    "order": 21,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>In RAG systems, LLM-as-a-Judge is often used for <strong>reranking</strong>, <strong>answer validation</strong>, or <strong>end-to-end quality control</strong>.</li>\n</ul>\n<h4 id=\"rag-reranking-with-ltr--llm-judges\">RAG Reranking with LTR + LLM Judges</h4>\n<ul>\n  <li>\n    <p>A typical RAG pipeline:</p>\n\n    <ol>\n      <li>Retrieve <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-91-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1061\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1062\"><span class=\"mi\" id=\"MathJax-Span-1063\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-91\">k</script> candidate documents using a bi-encoder retriever</li>\n      <li>Re-rank candidates using:\n        <ul>\n          <li>Cross-encoder LTR models (monoBERT, ListBERT, ListT5)</li>\n          <li>Or an LLM-as-a-Judge scoring relevance/faithfulness</li>\n        </ul>\n      </li>\n      <li>Generate an answer conditioned on top-ranked documents</li>\n    </ol>\n  </li>\n  <li>\n    <p>This design is discussed in <a href=\"https://arxiv.org/abs/2005.11401\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a> by Lewis et al. (2020).</p>\n  </li>\n  <li>\n    <p>LLM-as-a-Judge is particularly useful when:</p>\n\n    <ul>\n      <li>Relevance is subtle or multi-dimensional</li>\n      <li>Faithfulness to retrieved context must be enforced</li>\n      <li>Ranking criteria go beyond lexical or semantic similarity</li>\n    </ul>\n  </li>\n</ul>\n<p>A typical RAG pipeline:</p>\n<ol>\n      <li>Retrieve <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-91-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1061\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1062\"><span class=\"mi\" id=\"MathJax-Span-1063\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-91\">k</script> candidate documents using a bi-encoder retriever</li>\n      <li>Re-rank candidates using:\n        <ul>\n          <li>Cross-encoder LTR models (monoBERT, ListBERT, ListT5)</li>\n          <li>Or an LLM-as-a-Judge scoring relevance/faithfulness</li>\n        </ul>\n      </li>\n      <li>Generate an answer conditioned on top-ranked documents</li>\n    </ol>\n<ul>\n          <li>Cross-encoder LTR models (monoBERT, ListBERT, ListT5)</li>\n          <li>Or an LLM-as-a-Judge scoring relevance/faithfulness</li>\n        </ul>\n<p>This design is discussed in <a href=\"https://arxiv.org/abs/2005.11401\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a> by Lewis et al. (2020).</p>\n<p>LLM-as-a-Judge is particularly useful when:</p>\n<ul>\n      <li>Relevance is subtle or multi-dimensional</li>\n      <li>Faithfulness to retrieved context must be enforced</li>\n      <li>Ranking criteria go beyond lexical or semantic similarity</li>\n    </ul>",
    "contentMarkdown": "*   In RAG systems, LLM-as-a-Judge is often used for **reranking**, **answer validation**, or **end-to-end quality control**.\n\n#### RAG Reranking with LTR + LLM Judges\n\n*   A typical RAG pipeline:\n    \n    1.  Retrieve kkk candidate documents using a bi-encoder retriever\n    2.  Re-rank candidates using:\n        *   Cross-encoder LTR models (monoBERT, ListBERT, ListT5)\n        *   Or an LLM-as-a-Judge scoring relevance/faithfulness\n    3.  Generate an answer conditioned on top-ranked documents\n*   This design is discussed in [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Lewis et al. (2020).\n    \n*   LLM-as-a-Judge is particularly useful when:\n    \n    *   Relevance is subtle or multi-dimensional\n    *   Faithfulness to retrieved context must be enforced\n    *   Ranking criteria go beyond lexical or semantic similarity\n\nA typical RAG pipeline:\n\n1.  Retrieve kkk candidate documents using a bi-encoder retriever\n2.  Re-rank candidates using:\n    *   Cross-encoder LTR models (monoBERT, ListBERT, ListT5)\n    *   Or an LLM-as-a-Judge scoring relevance/faithfulness\n3.  Generate an answer conditioned on top-ranked documents\n\n*   Cross-encoder LTR models (monoBERT, ListBERT, ListT5)\n*   Or an LLM-as-a-Judge scoring relevance/faithfulness\n\nThis design is discussed in [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Lewis et al. (2020).\n\nLLM-as-a-Judge is particularly useful when:\n\n*   Relevance is subtle or multi-dimensional\n*   Faithfulness to retrieved context must be enforced\n*   Ranking criteria go beyond lexical or semantic similarity",
    "contentLength": 4736,
    "wordCount": 204,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#llm-as-a-judge-inside-retrieval-augmented-generation-(rag)"
  },
  {
    "id": "ai-LLM-as-a-judge-combining-prompted-judges-with-trained-rankers-22",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Putting It All Together: LLM-as-a-Judge in Modern Evaluation and RAG Pipelines",
    "title": "Combining Prompted Judges with Trained Rankers",
    "order": 22,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>In practice, systems often use a <strong>hybrid approach</strong>:</p>\n\n    <ul>\n      <li>\n        <p><strong>LLM-as-a-Judge</strong> for:</p>\n\n        <ul>\n          <li>Bootstrapping labels</li>\n          <li>Evaluating new tasks</li>\n          <li>Handling subjective criteria</li>\n        </ul>\n      </li>\n      <li>\n        <ul>\n          <li>\n            <p><strong>Trained encoder LTR models</strong> for:</p>\n          </li>\n          <li>Low-latency inference</li>\n          <li>Large-scale ranking</li>\n          <li>Stable, repeatable behavior</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>A common pattern:</p>\n\n    <ol>\n      <li>Use an LLM judge to score or rank outputs offline</li>\n      <li>Train a pointwise, pairwise, or listwise ranker on these signals</li>\n      <li>Deploy the ranker in production</li>\n      <li>Periodically refresh supervision with the judge</li>\n    </ol>\n  </li>\n  <li>\n    <p>This mirrors the teacher–student paradigm described in <a href=\"https://arxiv.org/abs/2303.17071\">Distilling Step-by-Step</a> by Magister et al. (2023).</p>\n  </li>\n</ul>\n<p>In practice, systems often use a <strong>hybrid approach</strong>:</p>\n<ul>\n      <li>\n        <p><strong>LLM-as-a-Judge</strong> for:</p>\n\n        <ul>\n          <li>Bootstrapping labels</li>\n          <li>Evaluating new tasks</li>\n          <li>Handling subjective criteria</li>\n        </ul>\n      </li>\n      <li>\n        <ul>\n          <li>\n            <p><strong>Trained encoder LTR models</strong> for:</p>\n          </li>\n          <li>Low-latency inference</li>\n          <li>Large-scale ranking</li>\n          <li>Stable, repeatable behavior</li>\n        </ul>\n      </li>\n    </ul>\n<p><strong>LLM-as-a-Judge</strong> for:</p>\n<ul>\n          <li>Bootstrapping labels</li>\n          <li>Evaluating new tasks</li>\n          <li>Handling subjective criteria</li>\n        </ul>\n<ul>\n          <li>\n            <p><strong>Trained encoder LTR models</strong> for:</p>\n          </li>\n          <li>Low-latency inference</li>\n          <li>Large-scale ranking</li>\n          <li>Stable, repeatable behavior</li>\n        </ul>\n<p><strong>Trained encoder LTR models</strong> for:</p>\n<p>A common pattern:</p>\n<ol>\n      <li>Use an LLM judge to score or rank outputs offline</li>\n      <li>Train a pointwise, pairwise, or listwise ranker on these signals</li>\n      <li>Deploy the ranker in production</li>\n      <li>Periodically refresh supervision with the judge</li>\n    </ol>\n<p>This mirrors the teacher–student paradigm described in <a href=\"https://arxiv.org/abs/2303.17071\">Distilling Step-by-Step</a> by Magister et al. (2023).</p>",
    "contentMarkdown": "*   In practice, systems often use a **hybrid approach**:\n    \n    *   **LLM-as-a-Judge** for:\n        \n        *   Bootstrapping labels\n        *   Evaluating new tasks\n        *   Handling subjective criteria\n    *   *   **Trained encoder LTR models** for:\n            \n        *   Low-latency inference\n        *   Large-scale ranking\n        *   Stable, repeatable behavior\n*   A common pattern:\n    \n    1.  Use an LLM judge to score or rank outputs offline\n    2.  Train a pointwise, pairwise, or listwise ranker on these signals\n    3.  Deploy the ranker in production\n    4.  Periodically refresh supervision with the judge\n*   This mirrors the teacher–student paradigm described in [Distilling Step-by-Step](https://arxiv.org/abs/2303.17071) by Magister et al. (2023).\n    \n\nIn practice, systems often use a **hybrid approach**:\n\n*   **LLM-as-a-Judge** for:\n    \n    *   Bootstrapping labels\n    *   Evaluating new tasks\n    *   Handling subjective criteria\n*   *   **Trained encoder LTR models** for:\n        \n    *   Low-latency inference\n    *   Large-scale ranking\n    *   Stable, repeatable behavior\n\n**LLM-as-a-Judge** for:\n\n*   Bootstrapping labels\n*   Evaluating new tasks\n*   Handling subjective criteria\n\n*   **Trained encoder LTR models** for:\n    \n*   Low-latency inference\n*   Large-scale ranking\n*   Stable, repeatable behavior\n\n**Trained encoder LTR models** for:\n\nA common pattern:\n\n1.  Use an LLM judge to score or rank outputs offline\n2.  Train a pointwise, pairwise, or listwise ranker on these signals\n3.  Deploy the ranker in production\n4.  Periodically refresh supervision with the judge\n\nThis mirrors the teacher–student paradigm described in [Distilling Step-by-Step](https://arxiv.org/abs/2303.17071) by Magister et al. (2023).",
    "contentLength": 2671,
    "wordCount": 219,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#combining-prompted-judges-with-trained-rankers"
  },
  {
    "id": "ai-LLM-as-a-judge-failure-modes-and-limitations-of-llm-as-a-judge-23",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Putting It All Together: LLM-as-a-Judge in Modern Evaluation and RAG Pipelines",
    "title": "Failure Modes and Limitations of LLM-as-a-Judge",
    "order": 23,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>Despite its power, LLM-as-a-Judge has known weaknesses.</li>\n</ul>\n<h4 id=\"bias-and-positional-effects\">Bias and Positional Effects</h4>\n<ul>\n  <li>\n    <p>LLM judges may exhibit:</p>\n\n    <ul>\n      <li>Preference for longer or more verbose outputs</li>\n      <li>Positional bias in pairwise or listwise setups</li>\n      <li>Over-rewarding fluent but incorrect responses</li>\n    </ul>\n  </li>\n  <li>These effects are analyzed in <a href=\"https://arxiv.org/abs/2305.17926\">Large Language Models Are Not Fair Evaluators</a> by Wang et al. (2023).</li>\n  <li>A detailed discourse on biases in LLM-as-a-Judge systems is available in the <a href=\"#biases-and-mitigation-strategies\">Biases and Mitigation Strategies</a> section.</li>\n</ul>\n<p>LLM judges may exhibit:</p>\n<ul>\n      <li>Preference for longer or more verbose outputs</li>\n      <li>Positional bias in pairwise or listwise setups</li>\n      <li>Over-rewarding fluent but incorrect responses</li>\n    </ul>\n<h4 id=\"prompt-sensitivity-and-variance\">Prompt Sensitivity and Variance</h4>\n<ul>\n  <li>\n    <p>Judge outputs can vary with:</p>\n\n    <ul>\n      <li>Prompt phrasing</li>\n      <li>Ordering of criteria</li>\n      <li>Presence or absence of examples</li>\n    </ul>\n  </li>\n  <li>\n    <p>Mitigations include:</p>\n\n    <ul>\n      <li>Anchored examples (as in your prompt)</li>\n      <li>Schema-constrained outputs</li>\n      <li>Majority voting or self-consistency</li>\n    </ul>\n  </li>\n  <li>\n    <p>See <a href=\"https://arxiv.org/abs/2203.11171\">Self-Consistency Improves Chain of Thought Reasoning</a> by Wang et al. (2022).</p>\n  </li>\n</ul>\n<p>Judge outputs can vary with:</p>\n<ul>\n      <li>Prompt phrasing</li>\n      <li>Ordering of criteria</li>\n      <li>Presence or absence of examples</li>\n    </ul>\n<p>Mitigations include:</p>\n<ul>\n      <li>Anchored examples (as in your prompt)</li>\n      <li>Schema-constrained outputs</li>\n      <li>Majority voting or self-consistency</li>\n    </ul>\n<p>See <a href=\"https://arxiv.org/abs/2203.11171\">Self-Consistency Improves Chain of Thought Reasoning</a> by Wang et al. (2022).</p>\n<h4 id=\"over-optimization-and-reward-hacking\">Over-Optimization and Reward Hacking</h4>\n<ul>\n  <li>\n    <p>When models are trained directly against a fixed judge:</p>\n\n    <ul>\n      <li>They may exploit blind spots in the rubric</li>\n      <li>Scores may increase without true quality gains</li>\n    </ul>\n  </li>\n  <li>\n    <p>This is analogous to reward hacking in RL, discussed in <a href=\"https://arxiv.org/abs/1903.03400\">Specification Gaming</a> by Krakovna et al. (2020).</p>\n  </li>\n</ul>\n<p>When models are trained directly against a fixed judge:</p>\n<ul>\n      <li>They may exploit blind spots in the rubric</li>\n      <li>Scores may increase without true quality gains</li>\n    </ul>\n<p>This is analogous to reward hacking in RL, discussed in <a href=\"https://arxiv.org/abs/1903.03400\">Specification Gaming</a> by Krakovna et al. (2020).</p>",
    "contentMarkdown": "*   Despite its power, LLM-as-a-Judge has known weaknesses.\n\n#### Bias and Positional Effects\n\n*   LLM judges may exhibit:\n    \n    *   Preference for longer or more verbose outputs\n    *   Positional bias in pairwise or listwise setups\n    *   Over-rewarding fluent but incorrect responses\n*   These effects are analyzed in [Large Language Models Are Not Fair Evaluators](https://arxiv.org/abs/2305.17926) by Wang et al. (2023).\n*   A detailed discourse on biases in LLM-as-a-Judge systems is available in the [Biases and Mitigation Strategies](#biases-and-mitigation-strategies) section.\n\nLLM judges may exhibit:\n\n*   Preference for longer or more verbose outputs\n*   Positional bias in pairwise or listwise setups\n*   Over-rewarding fluent but incorrect responses\n\n#### Prompt Sensitivity and Variance\n\n*   Judge outputs can vary with:\n    \n    *   Prompt phrasing\n    *   Ordering of criteria\n    *   Presence or absence of examples\n*   Mitigations include:\n    \n    *   Anchored examples (as in your prompt)\n    *   Schema-constrained outputs\n    *   Majority voting or self-consistency\n*   See [Self-Consistency Improves Chain of Thought Reasoning](https://arxiv.org/abs/2203.11171) by Wang et al. (2022).\n    \n\nJudge outputs can vary with:\n\n*   Prompt phrasing\n*   Ordering of criteria\n*   Presence or absence of examples\n\nMitigations include:\n\n*   Anchored examples (as in your prompt)\n*   Schema-constrained outputs\n*   Majority voting or self-consistency\n\nSee [Self-Consistency Improves Chain of Thought Reasoning](https://arxiv.org/abs/2203.11171) by Wang et al. (2022).\n\n#### Over-Optimization and Reward Hacking\n\n*   When models are trained directly against a fixed judge:\n    \n    *   They may exploit blind spots in the rubric\n    *   Scores may increase without true quality gains\n*   This is analogous to reward hacking in RL, discussed in [Specification Gaming](https://arxiv.org/abs/1903.03400) by Krakovna et al. (2020).\n    \n\nWhen models are trained directly against a fixed judge:\n\n*   They may exploit blind spots in the rubric\n*   Scores may increase without true quality gains\n\nThis is analogous to reward hacking in RL, discussed in [Specification Gaming](https://arxiv.org/abs/1903.03400) by Krakovna et al. (2020).",
    "contentLength": 2963,
    "wordCount": 297,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#failure-modes-and-limitations-of-llm-as-a-judge"
  },
  {
    "id": "ai-LLM-as-a-judge-best-practices-for-production-use-24",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Putting It All Together: LLM-as-a-Judge in Modern Evaluation and RAG Pipelines",
    "title": "Best Practices for Production Use",
    "order": 24,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>\n    <p>Empirically grounded best practices include:</p>\n\n    <ul>\n      <li>Use <strong>pointwise judging</strong> by default; escalate to pairwise/listwise only when needed</li>\n      <li>Keep rubrics simple and interpretable</li>\n      <li>Separate hard constraints (binary) from subjective criteria (Likert-type)</li>\n      <li>Periodically revalidate judge alignment with humans</li>\n      <li>Avoid training directly against a single static judge</li>\n    </ul>\n  </li>\n  <li>\n    <p>These principles align with guidance from <a href=\"https://arxiv.org/abs/2307.03109\">How to Evaluate Language Models: A Survey</a> by Chang et al. (2023).</p>\n  </li>\n</ul>\n<p>Empirically grounded best practices include:</p>\n<ul>\n      <li>Use <strong>pointwise judging</strong> by default; escalate to pairwise/listwise only when needed</li>\n      <li>Keep rubrics simple and interpretable</li>\n      <li>Separate hard constraints (binary) from subjective criteria (Likert-type)</li>\n      <li>Periodically revalidate judge alignment with humans</li>\n      <li>Avoid training directly against a single static judge</li>\n    </ul>\n<p>These principles align with guidance from <a href=\"https://arxiv.org/abs/2307.03109\">How to Evaluate Language Models: A Survey</a> by Chang et al. (2023).</p>",
    "contentMarkdown": "*   Empirically grounded best practices include:\n    \n    *   Use **pointwise judging** by default; escalate to pairwise/listwise only when needed\n    *   Keep rubrics simple and interpretable\n    *   Separate hard constraints (binary) from subjective criteria (Likert-type)\n    *   Periodically revalidate judge alignment with humans\n    *   Avoid training directly against a single static judge\n*   These principles align with guidance from [How to Evaluate Language Models: A Survey](https://arxiv.org/abs/2307.03109) by Chang et al. (2023).\n    \n\nEmpirically grounded best practices include:\n\n*   Use **pointwise judging** by default; escalate to pairwise/listwise only when needed\n*   Keep rubrics simple and interpretable\n*   Separate hard constraints (binary) from subjective criteria (Likert-type)\n*   Periodically revalidate judge alignment with humans\n*   Avoid training directly against a single static judge\n\nThese principles align with guidance from [How to Evaluate Language Models: A Survey](https://arxiv.org/abs/2307.03109) by Chang et al. (2023).",
    "contentLength": 1293,
    "wordCount": 134,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#best-practices-for-production-use"
  },
  {
    "id": "ai-LLM-as-a-judge-summary-and-outlook-25",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Putting It All Together: LLM-as-a-Judge in Modern Evaluation and RAG Pipelines",
    "title": "Summary and Outlook",
    "order": 25,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>\n    <p>LLM-as-a-Judge represents a shift from brittle, task-specific metrics toward <strong>learned, semantic evaluation</strong>. When combined with Learning-to-Rank methods and neural ranking models:</p>\n\n    <ul>\n      <li>It enables scalable, flexible evaluation</li>\n      <li>It provides supervision for ranking and reward models</li>\n      <li>It integrates naturally into RAG and alignment pipelines</li>\n    </ul>\n  </li>\n  <li>\n    <p>Future directions include:</p>\n\n    <ul>\n      <li>Multi-judge ensembles</li>\n      <li>Calibrated uncertainty estimation</li>\n      <li>Hybrid human–AI evaluation loops</li>\n      <li>Formal guarantees on judge consistency</li>\n    </ul>\n  </li>\n</ul>\n<p>LLM-as-a-Judge represents a shift from brittle, task-specific metrics toward <strong>learned, semantic evaluation</strong>. When combined with Learning-to-Rank methods and neural ranking models:</p>\n<ul>\n      <li>It enables scalable, flexible evaluation</li>\n      <li>It provides supervision for ranking and reward models</li>\n      <li>It integrates naturally into RAG and alignment pipelines</li>\n    </ul>\n<p>Future directions include:</p>\n<ul>\n      <li>Multi-judge ensembles</li>\n      <li>Calibrated uncertainty estimation</li>\n      <li>Hybrid human–AI evaluation loops</li>\n      <li>Formal guarantees on judge consistency</li>\n    </ul>",
    "contentMarkdown": "*   LLM-as-a-Judge represents a shift from brittle, task-specific metrics toward **learned, semantic evaluation**. When combined with Learning-to-Rank methods and neural ranking models:\n    \n    *   It enables scalable, flexible evaluation\n    *   It provides supervision for ranking and reward models\n    *   It integrates naturally into RAG and alignment pipelines\n*   Future directions include:\n    \n    *   Multi-judge ensembles\n    *   Calibrated uncertainty estimation\n    *   Hybrid human–AI evaluation loops\n    *   Formal guarantees on judge consistency\n\nLLM-as-a-Judge represents a shift from brittle, task-specific metrics toward **learned, semantic evaluation**. When combined with Learning-to-Rank methods and neural ranking models:\n\n*   It enables scalable, flexible evaluation\n*   It provides supervision for ranking and reward models\n*   It integrates naturally into RAG and alignment pipelines\n\nFuture directions include:\n\n*   Multi-judge ensembles\n*   Calibrated uncertainty estimation\n*   Hybrid human–AI evaluation loops\n*   Formal guarantees on judge consistency",
    "contentLength": 1360,
    "wordCount": 134,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#summary-and-outlook"
  },
  {
    "id": "ai-LLM-as-a-judge-motivation-why-a-single-judge-is-not-enough-26",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Panel / Jury of LLMs-as-Judges",
    "title": "Motivation: Why a Single Judge is Not Enough",
    "order": 26,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>Most LLM-as-a-Judge systems rely on a <strong>single strong evaluator</strong>, often a frontier proprietary model such as GPT-4. While convenient, this design has several fundamental limitations:</p>\n\n    <ol>\n      <li>\n        <p><strong>Intra-model bias</strong>: Judge models tend to recognize and favor outputs that are stylistically, semantically, or distributionally similar to their own generations. This self-preference effect is empirically documented in <a href=\"https://arxiv.org/abs/2401.01917\">LLM Evaluators Recognize and Favor Their Own Generations</a> by Panickssery et al. (2024).</p>\n      </li>\n      <li>\n        <p><strong>High variance and prompt sensitivity</strong>: Small changes in prompt wording or formatting can produce large swings in evaluation outcomes, particularly for large models that over-reason or inject external knowledge. This instability is demonstrated in <a href=\"https://arxiv.org/abs/2306.05685\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> by Zheng et al. (2023).</p>\n      </li>\n      <li>\n        <p><strong>Cost and latency at scale</strong>: Using a single frontier model as a judge is expensive and slow, limiting scalability, accessibility, and continuous evaluation.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p><a href=\"https://arxiv.org/abs/2404.18796\">Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models</a> by Verga et al. (2024) empirically show that even very strong models such as GPT-4 can be <strong>surprisingly weak or unstable judges</strong> in certain settings, motivating a shift away from single-oracle evaluation.</p>\n  </li>\n  <li>\n    <p>The core insight is that <strong>evaluation suffers from the same variance and subjectivity problems as human annotation</strong>, and therefore benefits from aggregation rather than reliance on a single rater—an idea long established in IR evaluation practice.</p>\n  </li>\n</ul>\n<p>Most LLM-as-a-Judge systems rely on a <strong>single strong evaluator</strong>, often a frontier proprietary model such as GPT-4. While convenient, this design has several fundamental limitations:</p>\n<ol>\n      <li>\n        <p><strong>Intra-model bias</strong>: Judge models tend to recognize and favor outputs that are stylistically, semantically, or distributionally similar to their own generations. This self-preference effect is empirically documented in <a href=\"https://arxiv.org/abs/2401.01917\">LLM Evaluators Recognize and Favor Their Own Generations</a> by Panickssery et al. (2024).</p>\n      </li>\n      <li>\n        <p><strong>High variance and prompt sensitivity</strong>: Small changes in prompt wording or formatting can produce large swings in evaluation outcomes, particularly for large models that over-reason or inject external knowledge. This instability is demonstrated in <a href=\"https://arxiv.org/abs/2306.05685\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> by Zheng et al. (2023).</p>\n      </li>\n      <li>\n        <p><strong>Cost and latency at scale</strong>: Using a single frontier model as a judge is expensive and slow, limiting scalability, accessibility, and continuous evaluation.</p>\n      </li>\n    </ol>\n<p><strong>Intra-model bias</strong>: Judge models tend to recognize and favor outputs that are stylistically, semantically, or distributionally similar to their own generations. This self-preference effect is empirically documented in <a href=\"https://arxiv.org/abs/2401.01917\">LLM Evaluators Recognize and Favor Their Own Generations</a> by Panickssery et al. (2024).</p>\n<p><strong>High variance and prompt sensitivity</strong>: Small changes in prompt wording or formatting can produce large swings in evaluation outcomes, particularly for large models that over-reason or inject external knowledge. This instability is demonstrated in <a href=\"https://arxiv.org/abs/2306.05685\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> by Zheng et al. (2023).</p>\n<p><strong>Cost and latency at scale</strong>: Using a single frontier model as a judge is expensive and slow, limiting scalability, accessibility, and continuous evaluation.</p>\n<p><a href=\"https://arxiv.org/abs/2404.18796\">Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models</a> by Verga et al. (2024) empirically show that even very strong models such as GPT-4 can be <strong>surprisingly weak or unstable judges</strong> in certain settings, motivating a shift away from single-oracle evaluation.</p>\n<p>The core insight is that <strong>evaluation suffers from the same variance and subjectivity problems as human annotation</strong>, and therefore benefits from aggregation rather than reliance on a single rater—an idea long established in IR evaluation practice.</p>",
    "contentMarkdown": "*   Most LLM-as-a-Judge systems rely on a **single strong evaluator**, often a frontier proprietary model such as GPT-4. While convenient, this design has several fundamental limitations:\n    \n    1.  **Intra-model bias**: Judge models tend to recognize and favor outputs that are stylistically, semantically, or distributionally similar to their own generations. This self-preference effect is empirically documented in [LLM Evaluators Recognize and Favor Their Own Generations](https://arxiv.org/abs/2401.01917) by Panickssery et al. (2024).\n        \n    2.  **High variance and prompt sensitivity**: Small changes in prompt wording or formatting can produce large swings in evaluation outcomes, particularly for large models that over-reason or inject external knowledge. This instability is demonstrated in [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) by Zheng et al. (2023).\n        \n    3.  **Cost and latency at scale**: Using a single frontier model as a judge is expensive and slow, limiting scalability, accessibility, and continuous evaluation.\n        \n*   [Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models](https://arxiv.org/abs/2404.18796) by Verga et al. (2024) empirically show that even very strong models such as GPT-4 can be **surprisingly weak or unstable judges** in certain settings, motivating a shift away from single-oracle evaluation.\n    \n*   The core insight is that **evaluation suffers from the same variance and subjectivity problems as human annotation**, and therefore benefits from aggregation rather than reliance on a single rater—an idea long established in IR evaluation practice.\n    \n\nMost LLM-as-a-Judge systems rely on a **single strong evaluator**, often a frontier proprietary model such as GPT-4. While convenient, this design has several fundamental limitations:\n\n1.  **Intra-model bias**: Judge models tend to recognize and favor outputs that are stylistically, semantically, or distributionally similar to their own generations. This self-preference effect is empirically documented in [LLM Evaluators Recognize and Favor Their Own Generations](https://arxiv.org/abs/2401.01917) by Panickssery et al. (2024).\n    \n2.  **High variance and prompt sensitivity**: Small changes in prompt wording or formatting can produce large swings in evaluation outcomes, particularly for large models that over-reason or inject external knowledge. This instability is demonstrated in [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) by Zheng et al. (2023).\n    \n3.  **Cost and latency at scale**: Using a single frontier model as a judge is expensive and slow, limiting scalability, accessibility, and continuous evaluation.\n    \n\n**Intra-model bias**: Judge models tend to recognize and favor outputs that are stylistically, semantically, or distributionally similar to their own generations. This self-preference effect is empirically documented in [LLM Evaluators Recognize and Favor Their Own Generations](https://arxiv.org/abs/2401.01917) by Panickssery et al. (2024).\n\n**High variance and prompt sensitivity**: Small changes in prompt wording or formatting can produce large swings in evaluation outcomes, particularly for large models that over-reason or inject external knowledge. This instability is demonstrated in [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) by Zheng et al. (2023).\n\n**Cost and latency at scale**: Using a single frontier model as a judge is expensive and slow, limiting scalability, accessibility, and continuous evaluation.\n\n[Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models](https://arxiv.org/abs/2404.18796) by Verga et al. (2024) empirically show that even very strong models such as GPT-4 can be **surprisingly weak or unstable judges** in certain settings, motivating a shift away from single-oracle evaluation.\n\nThe core insight is that **evaluation suffers from the same variance and subjectivity problems as human annotation**, and therefore benefits from aggregation rather than reliance on a single rater—an idea long established in IR evaluation practice.",
    "contentLength": 4805,
    "wordCount": 551,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#motivation:-why-a-single-judge-is-not-enough"
  },
  {
    "id": "ai-LLM-as-a-judge-panel-of-llm-evaluators-poll-core-concept-27",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Panel / Jury of LLMs-as-Judges",
    "title": "Panel of LLM Evaluators (PoLL): Core Concept",
    "order": 27,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>A <strong>Panel of LLM Evaluators (PoLL)</strong> replaces a single judge <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-92-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>J</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1064\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1065\"><span class=\"mi\" id=\"MathJax-Span-1066\" style=\"font-family: STIXGeneral-Italic;\">J<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>J</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-92\">J</script> with a set of heterogeneous judges:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-93-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>J</mi><mn>1</mn></msub><mo>,</mo><msub><mi>J</mi><mn>2</mn></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>J</mi><mi>k</mi></msub></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1067\" style=\"width: 6.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.211em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.21em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1068\"><span class=\"texatom\" id=\"MathJax-Span-1069\"><span class=\"mrow\" id=\"MathJax-Span-1070\"><span class=\"msubsup\" id=\"MathJax-Span-1071\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1072\" style=\"font-family: STIXGeneral-Italic;\">J<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-1073\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1074\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1075\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1076\" style=\"font-family: STIXGeneral-Italic;\">J<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-1077\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1078\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-1079\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-1080\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1081\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1082\" style=\"font-family: STIXGeneral-Italic;\">J<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1083\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>J</mi><mn>1</mn></msub><mo>,</mo><msub><mi>J</mi><mn>2</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>J</mi><mi>k</mi></msub></mrow></math></span></span></div>\n<ul>\n  <li>\n    <p>Ideally, these judges are drawn from <strong>different model families</strong>.</p>\n  </li>\n  <li>\n    <p>Instead of computing an evaluation score as:</p>\n  </li>\n</ul>\n<p>Ideally, these judges are drawn from <strong>different model families</strong>.</p>\n<p>Instead of computing an evaluation score as:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-94-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>score</mtext><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>J</mi><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1084\" style=\"width: 7.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.94em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1085\"><span class=\"mtext\" id=\"MathJax-Span-1086\" style=\"font-family: STIXGeneral-Regular;\">score</span><span class=\"mo\" id=\"MathJax-Span-1087\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1088\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1089\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1090\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1091\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">J<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1092\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1093\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1094\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>score</mtext><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>J</mi><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>\n    <p>PoLL computes:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-95-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>score</mtext><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>f</mi><mrow class=&quot;MJX-TeXAtom-OPEN&quot;><mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;>(</mo></mrow><msub><mi>J</mi><mn>1</mn></msub><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo><mo>,</mo><msub><mi>J</mi><mn>2</mn></msub><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>J</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-CLOSE&quot;><mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;>)</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1095\" style=\"width: 17.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.482em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.086em, 1014.33em, 3.753em, -999.997em); top: -3.174em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1096\"><span class=\"mtext\" id=\"MathJax-Span-1097\" style=\"font-family: STIXGeneral-Regular;\">score</span><span class=\"mo\" id=\"MathJax-Span-1098\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1099\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1100\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1101\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1102\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-1103\"><span class=\"mrow\" id=\"MathJax-Span-1104\"><span class=\"mo\" id=\"MathJax-Span-1105\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">(</span></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1106\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1107\" style=\"font-family: STIXGeneral-Italic;\">J<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-1108\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1109\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1110\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1111\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1112\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1113\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1114\" style=\"font-family: STIXGeneral-Italic;\">J<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-1115\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1116\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1117\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1118\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1119\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-1120\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-1121\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1122\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1123\" style=\"font-family: STIXGeneral-Italic;\">J<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1124\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1125\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1126\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1127\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"texatom\" id=\"MathJax-Span-1128\"><span class=\"mrow\" id=\"MathJax-Span-1129\"><span class=\"mo\" id=\"MathJax-Span-1130\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">)</span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.18em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>score</mtext><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>f</mi><mrow class=\"MJX-TeXAtom-OPEN\"><mo maxsize=\"1.2em\" minsize=\"1.2em\">(</mo></mrow><msub><mi>J</mi><mn>1</mn></msub><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo><mo>,</mo><msub><mi>J</mi><mn>2</mn></msub><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>J</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-CLOSE\"><mo maxsize=\"1.2em\" minsize=\"1.2em\">)</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-95\">\\text{score}(a) = f\\bigl(J_1(a), J_2(a), \\dots, J_k(a)\\bigr)</script>\n\n    <ul>\n      <li>\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-96-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>a</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1131\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1132\"><span class=\"mi\" id=\"MathJax-Span-1133\" style=\"font-family: STIXGeneral-Italic;\">a</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>a</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-96\">a</script> is the model-generated output being evaluated</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-97-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>J</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1134\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.77em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1135\"><span class=\"msubsup\" id=\"MathJax-Span-1136\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1137\" style=\"font-family: STIXGeneral-Italic;\">J<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1138\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1139\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1140\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1141\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>J</mi><mi>i</mi></msub><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-97\">J_i(a)</script> is the judgment from the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-98-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1142\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1143\"><span class=\"mi\" id=\"MathJax-Span-1144\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-98\">i</script>-th evaluator</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-99-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1145\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1146\"><span class=\"mi\" id=\"MathJax-Span-1147\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-99\">f</script> is an aggregation (pooling or voting) function</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>This formulation mirrors classical techniques for reducing annotator variance in information retrieval and QA evaluation, as discussed in <a href=\"https://dl.acm.org/doi/10.1145/290941.291017\">Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness</a> by Voorhees (1998).</p>\n  </li>\n</ul>\n<p>PoLL computes:</p>\n<ul>\n      <li>\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-96-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>a</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1131\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1132\"><span class=\"mi\" id=\"MathJax-Span-1133\" style=\"font-family: STIXGeneral-Italic;\">a</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>a</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-96\">a</script> is the model-generated output being evaluated</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-97-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>J</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1134\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.77em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1135\"><span class=\"msubsup\" id=\"MathJax-Span-1136\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1137\" style=\"font-family: STIXGeneral-Italic;\">J<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1138\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1139\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1140\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1141\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>J</mi><mi>i</mi></msub><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-97\">J_i(a)</script> is the judgment from the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-98-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1142\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1143\"><span class=\"mi\" id=\"MathJax-Span-1144\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-98\">i</script>-th evaluator</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-99-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1145\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1146\"><span class=\"mi\" id=\"MathJax-Span-1147\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-99\">f</script> is an aggregation (pooling or voting) function</li>\n        </ul>\n      </li>\n    </ul>\n<p>where:</p>\n<ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-96-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>a</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1131\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1132\"><span class=\"mi\" id=\"MathJax-Span-1133\" style=\"font-family: STIXGeneral-Italic;\">a</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>a</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-96\">a</script> is the model-generated output being evaluated</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-97-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>J</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1134\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.77em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1135\"><span class=\"msubsup\" id=\"MathJax-Span-1136\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1137\" style=\"font-family: STIXGeneral-Italic;\">J<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1138\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1139\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1140\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1141\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>J</mi><mi>i</mi></msub><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-97\">J_i(a)</script> is the judgment from the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-98-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1142\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1143\"><span class=\"mi\" id=\"MathJax-Span-1144\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-98\">i</script>-th evaluator</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-99-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1145\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1146\"><span class=\"mi\" id=\"MathJax-Span-1147\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-99\">f</script> is an aggregation (pooling or voting) function</li>\n        </ul>\n<p>This formulation mirrors classical techniques for reducing annotator variance in information retrieval and QA evaluation, as discussed in <a href=\"https://dl.acm.org/doi/10.1145/290941.291017\">Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness</a> by Voorhees (1998).</p>",
    "contentMarkdown": "*   A **Panel of LLM Evaluators (PoLL)** replaces a single judge JJJ with a set of heterogeneous judges:\n\nJ1,J2,…,JkJ1,J2,…,Jk\n\n*   Ideally, these judges are drawn from **different model families**.\n    \n*   Instead of computing an evaluation score as:\n    \n\nIdeally, these judges are drawn from **different model families**.\n\nInstead of computing an evaluation score as:\n\nscore(a)\\=J(a)score(a)\\=J(a)\n\n*   PoLL computes:\n    \n    score(a)\\=f(J1(a),J2(a),…,Jk(a))score(a)\\=f(J1(a),J2(a),…,Jk(a))\n    \n    \\\\text{score}(a) = f\\\\bigl(J\\_1(a), J\\_2(a), \\\\dots, J\\_k(a)\\\\bigr)\n    *   where:\n        \n        *   aaa is the model-generated output being evaluated\n        *   Ji(a)Ji(a)J\\_i(a) is the judgment from the iii\\-th evaluator\n        *   fff is an aggregation (pooling or voting) function\n*   This formulation mirrors classical techniques for reducing annotator variance in information retrieval and QA evaluation, as discussed in [Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness](https://dl.acm.org/doi/10.1145/290941.291017) by Voorhees (1998).\n    \n\nPoLL computes:\n\n*   where:\n    \n    *   aaa is the model-generated output being evaluated\n    *   Ji(a)Ji(a)J\\_i(a) is the judgment from the iii\\-th evaluator\n    *   fff is an aggregation (pooling or voting) function\n\nwhere:\n\n*   aaa is the model-generated output being evaluated\n*   Ji(a)Ji(a)J\\_i(a) is the judgment from the iii\\-th evaluator\n*   fff is an aggregation (pooling or voting) function\n\nThis formulation mirrors classical techniques for reducing annotator variance in information retrieval and QA evaluation, as discussed in [Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness](https://dl.acm.org/doi/10.1145/290941.291017) by Voorhees (1998).",
    "contentLength": 35494,
    "wordCount": 212,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#panel-of-llm-evaluators-(poll):-core-concept"
  },
  {
    "id": "ai-LLM-as-a-judge-judge-diversity-and-panel-composition-28",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Panel / Jury of LLMs-as-Judges",
    "title": "Judge Diversity and Panel Composition",
    "order": 28,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>A key design principle of PoLL is <strong>heterogeneity</strong>. The PoLL study constructs panels using evaluators from disjoint model families, including:</p>\n\n    <ul>\n      <li>GPT-3.5 (OpenAI)</li>\n      <li>Claude 3 Haiku (Anthropic)</li>\n      <li>Command R (Cohere)</li>\n    </ul>\n  </li>\n  <li>\n    <p>The motivation is to reduce <strong>correlated errors and self-preference effects</strong>, which arise when judges share training data, architectural biases, or generation styles.</p>\n  </li>\n  <li>\n    <p>Crucially, Verga et al. (2024) find that:</p>\n  </li>\n</ul>\n<p>A key design principle of PoLL is <strong>heterogeneity</strong>. The PoLL study constructs panels using evaluators from disjoint model families, including:</p>\n<ul>\n      <li>GPT-3.5 (OpenAI)</li>\n      <li>Claude 3 Haiku (Anthropic)</li>\n      <li>Command R (Cohere)</li>\n    </ul>\n<p>The motivation is to reduce <strong>correlated errors and self-preference effects</strong>, which arise when judges share training data, architectural biases, or generation styles.</p>\n<p>Crucially, Verga et al. (2024) find that:</p>\n<blockquote>\n  <p><strong>A panel of smaller, cheaper, heterogeneous models consistently outperforms a single large model as a judge</strong>, when measured against human judgments.</p>\n</blockquote>\n<p><strong>A panel of smaller, cheaper, heterogeneous models consistently outperforms a single large model as a judge</strong>, when measured against human judgments.</p>\n<ul>\n  <li>This supports broader findings that evaluator quality is <strong>task- and bias-dependent</strong>, rather than monotonically increasing with model size, as also observed in <a href=\"https://arxiv.org/abs/2403.02839\">An Empirical Study of LLM-as-a-Judge for LLM Evaluation</a> by Huang et al. (2024).</li>\n</ul>",
    "contentMarkdown": "*   A key design principle of PoLL is **heterogeneity**. The PoLL study constructs panels using evaluators from disjoint model families, including:\n    \n    *   GPT-3.5 (OpenAI)\n    *   Claude 3 Haiku (Anthropic)\n    *   Command R (Cohere)\n*   The motivation is to reduce **correlated errors and self-preference effects**, which arise when judges share training data, architectural biases, or generation styles.\n    \n*   Crucially, Verga et al. (2024) find that:\n    \n\nA key design principle of PoLL is **heterogeneity**. The PoLL study constructs panels using evaluators from disjoint model families, including:\n\n*   GPT-3.5 (OpenAI)\n*   Claude 3 Haiku (Anthropic)\n*   Command R (Cohere)\n\nThe motivation is to reduce **correlated errors and self-preference effects**, which arise when judges share training data, architectural biases, or generation styles.\n\nCrucially, Verga et al. (2024) find that:\n\n> **A panel of smaller, cheaper, heterogeneous models consistently outperforms a single large model as a judge**, when measured against human judgments.\n\n**A panel of smaller, cheaper, heterogeneous models consistently outperforms a single large model as a judge**, when measured against human judgments.\n\n*   This supports broader findings that evaluator quality is **task- and bias-dependent**, rather than monotonically increasing with model size, as also observed in [An Empirical Study of LLM-as-a-Judge for LLM Evaluation](https://arxiv.org/abs/2403.02839) by Huang et al. (2024).",
    "contentLength": 1814,
    "wordCount": 204,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#judge-diversity-and-panel-composition"
  },
  {
    "id": "ai-LLM-as-a-judge-aggregation-and-voting-strategies-29",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Panel / Jury of LLMs-as-Judges",
    "title": "Aggregation and Voting Strategies",
    "order": 29,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>Different evaluation scales require different aggregation functions.</li>\n</ul>\n<h4 id=\"binary-judgments-eg-qa-correctness\">Binary Judgments (e.g., QA Correctness)</h4>\n<ul>\n  <li>Max pooling is effective:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-100-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><munder><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo><mi>j</mi></munder><msub><mi>J</mi><mi>j</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1148\" style=\"width: 9.221em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1007.61em, 3.284em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1149\"><span class=\"msubsup\" id=\"MathJax-Span-1150\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1151\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-1152\"><span class=\"mrow\" id=\"MathJax-Span-1153\"><span class=\"mo\" id=\"MathJax-Span-1154\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">max</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1155\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1156\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1157\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1158\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-1159\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1001.72em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1160\" style=\"font-family: STIXGeneral-Regular;\">max</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.43em, -999.997em); top: -3.331em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-1161\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1162\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1163\" style=\"font-family: STIXGeneral-Italic;\">J<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1164\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1165\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1166\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1167\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.184em; border-left: 0px solid; width: 0px; height: 2.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo movablelimits=\"true\" form=\"prefix\">max</mo></mrow></msub><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munder><mo movablelimits=\"true\" form=\"prefix\">max</mo><mi>j</mi></munder><msub><mi>J</mi><mi>j</mi></msub><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>This favors recall and avoids false negatives when at least one judge correctly identifies validity.</li>\n</ul>\n<h4 id=\"ordinal-or-likert-type-scores-eg-15-ratings\">Ordinal or Likert-Type Scores (e.g., 1–5 Ratings)</h4>\n<ul>\n  <li>Mean (average) pooling is preferred:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-101-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>avg</mtext></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mfrac><mn>1</mn><mi>k</mi></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>J</mi><mi>j</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1168\" style=\"width: 9.69em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.076em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.263em, 1008.02em, 3.753em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1169\"><span class=\"msubsup\" id=\"MathJax-Span-1170\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1171\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-1172\"><span class=\"mrow\" id=\"MathJax-Span-1173\"><span class=\"mtext\" id=\"MathJax-Span-1174\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">avg</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1175\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1176\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1177\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1178\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-1179\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-1180\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.206em;\"><span class=\"mi\" id=\"MathJax-Span-1181\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.63em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.628em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-1182\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1183\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.43em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-1184\"><span class=\"mrow\" id=\"MathJax-Span-1185\"><span class=\"mi\" id=\"MathJax-Span-1186\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1187\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-1188\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1189\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1190\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1191\" style=\"font-family: STIXGeneral-Italic;\">J<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1192\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1193\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1194\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1195\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.747em; border-left: 0px solid; width: 0px; height: 3.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>avg</mtext></mrow></msub><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>k</mi></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>J</mi><mi>j</mi></msub><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>Averaging produces smoother, more stable scores than majority voting for ordinal scales.</li>\n</ul>\n<h4 id=\"pairwise-preference-judgments\">Pairwise Preference Judgments</h4>\n<ul>\n  <li>\n    <p>Each judge produces a preference <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-102-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>a</mi><mo>&amp;#x227B;</mo><mi>b</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1196\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.09em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1197\"><span class=\"mi\" id=\"MathJax-Span-1198\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1199\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≻</span><span class=\"mi\" id=\"MathJax-Span-1200\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">b</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>a</mi><mo>≻</mo><mi>b</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-102\">a \\succ b</script>, and aggregation is typically performed via <strong>majority voting</strong> or averaged logits.</p>\n  </li>\n  <li>\n    <p>The choice of aggregation function directly affects bias, variance, and sensitivity, and should be aligned with the underlying rubric scale.</p>\n  </li>\n</ul>\n<p>Each judge produces a preference <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-102-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>a</mi><mo>&amp;#x227B;</mo><mi>b</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1196\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.09em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1197\"><span class=\"mi\" id=\"MathJax-Span-1198\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1199\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≻</span><span class=\"mi\" id=\"MathJax-Span-1200\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">b</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>a</mi><mo>≻</mo><mi>b</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-102\">a \\succ b</script>, and aggregation is typically performed via <strong>majority voting</strong> or averaged logits.</p>\n<p>The choice of aggregation function directly affects bias, variance, and sensitivity, and should be aligned with the underlying rubric scale.</p>",
    "contentMarkdown": "*   Different evaluation scales require different aggregation functions.\n\n#### Binary Judgments (e.g., QA Correctness)\n\n*   Max pooling is effective:\n\nfmax(a)\\=maxjJj(a)fmax(a)\\=maxjJj(a)\n\n*   This favors recall and avoids false negatives when at least one judge correctly identifies validity.\n\n#### Ordinal or Likert-Type Scores (e.g., 1–5 Ratings)\n\n*   Mean (average) pooling is preferred:\n\nfavg(a)\\=1k∑j\\=1kJj(a)favg(a)\\=1k∑j\\=1kJj(a)\n\n*   Averaging produces smoother, more stable scores than majority voting for ordinal scales.\n\n#### Pairwise Preference Judgments\n\n*   Each judge produces a preference a≻ba≻ba \\\\succ b, and aggregation is typically performed via **majority voting** or averaged logits.\n    \n*   The choice of aggregation function directly affects bias, variance, and sensitivity, and should be aligned with the underlying rubric scale.\n    \n\nEach judge produces a preference a≻ba≻ba \\\\succ b, and aggregation is typically performed via **majority voting** or averaged logits.\n\nThe choice of aggregation function directly affects bias, variance, and sensitivity, and should be aligned with the underlying rubric scale.",
    "contentLength": 16627,
    "wordCount": 148,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#aggregation-and-voting-strategies"
  },
  {
    "id": "ai-LLM-as-a-judge-empirical-results-and-human-correlation-30",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Panel / Jury of LLMs-as-Judges",
    "title": "Empirical Results and Human Correlation",
    "order": 30,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>\n    <p>PoLL is evaluated across the following data across six datasets, including KILT-NQ, TriviaQA, HotpotQA, Bamboogle, and Arena-Hard:</p>\n\n    <ul>\n      <li>Single-hop QA</li>\n      <li>Multi-hop QA</li>\n      <li>Pairwise chatbot evaluation (Chatbot Arena / Arena-Hard)</li>\n    </ul>\n  </li>\n  <li>\n    <p>The following figure shows (top) rankings of model performance change drastically depending on which LLM is used as the judge on KILT-NQ; (bottom) panel of LLM evaluators (PoLL) has the highest Cohen’s <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-103-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03BA;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1201\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1202\"><span class=\"mi\" id=\"MathJax-Span-1203\" style=\"font-family: STIXGeneral-Italic;\">κ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>κ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-103\">\\kappa</script> correlation with human judgements.**</p>\n  </li>\n</ul>\n<p>PoLL is evaluated across the following data across six datasets, including KILT-NQ, TriviaQA, HotpotQA, Bamboogle, and Arena-Hard:</p>\n<ul>\n      <li>Single-hop QA</li>\n      <li>Multi-hop QA</li>\n      <li>Pairwise chatbot evaluation (Chatbot Arena / Arena-Hard)</li>\n    </ul>\n<p>The following figure shows (top) rankings of model performance change drastically depending on which LLM is used as the judge on KILT-NQ; (bottom) panel of LLM evaluators (PoLL) has the highest Cohen’s <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-103-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03BA;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1201\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1202\"><span class=\"mi\" id=\"MathJax-Span-1203\" style=\"font-family: STIXGeneral-Italic;\">κ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>κ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-103\">\\kappa</script> correlation with human judgements.**</p>\n<p><img src=\"/primers/ai/assets/LLM-as-a-judge/PoLL.jpg\" alt=\"\"></p>\n<ul>\n  <li>\n    <p>Key empirical findings from Verga et al. (2024):</p>\n\n    <ul>\n      <li>Higher Cohen’s <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-104-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03BA;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1204\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1205\"><span class=\"mi\" id=\"MathJax-Span-1206\" style=\"font-family: STIXGeneral-Italic;\">κ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>κ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-104\">\\kappa</script> agreement with humans on QA tasks</li>\n      <li>Higher Kendall–<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-105-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C4;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1207\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1208\"><span class=\"mi\" id=\"MathJax-Span-1209\" style=\"font-family: STIXGeneral-Italic;\">τ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>τ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-105\">\\tau</script> and Pearson correlation with human rankings</li>\n      <li>Lower variance across prompts and evaluator choices</li>\n      <li>Dramatically reduced intra-model bias (models no longer rank themselves highest)</li>\n    </ul>\n  </li>\n  <li>\n    <p>These results directly reinforce concerns raised in <a href=\"https://arxiv.org/abs/2305.17926\">Large Language Models Are Not Fair Evaluators</a> by Wang et al. (2023).</p>\n  </li>\n</ul>\n<p>Key empirical findings from Verga et al. (2024):</p>\n<ul>\n      <li>Higher Cohen’s <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-104-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03BA;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1204\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1205\"><span class=\"mi\" id=\"MathJax-Span-1206\" style=\"font-family: STIXGeneral-Italic;\">κ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>κ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-104\">\\kappa</script> agreement with humans on QA tasks</li>\n      <li>Higher Kendall–<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-105-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C4;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1207\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1208\"><span class=\"mi\" id=\"MathJax-Span-1209\" style=\"font-family: STIXGeneral-Italic;\">τ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>τ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-105\">\\tau</script> and Pearson correlation with human rankings</li>\n      <li>Lower variance across prompts and evaluator choices</li>\n      <li>Dramatically reduced intra-model bias (models no longer rank themselves highest)</li>\n    </ul>\n<p>These results directly reinforce concerns raised in <a href=\"https://arxiv.org/abs/2305.17926\">Large Language Models Are Not Fair Evaluators</a> by Wang et al. (2023).</p>",
    "contentMarkdown": "*   PoLL is evaluated across the following data across six datasets, including KILT-NQ, TriviaQA, HotpotQA, Bamboogle, and Arena-Hard:\n    \n    *   Single-hop QA\n    *   Multi-hop QA\n    *   Pairwise chatbot evaluation (Chatbot Arena / Arena-Hard)\n*   The following figure shows (top) rankings of model performance change drastically depending on which LLM is used as the judge on KILT-NQ; (bottom) panel of LLM evaluators (PoLL) has the highest Cohen’s κκ\\\\kappa correlation with human judgements.\\*\\*\n    \n\nPoLL is evaluated across the following data across six datasets, including KILT-NQ, TriviaQA, HotpotQA, Bamboogle, and Arena-Hard:\n\n*   Single-hop QA\n*   Multi-hop QA\n*   Pairwise chatbot evaluation (Chatbot Arena / Arena-Hard)\n\nThe following figure shows (top) rankings of model performance change drastically depending on which LLM is used as the judge on KILT-NQ; (bottom) panel of LLM evaluators (PoLL) has the highest Cohen’s κκ\\\\kappa correlation with human judgements.\\*\\*\n\n![](/primers/ai/assets/LLM-as-a-judge/PoLL.jpg)\n\n*   Key empirical findings from Verga et al. (2024):\n    \n    *   Higher Cohen’s κκ\\\\kappa agreement with humans on QA tasks\n    *   Higher Kendall–ττ\\\\tau and Pearson correlation with human rankings\n    *   Lower variance across prompts and evaluator choices\n    *   Dramatically reduced intra-model bias (models no longer rank themselves highest)\n*   These results directly reinforce concerns raised in [Large Language Models Are Not Fair Evaluators](https://arxiv.org/abs/2305.17926) by Wang et al. (2023).\n    \n\nKey empirical findings from Verga et al. (2024):\n\n*   Higher Cohen’s κκ\\\\kappa agreement with humans on QA tasks\n*   Higher Kendall–ττ\\\\tau and Pearson correlation with human rankings\n*   Lower variance across prompts and evaluator choices\n*   Dramatically reduced intra-model bias (models no longer rank themselves highest)\n\nThese results directly reinforce concerns raised in [Large Language Models Are Not Fair Evaluators](https://arxiv.org/abs/2305.17926) by Wang et al. (2023).",
    "contentLength": 10074,
    "wordCount": 271,
    "hasCode": false,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#empirical-results-and-human-correlation"
  },
  {
    "id": "ai-LLM-as-a-judge-cost-and-latency-advantages-31",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Panel / Jury of LLMs-as-Judges",
    "title": "Cost and Latency Advantages",
    "order": 31,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>\n    <p>A counterintuitive but important result is that PoLL is <strong>significantly cheaper and often faster</strong> than using a single large judge:</p>\n\n    <ul>\n      <li>A three-model PoLL is reported to be <strong>7–8× cheaper</strong> than GPT-4 Turbo</li>\n      <li>Parallel execution of smaller models reduces wall-clock latency</li>\n    </ul>\n  </li>\n  <li>\n    <p>This undermines the assumption that the best judge must always be the largest or most capable model.</p>\n  </li>\n</ul>\n<p>A counterintuitive but important result is that PoLL is <strong>significantly cheaper and often faster</strong> than using a single large judge:</p>\n<ul>\n      <li>A three-model PoLL is reported to be <strong>7–8× cheaper</strong> than GPT-4 Turbo</li>\n      <li>Parallel execution of smaller models reduces wall-clock latency</li>\n    </ul>\n<p>This undermines the assumption that the best judge must always be the largest or most capable model.</p>",
    "contentMarkdown": "*   A counterintuitive but important result is that PoLL is **significantly cheaper and often faster** than using a single large judge:\n    \n    *   A three-model PoLL is reported to be **7–8× cheaper** than GPT-4 Turbo\n    *   Parallel execution of smaller models reduces wall-clock latency\n*   This undermines the assumption that the best judge must always be the largest or most capable model.\n    \n\nA counterintuitive but important result is that PoLL is **significantly cheaper and often faster** than using a single large judge:\n\n*   A three-model PoLL is reported to be **7–8× cheaper** than GPT-4 Turbo\n*   Parallel execution of smaller models reduces wall-clock latency\n\nThis undermines the assumption that the best judge must always be the largest or most capable model.",
    "contentLength": 959,
    "wordCount": 120,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#cost-and-latency-advantages"
  },
  {
    "id": "ai-LLM-as-a-judge-relationship-to-llm-as-a-judge-and-ltr-paradigms-32",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Panel / Jury of LLMs-as-Judges",
    "title": "Relationship to LLM-as-a-Judge and LTR Paradigms",
    "order": 32,
    "orderInChapter": 7,
    "contentHtml": "<ul>\n  <li>\n    <p>Panels of judges can be applied across <strong>all evaluation and ranking paradigms</strong>:</p>\n  </li>\n  <li><strong>Pointwise</strong>: each judge scores outputs independently; scores are pooled</li>\n  <li><strong>Pairwise</strong>: each judge compares outputs; preferences are aggregated</li>\n  <li>\n    <p><strong>Listwise</strong>: each judge ranks a list; rankings are combined (e.g., Borda count)</p>\n  </li>\n  <li>In practice:</li>\n</ul>\n<p>Panels of judges can be applied across <strong>all evaluation and ranking paradigms</strong>:</p>\n<p><strong>Listwise</strong>: each judge ranks a list; rankings are combined (e.g., Borda count)</p>\n<blockquote>\n  <p>Panels are most commonly used in <strong>pointwise and pairwise LLM-as-a-Judge setups</strong>, where they deliver the largest robustness gains per unit cost.</p>\n</blockquote>\n<p>Panels are most commonly used in <strong>pointwise and pairwise LLM-as-a-Judge setups</strong>, where they deliver the largest robustness gains per unit cost.</p>\n<ul>\n  <li>This complements listwise ranking approaches such as\n<a href=\"https://arxiv.org/abs/2206.15198\">ListBERT</a> by Kumar et al. (2022) and\n<a href=\"https://arxiv.org/abs/2402.15838\">ListT5</a> by Yoon et al. (2024), where diversity is typically introduced at the data or decoding level rather than the evaluator level.</li>\n</ul>",
    "contentMarkdown": "*   Panels of judges can be applied across **all evaluation and ranking paradigms**:\n    \n*   **Pointwise**: each judge scores outputs independently; scores are pooled\n*   **Pairwise**: each judge compares outputs; preferences are aggregated\n*   **Listwise**: each judge ranks a list; rankings are combined (e.g., Borda count)\n    \n*   In practice:\n\nPanels of judges can be applied across **all evaluation and ranking paradigms**:\n\n**Listwise**: each judge ranks a list; rankings are combined (e.g., Borda count)\n\n> Panels are most commonly used in **pointwise and pairwise LLM-as-a-Judge setups**, where they deliver the largest robustness gains per unit cost.\n\nPanels are most commonly used in **pointwise and pairwise LLM-as-a-Judge setups**, where they deliver the largest robustness gains per unit cost.\n\n*   This complements listwise ranking approaches such as [ListBERT](https://arxiv.org/abs/2206.15198) by Kumar et al. (2022) and [ListT5](https://arxiv.org/abs/2402.15838) by Yoon et al. (2024), where diversity is typically introduced at the data or decoding level rather than the evaluator level.",
    "contentLength": 1367,
    "wordCount": 152,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#relationship-to-llm-as-a-judge-and-ltr-paradigms"
  },
  {
    "id": "ai-LLM-as-a-judge-practical-guidance-when-to-use-a-panel-of-judges-33",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Panel / Jury of LLMs-as-Judges",
    "title": "Practical Guidance: When to Use a Panel of Judges",
    "order": 33,
    "orderInChapter": 8,
    "contentHtml": "<ul>\n  <li>\n    <p>Use a <strong>panel of LLM judges</strong> when:</p>\n\n    <ul>\n      <li>Evaluation bias has high downstream cost</li>\n      <li>Human agreement is critical</li>\n      <li>You are benchmarking, publishing, or reporting results</li>\n      <li>Single-judge variance is unacceptable</li>\n      <li>Cost constraints rule out a single large judge</li>\n    </ul>\n  </li>\n  <li>\n    <p>Panels are less necessary when:</p>\n\n    <ul>\n      <li>Tasks are trivial or highly objective</li>\n      <li>Absolute correctness can be verified symbolically</li>\n      <li>Latency budgets are extremely tight</li>\n      <li>Trained rankers already handle evaluation in production</li>\n    </ul>\n  </li>\n</ul>\n<p>Use a <strong>panel of LLM judges</strong> when:</p>\n<ul>\n      <li>Evaluation bias has high downstream cost</li>\n      <li>Human agreement is critical</li>\n      <li>You are benchmarking, publishing, or reporting results</li>\n      <li>Single-judge variance is unacceptable</li>\n      <li>Cost constraints rule out a single large judge</li>\n    </ul>\n<p>Panels are less necessary when:</p>\n<ul>\n      <li>Tasks are trivial or highly objective</li>\n      <li>Absolute correctness can be verified symbolically</li>\n      <li>Latency budgets are extremely tight</li>\n      <li>Trained rankers already handle evaluation in production</li>\n    </ul>",
    "contentMarkdown": "*   Use a **panel of LLM judges** when:\n    \n    *   Evaluation bias has high downstream cost\n    *   Human agreement is critical\n    *   You are benchmarking, publishing, or reporting results\n    *   Single-judge variance is unacceptable\n    *   Cost constraints rule out a single large judge\n*   Panels are less necessary when:\n    \n    *   Tasks are trivial or highly objective\n    *   Absolute correctness can be verified symbolically\n    *   Latency budgets are extremely tight\n    *   Trained rankers already handle evaluation in production\n\nUse a **panel of LLM judges** when:\n\n*   Evaluation bias has high downstream cost\n*   Human agreement is critical\n*   You are benchmarking, publishing, or reporting results\n*   Single-judge variance is unacceptable\n*   Cost constraints rule out a single large judge\n\nPanels are less necessary when:\n\n*   Tasks are trivial or highly objective\n*   Absolute correctness can be verified symbolically\n*   Latency budgets are extremely tight\n*   Trained rankers already handle evaluation in production",
    "contentLength": 1356,
    "wordCount": 150,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#practical-guidance:-when-to-use-a-panel-of-judges"
  },
  {
    "id": "ai-LLM-as-a-judge-takeaways-34",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Panel / Jury of LLMs-as-Judges",
    "title": "Takeaways",
    "order": 34,
    "orderInChapter": 9,
    "contentHtml": "<ul>\n  <li>\n    <p>Panels of LLMs-as-Judges transform evaluation from a single-oracle decision into a <strong>statistical estimation problem</strong>. By leveraging diversity, aggregation, and redundancy, PoLL delivers:</p>\n\n    <ul>\n      <li>Higher human alignment</li>\n      <li>Reduced bias</li>\n      <li>Lower variance</li>\n      <li>Better cost–performance trade-offs</li>\n    </ul>\n  </li>\n  <li>\n    <p>As such, jury-style evaluation forms a natural foundation for robust LLM-as-a-Judge systems and directly motivates the subsequent discussion of <strong>biases in LLM-as-a-Judge and mitigation strategies</strong>.</p>\n  </li>\n</ul>\n<p>Panels of LLMs-as-Judges transform evaluation from a single-oracle decision into a <strong>statistical estimation problem</strong>. By leveraging diversity, aggregation, and redundancy, PoLL delivers:</p>\n<ul>\n      <li>Higher human alignment</li>\n      <li>Reduced bias</li>\n      <li>Lower variance</li>\n      <li>Better cost–performance trade-offs</li>\n    </ul>\n<p>As such, jury-style evaluation forms a natural foundation for robust LLM-as-a-Judge systems and directly motivates the subsequent discussion of <strong>biases in LLM-as-a-Judge and mitigation strategies</strong>.</p>",
    "contentMarkdown": "*   Panels of LLMs-as-Judges transform evaluation from a single-oracle decision into a **statistical estimation problem**. By leveraging diversity, aggregation, and redundancy, PoLL delivers:\n    \n    *   Higher human alignment\n    *   Reduced bias\n    *   Lower variance\n    *   Better cost–performance trade-offs\n*   As such, jury-style evaluation forms a natural foundation for robust LLM-as-a-Judge systems and directly motivates the subsequent discussion of **biases in LLM-as-a-Judge and mitigation strategies**.\n    \n\nPanels of LLMs-as-Judges transform evaluation from a single-oracle decision into a **statistical estimation problem**. By leveraging diversity, aggregation, and redundancy, PoLL delivers:\n\n*   Higher human alignment\n*   Reduced bias\n*   Lower variance\n*   Better cost–performance trade-offs\n\nAs such, jury-style evaluation forms a natural foundation for robust LLM-as-a-Judge systems and directly motivates the subsequent discussion of **biases in LLM-as-a-Judge and mitigation strategies**.",
    "contentLength": 1231,
    "wordCount": 124,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#takeaways"
  },
  {
    "id": "ai-LLM-as-a-judge-why-multimodal-judges-are-necessary-35",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Multimodal LLMs-as-Judges (LMM / VLM-as-a-Judge)",
    "title": "Why Multimodal Judges are Necessary",
    "order": 35,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>Text-only LLM judges are fundamentally limited when evaluating multimodal tasks:</p>\n\n    <ul>\n      <li>They cannot directly perceive visual content</li>\n      <li>They rely on intermediate captions or OCR pipelines</li>\n      <li>Errors propagate from perception to judgment</li>\n      <li>Multiple inference stages increase cost and latency</li>\n    </ul>\n  </li>\n  <li>\n    <p>These limitations are explicitly discussed in <a href=\"https://arxiv.org/abs/2401.06591\">Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation</a> by Lee et al. (2024). The following figure (<a href=\"https://arxiv.org/abs/2401.06591\">source</a>) illustrates the fact that conventional metrics measure the similarity between the response and ground-truth answer, which is not expressive enough. Moreover, it could not pinpoint what is missing within the response with respect to the evaluation criteria. In contrast, the VLM-as-a-Judge pipeline provides not only the flexibility to adhere to arbitrary evaluation criteria but also provides detailed language feedback that specifically pinpoints the deficiencies.</p>\n  </li>\n</ul>\n<p>Text-only LLM judges are fundamentally limited when evaluating multimodal tasks:</p>\n<ul>\n      <li>They cannot directly perceive visual content</li>\n      <li>They rely on intermediate captions or OCR pipelines</li>\n      <li>Errors propagate from perception to judgment</li>\n      <li>Multiple inference stages increase cost and latency</li>\n    </ul>\n<p>These limitations are explicitly discussed in <a href=\"https://arxiv.org/abs/2401.06591\">Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation</a> by Lee et al. (2024). The following figure (<a href=\"https://arxiv.org/abs/2401.06591\">source</a>) illustrates the fact that conventional metrics measure the similarity between the response and ground-truth answer, which is not expressive enough. Moreover, it could not pinpoint what is missing within the response with respect to the evaluation criteria. In contrast, the VLM-as-a-Judge pipeline provides not only the flexibility to adhere to arbitrary evaluation criteria but also provides detailed language feedback that specifically pinpoints the deficiencies.</p>\n<p><img src=\"/primers/ai/assets/LLM-as-a-judge/Prometheus-Vision_1.jpg\" alt=\"\"></p>\n<ul>\n  <li>As a result, <strong>VLM-as-a-Judge</strong> (or LMM-as-a-Judge) has emerged, where the judge model directly processes the same multimodal inputs as the model being evaluated.</li>\n</ul>",
    "contentMarkdown": "*   Text-only LLM judges are fundamentally limited when evaluating multimodal tasks:\n    \n    *   They cannot directly perceive visual content\n    *   They rely on intermediate captions or OCR pipelines\n    *   Errors propagate from perception to judgment\n    *   Multiple inference stages increase cost and latency\n*   These limitations are explicitly discussed in [Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation](https://arxiv.org/abs/2401.06591) by Lee et al. (2024). The following figure ([source](https://arxiv.org/abs/2401.06591)) illustrates the fact that conventional metrics measure the similarity between the response and ground-truth answer, which is not expressive enough. Moreover, it could not pinpoint what is missing within the response with respect to the evaluation criteria. In contrast, the VLM-as-a-Judge pipeline provides not only the flexibility to adhere to arbitrary evaluation criteria but also provides detailed language feedback that specifically pinpoints the deficiencies.\n    \n\nText-only LLM judges are fundamentally limited when evaluating multimodal tasks:\n\n*   They cannot directly perceive visual content\n*   They rely on intermediate captions or OCR pipelines\n*   Errors propagate from perception to judgment\n*   Multiple inference stages increase cost and latency\n\nThese limitations are explicitly discussed in [Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation](https://arxiv.org/abs/2401.06591) by Lee et al. (2024). The following figure ([source](https://arxiv.org/abs/2401.06591)) illustrates the fact that conventional metrics measure the similarity between the response and ground-truth answer, which is not expressive enough. Moreover, it could not pinpoint what is missing within the response with respect to the evaluation criteria. In contrast, the VLM-as-a-Judge pipeline provides not only the flexibility to adhere to arbitrary evaluation criteria but also provides detailed language feedback that specifically pinpoints the deficiencies.\n\n![](/primers/ai/assets/LLM-as-a-judge/Prometheus-Vision_1.jpg)\n\n*   As a result, **VLM-as-a-Judge** (or LMM-as-a-Judge) has emerged, where the judge model directly processes the same multimodal inputs as the model being evaluated.",
    "contentLength": 2539,
    "wordCount": 285,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#why-multimodal-judges-are-necessary"
  },
  {
    "id": "ai-LLM-as-a-judge-paradigm-vlm-lmm-as-a-judge-36",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Multimodal LLMs-as-Judges (LMM / VLM-as-a-Judge)",
    "title": "Paradigm: VLM / LMM-as-a-Judge",
    "order": 36,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Formally, a multimodal judge estimates:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-106-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><msub><mi>f</mi><mi>&amp;#x03B8;</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>image</mtext></mrow></msup><mo>,</mo><msup><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>text</mtext></mrow></msup><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1210\" style=\"width: 10.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1008.75em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1211\"><span class=\"msubsup\" id=\"MathJax-Span-1212\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1213\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-1214\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1215\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-1216\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1217\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"mi\" id=\"MathJax-Span-1218\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1219\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1220\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1221\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1222\"><span class=\"mrow\" id=\"MathJax-Span-1223\"><span class=\"mtext\" id=\"MathJax-Span-1224\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">image</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1225\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1226\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1227\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1228\"><span class=\"mrow\" id=\"MathJax-Span-1229\"><span class=\"mtext\" id=\"MathJax-Span-1230\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">text</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1231\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1232\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1233\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1234\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1235\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>image</mtext></mrow></msup><mo>,</mo><msup><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>text</mtext></mrow></msup><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>\n    <p>where:</p>\n\n    <ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-107-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>image</mtext></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1236\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1002.29em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1237\"><span class=\"msubsup\" id=\"MathJax-Span-1238\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1239\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1240\"><span class=\"mrow\" id=\"MathJax-Span-1241\"><span class=\"mtext\" id=\"MathJax-Span-1242\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">image</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>image</mtext></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-107\">x^{\\text{image}}</script> is the visual input</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-108-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>text</mtext></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1243\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1001.62em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1244\"><span class=\"msubsup\" id=\"MathJax-Span-1245\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1246\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1247\"><span class=\"mrow\" id=\"MathJax-Span-1248\"><span class=\"mtext\" id=\"MathJax-Span-1249\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">text</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>text</mtext></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-108\">x^{\\text{text}}</script> is the instruction or question</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-109-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1250\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1251\"><span class=\"msubsup\" id=\"MathJax-Span-1252\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1253\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1254\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-109\">y_i</script> is the candidate response</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-110-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1255\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1256\"><span class=\"msubsup\" id=\"MathJax-Span-1257\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1258\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-1259\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-110\">s_i</script> is a score, label, or preference</li>\n    </ul>\n  </li>\n  <li>\n    <p>This mirrors pointwise and pairwise LTR, but with <strong>joint multimodal grounding</strong>.</p>\n  </li>\n  <li>\n    <p>Multimodal judges are now used for:</p>\n\n    <ul>\n      <li>Visual instruction following</li>\n      <li>Image and video captioning evaluation</li>\n      <li>Visual QA and reasoning</li>\n      <li>Hallucination detection</li>\n      <li>Preference learning and reward modeling</li>\n    </ul>\n  </li>\n</ul>\n<p>where:</p>\n<ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-107-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>image</mtext></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1236\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1002.29em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1237\"><span class=\"msubsup\" id=\"MathJax-Span-1238\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1239\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1240\"><span class=\"mrow\" id=\"MathJax-Span-1241\"><span class=\"mtext\" id=\"MathJax-Span-1242\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">image</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>image</mtext></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-107\">x^{\\text{image}}</script> is the visual input</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-108-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>text</mtext></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1243\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1001.62em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1244\"><span class=\"msubsup\" id=\"MathJax-Span-1245\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1246\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1247\"><span class=\"mrow\" id=\"MathJax-Span-1248\"><span class=\"mtext\" id=\"MathJax-Span-1249\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">text</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>text</mtext></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-108\">x^{\\text{text}}</script> is the instruction or question</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-109-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1250\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1251\"><span class=\"msubsup\" id=\"MathJax-Span-1252\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1253\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1254\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-109\">y_i</script> is the candidate response</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-110-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1255\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1256\"><span class=\"msubsup\" id=\"MathJax-Span-1257\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1258\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-1259\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-110\">s_i</script> is a score, label, or preference</li>\n    </ul>\n<p>This mirrors pointwise and pairwise LTR, but with <strong>joint multimodal grounding</strong>.</p>\n<p>Multimodal judges are now used for:</p>\n<ul>\n      <li>Visual instruction following</li>\n      <li>Image and video captioning evaluation</li>\n      <li>Visual QA and reasoning</li>\n      <li>Hallucination detection</li>\n      <li>Preference learning and reward modeling</li>\n    </ul>",
    "contentMarkdown": "*   Formally, a multimodal judge estimates:\n\nsi\\=fθ(ximage,xtext,yi)si\\=fθ(ximage,xtext,yi)\n\n*   where:\n    \n    *   ximageximagex^{\\\\text{image}} is the visual input\n    *   xtextxtextx^{\\\\text{text}} is the instruction or question\n    *   yiyiy\\_i is the candidate response\n    *   sisis\\_i is a score, label, or preference\n*   This mirrors pointwise and pairwise LTR, but with **joint multimodal grounding**.\n    \n*   Multimodal judges are now used for:\n    \n    *   Visual instruction following\n    *   Image and video captioning evaluation\n    *   Visual QA and reasoning\n    *   Hallucination detection\n    *   Preference learning and reward modeling\n\nwhere:\n\n*   ximageximagex^{\\\\text{image}} is the visual input\n*   xtextxtextx^{\\\\text{text}} is the instruction or question\n*   yiyiy\\_i is the candidate response\n*   sisis\\_i is a score, label, or preference\n\nThis mirrors pointwise and pairwise LTR, but with **joint multimodal grounding**.\n\nMultimodal judges are now used for:\n\n*   Visual instruction following\n*   Image and video captioning evaluation\n*   Visual QA and reasoning\n*   Hallucination detection\n*   Preference learning and reward modeling",
    "contentLength": 23508,
    "wordCount": 148,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#paradigm:-vlm-/-lmm-as-a-judge"
  },
  {
    "id": "ai-LLM-as-a-judge-llava-critic-generalist-multimodal-judge-37",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Multimodal LLMs-as-Judges (LMM / VLM-as-a-Judge)",
    "title": "LLaVA-Critic: Generalist Multimodal Judge",
    "order": 37,
    "orderInChapter": 3,
    "contentHtml": "<h4 id=\"overview-1\">Overview</h4>\n<ul>\n  <li>\n    <p><strong>LLaVA-Critic</strong> is the first open-source <strong>generalist multimodal evaluator</strong>, explicitly trained to act as a judge across a wide range of vision–language tasks.</p>\n  </li>\n  <li>\n    <p>Paper: <a href=\"https://arxiv.org/abs/2410.02712\">LLaVA-Critic: Learning to Evaluate Multimodal Models</a> by Xiong et al. (2024)</p>\n  </li>\n  <li>\n    <p>Project page: <a href=\"https://llava-vl.github.io/blog/llava-critic\">LLaVA-Critic blog</a></p>\n  </li>\n</ul>\n<p><strong>LLaVA-Critic</strong> is the first open-source <strong>generalist multimodal evaluator</strong>, explicitly trained to act as a judge across a wide range of vision–language tasks.</p>\n<p>Paper: <a href=\"https://arxiv.org/abs/2410.02712\">LLaVA-Critic: Learning to Evaluate Multimodal Models</a> by Xiong et al. (2024)</p>\n<p>Project page: <a href=\"https://llava-vl.github.io/blog/llava-critic\">LLaVA-Critic blog</a></p>\n<h4 id=\"key-contributions\">Key Contributions</h4>\n<ul>\n  <li>Introduces <strong>critic instruction-following data</strong> for multimodal evaluation</li>\n  <li>Supports both <strong>pointwise scoring</strong> and <strong>pairwise ranking</strong></li>\n  <li>Produces <strong>scores + natural language justifications</strong></li>\n  <li>Matches or exceeds GPT-4V / GPT-4o alignment on multiple benchmarks</li>\n  <li>Enables <strong>preference learning</strong> for LMM alignment</li>\n</ul>\n<h4 id=\"training-data-and-setup\">Training Data and Setup</h4>\n<ul>\n  <li>Each training instance follows:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-111-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo stretchy=&quot;false&quot;>(</mo><mtext>Image</mtext><mo>,</mo><mtext>Question</mtext><mo>,</mo><mtext>Response</mtext><mo>,</mo><mtext>Criteria</mtext><mo>,</mo><mtext>Score</mtext><mo>,</mo><mtext>Reason</mtext><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1260\" style=\"width: 25.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 21.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1021.15em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1261\"><span class=\"mo\" id=\"MathJax-Span-1262\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mtext\" id=\"MathJax-Span-1263\" style=\"font-family: STIXGeneral-Regular;\">Image</span><span class=\"mo\" id=\"MathJax-Span-1264\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-1265\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">Question</span><span class=\"mo\" id=\"MathJax-Span-1266\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-1267\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">Response</span><span class=\"mo\" id=\"MathJax-Span-1268\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-1269\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">Criteria</span><span class=\"mo\" id=\"MathJax-Span-1270\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-1271\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">Score</span><span class=\"mo\" id=\"MathJax-Span-1272\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-1273\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">Reason</span><span class=\"mo\" id=\"MathJax-Span-1274\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mo stretchy=\"false\">(</mo><mtext>Image</mtext><mo>,</mo><mtext>Question</mtext><mo>,</mo><mtext>Response</mtext><mo>,</mo><mtext>Criteria</mtext><mo>,</mo><mtext>Score</mtext><mo>,</mo><mtext>Reason</mtext><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>… or, in pairwise mode:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-112-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo stretchy=&quot;false&quot;>(</mo><mtext>Image</mtext><mo>,</mo><mtext>Question</mtext><mo>,</mo><msub><mtext>Response</mtext><mn>1</mn></msub><mo>,</mo><msub><mtext>Response</mtext><mn>2</mn></msub><mo>,</mo><mtext>Criteria</mtext><mo>,</mo><mtext>Preference</mtext><mo>,</mo><mtext>Reason</mtext><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1275\" style=\"width: 34.065em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 28.388em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1028.34em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1276\"><span class=\"mo\" id=\"MathJax-Span-1277\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mtext\" id=\"MathJax-Span-1278\" style=\"font-family: STIXGeneral-Regular;\">Image</span><span class=\"mo\" id=\"MathJax-Span-1279\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-1280\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">Question</span><span class=\"mo\" id=\"MathJax-Span-1281\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1282\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 4.273em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1003.8em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mtext\" id=\"MathJax-Span-1283\" style=\"font-family: STIXGeneral-Regular;\">Response</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 3.857em;\"><span class=\"mn\" id=\"MathJax-Span-1284\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1285\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1286\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 4.273em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1003.8em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mtext\" id=\"MathJax-Span-1287\" style=\"font-family: STIXGeneral-Regular;\">Response</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 3.857em;\"><span class=\"mn\" id=\"MathJax-Span-1288\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1289\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-1290\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">Criteria</span><span class=\"mo\" id=\"MathJax-Span-1291\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-1292\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">Preference</span><span class=\"mo\" id=\"MathJax-Span-1293\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-1294\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">Reason</span><span class=\"mo\" id=\"MathJax-Span-1295\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mo stretchy=\"false\">(</mo><mtext>Image</mtext><mo>,</mo><mtext>Question</mtext><mo>,</mo><msub><mtext>Response</mtext><mn>1</mn></msub><mo>,</mo><msub><mtext>Response</mtext><mn>2</mn></msub><mo>,</mo><mtext>Criteria</mtext><mo>,</mo><mtext>Preference</mtext><mo>,</mo><mtext>Reason</mtext><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>The model is fine-tuned using <strong>standard autoregressive cross-entropy loss</strong> over both the judgment and justification tokens.</li>\n</ul>\n<h4 id=\"architecture-5\">Architecture</h4>\n<ul>\n  <li><strong>Backbone:</strong> LLaVA-OneVision (7B / 72B)</li>\n  <li>Multimodal encoder + language decoder</li>\n  <li>\n    <p>Evaluation treated as instruction-following generation</p>\n  </li>\n  <li>The following figure (<a href=\"https://arxiv.org/abs/2410.02712\">source</a>) shows an example of LLaVA-Critic training data. The top block shows pointwise scoring, where LLaVA-Critic predicts a score to evaluate a single response’s quality; the bottom block illustrates pairwise ranking, where it rank response pairs. In both settings, LLaVA-Critic learns to provide reasons for its judgments.</li>\n</ul>\n<p>Evaluation treated as instruction-following generation</p>\n<p><img src=\"/primers/ai/assets/LLM-as-a-judge/LLaVA_Critic.jpg\" alt=\"\"></p>\n<h4 id=\"evaluation-scenarios\">Evaluation Scenarios</h4>\n<ul>\n  <li>\n    <p>LLaVA-Critic is evaluated on:</p>\n\n    <ul>\n      <li>Visual chat benchmarks (LLaVA-in-the-Wild, LLaVA-Wilder)</li>\n      <li>Integrated capability benchmarks (MMVet)</li>\n      <li>Hallucination detection (MMHal-Bench)</li>\n      <li>Preference benchmarks (WildVision Arena)</li>\n    </ul>\n  </li>\n  <li>\n    <p>It shows strong <strong>Pearson correlation</strong> and <strong>Kendall’s <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-113-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C4;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1296\" style=\"width: 0.571em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.467em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.604em, 1000.47em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1297\"><span class=\"mi\" id=\"MathJax-Span-1298\" style=\"font-family: STIXGeneral-Italic;\">τ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>τ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-113\">\\tau</script></strong> with GPT-4o and human judgments, even at 7B scale.</p>\n  </li>\n</ul>\n<p>LLaVA-Critic is evaluated on:</p>\n<ul>\n      <li>Visual chat benchmarks (LLaVA-in-the-Wild, LLaVA-Wilder)</li>\n      <li>Integrated capability benchmarks (MMVet)</li>\n      <li>Hallucination detection (MMHal-Bench)</li>\n      <li>Preference benchmarks (WildVision Arena)</li>\n    </ul>\n<p>It shows strong <strong>Pearson correlation</strong> and <strong>Kendall’s <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-113-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C4;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1296\" style=\"width: 0.571em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.467em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.604em, 1000.47em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1297\"><span class=\"mi\" id=\"MathJax-Span-1298\" style=\"font-family: STIXGeneral-Italic;\">τ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>τ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-113\">\\tau</script></strong> with GPT-4o and human judgments, even at 7B scale.</p>",
    "contentMarkdown": "#### Overview\n\n*   **LLaVA-Critic** is the first open-source **generalist multimodal evaluator**, explicitly trained to act as a judge across a wide range of vision–language tasks.\n    \n*   Paper: [LLaVA-Critic: Learning to Evaluate Multimodal Models](https://arxiv.org/abs/2410.02712) by Xiong et al. (2024)\n    \n*   Project page: [LLaVA-Critic blog](https://llava-vl.github.io/blog/llava-critic)\n    \n\n**LLaVA-Critic** is the first open-source **generalist multimodal evaluator**, explicitly trained to act as a judge across a wide range of vision–language tasks.\n\nPaper: [LLaVA-Critic: Learning to Evaluate Multimodal Models](https://arxiv.org/abs/2410.02712) by Xiong et al. (2024)\n\nProject page: [LLaVA-Critic blog](https://llava-vl.github.io/blog/llava-critic)\n\n#### Key Contributions\n\n*   Introduces **critic instruction-following data** for multimodal evaluation\n*   Supports both **pointwise scoring** and **pairwise ranking**\n*   Produces **scores + natural language justifications**\n*   Matches or exceeds GPT-4V / GPT-4o alignment on multiple benchmarks\n*   Enables **preference learning** for LMM alignment\n\n#### Training Data and Setup\n\n*   Each training instance follows:\n\n(Image,Question,Response,Criteria,Score,Reason)(Image,Question,Response,Criteria,Score,Reason)\n\n*   … or, in pairwise mode:\n\n(Image,Question,Response1,Response2,Criteria,Preference,Reason)(Image,Question,Response1,Response2,Criteria,Preference,Reason)\n\n*   The model is fine-tuned using **standard autoregressive cross-entropy loss** over both the judgment and justification tokens.\n\n#### Architecture\n\n*   **Backbone:** LLaVA-OneVision (7B / 72B)\n*   Multimodal encoder + language decoder\n*   Evaluation treated as instruction-following generation\n    \n*   The following figure ([source](https://arxiv.org/abs/2410.02712)) shows an example of LLaVA-Critic training data. The top block shows pointwise scoring, where LLaVA-Critic predicts a score to evaluate a single response’s quality; the bottom block illustrates pairwise ranking, where it rank response pairs. In both settings, LLaVA-Critic learns to provide reasons for its judgments.\n\nEvaluation treated as instruction-following generation\n\n![](/primers/ai/assets/LLM-as-a-judge/LLaVA_Critic.jpg)\n\n#### Evaluation Scenarios\n\n*   LLaVA-Critic is evaluated on:\n    \n    *   Visual chat benchmarks (LLaVA-in-the-Wild, LLaVA-Wilder)\n    *   Integrated capability benchmarks (MMVet)\n    *   Hallucination detection (MMHal-Bench)\n    *   Preference benchmarks (WildVision Arena)\n*   It shows strong **Pearson correlation** and **Kendall’s ττ\\\\tau** with GPT-4o and human judgments, even at 7B scale.\n    \n\nLLaVA-Critic is evaluated on:\n\n*   Visual chat benchmarks (LLaVA-in-the-Wild, LLaVA-Wilder)\n*   Integrated capability benchmarks (MMVet)\n*   Hallucination detection (MMHal-Bench)\n*   Preference benchmarks (WildVision Arena)\n\nIt shows strong **Pearson correlation** and **Kendall’s ττ\\\\tau** with GPT-4o and human judgments, even at 7B scale.",
    "contentLength": 13537,
    "wordCount": 324,
    "hasCode": false,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#llava-critic:-generalist-multimodal-judge"
  },
  {
    "id": "ai-LLM-as-a-judge-prometheus-vision-fine-grained-multimodal-evaluati-38",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Multimodal LLMs-as-Judges (LMM / VLM-as-a-Judge)",
    "title": "Prometheus-Vision: Fine-Grained Multimodal Evaluation",
    "order": 38,
    "orderInChapter": 4,
    "contentHtml": "<h4 id=\"overview-2\">Overview</h4>\n<ul>\n  <li>\n    <p><strong>Prometheus-Vision</strong> focuses on <strong>fine-grained, rubric-conditioned evaluation</strong>, emphasizing transparency and interpretability.</p>\n  </li>\n  <li>\n    <p>Paper: <a href=\"https://arxiv.org/abs/2401.06591\">Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation</a> by Lee et al. (2024)</p>\n  </li>\n  <li>\n    <p>Code and models: <a href=\"https://github.com/kaistAI/prometheus-vision\">Prometheus-Vision GitHub</a></p>\n  </li>\n</ul>\n<p><strong>Prometheus-Vision</strong> focuses on <strong>fine-grained, rubric-conditioned evaluation</strong>, emphasizing transparency and interpretability.</p>\n<p>Paper: <a href=\"https://arxiv.org/abs/2401.06591\">Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation</a> by Lee et al. (2024)</p>\n<p>Code and models: <a href=\"https://github.com/kaistAI/prometheus-vision\">Prometheus-Vision GitHub</a></p>\n<h4 id=\"core-idea\">Core Idea</h4>\n<ul>\n  <li>\n    <p>Instead of coarse criteria (helpfulness, relevance), Prometheus-Vision evaluates responses using <strong>custom, instance-specific rubrics</strong>, each with explicit score descriptions.</p>\n  </li>\n  <li>\n    <p>Each instance includes:</p>\n\n    <ul>\n      <li>Image</li>\n      <li>Instruction</li>\n      <li>Response to evaluate</li>\n      <li>Customized score rubric</li>\n      <li>Reference answer</li>\n      <li>Output: feedback + score</li>\n    </ul>\n  </li>\n  <li>\n    <p>This enables <strong>rubric-conditioned judgment</strong>, analogous to trained human graders.</p>\n  </li>\n  <li>\n    <p>The following figure (<a href=\"https://arxiv.org/abs/2401.06591\">source</a>) illustrates the fact that previous automatic metrics could not capture whether a VLM’s response is aware of aesthetic harmony. With Prometheus-Vision, users could define customized score rubrics that they care about instead of assessing based on coarse-grained criteria such as helpfulness, relevance, accuracy, and comprehensiveness. Each component within the Perception Collection consists of 5 input components: an instruction, a real-world image, a response to evaluate, a customized score rubric, and a reference answer. Based on this, Prometheus-Vision is trained to generate a language feedback and a score decision.</p>\n  </li>\n</ul>\n<p>Instead of coarse criteria (helpfulness, relevance), Prometheus-Vision evaluates responses using <strong>custom, instance-specific rubrics</strong>, each with explicit score descriptions.</p>\n<p>Each instance includes:</p>\n<ul>\n      <li>Image</li>\n      <li>Instruction</li>\n      <li>Response to evaluate</li>\n      <li>Customized score rubric</li>\n      <li>Reference answer</li>\n      <li>Output: feedback + score</li>\n    </ul>\n<p>This enables <strong>rubric-conditioned judgment</strong>, analogous to trained human graders.</p>\n<p>The following figure (<a href=\"https://arxiv.org/abs/2401.06591\">source</a>) illustrates the fact that previous automatic metrics could not capture whether a VLM’s response is aware of aesthetic harmony. With Prometheus-Vision, users could define customized score rubrics that they care about instead of assessing based on coarse-grained criteria such as helpfulness, relevance, accuracy, and comprehensiveness. Each component within the Perception Collection consists of 5 input components: an instruction, a real-world image, a response to evaluate, a customized score rubric, and a reference answer. Based on this, Prometheus-Vision is trained to generate a language feedback and a score decision.</p>\n<p><img src=\"/primers/ai/assets/LLM-as-a-judge/Prometheus-Vision_2.jpg\" alt=\"\"></p>\n<h4 id=\"architecture-and-training\">Architecture and Training</h4>\n<ul>\n  <li>Backbone: LLaVA-1.5 (7B / 13B)</li>\n  <li>\n    <p>Training objective: sequential generation of</p>\n\n    <ol>\n      <li>Language feedback (rationale)</li>\n      <li>Scalar score</li>\n    </ol>\n  </li>\n  <li>Loss function (autoregressive cross-entropy):</li>\n</ul>\n<p>Training objective: sequential generation of</p>\n<ol>\n      <li>Language feedback (rationale)</li>\n      <li>Scalar score</li>\n    </ol>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-114-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mo>=</mo><mo>&amp;#x2212;</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi></mrow></munder><mi>log</mi><mo>&amp;#x2061;</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mi>t</mi></msub><mo>&amp;#x2223;</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1299\" style=\"width: 10.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.013em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1008.96em, 3.648em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1300\"><span class=\"texatom\" id=\"MathJax-Span-1301\"><span class=\"mrow\" id=\"MathJax-Span-1302\"><span class=\"mi\" id=\"MathJax-Span-1303\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span class=\"mo\" id=\"MathJax-Span-1304\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-1305\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"munderover\" id=\"MathJax-Span-1306\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1307\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.21em, 4.273em, -999.997em); top: -2.862em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1308\"><span class=\"mrow\" id=\"MathJax-Span-1309\"><span class=\"mi\" id=\"MathJax-Span-1310\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-1311\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-1312\"></span><span class=\"mi\" id=\"MathJax-Span-1313\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">P</span><span class=\"mo\" id=\"MathJax-Span-1314\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1315\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1316\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1317\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1318\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-1319\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1320\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 2.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi></mrow></munder><mi>log</mi><mo>⁡</mo><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>t</mi></msub><mo>∣</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>A fixed delimiter phrase (“So the overall score is”) is used to stabilize decoding.</li>\n</ul>\n<h4 id=\"dataset-perception-collection\">Dataset: Perception Collection</h4>\n<ul>\n  <li>15K fine-grained score rubrics</li>\n  <li>5K real-world images</li>\n  <li>150K evaluated responses</li>\n  <li>Balanced score distribution (1–5)</li>\n</ul>\n<h4 id=\"empirical-results\">Empirical Results</h4>\n<ul>\n  <li>\n    <p>Prometheus-Vision achieves:</p>\n\n    <ul>\n      <li>Pearson correlation up to <strong>0.786</strong> with human evaluators</li>\n      <li>Competitive or superior alignment to GPT-4V on multiple benchmarks</li>\n      <li>High-quality natural language feedback preferred by humans</li>\n    </ul>\n  </li>\n</ul>\n<p>Prometheus-Vision achieves:</p>\n<ul>\n      <li>Pearson correlation up to <strong>0.786</strong> with human evaluators</li>\n      <li>Competitive or superior alignment to GPT-4V on multiple benchmarks</li>\n      <li>High-quality natural language feedback preferred by humans</li>\n    </ul>",
    "contentMarkdown": "#### Overview\n\n*   **Prometheus-Vision** focuses on **fine-grained, rubric-conditioned evaluation**, emphasizing transparency and interpretability.\n    \n*   Paper: [Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation](https://arxiv.org/abs/2401.06591) by Lee et al. (2024)\n    \n*   Code and models: [Prometheus-Vision GitHub](https://github.com/kaistAI/prometheus-vision)\n    \n\n**Prometheus-Vision** focuses on **fine-grained, rubric-conditioned evaluation**, emphasizing transparency and interpretability.\n\nPaper: [Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation](https://arxiv.org/abs/2401.06591) by Lee et al. (2024)\n\nCode and models: [Prometheus-Vision GitHub](https://github.com/kaistAI/prometheus-vision)\n\n#### Core Idea\n\n*   Instead of coarse criteria (helpfulness, relevance), Prometheus-Vision evaluates responses using **custom, instance-specific rubrics**, each with explicit score descriptions.\n    \n*   Each instance includes:\n    \n    *   Image\n    *   Instruction\n    *   Response to evaluate\n    *   Customized score rubric\n    *   Reference answer\n    *   Output: feedback + score\n*   This enables **rubric-conditioned judgment**, analogous to trained human graders.\n    \n*   The following figure ([source](https://arxiv.org/abs/2401.06591)) illustrates the fact that previous automatic metrics could not capture whether a VLM’s response is aware of aesthetic harmony. With Prometheus-Vision, users could define customized score rubrics that they care about instead of assessing based on coarse-grained criteria such as helpfulness, relevance, accuracy, and comprehensiveness. Each component within the Perception Collection consists of 5 input components: an instruction, a real-world image, a response to evaluate, a customized score rubric, and a reference answer. Based on this, Prometheus-Vision is trained to generate a language feedback and a score decision.\n    \n\nInstead of coarse criteria (helpfulness, relevance), Prometheus-Vision evaluates responses using **custom, instance-specific rubrics**, each with explicit score descriptions.\n\nEach instance includes:\n\n*   Image\n*   Instruction\n*   Response to evaluate\n*   Customized score rubric\n*   Reference answer\n*   Output: feedback + score\n\nThis enables **rubric-conditioned judgment**, analogous to trained human graders.\n\nThe following figure ([source](https://arxiv.org/abs/2401.06591)) illustrates the fact that previous automatic metrics could not capture whether a VLM’s response is aware of aesthetic harmony. With Prometheus-Vision, users could define customized score rubrics that they care about instead of assessing based on coarse-grained criteria such as helpfulness, relevance, accuracy, and comprehensiveness. Each component within the Perception Collection consists of 5 input components: an instruction, a real-world image, a response to evaluate, a customized score rubric, and a reference answer. Based on this, Prometheus-Vision is trained to generate a language feedback and a score decision.\n\n![](/primers/ai/assets/LLM-as-a-judge/Prometheus-Vision_2.jpg)\n\n#### Architecture and Training\n\n*   Backbone: LLaVA-1.5 (7B / 13B)\n*   Training objective: sequential generation of\n    \n    1.  Language feedback (rationale)\n    2.  Scalar score\n*   Loss function (autoregressive cross-entropy):\n\nTraining objective: sequential generation of\n\n1.  Language feedback (rationale)\n2.  Scalar score\n\n\\=−∑tlogP(yt∣x)L\\=−∑tlog⁡P(yt∣x)\n\n*   A fixed delimiter phrase (“So the overall score is”) is used to stabilize decoding.\n\n#### Dataset: Perception Collection\n\n*   15K fine-grained score rubrics\n*   5K real-world images\n*   150K evaluated responses\n*   Balanced score distribution (1–5)\n\n#### Empirical Results\n\n*   Prometheus-Vision achieves:\n    \n    *   Pearson correlation up to **0.786** with human evaluators\n    *   Competitive or superior alignment to GPT-4V on multiple benchmarks\n    *   High-quality natural language feedback preferred by humans\n\nPrometheus-Vision achieves:\n\n*   Pearson correlation up to **0.786** with human evaluators\n*   Competitive or superior alignment to GPT-4V on multiple benchmarks\n*   High-quality natural language feedback preferred by humans",
    "contentLength": 9955,
    "wordCount": 497,
    "hasCode": false,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#prometheus-vision:-fine-grained-multimodal-evaluation"
  },
  {
    "id": "ai-LLM-as-a-judge-multimodal-judges-in-preference-learning-39",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Multimodal LLMs-as-Judges (LMM / VLM-as-a-Judge)",
    "title": "Multimodal Judges in Preference Learning",
    "order": 39,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>\n    <p>Both LLaVA-Critic and Prometheus-Vision are used to generate <strong>reward signals</strong> for multimodal preference learning.</p>\n  </li>\n  <li>\n    <p>For example, LLaVA-Critic is used to generate pairwise preferences for <strong>Direct Preference Optimization (DPO)</strong>:</p>\n  </li>\n</ul>\n<p>Both LLaVA-Critic and Prometheus-Vision are used to generate <strong>reward signals</strong> for multimodal preference learning.</p>\n<p>For example, LLaVA-Critic is used to generate pairwise preferences for <strong>Direct Preference Optimization (DPO)</strong>:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-115-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>L</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>DPO</mtext></mrow></msub><mo>=</mo><mo>&amp;#x2212;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>E</mi></mrow><mrow><mo>[</mo><mrow><mi>log</mi><mo>&amp;#x2061;</mo><mi>&amp;#x03C3;</mi><mrow><mo>(</mo><mrow><mi>&amp;#x03B2;</mi><mo stretchy=&quot;false&quot;>(</mo><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>y</mi><mo>+</mo></msup><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>y</mi><mo>&amp;#x2212;</mo></msup><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1321\" style=\"width: 18.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 15.107em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.878em, 1014.95em, 3.44em, -999.997em); top: -2.914em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1322\"><span class=\"msubsup\" id=\"MathJax-Span-1323\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1324\"><span class=\"mrow\" id=\"MathJax-Span-1325\"><span class=\"mi\" id=\"MathJax-Span-1326\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1327\"><span class=\"mrow\" id=\"MathJax-Span-1328\"><span class=\"mtext\" id=\"MathJax-Span-1329\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">DPO</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1330\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-1331\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"texatom\" id=\"MathJax-Span-1332\"><span class=\"mrow\" id=\"MathJax-Span-1333\"><span class=\"mi\" id=\"MathJax-Span-1334\" style=\"font-family: STIXGeneral-Regular;\">𝔼</span></span></span><span class=\"mrow\" id=\"MathJax-Span-1335\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-1336\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">[</span></span><span class=\"mrow\" id=\"MathJax-Span-1337\"><span class=\"mi\" id=\"MathJax-Span-1338\" style=\"font-family: STIXGeneral-Regular;\">log</span><span class=\"mo\" id=\"MathJax-Span-1339\"></span><span class=\"mi\" id=\"MathJax-Span-1340\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mrow\" id=\"MathJax-Span-1341\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-1342\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-1343\"><span class=\"mi\" id=\"MathJax-Span-1344\" style=\"font-family: STIXGeneral-Italic;\">β<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1345\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1346\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1347\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1348\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1349\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.471em;\"><span class=\"mo\" id=\"MathJax-Span-1350\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1351\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1352\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-1353\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1354\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1355\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1356\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.471em;\"><span class=\"mo\" id=\"MathJax-Span-1357\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1358\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1359\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span class=\"mo\" id=\"MathJax-Span-1360\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">)</span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1361\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">]</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.919em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">L</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mtext>DPO</mtext></mrow></msub><mo>=</mo><mo>−</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">E</mi></mrow><mrow><mo>[</mo><mrow><mi>log</mi><mo>⁡</mo><mi>σ</mi><mrow><mo>(</mo><mrow><mi>β</mi><mo stretchy=\"false\">(</mo><mi>r</mi><mo stretchy=\"false\">(</mo><msup><mi>y</mi><mo>+</mo></msup><mo stretchy=\"false\">)</mo><mo>−</mo><mi>r</mi><mo stretchy=\"false\">(</mo><msup><mi>y</mi><mo>−</mo></msup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow></math></span></span></div>\n<ul>\n  <li>This enables <strong>RLAIF-style alignment</strong> for multimodal models, reducing reliance on costly human annotations.</li>\n</ul>",
    "contentMarkdown": "*   Both LLaVA-Critic and Prometheus-Vision are used to generate **reward signals** for multimodal preference learning.\n    \n*   For example, LLaVA-Critic is used to generate pairwise preferences for **Direct Preference Optimization (DPO)**:\n    \n\nBoth LLaVA-Critic and Prometheus-Vision are used to generate **reward signals** for multimodal preference learning.\n\nFor example, LLaVA-Critic is used to generate pairwise preferences for **Direct Preference Optimization (DPO)**:\n\nDPO\\=−𝔼\\[logσ(β(r(y+)−r(y−)))\\]LDPO\\=−E\\[log⁡σ(β(r(y+)−r(y−)))\\]\n\n*   This enables **RLAIF-style alignment** for multimodal models, reducing reliance on costly human annotations.",
    "contentLength": 8317,
    "wordCount": 73,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#multimodal-judges-in-preference-learning"
  },
  {
    "id": "ai-LLM-as-a-judge-when-to-use-multimodal-judges-40",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Multimodal LLMs-as-Judges (LMM / VLM-as-a-Judge)",
    "title": "When to Use Multimodal Judges",
    "order": 40,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>\n    <p>Multimodal LLMs-as-Judges are essential when:</p>\n\n    <ul>\n      <li>Evaluation depends on visual grounding</li>\n      <li>Hallucinations must be detected against images</li>\n      <li>Fine-grained visual attributes matter</li>\n      <li>Preference learning is multimodal</li>\n      <li>End-to-end vision–language behavior is evaluated</li>\n    </ul>\n  </li>\n  <li>\n    <p>They are less necessary when:</p>\n\n    <ul>\n      <li>Tasks are purely textual</li>\n      <li>Visual inputs can be deterministically verified</li>\n      <li>Latency budgets are extremely tight</li>\n    </ul>\n  </li>\n</ul>\n<p>Multimodal LLMs-as-Judges are essential when:</p>\n<ul>\n      <li>Evaluation depends on visual grounding</li>\n      <li>Hallucinations must be detected against images</li>\n      <li>Fine-grained visual attributes matter</li>\n      <li>Preference learning is multimodal</li>\n      <li>End-to-end vision–language behavior is evaluated</li>\n    </ul>\n<p>They are less necessary when:</p>\n<ul>\n      <li>Tasks are purely textual</li>\n      <li>Visual inputs can be deterministically verified</li>\n      <li>Latency budgets are extremely tight</li>\n    </ul>",
    "contentMarkdown": "*   Multimodal LLMs-as-Judges are essential when:\n    \n    *   Evaluation depends on visual grounding\n    *   Hallucinations must be detected against images\n    *   Fine-grained visual attributes matter\n    *   Preference learning is multimodal\n    *   End-to-end vision–language behavior is evaluated\n*   They are less necessary when:\n    \n    *   Tasks are purely textual\n    *   Visual inputs can be deterministically verified\n    *   Latency budgets are extremely tight\n\nMultimodal LLMs-as-Judges are essential when:\n\n*   Evaluation depends on visual grounding\n*   Hallucinations must be detected against images\n*   Fine-grained visual attributes matter\n*   Preference learning is multimodal\n*   End-to-end vision–language behavior is evaluated\n\nThey are less necessary when:\n\n*   Tasks are purely textual\n*   Visual inputs can be deterministically verified\n*   Latency budgets are extremely tight",
    "contentLength": 1170,
    "wordCount": 116,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#when-to-use-multimodal-judges"
  },
  {
    "id": "ai-LLM-as-a-judge-motivation-why-rl-for-judges-41",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Reinforcement Learning for LLMs-as-Judges",
    "title": "Motivation: Why RL for Judges?",
    "order": 41,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>\n    <p>Earlier LLM-as-a-Judge systems relied on:</p>\n\n    <ul>\n      <li>Zero-shot or few-shot prompting</li>\n      <li>Supervised fine-tuning on judgment examples</li>\n      <li>Preference-based optimization such as DPO</li>\n    </ul>\n  </li>\n  <li>\n    <p>While effective, these approaches suffer from two core limitations:</p>\n\n    <ol>\n      <li>\n        <p><strong>Reasoning is implicit and uncontrolled</strong>: Judges may produce chain-of-thought reasoning, but it is not directly optimized for correctness or consistency.</p>\n      </li>\n      <li>\n        <p><strong>Non-verifiable evaluation tasks</strong>: Many judgment tasks (e.g., helpfulness, writing quality) lack ground-truth labels, making supervised learning difficult.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p>The key insight behind J1 is that <strong>judgment quality can be optimized if evaluation tasks are reformulated to produce verifiable reward signals</strong>, enabling RL even for traditionally subjective judgments.</p>\n  </li>\n</ul>\n<p>Earlier LLM-as-a-Judge systems relied on:</p>\n<ul>\n      <li>Zero-shot or few-shot prompting</li>\n      <li>Supervised fine-tuning on judgment examples</li>\n      <li>Preference-based optimization such as DPO</li>\n    </ul>\n<p>While effective, these approaches suffer from two core limitations:</p>\n<ol>\n      <li>\n        <p><strong>Reasoning is implicit and uncontrolled</strong>: Judges may produce chain-of-thought reasoning, but it is not directly optimized for correctness or consistency.</p>\n      </li>\n      <li>\n        <p><strong>Non-verifiable evaluation tasks</strong>: Many judgment tasks (e.g., helpfulness, writing quality) lack ground-truth labels, making supervised learning difficult.</p>\n      </li>\n    </ol>\n<p><strong>Reasoning is implicit and uncontrolled</strong>: Judges may produce chain-of-thought reasoning, but it is not directly optimized for correctness or consistency.</p>\n<p><strong>Non-verifiable evaluation tasks</strong>: Many judgment tasks (e.g., helpfulness, writing quality) lack ground-truth labels, making supervised learning difficult.</p>\n<p>The key insight behind J1 is that <strong>judgment quality can be optimized if evaluation tasks are reformulated to produce verifiable reward signals</strong>, enabling RL even for traditionally subjective judgments.</p>",
    "contentMarkdown": "*   Earlier LLM-as-a-Judge systems relied on:\n    \n    *   Zero-shot or few-shot prompting\n    *   Supervised fine-tuning on judgment examples\n    *   Preference-based optimization such as DPO\n*   While effective, these approaches suffer from two core limitations:\n    \n    1.  **Reasoning is implicit and uncontrolled**: Judges may produce chain-of-thought reasoning, but it is not directly optimized for correctness or consistency.\n        \n    2.  **Non-verifiable evaluation tasks**: Many judgment tasks (e.g., helpfulness, writing quality) lack ground-truth labels, making supervised learning difficult.\n        \n*   The key insight behind J1 is that **judgment quality can be optimized if evaluation tasks are reformulated to produce verifiable reward signals**, enabling RL even for traditionally subjective judgments.\n    \n\nEarlier LLM-as-a-Judge systems relied on:\n\n*   Zero-shot or few-shot prompting\n*   Supervised fine-tuning on judgment examples\n*   Preference-based optimization such as DPO\n\nWhile effective, these approaches suffer from two core limitations:\n\n1.  **Reasoning is implicit and uncontrolled**: Judges may produce chain-of-thought reasoning, but it is not directly optimized for correctness or consistency.\n    \n2.  **Non-verifiable evaluation tasks**: Many judgment tasks (e.g., helpfulness, writing quality) lack ground-truth labels, making supervised learning difficult.\n    \n\n**Reasoning is implicit and uncontrolled**: Judges may produce chain-of-thought reasoning, but it is not directly optimized for correctness or consistency.\n\n**Non-verifiable evaluation tasks**: Many judgment tasks (e.g., helpfulness, writing quality) lack ground-truth labels, making supervised learning difficult.\n\nThe key insight behind J1 is that **judgment quality can be optimized if evaluation tasks are reformulated to produce verifiable reward signals**, enabling RL even for traditionally subjective judgments.",
    "contentLength": 2345,
    "wordCount": 238,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#motivation:-why-rl-for-judges?"
  },
  {
    "id": "ai-LLM-as-a-judge-core-idea-thinking-llm-as-a-judge-via-rl-42",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Reinforcement Learning for LLMs-as-Judges",
    "title": "Core Idea: Thinking-LLM-as-a-Judge Via RL",
    "order": 42,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>\n    <p>J1 reframes LLM-as-a-Judge as a <strong>reasoning-first policy optimization problem</strong>.</p>\n  </li>\n  <li>\n    <p>Instead of directly predicting a score or preference, the judge is trained to:</p>\n\n    <ol>\n      <li>Generate explicit chain-of-thought reasoning</li>\n      <li>Arrive at a final verdict or score</li>\n      <li>Receive reward based on judgment correctness and consistency</li>\n    </ol>\n  </li>\n  <li>\n    <p>Formally, the judge produces:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-116-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x223C;</mo><msub><mi>&amp;#x03C0;</mi><mi>&amp;#x03B8;</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mo>&amp;#x22C5;</mo><mo>&amp;#x2223;</mo><mi>x</mi><mo>,</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1362\" style=\"width: 8.961em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.451em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1007.4em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1363\"><span class=\"mo\" id=\"MathJax-Span-1364\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1365\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1366\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1367\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">y</span><span class=\"mo\" id=\"MathJax-Span-1368\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1369\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∼</span><span class=\"msubsup\" id=\"MathJax-Span-1370\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1371\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-1372\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1373\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mo\" id=\"MathJax-Span-1374\" style=\"font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-1375\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-1376\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1377\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1378\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">a</span><span class=\"mo\" id=\"MathJax-Span-1379\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>∼</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><mo>⋅</mo><mo>∣</mo><mi>x</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-116\">(t, y) \\sim \\pi_\\theta(\\cdot \\mid x, a)</script>\n\n    <ul>\n      <li>\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-117-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1380\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1381\"><span class=\"mi\" id=\"MathJax-Span-1382\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-117\">x</script> is the instruction or prompt</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-118-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>a</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1383\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1384\"><span class=\"mi\" id=\"MathJax-Span-1385\" style=\"font-family: STIXGeneral-Italic;\">a</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>a</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-118\">a</script> is one or more candidate responses</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-119-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1386\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1387\"><span class=\"mi\" id=\"MathJax-Span-1388\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-119\">t</script> is the generated reasoning trace</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-120-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1389\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1390\"><span class=\"mi\" id=\"MathJax-Span-1391\" style=\"font-family: STIXGeneral-Italic;\">y</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-120\">y</script> is the final score or verdict</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>RL is used to optimize <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-121-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03C0;</mi><mi>&amp;#x03B8;</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1392\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1393\"><span class=\"msubsup\" id=\"MathJax-Span-1394\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1395\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-1396\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>π</mi><mi>θ</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-121\">\\pi_\\theta</script> such that both <strong>the reasoning and the final judgment</strong> improve over time.</p>\n  </li>\n</ul>\n<p>J1 reframes LLM-as-a-Judge as a <strong>reasoning-first policy optimization problem</strong>.</p>\n<p>Instead of directly predicting a score or preference, the judge is trained to:</p>\n<ol>\n      <li>Generate explicit chain-of-thought reasoning</li>\n      <li>Arrive at a final verdict or score</li>\n      <li>Receive reward based on judgment correctness and consistency</li>\n    </ol>\n<p>Formally, the judge produces:</p>\n<ul>\n      <li>\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-117-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1380\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1381\"><span class=\"mi\" id=\"MathJax-Span-1382\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-117\">x</script> is the instruction or prompt</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-118-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>a</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1383\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1384\"><span class=\"mi\" id=\"MathJax-Span-1385\" style=\"font-family: STIXGeneral-Italic;\">a</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>a</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-118\">a</script> is one or more candidate responses</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-119-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1386\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1387\"><span class=\"mi\" id=\"MathJax-Span-1388\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-119\">t</script> is the generated reasoning trace</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-120-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1389\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1390\"><span class=\"mi\" id=\"MathJax-Span-1391\" style=\"font-family: STIXGeneral-Italic;\">y</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-120\">y</script> is the final score or verdict</li>\n        </ul>\n      </li>\n    </ul>\n<p>where:</p>\n<ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-117-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1380\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1381\"><span class=\"mi\" id=\"MathJax-Span-1382\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-117\">x</script> is the instruction or prompt</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-118-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>a</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1383\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1384\"><span class=\"mi\" id=\"MathJax-Span-1385\" style=\"font-family: STIXGeneral-Italic;\">a</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>a</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-118\">a</script> is one or more candidate responses</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-119-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1386\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1387\"><span class=\"mi\" id=\"MathJax-Span-1388\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-119\">t</script> is the generated reasoning trace</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-120-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1389\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1390\"><span class=\"mi\" id=\"MathJax-Span-1391\" style=\"font-family: STIXGeneral-Italic;\">y</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-120\">y</script> is the final score or verdict</li>\n        </ul>\n<p>RL is used to optimize <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-121-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03C0;</mi><mi>&amp;#x03B8;</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1392\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1393\"><span class=\"msubsup\" id=\"MathJax-Span-1394\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1395\" style=\"font-family: STIXGeneral-Italic;\">π<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-1396\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>π</mi><mi>θ</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-121\">\\pi_\\theta</script> such that both <strong>the reasoning and the final judgment</strong> improve over time.</p>",
    "contentMarkdown": "*   J1 reframes LLM-as-a-Judge as a **reasoning-first policy optimization problem**.\n    \n*   Instead of directly predicting a score or preference, the judge is trained to:\n    \n    1.  Generate explicit chain-of-thought reasoning\n    2.  Arrive at a final verdict or score\n    3.  Receive reward based on judgment correctness and consistency\n*   Formally, the judge produces:\n    \n    (t,y)∼πθ(⋅∣x,a)(t,y)∼πθ(⋅∣x,a)\n    \n    (t, y) \\\\sim \\\\pi\\_\\\\theta(\\\\cdot \\\\mid x, a)\n    *   where:\n        \n        *   xxx is the instruction or prompt\n        *   aaa is one or more candidate responses\n        *   ttt is the generated reasoning trace\n        *   yyy is the final score or verdict\n*   RL is used to optimize πθπθ\\\\pi\\_\\\\theta such that both **the reasoning and the final judgment** improve over time.\n    \n\nJ1 reframes LLM-as-a-Judge as a **reasoning-first policy optimization problem**.\n\nInstead of directly predicting a score or preference, the judge is trained to:\n\n1.  Generate explicit chain-of-thought reasoning\n2.  Arrive at a final verdict or score\n3.  Receive reward based on judgment correctness and consistency\n\nFormally, the judge produces:\n\n*   where:\n    \n    *   xxx is the instruction or prompt\n    *   aaa is one or more candidate responses\n    *   ttt is the generated reasoning trace\n    *   yyy is the final score or verdict\n\nwhere:\n\n*   xxx is the instruction or prompt\n*   aaa is one or more candidate responses\n*   ttt is the generated reasoning trace\n*   yyy is the final score or verdict\n\nRL is used to optimize πθπθ\\\\pi\\_\\\\theta such that both **the reasoning and the final judgment** improve over time.",
    "contentLength": 25018,
    "wordCount": 239,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#core-idea:-thinking-llm-as-a-judge-via-rl"
  },
  {
    "id": "ai-LLM-as-a-judge-unified-verifiable-training-via-synthetic-data-43",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Reinforcement Learning for LLMs-as-Judges",
    "title": "Unified Verifiable Training Via Synthetic Data",
    "order": 43,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>\n    <p>A central challenge is that many evaluation tasks are <em>non-verifiable</em>. J1 addresses this by converting <strong>both verifiable and non-verifiable evaluation tasks into a unified, verifiable format</strong>.</p>\n  </li>\n  <li>\n    <p>This is achieved through <strong>synthetic preference pair generation</strong>, building on techniques from\n<a href=\"https://arxiv.org/abs/2408.02666\">Self-Taught Evaluators</a> by Wang et al. (2024) and\n<a href=\"https://openreview.net/forum?id=PNRznmmWP7\">Learning to Plan &amp; Reason for Evaluation with Thinking-LLM-as-a-Judge</a> by Saha et al. (2025).</p>\n  </li>\n  <li>\n    <p>Key aspects:</p>\n\n    <ul>\n      <li>Preference pairs <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-122-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1397\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1398\"><span class=\"mo\" id=\"MathJax-Span-1399\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1400\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1401\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1402\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">b</span><span class=\"mo\" id=\"MathJax-Span-1403\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-122\">(a, b)</script> are generated synthetically</li>\n      <li>One response is designated as preferred</li>\n      <li>The judge’s task becomes <em>predicting the better response</em>, a verifiable objective</li>\n    </ul>\n  </li>\n  <li>\n    <p>This reformulation allows RL to be applied uniformly across QA, reasoning, writing, safety, and instruction-following tasks.</p>\n  </li>\n</ul>\n<p>A central challenge is that many evaluation tasks are <em>non-verifiable</em>. J1 addresses this by converting <strong>both verifiable and non-verifiable evaluation tasks into a unified, verifiable format</strong>.</p>\n<p>This is achieved through <strong>synthetic preference pair generation</strong>, building on techniques from\n<a href=\"https://arxiv.org/abs/2408.02666\">Self-Taught Evaluators</a> by Wang et al. (2024) and\n<a href=\"https://openreview.net/forum?id=PNRznmmWP7\">Learning to Plan &amp; Reason for Evaluation with Thinking-LLM-as-a-Judge</a> by Saha et al. (2025).</p>\n<p>Key aspects:</p>\n<ul>\n      <li>Preference pairs <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-122-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1397\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1398\"><span class=\"mo\" id=\"MathJax-Span-1399\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1400\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1401\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1402\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">b</span><span class=\"mo\" id=\"MathJax-Span-1403\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-122\">(a, b)</script> are generated synthetically</li>\n      <li>One response is designated as preferred</li>\n      <li>The judge’s task becomes <em>predicting the better response</em>, a verifiable objective</li>\n    </ul>\n<p>This reformulation allows RL to be applied uniformly across QA, reasoning, writing, safety, and instruction-following tasks.</p>",
    "contentMarkdown": "*   A central challenge is that many evaluation tasks are _non-verifiable_. J1 addresses this by converting **both verifiable and non-verifiable evaluation tasks into a unified, verifiable format**.\n    \n*   This is achieved through **synthetic preference pair generation**, building on techniques from [Self-Taught Evaluators](https://arxiv.org/abs/2408.02666) by Wang et al. (2024) and [Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge](https://openreview.net/forum?id=PNRznmmWP7) by Saha et al. (2025).\n    \n*   Key aspects:\n    \n    *   Preference pairs (a,b)(a,b)(a, b) are generated synthetically\n    *   One response is designated as preferred\n    *   The judge’s task becomes _predicting the better response_, a verifiable objective\n*   This reformulation allows RL to be applied uniformly across QA, reasoning, writing, safety, and instruction-following tasks.\n    \n\nA central challenge is that many evaluation tasks are _non-verifiable_. J1 addresses this by converting **both verifiable and non-verifiable evaluation tasks into a unified, verifiable format**.\n\nThis is achieved through **synthetic preference pair generation**, building on techniques from [Self-Taught Evaluators](https://arxiv.org/abs/2408.02666) by Wang et al. (2024) and [Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge](https://openreview.net/forum?id=PNRznmmWP7) by Saha et al. (2025).\n\nKey aspects:\n\n*   Preference pairs (a,b)(a,b)(a, b) are generated synthetically\n*   One response is designated as preferred\n*   The judge’s task becomes _predicting the better response_, a verifiable objective\n\nThis reformulation allows RL to be applied uniformly across QA, reasoning, writing, safety, and instruction-following tasks.",
    "contentLength": 5517,
    "wordCount": 214,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#unified-verifiable-training-via-synthetic-data"
  },
  {
    "id": "ai-LLM-as-a-judge-reward-design-optimizing-judgment-quality-44",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Reinforcement Learning for LLMs-as-Judges",
    "title": "Reward Design: Optimizing Judgment Quality",
    "order": 44,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>J1 uses <strong>rule-based, verifiable rewards</strong>, avoiding the need for a separate learned reward model.</li>\n</ul>\n<h4 id=\"verdict-correctness-reward\">Verdict Correctness Reward</h4>\n<ul>\n  <li>The primary reward is binary:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-123-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>correct</mtext></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign=&quot;left left&quot; rowspacing=&quot;.2em&quot; columnspacing=&quot;1em&quot; displaystyle=&quot;false&quot;><mtr><mtd><mn>1</mn></mtd><mtd><mtext>if final verdict is correct</mtext></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mtext>otherwise</mtext></mtd></mtr></mtable><mo fence=&quot;true&quot; stretchy=&quot;true&quot; symmetric=&quot;true&quot;></mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1404\" style=\"width: 19.273em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 16.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.294em, 1016.04em, 4.846em, -999.997em); top: -3.799em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1405\"><span class=\"msubsup\" id=\"MathJax-Span-1406\"><span style=\"display: inline-block; position: relative; width: 2.451em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1407\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-1408\"><span class=\"mrow\" id=\"MathJax-Span-1409\"><span class=\"mtext\" id=\"MathJax-Span-1410\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">correct</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1411\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mrow\" id=\"MathJax-Span-1412\" style=\"padding-left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-1413\" style=\"vertical-align: -0.466em;\"><span><span style=\"font-size: 111%; font-family: STIXSizeTwoSym;\">{</span></span></span><span class=\"mtable\" id=\"MathJax-Span-1414\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 11.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.607em, 1000.47em, 4.846em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.581em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-1415\"><span class=\"mrow\" id=\"MathJax-Span-1416\"><span class=\"mn\" id=\"MathJax-Span-1417\" style=\"font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.331em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-1421\"><span class=\"mrow\" id=\"MathJax-Span-1422\"><span class=\"mn\" id=\"MathJax-Span-1423\" style=\"font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.607em, 1009.69em, 4.846em, -999.997em); top: -4.008em; left: 1.617em;\"><span style=\"display: inline-block; position: relative; width: 9.69em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1009.69em, 4.169em, -999.997em); top: -4.581em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-1418\"><span class=\"mrow\" id=\"MathJax-Span-1419\"><span class=\"mtext\" id=\"MathJax-Span-1420\" style=\"font-family: STIXGeneral-Regular;\">if final verdict is correct</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.86em, 4.169em, -999.997em); top: -3.331em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-1424\"><span class=\"mrow\" id=\"MathJax-Span-1425\"><span class=\"mtext\" id=\"MathJax-Span-1426\" style=\"font-family: STIXGeneral-Regular;\">otherwise</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1427\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.805em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 2.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>r</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>correct</mtext></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign=\"left left\" rowspacing=\".2em\" columnspacing=\"1em\" displaystyle=\"false\"><mtr><mtd><mn>1</mn></mtd><mtd><mtext>if final verdict is correct</mtext></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mtext>otherwise</mtext></mtd></mtr></mtable><mo fence=\"true\" stretchy=\"true\" symmetric=\"true\"></mo></mrow></math></span></span></div>\n<h4 id=\"consistency-reward-mitigating-positional-bias\">Consistency Reward (Mitigating Positional Bias)</h4>\n<ul>\n  <li>\n    <p>To combat positional bias in pairwise judging, J1 introduces a <strong>consistency reward</strong>:</p>\n  </li>\n  <li>Each preference pair is evaluated in both orders <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-124-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1428\" style=\"width: 3.648em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.023em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.97em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1429\"><span class=\"mo\" id=\"MathJax-Span-1430\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1431\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1432\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1433\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">a</span><span class=\"mo\" id=\"MathJax-Span-1434\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1435\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">b</span><span class=\"mo\" id=\"MathJax-Span-1436\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-124\">(x, a, b)</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-125-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1437\" style=\"width: 3.648em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.023em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.97em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1438\"><span class=\"mo\" id=\"MathJax-Span-1439\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1440\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1441\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1442\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">b</span><span class=\"mo\" id=\"MathJax-Span-1443\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1444\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">a</span><span class=\"mo\" id=\"MathJax-Span-1445\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-125\">(x, b, a)</script></li>\n  <li>Reward is granted only if the judge is correct in both cases</li>\n</ul>\n<p>To combat positional bias in pairwise judging, J1 introduces a <strong>consistency reward</strong>:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-126-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>consistency</mtext></mrow></msub><mo>=</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn mathvariant=&quot;double-struck&quot;>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-OPEN&quot;><mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;>[</mo></mrow><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>a</mi><mo>&amp;#x227B;</mo><mi>b</mi></mrow></msub><mo>=</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>b</mi><mo>&amp;#x227A;</mo><mi>a</mi></mrow></msub><mrow class=&quot;MJX-TeXAtom-CLOSE&quot;><mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;>]</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1446\" style=\"width: 13.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.086em, 1010.63em, 3.753em, -999.997em); top: -3.174em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1447\"><span class=\"msubsup\" id=\"MathJax-Span-1448\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1449\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-1450\"><span class=\"mrow\" id=\"MathJax-Span-1451\"><span class=\"mtext\" id=\"MathJax-Span-1452\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">consistency</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1453\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"texatom\" id=\"MathJax-Span-1454\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-1455\"><span class=\"mn\" id=\"MathJax-Span-1456\" style=\"font-family: STIXGeneral-Regular;\">𝟙</span></span></span><span class=\"texatom\" id=\"MathJax-Span-1457\"><span class=\"mrow\" id=\"MathJax-Span-1458\"><span class=\"mo\" id=\"MathJax-Span-1459\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">[</span></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1460\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1461\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1462\"><span class=\"mrow\" id=\"MathJax-Span-1463\"><span class=\"mi\" id=\"MathJax-Span-1464\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-1465\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">≻</span><span class=\"mi\" id=\"MathJax-Span-1466\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1467\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-1468\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1469\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1470\"><span class=\"mrow\" id=\"MathJax-Span-1471\"><span class=\"mi\" id=\"MathJax-Span-1472\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span class=\"mo\" id=\"MathJax-Span-1473\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">≺</span><span class=\"mi\" id=\"MathJax-Span-1474\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-1475\"><span class=\"mrow\" id=\"MathJax-Span-1476\"><span class=\"mo\" id=\"MathJax-Span-1477\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">]</span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.18em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>r</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>consistency</mtext></mrow></msub><mo>=</mo><mrow class=\"MJX-TeXAtom-ORD\"><mn mathvariant=\"double-struck\">1</mn></mrow><mrow class=\"MJX-TeXAtom-OPEN\"><mo maxsize=\"1.2em\" minsize=\"1.2em\">[</mo></mrow><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>a</mi><mo>≻</mo><mi>b</mi></mrow></msub><mo>=</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>b</mi><mo>≺</mo><mi>a</mi></mrow></msub><mrow class=\"MJX-TeXAtom-CLOSE\"><mo maxsize=\"1.2em\" minsize=\"1.2em\">]</mo></mrow></math></span></span></div>\n<ul>\n  <li>This directly targets a long-standing failure mode in LLM judges, previously identified in\n<a href=\"https://aclanthology.org/2024.acl-long.511/\">Large Language Models Are Not Fair Evaluators</a> by Wang et al. (2024).</li>\n</ul>",
    "contentMarkdown": "*   J1 uses **rule-based, verifiable rewards**, avoiding the need for a separate learned reward model.\n\n#### Verdict Correctness Reward\n\n*   The primary reward is binary:\n\nrcorrect\\={10if final verdict is correctotherwisercorrect\\={1if final verdict is correct0otherwise\n\n#### Consistency Reward (Mitigating Positional Bias)\n\n*   To combat positional bias in pairwise judging, J1 introduces a **consistency reward**:\n    \n*   Each preference pair is evaluated in both orders (x,a,b)(x,a,b)(x, a, b) and (x,b,a)(x,b,a)(x, b, a)\n*   Reward is granted only if the judge is correct in both cases\n\nTo combat positional bias in pairwise judging, J1 introduces a **consistency reward**:\n\nrconsistency\\=𝟙\\[ya≻b\\=yb≺a\\]rconsistency\\=1\\[ya≻b\\=yb≺a\\]\n\n*   This directly targets a long-standing failure mode in LLM judges, previously identified in [Large Language Models Are Not Fair Evaluators](https://aclanthology.org/2024.acl-long.511/) by Wang et al. (2024).",
    "contentLength": 16911,
    "wordCount": 121,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#reward-design:-optimizing-judgment-quality"
  },
  {
    "id": "ai-LLM-as-a-judge-rl-algorithm-grpo-45",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Reinforcement Learning for LLMs-as-Judges",
    "title": "RL Algorithm: GRPO",
    "order": 45,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>\n    <p>J1 uses <strong>Group Relative Policy Optimization (GRPO)</strong>, introduced in\n<a href=\"https://arxiv.org/abs/2402.03300\">DeepSeekMath</a> by Shao et al. (2024), and later popularized by\n<a href=\"https://arxiv.org/abs/2501.12948\">DeepSeek-R1</a> by Guo et al. (2025).</p>\n  </li>\n  <li>\n    <p>GRPO optimizes relative outcomes within sampled groups rather than absolute rewards, making it well-suited for preference-based judgment tasks.</p>\n  </li>\n  <li>\n    <p>Conceptually, the objective maximizes:</p>\n  </li>\n</ul>\n<p>J1 uses <strong>Group Relative Policy Optimization (GRPO)</strong>, introduced in\n<a href=\"https://arxiv.org/abs/2402.03300\">DeepSeekMath</a> by Shao et al. (2024), and later popularized by\n<a href=\"https://arxiv.org/abs/2501.12948\">DeepSeek-R1</a> by Guo et al. (2025).</p>\n<p>GRPO optimizes relative outcomes within sampled groups rather than absolute rewards, making it well-suited for preference-based judgment tasks.</p>\n<p>Conceptually, the objective maximizes:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-127-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>E</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msub><mrow class=&quot;MJX-TeXAtom-OPEN&quot;><mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;>[</mo></mrow><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>E</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><msup><mi>t</mi><mo>&amp;#x2032;</mo></msup><mo>,</mo><msup><mi>y</mi><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo></mrow></msub><mo stretchy=&quot;false&quot;>[</mo><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>t</mi><mo>&amp;#x2032;</mo></msup><mo>,</mo><msup><mi>y</mi><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>]</mo><mrow class=&quot;MJX-TeXAtom-CLOSE&quot;><mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;>]</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1478\" style=\"width: 14.378em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.982em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.086em, 1011.77em, 3.753em, -999.997em); top: -3.174em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1479\"><span class=\"msubsup\" id=\"MathJax-Span-1480\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1481\"><span class=\"mrow\" id=\"MathJax-Span-1482\"><span class=\"mi\" id=\"MathJax-Span-1483\" style=\"font-family: STIXGeneral-Regular;\">𝔼</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1484\"><span class=\"mrow\" id=\"MathJax-Span-1485\"><span class=\"mo\" id=\"MathJax-Span-1486\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1487\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1488\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1489\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span class=\"mo\" id=\"MathJax-Span-1490\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-1491\"><span class=\"mrow\" id=\"MathJax-Span-1492\"><span class=\"mo\" id=\"MathJax-Span-1493\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">[</span></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-1494\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1495\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1496\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1497\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1498\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">y</span><span class=\"mo\" id=\"MathJax-Span-1499\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1500\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-1501\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1502\"><span class=\"mrow\" id=\"MathJax-Span-1503\"><span class=\"mi\" id=\"MathJax-Span-1504\" style=\"font-family: STIXGeneral-Regular;\">𝔼</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1505\"><span class=\"mrow\" id=\"MathJax-Span-1506\"><span class=\"mo\" id=\"MathJax-Span-1507\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"msup\" id=\"MathJax-Span-1508\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.492em, 1000.21em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1509\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.216em; left: 0.263em;\"><span class=\"mo\" id=\"MathJax-Span-1510\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1511\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"msup\" id=\"MathJax-Span-1512\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1513\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.216em; left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-1514\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1515\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1516\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mi\" id=\"MathJax-Span-1517\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1518\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msup\" id=\"MathJax-Span-1519\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1520\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.367em;\"><span class=\"mo\" id=\"MathJax-Span-1521\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1522\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msup\" id=\"MathJax-Span-1523\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1524\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.471em;\"><span class=\"mo\" id=\"MathJax-Span-1525\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1526\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1527\" style=\"font-family: STIXGeneral-Regular;\">]</span><span class=\"texatom\" id=\"MathJax-Span-1528\"><span class=\"mrow\" id=\"MathJax-Span-1529\"><span class=\"mo\" id=\"MathJax-Span-1530\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">]</span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.18em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">E</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></msub><mrow class=\"MJX-TeXAtom-OPEN\"><mo maxsize=\"1.2em\" minsize=\"1.2em\">[</mo></mrow><mi>r</mi><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>−</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">E</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><msup><mi>t</mi><mo>′</mo></msup><mo>,</mo><msup><mi>y</mi><mo>′</mo></msup><mo stretchy=\"false\">)</mo></mrow></msub><mo stretchy=\"false\">[</mo><mi>r</mi><mo stretchy=\"false\">(</mo><msup><mi>t</mi><mo>′</mo></msup><mo>,</mo><msup><mi>y</mi><mo>′</mo></msup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo><mrow class=\"MJX-TeXAtom-CLOSE\"><mo maxsize=\"1.2em\" minsize=\"1.2em\">]</mo></mrow></math></span></span></div>\n<ul>\n  <li>This stabilizes training and encourages <em>better-than-baseline</em> reasoning trajectories.</li>\n</ul>",
    "contentMarkdown": "*   J1 uses **Group Relative Policy Optimization (GRPO)**, introduced in [DeepSeekMath](https://arxiv.org/abs/2402.03300) by Shao et al. (2024), and later popularized by [DeepSeek-R1](https://arxiv.org/abs/2501.12948) by Guo et al. (2025).\n    \n*   GRPO optimizes relative outcomes within sampled groups rather than absolute rewards, making it well-suited for preference-based judgment tasks.\n    \n*   Conceptually, the objective maximizes:\n    \n\nJ1 uses **Group Relative Policy Optimization (GRPO)**, introduced in [DeepSeekMath](https://arxiv.org/abs/2402.03300) by Shao et al. (2024), and later popularized by [DeepSeek-R1](https://arxiv.org/abs/2501.12948) by Guo et al. (2025).\n\nGRPO optimizes relative outcomes within sampled groups rather than absolute rewards, making it well-suited for preference-based judgment tasks.\n\nConceptually, the objective maximizes:\n\n𝔼(t,y)\\[r(t,y)−𝔼(t′,y′)\\[r(t′,y′)\\]\\]E(t,y)\\[r(t,y)−E(t′,y′)\\[r(t′,y′)\\]\\]\n\n*   This stabilizes training and encourages _better-than-baseline_ reasoning trajectories.",
    "contentLength": 11811,
    "wordCount": 107,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#rl-algorithm:-grpo"
  },
  {
    "id": "ai-LLM-as-a-judge-judge-formulations-trained-with-rl-46",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Reinforcement Learning for LLMs-as-Judges",
    "title": "Judge Formulations Trained with RL",
    "order": 46,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>\n    <p>J1 explores multiple judge formulations within the same RL framework:</p>\n\n    <ul>\n      <li>\n        <p><strong>Pairwise J1 (verdict-based)</strong>: Outputs reasoning <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-128-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1531\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1532\"><span class=\"mi\" id=\"MathJax-Span-1533\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-128\">t</script> and a final preference verdict</p>\n      </li>\n      <li>\n        <p><strong>Pairwise J1 (score-based)</strong>: Outputs reasoning plus real-valued scores <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-129-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>a</mi></msub><mo>,</mo><msub><mi>s</mi><mi>b</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1534\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1002.14em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1535\"><span class=\"msubsup\" id=\"MathJax-Span-1536\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1537\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-1538\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1539\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1540\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1541\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-1542\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>a</mi></msub><mo>,</mo><msub><mi>s</mi><mi>b</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-129\">s_a, s_b</script></p>\n      </li>\n      <li>\n        <p><strong>Pointwise J1</strong>: Outputs a scalar quality score for a single response</p>\n      </li>\n      <li>\n        <p><strong>Multitask J1</strong>: Jointly trained to perform both pointwise and pairwise judging</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>A key contribution is showing that <strong>pointwise judges can be trained purely from pairwise supervision</strong>, yielding inherently position-consistent evaluators.</p>\n  </li>\n</ul>\n<p>J1 explores multiple judge formulations within the same RL framework:</p>\n<ul>\n      <li>\n        <p><strong>Pairwise J1 (verdict-based)</strong>: Outputs reasoning <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-128-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1531\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1532\"><span class=\"mi\" id=\"MathJax-Span-1533\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-128\">t</script> and a final preference verdict</p>\n      </li>\n      <li>\n        <p><strong>Pairwise J1 (score-based)</strong>: Outputs reasoning plus real-valued scores <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-129-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>a</mi></msub><mo>,</mo><msub><mi>s</mi><mi>b</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1534\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1002.14em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1535\"><span class=\"msubsup\" id=\"MathJax-Span-1536\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1537\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-1538\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1539\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1540\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1541\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-1542\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>a</mi></msub><mo>,</mo><msub><mi>s</mi><mi>b</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-129\">s_a, s_b</script></p>\n      </li>\n      <li>\n        <p><strong>Pointwise J1</strong>: Outputs a scalar quality score for a single response</p>\n      </li>\n      <li>\n        <p><strong>Multitask J1</strong>: Jointly trained to perform both pointwise and pairwise judging</p>\n      </li>\n    </ul>\n<p><strong>Pairwise J1 (verdict-based)</strong>: Outputs reasoning <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-128-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1531\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1532\"><span class=\"mi\" id=\"MathJax-Span-1533\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-128\">t</script> and a final preference verdict</p>\n<p><strong>Pairwise J1 (score-based)</strong>: Outputs reasoning plus real-valued scores <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-129-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>s</mi><mi>a</mi></msub><mo>,</mo><msub><mi>s</mi><mi>b</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1534\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1002.14em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1535\"><span class=\"msubsup\" id=\"MathJax-Span-1536\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1537\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-1538\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1539\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1540\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1541\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-1542\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>s</mi><mi>a</mi></msub><mo>,</mo><msub><mi>s</mi><mi>b</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-129\">s_a, s_b</script></p>\n<p><strong>Pointwise J1</strong>: Outputs a scalar quality score for a single response</p>\n<p><strong>Multitask J1</strong>: Jointly trained to perform both pointwise and pairwise judging</p>\n<p>A key contribution is showing that <strong>pointwise judges can be trained purely from pairwise supervision</strong>, yielding inherently position-consistent evaluators.</p>",
    "contentMarkdown": "*   J1 explores multiple judge formulations within the same RL framework:\n    \n    *   **Pairwise J1 (verdict-based)**: Outputs reasoning ttt and a final preference verdict\n        \n    *   **Pairwise J1 (score-based)**: Outputs reasoning plus real-valued scores sa,sbsa,sbs\\_a, s\\_b\n        \n    *   **Pointwise J1**: Outputs a scalar quality score for a single response\n        \n    *   **Multitask J1**: Jointly trained to perform both pointwise and pairwise judging\n        \n*   A key contribution is showing that **pointwise judges can be trained purely from pairwise supervision**, yielding inherently position-consistent evaluators.\n    \n\nJ1 explores multiple judge formulations within the same RL framework:\n\n*   **Pairwise J1 (verdict-based)**: Outputs reasoning ttt and a final preference verdict\n    \n*   **Pairwise J1 (score-based)**: Outputs reasoning plus real-valued scores sa,sbsa,sbs\\_a, s\\_b\n    \n*   **Pointwise J1**: Outputs a scalar quality score for a single response\n    \n*   **Multitask J1**: Jointly trained to perform both pointwise and pairwise judging\n    \n\n**Pairwise J1 (verdict-based)**: Outputs reasoning ttt and a final preference verdict\n\n**Pairwise J1 (score-based)**: Outputs reasoning plus real-valued scores sa,sbsa,sbs\\_a, s\\_b\n\n**Pointwise J1**: Outputs a scalar quality score for a single response\n\n**Multitask J1**: Jointly trained to perform both pointwise and pairwise judging\n\nA key contribution is showing that **pointwise judges can be trained purely from pairwise supervision**, yielding inherently position-consistent evaluators.",
    "contentLength": 13958,
    "wordCount": 197,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#judge-formulations-trained-with-rl"
  },
  {
    "id": "ai-LLM-as-a-judge-learned-reasoning-behaviors-47",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Reinforcement Learning for LLMs-as-Judges",
    "title": "Learned Reasoning Behaviors",
    "order": 47,
    "orderInChapter": 7,
    "contentHtml": "<ul>\n  <li>\n    <p>Qualitative analysis in the J1 paper shows that RL-trained judges learn systematic evaluation strategies:</p>\n\n    <ul>\n      <li>Dynamic generation of evaluation criteria</li>\n      <li>Self-generated reference answers</li>\n      <li>Iterative self-correction</li>\n      <li>Explicit error localization and feedback</li>\n    </ul>\n  </li>\n  <li>\n    <p>The following figure (<a href=\"https://arxiv.org/abs/2505.10320\">source</a>) illustrates of the thinking patterns of pairwise and pointwise J1 models during RL training.J1 learns to outline evaluation criteria, generate reference answers, re-evaluate correctness, and compare between responses. Pairwise setup outputs a final verdict indicating the better response, pointwise generates a real-valued score, with a higher score for the better response. Note that these behaviors emerge <strong>without human-written rationales</strong>, purely from RL optimization on synthetic data.</p>\n  </li>\n</ul>\n<p>Qualitative analysis in the J1 paper shows that RL-trained judges learn systematic evaluation strategies:</p>\n<ul>\n      <li>Dynamic generation of evaluation criteria</li>\n      <li>Self-generated reference answers</li>\n      <li>Iterative self-correction</li>\n      <li>Explicit error localization and feedback</li>\n    </ul>\n<p>The following figure (<a href=\"https://arxiv.org/abs/2505.10320\">source</a>) illustrates of the thinking patterns of pairwise and pointwise J1 models during RL training.J1 learns to outline evaluation criteria, generate reference answers, re-evaluate correctness, and compare between responses. Pairwise setup outputs a final verdict indicating the better response, pointwise generates a real-valued score, with a higher score for the better response. Note that these behaviors emerge <strong>without human-written rationales</strong>, purely from RL optimization on synthetic data.</p>\n<p><img src=\"/primers/ai/assets/LLM-as-a-judge/J1.jpg\" alt=\"\"></p>",
    "contentMarkdown": "*   Qualitative analysis in the J1 paper shows that RL-trained judges learn systematic evaluation strategies:\n    \n    *   Dynamic generation of evaluation criteria\n    *   Self-generated reference answers\n    *   Iterative self-correction\n    *   Explicit error localization and feedback\n*   The following figure ([source](https://arxiv.org/abs/2505.10320)) illustrates of the thinking patterns of pairwise and pointwise J1 models during RL training.J1 learns to outline evaluation criteria, generate reference answers, re-evaluate correctness, and compare between responses. Pairwise setup outputs a final verdict indicating the better response, pointwise generates a real-valued score, with a higher score for the better response. Note that these behaviors emerge **without human-written rationales**, purely from RL optimization on synthetic data.\n    \n\nQualitative analysis in the J1 paper shows that RL-trained judges learn systematic evaluation strategies:\n\n*   Dynamic generation of evaluation criteria\n*   Self-generated reference answers\n*   Iterative self-correction\n*   Explicit error localization and feedback\n\nThe following figure ([source](https://arxiv.org/abs/2505.10320)) illustrates of the thinking patterns of pairwise and pointwise J1 models during RL training.J1 learns to outline evaluation criteria, generate reference answers, re-evaluate correctness, and compare between responses. Pairwise setup outputs a final verdict indicating the better response, pointwise generates a real-valued score, with a higher score for the better response. Note that these behaviors emerge **without human-written rationales**, purely from RL optimization on synthetic data.\n\n![](/primers/ai/assets/LLM-as-a-judge/J1.jpg)",
    "contentLength": 1960,
    "wordCount": 209,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#learned-reasoning-behaviors"
  },
  {
    "id": "ai-LLM-as-a-judge-empirical-performance-48",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Reinforcement Learning for LLMs-as-Judges",
    "title": "Empirical Performance",
    "order": 48,
    "orderInChapter": 8,
    "contentHtml": "<ul>\n  <li>\n    <p>J1 achieves state-of-the-art results across major judge and reward-model benchmarks, including:</p>\n\n    <ul>\n      <li>\n        <p><strong>PPE (Preference Proxy Evaluations)</strong>:</p>\n\n        <ul>\n          <li>Proposed in <a href=\"https://github.com/lmarena/PPE\">PPE: Preference Proxy Evaluations</a> by Chiang et al. (2024).</li>\n          <li>A benchmark for evaluating reward models and LLM judges using proxy preference signals that correlate with human judgments and downstream RLHF performance. PPE is commonly used to assess preference modeling and judge reliability at scale.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>RewardBench</strong>:</p>\n\n        <ul>\n          <li>Proposed in <a href=\"https://arxiv.org/abs/2403.08555\">RewardBench: Evaluating Reward Models for Language Modeling</a> by Lambert et al. (2024).</li>\n          <li>A comprehensive benchmark designed to evaluate reward models and LLM-as-a-Judge systems across a wide range of tasks, including instruction following, safety, reasoning, and preference alignment. It measures how well reward signals reflect human preferences.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>JudgeBench</strong>:</p>\n\n        <ul>\n          <li>Proposed in <a href=\"https://arxiv.org/abs/2410.12784\">JudgeBench: A Benchmark for Evaluating LLM-based Judges</a> by Liu et al. (2024).</li>\n          <li>A benchmark specifically designed to evaluate LLM-based judges, focusing on difficult pairwise and pointwise judgment scenarios where traditional automatic metrics fail. It includes tasks across reasoning, knowledge, math, and coding.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>RM-Bench</strong>:</p>\n\n        <ul>\n          <li>Proposed in <a href=\"https://arxiv.org/abs/2408.02666\">RM-Bench</a> by Wang et al. (2024).</li>\n          <li>A reward-model benchmarking suite used to evaluate both scalar and generative reward models, as well as LLM judges, across preference learning and correctness-based evaluation tasks. RM-Bench is often reported alongside PPE and RewardBench.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>FollowBenchEval</strong>:</p>\n\n        <ul>\n          <li>Proposed in <a href=\"https://arxiv.org/abs/2505.10320\">FollowBenchEval</a> by Whitehouse et al. (2025).</li>\n          <li>An evaluation suite that measures how well reward models and LLM judges assess instruction-following quality, including consistency, preference accuracy, and positional robustness in pairwise comparisons.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>Notably:</p>\n\n    <ul>\n      <li><strong>J1-Qwen-32B</strong> outperforms much larger models such as DeepSeek-R1-671B</li>\n      <li>J1 surpasses both scalar reward models and generative reward models trained on far more data</li>\n      <li>Test-time scaling (self-consistency, score averaging) further improves accuracy</li>\n    </ul>\n  </li>\n  <li>\n    <p>These results demonstrate that <strong>online RL is a powerful mechanism for training generalist, high-quality LLM judges</strong>.</p>\n  </li>\n</ul>\n<p>J1 achieves state-of-the-art results across major judge and reward-model benchmarks, including:</p>\n<ul>\n      <li>\n        <p><strong>PPE (Preference Proxy Evaluations)</strong>:</p>\n\n        <ul>\n          <li>Proposed in <a href=\"https://github.com/lmarena/PPE\">PPE: Preference Proxy Evaluations</a> by Chiang et al. (2024).</li>\n          <li>A benchmark for evaluating reward models and LLM judges using proxy preference signals that correlate with human judgments and downstream RLHF performance. PPE is commonly used to assess preference modeling and judge reliability at scale.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>RewardBench</strong>:</p>\n\n        <ul>\n          <li>Proposed in <a href=\"https://arxiv.org/abs/2403.08555\">RewardBench: Evaluating Reward Models for Language Modeling</a> by Lambert et al. (2024).</li>\n          <li>A comprehensive benchmark designed to evaluate reward models and LLM-as-a-Judge systems across a wide range of tasks, including instruction following, safety, reasoning, and preference alignment. It measures how well reward signals reflect human preferences.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>JudgeBench</strong>:</p>\n\n        <ul>\n          <li>Proposed in <a href=\"https://arxiv.org/abs/2410.12784\">JudgeBench: A Benchmark for Evaluating LLM-based Judges</a> by Liu et al. (2024).</li>\n          <li>A benchmark specifically designed to evaluate LLM-based judges, focusing on difficult pairwise and pointwise judgment scenarios where traditional automatic metrics fail. It includes tasks across reasoning, knowledge, math, and coding.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>RM-Bench</strong>:</p>\n\n        <ul>\n          <li>Proposed in <a href=\"https://arxiv.org/abs/2408.02666\">RM-Bench</a> by Wang et al. (2024).</li>\n          <li>A reward-model benchmarking suite used to evaluate both scalar and generative reward models, as well as LLM judges, across preference learning and correctness-based evaluation tasks. RM-Bench is often reported alongside PPE and RewardBench.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>FollowBenchEval</strong>:</p>\n\n        <ul>\n          <li>Proposed in <a href=\"https://arxiv.org/abs/2505.10320\">FollowBenchEval</a> by Whitehouse et al. (2025).</li>\n          <li>An evaluation suite that measures how well reward models and LLM judges assess instruction-following quality, including consistency, preference accuracy, and positional robustness in pairwise comparisons.</li>\n        </ul>\n      </li>\n    </ul>\n<p><strong>PPE (Preference Proxy Evaluations)</strong>:</p>\n<ul>\n          <li>Proposed in <a href=\"https://github.com/lmarena/PPE\">PPE: Preference Proxy Evaluations</a> by Chiang et al. (2024).</li>\n          <li>A benchmark for evaluating reward models and LLM judges using proxy preference signals that correlate with human judgments and downstream RLHF performance. PPE is commonly used to assess preference modeling and judge reliability at scale.</li>\n        </ul>\n<p><strong>RewardBench</strong>:</p>\n<ul>\n          <li>Proposed in <a href=\"https://arxiv.org/abs/2403.08555\">RewardBench: Evaluating Reward Models for Language Modeling</a> by Lambert et al. (2024).</li>\n          <li>A comprehensive benchmark designed to evaluate reward models and LLM-as-a-Judge systems across a wide range of tasks, including instruction following, safety, reasoning, and preference alignment. It measures how well reward signals reflect human preferences.</li>\n        </ul>\n<p><strong>JudgeBench</strong>:</p>\n<ul>\n          <li>Proposed in <a href=\"https://arxiv.org/abs/2410.12784\">JudgeBench: A Benchmark for Evaluating LLM-based Judges</a> by Liu et al. (2024).</li>\n          <li>A benchmark specifically designed to evaluate LLM-based judges, focusing on difficult pairwise and pointwise judgment scenarios where traditional automatic metrics fail. It includes tasks across reasoning, knowledge, math, and coding.</li>\n        </ul>\n<p><strong>RM-Bench</strong>:</p>\n<ul>\n          <li>Proposed in <a href=\"https://arxiv.org/abs/2408.02666\">RM-Bench</a> by Wang et al. (2024).</li>\n          <li>A reward-model benchmarking suite used to evaluate both scalar and generative reward models, as well as LLM judges, across preference learning and correctness-based evaluation tasks. RM-Bench is often reported alongside PPE and RewardBench.</li>\n        </ul>\n<p><strong>FollowBenchEval</strong>:</p>\n<ul>\n          <li>Proposed in <a href=\"https://arxiv.org/abs/2505.10320\">FollowBenchEval</a> by Whitehouse et al. (2025).</li>\n          <li>An evaluation suite that measures how well reward models and LLM judges assess instruction-following quality, including consistency, preference accuracy, and positional robustness in pairwise comparisons.</li>\n        </ul>\n<p>Notably:</p>\n<ul>\n      <li><strong>J1-Qwen-32B</strong> outperforms much larger models such as DeepSeek-R1-671B</li>\n      <li>J1 surpasses both scalar reward models and generative reward models trained on far more data</li>\n      <li>Test-time scaling (self-consistency, score averaging) further improves accuracy</li>\n    </ul>\n<p>These results demonstrate that <strong>online RL is a powerful mechanism for training generalist, high-quality LLM judges</strong>.</p>\n<h4 id=\"rewardbench-v2\">RewardBench V2</h4>\n<ul>\n  <li>\n    <p>While <strong>J1</strong> reports strong performance across PPE, RewardBench v1, JudgeBench, RM-Bench, and FollowBenchEval, it <strong>does not evaluate on RewardBench v2</strong>, which represents a substantial evolution in reward model and LLM-judge evaluation methodology.</p>\n  </li>\n  <li>\n    <p><strong>RewardBench v2</strong> was proposed in <a href=\"https://arxiv.org/abs/2506.01937\">REWARDBENCH 2: Advancing Reward Model Evaluation</a> by Malik et al. (2025).</p>\n  </li>\n</ul>\n<p>While <strong>J1</strong> reports strong performance across PPE, RewardBench v1, JudgeBench, RM-Bench, and FollowBenchEval, it <strong>does not evaluate on RewardBench v2</strong>, which represents a substantial evolution in reward model and LLM-judge evaluation methodology.</p>\n<p><strong>RewardBench v2</strong> was proposed in <a href=\"https://arxiv.org/abs/2506.01937\">REWARDBENCH 2: Advancing Reward Model Evaluation</a> by Malik et al. (2025).</p>\n<h5 id=\"key-differences-between-rewardbench-v1-and-rewardbench-v2\">Key Differences Between RewardBench V1 and RewardBench V2</h5>\n<ul>\n  <li>\n    <p><strong>Unseen human prompts</strong>:</p>\n\n    <ul>\n      <li>Unlike RewardBench v1, which reused prompts from downstream evaluations, RewardBench v2 is built primarily on <strong>new, previously unseen human prompts</strong>, largely sourced from WildChat.</li>\n      <li>This design significantly reduces contamination and benchmark leakage, making correlations with downstream performance more meaningful.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Best-of-N (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-130-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mo>&amp;gt;</mo><mn>2</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1543\" style=\"width: 2.998em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.482em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1002.43em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1544\"><span class=\"mi\" id=\"MathJax-Span-1545\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1546\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.313em;\">&gt;</span><span class=\"mn\" id=\"MathJax-Span-1547\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.313em;\">2</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi><mo>&gt;</mo><mn>2</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-130\">N > 2</script>) evaluation format</strong>:</p>\n\n    <ul>\n      <li>RewardBench v2 moves from a binary chosen–rejected setup to a <strong>1-chosen vs. 3-rejected</strong> format.</li>\n      <li>This lowers the random baseline from 50% to 25%, increasing headroom and making strong reward models easier to distinguish.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Expanded multi-skill coverage</strong>:</p>\n\n    <ul>\n      <li>\n        <p>RewardBench v2 evaluates six domains:</p>\n\n        <ul>\n          <li>Factuality</li>\n          <li>Precise instruction following</li>\n          <li>Math</li>\n          <li>Safety</li>\n          <li>Focus (on-topic quality)</li>\n          <li>Ties (new domain testing calibration among multiple equally correct answers)</li>\n        </ul>\n      </li>\n      <li>\n        <p>The <strong>Ties</strong> domain is entirely new and explicitly tests whether reward models avoid arbitrary over-preference among equally valid answers.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Accuracy-based evaluation with downstream grounding</strong>:</p>\n\n    <ul>\n      <li>\n        <p>Unlike preference-only benchmarks, RewardBench v2 emphasizes <strong>accuracy-based scoring</strong>, while still demonstrating strong correlation with:</p>\n\n        <ul>\n          <li>Best-of-N inference-time scaling</li>\n          <li>PPO-based RLHF training</li>\n        </ul>\n      </li>\n      <li>\n        <p>Models score on average <strong>20+ points lower</strong> than on RewardBench v1, indicating substantially increased difficulty.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Stronger empirical linkage to downstream performance</strong>:</p>\n\n    <ul>\n      <li>\n        <p>RewardBench v2 shows:</p>\n\n        <ul>\n          <li>Pearson correlation ≈ 0.87 with Best-of-N downstream task performance</li>\n          <li>Meaningful but saturating correlation with PPO-based RLHF, highlighting that benchmark accuracy is necessary but not sufficient for RL success</li>\n        </ul>\n      </li>\n      <li>\n        <p>Crucially, the benchmark reveals <strong>lineage mismatch effects</strong>, where high-scoring reward models can still perform poorly in RLHF if misaligned with the policy model’s base distribution.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>The following figure (<a href=\"https://arxiv.org/abs/2506.01937\">source</a>) shows RewardBench 2’s benchmark construction and evaluation setup, highlighting its unseen human prompts, best-of-4 format, expanded domains (including Ties), and improved correlation with downstream RL fine-tuning and best-of-N sampling performance.</p>\n  </li>\n</ul>\n<p><strong>Unseen human prompts</strong>:</p>\n<ul>\n      <li>Unlike RewardBench v1, which reused prompts from downstream evaluations, RewardBench v2 is built primarily on <strong>new, previously unseen human prompts</strong>, largely sourced from WildChat.</li>\n      <li>This design significantly reduces contamination and benchmark leakage, making correlations with downstream performance more meaningful.</li>\n    </ul>\n<p><strong>Best-of-N (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-130-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mo>&amp;gt;</mo><mn>2</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1543\" style=\"width: 2.998em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.482em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1002.43em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1544\"><span class=\"mi\" id=\"MathJax-Span-1545\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1546\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.313em;\">&gt;</span><span class=\"mn\" id=\"MathJax-Span-1547\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.313em;\">2</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi><mo>&gt;</mo><mn>2</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-130\">N > 2</script>) evaluation format</strong>:</p>\n<ul>\n      <li>RewardBench v2 moves from a binary chosen–rejected setup to a <strong>1-chosen vs. 3-rejected</strong> format.</li>\n      <li>This lowers the random baseline from 50% to 25%, increasing headroom and making strong reward models easier to distinguish.</li>\n    </ul>\n<p><strong>Expanded multi-skill coverage</strong>:</p>\n<ul>\n      <li>\n        <p>RewardBench v2 evaluates six domains:</p>\n\n        <ul>\n          <li>Factuality</li>\n          <li>Precise instruction following</li>\n          <li>Math</li>\n          <li>Safety</li>\n          <li>Focus (on-topic quality)</li>\n          <li>Ties (new domain testing calibration among multiple equally correct answers)</li>\n        </ul>\n      </li>\n      <li>\n        <p>The <strong>Ties</strong> domain is entirely new and explicitly tests whether reward models avoid arbitrary over-preference among equally valid answers.</p>\n      </li>\n    </ul>\n<p>RewardBench v2 evaluates six domains:</p>\n<ul>\n          <li>Factuality</li>\n          <li>Precise instruction following</li>\n          <li>Math</li>\n          <li>Safety</li>\n          <li>Focus (on-topic quality)</li>\n          <li>Ties (new domain testing calibration among multiple equally correct answers)</li>\n        </ul>\n<p>The <strong>Ties</strong> domain is entirely new and explicitly tests whether reward models avoid arbitrary over-preference among equally valid answers.</p>\n<p><strong>Accuracy-based evaluation with downstream grounding</strong>:</p>\n<ul>\n      <li>\n        <p>Unlike preference-only benchmarks, RewardBench v2 emphasizes <strong>accuracy-based scoring</strong>, while still demonstrating strong correlation with:</p>\n\n        <ul>\n          <li>Best-of-N inference-time scaling</li>\n          <li>PPO-based RLHF training</li>\n        </ul>\n      </li>\n      <li>\n        <p>Models score on average <strong>20+ points lower</strong> than on RewardBench v1, indicating substantially increased difficulty.</p>\n      </li>\n    </ul>\n<p>Unlike preference-only benchmarks, RewardBench v2 emphasizes <strong>accuracy-based scoring</strong>, while still demonstrating strong correlation with:</p>\n<ul>\n          <li>Best-of-N inference-time scaling</li>\n          <li>PPO-based RLHF training</li>\n        </ul>\n<p>Models score on average <strong>20+ points lower</strong> than on RewardBench v1, indicating substantially increased difficulty.</p>\n<p><strong>Stronger empirical linkage to downstream performance</strong>:</p>\n<ul>\n      <li>\n        <p>RewardBench v2 shows:</p>\n\n        <ul>\n          <li>Pearson correlation ≈ 0.87 with Best-of-N downstream task performance</li>\n          <li>Meaningful but saturating correlation with PPO-based RLHF, highlighting that benchmark accuracy is necessary but not sufficient for RL success</li>\n        </ul>\n      </li>\n      <li>\n        <p>Crucially, the benchmark reveals <strong>lineage mismatch effects</strong>, where high-scoring reward models can still perform poorly in RLHF if misaligned with the policy model’s base distribution.</p>\n      </li>\n    </ul>\n<p>RewardBench v2 shows:</p>\n<ul>\n          <li>Pearson correlation ≈ 0.87 with Best-of-N downstream task performance</li>\n          <li>Meaningful but saturating correlation with PPO-based RLHF, highlighting that benchmark accuracy is necessary but not sufficient for RL success</li>\n        </ul>\n<p>Crucially, the benchmark reveals <strong>lineage mismatch effects</strong>, where high-scoring reward models can still perform poorly in RLHF if misaligned with the policy model’s base distribution.</p>\n<p>The following figure (<a href=\"https://arxiv.org/abs/2506.01937\">source</a>) shows RewardBench 2’s benchmark construction and evaluation setup, highlighting its unseen human prompts, best-of-4 format, expanded domains (including Ties), and improved correlation with downstream RL fine-tuning and best-of-N sampling performance.</p>\n<p><img src=\"/primers/ai/assets/LLM-as-a-judge/RewardBenchv2.jpg\" alt=\"\"></p>\n<h5 id=\"implications-for-j1-and-llm-as-a-judge-systems\">Implications for J1 and LLM-as-a-Judge Systems</h5>\n<ul>\n  <li>\n    <p>Although J1’s absence from RewardBench v2 does not invalidate its reported strengths, RewardBench v2 introduces <strong>failure modes and calibration challenges</strong> not covered by earlier benchmarks.</p>\n  </li>\n  <li>\n    <p>In particular:</p>\n\n    <ul>\n      <li>The <strong>Ties</strong> domain stresses judge calibration rather than raw preference discrimination.</li>\n      <li>The Best-of-4 setup penalizes overly confident or brittle reward signals.</li>\n    </ul>\n  </li>\n  <li>\n    <p>As RewardBench v2 gains adoption, it is likely to become a <strong>standard reference benchmark</strong> for next-generation LLM judges and reward models, complementing (and in some cases superseding) RewardBench v1.</p>\n  </li>\n  <li>\n    <p>Practically, future evaluations of J1-like systems would benefit from reporting RewardBench v2 results to demonstrate robustness under <strong>harder, more realistic, and less-contaminated evaluation conditions</strong>.</p>\n  </li>\n</ul>\n<p>Although J1’s absence from RewardBench v2 does not invalidate its reported strengths, RewardBench v2 introduces <strong>failure modes and calibration challenges</strong> not covered by earlier benchmarks.</p>\n<p>In particular:</p>\n<ul>\n      <li>The <strong>Ties</strong> domain stresses judge calibration rather than raw preference discrimination.</li>\n      <li>The Best-of-4 setup penalizes overly confident or brittle reward signals.</li>\n    </ul>\n<p>As RewardBench v2 gains adoption, it is likely to become a <strong>standard reference benchmark</strong> for next-generation LLM judges and reward models, complementing (and in some cases superseding) RewardBench v1.</p>\n<p>Practically, future evaluations of J1-like systems would benefit from reporting RewardBench v2 results to demonstrate robustness under <strong>harder, more realistic, and less-contaminated evaluation conditions</strong>.</p>",
    "contentMarkdown": "*   J1 achieves state-of-the-art results across major judge and reward-model benchmarks, including:\n    \n    *   **PPE (Preference Proxy Evaluations)**:\n        \n        *   Proposed in [PPE: Preference Proxy Evaluations](https://github.com/lmarena/PPE) by Chiang et al. (2024).\n        *   A benchmark for evaluating reward models and LLM judges using proxy preference signals that correlate with human judgments and downstream RLHF performance. PPE is commonly used to assess preference modeling and judge reliability at scale.\n    *   **RewardBench**:\n        \n        *   Proposed in [RewardBench: Evaluating Reward Models for Language Modeling](https://arxiv.org/abs/2403.08555) by Lambert et al. (2024).\n        *   A comprehensive benchmark designed to evaluate reward models and LLM-as-a-Judge systems across a wide range of tasks, including instruction following, safety, reasoning, and preference alignment. It measures how well reward signals reflect human preferences.\n    *   **JudgeBench**:\n        \n        *   Proposed in [JudgeBench: A Benchmark for Evaluating LLM-based Judges](https://arxiv.org/abs/2410.12784) by Liu et al. (2024).\n        *   A benchmark specifically designed to evaluate LLM-based judges, focusing on difficult pairwise and pointwise judgment scenarios where traditional automatic metrics fail. It includes tasks across reasoning, knowledge, math, and coding.\n    *   **RM-Bench**:\n        \n        *   Proposed in [RM-Bench](https://arxiv.org/abs/2408.02666) by Wang et al. (2024).\n        *   A reward-model benchmarking suite used to evaluate both scalar and generative reward models, as well as LLM judges, across preference learning and correctness-based evaluation tasks. RM-Bench is often reported alongside PPE and RewardBench.\n    *   **FollowBenchEval**:\n        \n        *   Proposed in [FollowBenchEval](https://arxiv.org/abs/2505.10320) by Whitehouse et al. (2025).\n        *   An evaluation suite that measures how well reward models and LLM judges assess instruction-following quality, including consistency, preference accuracy, and positional robustness in pairwise comparisons.\n*   Notably:\n    \n    *   **J1-Qwen-32B** outperforms much larger models such as DeepSeek-R1-671B\n    *   J1 surpasses both scalar reward models and generative reward models trained on far more data\n    *   Test-time scaling (self-consistency, score averaging) further improves accuracy\n*   These results demonstrate that **online RL is a powerful mechanism for training generalist, high-quality LLM judges**.\n    \n\nJ1 achieves state-of-the-art results across major judge and reward-model benchmarks, including:\n\n*   **PPE (Preference Proxy Evaluations)**:\n    \n    *   Proposed in [PPE: Preference Proxy Evaluations](https://github.com/lmarena/PPE) by Chiang et al. (2024).\n    *   A benchmark for evaluating reward models and LLM judges using proxy preference signals that correlate with human judgments and downstream RLHF performance. PPE is commonly used to assess preference modeling and judge reliability at scale.\n*   **RewardBench**:\n    \n    *   Proposed in [RewardBench: Evaluating Reward Models for Language Modeling](https://arxiv.org/abs/2403.08555) by Lambert et al. (2024).\n    *   A comprehensive benchmark designed to evaluate reward models and LLM-as-a-Judge systems across a wide range of tasks, including instruction following, safety, reasoning, and preference alignment. It measures how well reward signals reflect human preferences.\n*   **JudgeBench**:\n    \n    *   Proposed in [JudgeBench: A Benchmark for Evaluating LLM-based Judges](https://arxiv.org/abs/2410.12784) by Liu et al. (2024).\n    *   A benchmark specifically designed to evaluate LLM-based judges, focusing on difficult pairwise and pointwise judgment scenarios where traditional automatic metrics fail. It includes tasks across reasoning, knowledge, math, and coding.\n*   **RM-Bench**:\n    \n    *   Proposed in [RM-Bench](https://arxiv.org/abs/2408.02666) by Wang et al. (2024).\n    *   A reward-model benchmarking suite used to evaluate both scalar and generative reward models, as well as LLM judges, across preference learning and correctness-based evaluation tasks. RM-Bench is often reported alongside PPE and RewardBench.\n*   **FollowBenchEval**:\n    \n    *   Proposed in [FollowBenchEval](https://arxiv.org/abs/2505.10320) by Whitehouse et al. (2025).\n    *   An evaluation suite that measures how well reward models and LLM judges assess instruction-following quality, including consistency, preference accuracy, and positional robustness in pairwise comparisons.\n\n**PPE (Preference Proxy Evaluations)**:\n\n*   Proposed in [PPE: Preference Proxy Evaluations](https://github.com/lmarena/PPE) by Chiang et al. (2024).\n*   A benchmark for evaluating reward models and LLM judges using proxy preference signals that correlate with human judgments and downstream RLHF performance. PPE is commonly used to assess preference modeling and judge reliability at scale.\n\n**RewardBench**:\n\n*   Proposed in [RewardBench: Evaluating Reward Models for Language Modeling](https://arxiv.org/abs/2403.08555) by Lambert et al. (2024).\n*   A comprehensive benchmark designed to evaluate reward models and LLM-as-a-Judge systems across a wide range of tasks, including instruction following, safety, reasoning, and preference alignment. It measures how well reward signals reflect human preferences.\n\n**JudgeBench**:\n\n*   Proposed in [JudgeBench: A Benchmark for Evaluating LLM-based Judges](https://arxiv.org/abs/2410.12784) by Liu et al. (2024).\n*   A benchmark specifically designed to evaluate LLM-based judges, focusing on difficult pairwise and pointwise judgment scenarios where traditional automatic metrics fail. It includes tasks across reasoning, knowledge, math, and coding.\n\n**RM-Bench**:\n\n*   Proposed in [RM-Bench](https://arxiv.org/abs/2408.02666) by Wang et al. (2024).\n*   A reward-model benchmarking suite used to evaluate both scalar and generative reward models, as well as LLM judges, across preference learning and correctness-based evaluation tasks. RM-Bench is often reported alongside PPE and RewardBench.\n\n**FollowBenchEval**:\n\n*   Proposed in [FollowBenchEval](https://arxiv.org/abs/2505.10320) by Whitehouse et al. (2025).\n*   An evaluation suite that measures how well reward models and LLM judges assess instruction-following quality, including consistency, preference accuracy, and positional robustness in pairwise comparisons.\n\nNotably:\n\n*   **J1-Qwen-32B** outperforms much larger models such as DeepSeek-R1-671B\n*   J1 surpasses both scalar reward models and generative reward models trained on far more data\n*   Test-time scaling (self-consistency, score averaging) further improves accuracy\n\nThese results demonstrate that **online RL is a powerful mechanism for training generalist, high-quality LLM judges**.\n\n#### RewardBench V2\n\n*   While **J1** reports strong performance across PPE, RewardBench v1, JudgeBench, RM-Bench, and FollowBenchEval, it **does not evaluate on RewardBench v2**, which represents a substantial evolution in reward model and LLM-judge evaluation methodology.\n    \n*   **RewardBench v2** was proposed in [REWARDBENCH 2: Advancing Reward Model Evaluation](https://arxiv.org/abs/2506.01937) by Malik et al. (2025).\n    \n\nWhile **J1** reports strong performance across PPE, RewardBench v1, JudgeBench, RM-Bench, and FollowBenchEval, it **does not evaluate on RewardBench v2**, which represents a substantial evolution in reward model and LLM-judge evaluation methodology.\n\n**RewardBench v2** was proposed in [REWARDBENCH 2: Advancing Reward Model Evaluation](https://arxiv.org/abs/2506.01937) by Malik et al. (2025).\n\n##### Key Differences Between RewardBench V1 and RewardBench V2\n\n*   **Unseen human prompts**:\n    \n    *   Unlike RewardBench v1, which reused prompts from downstream evaluations, RewardBench v2 is built primarily on **new, previously unseen human prompts**, largely sourced from WildChat.\n    *   This design significantly reduces contamination and benchmark leakage, making correlations with downstream performance more meaningful.\n*   **Best-of-N (N\\>2N\\>2N > 2) evaluation format**:\n    \n    *   RewardBench v2 moves from a binary chosen–rejected setup to a **1-chosen vs. 3-rejected** format.\n    *   This lowers the random baseline from 50% to 25%, increasing headroom and making strong reward models easier to distinguish.\n*   **Expanded multi-skill coverage**:\n    \n    *   RewardBench v2 evaluates six domains:\n        \n        *   Factuality\n        *   Precise instruction following\n        *   Math\n        *   Safety\n        *   Focus (on-topic quality)\n        *   Ties (new domain testing calibration among multiple equally correct answers)\n    *   The **Ties** domain is entirely new and explicitly tests whether reward models avoid arbitrary over-preference among equally valid answers.\n        \n*   **Accuracy-based evaluation with downstream grounding**:\n    \n    *   Unlike preference-only benchmarks, RewardBench v2 emphasizes **accuracy-based scoring**, while still demonstrating strong correlation with:\n        \n        *   Best-of-N inference-time scaling\n        *   PPO-based RLHF training\n    *   Models score on average **20+ points lower** than on RewardBench v1, indicating substantially increased difficulty.\n        \n*   **Stronger empirical linkage to downstream performance**:\n    \n    *   RewardBench v2 shows:\n        \n        *   Pearson correlation ≈ 0.87 with Best-of-N downstream task performance\n        *   Meaningful but saturating correlation with PPO-based RLHF, highlighting that benchmark accuracy is necessary but not sufficient for RL success\n    *   Crucially, the benchmark reveals **lineage mismatch effects**, where high-scoring reward models can still perform poorly in RLHF if misaligned with the policy model’s base distribution.\n        \n*   The following figure ([source](https://arxiv.org/abs/2506.01937)) shows RewardBench 2’s benchmark construction and evaluation setup, highlighting its unseen human prompts, best-of-4 format, expanded domains (including Ties), and improved correlation with downstream RL fine-tuning and best-of-N sampling performance.\n    \n\n**Unseen human prompts**:\n\n*   Unlike RewardBench v1, which reused prompts from downstream evaluations, RewardBench v2 is built primarily on **new, previously unseen human prompts**, largely sourced from WildChat.\n*   This design significantly reduces contamination and benchmark leakage, making correlations with downstream performance more meaningful.\n\n**Best-of-N (N\\>2N\\>2N > 2) evaluation format**:\n\n*   RewardBench v2 moves from a binary chosen–rejected setup to a **1-chosen vs. 3-rejected** format.\n*   This lowers the random baseline from 50% to 25%, increasing headroom and making strong reward models easier to distinguish.\n\n**Expanded multi-skill coverage**:\n\n*   RewardBench v2 evaluates six domains:\n    \n    *   Factuality\n    *   Precise instruction following\n    *   Math\n    *   Safety\n    *   Focus (on-topic quality)\n    *   Ties (new domain testing calibration among multiple equally correct answers)\n*   The **Ties** domain is entirely new and explicitly tests whether reward models avoid arbitrary over-preference among equally valid answers.\n    \n\nRewardBench v2 evaluates six domains:\n\n*   Factuality\n*   Precise instruction following\n*   Math\n*   Safety\n*   Focus (on-topic quality)\n*   Ties (new domain testing calibration among multiple equally correct answers)\n\nThe **Ties** domain is entirely new and explicitly tests whether reward models avoid arbitrary over-preference among equally valid answers.\n\n**Accuracy-based evaluation with downstream grounding**:\n\n*   Unlike preference-only benchmarks, RewardBench v2 emphasizes **accuracy-based scoring**, while still demonstrating strong correlation with:\n    \n    *   Best-of-N inference-time scaling\n    *   PPO-based RLHF training\n*   Models score on average **20+ points lower** than on RewardBench v1, indicating substantially increased difficulty.\n    \n\nUnlike preference-only benchmarks, RewardBench v2 emphasizes **accuracy-based scoring**, while still demonstrating strong correlation with:\n\n*   Best-of-N inference-time scaling\n*   PPO-based RLHF training\n\nModels score on average **20+ points lower** than on RewardBench v1, indicating substantially increased difficulty.\n\n**Stronger empirical linkage to downstream performance**:\n\n*   RewardBench v2 shows:\n    \n    *   Pearson correlation ≈ 0.87 with Best-of-N downstream task performance\n    *   Meaningful but saturating correlation with PPO-based RLHF, highlighting that benchmark accuracy is necessary but not sufficient for RL success\n*   Crucially, the benchmark reveals **lineage mismatch effects**, where high-scoring reward models can still perform poorly in RLHF if misaligned with the policy model’s base distribution.\n    \n\nRewardBench v2 shows:\n\n*   Pearson correlation ≈ 0.87 with Best-of-N downstream task performance\n*   Meaningful but saturating correlation with PPO-based RLHF, highlighting that benchmark accuracy is necessary but not sufficient for RL success\n\nCrucially, the benchmark reveals **lineage mismatch effects**, where high-scoring reward models can still perform poorly in RLHF if misaligned with the policy model’s base distribution.\n\nThe following figure ([source](https://arxiv.org/abs/2506.01937)) shows RewardBench 2’s benchmark construction and evaluation setup, highlighting its unseen human prompts, best-of-4 format, expanded domains (including Ties), and improved correlation with downstream RL fine-tuning and best-of-N sampling performance.\n\n![](/primers/ai/assets/LLM-as-a-judge/RewardBenchv2.jpg)\n\n##### Implications for J1 and LLM-as-a-Judge Systems\n\n*   Although J1’s absence from RewardBench v2 does not invalidate its reported strengths, RewardBench v2 introduces **failure modes and calibration challenges** not covered by earlier benchmarks.\n    \n*   In particular:\n    \n    *   The **Ties** domain stresses judge calibration rather than raw preference discrimination.\n    *   The Best-of-4 setup penalizes overly confident or brittle reward signals.\n*   As RewardBench v2 gains adoption, it is likely to become a **standard reference benchmark** for next-generation LLM judges and reward models, complementing (and in some cases superseding) RewardBench v1.\n    \n*   Practically, future evaluations of J1-like systems would benefit from reporting RewardBench v2 results to demonstrate robustness under **harder, more realistic, and less-contaminated evaluation conditions**.\n    \n\nAlthough J1’s absence from RewardBench v2 does not invalidate its reported strengths, RewardBench v2 introduces **failure modes and calibration challenges** not covered by earlier benchmarks.\n\nIn particular:\n\n*   The **Ties** domain stresses judge calibration rather than raw preference discrimination.\n*   The Best-of-4 setup penalizes overly confident or brittle reward signals.\n\nAs RewardBench v2 gains adoption, it is likely to become a **standard reference benchmark** for next-generation LLM judges and reward models, complementing (and in some cases superseding) RewardBench v1.\n\nPractically, future evaluations of J1-like systems would benefit from reporting RewardBench v2 results to demonstrate robustness under **harder, more realistic, and less-contaminated evaluation conditions**.",
    "contentLength": 22303,
    "wordCount": 1871,
    "hasCode": false,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#empirical-performance"
  },
  {
    "id": "ai-LLM-as-a-judge-relationship-to-panels-and-multimodal-judges-49",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Reinforcement Learning for LLMs-as-Judges",
    "title": "Relationship to Panels and Multimodal Judges",
    "order": 49,
    "orderInChapter": 9,
    "contentHtml": "<ul>\n  <li>\n    <p>RL is complementary to other robustness strategies:</p>\n\n    <ul>\n      <li><strong>Panels of LLMs-as-Judges</strong>: RL improves each judge’s internal reasoning; panels reduce variance across judges</li>\n      <li><strong>Pointwise vs. pairwise paradigms</strong>: J1 unifies both within a single multitask model</li>\n      <li><strong>Test-time scaling</strong>: RL-trained judges benefit strongly from majority voting and averaging</li>\n    </ul>\n  </li>\n  <li>\n    <p>In practice, the strongest systems combine <strong>RL-trained thinking judges + panel aggregation</strong>.</p>\n  </li>\n</ul>\n<p>RL is complementary to other robustness strategies:</p>\n<ul>\n      <li><strong>Panels of LLMs-as-Judges</strong>: RL improves each judge’s internal reasoning; panels reduce variance across judges</li>\n      <li><strong>Pointwise vs. pairwise paradigms</strong>: J1 unifies both within a single multitask model</li>\n      <li><strong>Test-time scaling</strong>: RL-trained judges benefit strongly from majority voting and averaging</li>\n    </ul>\n<p>In practice, the strongest systems combine <strong>RL-trained thinking judges + panel aggregation</strong>.</p>",
    "contentMarkdown": "*   RL is complementary to other robustness strategies:\n    \n    *   **Panels of LLMs-as-Judges**: RL improves each judge’s internal reasoning; panels reduce variance across judges\n    *   **Pointwise vs. pairwise paradigms**: J1 unifies both within a single multitask model\n    *   **Test-time scaling**: RL-trained judges benefit strongly from majority voting and averaging\n*   In practice, the strongest systems combine **RL-trained thinking judges + panel aggregation**.\n    \n\nRL is complementary to other robustness strategies:\n\n*   **Panels of LLMs-as-Judges**: RL improves each judge’s internal reasoning; panels reduce variance across judges\n*   **Pointwise vs. pairwise paradigms**: J1 unifies both within a single multitask model\n*   **Test-time scaling**: RL-trained judges benefit strongly from majority voting and averaging\n\nIn practice, the strongest systems combine **RL-trained thinking judges + panel aggregation**.",
    "contentLength": 1181,
    "wordCount": 120,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#relationship-to-panels-and-multimodal-judges"
  },
  {
    "id": "ai-LLM-as-a-judge-key-takeaways-50",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Reinforcement Learning for LLMs-as-Judges",
    "title": "Key Takeaways",
    "order": 50,
    "orderInChapter": 10,
    "contentHtml": "<ul>\n  <li>RL allows <strong>direct optimization of judgment reasoning</strong>, not just outputs</li>\n  <li>Synthetic preference data enables RL even for subjective evaluation tasks</li>\n  <li>Consistency-based rewards effectively mitigate positional bias</li>\n  <li>RL-trained judges outperform both prompted judges and traditional reward models</li>\n</ul>",
    "contentMarkdown": "*   RL allows **direct optimization of judgment reasoning**, not just outputs\n*   Synthetic preference data enables RL even for subjective evaluation tasks\n*   Consistency-based rewards effectively mitigate positional bias\n*   RL-trained judges outperform both prompted judges and traditional reward models",
    "contentLength": 358,
    "wordCount": 40,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#key-takeaways"
  },
  {
    "id": "ai-LLM-as-a-judge-length-and-verbosity-bias-51",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Biases and Mitigation Strategies",
    "title": "Length and Verbosity Bias",
    "order": 51,
    "orderInChapter": 1,
    "contentHtml": "<h4 id=\"description\">Description</h4>\n<ul>\n  <li>\n    <p>LLM judges tend to prefer <strong>longer, more verbose outputs</strong>, even when verbosity does not correlate with correctness or usefulness. Longer answers often appear more fluent, detailed, and confident, which can mislead judges into assigning higher scores.</p>\n  </li>\n  <li>\n    <p>This bias is empirically documented in <a href=\"https://arxiv.org/abs/2305.17926\">Large Language Models Are Not Fair Evaluators</a> by Wang et al. (2023).</p>\n  </li>\n</ul>\n<p>LLM judges tend to prefer <strong>longer, more verbose outputs</strong>, even when verbosity does not correlate with correctness or usefulness. Longer answers often appear more fluent, detailed, and confident, which can mislead judges into assigning higher scores.</p>\n<p>This bias is empirically documented in <a href=\"https://arxiv.org/abs/2305.17926\">Large Language Models Are Not Fair Evaluators</a> by Wang et al. (2023).</p>\n<h4 id=\"why-it-happens\">Why It Happens</h4>\n<ul>\n  <li>Pretraining data rewards explanatory, verbose text</li>\n  <li>Instruction tuning often emphasizes “helpfulness”</li>\n  <li>Fluency is conflated with quality in latent representations</li>\n</ul>\n<h4 id=\"mitigation-strategies\">Mitigation Strategies</h4>\n<ul>\n  <li><strong>Explicit conciseness criteria</strong>: Add a dedicated conciseness dimension (as in your rubric) to counterbalance verbosity.</li>\n  <li><strong>Length normalization</strong>: Penalize or bucket scores by length bands.</li>\n  <li><strong>Hard constraints</strong>: Explicitly instruct judges to ignore verbosity unless required by the task.</li>\n  <li>\n    <p><strong>Pairwise comparison with swapped lengths</strong>: Use pairwise prompts where one answer is shorter but correct.</p>\n  </li>\n  <li>Supported by practices in <a href=\"https://arxiv.org/abs/2306.05685\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> by Zheng et al. (2023).</li>\n</ul>\n<p><strong>Pairwise comparison with swapped lengths</strong>: Use pairwise prompts where one answer is shorter but correct.</p>",
    "contentMarkdown": "#### Description\n\n*   LLM judges tend to prefer **longer, more verbose outputs**, even when verbosity does not correlate with correctness or usefulness. Longer answers often appear more fluent, detailed, and confident, which can mislead judges into assigning higher scores.\n    \n*   This bias is empirically documented in [Large Language Models Are Not Fair Evaluators](https://arxiv.org/abs/2305.17926) by Wang et al. (2023).\n    \n\nLLM judges tend to prefer **longer, more verbose outputs**, even when verbosity does not correlate with correctness or usefulness. Longer answers often appear more fluent, detailed, and confident, which can mislead judges into assigning higher scores.\n\nThis bias is empirically documented in [Large Language Models Are Not Fair Evaluators](https://arxiv.org/abs/2305.17926) by Wang et al. (2023).\n\n#### Why It Happens\n\n*   Pretraining data rewards explanatory, verbose text\n*   Instruction tuning often emphasizes “helpfulness”\n*   Fluency is conflated with quality in latent representations\n\n#### Mitigation Strategies\n\n*   **Explicit conciseness criteria**: Add a dedicated conciseness dimension (as in your rubric) to counterbalance verbosity.\n*   **Length normalization**: Penalize or bucket scores by length bands.\n*   **Hard constraints**: Explicitly instruct judges to ignore verbosity unless required by the task.\n*   **Pairwise comparison with swapped lengths**: Use pairwise prompts where one answer is shorter but correct.\n    \n*   Supported by practices in [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) by Zheng et al. (2023).\n\n**Pairwise comparison with swapped lengths**: Use pairwise prompts where one answer is shorter but correct.",
    "contentLength": 2071,
    "wordCount": 229,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#length-and-verbosity-bias"
  },
  {
    "id": "ai-LLM-as-a-judge-positional-and-ordering-bias-52",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Biases and Mitigation Strategies",
    "title": "Positional and Ordering Bias",
    "order": 52,
    "orderInChapter": 2,
    "contentHtml": "<h4 id=\"description-1\">Description</h4>\n<ul>\n  <li>\n    <p>In pairwise or listwise evaluation, LLM judges may favor:</p>\n\n    <ul>\n      <li>The first item presented</li>\n      <li>The last item presented</li>\n      <li>Items appearing earlier in a list</li>\n    </ul>\n  </li>\n  <li>\n    <p>This positional bias affects both absolute judgments and relative rankings.</p>\n  </li>\n</ul>\n<p>In pairwise or listwise evaluation, LLM judges may favor:</p>\n<ul>\n      <li>The first item presented</li>\n      <li>The last item presented</li>\n      <li>Items appearing earlier in a list</li>\n    </ul>\n<p>This positional bias affects both absolute judgments and relative rankings.</p>\n<h4 id=\"why-it-happens-1\">Why It Happens</h4>\n<ul>\n  <li>Transformer attention asymmetries</li>\n  <li>Instruction-following heuristics (“evaluate the first answer”)</li>\n  <li>\n    <p>Exposure bias from conversational data</p>\n  </li>\n  <li>Documented in <a href=\"https://arxiv.org/abs/2305.17926\">A Large-Scale Analysis of Evaluation Biases in LLMs</a> by Wang et al. (2023).</li>\n</ul>\n<p>Exposure bias from conversational data</p>\n<h4 id=\"mitigation-strategies-1\">Mitigation Strategies</h4>\n<ul>\n  <li><strong>Randomized ordering</strong>: Shuffle candidate order across evaluations.</li>\n  <li><strong>Bidirectional evaluation</strong>: Evaluate (A, B) and (B, A) and average results.</li>\n  <li><strong>Explicit neutrality instructions</strong>: Instruct the judge to ignore position and treat all candidates symmetrically.</li>\n  <li><strong>Pointwise fallback</strong>: Use pointwise scoring when ordering effects dominate.</li>\n</ul>",
    "contentMarkdown": "#### Description\n\n*   In pairwise or listwise evaluation, LLM judges may favor:\n    \n    *   The first item presented\n    *   The last item presented\n    *   Items appearing earlier in a list\n*   This positional bias affects both absolute judgments and relative rankings.\n    \n\nIn pairwise or listwise evaluation, LLM judges may favor:\n\n*   The first item presented\n*   The last item presented\n*   Items appearing earlier in a list\n\nThis positional bias affects both absolute judgments and relative rankings.\n\n#### Why It Happens\n\n*   Transformer attention asymmetries\n*   Instruction-following heuristics (“evaluate the first answer”)\n*   Exposure bias from conversational data\n    \n*   Documented in [A Large-Scale Analysis of Evaluation Biases in LLMs](https://arxiv.org/abs/2305.17926) by Wang et al. (2023).\n\nExposure bias from conversational data\n\n#### Mitigation Strategies\n\n*   **Randomized ordering**: Shuffle candidate order across evaluations.\n*   **Bidirectional evaluation**: Evaluate (A, B) and (B, A) and average results.\n*   **Explicit neutrality instructions**: Instruct the judge to ignore position and treat all candidates symmetrically.\n*   **Pointwise fallback**: Use pointwise scoring when ordering effects dominate.",
    "contentLength": 1617,
    "wordCount": 166,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#positional-and-ordering-bias"
  },
  {
    "id": "ai-LLM-as-a-judge-self-preference-and-model-identity-bias-53",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Biases and Mitigation Strategies",
    "title": "Self-Preference and Model Identity Bias",
    "order": 53,
    "orderInChapter": 3,
    "contentHtml": "<h4 id=\"description-2\">Description</h4>\n<ul>\n  <li>\n    <p>LLMs often prefer outputs that resemble their own style, reasoning patterns, or phrasing. When the judge model is similar to the generator, this can lead to <strong>self-reinforcement bias</strong>.</p>\n  </li>\n  <li>\n    <p>Observed in both evaluation and reward modeling contexts, including <a href=\"https://dl.acm.org/doi/10.1145/3442188.3445922\">On the Dangers of Stochastic Parrots</a> by Bender et al. (2021) (indirectly) and later preference modeling work.</p>\n  </li>\n</ul>\n<p>LLMs often prefer outputs that resemble their own style, reasoning patterns, or phrasing. When the judge model is similar to the generator, this can lead to <strong>self-reinforcement bias</strong>.</p>\n<p>Observed in both evaluation and reward modeling contexts, including <a href=\"https://dl.acm.org/doi/10.1145/3442188.3445922\">On the Dangers of Stochastic Parrots</a> by Bender et al. (2021) (indirectly) and later preference modeling work.</p>\n<h4 id=\"why-it-happens-2\">Why It Happens</h4>\n<ul>\n  <li>Shared pretraining distributions</li>\n  <li>Latent style matching</li>\n  <li>Instruction-following alignment effects</li>\n</ul>\n<h4 id=\"mitigation-strategies-2\">Mitigation Strategies</h4>\n<ul>\n  <li><strong>Judge–generator separation</strong>: Use a different model family or version for judging.</li>\n  <li><strong>Ensemble judges</strong>: Average scores across heterogeneous judge models.</li>\n  <li><strong>Style-agnostic rubrics</strong>: Focus criteria on factuality and constraints, not phrasing.</li>\n  <li>\n    <p><strong>Human calibration checks</strong>: Periodically validate judge preferences against human annotations.</p>\n  </li>\n  <li>Used in practice in RLAIF systems such as <a href=\"https://arxiv.org/abs/2302.08582\">Training Language Models from AI Feedback</a> by Bai et al. (2023).</li>\n</ul>\n<p><strong>Human calibration checks</strong>: Periodically validate judge preferences against human annotations.</p>",
    "contentMarkdown": "#### Description\n\n*   LLMs often prefer outputs that resemble their own style, reasoning patterns, or phrasing. When the judge model is similar to the generator, this can lead to **self-reinforcement bias**.\n    \n*   Observed in both evaluation and reward modeling contexts, including [On the Dangers of Stochastic Parrots](https://dl.acm.org/doi/10.1145/3442188.3445922) by Bender et al. (2021) (indirectly) and later preference modeling work.\n    \n\nLLMs often prefer outputs that resemble their own style, reasoning patterns, or phrasing. When the judge model is similar to the generator, this can lead to **self-reinforcement bias**.\n\nObserved in both evaluation and reward modeling contexts, including [On the Dangers of Stochastic Parrots](https://dl.acm.org/doi/10.1145/3442188.3445922) by Bender et al. (2021) (indirectly) and later preference modeling work.\n\n#### Why It Happens\n\n*   Shared pretraining distributions\n*   Latent style matching\n*   Instruction-following alignment effects\n\n#### Mitigation Strategies\n\n*   **Judge–generator separation**: Use a different model family or version for judging.\n*   **Ensemble judges**: Average scores across heterogeneous judge models.\n*   **Style-agnostic rubrics**: Focus criteria on factuality and constraints, not phrasing.\n*   **Human calibration checks**: Periodically validate judge preferences against human annotations.\n    \n*   Used in practice in RLAIF systems such as [Training Language Models from AI Feedback](https://arxiv.org/abs/2302.08582) by Bai et al. (2023).\n\n**Human calibration checks**: Periodically validate judge preferences against human annotations.",
    "contentLength": 1981,
    "wordCount": 204,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#self-preference-and-model-identity-bias"
  },
  {
    "id": "ai-LLM-as-a-judge-over-confidence-and-hallucination-blindness-54",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Biases and Mitigation Strategies",
    "title": "Over-Confidence and Hallucination Blindness",
    "order": 54,
    "orderInChapter": 4,
    "contentHtml": "<h4 id=\"description-3\">Description</h4>\n<ul>\n  <li>\n    <p>LLM judges may fail to penalize <strong>confident but incorrect</strong> outputs, especially when errors are subtle, technical, or require external knowledge.</p>\n  </li>\n  <li>\n    <p>This issue is closely related to hallucination detection challenges discussed in <a href=\"https://arxiv.org/abs/2008.01415\">Evaluating the Factual Consistency of Summaries</a> by Kryściński et al. (2020).</p>\n  </li>\n</ul>\n<p>LLM judges may fail to penalize <strong>confident but incorrect</strong> outputs, especially when errors are subtle, technical, or require external knowledge.</p>\n<p>This issue is closely related to hallucination detection challenges discussed in <a href=\"https://arxiv.org/abs/2008.01415\">Evaluating the Factual Consistency of Summaries</a> by Kryściński et al. (2020).</p>\n<h4 id=\"why-it-happens-3\">Why It Happens</h4>\n<ul>\n  <li>Judges rely on internal world knowledge, which may be incomplete</li>\n  <li>Fluent text triggers higher perceived plausibility</li>\n  <li>Lack of grounding signals</li>\n</ul>\n<h4 id=\"mitigation-strategies-3\">Mitigation Strategies</h4>\n<ul>\n  <li><strong>Faithfulness criteria</strong>: Explicitly separate factual correctness from fluency.</li>\n  <li><strong>Context-aware judging</strong>: Provide the source document or evidence to the judge.</li>\n  <li><strong>Binary constraint checks</strong>: Use yes/no checks for factual violations before subjective scoring.</li>\n  <li>\n    <p><strong>Hybrid pipelines</strong>: Combine LLM judges with symbolic or retrieval-based validators.</p>\n  </li>\n  <li>Common in RAG evaluation pipelines such as those described in <a href=\"https://arxiv.org/abs/2005.11401\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a> by Lewis et al. (2020).</li>\n</ul>\n<p><strong>Hybrid pipelines</strong>: Combine LLM judges with symbolic or retrieval-based validators.</p>",
    "contentMarkdown": "#### Description\n\n*   LLM judges may fail to penalize **confident but incorrect** outputs, especially when errors are subtle, technical, or require external knowledge.\n    \n*   This issue is closely related to hallucination detection challenges discussed in [Evaluating the Factual Consistency of Summaries](https://arxiv.org/abs/2008.01415) by Kryściński et al. (2020).\n    \n\nLLM judges may fail to penalize **confident but incorrect** outputs, especially when errors are subtle, technical, or require external knowledge.\n\nThis issue is closely related to hallucination detection challenges discussed in [Evaluating the Factual Consistency of Summaries](https://arxiv.org/abs/2008.01415) by Kryściński et al. (2020).\n\n#### Why It Happens\n\n*   Judges rely on internal world knowledge, which may be incomplete\n*   Fluent text triggers higher perceived plausibility\n*   Lack of grounding signals\n\n#### Mitigation Strategies\n\n*   **Faithfulness criteria**: Explicitly separate factual correctness from fluency.\n*   **Context-aware judging**: Provide the source document or evidence to the judge.\n*   **Binary constraint checks**: Use yes/no checks for factual violations before subjective scoring.\n*   **Hybrid pipelines**: Combine LLM judges with symbolic or retrieval-based validators.\n    \n*   Common in RAG evaluation pipelines such as those described in [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Lewis et al. (2020).\n\n**Hybrid pipelines**: Combine LLM judges with symbolic or retrieval-based validators.",
    "contentLength": 1919,
    "wordCount": 195,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#over-confidence-and-hallucination-blindness"
  },
  {
    "id": "ai-LLM-as-a-judge-prompt-sensitivity-and-calibration-drift-55",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Biases and Mitigation Strategies",
    "title": "Prompt Sensitivity and Calibration Drift",
    "order": 55,
    "orderInChapter": 5,
    "contentHtml": "<h4 id=\"description-4\">Description</h4>\n<ul>\n  <li>\n    <p>LLM-as-a-Judge outputs can vary significantly with small prompt changes, leading to instability across runs or versions.</p>\n  </li>\n  <li>\n    <p>Analyzed in <a href=\"https://arxiv.org/abs/2307.03109\">How to Evaluate Language Models: A Survey</a> by Chang et al. (2023).</p>\n  </li>\n</ul>\n<p>LLM-as-a-Judge outputs can vary significantly with small prompt changes, leading to instability across runs or versions.</p>\n<p>Analyzed in <a href=\"https://arxiv.org/abs/2307.03109\">How to Evaluate Language Models: A Survey</a> by Chang et al. (2023).</p>\n<h4 id=\"why-it-happens-4\">Why It Happens</h4>\n<ul>\n  <li>Implicit instruction weighting</li>\n  <li>Order effects in criteria presentation</li>\n  <li>Ambiguous scale definitions</li>\n</ul>\n<h4 id=\"mitigation-strategies-4\">Mitigation Strategies</h4>\n<ul>\n  <li><strong>Anchored examples</strong>: Provide example inputs for each score level (as in your prompt).</li>\n  <li><strong>Schema-constrained outputs</strong>: Enforce JSON or schema-based responses.</li>\n  <li><strong>Prompt freezing</strong>: Version and lock judge prompts.</li>\n  <li><strong>Self-consistency</strong>: Sample multiple judge outputs and aggregate, as in <a href=\"https://arxiv.org/abs/2203.11171\">Self-Consistency Improves Chain of Thought Reasoning</a> by Wang et al. (2022).</li>\n</ul>",
    "contentMarkdown": "#### Description\n\n*   LLM-as-a-Judge outputs can vary significantly with small prompt changes, leading to instability across runs or versions.\n    \n*   Analyzed in [How to Evaluate Language Models: A Survey](https://arxiv.org/abs/2307.03109) by Chang et al. (2023).\n    \n\nLLM-as-a-Judge outputs can vary significantly with small prompt changes, leading to instability across runs or versions.\n\nAnalyzed in [How to Evaluate Language Models: A Survey](https://arxiv.org/abs/2307.03109) by Chang et al. (2023).\n\n#### Why It Happens\n\n*   Implicit instruction weighting\n*   Order effects in criteria presentation\n*   Ambiguous scale definitions\n\n#### Mitigation Strategies\n\n*   **Anchored examples**: Provide example inputs for each score level (as in your prompt).\n*   **Schema-constrained outputs**: Enforce JSON or schema-based responses.\n*   **Prompt freezing**: Version and lock judge prompts.\n*   **Self-consistency**: Sample multiple judge outputs and aggregate, as in [Self-Consistency Improves Chain of Thought Reasoning](https://arxiv.org/abs/2203.11171) by Wang et al. (2022).",
    "contentLength": 1372,
    "wordCount": 136,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#prompt-sensitivity-and-calibration-drift"
  },
  {
    "id": "ai-LLM-as-a-judge-reward-hacking-and-over-optimization-56",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Biases and Mitigation Strategies",
    "title": "Reward Hacking and Over-Optimization",
    "order": 56,
    "orderInChapter": 6,
    "contentHtml": "<h4 id=\"description-5\">Description</h4>\n<ul>\n  <li>\n    <p>When models are trained directly against a fixed judge, they may learn to exploit weaknesses in the rubric rather than improve true quality.</p>\n  </li>\n  <li>\n    <p>This phenomenon mirrors classical reward hacking, discussed in <a href=\"https://arxiv.org/abs/1903.03400\">Specification Gaming</a> by Krakovna et al. (2020).</p>\n  </li>\n</ul>\n<p>When models are trained directly against a fixed judge, they may learn to exploit weaknesses in the rubric rather than improve true quality.</p>\n<p>This phenomenon mirrors classical reward hacking, discussed in <a href=\"https://arxiv.org/abs/1903.03400\">Specification Gaming</a> by Krakovna et al. (2020).</p>\n<h4 id=\"why-it-happens-5\">Why It Happens</h4>\n<ul>\n  <li>Static evaluation criteria</li>\n  <li>Predictable judge behavior</li>\n  <li>Optimization pressure without diversity</li>\n</ul>\n<h4 id=\"mitigation-strategies-5\">Mitigation Strategies</h4>\n<ul>\n  <li><strong>Judge rotation</strong>: Periodically update or ensemble judges.</li>\n  <li><strong>Mixed supervision</strong>: Combine human and AI judgments.</li>\n  <li><strong>Adversarial testing</strong>: Probe judge failure modes explicitly.</li>\n  <li><strong>Holdout evaluators</strong>: Maintain unseen evaluation criteria.</li>\n</ul>",
    "contentMarkdown": "#### Description\n\n*   When models are trained directly against a fixed judge, they may learn to exploit weaknesses in the rubric rather than improve true quality.\n    \n*   This phenomenon mirrors classical reward hacking, discussed in [Specification Gaming](https://arxiv.org/abs/1903.03400) by Krakovna et al. (2020).\n    \n\nWhen models are trained directly against a fixed judge, they may learn to exploit weaknesses in the rubric rather than improve true quality.\n\nThis phenomenon mirrors classical reward hacking, discussed in [Specification Gaming](https://arxiv.org/abs/1903.03400) by Krakovna et al. (2020).\n\n#### Why It Happens\n\n*   Static evaluation criteria\n*   Predictable judge behavior\n*   Optimization pressure without diversity\n\n#### Mitigation Strategies\n\n*   **Judge rotation**: Periodically update or ensemble judges.\n*   **Mixed supervision**: Combine human and AI judgments.\n*   **Adversarial testing**: Probe judge failure modes explicitly.\n*   **Holdout evaluators**: Maintain unseen evaluation criteria.",
    "contentLength": 1304,
    "wordCount": 131,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#reward-hacking-and-over-optimization"
  },
  {
    "id": "ai-LLM-as-a-judge-takeaways-57",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "Biases and Mitigation Strategies",
    "title": "Takeaways",
    "order": 57,
    "orderInChapter": 7,
    "contentHtml": "<ul>\n  <li>\n    <p>LLM-as-a-Judge is powerful but <strong>not objective by default</strong>. Robust systems:</p>\n\n    <ul>\n      <li>Anticipate common bias modes</li>\n      <li>Encode counterbalances directly into rubrics</li>\n      <li>Use escalation strategies (pointwise <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-131-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1548\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1549\"><span class=\"mo\" id=\"MathJax-Span-1550\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-131\">\\rightarrow</script> pairwise <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-132-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1551\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1552\"><span class=\"mo\" id=\"MathJax-Span-1553\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-132\">\\rightarrow</script> listwise)</li>\n      <li>Periodically validate against human judgment</li>\n    </ul>\n  </li>\n  <li>\n    <p>With these safeguards in place, LLM-as-a-Judge becomes a reliable, scalable component of modern evaluation and ranking pipelines.</p>\n  </li>\n</ul>\n<p>LLM-as-a-Judge is powerful but <strong>not objective by default</strong>. Robust systems:</p>\n<ul>\n      <li>Anticipate common bias modes</li>\n      <li>Encode counterbalances directly into rubrics</li>\n      <li>Use escalation strategies (pointwise <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-131-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1548\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1549\"><span class=\"mo\" id=\"MathJax-Span-1550\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-131\">\\rightarrow</script> pairwise <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-132-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1551\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1552\"><span class=\"mo\" id=\"MathJax-Span-1553\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-132\">\\rightarrow</script> listwise)</li>\n      <li>Periodically validate against human judgment</li>\n    </ul>\n<p>With these safeguards in place, LLM-as-a-Judge becomes a reliable, scalable component of modern evaluation and ranking pipelines.</p>",
    "contentMarkdown": "*   LLM-as-a-Judge is powerful but **not objective by default**. Robust systems:\n    \n    *   Anticipate common bias modes\n    *   Encode counterbalances directly into rubrics\n    *   Use escalation strategies (pointwise →→\\\\rightarrow pairwise →→\\\\rightarrow listwise)\n    *   Periodically validate against human judgment\n*   With these safeguards in place, LLM-as-a-Judge becomes a reliable, scalable component of modern evaluation and ranking pipelines.\n    \n\nLLM-as-a-Judge is powerful but **not objective by default**. Robust systems:\n\n*   Anticipate common bias modes\n*   Encode counterbalances directly into rubrics\n*   Use escalation strategies (pointwise →→\\\\rightarrow pairwise →→\\\\rightarrow listwise)\n*   Periodically validate against human judgment\n\nWith these safeguards in place, LLM-as-a-Judge becomes a reliable, scalable component of modern evaluation and ranking pipelines.",
    "contentLength": 6021,
    "wordCount": 108,
    "hasCode": false,
    "hasMath": true,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#takeaways"
  },
  {
    "id": "ai-LLM-as-a-judge-pointwise-evaluation-when-absolute-scoring-is-enou-58",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "When to Use Which Paradigm: a Practical Decision Guide",
    "title": "Pointwise Evaluation: When Absolute Scoring is Enough",
    "order": 58,
    "orderInChapter": 1,
    "contentHtml": "<h4 id=\"when-to-use-pointwise\">When to Use Pointwise</h4>\n<ul>\n  <li>\n    <p>Use <strong>pointwise evaluation</strong> when:</p>\n\n    <ul>\n      <li>You need to score each output independently</li>\n      <li>Outputs will be filtered, thresholded, or aggregated later</li>\n      <li>The task has reasonably well-defined criteria</li>\n      <li>You want maximal scalability and parallelism</li>\n      <li>Latency and cost are important constraints</li>\n      <li>You are bootstrapping supervision data</li>\n      <li>Global ordering between outputs is not required</li>\n    </ul>\n  </li>\n  <li>\n    <p>Typical use cases:</p>\n\n    <ul>\n      <li>Offline model evaluation</li>\n      <li>Dataset filtering and quality control</li>\n      <li>Reward modeling targets</li>\n      <li>CI-style regression testing</li>\n      <li>RAG document scoring</li>\n      <li>Safety or policy compliance checks</li>\n      <li>Hallucination detection with binary or ternary criteria</li>\n    </ul>\n  </li>\n  <li>\n    <p>This is the <strong>default choice</strong> in most production systems.</p>\n  </li>\n</ul>\n<p>Use <strong>pointwise evaluation</strong> when:</p>\n<ul>\n      <li>You need to score each output independently</li>\n      <li>Outputs will be filtered, thresholded, or aggregated later</li>\n      <li>The task has reasonably well-defined criteria</li>\n      <li>You want maximal scalability and parallelism</li>\n      <li>Latency and cost are important constraints</li>\n      <li>You are bootstrapping supervision data</li>\n      <li>Global ordering between outputs is not required</li>\n    </ul>\n<p>Typical use cases:</p>\n<ul>\n      <li>Offline model evaluation</li>\n      <li>Dataset filtering and quality control</li>\n      <li>Reward modeling targets</li>\n      <li>CI-style regression testing</li>\n      <li>RAG document scoring</li>\n      <li>Safety or policy compliance checks</li>\n      <li>Hallucination detection with binary or ternary criteria</li>\n    </ul>\n<p>This is the <strong>default choice</strong> in most production systems.</p>\n<h4 id=\"why-pointwise-works-well\">Why Pointwise Works Well</h4>\n<ul>\n  <li>Simple mental model: “Is this output good?”</li>\n  <li>Easy to operationalize with structured rubrics</li>\n  <li>Works naturally with mixed-scale criteria (binary, ternary, Likert-type)</li>\n  <li>Judge calls are embarrassingly parallel</li>\n  <li>\n    <p>Maps directly to pointwise LTR models (e.g., monoBERT-style scoring)</p>\n  </li>\n  <li>As noted earlier, <strong>LLM-as-a-judge models most commonly perform pointwise evaluation (scoring each output independently)</strong>, as documented in <a href=\"https://arxiv.org/abs/2306.05685\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> by Zheng et al. (2023).</li>\n</ul>\n<p>Maps directly to pointwise LTR models (e.g., monoBERT-style scoring)</p>\n<h4 id=\"extensions-panels-and-multimodal-judges-in-pointwise-settings\">Extensions: Panels and Multimodal Judges in Pointwise Settings</h4>\n<ul>\n  <li>\n    <p>Pointwise evaluation is also the <strong>most common entry point</strong> for both:</p>\n  </li>\n  <li><strong>Panels of LLMs-as-Judges</strong>, where multiple judges score each output independently and scores are aggregated (e.g., averaged or max-pooled), as shown in <a href=\"https://arxiv.org/abs/2404.18796\">Replacing Judges with Juries</a> by Verga et al. (2024).</li>\n  <li><strong>Multimodal LLMs-as-Judges</strong>, where the judge consumes images (or video) alongside text to score outputs for vision–language tasks, such as <a href=\"https://arxiv.org/abs/2410.02712\">LLaVA-Critic</a> by Xiong et al. (2024) and <a href=\"https://arxiv.org/abs/2401.06591\">Prometheus-Vision</a> by Lee et al. (2024).</li>\n  <li>In practice, most multimodal judging today is <strong>pointwise</strong>, because it is simpler, cheaper, and easier to calibrate than multimodal pairwise or listwise evaluation.</li>\n</ul>\n<p>Pointwise evaluation is also the <strong>most common entry point</strong> for both:</p>\n<h4 id=\"when-pointwise-breaks-down\">When Pointwise Breaks Down</h4>\n<ul>\n  <li>Judges struggle to calibrate absolute scores</li>\n  <li>Small differences between outputs matter</li>\n  <li>Outputs are all “good enough,” but ordering matters</li>\n  <li>Scale drift across batches becomes noticeable</li>\n  <li>Single-judge bias or instability becomes apparent</li>\n  <li>When these issues appear, <strong>pairwise evaluation or judge panels</strong> are often the next step.</li>\n</ul>",
    "contentMarkdown": "#### When to Use Pointwise\n\n*   Use **pointwise evaluation** when:\n    \n    *   You need to score each output independently\n    *   Outputs will be filtered, thresholded, or aggregated later\n    *   The task has reasonably well-defined criteria\n    *   You want maximal scalability and parallelism\n    *   Latency and cost are important constraints\n    *   You are bootstrapping supervision data\n    *   Global ordering between outputs is not required\n*   Typical use cases:\n    \n    *   Offline model evaluation\n    *   Dataset filtering and quality control\n    *   Reward modeling targets\n    *   CI-style regression testing\n    *   RAG document scoring\n    *   Safety or policy compliance checks\n    *   Hallucination detection with binary or ternary criteria\n*   This is the **default choice** in most production systems.\n    \n\nUse **pointwise evaluation** when:\n\n*   You need to score each output independently\n*   Outputs will be filtered, thresholded, or aggregated later\n*   The task has reasonably well-defined criteria\n*   You want maximal scalability and parallelism\n*   Latency and cost are important constraints\n*   You are bootstrapping supervision data\n*   Global ordering between outputs is not required\n\nTypical use cases:\n\n*   Offline model evaluation\n*   Dataset filtering and quality control\n*   Reward modeling targets\n*   CI-style regression testing\n*   RAG document scoring\n*   Safety or policy compliance checks\n*   Hallucination detection with binary or ternary criteria\n\nThis is the **default choice** in most production systems.\n\n#### Why Pointwise Works Well\n\n*   Simple mental model: “Is this output good?”\n*   Easy to operationalize with structured rubrics\n*   Works naturally with mixed-scale criteria (binary, ternary, Likert-type)\n*   Judge calls are embarrassingly parallel\n*   Maps directly to pointwise LTR models (e.g., monoBERT-style scoring)\n    \n*   As noted earlier, **LLM-as-a-judge models most commonly perform pointwise evaluation (scoring each output independently)**, as documented in [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) by Zheng et al. (2023).\n\nMaps directly to pointwise LTR models (e.g., monoBERT-style scoring)\n\n#### Extensions: Panels and Multimodal Judges in Pointwise Settings\n\n*   Pointwise evaluation is also the **most common entry point** for both:\n    \n*   **Panels of LLMs-as-Judges**, where multiple judges score each output independently and scores are aggregated (e.g., averaged or max-pooled), as shown in [Replacing Judges with Juries](https://arxiv.org/abs/2404.18796) by Verga et al. (2024).\n*   **Multimodal LLMs-as-Judges**, where the judge consumes images (or video) alongside text to score outputs for vision–language tasks, such as [LLaVA-Critic](https://arxiv.org/abs/2410.02712) by Xiong et al. (2024) and [Prometheus-Vision](https://arxiv.org/abs/2401.06591) by Lee et al. (2024).\n*   In practice, most multimodal judging today is **pointwise**, because it is simpler, cheaper, and easier to calibrate than multimodal pairwise or listwise evaluation.\n\nPointwise evaluation is also the **most common entry point** for both:\n\n#### When Pointwise Breaks Down\n\n*   Judges struggle to calibrate absolute scores\n*   Small differences between outputs matter\n*   Outputs are all “good enough,” but ordering matters\n*   Scale drift across batches becomes noticeable\n*   Single-judge bias or instability becomes apparent\n*   When these issues appear, **pairwise evaluation or judge panels** are often the next step.",
    "contentLength": 4463,
    "wordCount": 476,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#pointwise-evaluation:-when-absolute-scoring-is-enough"
  },
  {
    "id": "ai-LLM-as-a-judge-pairwise-evaluation-when-relative-preference-matte-59",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "When to Use Which Paradigm: a Practical Decision Guide",
    "title": "Pairwise Evaluation: When Relative Preference Matters",
    "order": 59,
    "orderInChapter": 2,
    "contentHtml": "<h4 id=\"when-to-use-pairwise\">When to Use Pairwise</h4>\n<ul>\n  <li>\n    <p>Use <strong>pairwise evaluation</strong> when:</p>\n\n    <ul>\n      <li>You care about <em>which output is better</em>, not how good it is</li>\n      <li>Outputs are close in quality</li>\n      <li>Absolute scores are noisy or poorly calibrated</li>\n      <li>Human-like preference judgments are desired</li>\n      <li>You are selecting between two or a small number of candidates</li>\n      <li>You want to reduce scale interpretation ambiguity</li>\n    </ul>\n  </li>\n  <li>\n    <p>Typical use cases:</p>\n\n    <ul>\n      <li>A/B testing between models</li>\n      <li>Preference modeling and reward learning</li>\n      <li>Human-in-the-loop evaluation</li>\n      <li>Model selection</li>\n      <li>Tie-breaking among top candidates</li>\n      <li>Pairwise reward modeling for RLHF or RLAIF</li>\n    </ul>\n  </li>\n</ul>\n<p>Use <strong>pairwise evaluation</strong> when:</p>\n<ul>\n      <li>You care about <em>which output is better</em>, not how good it is</li>\n      <li>Outputs are close in quality</li>\n      <li>Absolute scores are noisy or poorly calibrated</li>\n      <li>Human-like preference judgments are desired</li>\n      <li>You are selecting between two or a small number of candidates</li>\n      <li>You want to reduce scale interpretation ambiguity</li>\n    </ul>\n<p>Typical use cases:</p>\n<ul>\n      <li>A/B testing between models</li>\n      <li>Preference modeling and reward learning</li>\n      <li>Human-in-the-loop evaluation</li>\n      <li>Model selection</li>\n      <li>Tie-breaking among top candidates</li>\n      <li>Pairwise reward modeling for RLHF or RLAIF</li>\n    </ul>\n<h4 id=\"why-pairwise-often-improves-reliability\">Why Pairwise Often Improves Reliability</h4>\n<ul>\n  <li>\n    <p>Pairwise judgments reduce cognitive load for both humans and models:</p>\n\n    <ul>\n      <li>Easier to answer “Which is better?” than “How good is this?”</li>\n      <li>Reduces ambiguity in ordinal or Likert-type scales</li>\n      <li>Higher inter-annotator and human–model agreement, as shown in <a href=\"https://arxiv.org/abs/2305.17926\">Large Language Models Are Not Fair Evaluators</a> by Wang et al. (2023)</li>\n    </ul>\n  </li>\n  <li>\n    <p>Pairwise evaluation aligns naturally with:</p>\n\n    <ul>\n      <li>Pairwise LTR objectives (RankNet, duoBERT)</li>\n      <li>Preference datasets used in RLHF and RLAIF</li>\n      <li>Direct Preference Optimization (DPO)</li>\n    </ul>\n  </li>\n</ul>\n<p>Pairwise judgments reduce cognitive load for both humans and models:</p>\n<ul>\n      <li>Easier to answer “Which is better?” than “How good is this?”</li>\n      <li>Reduces ambiguity in ordinal or Likert-type scales</li>\n      <li>Higher inter-annotator and human–model agreement, as shown in <a href=\"https://arxiv.org/abs/2305.17926\">Large Language Models Are Not Fair Evaluators</a> by Wang et al. (2023)</li>\n    </ul>\n<p>Pairwise evaluation aligns naturally with:</p>\n<ul>\n      <li>Pairwise LTR objectives (RankNet, duoBERT)</li>\n      <li>Preference datasets used in RLHF and RLAIF</li>\n      <li>Direct Preference Optimization (DPO)</li>\n    </ul>\n<h4 id=\"panels-and-multimodal-pairwise-evaluation\">Panels and Multimodal Pairwise Evaluation</h4>\n<ul>\n  <li>\n    <p>Pairwise evaluation is especially effective when combined with <strong>panels of judges</strong>:</p>\n\n    <ul>\n      <li>Each judge independently decides which output is better</li>\n      <li>Preferences are aggregated via majority vote or averaged logits</li>\n      <li>Reduces positional bias and self-preference effects</li>\n    </ul>\n  </li>\n  <li>\n    <p>This strategy is empirically validated in <a href=\"https://arxiv.org/abs/2404.18796\">Replacing Judges with Juries</a> by Verga et al. (2024), which shows that small, diverse panels outperform single large judges in pairwise settings.</p>\n  </li>\n  <li>\n    <p>Multimodal pairwise judging is used less frequently, but is critical when:</p>\n\n    <ul>\n      <li>Visual grounding is required to decide preference</li>\n      <li>Two captions, answers, or explanations must be compared against an image</li>\n      <li>Reward signals are generated for multimodal preference learning</li>\n    </ul>\n  </li>\n  <li>\n    <p>LLaVA-Critic explicitly supports such pairwise multimodal judging.</p>\n  </li>\n</ul>\n<p>Pairwise evaluation is especially effective when combined with <strong>panels of judges</strong>:</p>\n<ul>\n      <li>Each judge independently decides which output is better</li>\n      <li>Preferences are aggregated via majority vote or averaged logits</li>\n      <li>Reduces positional bias and self-preference effects</li>\n    </ul>\n<p>This strategy is empirically validated in <a href=\"https://arxiv.org/abs/2404.18796\">Replacing Judges with Juries</a> by Verga et al. (2024), which shows that small, diverse panels outperform single large judges in pairwise settings.</p>\n<p>Multimodal pairwise judging is used less frequently, but is critical when:</p>\n<ul>\n      <li>Visual grounding is required to decide preference</li>\n      <li>Two captions, answers, or explanations must be compared against an image</li>\n      <li>Reward signals are generated for multimodal preference learning</li>\n    </ul>\n<p>LLaVA-Critic explicitly supports such pairwise multimodal judging.</p>\n<h4 id=\"trade-offs\">Trade-offs</h4>\n<ul>\n  <li>Quadratic cost in number of candidates</li>\n  <li>Harder to aggregate across many outputs</li>\n  <li>\n    <p>Still local: no global consistency guarantees</p>\n  </li>\n  <li>If you need <em>global ordering</em> across many outputs, listwise evaluation becomes appropriate.</li>\n</ul>\n<p>Still local: no global consistency guarantees</p>",
    "contentMarkdown": "#### When to Use Pairwise\n\n*   Use **pairwise evaluation** when:\n    \n    *   You care about _which output is better_, not how good it is\n    *   Outputs are close in quality\n    *   Absolute scores are noisy or poorly calibrated\n    *   Human-like preference judgments are desired\n    *   You are selecting between two or a small number of candidates\n    *   You want to reduce scale interpretation ambiguity\n*   Typical use cases:\n    \n    *   A/B testing between models\n    *   Preference modeling and reward learning\n    *   Human-in-the-loop evaluation\n    *   Model selection\n    *   Tie-breaking among top candidates\n    *   Pairwise reward modeling for RLHF or RLAIF\n\nUse **pairwise evaluation** when:\n\n*   You care about _which output is better_, not how good it is\n*   Outputs are close in quality\n*   Absolute scores are noisy or poorly calibrated\n*   Human-like preference judgments are desired\n*   You are selecting between two or a small number of candidates\n*   You want to reduce scale interpretation ambiguity\n\nTypical use cases:\n\n*   A/B testing between models\n*   Preference modeling and reward learning\n*   Human-in-the-loop evaluation\n*   Model selection\n*   Tie-breaking among top candidates\n*   Pairwise reward modeling for RLHF or RLAIF\n\n#### Why Pairwise Often Improves Reliability\n\n*   Pairwise judgments reduce cognitive load for both humans and models:\n    \n    *   Easier to answer “Which is better?” than “How good is this?”\n    *   Reduces ambiguity in ordinal or Likert-type scales\n    *   Higher inter-annotator and human–model agreement, as shown in [Large Language Models Are Not Fair Evaluators](https://arxiv.org/abs/2305.17926) by Wang et al. (2023)\n*   Pairwise evaluation aligns naturally with:\n    \n    *   Pairwise LTR objectives (RankNet, duoBERT)\n    *   Preference datasets used in RLHF and RLAIF\n    *   Direct Preference Optimization (DPO)\n\nPairwise judgments reduce cognitive load for both humans and models:\n\n*   Easier to answer “Which is better?” than “How good is this?”\n*   Reduces ambiguity in ordinal or Likert-type scales\n*   Higher inter-annotator and human–model agreement, as shown in [Large Language Models Are Not Fair Evaluators](https://arxiv.org/abs/2305.17926) by Wang et al. (2023)\n\nPairwise evaluation aligns naturally with:\n\n*   Pairwise LTR objectives (RankNet, duoBERT)\n*   Preference datasets used in RLHF and RLAIF\n*   Direct Preference Optimization (DPO)\n\n#### Panels and Multimodal Pairwise Evaluation\n\n*   Pairwise evaluation is especially effective when combined with **panels of judges**:\n    \n    *   Each judge independently decides which output is better\n    *   Preferences are aggregated via majority vote or averaged logits\n    *   Reduces positional bias and self-preference effects\n*   This strategy is empirically validated in [Replacing Judges with Juries](https://arxiv.org/abs/2404.18796) by Verga et al. (2024), which shows that small, diverse panels outperform single large judges in pairwise settings.\n    \n*   Multimodal pairwise judging is used less frequently, but is critical when:\n    \n    *   Visual grounding is required to decide preference\n    *   Two captions, answers, or explanations must be compared against an image\n    *   Reward signals are generated for multimodal preference learning\n*   LLaVA-Critic explicitly supports such pairwise multimodal judging.\n    \n\nPairwise evaluation is especially effective when combined with **panels of judges**:\n\n*   Each judge independently decides which output is better\n*   Preferences are aggregated via majority vote or averaged logits\n*   Reduces positional bias and self-preference effects\n\nThis strategy is empirically validated in [Replacing Judges with Juries](https://arxiv.org/abs/2404.18796) by Verga et al. (2024), which shows that small, diverse panels outperform single large judges in pairwise settings.\n\nMultimodal pairwise judging is used less frequently, but is critical when:\n\n*   Visual grounding is required to decide preference\n*   Two captions, answers, or explanations must be compared against an image\n*   Reward signals are generated for multimodal preference learning\n\nLLaVA-Critic explicitly supports such pairwise multimodal judging.\n\n#### Trade-offs\n\n*   Quadratic cost in number of candidates\n*   Harder to aggregate across many outputs\n*   Still local: no global consistency guarantees\n    \n*   If you need _global ordering_ across many outputs, listwise evaluation becomes appropriate.\n\nStill local: no global consistency guarantees",
    "contentLength": 5679,
    "wordCount": 621,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#pairwise-evaluation:-when-relative-preference-matters"
  },
  {
    "id": "ai-LLM-as-a-judge-listwise-evaluation-when-global-ranking-quality-ma-60",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "When to Use Which Paradigm: a Practical Decision Guide",
    "title": "Listwise Evaluation: When Global Ranking Quality Matters",
    "order": 60,
    "orderInChapter": 3,
    "contentHtml": "<h4 id=\"when-to-use-listwise\">When to Use Listwise</h4>\n<ul>\n  <li>\n    <p>Use <strong>listwise evaluation</strong> when:</p>\n\n    <ul>\n      <li>You must rank many outputs at once</li>\n      <li>Global consistency across outputs matters</li>\n      <li>You want to optimize ranking metrics directly</li>\n      <li>Interactions between outputs affect evaluation</li>\n      <li>You can afford higher compute and memory cost</li>\n    </ul>\n  </li>\n  <li>\n    <p>Typical use cases:</p>\n\n    <ul>\n      <li>Final-stage reranking in RAG pipelines</li>\n      <li>Search result ordering</li>\n      <li>Leaderboard construction</li>\n      <li>Evaluation of multiple candidate generations</li>\n      <li>Research settings where ranking quality is paramount</li>\n    </ul>\n  </li>\n</ul>\n<p>Use <strong>listwise evaluation</strong> when:</p>\n<ul>\n      <li>You must rank many outputs at once</li>\n      <li>Global consistency across outputs matters</li>\n      <li>You want to optimize ranking metrics directly</li>\n      <li>Interactions between outputs affect evaluation</li>\n      <li>You can afford higher compute and memory cost</li>\n    </ul>\n<p>Typical use cases:</p>\n<ul>\n      <li>Final-stage reranking in RAG pipelines</li>\n      <li>Search result ordering</li>\n      <li>Leaderboard construction</li>\n      <li>Evaluation of multiple candidate generations</li>\n      <li>Research settings where ranking quality is paramount</li>\n    </ul>\n<h4 id=\"why-listwise-is-powerful\">Why Listwise is Powerful</h4>\n<ul>\n  <li>\n    <p>Listwise methods:</p>\n\n    <ul>\n      <li>Optimize ranking quality holistically</li>\n      <li>Capture inter-item dependencies</li>\n      <li>Avoid local inconsistencies (e.g., A &gt; B, B &gt; C, but C &gt; A)</li>\n    </ul>\n  </li>\n  <li>\n    <p>Modern listwise systems include:</p>\n\n    <ul>\n      <li>Encoder-only listwise rankers such as <a href=\"https://arxiv.org/abs/2206.15198\">ListBERT</a> by Kumar et al. (2022)</li>\n      <li>Encoder–decoder listwise rankers such as <a href=\"https://arxiv.org/abs/2402.15838\">ListT5</a> by Yoon et al. (2024)</li>\n      <li>Agentic and reasoning-based rerankers such as <a href=\"https://arxiv.org/abs/2505.20046\">REARANK</a> by Zhang et al. (2025)</li>\n    </ul>\n  </li>\n</ul>\n<p>Listwise methods:</p>\n<ul>\n      <li>Optimize ranking quality holistically</li>\n      <li>Capture inter-item dependencies</li>\n      <li>Avoid local inconsistencies (e.g., A &gt; B, B &gt; C, but C &gt; A)</li>\n    </ul>\n<p>Modern listwise systems include:</p>\n<ul>\n      <li>Encoder-only listwise rankers such as <a href=\"https://arxiv.org/abs/2206.15198\">ListBERT</a> by Kumar et al. (2022)</li>\n      <li>Encoder–decoder listwise rankers such as <a href=\"https://arxiv.org/abs/2402.15838\">ListT5</a> by Yoon et al. (2024)</li>\n      <li>Agentic and reasoning-based rerankers such as <a href=\"https://arxiv.org/abs/2505.20046\">REARANK</a> by Zhang et al. (2025)</li>\n    </ul>\n<h4 id=\"panels-and-multimodal-listwise-evaluation\">Panels and Multimodal Listwise Evaluation</h4>\n<ul>\n  <li>\n    <p>Listwise evaluation can also be combined with:</p>\n\n    <ul>\n      <li><strong>Panels of judges</strong>, using rank aggregation methods (e.g., average rank, Borda count)</li>\n      <li><strong>Multimodal judges</strong>, when ranking depends on visual evidence</li>\n    </ul>\n  </li>\n  <li>\n    <p>However, this combination is <strong>computationally expensive</strong> and is typically reserved for:</p>\n\n    <ul>\n      <li>Small candidate sets</li>\n      <li>Offline evaluation</li>\n      <li>Research benchmarks</li>\n    </ul>\n  </li>\n</ul>\n<p>Listwise evaluation can also be combined with:</p>\n<ul>\n      <li><strong>Panels of judges</strong>, using rank aggregation methods (e.g., average rank, Borda count)</li>\n      <li><strong>Multimodal judges</strong>, when ranking depends on visual evidence</li>\n    </ul>\n<p>However, this combination is <strong>computationally expensive</strong> and is typically reserved for:</p>\n<ul>\n      <li>Small candidate sets</li>\n      <li>Offline evaluation</li>\n      <li>Research benchmarks</li>\n    </ul>\n<h4 id=\"trade-offs-1\">Trade-offs</h4>\n<ul>\n  <li>Highest computational cost</li>\n  <li>More complex training and inference</li>\n  <li>Harder to debug and calibrate</li>\n  <li>Often unnecessary unless ranking quality is critical</li>\n</ul>",
    "contentMarkdown": "#### When to Use Listwise\n\n*   Use **listwise evaluation** when:\n    \n    *   You must rank many outputs at once\n    *   Global consistency across outputs matters\n    *   You want to optimize ranking metrics directly\n    *   Interactions between outputs affect evaluation\n    *   You can afford higher compute and memory cost\n*   Typical use cases:\n    \n    *   Final-stage reranking in RAG pipelines\n    *   Search result ordering\n    *   Leaderboard construction\n    *   Evaluation of multiple candidate generations\n    *   Research settings where ranking quality is paramount\n\nUse **listwise evaluation** when:\n\n*   You must rank many outputs at once\n*   Global consistency across outputs matters\n*   You want to optimize ranking metrics directly\n*   Interactions between outputs affect evaluation\n*   You can afford higher compute and memory cost\n\nTypical use cases:\n\n*   Final-stage reranking in RAG pipelines\n*   Search result ordering\n*   Leaderboard construction\n*   Evaluation of multiple candidate generations\n*   Research settings where ranking quality is paramount\n\n#### Why Listwise is Powerful\n\n*   Listwise methods:\n    \n    *   Optimize ranking quality holistically\n    *   Capture inter-item dependencies\n    *   Avoid local inconsistencies (e.g., A > B, B > C, but C > A)\n*   Modern listwise systems include:\n    \n    *   Encoder-only listwise rankers such as [ListBERT](https://arxiv.org/abs/2206.15198) by Kumar et al. (2022)\n    *   Encoder–decoder listwise rankers such as [ListT5](https://arxiv.org/abs/2402.15838) by Yoon et al. (2024)\n    *   Agentic and reasoning-based rerankers such as [REARANK](https://arxiv.org/abs/2505.20046) by Zhang et al. (2025)\n\nListwise methods:\n\n*   Optimize ranking quality holistically\n*   Capture inter-item dependencies\n*   Avoid local inconsistencies (e.g., A > B, B > C, but C > A)\n\nModern listwise systems include:\n\n*   Encoder-only listwise rankers such as [ListBERT](https://arxiv.org/abs/2206.15198) by Kumar et al. (2022)\n*   Encoder–decoder listwise rankers such as [ListT5](https://arxiv.org/abs/2402.15838) by Yoon et al. (2024)\n*   Agentic and reasoning-based rerankers such as [REARANK](https://arxiv.org/abs/2505.20046) by Zhang et al. (2025)\n\n#### Panels and Multimodal Listwise Evaluation\n\n*   Listwise evaluation can also be combined with:\n    \n    *   **Panels of judges**, using rank aggregation methods (e.g., average rank, Borda count)\n    *   **Multimodal judges**, when ranking depends on visual evidence\n*   However, this combination is **computationally expensive** and is typically reserved for:\n    \n    *   Small candidate sets\n    *   Offline evaluation\n    *   Research benchmarks\n\nListwise evaluation can also be combined with:\n\n*   **Panels of judges**, using rank aggregation methods (e.g., average rank, Borda count)\n*   **Multimodal judges**, when ranking depends on visual evidence\n\nHowever, this combination is **computationally expensive** and is typically reserved for:\n\n*   Small candidate sets\n*   Offline evaluation\n*   Research benchmarks\n\n#### Trade-offs\n\n*   Highest computational cost\n*   More complex training and inference\n*   Harder to debug and calibrate\n*   Often unnecessary unless ranking quality is critical",
    "contentLength": 4329,
    "wordCount": 424,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#listwise-evaluation:-when-global-ranking-quality-matters"
  },
  {
    "id": "ai-LLM-as-a-judge-a-simple-escalation-strategy-including-judges-and--61",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "When to Use Which Paradigm: a Practical Decision Guide",
    "title": "A Simple Escalation Strategy (Including Judges and Modalities)",
    "order": 61,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>\n    <p>A practical escalation strategy used in real systems:</p>\n\n    <ol>\n      <li>Start with <strong>pointwise LLM-as-a-Judge (single, text-only)</strong></li>\n      <li>\n        <p>Add <strong>a panel of judges</strong> if:</p>\n\n        <ul>\n          <li>Scores are unstable</li>\n          <li>Bias is suspected</li>\n        </ul>\n      </li>\n      <li>\n        <p>Move to <strong>pairwise evaluation</strong> if:</p>\n\n        <ul>\n          <li>Differences are subtle</li>\n          <li>Calibration is unreliable</li>\n        </ul>\n      </li>\n      <li>\n        <p>Introduce <strong>multimodal judges</strong> if:</p>\n\n        <ul>\n          <li>Evaluation depends on images or video</li>\n        </ul>\n      </li>\n      <li>\n        <p>Use <strong>listwise evaluation</strong> only when:</p>\n\n        <ul>\n          <li>You must rank many candidates</li>\n          <li>Global ordering quality is essential</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p>This mirrors how humans evaluate: absolute judgments first, comparisons next, full rankings last—while consulting multiple evaluators when stakes are high.</p>\n  </li>\n</ul>\n<p>A practical escalation strategy used in real systems:</p>\n<ol>\n      <li>Start with <strong>pointwise LLM-as-a-Judge (single, text-only)</strong></li>\n      <li>\n        <p>Add <strong>a panel of judges</strong> if:</p>\n\n        <ul>\n          <li>Scores are unstable</li>\n          <li>Bias is suspected</li>\n        </ul>\n      </li>\n      <li>\n        <p>Move to <strong>pairwise evaluation</strong> if:</p>\n\n        <ul>\n          <li>Differences are subtle</li>\n          <li>Calibration is unreliable</li>\n        </ul>\n      </li>\n      <li>\n        <p>Introduce <strong>multimodal judges</strong> if:</p>\n\n        <ul>\n          <li>Evaluation depends on images or video</li>\n        </ul>\n      </li>\n      <li>\n        <p>Use <strong>listwise evaluation</strong> only when:</p>\n\n        <ul>\n          <li>You must rank many candidates</li>\n          <li>Global ordering quality is essential</li>\n        </ul>\n      </li>\n    </ol>\n<p>Add <strong>a panel of judges</strong> if:</p>\n<ul>\n          <li>Scores are unstable</li>\n          <li>Bias is suspected</li>\n        </ul>\n<p>Move to <strong>pairwise evaluation</strong> if:</p>\n<ul>\n          <li>Differences are subtle</li>\n          <li>Calibration is unreliable</li>\n        </ul>\n<p>Introduce <strong>multimodal judges</strong> if:</p>\n<ul>\n          <li>Evaluation depends on images or video</li>\n        </ul>\n<p>Use <strong>listwise evaluation</strong> only when:</p>\n<ul>\n          <li>You must rank many candidates</li>\n          <li>Global ordering quality is essential</li>\n        </ul>\n<p>This mirrors how humans evaluate: absolute judgments first, comparisons next, full rankings last—while consulting multiple evaluators when stakes are high.</p>",
    "contentMarkdown": "*   A practical escalation strategy used in real systems:\n    \n    1.  Start with **pointwise LLM-as-a-Judge (single, text-only)**\n    2.  Add **a panel of judges** if:\n        \n        *   Scores are unstable\n        *   Bias is suspected\n    3.  Move to **pairwise evaluation** if:\n        \n        *   Differences are subtle\n        *   Calibration is unreliable\n    4.  Introduce **multimodal judges** if:\n        \n        *   Evaluation depends on images or video\n    5.  Use **listwise evaluation** only when:\n        \n        *   You must rank many candidates\n        *   Global ordering quality is essential\n*   This mirrors how humans evaluate: absolute judgments first, comparisons next, full rankings last—while consulting multiple evaluators when stakes are high.\n    \n\nA practical escalation strategy used in real systems:\n\n1.  Start with **pointwise LLM-as-a-Judge (single, text-only)**\n2.  Add **a panel of judges** if:\n    \n    *   Scores are unstable\n    *   Bias is suspected\n3.  Move to **pairwise evaluation** if:\n    \n    *   Differences are subtle\n    *   Calibration is unreliable\n4.  Introduce **multimodal judges** if:\n    \n    *   Evaluation depends on images or video\n5.  Use **listwise evaluation** only when:\n    \n    *   You must rank many candidates\n    *   Global ordering quality is essential\n\nAdd **a panel of judges** if:\n\n*   Scores are unstable\n*   Bias is suspected\n\nMove to **pairwise evaluation** if:\n\n*   Differences are subtle\n*   Calibration is unreliable\n\nIntroduce **multimodal judges** if:\n\n*   Evaluation depends on images or video\n\nUse **listwise evaluation** only when:\n\n*   You must rank many candidates\n*   Global ordering quality is essential\n\nThis mirrors how humans evaluate: absolute judgments first, comparisons next, full rankings last—while consulting multiple evaluators when stakes are high.",
    "contentLength": 2893,
    "wordCount": 245,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#a-simple-escalation-strategy-(including-judges-and-modalities)"
  },
  {
    "id": "ai-LLM-as-a-judge-comparative-analysis-across-evaluation-dimensions-62",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "When to Use Which Paradigm: a Practical Decision Guide",
    "title": "Comparative Analysis Across Evaluation Dimensions",
    "order": 62,
    "orderInChapter": 5,
    "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Evaluation Dimension</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Configuration</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Typical Use Cases</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\" rowspan=\"3\">Evaluation granularity</td>\n<td class=\"tg-tleft-valign-first\">Pointwise</td>\n<td class=\"tg-tleft-valign-second\">Independent scoring, filtering, CI evaluation, reward bootstrapping, default evaluation mode</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Pairwise</td>\n<td class=\"tg-tleft-valign-second\">A/B testing, preference learning, tie-breaking, calibration-sensitive evaluation</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Listwise</td>\n<td class=\"tg-tleft-valign-second\">Final-stage reranking, leaderboard construction, global ordering of many candidates</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\" rowspan=\"2\">Judge capacity</td>\n<td class=\"tg-tleft-valign-first\">Single judge</td>\n<td class=\"tg-tleft-valign-second\">Low-cost evaluation, rapid iteration, prototyping, low-stakes tasks</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Panel of judges</td>\n<td class=\"tg-tleft-valign-second\">Bias reduction, robustness, human-aligned scoring, high-stakes evaluation</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\" rowspan=\"2\">Input modality</td>\n<td class=\"tg-tleft-valign-first\">Text-only</td>\n<td class=\"tg-tleft-valign-second\">Language modeling, reasoning, summarization, code and policy evaluation</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Multimodal</td>\n<td class=\"tg-tleft-valign-second\">Vision–language tasks, captioning, VQA, multimodal preference learning</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Evaluation Dimension</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Configuration</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Typical Use Cases</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\" rowspan=\"3\">Evaluation granularity</td>\n<td class=\"tg-tleft-valign-first\">Pointwise</td>\n<td class=\"tg-tleft-valign-second\">Independent scoring, filtering, CI evaluation, reward bootstrapping, default evaluation mode</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Pairwise</td>\n<td class=\"tg-tleft-valign-second\">A/B testing, preference learning, tie-breaking, calibration-sensitive evaluation</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Listwise</td>\n<td class=\"tg-tleft-valign-second\">Final-stage reranking, leaderboard construction, global ordering of many candidates</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\" rowspan=\"2\">Judge capacity</td>\n<td class=\"tg-tleft-valign-first\">Single judge</td>\n<td class=\"tg-tleft-valign-second\">Low-cost evaluation, rapid iteration, prototyping, low-stakes tasks</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Panel of judges</td>\n<td class=\"tg-tleft-valign-second\">Bias reduction, robustness, human-aligned scoring, high-stakes evaluation</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\" rowspan=\"2\">Input modality</td>\n<td class=\"tg-tleft-valign-first\">Text-only</td>\n<td class=\"tg-tleft-valign-second\">Language modeling, reasoning, summarization, code and policy evaluation</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Multimodal</td>\n<td class=\"tg-tleft-valign-second\">Vision–language tasks, captioning, VQA, multimodal preference learning</td>\n</tr>\n</tbody>\n</table>",
    "contentMarkdown": "**Evaluation Dimension**\n\n**Configuration**\n\n**Typical Use Cases**\n\nEvaluation granularity\n\nPointwise\n\nIndependent scoring, filtering, CI evaluation, reward bootstrapping, default evaluation mode\n\nPairwise\n\nA/B testing, preference learning, tie-breaking, calibration-sensitive evaluation\n\nListwise\n\nFinal-stage reranking, leaderboard construction, global ordering of many candidates\n\nJudge capacity\n\nSingle judge\n\nLow-cost evaluation, rapid iteration, prototyping, low-stakes tasks\n\nPanel of judges\n\nBias reduction, robustness, human-aligned scoring, high-stakes evaluation\n\nInput modality\n\nText-only\n\nLanguage modeling, reasoning, summarization, code and policy evaluation\n\nMultimodal\n\nVision–language tasks, captioning, VQA, multimodal preference learning\n\n**Evaluation Dimension**\n\n**Configuration**\n\n**Typical Use Cases**\n\nEvaluation granularity\n\nPointwise\n\nIndependent scoring, filtering, CI evaluation, reward bootstrapping, default evaluation mode\n\nPairwise\n\nA/B testing, preference learning, tie-breaking, calibration-sensitive evaluation\n\nListwise\n\nFinal-stage reranking, leaderboard construction, global ordering of many candidates\n\nJudge capacity\n\nSingle judge\n\nLow-cost evaluation, rapid iteration, prototyping, low-stakes tasks\n\nPanel of judges\n\nBias reduction, robustness, human-aligned scoring, high-stakes evaluation\n\nInput modality\n\nText-only\n\nLanguage modeling, reasoning, summarization, code and policy evaluation\n\nMultimodal\n\nVision–language tasks, captioning, VQA, multimodal preference learning",
    "contentLength": 3543,
    "wordCount": 154,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#comparative-analysis-across-evaluation-dimensions"
  },
  {
    "id": "ai-LLM-as-a-judge-practical-tips-63",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "When to Use Which Paradigm: a Practical Decision Guide",
    "title": "Practical Tips",
    "order": 63,
    "orderInChapter": 6,
    "contentHtml": "<h4 id=\"use-pointwise-evaluation-by-default\">Use Pointwise Evaluation by Default</h4>\n<ul>\n  <li>\n    <p>Pointwise evaluation should be the starting point for almost all LLM-as-a-Judge systems because it imposes the fewest assumptions on the evaluation problem.</p>\n  </li>\n  <li>\n    <p>In pointwise evaluation, each output is judged independently against a rubric. This matches how most quality criteria are defined in practice (correctness, safety, faithfulness, clarity) and maps cleanly to scalable infrastructure: evaluations are parallelizable, easy to cache, and easy to aggregate downstream.</p>\n  </li>\n  <li>\n    <p>Pointwise judging also aligns naturally with how LLMs are instruction-tuned. Asking “Is this output acceptable under these criteria?” is a task modern LLMs handle reliably, especially when rubrics include anchored examples and constrained output schemas.</p>\n  </li>\n  <li>\n    <p>Most importantly, pointwise evaluation gives you <strong>useful signals early</strong>. Even if scores are noisy, they are sufficient for filtering, regression testing, and bootstrapping supervision. In practice, many systems never need to move beyond pointwise judging.</p>\n  </li>\n  <li>\n    <p><strong>Escalation trigger:</strong> move on only if absolute scores become unstable, poorly calibrated, or insufficient to distinguish high-quality outputs.</p>\n  </li>\n</ul>\n<p>Pointwise evaluation should be the starting point for almost all LLM-as-a-Judge systems because it imposes the fewest assumptions on the evaluation problem.</p>\n<p>In pointwise evaluation, each output is judged independently against a rubric. This matches how most quality criteria are defined in practice (correctness, safety, faithfulness, clarity) and maps cleanly to scalable infrastructure: evaluations are parallelizable, easy to cache, and easy to aggregate downstream.</p>\n<p>Pointwise judging also aligns naturally with how LLMs are instruction-tuned. Asking “Is this output acceptable under these criteria?” is a task modern LLMs handle reliably, especially when rubrics include anchored examples and constrained output schemas.</p>\n<p>Most importantly, pointwise evaluation gives you <strong>useful signals early</strong>. Even if scores are noisy, they are sufficient for filtering, regression testing, and bootstrapping supervision. In practice, many systems never need to move beyond pointwise judging.</p>\n<p><strong>Escalation trigger:</strong> move on only if absolute scores become unstable, poorly calibrated, or insufficient to distinguish high-quality outputs.</p>\n<h4 id=\"add-judge-panels-before-changing-paradigms\">Add Judge Panels Before Changing Paradigms</h4>\n<ul>\n  <li>\n    <p>If pointwise evaluation starts to show variance, bias, or inconsistency, the first escalation should be <strong>adding judges</strong>, not changing the evaluation granularity.</p>\n  </li>\n  <li>\n    <p>Panels of LLMs-as-Judges reduce error by aggregation rather than by making the task harder for a single judge. Empirically, averaging or pooling judgments from diverse models improves alignment with human evaluations more reliably than switching to pairwise or listwise judging with a single model.</p>\n  </li>\n  <li>\n    <p>Judge panels specifically mitigate:</p>\n\n    <ul>\n      <li>Self-preference bias (models favoring their own outputs)</li>\n      <li>Prompt sensitivity and formatting artifacts</li>\n      <li>Overconfidence in fluent but incorrect answers</li>\n    </ul>\n  </li>\n  <li>\n    <p>Operationally, panels are easy to introduce because they preserve the same prompt and evaluation interface. You simply replicate the judge call across models and aggregate results.</p>\n  </li>\n  <li>\n    <p><strong>Escalation trigger:</strong> move beyond panels only if <strong>relative ordering</strong> between outputs, rather than absolute quality, becomes the core requirement.</p>\n  </li>\n</ul>\n<p>If pointwise evaluation starts to show variance, bias, or inconsistency, the first escalation should be <strong>adding judges</strong>, not changing the evaluation granularity.</p>\n<p>Panels of LLMs-as-Judges reduce error by aggregation rather than by making the task harder for a single judge. Empirically, averaging or pooling judgments from diverse models improves alignment with human evaluations more reliably than switching to pairwise or listwise judging with a single model.</p>\n<p>Judge panels specifically mitigate:</p>\n<ul>\n      <li>Self-preference bias (models favoring their own outputs)</li>\n      <li>Prompt sensitivity and formatting artifacts</li>\n      <li>Overconfidence in fluent but incorrect answers</li>\n    </ul>\n<p>Operationally, panels are easy to introduce because they preserve the same prompt and evaluation interface. You simply replicate the judge call across models and aggregate results.</p>\n<p><strong>Escalation trigger:</strong> move beyond panels only if <strong>relative ordering</strong> between outputs, rather than absolute quality, becomes the core requirement.</p>\n<h4 id=\"use-pairwise-evaluation-when-relative-preference-matters\">Use Pairwise Evaluation When Relative Preference Matters</h4>\n<ul>\n  <li>\n    <p>Pairwise evaluation should be introduced when the question is no longer “Is this good?” but rather “Which of these is better?”</p>\n  </li>\n  <li>\n    <p>This situation arises when:</p>\n\n    <ul>\n      <li>Outputs are close in quality</li>\n      <li>Most candidates pass minimum quality thresholds</li>\n      <li>Absolute scores compress or saturate</li>\n      <li>Human decision-making would naturally be comparative</li>\n    </ul>\n  </li>\n  <li>\n    <p>Pairwise judging reduces calibration issues because it eliminates the need to interpret score scales. Both humans and LLMs are more reliable when making comparative judgments, especially under subtle trade-offs.</p>\n  </li>\n  <li>\n    <p>In practice, pairwise evaluation is most effective when combined with judge panels. Each judge provides a preference, and aggregation yields a stable decision even if individual judgments disagree.</p>\n  </li>\n  <li>\n    <p><strong>Escalation trigger:</strong> move beyond pairwise only if you must rank <strong>many outputs simultaneously</strong> and require global consistency.</p>\n  </li>\n</ul>\n<p>Pairwise evaluation should be introduced when the question is no longer “Is this good?” but rather “Which of these is better?”</p>\n<p>This situation arises when:</p>\n<ul>\n      <li>Outputs are close in quality</li>\n      <li>Most candidates pass minimum quality thresholds</li>\n      <li>Absolute scores compress or saturate</li>\n      <li>Human decision-making would naturally be comparative</li>\n    </ul>\n<p>Pairwise judging reduces calibration issues because it eliminates the need to interpret score scales. Both humans and LLMs are more reliable when making comparative judgments, especially under subtle trade-offs.</p>\n<p>In practice, pairwise evaluation is most effective when combined with judge panels. Each judge provides a preference, and aggregation yields a stable decision even if individual judgments disagree.</p>\n<p><strong>Escalation trigger:</strong> move beyond pairwise only if you must rank <strong>many outputs simultaneously</strong> and require global consistency.</p>\n<h4 id=\"introduce-multimodal-judges-only-when-grounding-is-required\">Introduce Multimodal Judges Only When Grounding is Required</h4>\n<ul>\n  <li>\n    <p>Multimodal LLMs-as-Judges should be introduced only when the evaluation task depends on <strong>non-textual evidence</strong>.</p>\n  </li>\n  <li>\n    <p>Examples include:</p>\n\n    <ul>\n      <li>Visual question answering</li>\n      <li>Image-grounded explanation evaluation</li>\n      <li>Caption correctness and hallucination detection</li>\n      <li>Multimodal preference learning</li>\n    </ul>\n  </li>\n  <li>\n    <p>If the task can be reliably reduced to text (e.g., via deterministic preprocessing), text-only judges are usually cheaper, faster, and more stable.</p>\n  </li>\n  <li>\n    <p>Multimodal judging increases system complexity: it requires multimodal inputs, heavier models, and careful prompt design to ensure grounding is actually used. As a result, multimodal judges should be treated as <strong>specialized evaluators</strong>, not defaults.</p>\n  </li>\n  <li>\n    <p><strong>Escalation trigger:</strong> introduce multimodal judges only when text-only judges fail due to missing perceptual information.</p>\n  </li>\n</ul>\n<p>Multimodal LLMs-as-Judges should be introduced only when the evaluation task depends on <strong>non-textual evidence</strong>.</p>\n<p>Examples include:</p>\n<ul>\n      <li>Visual question answering</li>\n      <li>Image-grounded explanation evaluation</li>\n      <li>Caption correctness and hallucination detection</li>\n      <li>Multimodal preference learning</li>\n    </ul>\n<p>If the task can be reliably reduced to text (e.g., via deterministic preprocessing), text-only judges are usually cheaper, faster, and more stable.</p>\n<p>Multimodal judging increases system complexity: it requires multimodal inputs, heavier models, and careful prompt design to ensure grounding is actually used. As a result, multimodal judges should be treated as <strong>specialized evaluators</strong>, not defaults.</p>\n<p><strong>Escalation trigger:</strong> introduce multimodal judges only when text-only judges fail due to missing perceptual information.</p>\n<h4 id=\"use-listwise-evaluation-sparingly-when-ranking-quality-outweighs-cost\">Use Listwise Evaluation Sparingly, When Ranking Quality Outweighs Cost</h4>\n<ul>\n  <li>\n    <p>Listwise evaluation is the most expressive but also the most expensive paradigm. It should be reserved for cases where <strong>global ranking quality is the primary objective</strong>.</p>\n  </li>\n  <li>\n    <p>This includes:</p>\n\n    <ul>\n      <li>Final-stage reranking in retrieval or RAG pipelines</li>\n      <li>Benchmark leaderboard construction</li>\n      <li>Research settings where ranking metrics matter more than cost</li>\n    </ul>\n  </li>\n  <li>\n    <p>Listwise methods allow judges or models to reason over interactions among all candidates, avoiding local inconsistencies inherent in pointwise or pairwise decisions. However, they come with significant drawbacks: higher computational cost, reduced scalability, and increased difficulty in debugging failures.</p>\n  </li>\n  <li>\n    <p>In production systems, listwise evaluation is often applied to <strong>small candidate sets</strong> or performed offline, with pointwise or pairwise methods handling the majority of evaluations.</p>\n  </li>\n  <li>\n    <p><strong>Escalation trigger:</strong> use listwise evaluation only when pairwise comparisons fail to produce a consistent or acceptable ordering.</p>\n  </li>\n</ul>\n<p>Listwise evaluation is the most expressive but also the most expensive paradigm. It should be reserved for cases where <strong>global ranking quality is the primary objective</strong>.</p>\n<p>This includes:</p>\n<ul>\n      <li>Final-stage reranking in retrieval or RAG pipelines</li>\n      <li>Benchmark leaderboard construction</li>\n      <li>Research settings where ranking metrics matter more than cost</li>\n    </ul>\n<p>Listwise methods allow judges or models to reason over interactions among all candidates, avoiding local inconsistencies inherent in pointwise or pairwise decisions. However, they come with significant drawbacks: higher computational cost, reduced scalability, and increased difficulty in debugging failures.</p>\n<p>In production systems, listwise evaluation is often applied to <strong>small candidate sets</strong> or performed offline, with pointwise or pairwise methods handling the majority of evaluations.</p>\n<p><strong>Escalation trigger:</strong> use listwise evaluation only when pairwise comparisons fail to produce a consistent or acceptable ordering.</p>\n<h4 id=\"the-core-principle\">The Core Principle</h4>\n<ul>\n  <li>\n    <p>Across all dimensions, the guiding principle is: <strong>Increase robustness before increasing complexity.</strong></p>\n  </li>\n  <li>\n    <p>Start simple, add redundancy (panels), then change the evaluation task (pairwise), and only then adopt the most expressive (and expensive) paradigms such as multimodal or listwise judging.</p>\n  </li>\n</ul>\n<p>Across all dimensions, the guiding principle is: <strong>Increase robustness before increasing complexity.</strong></p>\n<p>Start simple, add redundancy (panels), then change the evaluation task (pairwise), and only then adopt the most expressive (and expensive) paradigms such as multimodal or listwise judging.</p>",
    "contentMarkdown": "#### Use Pointwise Evaluation by Default\n\n*   Pointwise evaluation should be the starting point for almost all LLM-as-a-Judge systems because it imposes the fewest assumptions on the evaluation problem.\n    \n*   In pointwise evaluation, each output is judged independently against a rubric. This matches how most quality criteria are defined in practice (correctness, safety, faithfulness, clarity) and maps cleanly to scalable infrastructure: evaluations are parallelizable, easy to cache, and easy to aggregate downstream.\n    \n*   Pointwise judging also aligns naturally with how LLMs are instruction-tuned. Asking “Is this output acceptable under these criteria?” is a task modern LLMs handle reliably, especially when rubrics include anchored examples and constrained output schemas.\n    \n*   Most importantly, pointwise evaluation gives you **useful signals early**. Even if scores are noisy, they are sufficient for filtering, regression testing, and bootstrapping supervision. In practice, many systems never need to move beyond pointwise judging.\n    \n*   **Escalation trigger:** move on only if absolute scores become unstable, poorly calibrated, or insufficient to distinguish high-quality outputs.\n    \n\nPointwise evaluation should be the starting point for almost all LLM-as-a-Judge systems because it imposes the fewest assumptions on the evaluation problem.\n\nIn pointwise evaluation, each output is judged independently against a rubric. This matches how most quality criteria are defined in practice (correctness, safety, faithfulness, clarity) and maps cleanly to scalable infrastructure: evaluations are parallelizable, easy to cache, and easy to aggregate downstream.\n\nPointwise judging also aligns naturally with how LLMs are instruction-tuned. Asking “Is this output acceptable under these criteria?” is a task modern LLMs handle reliably, especially when rubrics include anchored examples and constrained output schemas.\n\nMost importantly, pointwise evaluation gives you **useful signals early**. Even if scores are noisy, they are sufficient for filtering, regression testing, and bootstrapping supervision. In practice, many systems never need to move beyond pointwise judging.\n\n**Escalation trigger:** move on only if absolute scores become unstable, poorly calibrated, or insufficient to distinguish high-quality outputs.\n\n#### Add Judge Panels Before Changing Paradigms\n\n*   If pointwise evaluation starts to show variance, bias, or inconsistency, the first escalation should be **adding judges**, not changing the evaluation granularity.\n    \n*   Panels of LLMs-as-Judges reduce error by aggregation rather than by making the task harder for a single judge. Empirically, averaging or pooling judgments from diverse models improves alignment with human evaluations more reliably than switching to pairwise or listwise judging with a single model.\n    \n*   Judge panels specifically mitigate:\n    \n    *   Self-preference bias (models favoring their own outputs)\n    *   Prompt sensitivity and formatting artifacts\n    *   Overconfidence in fluent but incorrect answers\n*   Operationally, panels are easy to introduce because they preserve the same prompt and evaluation interface. You simply replicate the judge call across models and aggregate results.\n    \n*   **Escalation trigger:** move beyond panels only if **relative ordering** between outputs, rather than absolute quality, becomes the core requirement.\n    \n\nIf pointwise evaluation starts to show variance, bias, or inconsistency, the first escalation should be **adding judges**, not changing the evaluation granularity.\n\nPanels of LLMs-as-Judges reduce error by aggregation rather than by making the task harder for a single judge. Empirically, averaging or pooling judgments from diverse models improves alignment with human evaluations more reliably than switching to pairwise or listwise judging with a single model.\n\nJudge panels specifically mitigate:\n\n*   Self-preference bias (models favoring their own outputs)\n*   Prompt sensitivity and formatting artifacts\n*   Overconfidence in fluent but incorrect answers\n\nOperationally, panels are easy to introduce because they preserve the same prompt and evaluation interface. You simply replicate the judge call across models and aggregate results.\n\n**Escalation trigger:** move beyond panels only if **relative ordering** between outputs, rather than absolute quality, becomes the core requirement.\n\n#### Use Pairwise Evaluation When Relative Preference Matters\n\n*   Pairwise evaluation should be introduced when the question is no longer “Is this good?” but rather “Which of these is better?”\n    \n*   This situation arises when:\n    \n    *   Outputs are close in quality\n    *   Most candidates pass minimum quality thresholds\n    *   Absolute scores compress or saturate\n    *   Human decision-making would naturally be comparative\n*   Pairwise judging reduces calibration issues because it eliminates the need to interpret score scales. Both humans and LLMs are more reliable when making comparative judgments, especially under subtle trade-offs.\n    \n*   In practice, pairwise evaluation is most effective when combined with judge panels. Each judge provides a preference, and aggregation yields a stable decision even if individual judgments disagree.\n    \n*   **Escalation trigger:** move beyond pairwise only if you must rank **many outputs simultaneously** and require global consistency.\n    \n\nPairwise evaluation should be introduced when the question is no longer “Is this good?” but rather “Which of these is better?”\n\nThis situation arises when:\n\n*   Outputs are close in quality\n*   Most candidates pass minimum quality thresholds\n*   Absolute scores compress or saturate\n*   Human decision-making would naturally be comparative\n\nPairwise judging reduces calibration issues because it eliminates the need to interpret score scales. Both humans and LLMs are more reliable when making comparative judgments, especially under subtle trade-offs.\n\nIn practice, pairwise evaluation is most effective when combined with judge panels. Each judge provides a preference, and aggregation yields a stable decision even if individual judgments disagree.\n\n**Escalation trigger:** move beyond pairwise only if you must rank **many outputs simultaneously** and require global consistency.\n\n#### Introduce Multimodal Judges Only When Grounding is Required\n\n*   Multimodal LLMs-as-Judges should be introduced only when the evaluation task depends on **non-textual evidence**.\n    \n*   Examples include:\n    \n    *   Visual question answering\n    *   Image-grounded explanation evaluation\n    *   Caption correctness and hallucination detection\n    *   Multimodal preference learning\n*   If the task can be reliably reduced to text (e.g., via deterministic preprocessing), text-only judges are usually cheaper, faster, and more stable.\n    \n*   Multimodal judging increases system complexity: it requires multimodal inputs, heavier models, and careful prompt design to ensure grounding is actually used. As a result, multimodal judges should be treated as **specialized evaluators**, not defaults.\n    \n*   **Escalation trigger:** introduce multimodal judges only when text-only judges fail due to missing perceptual information.\n    \n\nMultimodal LLMs-as-Judges should be introduced only when the evaluation task depends on **non-textual evidence**.\n\nExamples include:\n\n*   Visual question answering\n*   Image-grounded explanation evaluation\n*   Caption correctness and hallucination detection\n*   Multimodal preference learning\n\nIf the task can be reliably reduced to text (e.g., via deterministic preprocessing), text-only judges are usually cheaper, faster, and more stable.\n\nMultimodal judging increases system complexity: it requires multimodal inputs, heavier models, and careful prompt design to ensure grounding is actually used. As a result, multimodal judges should be treated as **specialized evaluators**, not defaults.\n\n**Escalation trigger:** introduce multimodal judges only when text-only judges fail due to missing perceptual information.\n\n#### Use Listwise Evaluation Sparingly, When Ranking Quality Outweighs Cost\n\n*   Listwise evaluation is the most expressive but also the most expensive paradigm. It should be reserved for cases where **global ranking quality is the primary objective**.\n    \n*   This includes:\n    \n    *   Final-stage reranking in retrieval or RAG pipelines\n    *   Benchmark leaderboard construction\n    *   Research settings where ranking metrics matter more than cost\n*   Listwise methods allow judges or models to reason over interactions among all candidates, avoiding local inconsistencies inherent in pointwise or pairwise decisions. However, they come with significant drawbacks: higher computational cost, reduced scalability, and increased difficulty in debugging failures.\n    \n*   In production systems, listwise evaluation is often applied to **small candidate sets** or performed offline, with pointwise or pairwise methods handling the majority of evaluations.\n    \n*   **Escalation trigger:** use listwise evaluation only when pairwise comparisons fail to produce a consistent or acceptable ordering.\n    \n\nListwise evaluation is the most expressive but also the most expensive paradigm. It should be reserved for cases where **global ranking quality is the primary objective**.\n\nThis includes:\n\n*   Final-stage reranking in retrieval or RAG pipelines\n*   Benchmark leaderboard construction\n*   Research settings where ranking metrics matter more than cost\n\nListwise methods allow judges or models to reason over interactions among all candidates, avoiding local inconsistencies inherent in pointwise or pairwise decisions. However, they come with significant drawbacks: higher computational cost, reduced scalability, and increased difficulty in debugging failures.\n\nIn production systems, listwise evaluation is often applied to **small candidate sets** or performed offline, with pointwise or pairwise methods handling the majority of evaluations.\n\n**Escalation trigger:** use listwise evaluation only when pairwise comparisons fail to produce a consistent or acceptable ordering.\n\n#### The Core Principle\n\n*   Across all dimensions, the guiding principle is: **Increase robustness before increasing complexity.**\n    \n*   Start simple, add redundancy (panels), then change the evaluation task (pairwise), and only then adopt the most expressive (and expensive) paradigms such as multimodal or listwise judging.\n    \n\nAcross all dimensions, the guiding principle is: **Increase robustness before increasing complexity.**\n\nStart simple, add redundancy (panels), then change the evaluation task (pairwise), and only then adopt the most expressive (and expensive) paradigms such as multimodal or listwise judging.",
    "contentLength": 12575,
    "wordCount": 1447,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#practical-tips"
  },
  {
    "id": "ai-LLM-as-a-judge-llm-as-a-judge-foundations-and-bias-analysis-64",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "References",
    "title": "LLM-as-a-Judge Foundations and Bias Analysis",
    "order": 64,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li><a href=\"https://arxiv.org/abs/2306.05685\">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> by Zheng et al. (2023)</li>\n  <li><a href=\"https://arxiv.org/abs/2305.17926\">Large Language Models Are Not Fair Evaluators</a> by Wang et al. (2023)</li>\n  <li><a href=\"https://arxiv.org/abs/2401.01917\">LLM Evaluators Recognize and Favor Their Own Generations</a> by Panickssery et al. (2024)</li>\n  <li><a href=\"https://arxiv.org/abs/2403.02839\">An Empirical Study of LLM-as-a-Judge for LLM Evaluation</a> by Huang et al. (2024)</li>\n</ul>",
    "contentMarkdown": "*   [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685) by Zheng et al. (2023)\n*   [Large Language Models Are Not Fair Evaluators](https://arxiv.org/abs/2305.17926) by Wang et al. (2023)\n*   [LLM Evaluators Recognize and Favor Their Own Generations](https://arxiv.org/abs/2401.01917) by Panickssery et al. (2024)\n*   [An Empirical Study of LLM-as-a-Judge for LLM Evaluation](https://arxiv.org/abs/2403.02839) by Huang et al. (2024)",
    "contentLength": 553,
    "wordCount": 54,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#llm-as-a-judge-foundations-and-bias-analysis"
  },
  {
    "id": "ai-LLM-as-a-judge-panels-juries-of-llms-as-judges-65",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "References",
    "title": "Panels / Juries of LLMs-as-Judges",
    "order": 65,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li><a href=\"https://arxiv.org/abs/2404.18796\">Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models</a> by Verga et al. (2024)</li>\n  <li><a href=\"https://dl.acm.org/doi/10.1145/290941.291017\">Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness</a> by Voorhees (1998)</li>\n</ul>",
    "contentMarkdown": "*   [Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models](https://arxiv.org/abs/2404.18796) by Verga et al. (2024)\n*   [Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness](https://dl.acm.org/doi/10.1145/290941.291017) by Voorhees (1998)",
    "contentLength": 349,
    "wordCount": 33,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#panels-/-juries-of-llms-as-judges"
  },
  {
    "id": "ai-LLM-as-a-judge-multimodal-llms-as-judges-66",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "References",
    "title": "Multimodal LLMs-as-Judges",
    "order": 66,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li><a href=\"https://arxiv.org/abs/2401.06591\">Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation</a> by Lee et al. (2024)</li>\n  <li><a href=\"https://arxiv.org/abs/2410.02712\">LLaVA-Critic: Learning to Evaluate Multimodal Models</a> by Xiong et al. (2024)</li>\n  <li><a href=\"https://arxiv.org/abs/2304.08485\">LLaVA: Large Language and Vision Assistant</a> by Li et al. (2023)</li>\n</ul>",
    "contentMarkdown": "*   [Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation](https://arxiv.org/abs/2401.06591) by Lee et al. (2024)\n*   [LLaVA-Critic: Learning to Evaluate Multimodal Models](https://arxiv.org/abs/2410.02712) by Xiong et al. (2024)\n*   [LLaVA: Large Language and Vision Assistant](https://arxiv.org/abs/2304.08485) by Li et al. (2023)",
    "contentLength": 424,
    "wordCount": 39,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#multimodal-llms-as-judges"
  },
  {
    "id": "ai-LLM-as-a-judge-learning-to-rank-ltr-and-neural-re-ranking-67",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "References",
    "title": "Learning-to-Rank (LTR) and Neural Re-ranking",
    "order": 67,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li><a href=\"https://arxiv.org/abs/1910.14424\">Multi-Stage Document Ranking with BERT</a> by Nogueira et al. (2019)</li>\n  <li><a href=\"https://arxiv.org/abs/2206.15198\">ListBERT: Learning to Rank E-commerce Products with Listwise BERT</a> by Kumar et al. (2022)</li>\n  <li><a href=\"https://arxiv.org/abs/2402.15838\">ListT5: Listwise Re-ranking with Fusion-in-Decoder Improves Zero-shot Retrieval</a> by Yoon et al. (2024)</li>\n  <li><a href=\"https://arxiv.org/abs/2505.20046\">REARANK: Reasoning Re-ranking Agent via Reinforcement Learning</a> by Zhang et al. (2025)</li>\n  <li><a href=\"https://arxiv.org/abs/2505.14432\">Rank-K: Test-Time Reasoning for Listwise Reranking</a> by Yang et al. (2025)</li>\n</ul>",
    "contentMarkdown": "*   [Multi-Stage Document Ranking with BERT](https://arxiv.org/abs/1910.14424) by Nogueira et al. (2019)\n*   [ListBERT: Learning to Rank E-commerce Products with Listwise BERT](https://arxiv.org/abs/2206.15198) by Kumar et al. (2022)\n*   [ListT5: Listwise Re-ranking with Fusion-in-Decoder Improves Zero-shot Retrieval](https://arxiv.org/abs/2402.15838) by Yoon et al. (2024)\n*   [REARANK: Reasoning Re-ranking Agent via Reinforcement Learning](https://arxiv.org/abs/2505.20046) by Zhang et al. (2025)\n*   [Rank-K: Test-Time Reasoning for Listwise Reranking](https://arxiv.org/abs/2505.14432) by Yang et al. (2025)",
    "contentLength": 715,
    "wordCount": 65,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#learning-to-rank-(ltr)-and-neural-re-ranking"
  },
  {
    "id": "ai-LLM-as-a-judge-reinforcement-learning-for-llms-as-judges-and-rewa-68",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "References",
    "title": "Reinforcement Learning for LLMs-as-Judges and Reward Modeling",
    "order": 68,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li><a href=\"https://arxiv.org/abs/2505.10320\">J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning</a> by Whitehouse et al. (2025)</li>\n  <li><a href=\"https://arxiv.org/abs/2402.03300\">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a> by Shao et al. (2024)</li>\n  <li><a href=\"https://arxiv.org/abs/2501.12948\">DeepSeek-R1: Incentivizing Reasoning in LLMs via Reinforcement Learning</a> by Guo et al. (2025)</li>\n  <li><a href=\"https://arxiv.org/abs/2408.02666\">Self-Taught Evaluators</a> by Wang et al. (2024)</li>\n  <li><a href=\"https://openreview.net/forum?id=PNRznmmWP7\">Learning to Plan and Reason for Evaluation with Thinking-LLM-as-a-Judge</a> by Saha et al. (2025)</li>\n</ul>",
    "contentMarkdown": "*   [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320) by Whitehouse et al. (2025)\n*   [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300) by Shao et al. (2024)\n*   [DeepSeek-R1: Incentivizing Reasoning in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948) by Guo et al. (2025)\n*   [Self-Taught Evaluators](https://arxiv.org/abs/2408.02666) by Wang et al. (2024)\n*   [Learning to Plan and Reason for Evaluation with Thinking-LLM-as-a-Judge](https://openreview.net/forum?id=PNRznmmWP7) by Saha et al. (2025)",
    "contentLength": 747,
    "wordCount": 68,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#reinforcement-learning-for-llms-as-judges-and-reward-modeling"
  },
  {
    "id": "ai-LLM-as-a-judge-reward-model-and-judge-benchmarks-69",
    "articleSlug": "LLM-as-a-judge",
    "articleTitle": "LLM-as-a-Judge / Autoraters",
    "category": "Algorithms/Architecture",
    "chapter": "References",
    "title": "Reward Model and Judge Benchmarks",
    "order": 69,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li><a href=\"https://github.com/lmarena/PPE\">PPE: Preference Proxy Evaluations</a> by Chiang et al. (2024)</li>\n  <li><a href=\"https://arxiv.org/abs/2403.13787\">RewardBench: Evaluating Reward Models for Language Modeling</a> by Lambert et al. (2024)</li>\n  <li><a href=\"https://arxiv.org/abs/2506.01937\">RewardBench 2: Advancing Reward Model Evaluation</a> by Malik et al. (2025)</li>\n  <li><a href=\"https://arxiv.org/abs/2410.12784\">JudgeBench: A Benchmark for Evaluating LLM-based Judges</a> by Tan et al. (2024)</li>\n  <li><a href=\"https://arxiv.org/abs/2408.02666\">RM-Bench</a> by Wang et al. (2024)</li>\n  <li><a href=\"https://arxiv.org/abs/2505.10320\">FollowBenchEval</a> by Whitehouse et al. (2025)</li>\n</ul>",
    "contentMarkdown": "*   [PPE: Preference Proxy Evaluations](https://github.com/lmarena/PPE) by Chiang et al. (2024)\n*   [RewardBench: Evaluating Reward Models for Language Modeling](https://arxiv.org/abs/2403.13787) by Lambert et al. (2024)\n*   [RewardBench 2: Advancing Reward Model Evaluation](https://arxiv.org/abs/2506.01937) by Malik et al. (2025)\n*   [JudgeBench: A Benchmark for Evaluating LLM-based Judges](https://arxiv.org/abs/2410.12784) by Tan et al. (2024)\n*   [RM-Bench](https://arxiv.org/abs/2408.02666) by Wang et al. (2024)\n*   [FollowBenchEval](https://arxiv.org/abs/2505.10320) by Whitehouse et al. (2025)",
    "contentLength": 723,
    "wordCount": 62,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/LLM-as-a-judge/#reward-model-and-judge-benchmarks"
  }
]