[
  {
    "id": "ai-benchmarks-general-benchmarks-1",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Large Language Models (LLMs)",
    "title": "General Benchmarks",
    "subtitle": "Large Language Models (LLMs)",
    "contentHtml": "<h4 id=\"language-understanding\">Language Understanding</h4>\n<h5 id=\"glue-general-language-understanding-evaluation\">GLUE (General Language Understanding Evaluation)</h5>\n<ul>\n  <li><strong>Description:</strong> A set of nine tasks including question answering and textual entailment, designed to gauge general language understanding.</li>\n  <li><strong>Dataset Attributes:</strong> Diverse text genres from web text, fiction, and non-fiction, requiring models to handle a variety of language styles and complexities. The tasks range from single-sentence tasks (e.g., CoLA for linguistic acceptability) to sentence-pair tasks (e.g., MRPC for paraphrase detection).</li>\n  <li><strong>Reference:</strong> <a href=\"https://openreview.net/forum?id=rJ4km2R5t7\">“GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding”</a>.</li>\n</ul>\n<h5 id=\"superglue\">SuperGLUE</h5>\n<ul>\n  <li><strong>Description:</strong> A more challenging version of GLUE intended to push language models to their limits.</li>\n  <li><strong>Dataset Attributes:</strong> Includes more complex reasoning tasks over multiple domains, emphasizing inference, logic, and common sense. Tasks include Boolean Question (BoolQ), CommitmentBank (CB) for textual entailment, and Winogender schemas (WSC) for pronoun resolution.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1905.00537\">“SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems”</a>.</li>\n</ul>\n<h5 id=\"mmlu-massive-multitask-language-understanding\">MMLU (Massive Multitask Language Understanding)</h5>\n<ul>\n  <li><strong>Description:</strong> Assesses model performance across a broad range of subjects and task formats to test general knowledge.</li>\n  <li><strong>Dataset Attributes:</strong> Covers 57 tasks across subjects like humanities, STEM, and social sciences, requiring broad and specialized knowledge. The tasks vary from multiple-choice questions to open-ended questions, ensuring a comprehensive assessment of linguistic intelligence.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2009.03300\">“Measuring Massive Multitask Language Understanding”</a>.</li>\n</ul>\n<h5 id=\"mmlu-pro-massive-multitask-language-understanding-pro\">MMLU-Pro (Massive Multitask Language Understanding Pro)</h5>\n<ul>\n  <li><strong>Description:</strong> A robust and challenging dataset designed to rigorously benchmark large language models’ capabilities. With 12K complex questions across various disciplines, it enhances evaluation complexity and model robustness by increasing options from 4 to 10, making random guessing less effective. Unlike the original MMLU’s knowledge-driven questions, MMLU-Pro focuses on more difficult, reasoning-based problems, where chain-of-thought (CoT) results can be 20% higher than perplexity (PPL). This increased difficulty results in more consistent model performance, as seen with Llama-2-7B’s variance of within 1%, compared to 4-5% in the original MMLU.</li>\n  <li><strong>Dataset Attributes:</strong> 12K questions with 10 options each. Sources include Original MMLU, STEM websites, TheoremQA, and SciBench. Covers disciplines such as Math, Physics, Chemistry, Law, Engineering, Health, Psychology, Economics, Business, Biology, Philosophy, Computer Science, and History. Focus on reasoning, increased problem difficulty, and manual expert review by a panel of over ten experts.</li>\n  <li><strong>Reference:</strong> <a href=\"https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro\">Hugging Face: MMLU-Pro</a>.</li>\n</ul>\n<h5 id=\"big-bench-beyond-the-imitation-game-benchmark\">BIG-bench (Beyond the Imitation Game Benchmark)</h5>\n<ul>\n  <li><strong>Description:</strong> A comprehensive benchmark designed to evaluate a wide range of capabilities in language models, from simple tasks to complex reasoning.</li>\n  <li><strong>Dataset Attributes:</strong> Encompasses over 200 diverse tasks, including arithmetic, common-sense reasoning, language understanding, and more. It is designed to push the boundaries of current LLM capabilities by including both straightforward and highly complex tasks.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2206.04615\">“BIG-bench: A Large-Scale Evaluation of Language Models”</a>.</li>\n</ul>\n<h5 id=\"big-bench-hard\">BIG-bench Hard</h5>\n<ul>\n  <li><strong>Description:</strong> A subset of the BIG-bench benchmark focusing specifically on the most challenging tasks.</li>\n  <li><strong>Dataset Attributes:</strong> Consists of the hardest tasks from the BIG-bench suite, requiring advanced reasoning, problem-solving, and deep understanding. It aims to evaluate models’ performance on tasks that are significantly more difficult than typical benchmarks.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2206.04616\">“BIG-bench Hard: A Challenge Set for Language Models”</a>.</li>\n</ul>\n<h4 id=\"reasoning\">Reasoning</h4>\n<h5 id=\"hellaswag\">HellaSwag</h5>\n<ul>\n  <li><strong>Description:</strong> A dataset designed to evaluate common-sense reasoning through completion of context-dependent scenarios.</li>\n  <li><strong>Dataset Attributes:</strong> Challenges models to choose the most plausible continuation among four options, requiring nuanced understanding of everyday activities and scenarios. It features adversarially filtered examples to ensure difficulty and minimize data leakage from pre-training.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1905.07830\">“HellaSwag: Can a Machine Really Finish Your Sentence?”</a>.</li>\n</ul>\n<h5 id=\"winogrande\">WinoGrande</h5>\n<ul>\n  <li><strong>Description:</strong> A large-scale dataset for evaluating common-sense reasoning through Winograd schema challenges.</li>\n  <li><strong>Dataset Attributes:</strong> Includes a diverse set of sentences that require resolving ambiguous pronouns, emphasizing subtle distinctions in language understanding. The dataset is designed to address the limitations of smaller Winograd Schema datasets by providing scale and diversity.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1907.10641\">“WinoGrande: An Adversarial Winograd Schema Challenge at Scale”</a>.</li>\n</ul>\n<h5 id=\"arc-challenge-arc-c-and-arc-easy-arc-e\">ARC Challenge (ARC-c) and ARC Easy (ARC-e)</h5>\n<ul>\n  <li><strong>Description:</strong> The AI2 Reasoning Challenge (ARC) tests models on science exam questions, designed to be challenging for AI.</li>\n  <li><strong>Dataset Attributes:</strong> Comprised of grade-school science questions that demand complex reasoning and understanding, generally challenging for current AI systems. The ARC dataset is split into a challenging set (ARC-c) and an easier set (ARC-e) based on question difficulty.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1803.05457\">“Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge”</a>.</li>\n</ul>\n<h5 id=\"openbookqa-obqa\">OpenBookQA (OBQA)</h5>\n<ul>\n  <li><strong>Description:</strong> Focuses on science-based question answering that requires both retrieval of relevant facts and reasoning.</li>\n  <li><strong>Dataset Attributes:</strong> Challenges models to answer questions using both retrieved facts and reasoning, focusing on scientific knowledge. The dataset includes a small “open book” of 1,326 elementary-level science facts to aid in answering the questions.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1810.00920\">“OpenBookQA: A New Dataset for Open Book Question Answering”</a>.</li>\n</ul>\n<h5 id=\"commonsenseqa-cqa\">CommonsenseQA (CQA)</h5>\n<ul>\n  <li><strong>Description:</strong> A benchmark designed to probe models’ ability to reason about everyday knowledge.</li>\n  <li><strong>Dataset Attributes:</strong> Focuses on multiple-choice questions that require commonsense to answer, challenging the depth of models’ real-world understanding. The questions are designed to have one correct answer and four distractors, making the task non-trivial.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1811.00937\">“COMMONSENSEQA: A Question Answering Challenge Targeting Commonsense Knowledge”</a>.</li>\n</ul>\n<h5 id=\"graduate-level-google-proof-question-answering-gpqa\">Graduate-Level Google-Proof Question Answering (GPQA)</h5>\n<ul>\n  <li><strong>Description:</strong> Evaluates models’ ability to answer 448 multiple-choice questions written by domain experts in biology, physics, and chemistry.</li>\n  <li><strong>Dataset Attributes:</strong> Includes high-quality and extremely difficult questions: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are “Google-proof”).</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2311.12022\">“GPQA: A Benchmark for General Purpose Question Answering”</a>.</li>\n</ul>\n<h5 id=\"flash-fine-grained-language-agent-self-check-harness\">FLASH (Fine-grained Language Agent Self-Check Harness)</h5>\n<ul>\n  <li><strong>Description:</strong> FLASH is a benchmark designed to evaluate the self-checking abilities of large language models. It assesses how well models can reason through, verify, and correct their own outputs across diverse scenarios and task types.</li>\n  <li><strong>Dataset Attributes:</strong> Comprises 1,060 instances spanning 32 task types and 5 self-checking task variants (e.g., verification, correction, and explanation of mistakes). Tasks are derived from 20 existing datasets, and examples are filtered to ensure they contain reasoning steps with clear mistakes. FLASH emphasizes multi-step reasoning, logical flow, and the ability to detect and amend faulty outputs.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2502.01142\">“FLASH: A Fine-grained Language Agent Self-check Harness”</a>.</li>\n</ul>\n<h4 id=\"contextual-comprehension\">Contextual Comprehension</h4>\n<h5 id=\"lambada\">LAMBADA</h5>\n<ul>\n  <li><strong>Description:</strong> Focuses on predicting the last word of a passage, requiring a deep understanding of the context.</li>\n  <li><strong>Dataset Attributes:</strong> Passages where the last word requires significant contextual understanding, testing language models’ deep comprehension. The passages are drawn from novels and require broad contextual reasoning to accurately predict the final word.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1606.06031\">“The LAMBADA dataset: Word prediction requiring a broad discourse context”</a>.</li>\n</ul>\n<h5 id=\"boolq\">BoolQ</h5>\n<ul>\n  <li><strong>Description:</strong> A dataset for boolean question answering, focusing on reading comprehension.</li>\n  <li><strong>Dataset Attributes:</strong> Consists of yes/no questions based on Google search queries and their corresponding Wikipedia articles, requiring binary comprehension of text. The questions are naturally occurring and require understanding the passage to answer correctly.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1905.10044\">“BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions”</a>.</li>\n</ul>\n<h4 id=\"general-knowledge-and-skills\">General Knowledge and Skills</h4>\n<h5 id=\"triviaqa\">TriviaQA</h5>\n<ul>\n  <li><strong>Description:</strong> A widely used dataset consisting of trivia questions collected from various sources. It evaluates a model’s ability to answer open-domain questions with detailed and accurate responses. The dataset includes a mix of web-scraped and curated questions.</li>\n  <li><strong>Dataset Attributes:</strong> Contains over 650,000 question-answer pairs, including both verified and web-extracted answers, covering a broad range of general knowledge topics. The questions are accompanied by evidence documents to support answer validation.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1705.03551\">“TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension”</a>.</li>\n</ul>\n<h5 id=\"natural-questions-nq\">Natural Questions (NQ)</h5>\n<ul>\n  <li><strong>Description:</strong> Developed by Google, this benchmark consists of real questions posed by users to the Google search engine. It assesses a model’s ability to retrieve and generate accurate answers based on a comprehensive understanding of the query and relevant documents.</li>\n  <li><strong>Dataset Attributes:</strong> Includes 300,000 training examples with questions and long and short answer annotations, providing a rich resource for training and evaluating LLMs on real-world information retrieval and comprehension. The dataset focuses on long-form answers sourced from Wikipedia.</li>\n  <li><strong>Reference:</strong> <a href=\"https://research.google/pubs/pub47761/\">“Natural Questions: a Benchmark for Question Answering Research”</a>.</li>\n</ul>\n<h5 id=\"webquestions-wq\">WebQuestions (WQ)</h5>\n<ul>\n  <li><strong>Description:</strong> A dataset created to test a model’s ability to answer questions using information found on the web. The questions were obtained via the Google Suggest API, ensuring they reflect genuine user queries.</li>\n  <li><strong>Dataset Attributes:</strong> Comprises around 6,000 question-answer pairs, with answers derived from Freebase, allowing models to leverage structured knowledge bases to provide accurate responses. The dataset focuses on factual questions requiring specific, often entity-centric answers.</li>\n  <li><strong>Reference:</strong> <a href=\"https://www.aclweb.org/anthology/D13-1160/\">“WebQuestions: A Benchmark for Open-Domain Question Answering”</a>.</li>\n</ul>\n<h4 id=\"specialized-knowledge-and-skills\">Specialized Knowledge and Skills</h4>\n<h5 id=\"humaneval\">HumanEval</h5>\n<ul>\n  <li><strong>Description:</strong> Tests models on generating code snippets to solve programming tasks, evaluating coding abilities.</li>\n  <li><strong>Dataset Attributes:</strong> Programming problems requiring synthesis of function bodies, testing understanding of code logic and syntax. The dataset consists of prompts and corresponding reference solutions in Python, ensuring a clear standard for evaluation.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2107.03374\">“Evaluating Large Language Models Trained on Code”</a>.</li>\n</ul>\n<h5 id=\"physical-interaction-question-answering-piqa\">Physical Interaction Question Answering (PIQA)</h5>\n<ul>\n  <li><strong>Description:</strong> Evaluates understanding of physical properties through problem-solving scenarios.</li>\n  <li><strong>Dataset Attributes:</strong> Focuses on questions that require reasoning about everyday physical interactions, pushing models to understand and predict physical outcomes. The scenarios involve practical physical tasks and common sense, making the benchmark unique in testing physical reasoning.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1911.11641\">“PIQA: Reasoning about Physical Commonsense in Natural Language”</a>.</li>\n</ul>\n<h5 id=\"social-interaction-question-answering-siqa\">Social Interaction Question Answering (SIQA)</h5>\n<ul>\n  <li><strong>Description:</strong> Tests the ability of models to navigate social situations through multiple-choice questions.</li>\n  <li><strong>Dataset Attributes:</strong> Challenges models with scenarios involving human interactions, requiring understanding of social norms and behaviors. The questions are designed to assess social commonsense reasoning, with multiple plausible answers to evaluate nuanced understanding.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1904.09728\">“Social IQa: Commonsense Reasoning about Social Interactions”</a>.</li>\n</ul>\n<h5 id=\"graduate-level-google-proof-question-answering-gpqa-diamond\">Graduate-Level Google-Proof Question Answering (GPQA) Diamond</h5>\n<ul>\n  <li><strong>Description:</strong> A PhD-level science questions benchmark designed to evaluate a model’s capability to tackle extremely challenging, domain-specific science questions written by experts with advanced academic backgrounds.</li>\n  <li><strong>Dataset Attributes:</strong> Features high-difficulty, domain-specific multiple-choice questions in physics, chemistry, and biology. Questions emphasize reasoning, comprehension of advanced scientific concepts, and avoidance of “shortcut” heuristics.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2311.12022\">“GPQA: A Benchmark for General Purpose Question Answering”</a>.</li>\n</ul>",
    "contentMarkdown": "#### Language Understanding\n\n##### GLUE (General Language Understanding Evaluation)\n\n*   **Description:** A set of nine tasks including question answering and textual entailment, designed to gauge general language understanding.\n*   **Dataset Attributes:** Diverse text genres from web text, fiction, and non-fiction, requiring models to handle a variety of language styles and complexities. The tasks range from single-sentence tasks (e.g., CoLA for linguistic acceptability) to sentence-pair tasks (e.g., MRPC for paraphrase detection).\n*   **Reference:** [“GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding”](https://openreview.net/forum?id=rJ4km2R5t7).\n\n##### SuperGLUE\n\n*   **Description:** A more challenging version of GLUE intended to push language models to their limits.\n*   **Dataset Attributes:** Includes more complex reasoning tasks over multiple domains, emphasizing inference, logic, and common sense. Tasks include Boolean Question (BoolQ), CommitmentBank (CB) for textual entailment, and Winogender schemas (WSC) for pronoun resolution.\n*   **Reference:** [“SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems”](https://arxiv.org/abs/1905.00537).\n\n##### MMLU (Massive Multitask Language Understanding)\n\n*   **Description:** Assesses model performance across a broad range of subjects and task formats to test general knowledge.\n*   **Dataset Attributes:** Covers 57 tasks across subjects like humanities, STEM, and social sciences, requiring broad and specialized knowledge. The tasks vary from multiple-choice questions to open-ended questions, ensuring a comprehensive assessment of linguistic intelligence.\n*   **Reference:** [“Measuring Massive Multitask Language Understanding”](https://arxiv.org/abs/2009.03300).\n\n##### MMLU-Pro (Massive Multitask Language Understanding Pro)\n\n*   **Description:** A robust and challenging dataset designed to rigorously benchmark large language models’ capabilities. With 12K complex questions across various disciplines, it enhances evaluation complexity and model robustness by increasing options from 4 to 10, making random guessing less effective. Unlike the original MMLU’s knowledge-driven questions, MMLU-Pro focuses on more difficult, reasoning-based problems, where chain-of-thought (CoT) results can be 20% higher than perplexity (PPL). This increased difficulty results in more consistent model performance, as seen with Llama-2-7B’s variance of within 1%, compared to 4-5% in the original MMLU.\n*   **Dataset Attributes:** 12K questions with 10 options each. Sources include Original MMLU, STEM websites, TheoremQA, and SciBench. Covers disciplines such as Math, Physics, Chemistry, Law, Engineering, Health, Psychology, Economics, Business, Biology, Philosophy, Computer Science, and History. Focus on reasoning, increased problem difficulty, and manual expert review by a panel of over ten experts.\n*   **Reference:** [Hugging Face: MMLU-Pro](https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro).\n\n##### BIG-bench (Beyond the Imitation Game Benchmark)\n\n*   **Description:** A comprehensive benchmark designed to evaluate a wide range of capabilities in language models, from simple tasks to complex reasoning.\n*   **Dataset Attributes:** Encompasses over 200 diverse tasks, including arithmetic, common-sense reasoning, language understanding, and more. It is designed to push the boundaries of current LLM capabilities by including both straightforward and highly complex tasks.\n*   **Reference:** [“BIG-bench: A Large-Scale Evaluation of Language Models”](https://arxiv.org/abs/2206.04615).\n\n##### BIG-bench Hard\n\n*   **Description:** A subset of the BIG-bench benchmark focusing specifically on the most challenging tasks.\n*   **Dataset Attributes:** Consists of the hardest tasks from the BIG-bench suite, requiring advanced reasoning, problem-solving, and deep understanding. It aims to evaluate models’ performance on tasks that are significantly more difficult than typical benchmarks.\n*   **Reference:** [“BIG-bench Hard: A Challenge Set for Language Models”](https://arxiv.org/abs/2206.04616).\n\n#### Reasoning\n\n##### HellaSwag\n\n*   **Description:** A dataset designed to evaluate common-sense reasoning through completion of context-dependent scenarios.\n*   **Dataset Attributes:** Challenges models to choose the most plausible continuation among four options, requiring nuanced understanding of everyday activities and scenarios. It features adversarially filtered examples to ensure difficulty and minimize data leakage from pre-training.\n*   **Reference:** [“HellaSwag: Can a Machine Really Finish Your Sentence?”](https://arxiv.org/abs/1905.07830).\n\n##### WinoGrande\n\n*   **Description:** A large-scale dataset for evaluating common-sense reasoning through Winograd schema challenges.\n*   **Dataset Attributes:** Includes a diverse set of sentences that require resolving ambiguous pronouns, emphasizing subtle distinctions in language understanding. The dataset is designed to address the limitations of smaller Winograd Schema datasets by providing scale and diversity.\n*   **Reference:** [“WinoGrande: An Adversarial Winograd Schema Challenge at Scale”](https://arxiv.org/abs/1907.10641).\n\n##### ARC Challenge (ARC-c) and ARC Easy (ARC-e)\n\n*   **Description:** The AI2 Reasoning Challenge (ARC) tests models on science exam questions, designed to be challenging for AI.\n*   **Dataset Attributes:** Comprised of grade-school science questions that demand complex reasoning and understanding, generally challenging for current AI systems. The ARC dataset is split into a challenging set (ARC-c) and an easier set (ARC-e) based on question difficulty.\n*   **Reference:** [“Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge”](https://arxiv.org/abs/1803.05457).\n\n##### OpenBookQA (OBQA)\n\n*   **Description:** Focuses on science-based question answering that requires both retrieval of relevant facts and reasoning.\n*   **Dataset Attributes:** Challenges models to answer questions using both retrieved facts and reasoning, focusing on scientific knowledge. The dataset includes a small “open book” of 1,326 elementary-level science facts to aid in answering the questions.\n*   **Reference:** [“OpenBookQA: A New Dataset for Open Book Question Answering”](https://arxiv.org/abs/1810.00920).\n\n##### CommonsenseQA (CQA)\n\n*   **Description:** A benchmark designed to probe models’ ability to reason about everyday knowledge.\n*   **Dataset Attributes:** Focuses on multiple-choice questions that require commonsense to answer, challenging the depth of models’ real-world understanding. The questions are designed to have one correct answer and four distractors, making the task non-trivial.\n*   **Reference:** [“COMMONSENSEQA: A Question Answering Challenge Targeting Commonsense Knowledge”](https://arxiv.org/abs/1811.00937).\n\n##### Graduate-Level Google-Proof Question Answering (GPQA)\n\n*   **Description:** Evaluates models’ ability to answer 448 multiple-choice questions written by domain experts in biology, physics, and chemistry.\n*   **Dataset Attributes:** Includes high-quality and extremely difficult questions: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are “Google-proof”).\n*   **Reference:** [“GPQA: A Benchmark for General Purpose Question Answering”](https://arxiv.org/abs/2311.12022).\n\n##### FLASH (Fine-grained Language Agent Self-Check Harness)\n\n*   **Description:** FLASH is a benchmark designed to evaluate the self-checking abilities of large language models. It assesses how well models can reason through, verify, and correct their own outputs across diverse scenarios and task types.\n*   **Dataset Attributes:** Comprises 1,060 instances spanning 32 task types and 5 self-checking task variants (e.g., verification, correction, and explanation of mistakes). Tasks are derived from 20 existing datasets, and examples are filtered to ensure they contain reasoning steps with clear mistakes. FLASH emphasizes multi-step reasoning, logical flow, and the ability to detect and amend faulty outputs.\n*   **Reference:** [“FLASH: A Fine-grained Language Agent Self-check Harness”](https://arxiv.org/abs/2502.01142).\n\n#### Contextual Comprehension\n\n##### LAMBADA\n\n*   **Description:** Focuses on predicting the last word of a passage, requiring a deep understanding of the context.\n*   **Dataset Attributes:** Passages where the last word requires significant contextual understanding, testing language models’ deep comprehension. The passages are drawn from novels and require broad contextual reasoning to accurately predict the final word.\n*   **Reference:** [“The LAMBADA dataset: Word prediction requiring a broad discourse context”](https://arxiv.org/abs/1606.06031).\n\n##### BoolQ\n\n*   **Description:** A dataset for boolean question answering, focusing on reading comprehension.\n*   **Dataset Attributes:** Consists of yes/no questions based on Google search queries and their corresponding Wikipedia articles, requiring binary comprehension of text. The questions are naturally occurring and require understanding the passage to answer correctly.\n*   **Reference:** [“BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions”](https://arxiv.org/abs/1905.10044).\n\n#### General Knowledge and Skills\n\n##### TriviaQA\n\n*   **Description:** A widely used dataset consisting of trivia questions collected from various sources. It evaluates a model’s ability to answer open-domain questions with detailed and accurate responses. The dataset includes a mix of web-scraped and curated questions.\n*   **Dataset Attributes:** Contains over 650,000 question-answer pairs, including both verified and web-extracted answers, covering a broad range of general knowledge topics. The questions are accompanied by evidence documents to support answer validation.\n*   **Reference:** [“TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension”](https://arxiv.org/abs/1705.03551).\n\n##### Natural Questions (NQ)\n\n*   **Description:** Developed by Google, this benchmark consists of real questions posed by users to the Google search engine. It assesses a model’s ability to retrieve and generate accurate answers based on a comprehensive understanding of the query and relevant documents.\n*   **Dataset Attributes:** Includes 300,000 training examples with questions and long and short answer annotations, providing a rich resource for training and evaluating LLMs on real-world information retrieval and comprehension. The dataset focuses on long-form answers sourced from Wikipedia.\n*   **Reference:** [“Natural Questions: a Benchmark for Question Answering Research”](https://research.google/pubs/pub47761/).\n\n##### WebQuestions (WQ)\n\n*   **Description:** A dataset created to test a model’s ability to answer questions using information found on the web. The questions were obtained via the Google Suggest API, ensuring they reflect genuine user queries.\n*   **Dataset Attributes:** Comprises around 6,000 question-answer pairs, with answers derived from Freebase, allowing models to leverage structured knowledge bases to provide accurate responses. The dataset focuses on factual questions requiring specific, often entity-centric answers.\n*   **Reference:** [“WebQuestions: A Benchmark for Open-Domain Question Answering”](https://www.aclweb.org/anthology/D13-1160/).\n\n#### Specialized Knowledge and Skills\n\n##### HumanEval\n\n*   **Description:** Tests models on generating code snippets to solve programming tasks, evaluating coding abilities.\n*   **Dataset Attributes:** Programming problems requiring synthesis of function bodies, testing understanding of code logic and syntax. The dataset consists of prompts and corresponding reference solutions in Python, ensuring a clear standard for evaluation.\n*   **Reference:** [“Evaluating Large Language Models Trained on Code”](https://arxiv.org/abs/2107.03374).\n\n##### Physical Interaction Question Answering (PIQA)\n\n*   **Description:** Evaluates understanding of physical properties through problem-solving scenarios.\n*   **Dataset Attributes:** Focuses on questions that require reasoning about everyday physical interactions, pushing models to understand and predict physical outcomes. The scenarios involve practical physical tasks and common sense, making the benchmark unique in testing physical reasoning.\n*   **Reference:** [“PIQA: Reasoning about Physical Commonsense in Natural Language”](https://arxiv.org/abs/1911.11641).\n\n##### Social Interaction Question Answering (SIQA)\n\n*   **Description:** Tests the ability of models to navigate social situations through multiple-choice questions.\n*   **Dataset Attributes:** Challenges models with scenarios involving human interactions, requiring understanding of social norms and behaviors. The questions are designed to assess social commonsense reasoning, with multiple plausible answers to evaluate nuanced understanding.\n*   **Reference:** [“Social IQa: Commonsense Reasoning about Social Interactions”](https://arxiv.org/abs/1904.09728).\n\n##### Graduate-Level Google-Proof Question Answering (GPQA) Diamond\n\n*   **Description:** A PhD-level science questions benchmark designed to evaluate a model’s capability to tackle extremely challenging, domain-specific science questions written by experts with advanced academic backgrounds.\n*   **Dataset Attributes:** Features high-difficulty, domain-specific multiple-choice questions in physics, chemistry, and biology. Questions emphasize reasoning, comprehension of advanced scientific concepts, and avoidance of “shortcut” heuristics.\n*   **Reference:** [“GPQA: A Benchmark for General Purpose Question Answering”](https://arxiv.org/abs/2311.12022).",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 9,
    "tags": [
      "offlineonline evaluation",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 1699,
      "contentLength": 16756
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#general-benchmarks",
    "scrapedAt": "2025-12-28T11:55:20.779Z"
  },
  {
    "id": "ai-benchmarks-text-based-agents-2",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Large Language Models (LLMs)",
    "title": "Text-Based Agents",
    "subtitle": "Large Language Models (LLMs)",
    "contentHtml": "<ul>\n  <li>LLM-based agents are increasingly being developed for autonomous tasks such as web navigation, tool use, and real-world problem-solving. However, their evaluation requires specialized benchmarks that assess not only static model performance but also interactive capabilities, adaptability, and long-term decision-making.</li>\n  <li>These benchmarks focus on evaluating LLM agents that primarily operate using natural language, solving structured reasoning tasks, decision-making problems, and text-based simulations.</li>\n</ul>\n<h4 id=\"agentbench\">AgentBench</h4>\n<ul>\n  <li><strong>Description:</strong> AgentBench is a large-scale evaluation framework for LLM agents engaged in multi-agent collaboration, strategic planning, and decision-making tasks.</li>\n  <li><strong>Dataset Attributes:</strong> Covers a wide spectrum of agentic tasks, from negotiation and coordination to complex problem-solving.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2310.15222\">“AgentBench: A Benchmark for Autonomous LLM Agents”</a></li>\n</ul>\n<h4 id=\"clembench\">ClemBench</h4>\n<ul>\n  <li><strong>Description:</strong> ClemBench benchmarks LLM agents in structured dialogue, task automation, and contextual decision-making. It is particularly focused on chatbot performance in real-world scenarios.</li>\n  <li><strong>Dataset Attributes:</strong> Evaluates multi-turn reasoning, memory retention, and instruction-following in conversational AI.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2311.04389\">“ClemBench: A Benchmark for Real-World Task-Oriented LLM Agents”</a></li>\n</ul>\n<h4 id=\"toolbench\">ToolBench</h4>\n<ul>\n  <li><strong>Description:</strong> ToolBench measures the effectiveness of LLM agents in using external tools and APIs for task completion. It assesses agents’ ability to interface with databases, search engines, and computational tools.</li>\n  <li><strong>Dataset Attributes:</strong> Tests multi-step reasoning, tool integration, and adaptability in utilizing external resources.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2310.01719\">“ToolBench: Evaluating LLM Agents for Tool-Usage”</a></li>\n</ul>\n<h4 id=\"gentbench\">GentBench</h4>\n<ul>\n  <li><strong>Description:</strong> GentBench evaluates LLM agents on generative reasoning, structured problem-solving, and long-form task execution.</li>\n  <li><strong>Dataset Attributes:</strong> Tests creative problem-solving, planning, and structured information synthesis.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2312.00978\">“GentBench: Benchmarking Generative Agents in Long-Form Tasks”</a></li>\n</ul>\n<h4 id=\"swe-bench\">SWE-Bench</h4>\n<ul>\n  <li><strong>Description:</strong> SWE-Bench is designed to evaluate LLM-based agents in software engineering tasks, focusing on debugging, code comprehension, and automated pull request generation.</li>\n  <li><strong>Dataset Attributes:</strong> Uses real-world GitHub issues to benchmark AI capabilities in handling software maintenance tasks with automated correctness checks.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2310.03667\">“SWE-Bench: Evaluating Large Language Models for Software Engineering”</a></li>\n</ul>\n<h4 id=\"τ-bench\">Τ-bench</h4>\n<ul>\n  <li><strong>Description:</strong> τ-bench (Tool-Agent-User Interaction Benchmark) evaluates agents’ ability to engage in realistic, policy-constrained, multi-turn conversations with users and programmatic tools. It emphasizes domain-specific rule-following, database reasoning, and consistency under conversational stochasticity.</li>\n  <li><strong>Dataset Attributes:</strong> Includes two domains—τ-retail and τ-airline—with simulated human users, domain-specific APIs, and policy documents. Evaluates agent reliability via a novel <code class=\"language-plaintext highlighter-rouge\">pass^k</code> metric capturing success across multiple interaction trials.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2406.12045\">“τ-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains”</a></li>\n</ul>",
    "contentMarkdown": "*   LLM-based agents are increasingly being developed for autonomous tasks such as web navigation, tool use, and real-world problem-solving. However, their evaluation requires specialized benchmarks that assess not only static model performance but also interactive capabilities, adaptability, and long-term decision-making.\n*   These benchmarks focus on evaluating LLM agents that primarily operate using natural language, solving structured reasoning tasks, decision-making problems, and text-based simulations.\n\n#### AgentBench\n\n*   **Description:** AgentBench is a large-scale evaluation framework for LLM agents engaged in multi-agent collaboration, strategic planning, and decision-making tasks.\n*   **Dataset Attributes:** Covers a wide spectrum of agentic tasks, from negotiation and coordination to complex problem-solving.\n*   **Reference:** [“AgentBench: A Benchmark for Autonomous LLM Agents”](https://arxiv.org/abs/2310.15222)\n\n#### ClemBench\n\n*   **Description:** ClemBench benchmarks LLM agents in structured dialogue, task automation, and contextual decision-making. It is particularly focused on chatbot performance in real-world scenarios.\n*   **Dataset Attributes:** Evaluates multi-turn reasoning, memory retention, and instruction-following in conversational AI.\n*   **Reference:** [“ClemBench: A Benchmark for Real-World Task-Oriented LLM Agents”](https://arxiv.org/abs/2311.04389)\n\n#### ToolBench\n\n*   **Description:** ToolBench measures the effectiveness of LLM agents in using external tools and APIs for task completion. It assesses agents’ ability to interface with databases, search engines, and computational tools.\n*   **Dataset Attributes:** Tests multi-step reasoning, tool integration, and adaptability in utilizing external resources.\n*   **Reference:** [“ToolBench: Evaluating LLM Agents for Tool-Usage”](https://arxiv.org/abs/2310.01719)\n\n#### GentBench\n\n*   **Description:** GentBench evaluates LLM agents on generative reasoning, structured problem-solving, and long-form task execution.\n*   **Dataset Attributes:** Tests creative problem-solving, planning, and structured information synthesis.\n*   **Reference:** [“GentBench: Benchmarking Generative Agents in Long-Form Tasks”](https://arxiv.org/abs/2312.00978)\n\n#### SWE-Bench\n\n*   **Description:** SWE-Bench is designed to evaluate LLM-based agents in software engineering tasks, focusing on debugging, code comprehension, and automated pull request generation.\n*   **Dataset Attributes:** Uses real-world GitHub issues to benchmark AI capabilities in handling software maintenance tasks with automated correctness checks.\n*   **Reference:** [“SWE-Bench: Evaluating Large Language Models for Software Engineering”](https://arxiv.org/abs/2310.03667)\n\n#### Τ-bench\n\n*   **Description:** τ-bench (Tool-Agent-User Interaction Benchmark) evaluates agents’ ability to engage in realistic, policy-constrained, multi-turn conversations with users and programmatic tools. It emphasizes domain-specific rule-following, database reasoning, and consistency under conversational stochasticity.\n*   **Dataset Attributes:** Includes two domains—τ-retail and τ-airline—with simulated human users, domain-specific APIs, and policy documents. Evaluates agent reliability via a novel `pass^k` metric capturing success across multiple interaction trials.\n*   **Reference:** [“τ-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains”](https://arxiv.org/abs/2406.12045)",
    "order": 2,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "offlineonline evaluation",
      "llm"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 382,
      "contentLength": 4132
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#text-based-agents",
    "scrapedAt": "2025-12-28T11:55:20.779Z"
  },
  {
    "id": "ai-benchmarks-retrieval-augmented-generation-rag-benchmarks-3",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Large Language Models (LLMs)",
    "title": "Retrieval-Augmented Generation (RAG) Benchmarks",
    "subtitle": "Large Language Models (LLMs)",
    "contentHtml": "<h4 id=\"retrieval-benchmarks\">Retrieval Benchmarks</h4>\n<h5 id=\"beir-benchmarking-information-retrieval\">BEIR (Benchmarking Information Retrieval)</h5>\n<ul>\n  <li><strong>Description:</strong> A suite of tasks that evaluates information retrieval across diverse domains, including passage retrieval and question answering.</li>\n  <li><strong>Dataset Attributes:</strong> Covers tasks like fact retrieval, argument retrieval, and entity linking, featuring datasets like TREC-COVID and FiQA.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2104.08663\">BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models</a>.</li>\n</ul>\n<h5 id=\"ms-marco\">MS MARCO</h5>\n<ul>\n  <li><strong>Description:</strong> Focused on large-scale passage retrieval from web documents.</li>\n  <li><strong>Dataset Attributes:</strong> Real-world queries paired with relevant passages, allowing evaluation of retrieval and ranking at scale.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1611.09268\">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</a>.</li>\n</ul>\n<h5 id=\"kilt\">KILT</h5>\n<ul>\n  <li><strong>Description:</strong> A unified benchmark for knowledge-intensive tasks combining retrieval and generation tasks like fact-checking and open-domain question answering.</li>\n  <li><strong>Dataset Attributes:</strong> Integrates datasets like FEVER and TriviaQA, assessing models’ ability to retrieve accurate information and generate coherent responses.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2009.02252\">KILT: A Benchmark for Knowledge Intensive Language Tasks</a>.</li>\n</ul>\n<h4 id=\"generation-benchmarks\">Generation Benchmarks</h4>\n<h5 id=\"rag-qa-arena\">RAG-QA Arena</h5>\n<ul>\n  <li><strong>Description:</strong> Evaluates models’ ability to integrate retrieved knowledge into generation for robust and accurate question answering.</li>\n  <li><strong>Dataset Attributes:</strong> Focuses on domain robustness and long-form question answering, combining retrieval and generation.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2407.13998\">RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering</a>.</li>\n</ul>\n<h5 id=\"bird-sql\">BIRD-SQL</h5>\n<ul>\n  <li><strong>Description:</strong> Specializes in structured query generation for database interfaces.</li>\n  <li><strong>Dataset Attributes:</strong> Tests retrieval and text-to-SQL generation tasks with complex structured data.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2305.03111\">Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs</a>.</li>\n</ul>\n<h5 id=\"eli5-explain-like-im-5\">ELI5 (Explain Like I’m 5)</h5>\n<ul>\n  <li><strong>Description:</strong> Focused on retrieval-augmented, long-form generation for complex, user-submitted questions.</li>\n  <li><strong>Dataset Attributes:</strong> Challenges models to combine multi-step retrieval with detailed and comprehensible explanations.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1907.09190\">ELI5: Long Form Question Answering</a>.</li>\n</ul>",
    "contentMarkdown": "#### Retrieval Benchmarks\n\n##### BEIR (Benchmarking Information Retrieval)\n\n*   **Description:** A suite of tasks that evaluates information retrieval across diverse domains, including passage retrieval and question answering.\n*   **Dataset Attributes:** Covers tasks like fact retrieval, argument retrieval, and entity linking, featuring datasets like TREC-COVID and FiQA.\n*   **Reference:** [BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models](https://arxiv.org/abs/2104.08663).\n\n##### MS MARCO\n\n*   **Description:** Focused on large-scale passage retrieval from web documents.\n*   **Dataset Attributes:** Real-world queries paired with relevant passages, allowing evaluation of retrieval and ranking at scale.\n*   **Reference:** [MS MARCO: A Human Generated MAchine Reading COmprehension Dataset](https://arxiv.org/abs/1611.09268).\n\n##### KILT\n\n*   **Description:** A unified benchmark for knowledge-intensive tasks combining retrieval and generation tasks like fact-checking and open-domain question answering.\n*   **Dataset Attributes:** Integrates datasets like FEVER and TriviaQA, assessing models’ ability to retrieve accurate information and generate coherent responses.\n*   **Reference:** [KILT: A Benchmark for Knowledge Intensive Language Tasks](https://arxiv.org/abs/2009.02252).\n\n#### Generation Benchmarks\n\n##### RAG-QA Arena\n\n*   **Description:** Evaluates models’ ability to integrate retrieved knowledge into generation for robust and accurate question answering.\n*   **Dataset Attributes:** Focuses on domain robustness and long-form question answering, combining retrieval and generation.\n*   **Reference:** [RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering](https://arxiv.org/abs/2407.13998).\n\n##### BIRD-SQL\n\n*   **Description:** Specializes in structured query generation for database interfaces.\n*   **Dataset Attributes:** Tests retrieval and text-to-SQL generation tasks with complex structured data.\n*   **Reference:** [Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs](https://arxiv.org/abs/2305.03111).\n\n##### ELI5 (Explain Like I’m 5)\n\n*   **Description:** Focused on retrieval-augmented, long-form generation for complex, user-submitted questions.\n*   **Dataset Attributes:** Challenges models to combine multi-step retrieval with detailed and comprehensible explanations.\n*   **Reference:** [ELI5: Long Form Question Answering](https://arxiv.org/abs/1907.09190).",
    "order": 3,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "offlineonline evaluation",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 283,
      "contentLength": 3223
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#retrieval-augmented-generation-(rag)-benchmarks",
    "scrapedAt": "2025-12-28T11:55:20.779Z"
  },
  {
    "id": "ai-benchmarks-long-context-understanding-4",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Large Language Models (LLMs)",
    "title": "Long-Context Understanding",
    "subtitle": "Large Language Models (LLMs)",
    "contentHtml": "<h4 id=\"long-context-benchmarks\">Long-Context Benchmarks</h4>\n<h5 id=\"longbench\">LongBench</h5>\n<ul>\n  <li><strong>Description:</strong> Designed to assess models’ ability to manage and reason over extended text sequences.</li>\n  <li><strong>Dataset Attributes:</strong> Includes tasks such as long-form QA, summarization, and multi-document analysis, requiring comprehension of context over 16K tokens or more.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2308.14508\">LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding</a>.</li>\n</ul>\n<h5 id=\"ruler\">RULER</h5>\n<ul>\n  <li><strong>Description:</strong> Tests effective use of extended contexts through tasks like long-form QA and document-level reasoning.</li>\n  <li><strong>Dataset Attributes:</strong> Incorporates both synthetic and natural datasets, simulating extreme long-context scenarios.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2404.06654\">RULER: What’s the Real Context Size of Your Long-Context Language Models?</a>.</li>\n</ul>\n<h5 id=\"booksum\">BookSum</h5>\n<ul>\n  <li><strong>Description:</strong> Focuses on summarizing extended text such as full books or long documents.</li>\n  <li><strong>Dataset Attributes:</strong> Includes rich long-form narrative datasets, challenging models to condense large spans of information.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2105.08209\">BookSum: A Collection of Datasets for Long-form Narrative Summarization</a>.</li>\n</ul>\n<h4 id=\"narrative-comprehension\">Narrative Comprehension</h4>\n<h5 id=\"narrativeqa\">NarrativeQA</h5>\n<ul>\n  <li><strong>Description:</strong> Evaluates deep understanding of story arcs and long-form narratives through question answering.</li>\n  <li><strong>Dataset Attributes:</strong> Pairs stories with question-answer pairs, emphasizing multi-turn and long-context comprehension.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1712.07040\">The NarrativeQA Reading Comprehension Challenge</a>.</li>\n</ul>\n<h5 id=\"scrolls\">SCROLLS</h5>\n<ul>\n  <li><strong>Description:</strong> Focuses on evaluating long-range reasoning and contextual comprehension across multi-document narratives.</li>\n  <li><strong>Dataset Attributes:</strong> Combines datasets such as WikiSum and GovReport, aiming to assess multi-step reasoning.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2201.03533\">SCROLLS: Standardized CompaRison Over Long Language Sequences</a>.</li>\n</ul>\n<h5 id=\"summscreen\">SummScreen</h5>\n<ul>\n  <li><strong>Description:</strong> A specialized benchmark for summarizing long-form dialogues from TV series and movies.</li>\n  <li><strong>Dataset Attributes:</strong> Features multi-character dialogue with contextual dependencies, requiring models to understand both narrative flow and character-specific contexts.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2104.07091\">SummScreen: A Dataset for Abstractive Dialogue Summarization</a>.</li>\n</ul>",
    "contentMarkdown": "#### Long-Context Benchmarks\n\n##### LongBench\n\n*   **Description:** Designed to assess models’ ability to manage and reason over extended text sequences.\n*   **Dataset Attributes:** Includes tasks such as long-form QA, summarization, and multi-document analysis, requiring comprehension of context over 16K tokens or more.\n*   **Reference:** [LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding](https://arxiv.org/abs/2308.14508).\n\n##### RULER\n\n*   **Description:** Tests effective use of extended contexts through tasks like long-form QA and document-level reasoning.\n*   **Dataset Attributes:** Incorporates both synthetic and natural datasets, simulating extreme long-context scenarios.\n*   **Reference:** [RULER: What’s the Real Context Size of Your Long-Context Language Models?](https://arxiv.org/abs/2404.06654).\n\n##### BookSum\n\n*   **Description:** Focuses on summarizing extended text such as full books or long documents.\n*   **Dataset Attributes:** Includes rich long-form narrative datasets, challenging models to condense large spans of information.\n*   **Reference:** [BookSum: A Collection of Datasets for Long-form Narrative Summarization](https://arxiv.org/abs/2105.08209).\n\n#### Narrative Comprehension\n\n##### NarrativeQA\n\n*   **Description:** Evaluates deep understanding of story arcs and long-form narratives through question answering.\n*   **Dataset Attributes:** Pairs stories with question-answer pairs, emphasizing multi-turn and long-context comprehension.\n*   **Reference:** [The NarrativeQA Reading Comprehension Challenge](https://arxiv.org/abs/1712.07040).\n\n##### SCROLLS\n\n*   **Description:** Focuses on evaluating long-range reasoning and contextual comprehension across multi-document narratives.\n*   **Dataset Attributes:** Combines datasets such as WikiSum and GovReport, aiming to assess multi-step reasoning.\n*   **Reference:** [SCROLLS: Standardized CompaRison Over Long Language Sequences](https://arxiv.org/abs/2201.03533).\n\n##### SummScreen\n\n*   **Description:** A specialized benchmark for summarizing long-form dialogues from TV series and movies.\n*   **Dataset Attributes:** Features multi-character dialogue with contextual dependencies, requiring models to understand both narrative flow and character-specific contexts.\n*   **Reference:** [SummScreen: A Dataset for Abstractive Dialogue Summarization](https://arxiv.org/abs/2104.07091).",
    "order": 4,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "offlineonline evaluation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 262,
      "contentLength": 3054
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#long-context-understanding",
    "scrapedAt": "2025-12-28T11:55:20.779Z"
  },
  {
    "id": "ai-benchmarks-mathematical-and-scientific-reasoning-5",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Large Language Models (LLMs)",
    "title": "Mathematical and Scientific Reasoning",
    "subtitle": "Large Language Models (LLMs)",
    "contentHtml": "<h5 id=\"american-invitational-mathematics-examination-aime-2024\">American Invitational Mathematics Examination (AIME) 2024</h5>\n<ul>\n  <li><strong>Description:</strong> A math competition benchmark for evaluating the mathematical reasoning capabilities of models in solving problems inspired by the AIME.</li>\n  <li><strong>Dataset Attributes:</strong> Contains challenging mathematical problems that require advanced reasoning, creative problem-solving, and a deep understanding of high school and pre-college mathematics. Problems are designed to test models’ abilities to handle intricate multi-step solutions.</li>\n  <li><strong>Reference:</strong> <a href=\"https://artofproblemsolving.com/wiki/index.php/2024_AIME_I\">Art of Problem Solving 2024 AIME I</a>.</li>\n</ul>\n<h5 id=\"math\">MATH</h5>\n<ul>\n  <li><strong>Description:</strong> A comprehensive set of mathematical problems designed to challenge models on various levels of mathematics.</li>\n  <li><strong>Dataset Attributes:</strong> Contains complex, multi-step mathematical problems from various branches of mathematics, requiring advanced reasoning and problem-solving skills. Problems range from algebra and calculus to number theory and combinatorics, emphasizing detailed solutions and proofs.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2103.03874\">Measuring Mathematical Problem Solving With the MATH Dataset</a>; <a href=\"https://github.com/hendrycks/math\">Code</a>.</li>\n</ul>\n<h5 id=\"math-500\">MATH-500</h5>\n<ul>\n  <li><strong>Description:</strong> A subset of 500 problems from the MATH benchmark that OpenAI created in their <a href=\"https://arxiv.org/abs/2305.20050\">Let’s Verify Step by Step</a> paper.</li>\n  <li><strong>Dataset Attributes:</strong> Subset of <a href=\"https://github.com/openai/prm800k/tree/main?tab=readme-ov-file#math-splits\">PRM800K</a>, a process supervision dataset containing 800,000 step-level correctness labels for model-generated solutions to problems from the MATH dataset. As stated above, MATH contains complex, multi-step mathematical problems from various branches of mathematics, requiring advanced reasoning and problem-solving skills. Problems range from algebra and calculus to number theory and combinatorics, emphasizing detailed solutions and proofs.</li>\n  <li><strong>Reference:</strong> <a href=\"https://github.com/openai/prm800k/tree/main?tab=readme-ov-file#math-splits\">Code</a></li>\n</ul>\n<h5 id=\"gsm8k-grade-school-math-8k\">GSM8K (Grade School Math 8K)</h5>\n<ul>\n  <li><strong>Description:</strong> A benchmark for evaluating the reasoning capabilities of models through grade school level math problems.</li>\n  <li><strong>Dataset Attributes:</strong> Consists of arithmetic and word problems typical of elementary school mathematics, emphasizing logical and numerical reasoning. The dataset aims to test foundational math skills and the ability to apply these skills to solve straightforward problems.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2405.00332\">Can Language Models do Grade School Math?</a>.</li>\n</ul>\n<h5 id=\"metamathqa\">MetaMathQA</h5>\n<ul>\n  <li><strong>Description:</strong> A diverse collection of mathematical reasoning questions that aim to evaluate and improve the problem-solving capabilities of models.</li>\n  <li><strong>Dataset Attributes:</strong> Features a wide range of question types, from elementary to advanced mathematics, emphasizing not only the final answer but also the reasoning process leading to it. The dataset includes step-by-step solutions to foster reasoning and understanding in mathematical problem-solving.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2309.12284\">MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models</a>.</li>\n</ul>\n<h5 id=\"mathvista\">MathVista</h5>\n<ul>\n  <li><strong>Description:</strong> An advanced dataset designed to evaluate the mathematical reasoning and problem-solving capabilities of large language models.</li>\n  <li><strong>Dataset Attributes:</strong> Contains a wide variety of mathematical problems, from elementary arithmetic to complex calculus and linear algebra. Emphasizes not only the final answer but the step-by-step reasoning process required to arrive at the solution. The dataset is curated to challenge models with both straightforward calculations and intricate proofs.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2406.XXXXX\">MathVista: A Comprehensive Benchmark for Mathematical Reasoning in Language Models</a>.</li>\n</ul>",
    "contentMarkdown": "##### American Invitational Mathematics Examination (AIME) 2024\n\n*   **Description:** A math competition benchmark for evaluating the mathematical reasoning capabilities of models in solving problems inspired by the AIME.\n*   **Dataset Attributes:** Contains challenging mathematical problems that require advanced reasoning, creative problem-solving, and a deep understanding of high school and pre-college mathematics. Problems are designed to test models’ abilities to handle intricate multi-step solutions.\n*   **Reference:** [Art of Problem Solving 2024 AIME I](https://artofproblemsolving.com/wiki/index.php/2024_AIME_I).\n\n##### MATH\n\n*   **Description:** A comprehensive set of mathematical problems designed to challenge models on various levels of mathematics.\n*   **Dataset Attributes:** Contains complex, multi-step mathematical problems from various branches of mathematics, requiring advanced reasoning and problem-solving skills. Problems range from algebra and calculus to number theory and combinatorics, emphasizing detailed solutions and proofs.\n*   **Reference:** [Measuring Mathematical Problem Solving With the MATH Dataset](https://arxiv.org/abs/2103.03874); [Code](https://github.com/hendrycks/math).\n\n##### MATH-500\n\n*   **Description:** A subset of 500 problems from the MATH benchmark that OpenAI created in their [Let’s Verify Step by Step](https://arxiv.org/abs/2305.20050) paper.\n*   **Dataset Attributes:** Subset of [PRM800K](https://github.com/openai/prm800k/tree/main?tab=readme-ov-file#math-splits), a process supervision dataset containing 800,000 step-level correctness labels for model-generated solutions to problems from the MATH dataset. As stated above, MATH contains complex, multi-step mathematical problems from various branches of mathematics, requiring advanced reasoning and problem-solving skills. Problems range from algebra and calculus to number theory and combinatorics, emphasizing detailed solutions and proofs.\n*   **Reference:** [Code](https://github.com/openai/prm800k/tree/main?tab=readme-ov-file#math-splits)\n\n##### GSM8K (Grade School Math 8K)\n\n*   **Description:** A benchmark for evaluating the reasoning capabilities of models through grade school level math problems.\n*   **Dataset Attributes:** Consists of arithmetic and word problems typical of elementary school mathematics, emphasizing logical and numerical reasoning. The dataset aims to test foundational math skills and the ability to apply these skills to solve straightforward problems.\n*   **Reference:** [Can Language Models do Grade School Math?](https://arxiv.org/abs/2405.00332).\n\n##### MetaMathQA\n\n*   **Description:** A diverse collection of mathematical reasoning questions that aim to evaluate and improve the problem-solving capabilities of models.\n*   **Dataset Attributes:** Features a wide range of question types, from elementary to advanced mathematics, emphasizing not only the final answer but also the reasoning process leading to it. The dataset includes step-by-step solutions to foster reasoning and understanding in mathematical problem-solving.\n*   **Reference:** [MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models](https://arxiv.org/abs/2309.12284).\n\n##### MathVista\n\n*   **Description:** An advanced dataset designed to evaluate the mathematical reasoning and problem-solving capabilities of large language models.\n*   **Dataset Attributes:** Contains a wide variety of mathematical problems, from elementary arithmetic to complex calculus and linear algebra. Emphasizes not only the final answer but the step-by-step reasoning process required to arrive at the solution. The dataset is curated to challenge models with both straightforward calculations and intricate proofs.\n*   **Reference:** [MathVista: A Comprehensive Benchmark for Mathematical Reasoning in Language Models](https://arxiv.org/abs/2406.XXXXX).",
    "order": 5,
    "orderInChapter": 5,
    "difficulty": 2,
    "estimatedMinutes": 3,
    "tags": [
      "offlineonline evaluation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 452,
      "contentLength": 4571
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#mathematical-and-scientific-reasoning",
    "scrapedAt": "2025-12-28T11:55:20.779Z"
  },
  {
    "id": "ai-benchmarks-instruction-tuning-and-evaluation-6",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Large Language Models (LLMs)",
    "title": "Instruction Tuning and Evaluation",
    "subtitle": "Large Language Models (LLMs)",
    "contentHtml": "<h4 id=\"ifeval\">IFEval</h4>\n<ul>\n  <li><strong>Description:</strong> Focuses on evaluating instruction-following models using a wide range of real-world and simulated tasks.</li>\n  <li><strong>Dataset Attributes:</strong> Comprises diverse tasks that require models to interpret and execute instructions accurately, ranging from straightforward to complex scenarios. The tasks include data manipulation, information extraction, and user interaction simulations, providing a comprehensive assessment of the model’s instruction-following capabilities.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2401.00001\">“IFEval: A Benchmark for Instruction-Following Evaluation”</a>.</li>\n</ul>\n<h4 id=\"alpacaeval\">AlpacaEval</h4>\n<ul>\n  <li><strong>Description:</strong> Designed to evaluate instruction-following models using a diverse set of tasks and prompts.</li>\n  <li><strong>Dataset Attributes:</strong> Contains a variety of instruction types ranging from simple tasks to complex multi-step instructions. It emphasizes the ability of models to follow and execute detailed instructions accurately. The dataset includes tasks like translation, summarization, and question answering.</li>\n  <li><strong>Reference:</strong> <a href=\"https://github.com/tatsu-lab/alpaca_eval\">“AlpacaEval: A Comprehensive Evaluation Suite for Instruction-Following Models”</a>.</li>\n</ul>\n<h4 id=\"arena-hard\">Arena Hard</h4>\n<ul>\n  <li><strong>Description:</strong> Designed to rigorously test instruction-following models on challenging and complex tasks.</li>\n  <li><strong>Dataset Attributes:</strong> Features high-difficulty tasks that require nuanced understanding and execution of instructions. The tasks span various domains, including intricate problem-solving, advanced reasoning, and detailed multi-step processes, providing a thorough evaluation of the model’s capabilities.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2401.00002\">“Arena Hard: Benchmarking High-Difficulty Instruction-Following Tasks”</a>.</li>\n</ul>\n<h4 id=\"flan\">Flan</h4>\n<ul>\n  <li><strong>Description:</strong> Focuses on evaluating models trained with diverse instruction sets to assess their generalization capabilities.</li>\n  <li><strong>Dataset Attributes:</strong> Includes a wide array of tasks derived from existing benchmarks and real-world applications. The tasks span multiple domains, requiring models to adapt to various instruction styles and content areas. The benchmark is used to evaluate models trained on instruction tuning with the Flan collection.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2210.11416\">“Scaling Instruction-Finetuned Language Models”</a>.</li>\n</ul>\n<h4 id=\"self-instruct\">Self-Instruct</h4>\n<ul>\n  <li><strong>Description:</strong> Evaluates models using a method where the model generates its own instructions and responses, allowing for iterative self-improvement.</li>\n  <li><strong>Dataset Attributes:</strong> Contains tasks generated by the model itself, covering diverse areas such as common-sense reasoning, factual recall, and open-ended tasks. The benchmark tests the model’s ability to refine its instruction-following capabilities through self-generated data.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2212.10560\">“Self-Instruct: Aligning Language Models with Self-Generated Instructions”</a>.</li>\n</ul>\n<h4 id=\"dolly\">Dolly</h4>\n<ul>\n  <li><strong>Description:</strong> Evaluates models based on tasks and instructions derived from real-world use cases, emphasizing practical utility.</li>\n  <li><strong>Dataset Attributes:</strong> Includes instructions collected from enterprise use cases, focusing on practical and actionable tasks. The dataset aims to benchmark models on their ability to perform useful tasks in business and technical environments.</li>\n  <li><strong>Reference:</strong> <a href=\"https://www.databricks.com/blog/2023/03/24/dolly-v2-open-sourcing-6-billion-parameter-model-fine-tuned-gpt-3-5-like-instruction-dataset.html\">“Dolly: Open Sourcing Instruction-Following LLMs”</a>.</li>\n</ul>\n<h4 id=\"openai-codex-evaluations\">OpenAI Codex Evaluations</h4>\n<ul>\n  <li><strong>Description:</strong> A benchmark for evaluating instruction-following capabilities specifically in the context of code generation and programming tasks.</li>\n  <li><strong>Dataset Attributes:</strong> Contains programming challenges that require models to generate code based on natural language instructions. It evaluates the model’s ability to understand and execute programming-related instructions accurately.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2107.03374\">“Evaluating Large Language Models Trained on Code”</a>.</li>\n</ul>\n<h4 id=\"instructgpt-benchmarks\">InstructGPT Benchmarks</h4>\n<ul>\n  <li><strong>Description:</strong> Used to evaluate the performance of InstructGPT models, focusing on the ability to follow detailed and complex instructions.</li>\n  <li><strong>Dataset Attributes:</strong> Encompasses a variety of tasks including creative writing, problem-solving, and detailed explanations. The benchmark aims to assess the alignment of model outputs with user-provided instructions, ensuring the model’s responses are accurate and contextually appropriate.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2203.02155\">“Training language models to follow instructions with human feedback”</a>.</li>\n</ul>\n<h4 id=\"biggen-bench-big-generation-benchmark\">BigGen Bench (Big Generation Benchmark)</h4>\n<ul>\n  <li><strong>Description:</strong> BigGen Bench is a comprehensive generation benchmark that evaluates large language models across nine core capabilities using instance-specific evaluation criteria. It moves beyond general assessments of helpfulness to enable nuanced, fine-grained evaluation more aligned with human judgment.</li>\n  <li><strong>Dataset Attributes:</strong> Encompasses 77 tasks and 765 instances across capabilities such as reasoning, planning, tool usage, instruction following, and more. Each instance includes detailed prompts, reference answers, and a 5-point Likert-scale rubric designed to assess specific performance attributes. Evaluations are performed using both human raters and large language model evaluators. Tasks test abilities like hypothesis generation, code revision, multi-agent planning, and cultural awareness.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2406.05761\">“BigGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models”</a>.</li>\n</ul>",
    "contentMarkdown": "#### IFEval\n\n*   **Description:** Focuses on evaluating instruction-following models using a wide range of real-world and simulated tasks.\n*   **Dataset Attributes:** Comprises diverse tasks that require models to interpret and execute instructions accurately, ranging from straightforward to complex scenarios. The tasks include data manipulation, information extraction, and user interaction simulations, providing a comprehensive assessment of the model’s instruction-following capabilities.\n*   **Reference:** [“IFEval: A Benchmark for Instruction-Following Evaluation”](https://arxiv.org/abs/2401.00001).\n\n#### AlpacaEval\n\n*   **Description:** Designed to evaluate instruction-following models using a diverse set of tasks and prompts.\n*   **Dataset Attributes:** Contains a variety of instruction types ranging from simple tasks to complex multi-step instructions. It emphasizes the ability of models to follow and execute detailed instructions accurately. The dataset includes tasks like translation, summarization, and question answering.\n*   **Reference:** [“AlpacaEval: A Comprehensive Evaluation Suite for Instruction-Following Models”](https://github.com/tatsu-lab/alpaca_eval).\n\n#### Arena Hard\n\n*   **Description:** Designed to rigorously test instruction-following models on challenging and complex tasks.\n*   **Dataset Attributes:** Features high-difficulty tasks that require nuanced understanding and execution of instructions. The tasks span various domains, including intricate problem-solving, advanced reasoning, and detailed multi-step processes, providing a thorough evaluation of the model’s capabilities.\n*   **Reference:** [“Arena Hard: Benchmarking High-Difficulty Instruction-Following Tasks”](https://arxiv.org/abs/2401.00002).\n\n#### Flan\n\n*   **Description:** Focuses on evaluating models trained with diverse instruction sets to assess their generalization capabilities.\n*   **Dataset Attributes:** Includes a wide array of tasks derived from existing benchmarks and real-world applications. The tasks span multiple domains, requiring models to adapt to various instruction styles and content areas. The benchmark is used to evaluate models trained on instruction tuning with the Flan collection.\n*   **Reference:** [“Scaling Instruction-Finetuned Language Models”](https://arxiv.org/abs/2210.11416).\n\n#### Self-Instruct\n\n*   **Description:** Evaluates models using a method where the model generates its own instructions and responses, allowing for iterative self-improvement.\n*   **Dataset Attributes:** Contains tasks generated by the model itself, covering diverse areas such as common-sense reasoning, factual recall, and open-ended tasks. The benchmark tests the model’s ability to refine its instruction-following capabilities through self-generated data.\n*   **Reference:** [“Self-Instruct: Aligning Language Models with Self-Generated Instructions”](https://arxiv.org/abs/2212.10560).\n\n#### Dolly\n\n*   **Description:** Evaluates models based on tasks and instructions derived from real-world use cases, emphasizing practical utility.\n*   **Dataset Attributes:** Includes instructions collected from enterprise use cases, focusing on practical and actionable tasks. The dataset aims to benchmark models on their ability to perform useful tasks in business and technical environments.\n*   **Reference:** [“Dolly: Open Sourcing Instruction-Following LLMs”](https://www.databricks.com/blog/2023/03/24/dolly-v2-open-sourcing-6-billion-parameter-model-fine-tuned-gpt-3-5-like-instruction-dataset.html).\n\n#### OpenAI Codex Evaluations\n\n*   **Description:** A benchmark for evaluating instruction-following capabilities specifically in the context of code generation and programming tasks.\n*   **Dataset Attributes:** Contains programming challenges that require models to generate code based on natural language instructions. It evaluates the model’s ability to understand and execute programming-related instructions accurately.\n*   **Reference:** [“Evaluating Large Language Models Trained on Code”](https://arxiv.org/abs/2107.03374).\n\n#### InstructGPT Benchmarks\n\n*   **Description:** Used to evaluate the performance of InstructGPT models, focusing on the ability to follow detailed and complex instructions.\n*   **Dataset Attributes:** Encompasses a variety of tasks including creative writing, problem-solving, and detailed explanations. The benchmark aims to assess the alignment of model outputs with user-provided instructions, ensuring the model’s responses are accurate and contextually appropriate.\n*   **Reference:** [“Training language models to follow instructions with human feedback”](https://arxiv.org/abs/2203.02155).\n\n#### BigGen Bench (Big Generation Benchmark)\n\n*   **Description:** BigGen Bench is a comprehensive generation benchmark that evaluates large language models across nine core capabilities using instance-specific evaluation criteria. It moves beyond general assessments of helpfulness to enable nuanced, fine-grained evaluation more aligned with human judgment.\n*   **Dataset Attributes:** Encompasses 77 tasks and 765 instances across capabilities such as reasoning, planning, tool usage, instruction following, and more. Each instance includes detailed prompts, reference answers, and a 5-point Likert-scale rubric designed to assess specific performance attributes. Evaluations are performed using both human raters and large language model evaluators. Tasks test abilities like hypothesis generation, code revision, multi-agent planning, and cultural awareness.\n*   **Reference:** [“BigGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models”](https://arxiv.org/abs/2406.05761).",
    "order": 6,
    "orderInChapter": 6,
    "difficulty": 2,
    "estimatedMinutes": 4,
    "tags": [
      "offlineonline evaluation",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 648,
      "contentLength": 6640
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#instruction-tuning-and-evaluation",
    "scrapedAt": "2025-12-28T11:55:20.780Z"
  },
  {
    "id": "ai-benchmarks-multi-turn-conversation-benchmarks-7",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Large Language Models (LLMs)",
    "title": "Multi-Turn Conversation Benchmarks",
    "subtitle": "Large Language Models (LLMs)",
    "contentHtml": "<h4 id=\"mtbench\">MTBench</h4>\n<ul>\n  <li><strong>Description:</strong> A benchmark designed to evaluate the performance of multi-turn dialogue systems on instruction-following tasks.</li>\n  <li><strong>Dataset Attributes:</strong> Contains a variety of multi-turn conversations where the model must follow detailed and evolving instructions across multiple exchanges. The tasks involve complex dialogues requiring the model to maintain context and coherence over several turns.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2306.05685\">“Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena”</a>.</li>\n</ul>\n<h4 id=\"mt-eval\">MT-Eval</h4>\n<ul>\n  <li><strong>Description:</strong> A comprehensive benchmark designed to evaluate multi-turn conversational abilities.</li>\n  <li><strong>Dataset Attributes:</strong> By analyzing human-LLM conversations, interaction patterns are categorized into four types: recollection, expansion, refinement, and follow-up. Multi-turn queries for each category are either augmented from existing datasets or newly generated with GPT-4 to prevent data leakage. To examine factors affecting multi-turn abilities, single-turn versions of the 1170 multi-turn queries are created for performance comparison.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2401.16745\">“MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models”</a>.</li>\n</ul>\n<h4 id=\"mutual-multi-turn-dialogue-reasoning\">MuTual (Multi-Turn Dialogue Reasoning)</h4>\n<ul>\n  <li><strong>Description:</strong> A dataset designed for evaluating multi-turn reasoning in dialogue systems.</li>\n  <li><strong>Dataset Attributes:</strong> Features dialogues from Chinese high school English listening tests, requiring models to select the correct answer from multiple choices based on dialogue context. Emphasizes reasoning over multiple turns to derive the correct conclusion.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2004.04494\">“MuTual: A Dataset for Multi-Turn Dialogue Reasoning”</a>.</li>\n</ul>\n<h4 id=\"dailydialog\">DailyDialog</h4>\n<ul>\n  <li><strong>Description:</strong> A high-quality multi-turn dialogue dataset covering a wide range of everyday topics and scenarios.</li>\n  <li><strong>Dataset Attributes:</strong> Contains dialogues involving various everyday scenarios, annotated for dialogue act, emotion, and topic. Aimed at training and evaluating models on natural, human-like conversation.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1710.03957\">“DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset”</a>.</li>\n</ul>\n<h4 id=\"multiwoz-multi-domain-wizard-of-oz\">MultiWOZ (Multi-Domain Wizard of Oz)</h4>\n<ul>\n  <li><strong>Description:</strong> A comprehensive dataset for multi-turn task-oriented dialogues spanning multiple domains.</li>\n  <li><strong>Dataset Attributes:</strong> Includes dialogues that span multiple domains like booking, travel, and restaurant reservations, annotated with user intents and system responses. Designed to train models for complex dialogue management across different domains.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1810.00278\">“MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling”</a>.</li>\n</ul>\n<h4 id=\"taskmaster\">Taskmaster</h4>\n<ul>\n  <li><strong>Description:</strong> A diverse dataset for multi-turn conversations that include both spoken and written interactions.</li>\n  <li><strong>Dataset Attributes:</strong> Covers several domains with conversations sourced from both human-human and human-machine interactions, providing a rich resource for training dialogue systems on varied conversational data.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1909.05358\">“Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset”</a>.</li>\n</ul>\n<h4 id=\"persona-chat\">Persona-Chat</h4>\n<ul>\n  <li><strong>Description:</strong> A dataset focusing on persona-based dialogues to test models’ ability to maintain consistent personality traits across conversations.</li>\n  <li><strong>Dataset Attributes:</strong> Consists of conversations where each participant has a predefined persona, requiring models to generate responses that are consistent with the given persona traits. Designed to foster more engaging and personalized dialogues.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1801.07243\">“Personalizing Dialogue Agents: I have a dog, do you have pets too?”</a>.</li>\n</ul>\n<h4 id=\"dialogre\">DialogRE</h4>\n<ul>\n  <li><strong>Description:</strong> A large-scale dialog reasoning benchmark focusing on relational extraction from conversations.</li>\n  <li><strong>Dataset Attributes:</strong> Consists of dialogue instances annotated with relational facts, testing the model’s ability to understand and extract relationships from multi-turn dialogues. Includes a variety of domains and conversational contexts.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2004.08056\">“Dialogue-Based Relation Extraction”</a>.</li>\n</ul>",
    "contentMarkdown": "#### MTBench\n\n*   **Description:** A benchmark designed to evaluate the performance of multi-turn dialogue systems on instruction-following tasks.\n*   **Dataset Attributes:** Contains a variety of multi-turn conversations where the model must follow detailed and evolving instructions across multiple exchanges. The tasks involve complex dialogues requiring the model to maintain context and coherence over several turns.\n*   **Reference:** [“Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena”](https://arxiv.org/abs/2306.05685).\n\n#### MT-Eval\n\n*   **Description:** A comprehensive benchmark designed to evaluate multi-turn conversational abilities.\n*   **Dataset Attributes:** By analyzing human-LLM conversations, interaction patterns are categorized into four types: recollection, expansion, refinement, and follow-up. Multi-turn queries for each category are either augmented from existing datasets or newly generated with GPT-4 to prevent data leakage. To examine factors affecting multi-turn abilities, single-turn versions of the 1170 multi-turn queries are created for performance comparison.\n*   **Reference:** [“MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models”](https://arxiv.org/abs/2401.16745).\n\n#### MuTual (Multi-Turn Dialogue Reasoning)\n\n*   **Description:** A dataset designed for evaluating multi-turn reasoning in dialogue systems.\n*   **Dataset Attributes:** Features dialogues from Chinese high school English listening tests, requiring models to select the correct answer from multiple choices based on dialogue context. Emphasizes reasoning over multiple turns to derive the correct conclusion.\n*   **Reference:** [“MuTual: A Dataset for Multi-Turn Dialogue Reasoning”](https://arxiv.org/abs/2004.04494).\n\n#### DailyDialog\n\n*   **Description:** A high-quality multi-turn dialogue dataset covering a wide range of everyday topics and scenarios.\n*   **Dataset Attributes:** Contains dialogues involving various everyday scenarios, annotated for dialogue act, emotion, and topic. Aimed at training and evaluating models on natural, human-like conversation.\n*   **Reference:** [“DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset”](https://arxiv.org/abs/1710.03957).\n\n#### MultiWOZ (Multi-Domain Wizard of Oz)\n\n*   **Description:** A comprehensive dataset for multi-turn task-oriented dialogues spanning multiple domains.\n*   **Dataset Attributes:** Includes dialogues that span multiple domains like booking, travel, and restaurant reservations, annotated with user intents and system responses. Designed to train models for complex dialogue management across different domains.\n*   **Reference:** [“MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling”](https://arxiv.org/abs/1810.00278).\n\n#### Taskmaster\n\n*   **Description:** A diverse dataset for multi-turn conversations that include both spoken and written interactions.\n*   **Dataset Attributes:** Covers several domains with conversations sourced from both human-human and human-machine interactions, providing a rich resource for training dialogue systems on varied conversational data.\n*   **Reference:** [“Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset”](https://arxiv.org/abs/1909.05358).\n\n#### Persona-Chat\n\n*   **Description:** A dataset focusing on persona-based dialogues to test models’ ability to maintain consistent personality traits across conversations.\n*   **Dataset Attributes:** Consists of conversations where each participant has a predefined persona, requiring models to generate responses that are consistent with the given persona traits. Designed to foster more engaging and personalized dialogues.\n*   **Reference:** [“Personalizing Dialogue Agents: I have a dog, do you have pets too?”](https://arxiv.org/abs/1801.07243).\n\n#### DialogRE\n\n*   **Description:** A large-scale dialog reasoning benchmark focusing on relational extraction from conversations.\n*   **Dataset Attributes:** Consists of dialogue instances annotated with relational facts, testing the model’s ability to understand and extract relationships from multi-turn dialogues. Includes a variety of domains and conversational contexts.\n*   **Reference:** [“Dialogue-Based Relation Extraction”](https://arxiv.org/abs/2004.08056).",
    "order": 7,
    "orderInChapter": 7,
    "difficulty": 2,
    "estimatedMinutes": 3,
    "tags": [
      "offlineonline evaluation",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 499,
      "contentLength": 5145
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#multi-turn-conversation-benchmarks",
    "scrapedAt": "2025-12-28T11:55:20.780Z"
  },
  {
    "id": "ai-benchmarks-reward-model-evaluation-8",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Large Language Models (LLMs)",
    "title": "Reward Model Evaluation",
    "subtitle": "Large Language Models (LLMs)",
    "contentHtml": "<h4 id=\"rewardbench\">RewardBench</h4>\n<ul>\n  <li><strong>Description:</strong> A benchmark designed to assess reward model capabilities in four categories: Chat, Chat-Hard, Safety, and Reasoning.</li>\n  <li><strong>Dataset Attributes:</strong> A collection of prompt-chosen-rejected trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured, and out-of-distribution queries. Specific comparison datasets are created for RMs that involve subtle but verifiable reasons (e.g., bugs, incorrect facts) for preferring one answer over another.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2403.13787\">“RewardBench: Evaluating Reward Models for Language Modeling”</a>.\nYou’re right! Below is the properly formatted version, where each benchmark is a sub-section with its own bullets.</li>\n</ul>",
    "contentMarkdown": "#### RewardBench\n\n*   **Description:** A benchmark designed to assess reward model capabilities in four categories: Chat, Chat-Hard, Safety, and Reasoning.\n*   **Dataset Attributes:** A collection of prompt-chosen-rejected trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured, and out-of-distribution queries. Specific comparison datasets are created for RMs that involve subtle but verifiable reasons (e.g., bugs, incorrect facts) for preferring one answer over another.\n*   **Reference:** [“RewardBench: Evaluating Reward Models for Language Modeling”](https://arxiv.org/abs/2403.13787). You’re right! Below is the properly formatted version, where each benchmark is a sub-section with its own bullets.",
    "order": 8,
    "orderInChapter": 8,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "offlineonline evaluation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 95,
      "contentLength": 863
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#reward-model-evaluation",
    "scrapedAt": "2025-12-28T11:55:20.780Z"
  },
  {
    "id": "ai-benchmarks-medical-benchmarks-9",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Large Language Models (LLMs)",
    "title": "Medical Benchmarks",
    "subtitle": "Large Language Models (LLMs)",
    "contentHtml": "<ul>\n  <li>In the medical/biomedical field, benchmarks play a critical role in evaluating the ability of AI models to handle domain-specific tasks such as clinical decision support, medical image analysis, and processing of biomedical literature. Below is an overview of common benchmarks in these areas, including dataset attributes and references to original papers.</li>\n</ul>\n<h4 id=\"clinical-decision-support-and-patient-outcomes\">Clinical Decision Support and Patient Outcomes</h4>\n<h5 id=\"mimic-iii-medical-information-mart-for-intensive-care\">MIMIC-III (Medical Information Mart for Intensive Care)</h5>\n<ul>\n  <li><strong>Description:</strong> A widely used dataset comprising de-identified health data associated with over forty thousand patients who stayed in critical care units. This dataset is used for tasks such as predicting patient outcomes, extracting clinical information, and generating clinical notes.</li>\n  <li><strong>Dataset Attributes:</strong> Includes notes, lab test results, vital signs, medication records, diagnostic codes, and demographic information, requiring a comprehensive understanding of medical terminology, clinical narratives, and patient history.</li>\n  <li><strong>Reference:</strong> <a href=\"https://www.nature.com/articles/sdata201635\">“The MIMIC-III Clinical Database”</a></li>\n</ul>\n<h4 id=\"biomedical-question-answering\">Biomedical Question Answering</h4>\n<h5 id=\"bioasq\">BioASQ</h5>\n<ul>\n  <li><strong>Description:</strong> A challenge for testing biomedical semantic indexing and question-answering capabilities. The tasks include factoid, list-based, yes/no, and summary questions based on biomedical research articles.</li>\n  <li><strong>Dataset Attributes:</strong> Questions are crafted from the titles and abstracts of articles in PubMed, challenging models to retrieve and generate precise biomedical information. Includes large-scale training data and evaluation metrics that focus on precision and recall.</li>\n  <li><strong>Reference:</strong> <a href=\"https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0564-6\">“BioASQ: A challenge on large-scale biomedical semantic indexing and question answering”</a></li>\n</ul>\n<h5 id=\"medqa-usmle\">MedQA (USMLE)</h5>\n<ul>\n  <li><strong>Description:</strong> A question-answering benchmark based on the United States Medical Licensing Examination, which assesses a model’s ability to reason with medical knowledge under exam conditions.</li>\n  <li><strong>Dataset Attributes:</strong> Consists of multiple-choice questions with detailed explanations, reflecting real-world medical licensing exam scenarios that test comprehensive medical knowledge, clinical reasoning, and problem-solving skills.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2009.13081\">“What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams”</a></li>\n</ul>\n<h5 id=\"multimedqa\">MultiMedQA</h5>\n<ul>\n  <li><strong>Description:</strong> A benchmark collection that integrates multiple datasets for evaluating question answering across various medical fields, including consumer health, clinical medicine, and genetics.</li>\n  <li><strong>Dataset Attributes:</strong> Incorporates questions from several sources, requiring broad and deep medical knowledge across diverse sub-disciplines. Includes tasks such as multiple-choice questions, evidence retrieval, and fact verification.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2207.02715\">“MultiMedQA: Large-scale Multi-domain Medical Question Answering”</a></li>\n</ul>\n<h5 id=\"pubmedqa\">PubMedQA</h5>\n<ul>\n  <li><strong>Description:</strong> A dataset for natural language question-answering using abstracts from PubMed as the context, focusing on yes/no questions.</li>\n  <li><strong>Dataset Attributes:</strong> Questions derived from PubMed article titles with answers provided in the abstracts, emphasizing models’ ability to extract and verify factual information from scientific texts. Includes a balanced distribution of yes, no, and maybe answers.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1909.06146\">“PubMedQA: A Dataset for Biomedical Research Question Answering”</a></li>\n</ul>\n<h5 id=\"medmcqa\">MedMCQA</h5>\n<ul>\n  <li><strong>Description:</strong> A medical multiple-choice question-answering benchmark that evaluates comprehensive understanding and application of medical concepts.</li>\n  <li><strong>Dataset Attributes:</strong> Features challenging multiple-choice questions that cover a wide range of medical topics, testing not only knowledge but also deep understanding and reasoning skills in medical contexts. Questions are sourced from medical exams and expert annotations.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2203.14371\">“MedMCQA: A Large-scale Multi-domain Clinical Question Answering Dataset”</a></li>\n</ul>\n<h4 id=\"biomedical-language-understanding\">Biomedical Language Understanding</h4>\n<h5 id=\"blue-biomedical-language-understanding-evaluation\">BLUE (Biomedical Language Understanding Evaluation)</h5>\n<ul>\n  <li><strong>Description:</strong> A benchmark consisting of several diverse biomedical NLP tasks such as named entity recognition, relation extraction, and sentence similarity in the biomedical domain.</li>\n  <li><strong>Dataset Attributes:</strong> Utilizes various biomedical corpora, including PubMed abstracts, clinical trial reports, and electronic health records, emphasizing specialized language understanding and entity relations. Tasks are designed to evaluate both generalization and specialization in biomedical contexts.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1906.05474\">“BLUE: The Biomedical Language Understanding Evaluation Benchmark”</a></li>\n</ul>",
    "contentMarkdown": "*   In the medical/biomedical field, benchmarks play a critical role in evaluating the ability of AI models to handle domain-specific tasks such as clinical decision support, medical image analysis, and processing of biomedical literature. Below is an overview of common benchmarks in these areas, including dataset attributes and references to original papers.\n\n#### Clinical Decision Support and Patient Outcomes\n\n##### MIMIC-III (Medical Information Mart for Intensive Care)\n\n*   **Description:** A widely used dataset comprising de-identified health data associated with over forty thousand patients who stayed in critical care units. This dataset is used for tasks such as predicting patient outcomes, extracting clinical information, and generating clinical notes.\n*   **Dataset Attributes:** Includes notes, lab test results, vital signs, medication records, diagnostic codes, and demographic information, requiring a comprehensive understanding of medical terminology, clinical narratives, and patient history.\n*   **Reference:** [“The MIMIC-III Clinical Database”](https://www.nature.com/articles/sdata201635)\n\n#### Biomedical Question Answering\n\n##### BioASQ\n\n*   **Description:** A challenge for testing biomedical semantic indexing and question-answering capabilities. The tasks include factoid, list-based, yes/no, and summary questions based on biomedical research articles.\n*   **Dataset Attributes:** Questions are crafted from the titles and abstracts of articles in PubMed, challenging models to retrieve and generate precise biomedical information. Includes large-scale training data and evaluation metrics that focus on precision and recall.\n*   **Reference:** [“BioASQ: A challenge on large-scale biomedical semantic indexing and question answering”](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0564-6)\n\n##### MedQA (USMLE)\n\n*   **Description:** A question-answering benchmark based on the United States Medical Licensing Examination, which assesses a model’s ability to reason with medical knowledge under exam conditions.\n*   **Dataset Attributes:** Consists of multiple-choice questions with detailed explanations, reflecting real-world medical licensing exam scenarios that test comprehensive medical knowledge, clinical reasoning, and problem-solving skills.\n*   **Reference:** [“What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams”](https://arxiv.org/abs/2009.13081)\n\n##### MultiMedQA\n\n*   **Description:** A benchmark collection that integrates multiple datasets for evaluating question answering across various medical fields, including consumer health, clinical medicine, and genetics.\n*   **Dataset Attributes:** Incorporates questions from several sources, requiring broad and deep medical knowledge across diverse sub-disciplines. Includes tasks such as multiple-choice questions, evidence retrieval, and fact verification.\n*   **Reference:** [“MultiMedQA: Large-scale Multi-domain Medical Question Answering”](https://arxiv.org/abs/2207.02715)\n\n##### PubMedQA\n\n*   **Description:** A dataset for natural language question-answering using abstracts from PubMed as the context, focusing on yes/no questions.\n*   **Dataset Attributes:** Questions derived from PubMed article titles with answers provided in the abstracts, emphasizing models’ ability to extract and verify factual information from scientific texts. Includes a balanced distribution of yes, no, and maybe answers.\n*   **Reference:** [“PubMedQA: A Dataset for Biomedical Research Question Answering”](https://arxiv.org/abs/1909.06146)\n\n##### MedMCQA\n\n*   **Description:** A medical multiple-choice question-answering benchmark that evaluates comprehensive understanding and application of medical concepts.\n*   **Dataset Attributes:** Features challenging multiple-choice questions that cover a wide range of medical topics, testing not only knowledge but also deep understanding and reasoning skills in medical contexts. Questions are sourced from medical exams and expert annotations.\n*   **Reference:** [“MedMCQA: A Large-scale Multi-domain Clinical Question Answering Dataset”](https://arxiv.org/abs/2203.14371)\n\n#### Biomedical Language Understanding\n\n##### BLUE (Biomedical Language Understanding Evaluation)\n\n*   **Description:** A benchmark consisting of several diverse biomedical NLP tasks such as named entity recognition, relation extraction, and sentence similarity in the biomedical domain.\n*   **Dataset Attributes:** Utilizes various biomedical corpora, including PubMed abstracts, clinical trial reports, and electronic health records, emphasizing specialized language understanding and entity relations. Tasks are designed to evaluate both generalization and specialization in biomedical contexts.\n*   **Reference:** [“BLUE: The Biomedical Language Understanding Evaluation Benchmark”](https://arxiv.org/abs/1906.05474)",
    "order": 9,
    "orderInChapter": 9,
    "difficulty": 2,
    "estimatedMinutes": 3,
    "tags": [
      "offlineonline evaluation",
      "nlp"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 573,
      "contentLength": 5837
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#medical-benchmarks",
    "scrapedAt": "2025-12-28T11:55:20.780Z"
  },
  {
    "id": "ai-benchmarks-software-development-benchmarks-10",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Large Language Models (LLMs)",
    "title": "Software Development Benchmarks",
    "subtitle": "Large Language Models (LLMs)",
    "contentHtml": "<ul>\n  <li>These benchmarks challenge models on various aspects such as code generation, understanding, and debugging. Below is a detailed overview of common benchmarks used for evaluating code LLMs, including the attributes of their datasets and references to the original papers where these benchmarks were proposed.</li>\n</ul>\n<h4 id=\"humaneval-1\">HumanEval</h4>\n<ul>\n  <li><strong>Description:</strong> This benchmark is designed to test the ability of language models to generate code. It consists of a set of Python programming problems that require writing function definitions from scratch.</li>\n  <li><strong>Dataset Attributes:</strong> Includes 164 hand-crafted programming problems covering a range of difficulty levels, requiring understanding of problem statements and generation of functionally correct and efficient code. Problems are evaluated based on correctness and execution results.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2107.03374\">“Evaluating Large Language Models Trained on Code”</a></li>\n</ul>\n<h4 id=\"humaneval-2\">HumanEval+</h4>\n<ul>\n  <li><strong>Description:</strong> An extension of the HumanEval benchmark, aimed at assessing the ability of models to handle more intricate and diverse code generation tasks.</li>\n  <li><strong>Dataset Attributes:</strong> Includes a set of 300 Python programming problems that span a wider range of difficulty levels and require more sophisticated solutions. The problems are designed to test deeper understanding and creativity in code generation.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2402.56789\">“HumanEval+: Extending HumanEval for Advanced Code Generation Tasks”</a></li>\n</ul>\n<h4 id=\"mostly-basic-programming-problems-mbpp\">Mostly Basic Programming Problems (MBPP)</h4>\n<ul>\n  <li><strong>Description:</strong> A benchmark consisting of simple Python coding problems intended to evaluate the capabilities of code generation models in solving basic programming tasks.</li>\n  <li><strong>Dataset Attributes:</strong> Contains 974 Python programming problems, focusing on basic functionalities and common programming tasks that are relatively straightforward to solve. Problems range from simple arithmetic to basic data manipulation and control structures.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2108.07732\">“Program Synthesis with Large Language Models”</a></li>\n</ul>\n<h4 id=\"mbpp\">MBPP+</h4>\n<ul>\n  <li><strong>Description:</strong> An extension of the MBPP benchmark, designed to evaluate more complex and diverse programming tasks.</li>\n  <li><strong>Dataset Attributes:</strong> Comprises an expanded set of 1500 Python programming problems, including more complex and diverse tasks that require deeper problem-solving skills and understanding of advanced programming concepts. The problems cover a broader range of real-world scenarios and applications.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2305.01210\">“Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation”</a></li>\n</ul>\n<h4 id=\"swe-bench-1\">SWE-Bench</h4>\n<ul>\n  <li><strong>Description:</strong> This benchmark evaluates the ability of language models to generate software engineering-related code, focusing on practical tasks encountered in the industry.</li>\n  <li><strong>Dataset Attributes:</strong> Comprises a diverse set of software engineering tasks, including bug fixing, feature implementation, and code refactoring. The problems require understanding software specifications and generating correct and maintainable code.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2310.06770\">“SWE-bench: Can Language Models Resolve Real-World GitHub Issues?”</a></li>\n</ul>\n<h4 id=\"swe-bench-verified\">SWE-Bench Verified</h4>\n<ul>\n  <li><strong>Description:</strong> A comprehensive benchmark evaluating models on practical software engineering tasks such as feature implementation, bug fixing, and code refactoring.</li>\n  <li><strong>Dataset Attributes:</strong> Focuses on real-world software engineering challenges sourced from GitHub repositories. Problems require understanding of detailed specifications and generation of efficient, maintainable code. Evaluation is based on correctness, maintainability, and alignment with engineering practices.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2310.06770\">“SWE-bench: Can Language Models Resolve Real-World GitHub Issues?”</a></li>\n</ul>\n<h4 id=\"aider\">Aider</h4>\n<ul>\n  <li><strong>Description:</strong> A benchmark aimed at assessing the capabilities of models in aiding software development by providing intelligent code suggestions and improvements.</li>\n  <li><strong>Dataset Attributes:</strong> Includes a variety of real-world coding scenarios where models are evaluated based on their ability to offer meaningful code suggestions, improvements, and refactoring options. The dataset spans multiple programming languages and development contexts.</li>\n  <li><strong>Reference:</strong> <a href=\"https://github.com/paul-gauthier/aider\">Paul Gauthier’s GitHub</a></li>\n</ul>\n<h4 id=\"multipl-e\">MultiPL-E</h4>\n<ul>\n  <li><strong>Description:</strong> A benchmark designed to evaluate the performance of language models across multiple programming languages.</li>\n  <li><strong>Dataset Attributes:</strong> Contains a variety of programming problems that are translated into several programming languages, including Python, JavaScript, Java, and C++. The benchmark tests the model’s ability to understand and generate code in different syntaxes and paradigms.</li>\n  <li><strong>Reference:</strong> <a href=\"http://arxiv.org/abs/2208.08227\">“MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation”</a></li>\n</ul>\n<h4 id=\"bigcodebench\">BigCodeBench</h4>\n<ul>\n  <li><strong>Description:</strong> A benchmark to evaluate LLMs on challenging and complex coding tasks focused on realistic, function-level tasks that require the use of diverse libraries and complex reasoning.</li>\n  <li><strong>Dataset Attributes:</strong> Contains 1,140 tasks with 5.6 test cases each, covering 139 libraries in Python. Uses Pass@1 with greedy decoding and Elo rating for comprehensive evaluation. Tasks are created in a three-stage process, including synthetic data generation and cross-validation by humans. The best model is GPT-4 with 61.1%, followed by DeepSeek-Coder-V2. Best open model is DeepSeek-Coder-V2 with 59.7%, better than Claude 3 Opus or Gemini. Evaluation framework and Docker images are available for easy reproduction. Plans to extend to multilingualism.</li>\n  <li><strong>Reference:</strong> <a href=\"https://github.com/bigcode-project/bigcodebench\">Code</a>; <a href=\"https://huggingface.co/blog/leaderboard-bigcodebench\">Blog</a>; <a href=\"https://huggingface.co/spaces/bigcode/bigcodebench-leaderboard\">Leaderboard</a></li>\n</ul>\n<h4 id=\"swe-lancer\">SWE-Lancer</h4>\n<ul>\n  <li><strong>Description:</strong> SWE-Lancer is a benchmark of over 1,400 real-world freelance software engineering tasks sourced from Upwork, valued at $1 million in payouts. It includes both independent engineering tasks (ranging from $50 bug fixes to $32,000 feature implementations) and managerial tasks, where models must choose between technical implementation proposals.</li>\n  <li><strong>Dataset Attributes:</strong> Independent tasks are graded using triple-verified end-to-end tests by experienced software engineers, while managerial decisions are assessed against original engineering managers’ choices. The benchmark provides an economic impact analysis by mapping model performance to monetary value.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2502.12115\">“SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?”</a> and <a href=\"https://openai.com/index/swe-lancer/\">“Introducing the SWE-Lancer Benchmark”</a></li>\n</ul>\n<h4 id=\"code-debugging-and-error-detection\">Code Debugging and Error Detection</h4>\n<h5 id=\"ds-1000-deepsource-python-bugs-dataset\">DS-1000 (DeepSource Python Bugs Dataset)</h5>\n<ul>\n  <li><strong>Description:</strong> This dataset is used to evaluate the ability of models to detect bugs in Python code. It includes a diverse set of real-world bugs.</li>\n  <li><strong>Dataset Attributes:</strong> Comprises 1000 annotated Python functions with detailed bug annotations, testing models on their ability to identify and understand common coding errors. The dataset includes both syntactic and semantic bugs, providing a comprehensive debugging challenge.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2211.11501\">“DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation”</a></li>\n</ul>\n<h5 id=\"livecodebench\">LiveCodeBench</h5>\n<ul>\n  <li><strong>Description:</strong> A benchmark designed to test the effectiveness of code generation models in real-time collaborative coding environments.</li>\n  <li><strong>Dataset Attributes:</strong> Features a collection of coding tasks designed to simulate live coding sessions where models need to provide accurate and timely code completions and suggestions. The tasks cover various programming languages and development frameworks.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2403.07974\">“LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code”</a></li>\n</ul>\n<h4 id=\"comprehensive-code-understanding-and-multi-language-evaluation\">Comprehensive Code Understanding and Multi-language Evaluation</h4>\n<h5 id=\"codexglue\">CodeXGLUE</h5>\n<ul>\n  <li><strong>Description:</strong> A comprehensive benchmark that includes multiple tasks like code completion, code translation, and code repair across various programming languages.</li>\n  <li><strong>Dataset Attributes:</strong> Encompasses a range of programming challenges and languages, providing a broad assessment of models’ code understanding and generation across different contexts. The benchmark includes tasks for code summarization, code search, and clone detection, covering languages like Python, Java, and more.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2102.04664\">“CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation”</a></li>\n</ul>\n<h4 id=\"algorithmic-problem-solving\">Algorithmic Problem Solving</h4>\n<h5 id=\"competition-code-codeforces\">Competition Code (Codeforces)</h5>\n<ul>\n  <li><strong>Description:</strong> This benchmark includes competitive programming problems sourced from the Codeforces platform, known for its rigorous contests.</li>\n  <li><strong>Dataset Attributes:</strong> Features diverse problems covering areas like graph theory, dynamic programming, and computational geometry. Problems span beginner to advanced levels, requiring optimization and algorithmic depth.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2312.02143\">“Competition-Level Problems are Effective LLM Evaluators”</a></li>\n</ul>\n<h5 id=\"leetcode-problems\">LeetCode Problems</h5>\n<ul>\n  <li><strong>Description:</strong> A widely used benchmark for algorithmic problem solving, offering a comprehensive set of problems that test various algorithmic and data structure concepts.</li>\n  <li><strong>Dataset Attributes:</strong> Features thousands of problems across different categories such as arrays, linked lists, dynamic programming, and more. Problems range from easy to hard, providing a robust platform for evaluating algorithmic problem-solving skills.</li>\n  <li><strong>Reference:</strong> <a href=\"https://www.kaggle.com/datasets/eemanmajumder/the-leetcode-solution-dataset\">The LeetCode Solution Dataset on Kaggle</a></li>\n</ul>\n<h5 id=\"codeforces-problems\">Codeforces Problems</h5>\n<ul>\n  <li><strong>Description:</strong> This benchmark includes competitive programming problems from Codeforces, a platform known for its challenging contests and diverse problem sets.</li>\n  <li><strong>Dataset Attributes:</strong> Contains problems that are designed to test deep algorithmic understanding and optimization skills. The problems vary in difficulty and cover a wide range of topics including graph theory, combinatorics, and computational geometry.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2312.02143\">“Competition-Level Problems are Effective LLM Evaluators”</a></li>\n</ul>",
    "contentMarkdown": "*   These benchmarks challenge models on various aspects such as code generation, understanding, and debugging. Below is a detailed overview of common benchmarks used for evaluating code LLMs, including the attributes of their datasets and references to the original papers where these benchmarks were proposed.\n\n#### HumanEval\n\n*   **Description:** This benchmark is designed to test the ability of language models to generate code. It consists of a set of Python programming problems that require writing function definitions from scratch.\n*   **Dataset Attributes:** Includes 164 hand-crafted programming problems covering a range of difficulty levels, requiring understanding of problem statements and generation of functionally correct and efficient code. Problems are evaluated based on correctness and execution results.\n*   **Reference:** [“Evaluating Large Language Models Trained on Code”](https://arxiv.org/abs/2107.03374)\n\n#### HumanEval+\n\n*   **Description:** An extension of the HumanEval benchmark, aimed at assessing the ability of models to handle more intricate and diverse code generation tasks.\n*   **Dataset Attributes:** Includes a set of 300 Python programming problems that span a wider range of difficulty levels and require more sophisticated solutions. The problems are designed to test deeper understanding and creativity in code generation.\n*   **Reference:** [“HumanEval+: Extending HumanEval for Advanced Code Generation Tasks”](https://arxiv.org/abs/2402.56789)\n\n#### Mostly Basic Programming Problems (MBPP)\n\n*   **Description:** A benchmark consisting of simple Python coding problems intended to evaluate the capabilities of code generation models in solving basic programming tasks.\n*   **Dataset Attributes:** Contains 974 Python programming problems, focusing on basic functionalities and common programming tasks that are relatively straightforward to solve. Problems range from simple arithmetic to basic data manipulation and control structures.\n*   **Reference:** [“Program Synthesis with Large Language Models”](https://arxiv.org/abs/2108.07732)\n\n#### MBPP+\n\n*   **Description:** An extension of the MBPP benchmark, designed to evaluate more complex and diverse programming tasks.\n*   **Dataset Attributes:** Comprises an expanded set of 1500 Python programming problems, including more complex and diverse tasks that require deeper problem-solving skills and understanding of advanced programming concepts. The problems cover a broader range of real-world scenarios and applications.\n*   **Reference:** [“Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation”](https://arxiv.org/abs/2305.01210)\n\n#### SWE-Bench\n\n*   **Description:** This benchmark evaluates the ability of language models to generate software engineering-related code, focusing on practical tasks encountered in the industry.\n*   **Dataset Attributes:** Comprises a diverse set of software engineering tasks, including bug fixing, feature implementation, and code refactoring. The problems require understanding software specifications and generating correct and maintainable code.\n*   **Reference:** [“SWE-bench: Can Language Models Resolve Real-World GitHub Issues?”](https://arxiv.org/abs/2310.06770)\n\n#### SWE-Bench Verified\n\n*   **Description:** A comprehensive benchmark evaluating models on practical software engineering tasks such as feature implementation, bug fixing, and code refactoring.\n*   **Dataset Attributes:** Focuses on real-world software engineering challenges sourced from GitHub repositories. Problems require understanding of detailed specifications and generation of efficient, maintainable code. Evaluation is based on correctness, maintainability, and alignment with engineering practices.\n*   **Reference:** [“SWE-bench: Can Language Models Resolve Real-World GitHub Issues?”](https://arxiv.org/abs/2310.06770)\n\n#### Aider\n\n*   **Description:** A benchmark aimed at assessing the capabilities of models in aiding software development by providing intelligent code suggestions and improvements.\n*   **Dataset Attributes:** Includes a variety of real-world coding scenarios where models are evaluated based on their ability to offer meaningful code suggestions, improvements, and refactoring options. The dataset spans multiple programming languages and development contexts.\n*   **Reference:** [Paul Gauthier’s GitHub](https://github.com/paul-gauthier/aider)\n\n#### MultiPL-E\n\n*   **Description:** A benchmark designed to evaluate the performance of language models across multiple programming languages.\n*   **Dataset Attributes:** Contains a variety of programming problems that are translated into several programming languages, including Python, JavaScript, Java, and C++. The benchmark tests the model’s ability to understand and generate code in different syntaxes and paradigms.\n*   **Reference:** [“MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation”](http://arxiv.org/abs/2208.08227)\n\n#### BigCodeBench\n\n*   **Description:** A benchmark to evaluate LLMs on challenging and complex coding tasks focused on realistic, function-level tasks that require the use of diverse libraries and complex reasoning.\n*   **Dataset Attributes:** Contains 1,140 tasks with 5.6 test cases each, covering 139 libraries in Python. Uses Pass@1 with greedy decoding and Elo rating for comprehensive evaluation. Tasks are created in a three-stage process, including synthetic data generation and cross-validation by humans. The best model is GPT-4 with 61.1%, followed by DeepSeek-Coder-V2. Best open model is DeepSeek-Coder-V2 with 59.7%, better than Claude 3 Opus or Gemini. Evaluation framework and Docker images are available for easy reproduction. Plans to extend to multilingualism.\n*   **Reference:** [Code](https://github.com/bigcode-project/bigcodebench); [Blog](https://huggingface.co/blog/leaderboard-bigcodebench); [Leaderboard](https://huggingface.co/spaces/bigcode/bigcodebench-leaderboard)\n\n#### SWE-Lancer\n\n*   **Description:** SWE-Lancer is a benchmark of over 1,400 real-world freelance software engineering tasks sourced from Upwork, valued at $1 million in payouts. It includes both independent engineering tasks (ranging from $50 bug fixes to $32,000 feature implementations) and managerial tasks, where models must choose between technical implementation proposals.\n*   **Dataset Attributes:** Independent tasks are graded using triple-verified end-to-end tests by experienced software engineers, while managerial decisions are assessed against original engineering managers’ choices. The benchmark provides an economic impact analysis by mapping model performance to monetary value.\n*   **Reference:** [“SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?”](https://arxiv.org/abs/2502.12115) and [“Introducing the SWE-Lancer Benchmark”](https://openai.com/index/swe-lancer/)\n\n#### Code Debugging and Error Detection\n\n##### DS-1000 (DeepSource Python Bugs Dataset)\n\n*   **Description:** This dataset is used to evaluate the ability of models to detect bugs in Python code. It includes a diverse set of real-world bugs.\n*   **Dataset Attributes:** Comprises 1000 annotated Python functions with detailed bug annotations, testing models on their ability to identify and understand common coding errors. The dataset includes both syntactic and semantic bugs, providing a comprehensive debugging challenge.\n*   **Reference:** [“DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation”](https://arxiv.org/abs/2211.11501)\n\n##### LiveCodeBench\n\n*   **Description:** A benchmark designed to test the effectiveness of code generation models in real-time collaborative coding environments.\n*   **Dataset Attributes:** Features a collection of coding tasks designed to simulate live coding sessions where models need to provide accurate and timely code completions and suggestions. The tasks cover various programming languages and development frameworks.\n*   **Reference:** [“LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code”](https://arxiv.org/abs/2403.07974)\n\n#### Comprehensive Code Understanding and Multi-language Evaluation\n\n##### CodeXGLUE\n\n*   **Description:** A comprehensive benchmark that includes multiple tasks like code completion, code translation, and code repair across various programming languages.\n*   **Dataset Attributes:** Encompasses a range of programming challenges and languages, providing a broad assessment of models’ code understanding and generation across different contexts. The benchmark includes tasks for code summarization, code search, and clone detection, covering languages like Python, Java, and more.\n*   **Reference:** [“CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation”](https://arxiv.org/abs/2102.04664)\n\n#### Algorithmic Problem Solving\n\n##### Competition Code (Codeforces)\n\n*   **Description:** This benchmark includes competitive programming problems sourced from the Codeforces platform, known for its rigorous contests.\n*   **Dataset Attributes:** Features diverse problems covering areas like graph theory, dynamic programming, and computational geometry. Problems span beginner to advanced levels, requiring optimization and algorithmic depth.\n*   **Reference:** [“Competition-Level Problems are Effective LLM Evaluators”](https://arxiv.org/abs/2312.02143)\n\n##### LeetCode Problems\n\n*   **Description:** A widely used benchmark for algorithmic problem solving, offering a comprehensive set of problems that test various algorithmic and data structure concepts.\n*   **Dataset Attributes:** Features thousands of problems across different categories such as arrays, linked lists, dynamic programming, and more. Problems range from easy to hard, providing a robust platform for evaluating algorithmic problem-solving skills.\n*   **Reference:** [The LeetCode Solution Dataset on Kaggle](https://www.kaggle.com/datasets/eemanmajumder/the-leetcode-solution-dataset)\n\n##### Codeforces Problems\n\n*   **Description:** This benchmark includes competitive programming problems from Codeforces, a platform known for its challenging contests and diverse problem sets.\n*   **Dataset Attributes:** Contains problems that are designed to test deep algorithmic understanding and optimization skills. The problems vary in difficulty and cover a wide range of topics including graph theory, combinatorics, and computational geometry.\n*   **Reference:** [“Competition-Level Problems are Effective LLM Evaluators”](https://arxiv.org/abs/2312.02143)",
    "order": 10,
    "orderInChapter": 10,
    "difficulty": 2,
    "estimatedMinutes": 7,
    "tags": [
      "offlineonline evaluation",
      "machine learning",
      "gpt",
      "llm",
      "optimization",
      "cross-validation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 1283,
      "contentLength": 12538
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#software-development-benchmarks",
    "scrapedAt": "2025-12-28T11:55:20.780Z"
  },
  {
    "id": "ai-benchmarks-visual-question-answering-11",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Vision-Language Models (VLMs)",
    "title": "Visual Question Answering",
    "subtitle": "Vision-Language Models (VLMs)",
    "contentHtml": "<h4 id=\"visual-question-answering-vqa-and-vqav2\">Visual Question Answering (VQA) and VQAv2</h4>\n<ul>\n  <li><strong>Description:</strong> Requires models to answer questions about images, testing both visual comprehension and language processing.</li>\n  <li><strong>Dataset Attributes:</strong> Combines real and abstract images with questions that require understanding of object properties, spatial relationships, and activities. VQA includes open-ended questions, while VQAv2 provides a balanced dataset to reduce language biases.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1505.00468\">“VQA: Visual Question Answering”</a> and its subsequent updates.</li>\n</ul>\n<h4 id=\"textvqa\">TextVQA</h4>\n<ul>\n  <li><strong>Description:</strong> Focuses on models’ ability to answer questions based on textual information found within images, testing the intersection of visual and textual understanding.</li>\n  <li><strong>Dataset Attributes:</strong> Comprises images containing text in various forms, such as signs, documents, and advertisements. The questions require models to read and comprehend the text within the image to provide accurate answers. The dataset includes a diverse set of images and questions to evaluate comprehensive visual-textual reasoning.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1904.08920\">“TextVQA: Toward VQA Models That Can Read”</a></li>\n</ul>\n<h4 id=\"humanitys-last-exam-hle\">Humanity’s Last Exam (HLE)</h4>\n<ul>\n  <li><strong>Description:</strong> HLE is a multi-modal benchmark designed to be the final closed-ended academic benchmark of its kind, testing the limits of LLM capabilities in visual and textual reasoning. It consists of 2,700 highly challenging questions spanning mathematics, humanities, and the natural sciences, developed by global subject-matter experts.</li>\n  <li><strong>Dataset Attributes:</strong> Questions are formatted as either multiple-choice or short-answer, with some requiring image references for comprehension. Each question is verified for difficulty, ensuring it cannot be easily solved by current LLMs. HLE aims to provide a precise measurement of model capabilities as they approach expert human-level knowledge.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2501.14249\">“Humanity’s Last Exam”</a></li>\n</ul>",
    "contentMarkdown": "#### Visual Question Answering (VQA) and VQAv2\n\n*   **Description:** Requires models to answer questions about images, testing both visual comprehension and language processing.\n*   **Dataset Attributes:** Combines real and abstract images with questions that require understanding of object properties, spatial relationships, and activities. VQA includes open-ended questions, while VQAv2 provides a balanced dataset to reduce language biases.\n*   **Reference:** [“VQA: Visual Question Answering”](https://arxiv.org/abs/1505.00468) and its subsequent updates.\n\n#### TextVQA\n\n*   **Description:** Focuses on models’ ability to answer questions based on textual information found within images, testing the intersection of visual and textual understanding.\n*   **Dataset Attributes:** Comprises images containing text in various forms, such as signs, documents, and advertisements. The questions require models to read and comprehend the text within the image to provide accurate answers. The dataset includes a diverse set of images and questions to evaluate comprehensive visual-textual reasoning.\n*   **Reference:** [“TextVQA: Toward VQA Models That Can Read”](https://arxiv.org/abs/1904.08920)\n\n#### Humanity’s Last Exam (HLE)\n\n*   **Description:** HLE is a multi-modal benchmark designed to be the final closed-ended academic benchmark of its kind, testing the limits of LLM capabilities in visual and textual reasoning. It consists of 2,700 highly challenging questions spanning mathematics, humanities, and the natural sciences, developed by global subject-matter experts.\n*   **Dataset Attributes:** Questions are formatted as either multiple-choice or short-answer, with some requiring image references for comprehension. Each question is verified for difficulty, ensuring it cannot be easily solved by current LLMs. HLE aims to provide a precise measurement of model capabilities as they approach expert human-level knowledge.\n*   **Reference:** [“Humanity’s Last Exam”](https://arxiv.org/abs/2501.14249)",
    "order": 11,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "offlineonline evaluation",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 257,
      "contentLength": 2353
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#visual-question-answering",
    "scrapedAt": "2025-12-28T11:55:20.780Z"
  },
  {
    "id": "ai-benchmarks-image-captioning-12",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Vision-Language Models (VLMs)",
    "title": "Image Captioning",
    "subtitle": "Vision-Language Models (VLMs)",
    "contentHtml": "<h4 id=\"mscoco-captions\">MSCOCO Captions</h4>\n<ul>\n  <li><strong>Description:</strong> Models generate captions for images, focusing on accuracy and relevance of the visual descriptions.</li>\n  <li><strong>Dataset Attributes:</strong> Real-world images with annotations requiring descriptive and detailed captions that cover a broad range of everyday scenes and objects. The dataset includes over 330,000 images with five captions each, emphasizing diversity in descriptions.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1405.0312\">“Microsoft COCO: Common Objects in Context”</a></li>\n</ul>\n<h4 id=\"visualgenome-captions\">VisualGenome Captions</h4>\n<ul>\n  <li><strong>Description:</strong> Provides detailed scene descriptions to enable models to generate fine-grained and context-aware captions.</li>\n  <li><strong>Dataset Attributes:</strong> Contains 108,077 images with region-based annotations, where each image is densely labeled with multiple region captions. The dataset emphasizes relationships between objects and detailed scene understanding.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1602.07332\">“Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations”</a></li>\n</ul>\n<h4 id=\"flickr30k-captions\">Flickr30K Captions</h4>\n<ul>\n  <li><strong>Description:</strong> Focuses on describing visual content in natural language with an emphasis on human-centered and everyday scenes.</li>\n  <li><strong>Dataset Attributes:</strong> Contains 30,000 images, each paired with five captions describing people, objects, and activities. The dataset is widely used for evaluating captioning models’ ability to generate human-like descriptions.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1505.04870\">“Flickr30K Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models”</a></li>\n</ul>\n<h4 id=\"conceptual-captions\">Conceptual Captions</h4>\n<ul>\n  <li><strong>Description:</strong> Captions are automatically generated from web data, making the dataset suitable for training large-scale captioning models.</li>\n  <li><strong>Dataset Attributes:</strong> Contains approximately 3.3 million images with automatically generated captions extracted from web pages. The dataset focuses on diverse and noisy real-world captions, making it useful for training models on large-scale noisy data.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1803.09195\">“Conceptual Captions: A Cleaned, Hypernymed, Image Alt-Text Dataset For Automatic Image Captioning”</a></li>\n</ul>",
    "contentMarkdown": "#### MSCOCO Captions\n\n*   **Description:** Models generate captions for images, focusing on accuracy and relevance of the visual descriptions.\n*   **Dataset Attributes:** Real-world images with annotations requiring descriptive and detailed captions that cover a broad range of everyday scenes and objects. The dataset includes over 330,000 images with five captions each, emphasizing diversity in descriptions.\n*   **Reference:** [“Microsoft COCO: Common Objects in Context”](https://arxiv.org/abs/1405.0312)\n\n#### VisualGenome Captions\n\n*   **Description:** Provides detailed scene descriptions to enable models to generate fine-grained and context-aware captions.\n*   **Dataset Attributes:** Contains 108,077 images with region-based annotations, where each image is densely labeled with multiple region captions. The dataset emphasizes relationships between objects and detailed scene understanding.\n*   **Reference:** [“Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations”](https://arxiv.org/abs/1602.07332)\n\n#### Flickr30K Captions\n\n*   **Description:** Focuses on describing visual content in natural language with an emphasis on human-centered and everyday scenes.\n*   **Dataset Attributes:** Contains 30,000 images, each paired with five captions describing people, objects, and activities. The dataset is widely used for evaluating captioning models’ ability to generate human-like descriptions.\n*   **Reference:** [“Flickr30K Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models”](https://arxiv.org/abs/1505.04870)\n\n#### Conceptual Captions\n\n*   **Description:** Captions are automatically generated from web data, making the dataset suitable for training large-scale captioning models.\n*   **Dataset Attributes:** Contains approximately 3.3 million images with automatically generated captions extracted from web pages. The dataset focuses on diverse and noisy real-world captions, making it useful for training models on large-scale noisy data.\n*   **Reference:** [“Conceptual Captions: A Cleaned, Hypernymed, Image Alt-Text Dataset For Automatic Image Captioning”](https://arxiv.org/abs/1803.09195)",
    "order": 12,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "offlineonline evaluation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 255,
      "contentLength": 2620
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#image-captioning",
    "scrapedAt": "2025-12-28T11:55:20.780Z"
  },
  {
    "id": "ai-benchmarks-visual-reasoning-13",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Vision-Language Models (VLMs)",
    "title": "Visual Reasoning",
    "subtitle": "Vision-Language Models (VLMs)",
    "contentHtml": "<h4 id=\"nlvr2-natural-language-for-visual-reasoning-for-real\">NLVR2 (Natural Language for Visual Reasoning for Real)</h4>\n<ul>\n  <li><strong>Description:</strong> Evaluates reasoning about the relationship between textual descriptions and image pairs.</li>\n  <li><strong>Dataset Attributes:</strong> Pairs of photographs with text statements that models must verify, focusing on logical reasoning across visually disparate images. The dataset includes complex visual scenes requiring fine-grained reasoning about relationships and attributes.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1811.00491\">“A Corpus for Reasoning About Natural Language Grounded in Photographs”</a></li>\n</ul>\n<h4 id=\"mmbench\">MMBench</h4>\n<ul>\n  <li><strong>Description:</strong> Provides a comprehensive evaluation of models’ multimodal understanding across different tasks.</li>\n  <li><strong>Dataset Attributes:</strong> Includes tasks such as visual question answering, image captioning, and visual reasoning, focusing on the integration and understanding of visual and textual data. The dataset is designed to challenge models with a wide range of scenarios requiring both linguistic and visual comprehension.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2307.06281\">“MMBench: A Comprehensive Multimodal Benchmark for Evaluating Vision-Language Models”</a></li>\n</ul>\n<h4 id=\"mmmu-massive-multi-discipline-multimodal-understanding\">MMMU (Massive Multi-discipline Multimodal Understanding)</h4>\n<ul>\n  <li><strong>Description:</strong> Tests models’ ability to understand and generate responses based on both visual and textual stimuli.</li>\n  <li><strong>Dataset Attributes:</strong> Involves tasks like visual question answering, image captioning, and visual reasoning, testing both visual and textual understanding. The dataset includes diverse multimodal tasks designed to evaluate comprehensive understanding and generation abilities.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2311.16502\">“MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI”</a></li>\n</ul>",
    "contentMarkdown": "#### NLVR2 (Natural Language for Visual Reasoning for Real)\n\n*   **Description:** Evaluates reasoning about the relationship between textual descriptions and image pairs.\n*   **Dataset Attributes:** Pairs of photographs with text statements that models must verify, focusing on logical reasoning across visually disparate images. The dataset includes complex visual scenes requiring fine-grained reasoning about relationships and attributes.\n*   **Reference:** [“A Corpus for Reasoning About Natural Language Grounded in Photographs”](https://arxiv.org/abs/1811.00491)\n\n#### MMBench\n\n*   **Description:** Provides a comprehensive evaluation of models’ multimodal understanding across different tasks.\n*   **Dataset Attributes:** Includes tasks such as visual question answering, image captioning, and visual reasoning, focusing on the integration and understanding of visual and textual data. The dataset is designed to challenge models with a wide range of scenarios requiring both linguistic and visual comprehension.\n*   **Reference:** [“MMBench: A Comprehensive Multimodal Benchmark for Evaluating Vision-Language Models”](https://arxiv.org/abs/2307.06281)\n\n#### MMMU (Massive Multi-discipline Multimodal Understanding)\n\n*   **Description:** Tests models’ ability to understand and generate responses based on both visual and textual stimuli.\n*   **Dataset Attributes:** Involves tasks like visual question answering, image captioning, and visual reasoning, testing both visual and textual understanding. The dataset includes diverse multimodal tasks designed to evaluate comprehensive understanding and generation abilities.\n*   **Reference:** [“MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI”](https://arxiv.org/abs/2311.16502)",
    "order": 13,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "offlineonline evaluation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 210,
      "contentLength": 2167
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#visual-reasoning",
    "scrapedAt": "2025-12-28T11:55:20.780Z"
  },
  {
    "id": "ai-benchmarks-video-understanding-14",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Vision-Language Models (VLMs)",
    "title": "Video Understanding",
    "subtitle": "Vision-Language Models (VLMs)",
    "contentHtml": "<h4 id=\"perception-test\">Perception Test</h4>\n<ul>\n  <li><strong>Description:</strong> A benchmark designed to evaluate models on understanding and interpreting video content.</li>\n  <li><strong>Dataset Attributes:</strong> Video sequences requiring models to interpret dynamic scenes, focusing on object detection, movement prediction, and scene classification. The dataset includes real-world driving scenarios, making it relevant for autonomous vehicle research.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1905.00780\">“Perception Test: Benchmark for Autonomous Vehicle Perception”</a></li>\n</ul>",
    "contentMarkdown": "#### Perception Test\n\n*   **Description:** A benchmark designed to evaluate models on understanding and interpreting video content.\n*   **Dataset Attributes:** Video sequences requiring models to interpret dynamic scenes, focusing on object detection, movement prediction, and scene classification. The dataset includes real-world driving scenarios, making it relevant for autonomous vehicle research.\n*   **Reference:** [“Perception Test: Benchmark for Autonomous Vehicle Perception”](https://arxiv.org/abs/1905.00780)",
    "order": 14,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "offlineonline evaluation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 59,
      "contentLength": 625
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#video-understanding",
    "scrapedAt": "2025-12-28T11:55:20.780Z"
  },
  {
    "id": "ai-benchmarks-document-understanding-15",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Vision-Language Models (VLMs)",
    "title": "Document Understanding",
    "subtitle": "Vision-Language Models (VLMs)",
    "contentHtml": "<h4 id=\"table-understanding\">Table Understanding</h4>\n<h5 id=\"tat-qa-tabular-and-text-question-answering\">TAT-QA (Tabular and Text Question Answering)</h5>\n<ul>\n  <li><strong>Description:</strong> A benchmark for answering questions that require reasoning over tabular data and associated textual context.</li>\n  <li><strong>Dataset Attributes:</strong> Includes complex questions that demand multi-hop reasoning between tables and text. The dataset covers diverse domains like business and finance.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2103.13614\">“TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance”</a>.</li>\n</ul>\n<h5 id=\"tabfact\">TabFact</h5>\n<ul>\n  <li><strong>Description:</strong> A fact-checking benchmark for tables, where models must verify whether a natural language statement is entailed or contradicted by a table.</li>\n  <li><strong>Dataset Attributes:</strong> Contains 16,000 tables and 118,000 statements, testing models’ ability to comprehend, retrieve, and reason with structured data.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1909.02164\">“TabFact: A Large-scale Dataset for Table-based Fact Verification”</a>.</li>\n</ul>\n<h5 id=\"wikitables-questions-wtq\">WikiTables Questions (WTQ)</h5>\n<ul>\n  <li><strong>Description:</strong> A dataset for semantic parsing over tables, requiring models to generate SQL-like queries to answer natural language questions.</li>\n  <li><strong>Dataset Attributes:</strong> Consists of 22,000 questions based on Wikipedia tables, emphasizing logical reasoning and structured query generation.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1608.03902\">“The WikiTables Dataset: A Complex Real-World Table Question Answering Benchmark”</a>.</li>\n</ul>\n<h5 id=\"chartqa\">ChartQA</h5>\n<ul>\n  <li><strong>Description:</strong> A dataset focused on question answering over chart-based visual data, requiring models to interpret visual encodings and numerical data.</li>\n  <li><strong>Dataset Attributes:</strong> Contains questions designed to test comprehension of visualizations such as bar charts, line graphs, and pie charts, demanding reasoning over chart elements and associated metadata.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2203.10244\">“ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning”</a>.</li>\n</ul>\n<h4 id=\"scientific-documents\">Scientific Documents</h4>\n<h5 id=\"scrolls-summarization-and-cross-document-reasoning\">SCROLLS (Summarization and Cross-document Reasoning)</h5>\n<ul>\n  <li><strong>Description:</strong> Evaluates long-context reasoning across scientific documents, requiring multi-document synthesis for summarization and question answering.</li>\n  <li><strong>Dataset Attributes:</strong> Includes tasks such as summarization and question answering over extended scientific texts.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2201.03533\">“SCROLLS: Standardized CompaRison Over Long Language Sequences”</a>.</li>\n</ul>\n<h5 id=\"pubmedqa-1\">PubMedQA</h5>\n<ul>\n  <li><strong>Description:</strong> A dataset focusing on biomedical literature, designed for question answering tasks requiring comprehension of scientific abstracts.</li>\n  <li><strong>Dataset Attributes:</strong> Contains 1,000 annotated questions derived from PubMed, with yes/no/maybe answers, promoting domain-specific reasoning.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1909.06146\">“PubMedQA: A Dataset for Biomedical Research Question Answering”</a>.</li>\n</ul>\n<h5 id=\"arxivqa\">ArxivQA</h5>\n<ul>\n  <li><strong>Description:</strong> A question-answering benchmark built on scientific papers from arXiv.org, challenging models to extract and reason with domain-specific information.</li>\n  <li><strong>Dataset Attributes:</strong> Contains real-world questions requiring complex reasoning and synthesis from scientific documents.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2305.06499\">“ArxivQA: Open-Domain Question Answering over ArXiv Papers”</a>.</li>\n</ul>\n<h4 id=\"legal-and-financial-documents\">Legal and Financial Documents</h4>\n<h5 id=\"contract-understanding-atticus-dataset-cuad\">Contract Understanding Atticus Dataset (CUAD)</h5>\n<ul>\n  <li><strong>Description:</strong> Designed to test models’ ability to identify clauses and extract structured information from legal contracts.</li>\n  <li><strong>Dataset Attributes:</strong> Includes 13,000 annotations over 510 contracts, emphasizing legal reasoning and clause comprehension.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2103.06268\">“CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review”</a>.</li>\n</ul>\n<h5 id=\"finqa-financial-question-answering\">FINQA (Financial Question Answering)</h5>\n<ul>\n  <li><strong>Description:</strong> A dataset focusing on quantitative reasoning over financial documents, requiring both numerical and textual comprehension.</li>\n  <li><strong>Dataset Attributes:</strong> Includes complex questions requiring reasoning over tables, textual explanations, and multi-step calculations.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2109.00120\">“FINQA: A Dataset for Numerical Reasoning over Financial Data”</a>.</li>\n</ul>\n<h5 id=\"caselawqa\">CaseLawQA</h5>\n<ul>\n  <li><strong>Description:</strong> Focused on extracting and reasoning with information from legal case documents.</li>\n  <li><strong>Dataset Attributes:</strong> Includes questions that test the understanding of legal precedents, reasoning, and structured information retrieval.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2305.05612\">“CaseLawQA: A Dataset for Question Answering on Legal Case Documents”</a>.</li>\n</ul>\n<h4 id=\"multimodal-documents\">Multimodal Documents</h4>\n<h5 id=\"docvqa\">DocVQA</h5>\n<ul>\n  <li><strong>Description:</strong> A dataset for visual question answering over scanned documents, requiring models to reason with textual and visual elements.</li>\n  <li><strong>Dataset Attributes:</strong> Contains 50,000 questions across 12,000 documents, promoting OCR capabilities and multimodal reasoning.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2007.00398\">“DocVQA: A Dataset for VQA on Document Images”</a>.</li>\n</ul>\n<h5 id=\"infographicvqa\">InfographicVQA</h5>\n<ul>\n  <li><strong>Description:</strong> Focuses on question answering using information from infographics, combining text, visuals, and numerical data.</li>\n  <li><strong>Dataset Attributes:</strong> Contains questions requiring multi-modal understanding of infographic charts, diagrams, and annotations.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2106.12638\">“InfographicVQA: A Benchmark for Question Answering on Infographic Visualizations”</a>.</li>\n</ul>\n<h5 id=\"visualmrc-multimodal-reading-comprehension\">VisualMRC (Multimodal Reading Comprehension)</h5>\n<ul>\n  <li><strong>Description:</strong> Evaluates reading comprehension tasks over multimodal documents with text, images, and charts.</li>\n  <li><strong>Dataset Attributes:</strong> Includes tasks like answering questions based on multimodal content, integrating visual understanding with textual reasoning.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2203.05234\">“Multimodal Reading Comprehension with Multimodal Pre-trained Models”</a>.</li>\n</ul>\n<h5 id=\"omnidocbench\">OmniDocBench</h5>\n<ul>\n  <li><strong>Description:</strong> A comprehensive benchmark for document content extraction across diverse PDF types using multimodal input, enabling both module-level and end-to-end evaluation.</li>\n  <li><strong>Dataset Attributes:</strong> Contains 981 richly annotated pages from 9 document types (e.g., papers, textbooks, exam sheets), annotated with layout, reading order, and recognition attributes including text, tables, and formulas. Supports evaluations under various layout types and visual conditions (e.g., fuzzy scans, watermarks).</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2412.07626\">“OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations”</a>.</li>\n</ul>",
    "contentMarkdown": "#### Table Understanding\n\n##### TAT-QA (Tabular and Text Question Answering)\n\n*   **Description:** A benchmark for answering questions that require reasoning over tabular data and associated textual context.\n*   **Dataset Attributes:** Includes complex questions that demand multi-hop reasoning between tables and text. The dataset covers diverse domains like business and finance.\n*   **Reference:** [“TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance”](https://arxiv.org/abs/2103.13614).\n\n##### TabFact\n\n*   **Description:** A fact-checking benchmark for tables, where models must verify whether a natural language statement is entailed or contradicted by a table.\n*   **Dataset Attributes:** Contains 16,000 tables and 118,000 statements, testing models’ ability to comprehend, retrieve, and reason with structured data.\n*   **Reference:** [“TabFact: A Large-scale Dataset for Table-based Fact Verification”](https://arxiv.org/abs/1909.02164).\n\n##### WikiTables Questions (WTQ)\n\n*   **Description:** A dataset for semantic parsing over tables, requiring models to generate SQL-like queries to answer natural language questions.\n*   **Dataset Attributes:** Consists of 22,000 questions based on Wikipedia tables, emphasizing logical reasoning and structured query generation.\n*   **Reference:** [“The WikiTables Dataset: A Complex Real-World Table Question Answering Benchmark”](https://arxiv.org/abs/1608.03902).\n\n##### ChartQA\n\n*   **Description:** A dataset focused on question answering over chart-based visual data, requiring models to interpret visual encodings and numerical data.\n*   **Dataset Attributes:** Contains questions designed to test comprehension of visualizations such as bar charts, line graphs, and pie charts, demanding reasoning over chart elements and associated metadata.\n*   **Reference:** [“ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning”](https://arxiv.org/abs/2203.10244).\n\n#### Scientific Documents\n\n##### SCROLLS (Summarization and Cross-document Reasoning)\n\n*   **Description:** Evaluates long-context reasoning across scientific documents, requiring multi-document synthesis for summarization and question answering.\n*   **Dataset Attributes:** Includes tasks such as summarization and question answering over extended scientific texts.\n*   **Reference:** [“SCROLLS: Standardized CompaRison Over Long Language Sequences”](https://arxiv.org/abs/2201.03533).\n\n##### PubMedQA\n\n*   **Description:** A dataset focusing on biomedical literature, designed for question answering tasks requiring comprehension of scientific abstracts.\n*   **Dataset Attributes:** Contains 1,000 annotated questions derived from PubMed, with yes/no/maybe answers, promoting domain-specific reasoning.\n*   **Reference:** [“PubMedQA: A Dataset for Biomedical Research Question Answering”](https://arxiv.org/abs/1909.06146).\n\n##### ArxivQA\n\n*   **Description:** A question-answering benchmark built on scientific papers from arXiv.org, challenging models to extract and reason with domain-specific information.\n*   **Dataset Attributes:** Contains real-world questions requiring complex reasoning and synthesis from scientific documents.\n*   **Reference:** [“ArxivQA: Open-Domain Question Answering over ArXiv Papers”](https://arxiv.org/abs/2305.06499).\n\n#### Legal and Financial Documents\n\n##### Contract Understanding Atticus Dataset (CUAD)\n\n*   **Description:** Designed to test models’ ability to identify clauses and extract structured information from legal contracts.\n*   **Dataset Attributes:** Includes 13,000 annotations over 510 contracts, emphasizing legal reasoning and clause comprehension.\n*   **Reference:** [“CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review”](https://arxiv.org/abs/2103.06268).\n\n##### FINQA (Financial Question Answering)\n\n*   **Description:** A dataset focusing on quantitative reasoning over financial documents, requiring both numerical and textual comprehension.\n*   **Dataset Attributes:** Includes complex questions requiring reasoning over tables, textual explanations, and multi-step calculations.\n*   **Reference:** [“FINQA: A Dataset for Numerical Reasoning over Financial Data”](https://arxiv.org/abs/2109.00120).\n\n##### CaseLawQA\n\n*   **Description:** Focused on extracting and reasoning with information from legal case documents.\n*   **Dataset Attributes:** Includes questions that test the understanding of legal precedents, reasoning, and structured information retrieval.\n*   **Reference:** [“CaseLawQA: A Dataset for Question Answering on Legal Case Documents”](https://arxiv.org/abs/2305.05612).\n\n#### Multimodal Documents\n\n##### DocVQA\n\n*   **Description:** A dataset for visual question answering over scanned documents, requiring models to reason with textual and visual elements.\n*   **Dataset Attributes:** Contains 50,000 questions across 12,000 documents, promoting OCR capabilities and multimodal reasoning.\n*   **Reference:** [“DocVQA: A Dataset for VQA on Document Images”](https://arxiv.org/abs/2007.00398).\n\n##### InfographicVQA\n\n*   **Description:** Focuses on question answering using information from infographics, combining text, visuals, and numerical data.\n*   **Dataset Attributes:** Contains questions requiring multi-modal understanding of infographic charts, diagrams, and annotations.\n*   **Reference:** [“InfographicVQA: A Benchmark for Question Answering on Infographic Visualizations”](https://arxiv.org/abs/2106.12638).\n\n##### VisualMRC (Multimodal Reading Comprehension)\n\n*   **Description:** Evaluates reading comprehension tasks over multimodal documents with text, images, and charts.\n*   **Dataset Attributes:** Includes tasks like answering questions based on multimodal content, integrating visual understanding with textual reasoning.\n*   **Reference:** [“Multimodal Reading Comprehension with Multimodal Pre-trained Models”](https://arxiv.org/abs/2203.05234).\n\n##### OmniDocBench\n\n*   **Description:** A comprehensive benchmark for document content extraction across diverse PDF types using multimodal input, enabling both module-level and end-to-end evaluation.\n*   **Dataset Attributes:** Contains 981 richly annotated pages from 9 document types (e.g., papers, textbooks, exam sheets), annotated with layout, reading order, and recognition attributes including text, tables, and formulas. Supports evaluations under various layout types and visual conditions (e.g., fuzzy scans, watermarks).\n*   **Reference:** [“OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations”](https://arxiv.org/abs/2412.07626).",
    "order": 15,
    "orderInChapter": 5,
    "difficulty": 2,
    "estimatedMinutes": 4,
    "tags": [
      "offlineonline evaluation",
      "nlp"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 745,
      "contentLength": 8315
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#document-understanding",
    "scrapedAt": "2025-12-28T11:55:20.780Z"
  },
  {
    "id": "ai-benchmarks-medical-vlm-benchmarks-16",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Vision-Language Models (VLMs)",
    "title": "Medical VLM Benchmarks",
    "subtitle": "Vision-Language Models (VLMs)",
    "contentHtml": "<ul>\n  <li>Medical VLMs are essential in merging AI’s visual and linguistic analysis for healthcare applications. They are pivotal for developing systems that can interpret complex medical imagery alongside textual data, enhancing diagnostic accuracy and treatment efficiency. This section explores major benchmarks testing these interdisciplinary skills.</li>\n</ul>\n<h4 id=\"medical-image-annotation-and-retrieval\">Medical Image Annotation and Retrieval</h4>\n<h5 id=\"imageclefmed\">ImageCLEFmed</h5>\n<ul>\n  <li><strong>Description:</strong> Part of the ImageCLEF challenge, this benchmark tests image-based information retrieval, automatic annotation, and visual question answering using medical images.</li>\n  <li><strong>Dataset Attributes:</strong> Contains a wide array of medical imaging types, including radiographs, histopathology images, and MRI scans, necessitating the interpretation of complex visual medical data. Tasks range from multi-label classification to segmentation and retrieval.</li>\n  <li><strong>Reference:</strong> <a href=\"https://link.springer.com/chapter/10.1007/978-3-642-15754-7_37\">“ImageCLEF - the CLEF 2009 Cross-Language Image Retrieval Track”</a></li>\n</ul>\n<h4 id=\"disease-classification-and-detection\">Disease Classification and Detection</h4>\n<h5 id=\"chexpert\">CheXpert</h5>\n<ul>\n  <li><strong>Description:</strong> A large dataset of chest radiographs for identifying and classifying key thoracic pathologies. This benchmark is often used for tasks that involve reading and interpreting X-ray images.</li>\n  <li><strong>Dataset Attributes:</strong> Consists of over 200,000 chest radiographs annotated with findings from radiology reports, challenging models to accurately detect and diagnose multiple conditions such as pneumonia, pleural effusion, and cardiomegaly.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/1901.07031\">“CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison”</a></li>\n</ul>\n<h5 id=\"diabetic-retinopathy-detection\">Diabetic Retinopathy Detection</h5>\n<ul>\n  <li><strong>Description:</strong> Focused on the classification of retinal images to diagnose diabetic retinopathy, a common cause of vision loss.</li>\n  <li><strong>Dataset Attributes:</strong> Features high-resolution retinal images, where models need to detect subtle indicators of disease progression, requiring high levels of visual detail recognition. The dataset includes labels for different stages of retinopathy, emphasizing early detection and severity assessment.</li>\n  <li><strong>Reference:</strong> <a href=\"https://www.kaggle.com/c/diabetic-retinopathy-detection/data\">Diabetic Retinopathy Detection on Kaggle</a></li>\n</ul>",
    "contentMarkdown": "*   Medical VLMs are essential in merging AI’s visual and linguistic analysis for healthcare applications. They are pivotal for developing systems that can interpret complex medical imagery alongside textual data, enhancing diagnostic accuracy and treatment efficiency. This section explores major benchmarks testing these interdisciplinary skills.\n\n#### Medical Image Annotation and Retrieval\n\n##### ImageCLEFmed\n\n*   **Description:** Part of the ImageCLEF challenge, this benchmark tests image-based information retrieval, automatic annotation, and visual question answering using medical images.\n*   **Dataset Attributes:** Contains a wide array of medical imaging types, including radiographs, histopathology images, and MRI scans, necessitating the interpretation of complex visual medical data. Tasks range from multi-label classification to segmentation and retrieval.\n*   **Reference:** [“ImageCLEF - the CLEF 2009 Cross-Language Image Retrieval Track”](https://link.springer.com/chapter/10.1007/978-3-642-15754-7_37)\n\n#### Disease Classification and Detection\n\n##### CheXpert\n\n*   **Description:** A large dataset of chest radiographs for identifying and classifying key thoracic pathologies. This benchmark is often used for tasks that involve reading and interpreting X-ray images.\n*   **Dataset Attributes:** Consists of over 200,000 chest radiographs annotated with findings from radiology reports, challenging models to accurately detect and diagnose multiple conditions such as pneumonia, pleural effusion, and cardiomegaly.\n*   **Reference:** [“CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison”](https://arxiv.org/abs/1901.07031)\n\n##### Diabetic Retinopathy Detection\n\n*   **Description:** Focused on the classification of retinal images to diagnose diabetic retinopathy, a common cause of vision loss.\n*   **Dataset Attributes:** Features high-resolution retinal images, where models need to detect subtle indicators of disease progression, requiring high levels of visual detail recognition. The dataset includes labels for different stages of retinopathy, emphasizing early detection and severity assessment.\n*   **Reference:** [Diabetic Retinopathy Detection on Kaggle](https://www.kaggle.com/c/diabetic-retinopathy-detection/data)",
    "order": 16,
    "orderInChapter": 6,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "offlineonline evaluation"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 271,
      "contentLength": 2718
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#medical-vlm-benchmarks",
    "scrapedAt": "2025-12-28T11:55:20.781Z"
  },
  {
    "id": "ai-benchmarks-multimodal-agents-17",
    "domain": "ai_primers",
    "category": "Offline/Online Evaluation",
    "article": "LLM/VLM Benchmarks",
    "articleSlug": "benchmarks",
    "chapter": "Vision-Language Models (VLMs)",
    "title": "Multimodal Agents",
    "subtitle": "Vision-Language Models (VLMs)",
    "contentHtml": "<ul>\n  <li>LLM-based agents are increasingly being developed for autonomous tasks such as web navigation, tool use, and real-world problem-solving. However, their evaluation requires specialized benchmarks that assess not only static model performance but also interactive capabilities, adaptability, and long-term decision-making.</li>\n  <li>These benchmarks evaluate agents that operate across multiple modalities, such as vision, action, and interactive simulations, beyond pure text-based reasoning.</li>\n</ul>\n<h4 id=\"osworld\">OSWorld</h4>\n<ul>\n  <li><strong>Description:</strong> OSWorld evaluates multimodal agents that support task setup, execution-based evaluation, and interactive learning across operating systems. It can serve as a unified environment for evaluating open-ended computer tasks that involve arbitrary applications.</li>\n  <li><strong>Dataset Attributes:</strong> Designed to assess adaptability to new conditions and autonomous decision-making in an unpredictable setting.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2404.07972\">“OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments”</a></li>\n</ul>\n<h4 id=\"webarena\">WebArena</h4>\n<ul>\n  <li><strong>Description:</strong> WebArena is a benchmark that assesses LLM agents’ ability to interact with web interfaces, perform searches, and complete form-filling tasks in realistic browser-based environments.</li>\n  <li><strong>Dataset Attributes:</strong> Uses high-fidelity web simulations to test goal-directed web navigation and interaction capabilities.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2307.13854\">“WebArena: A Realistic Web Environment for Building Autonomous Agents”</a></li>\n</ul>\n<h4 id=\"webvoyager\">WebVoyager</h4>\n<ul>\n  <li><strong>Description:</strong> WebVoyager evaluates the autonomous exploration abilities of LLM agents, requiring them to browse, extract, and process information from diverse websites.</li>\n  <li><strong>Dataset Attributes:</strong> Tests adaptability to unfamiliar web structures, goal-directed browsing, and information retrieval efficiency.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2401.13919\">“WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models”</a></li>\n</ul>\n<h4 id=\"general-ai-agent-benchmark-gaia\">General AI Agent Benchmark (GAIA)</h4>\n<ul>\n  <li><strong>Description:</strong> GAIA assesses the robustness and problem-solving skills of AI agents across multiple domains, including gaming, online environments, and decision-making tasks.</li>\n  <li><strong>Dataset Attributes:</strong> Emphasizes general intelligence, multi-step reasoning, and adaptability across different test environments.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2311.12983\">“GAIA: A General AI Agent Benchmark”</a></li>\n</ul>\n<h4 id=\"interactive-grounded-language-understanding-iglu\">Interactive Grounded Language Understanding (IGLU)</h4>\n<ul>\n  <li><strong>Description:</strong> IGLU tests LLM-based agents in interactive 3D environments where they must understand natural language commands and manipulate objects accordingly.</li>\n  <li><strong>Dataset Attributes:</strong> Focuses on spatial reasoning, multi-modal understanding, and real-time task execution.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2107.09081\">“IGLU: Interactive Grounded Language Understanding in a Collaborative Environment”</a></li>\n</ul>\n<h4 id=\"mlagentbench\">MLAgentBench</h4>\n<ul>\n  <li><strong>Description:</strong> MLAgentBench is a benchmark designed for reinforcement learning agents powered by LLMs. It evaluates learning efficiency, adaptability, and performance across standardized RL environments.</li>\n  <li><strong>Dataset Attributes:</strong> Emphasizes reinforcement learning principles in conjunction with LLM-based reasoning.</li>\n  <li><strong>Reference:</strong> <a href=\"https://arxiv.org/abs/2312.06789\">“MLAgentBench: Evaluating RL Agents in Multi-Modal Environments”</a></li>\n</ul>",
    "contentMarkdown": "*   LLM-based agents are increasingly being developed for autonomous tasks such as web navigation, tool use, and real-world problem-solving. However, their evaluation requires specialized benchmarks that assess not only static model performance but also interactive capabilities, adaptability, and long-term decision-making.\n*   These benchmarks evaluate agents that operate across multiple modalities, such as vision, action, and interactive simulations, beyond pure text-based reasoning.\n\n#### OSWorld\n\n*   **Description:** OSWorld evaluates multimodal agents that support task setup, execution-based evaluation, and interactive learning across operating systems. It can serve as a unified environment for evaluating open-ended computer tasks that involve arbitrary applications.\n*   **Dataset Attributes:** Designed to assess adaptability to new conditions and autonomous decision-making in an unpredictable setting.\n*   **Reference:** [“OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments”](https://arxiv.org/abs/2404.07972)\n\n#### WebArena\n\n*   **Description:** WebArena is a benchmark that assesses LLM agents’ ability to interact with web interfaces, perform searches, and complete form-filling tasks in realistic browser-based environments.\n*   **Dataset Attributes:** Uses high-fidelity web simulations to test goal-directed web navigation and interaction capabilities.\n*   **Reference:** [“WebArena: A Realistic Web Environment for Building Autonomous Agents”](https://arxiv.org/abs/2307.13854)\n\n#### WebVoyager\n\n*   **Description:** WebVoyager evaluates the autonomous exploration abilities of LLM agents, requiring them to browse, extract, and process information from diverse websites.\n*   **Dataset Attributes:** Tests adaptability to unfamiliar web structures, goal-directed browsing, and information retrieval efficiency.\n*   **Reference:** [“WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models”](https://arxiv.org/abs/2401.13919)\n\n#### General AI Agent Benchmark (GAIA)\n\n*   **Description:** GAIA assesses the robustness and problem-solving skills of AI agents across multiple domains, including gaming, online environments, and decision-making tasks.\n*   **Dataset Attributes:** Emphasizes general intelligence, multi-step reasoning, and adaptability across different test environments.\n*   **Reference:** [“GAIA: A General AI Agent Benchmark”](https://arxiv.org/abs/2311.12983)\n\n#### Interactive Grounded Language Understanding (IGLU)\n\n*   **Description:** IGLU tests LLM-based agents in interactive 3D environments where they must understand natural language commands and manipulate objects accordingly.\n*   **Dataset Attributes:** Focuses on spatial reasoning, multi-modal understanding, and real-time task execution.\n*   **Reference:** [“IGLU: Interactive Grounded Language Understanding in a Collaborative Environment”](https://arxiv.org/abs/2107.09081)\n\n#### MLAgentBench\n\n*   **Description:** MLAgentBench is a benchmark designed for reinforcement learning agents powered by LLMs. It evaluates learning efficiency, adaptability, and performance across standardized RL environments.\n*   **Dataset Attributes:** Emphasizes reinforcement learning principles in conjunction with LLM-based reasoning.\n*   **Reference:** [“MLAgentBench: Evaluating RL Agents in Multi-Modal Environments”](https://arxiv.org/abs/2312.06789)",
    "order": 17,
    "orderInChapter": 7,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "offlineonline evaluation",
      "llm",
      "reinforcement learning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 381,
      "contentLength": 4081
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/benchmarks/#multimodal-agents",
    "scrapedAt": "2025-12-28T11:55:20.781Z"
  }
]