[
  {
    "id": "ai-agents-agent-core-llm-1",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "The Agent Framework",
    "title": "Agent Core (LLM)",
    "subtitle": "The Agent Framework",
    "contentHtml": "<ul>\n  <li>\n    <p>At the heart of the agent, the <strong>Agent Core</strong> functions as the primary decision-making engine, where OpenAI’s <strong>GPT-4</strong> is employed to handle high-level reasoning and dynamic task management. This component includes:</p>\n\n    <ul>\n      <li><strong>Decision-Making Engine</strong>: Analyzes input data, memory, and goals to generate suitable responses.</li>\n      <li><strong>Goal Management System</strong>: Continuously updates the agent’s objectives based on task progression.</li>\n      <li><strong>Integration Bus</strong>: Manages the flow of information between memory, tools, and planning modules, ensuring cohesive data exchange.</li>\n    </ul>\n  </li>\n  <li>\n    <p>The Agent Core uses the LLM’s capabilities to complete tasks, generate new tasks as needed, and dynamically adjust priorities based on the evolving task context.</p>\n  </li>\n</ul>\n<p>At the heart of the agent, the <strong>Agent Core</strong> functions as the primary decision-making engine, where OpenAI’s <strong>GPT-4</strong> is employed to handle high-level reasoning and dynamic task management. This component includes:</p>\n<ul>\n      <li><strong>Decision-Making Engine</strong>: Analyzes input data, memory, and goals to generate suitable responses.</li>\n      <li><strong>Goal Management System</strong>: Continuously updates the agent’s objectives based on task progression.</li>\n      <li><strong>Integration Bus</strong>: Manages the flow of information between memory, tools, and planning modules, ensuring cohesive data exchange.</li>\n    </ul>\n<p>The Agent Core uses the LLM’s capabilities to complete tasks, generate new tasks as needed, and dynamically adjust priorities based on the evolving task context.</p>",
    "contentMarkdown": "*   At the heart of the agent, the **Agent Core** functions as the primary decision-making engine, where OpenAI’s **GPT-4** is employed to handle high-level reasoning and dynamic task management. This component includes:\n    \n    *   **Decision-Making Engine**: Analyzes input data, memory, and goals to generate suitable responses.\n    *   **Goal Management System**: Continuously updates the agent’s objectives based on task progression.\n    *   **Integration Bus**: Manages the flow of information between memory, tools, and planning modules, ensuring cohesive data exchange.\n*   The Agent Core uses the LLM’s capabilities to complete tasks, generate new tasks as needed, and dynamically adjust priorities based on the evolving task context.\n    \n\nAt the heart of the agent, the **Agent Core** functions as the primary decision-making engine, where OpenAI’s **GPT-4** is employed to handle high-level reasoning and dynamic task management. This component includes:\n\n*   **Decision-Making Engine**: Analyzes input data, memory, and goals to generate suitable responses.\n*   **Goal Management System**: Continuously updates the agent’s objectives based on task progression.\n*   **Integration Bus**: Manages the flow of information between memory, tools, and planning modules, ensuring cohesive data exchange.\n\nThe Agent Core uses the LLM’s capabilities to complete tasks, generate new tasks as needed, and dynamically adjust priorities based on the evolving task context.",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 202,
      "contentLength": 1747
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#agent-core-(llm)",
    "scrapedAt": "2025-12-28T11:47:30.113Z"
  },
  {
    "id": "ai-agents-memory-modules-2",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "The Agent Framework",
    "title": "Memory Modules",
    "subtitle": "The Agent Framework",
    "contentHtml": "<ul>\n  <li>\n    <p>Memory is a fundamental part of the framework, with a vector databases (such as Pinecone, Weaviate, Chroma, etc.) providing robust storage and retrieval mechanisms for task-related data. The memory modules enhance the agent’s context-awareness and task relevance through:</p>\n\n    <ul>\n      <li><strong>Short-term Memory (STM)</strong>: Manages temporary data for immediate task requirements, stored in volatile structures like stacks or queues to support quick access and frequent clearing.</li>\n      <li><strong>Long-term Memory (LTM)</strong>: Uses vector databases for persistent storage of historical interactions, enabling the agent to reference past conversations or data over extended periods. Semantic similarity-based retrieval is employed to enhance relevance, factoring in recency and importance for efficient access.</li>\n    </ul>\n  </li>\n</ul>\n<p>Memory is a fundamental part of the framework, with a vector databases (such as Pinecone, Weaviate, Chroma, etc.) providing robust storage and retrieval mechanisms for task-related data. The memory modules enhance the agent’s context-awareness and task relevance through:</p>\n<ul>\n      <li><strong>Short-term Memory (STM)</strong>: Manages temporary data for immediate task requirements, stored in volatile structures like stacks or queues to support quick access and frequent clearing.</li>\n      <li><strong>Long-term Memory (LTM)</strong>: Uses vector databases for persistent storage of historical interactions, enabling the agent to reference past conversations or data over extended periods. Semantic similarity-based retrieval is employed to enhance relevance, factoring in recency and importance for efficient access.</li>\n    </ul>",
    "contentMarkdown": "*   Memory is a fundamental part of the framework, with a vector databases (such as Pinecone, Weaviate, Chroma, etc.) providing robust storage and retrieval mechanisms for task-related data. The memory modules enhance the agent’s context-awareness and task relevance through:\n    \n    *   **Short-term Memory (STM)**: Manages temporary data for immediate task requirements, stored in volatile structures like stacks or queues to support quick access and frequent clearing.\n    *   **Long-term Memory (LTM)**: Uses vector databases for persistent storage of historical interactions, enabling the agent to reference past conversations or data over extended periods. Semantic similarity-based retrieval is employed to enhance relevance, factoring in recency and importance for efficient access.\n\nMemory is a fundamental part of the framework, with a vector databases (such as Pinecone, Weaviate, Chroma, etc.) providing robust storage and retrieval mechanisms for task-related data. The memory modules enhance the agent’s context-awareness and task relevance through:\n\n*   **Short-term Memory (STM)**: Manages temporary data for immediate task requirements, stored in volatile structures like stacks or queues to support quick access and frequent clearing.\n*   **Long-term Memory (LTM)**: Uses vector databases for persistent storage of historical interactions, enabling the agent to reference past conversations or data over extended periods. Semantic similarity-based retrieval is employed to enhance relevance, factoring in recency and importance for efficient access.",
    "order": 2,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 211,
      "contentLength": 1724
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#memory-modules",
    "scrapedAt": "2025-12-28T11:47:30.113Z"
  },
  {
    "id": "ai-agents-tools-3",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "The Agent Framework",
    "title": "Tools",
    "subtitle": "The Agent Framework",
    "contentHtml": "<ul>\n  <li>\n    <p>Tools empower the agent with specialized capabilities to execute tasks precisely, often leveraging the <strong>LangChain framework</strong> for structured workflows. Tools include:</p>\n\n    <ul>\n      <li><strong>Executable Workflows</strong>: Defined within LangChain, providing structured, data-aware task handling.</li>\n      <li><strong>APIs</strong>: Facilitate secure access to both internal and external data sources, enriching the agent’s functional range.</li>\n      <li><strong>Middleware</strong>: Supports data exchange between the core and tools, handling formatting, error-checking, and ensuring security.</li>\n    </ul>\n  </li>\n  <li>\n    <p>LangChain’s integration enables the agent to dynamically interact with its environment, providing flexibility and adaptability across diverse tasks.</p>\n  </li>\n</ul>\n<p>Tools empower the agent with specialized capabilities to execute tasks precisely, often leveraging the <strong>LangChain framework</strong> for structured workflows. Tools include:</p>\n<ul>\n      <li><strong>Executable Workflows</strong>: Defined within LangChain, providing structured, data-aware task handling.</li>\n      <li><strong>APIs</strong>: Facilitate secure access to both internal and external data sources, enriching the agent’s functional range.</li>\n      <li><strong>Middleware</strong>: Supports data exchange between the core and tools, handling formatting, error-checking, and ensuring security.</li>\n    </ul>\n<p>LangChain’s integration enables the agent to dynamically interact with its environment, providing flexibility and adaptability across diverse tasks.</p>",
    "contentMarkdown": "*   Tools empower the agent with specialized capabilities to execute tasks precisely, often leveraging the **LangChain framework** for structured workflows. Tools include:\n    \n    *   **Executable Workflows**: Defined within LangChain, providing structured, data-aware task handling.\n    *   **APIs**: Facilitate secure access to both internal and external data sources, enriching the agent’s functional range.\n    *   **Middleware**: Supports data exchange between the core and tools, handling formatting, error-checking, and ensuring security.\n*   LangChain’s integration enables the agent to dynamically interact with its environment, providing flexibility and adaptability across diverse tasks.\n    \n\nTools empower the agent with specialized capabilities to execute tasks precisely, often leveraging the **LangChain framework** for structured workflows. Tools include:\n\n*   **Executable Workflows**: Defined within LangChain, providing structured, data-aware task handling.\n*   **APIs**: Facilitate secure access to both internal and external data sources, enriching the agent’s functional range.\n*   **Middleware**: Supports data exchange between the core and tools, handling formatting, error-checking, and ensuring security.\n\nLangChain’s integration enables the agent to dynamically interact with its environment, providing flexibility and adaptability across diverse tasks.",
    "order": 3,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 168,
      "contentLength": 1631
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#tools",
    "scrapedAt": "2025-12-28T11:47:30.113Z"
  },
  {
    "id": "ai-agents-planning-module-4",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "The Agent Framework",
    "title": "Planning Module",
    "subtitle": "The Agent Framework",
    "contentHtml": "<ul>\n  <li>For complex problem-solving, the Planning Module enables structured approaches like task decomposition and reflection to guide the agent in optimizing solutions. The Task Management system within this module utilizes a deque data structure to autonomously generate, manage, and prioritize tasks. It adjusts priorities in real-time as tasks are completed and new tasks are generated, ensuring goal-aligned task progression.</li>\n  <li>In summary, the LLM Agent Framework combines an LLM’s advanced language capabilities with a vector database’s efficient memory system and an agentic framework’s responsive tooling. These integrated components create a cohesive, powerful AI agent capable of adaptive, real-time decision-making and dynamic task execution across complex applications.</li>\n</ul>",
    "contentMarkdown": "*   For complex problem-solving, the Planning Module enables structured approaches like task decomposition and reflection to guide the agent in optimizing solutions. The Task Management system within this module utilizes a deque data structure to autonomously generate, manage, and prioritize tasks. It adjusts priorities in real-time as tasks are completed and new tasks are generated, ensuring goal-aligned task progression.\n*   In summary, the LLM Agent Framework combines an LLM’s advanced language capabilities with a vector database’s efficient memory system and an agentic framework’s responsive tooling. These integrated components create a cohesive, powerful AI agent capable of adaptive, real-time decision-making and dynamic task execution across complex applications.",
    "order": 4,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 106,
      "contentLength": 804
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#planning-module",
    "scrapedAt": "2025-12-28T11:47:30.113Z"
  },
  {
    "id": "ai-agents-reflection-5",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Design Patterns",
    "title": "Reflection",
    "subtitle": "Agentic Design Patterns",
    "contentHtml": "<h4 id=\"overview-1\">Overview</h4>\n<ul>\n  <li>\n    <p>To boost the effectiveness of LLMs, a pivotal approach is the incorporation of a reflective mechanism within their workflows. Reflection is a method by which LLMs improve their output quality through self-evaluation and iterative refinement. By implementing this approach, an LLM can autonomously recognize gaps in its output, adjust based on feedback, and ultimately deliver responses that are more precise, efficient, and contextually aligned with user needs. This structured, iterative process transforms the typical query-response interaction into a dynamic cycle of continuous improvement.</p>\n  </li>\n  <li>\n    <p>Reflection represents a relatively straightforward type of agentic workflow, yet it has proven to significantly enhance LLM output quality across diverse applications. By encouraging models to reflect on their performance, refine their responses, and utilize external tools for self-assessment, this design pattern enables models to deliver accurate, efficient, and contextually relevant results. This iterative process not only strengthens an LLM’s ability to produce high-quality outputs but also imbues it with a form of adaptability, allowing it to better meet complex, evolving requirements.</p>\n  </li>\n  <li>\n    <p>The integration of Reflection into agentic workflows is transformative, rendering LLMs more adaptable, self-aware, and capable of handling complex tasks autonomously. As a foundational design pattern, Reflection holds substantial promise for enhancing the efficacy and reliability of LLM-based applications. This approach highlights the growing capacity of these models to function as intelligent, self-improving agents, poised to meet the demands of increasingly sophisticated tasks with minimal human intervention.</p>\n  </li>\n</ul>\n<p>To boost the effectiveness of LLMs, a pivotal approach is the incorporation of a reflective mechanism within their workflows. Reflection is a method by which LLMs improve their output quality through self-evaluation and iterative refinement. By implementing this approach, an LLM can autonomously recognize gaps in its output, adjust based on feedback, and ultimately deliver responses that are more precise, efficient, and contextually aligned with user needs. This structured, iterative process transforms the typical query-response interaction into a dynamic cycle of continuous improvement.</p>\n<p>Reflection represents a relatively straightforward type of agentic workflow, yet it has proven to significantly enhance LLM output quality across diverse applications. By encouraging models to reflect on their performance, refine their responses, and utilize external tools for self-assessment, this design pattern enables models to deliver accurate, efficient, and contextually relevant results. This iterative process not only strengthens an LLM’s ability to produce high-quality outputs but also imbues it with a form of adaptability, allowing it to better meet complex, evolving requirements.</p>\n<p>The integration of Reflection into agentic workflows is transformative, rendering LLMs more adaptable, self-aware, and capable of handling complex tasks autonomously. As a foundational design pattern, Reflection holds substantial promise for enhancing the efficacy and reliability of LLM-based applications. This approach highlights the growing capacity of these models to function as intelligent, self-improving agents, poised to meet the demands of increasingly sophisticated tasks with minimal human intervention.</p>\n<h4 id=\"reflection-workflow-step-by-step-process\">Reflection Workflow: Step-by-Step Process</h4>\n<h5 id=\"initial-output-generation\">Initial Output Generation</h5>\n<ul>\n  <li>In a typical task, such as writing code, the LLM is first prompted to generate an initial response aimed at accomplishing a specific goal (e.g., completing “task X”). This response may serve as a draft that will later be subjected to further scrutiny.</li>\n</ul>\n<h5 id=\"self-evaluation-and-constructive-feedback\">Self-Evaluation and Constructive Feedback</h5>\n<ul>\n  <li>\n    <p>After producing an initial output, the LLM can be guided to assess its response. For instance, in the case of code generation, it may be prompted with:</p>\n\n    <blockquote>\n      <p>“Here’s code intended for task X: [previously generated code].<br>\nCheck the code carefully for correctness, style, and efficiency, and provide constructive criticism for improvement.”</p>\n    </blockquote>\n  </li>\n  <li>\n    <p>This self-critique phase enables the LLM to recognize any flaws in its work. It can identify issues related to correctness, efficiency, and stylistic quality, thus facilitating the detection of areas needing refinement.</p>\n  </li>\n</ul>\n<p>After producing an initial output, the LLM can be guided to assess its response. For instance, in the case of code generation, it may be prompted with:</p>\n<blockquote>\n      <p>“Here’s code intended for task X: [previously generated code].<br>\nCheck the code carefully for correctness, style, and efficiency, and provide constructive criticism for improvement.”</p>\n    </blockquote>\n<p>“Here’s code intended for task X: [previously generated code].<br>\nCheck the code carefully for correctness, style, and efficiency, and provide constructive criticism for improvement.”</p>\n<p>This self-critique phase enables the LLM to recognize any flaws in its work. It can identify issues related to correctness, efficiency, and stylistic quality, thus facilitating the detection of areas needing refinement.</p>\n<h5 id=\"revision-based-on-feedback\">Revision Based on Feedback</h5>\n<ul>\n  <li>\n    <p>Once the LLM generates feedback on its own output, the agentic workflow proceeds by prompting the model to integrate this feedback into a revised response. In this stage, the context given to the model includes both the original output and the constructive criticism it produced. The LLM then generates a refined version that reflects the improvements suggested during self-reflection.</p>\n  </li>\n  <li>\n    <p>This cycle of criticism and rewriting can be repeated multiple times, resulting in iterative enhancements that significantly elevate the quality of the final output.</p>\n  </li>\n</ul>\n<p>Once the LLM generates feedback on its own output, the agentic workflow proceeds by prompting the model to integrate this feedback into a revised response. In this stage, the context given to the model includes both the original output and the constructive criticism it produced. The LLM then generates a refined version that reflects the improvements suggested during self-reflection.</p>\n<p>This cycle of criticism and rewriting can be repeated multiple times, resulting in iterative enhancements that significantly elevate the quality of the final output.</p>\n<h4 id=\"beyond-self-reflection-integrating-additional-tools\">Beyond Self-Reflection: Integrating Additional Tools</h4>\n<ul>\n  <li>\n    <p>Reflection can be further augmented by equipping the LLM with tools that enable it to evaluate its own output quantitatively. For instance:</p>\n\n    <ul>\n      <li><strong>Code Evaluation:</strong> The model can run its code through unit tests to verify accuracy, using test cases to ensure correct results.</li>\n      <li><strong>Text Validation:</strong> The LLM can leverage internet searches or external databases to fact-check and verify textual content.</li>\n    </ul>\n  </li>\n  <li>\n    <p>When errors or inaccuracies are detected through these tools, the LLM can reflect on the discrepancies, producing additional feedback and proposing ways to improve the output. This tool-supported reflection enables the LLM to refine its responses even further, effectively combining self-critique with external validation.</p>\n  </li>\n</ul>\n<p>Reflection can be further augmented by equipping the LLM with tools that enable it to evaluate its own output quantitatively. For instance:</p>\n<ul>\n      <li><strong>Code Evaluation:</strong> The model can run its code through unit tests to verify accuracy, using test cases to ensure correct results.</li>\n      <li><strong>Text Validation:</strong> The LLM can leverage internet searches or external databases to fact-check and verify textual content.</li>\n    </ul>\n<p>When errors or inaccuracies are detected through these tools, the LLM can reflect on the discrepancies, producing additional feedback and proposing ways to improve the output. This tool-supported reflection enables the LLM to refine its responses even further, effectively combining self-critique with external validation.</p>\n<h4 id=\"multi-agent-framework-for-enhanced-reflection\">Multi-Agent Framework for Enhanced Reflection</h4>\n<ul>\n  <li>To optimize the Reflection process, a multi-agent framework can be utilized. In this configuration, two distinct agents are employed:</li>\n</ul>\n<ol>\n  <li><strong>Output Generation Agent:</strong> Primarily responsible for producing responses aimed at achieving the designated task effectively.</li>\n  <li><strong>Critique Agent:</strong> Tasked with critically evaluating the output of the first agent, offering constructive feedback to enhance its quality.</li>\n</ol>\n<ul>\n  <li>Through this dialogue between agents, the LLM achieves improved results, as the two agents collaboratively identify and rectify weaknesses in the output. This cooperative approach introduces a second level of reflection, allowing the LLM to gain insights that a single-agent setup might miss.</li>\n</ul>\n<h5 id=\"example-of-reflection-multi-agent-framework-for-iterative-code-improvement\">Example of Reflection: Multi-Agent Framework for Iterative Code Improvement</h5>\n<ul>\n  <li>In the context of Reflection, one effective implementation involves a multi-agent interaction where two agents—a Coder Agent and a Critic Agent—collaborate to refine code through iterative feedback and revisions, emphasizing the synergy between generation and critique.</li>\n</ul>\n<ol>\n  <li>\n    <p><strong>Initial Task and Code Generation</strong>: The process begins with a prompt given to the Coder Agent, instructing it to “write code for {task}.” The Coder Agent generates an initial version of the code, labeled here as <code class=\"language-plaintext highlighter-rouge\">do_task(x)</code>.</p>\n  </li>\n  <li>\n    <p><strong>Critique and Error Identification</strong>: The Critic Agent then reviews the initial code. In this case, it identifies a specific issue, stating, “There’s a bug on line 5. Fix it by…” and offers a constructive suggestion for improvement. This feedback allows the Coder Agent to understand where the code falls short.</p>\n  </li>\n  <li>\n    <p><strong>Code Revision</strong>: Based on the Critic Agent’s feedback, the Coder Agent revises its code, producing an updated version, <code class=\"language-plaintext highlighter-rouge\">do_task_v2(x)</code>. This revised code aims to address the issues highlighted in the first critique.</p>\n  </li>\n  <li>\n    <p><strong>Further Testing and Feedback</strong>: The Critic Agent assesses the new version, testing it further (such as through unit tests). Here, it notes that “It failed Unit Test 3” and advises a further change, indicating that additional refinements are necessary for accuracy.</p>\n  </li>\n  <li>\n    <p><strong>Final Iteration</strong>: The Coder Agent, with this additional guidance, creates yet another iteration of the code—<code class=\"language-plaintext highlighter-rouge\">do_task_v3(x)</code>. This repeated process of critique and revision continues until the code meets the desired standards for functionality and efficiency.</p>\n  </li>\n</ol>\n<p><strong>Initial Task and Code Generation</strong>: The process begins with a prompt given to the Coder Agent, instructing it to “write code for {task}.” The Coder Agent generates an initial version of the code, labeled here as <code class=\"language-plaintext highlighter-rouge\">do_task(x)</code>.</p>\n<p><strong>Critique and Error Identification</strong>: The Critic Agent then reviews the initial code. In this case, it identifies a specific issue, stating, “There’s a bug on line 5. Fix it by…” and offers a constructive suggestion for improvement. This feedback allows the Coder Agent to understand where the code falls short.</p>\n<p><strong>Code Revision</strong>: Based on the Critic Agent’s feedback, the Coder Agent revises its code, producing an updated version, <code class=\"language-plaintext highlighter-rouge\">do_task_v2(x)</code>. This revised code aims to address the issues highlighted in the first critique.</p>\n<p><strong>Further Testing and Feedback</strong>: The Critic Agent assesses the new version, testing it further (such as through unit tests). Here, it notes that “It failed Unit Test 3” and advises a further change, indicating that additional refinements are necessary for accuracy.</p>\n<p><strong>Final Iteration</strong>: The Coder Agent, with this additional guidance, creates yet another iteration of the code—<code class=\"language-plaintext highlighter-rouge\">do_task_v3(x)</code>. This repeated process of critique and revision continues until the code meets the desired standards for functionality and efficiency.</p>\n<ul>\n  <li>\n    <p>This example highlights the iterative nature of Reflection within a multi-agent framework. By engaging a Coder Agent focused on output generation and a Critic Agent dedicated to providing structured feedback, the system harnesses a continuous improvement loop. This interaction enables large language models to autonomously detect errors, refine logic, and improve their responses.</p>\n  </li>\n  <li>\n    <p>The multi-agent setup exemplifies how Reflection can be operationalized to produce high-quality, reliable results. This structured approach not only enhances the LLM’s output but also mirrors human collaborative workflows, where constructive feedback leads to better solutions through repeated refinement.</p>\n  </li>\n</ul>\n<p>This example highlights the iterative nature of Reflection within a multi-agent framework. By engaging a Coder Agent focused on output generation and a Critic Agent dedicated to providing structured feedback, the system harnesses a continuous improvement loop. This interaction enables large language models to autonomously detect errors, refine logic, and improve their responses.</p>\n<p>The multi-agent setup exemplifies how Reflection can be operationalized to produce high-quality, reliable results. This structured approach not only enhances the LLM’s output but also mirrors human collaborative workflows, where constructive feedback leads to better solutions through repeated refinement.</p>",
    "contentMarkdown": "#### Overview\n\n*   To boost the effectiveness of LLMs, a pivotal approach is the incorporation of a reflective mechanism within their workflows. Reflection is a method by which LLMs improve their output quality through self-evaluation and iterative refinement. By implementing this approach, an LLM can autonomously recognize gaps in its output, adjust based on feedback, and ultimately deliver responses that are more precise, efficient, and contextually aligned with user needs. This structured, iterative process transforms the typical query-response interaction into a dynamic cycle of continuous improvement.\n    \n*   Reflection represents a relatively straightforward type of agentic workflow, yet it has proven to significantly enhance LLM output quality across diverse applications. By encouraging models to reflect on their performance, refine their responses, and utilize external tools for self-assessment, this design pattern enables models to deliver accurate, efficient, and contextually relevant results. This iterative process not only strengthens an LLM’s ability to produce high-quality outputs but also imbues it with a form of adaptability, allowing it to better meet complex, evolving requirements.\n    \n*   The integration of Reflection into agentic workflows is transformative, rendering LLMs more adaptable, self-aware, and capable of handling complex tasks autonomously. As a foundational design pattern, Reflection holds substantial promise for enhancing the efficacy and reliability of LLM-based applications. This approach highlights the growing capacity of these models to function as intelligent, self-improving agents, poised to meet the demands of increasingly sophisticated tasks with minimal human intervention.\n    \n\nTo boost the effectiveness of LLMs, a pivotal approach is the incorporation of a reflective mechanism within their workflows. Reflection is a method by which LLMs improve their output quality through self-evaluation and iterative refinement. By implementing this approach, an LLM can autonomously recognize gaps in its output, adjust based on feedback, and ultimately deliver responses that are more precise, efficient, and contextually aligned with user needs. This structured, iterative process transforms the typical query-response interaction into a dynamic cycle of continuous improvement.\n\nReflection represents a relatively straightforward type of agentic workflow, yet it has proven to significantly enhance LLM output quality across diverse applications. By encouraging models to reflect on their performance, refine their responses, and utilize external tools for self-assessment, this design pattern enables models to deliver accurate, efficient, and contextually relevant results. This iterative process not only strengthens an LLM’s ability to produce high-quality outputs but also imbues it with a form of adaptability, allowing it to better meet complex, evolving requirements.\n\nThe integration of Reflection into agentic workflows is transformative, rendering LLMs more adaptable, self-aware, and capable of handling complex tasks autonomously. As a foundational design pattern, Reflection holds substantial promise for enhancing the efficacy and reliability of LLM-based applications. This approach highlights the growing capacity of these models to function as intelligent, self-improving agents, poised to meet the demands of increasingly sophisticated tasks with minimal human intervention.\n\n#### Reflection Workflow: Step-by-Step Process\n\n##### Initial Output Generation\n\n*   In a typical task, such as writing code, the LLM is first prompted to generate an initial response aimed at accomplishing a specific goal (e.g., completing “task X”). This response may serve as a draft that will later be subjected to further scrutiny.\n\n##### Self-Evaluation and Constructive Feedback\n\n*   After producing an initial output, the LLM can be guided to assess its response. For instance, in the case of code generation, it may be prompted with:\n    \n    > “Here’s code intended for task X: \\[previously generated code\\].  \n    > Check the code carefully for correctness, style, and efficiency, and provide constructive criticism for improvement.”\n    \n*   This self-critique phase enables the LLM to recognize any flaws in its work. It can identify issues related to correctness, efficiency, and stylistic quality, thus facilitating the detection of areas needing refinement.\n    \n\nAfter producing an initial output, the LLM can be guided to assess its response. For instance, in the case of code generation, it may be prompted with:\n\n> “Here’s code intended for task X: \\[previously generated code\\].  \n> Check the code carefully for correctness, style, and efficiency, and provide constructive criticism for improvement.”\n\n“Here’s code intended for task X: \\[previously generated code\\].  \nCheck the code carefully for correctness, style, and efficiency, and provide constructive criticism for improvement.”\n\nThis self-critique phase enables the LLM to recognize any flaws in its work. It can identify issues related to correctness, efficiency, and stylistic quality, thus facilitating the detection of areas needing refinement.\n\n##### Revision Based on Feedback\n\n*   Once the LLM generates feedback on its own output, the agentic workflow proceeds by prompting the model to integrate this feedback into a revised response. In this stage, the context given to the model includes both the original output and the constructive criticism it produced. The LLM then generates a refined version that reflects the improvements suggested during self-reflection.\n    \n*   This cycle of criticism and rewriting can be repeated multiple times, resulting in iterative enhancements that significantly elevate the quality of the final output.\n    \n\nOnce the LLM generates feedback on its own output, the agentic workflow proceeds by prompting the model to integrate this feedback into a revised response. In this stage, the context given to the model includes both the original output and the constructive criticism it produced. The LLM then generates a refined version that reflects the improvements suggested during self-reflection.\n\nThis cycle of criticism and rewriting can be repeated multiple times, resulting in iterative enhancements that significantly elevate the quality of the final output.\n\n#### Beyond Self-Reflection: Integrating Additional Tools\n\n*   Reflection can be further augmented by equipping the LLM with tools that enable it to evaluate its own output quantitatively. For instance:\n    \n    *   **Code Evaluation:** The model can run its code through unit tests to verify accuracy, using test cases to ensure correct results.\n    *   **Text Validation:** The LLM can leverage internet searches or external databases to fact-check and verify textual content.\n*   When errors or inaccuracies are detected through these tools, the LLM can reflect on the discrepancies, producing additional feedback and proposing ways to improve the output. This tool-supported reflection enables the LLM to refine its responses even further, effectively combining self-critique with external validation.\n    \n\nReflection can be further augmented by equipping the LLM with tools that enable it to evaluate its own output quantitatively. For instance:\n\n*   **Code Evaluation:** The model can run its code through unit tests to verify accuracy, using test cases to ensure correct results.\n*   **Text Validation:** The LLM can leverage internet searches or external databases to fact-check and verify textual content.\n\nWhen errors or inaccuracies are detected through these tools, the LLM can reflect on the discrepancies, producing additional feedback and proposing ways to improve the output. This tool-supported reflection enables the LLM to refine its responses even further, effectively combining self-critique with external validation.\n\n#### Multi-Agent Framework for Enhanced Reflection\n\n*   To optimize the Reflection process, a multi-agent framework can be utilized. In this configuration, two distinct agents are employed:\n\n1.  **Output Generation Agent:** Primarily responsible for producing responses aimed at achieving the designated task effectively.\n2.  **Critique Agent:** Tasked with critically evaluating the output of the first agent, offering constructive feedback to enhance its quality.\n\n*   Through this dialogue between agents, the LLM achieves improved results, as the two agents collaboratively identify and rectify weaknesses in the output. This cooperative approach introduces a second level of reflection, allowing the LLM to gain insights that a single-agent setup might miss.\n\n##### Example of Reflection: Multi-Agent Framework for Iterative Code Improvement\n\n*   In the context of Reflection, one effective implementation involves a multi-agent interaction where two agents—a Coder Agent and a Critic Agent—collaborate to refine code through iterative feedback and revisions, emphasizing the synergy between generation and critique.\n\n1.  **Initial Task and Code Generation**: The process begins with a prompt given to the Coder Agent, instructing it to “write code for {task}.” The Coder Agent generates an initial version of the code, labeled here as `do_task(x)`.\n    \n2.  **Critique and Error Identification**: The Critic Agent then reviews the initial code. In this case, it identifies a specific issue, stating, “There’s a bug on line 5. Fix it by…” and offers a constructive suggestion for improvement. This feedback allows the Coder Agent to understand where the code falls short.\n    \n3.  **Code Revision**: Based on the Critic Agent’s feedback, the Coder Agent revises its code, producing an updated version, `do_task_v2(x)`. This revised code aims to address the issues highlighted in the first critique.\n    \n4.  **Further Testing and Feedback**: The Critic Agent assesses the new version, testing it further (such as through unit tests). Here, it notes that “It failed Unit Test 3” and advises a further change, indicating that additional refinements are necessary for accuracy.\n    \n5.  **Final Iteration**: The Coder Agent, with this additional guidance, creates yet another iteration of the code—`do_task_v3(x)`. This repeated process of critique and revision continues until the code meets the desired standards for functionality and efficiency.\n    \n\n**Initial Task and Code Generation**: The process begins with a prompt given to the Coder Agent, instructing it to “write code for {task}.” The Coder Agent generates an initial version of the code, labeled here as `do_task(x)`.\n\n**Critique and Error Identification**: The Critic Agent then reviews the initial code. In this case, it identifies a specific issue, stating, “There’s a bug on line 5. Fix it by…” and offers a constructive suggestion for improvement. This feedback allows the Coder Agent to understand where the code falls short.\n\n**Code Revision**: Based on the Critic Agent’s feedback, the Coder Agent revises its code, producing an updated version, `do_task_v2(x)`. This revised code aims to address the issues highlighted in the first critique.\n\n**Further Testing and Feedback**: The Critic Agent assesses the new version, testing it further (such as through unit tests). Here, it notes that “It failed Unit Test 3” and advises a further change, indicating that additional refinements are necessary for accuracy.\n\n**Final Iteration**: The Coder Agent, with this additional guidance, creates yet another iteration of the code—`do_task_v3(x)`. This repeated process of critique and revision continues until the code meets the desired standards for functionality and efficiency.\n\n*   This example highlights the iterative nature of Reflection within a multi-agent framework. By engaging a Coder Agent focused on output generation and a Critic Agent dedicated to providing structured feedback, the system harnesses a continuous improvement loop. This interaction enables large language models to autonomously detect errors, refine logic, and improve their responses.\n    \n*   The multi-agent setup exemplifies how Reflection can be operationalized to produce high-quality, reliable results. This structured approach not only enhances the LLM’s output but also mirrors human collaborative workflows, where constructive feedback leads to better solutions through repeated refinement.\n    \n\nThis example highlights the iterative nature of Reflection within a multi-agent framework. By engaging a Coder Agent focused on output generation and a Critic Agent dedicated to providing structured feedback, the system harnesses a continuous improvement loop. This interaction enables large language models to autonomously detect errors, refine logic, and improve their responses.\n\nThe multi-agent setup exemplifies how Reflection can be operationalized to produce high-quality, reliable results. This structured approach not only enhances the LLM’s output but also mirrors human collaborative workflows, where constructive feedback leads to better solutions through repeated refinement.",
    "order": 5,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 10,
    "tags": [
      "algorithmsarchitecture",
      "llm"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 1850,
      "contentLength": 14631
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#reflection",
    "scrapedAt": "2025-12-28T11:47:30.113Z"
  },
  {
    "id": "ai-agents-functiontoolapi-calling-6",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Design Patterns",
    "title": "Function/Tool/API Calling",
    "subtitle": "Agentic Design Patterns",
    "contentHtml": "<ul>\n  <li>The advent of Tool Use in LLMs represents a pivotal design pattern in agentic AI workflows, enabling LLMs to perform a diverse range of tasks beyond text generation. Tool Use refers to the capability of an LLM to utilize specific functions—such as executing code, conducting web searches, or interacting with productivity tools—within its responses, effectively expanding its utility far beyond conventional, language-based outputs. This approach allows LLMs to tackle more complex queries and execute multifaceted tasks by selectively invoking various external tools. From answering specific questions to performing calculations, the use of function calls empowers LLMs to provide highly accurate and contextually informed responses.</li>\n  <li>A foundational example of Tool Use is seen in scenarios where users request information not available in the model’s pre-existing training data. For instance, if a user asks, “What is the best coffee maker according to reviewers?”, a model equipped with Tool Use may initiate a web search, fetching up-to-date information by generating a command string such as <code class=\"language-plaintext highlighter-rouge\">{tool: web-search, query: \"coffee maker reviews\"}</code>. Upon processing, the model retrieves relevant pages, synthesizes the data, and delivers an informed response. This dynamic response mechanism emerged from early realizations that traditional transformer-based language models, reliant solely on pre-trained knowledge, were inherently limited. By integrating a web search tool, developers enabled the model to access and incorporate fresh information into its output, a capability now widely adopted across various LLMs in consumer-facing applications.</li>\n  <li>Moreover, Tool Use enables LLMs to handle calculations and other tasks requiring precision that text generation alone cannot achieve. For example, when a user asks, “If I invest $100 at compound 7% interest for 12 years, what do I have at the end?”, an LLM could respond by executing a Python command like <code class=\"language-plaintext highlighter-rouge\">100 * (1+0.07)**12</code>. The LLM generates a string such as <code class=\"language-plaintext highlighter-rouge\">{tool: python-interpreter, code: \"100 * (1+0.07)**12\"}</code>, and then the calculation tool processes this command to deliver an accurate answer. This illustrates how Tool Use facilitates complex mathematical reasoning within conversational AI systems.</li>\n  <li>The scope of Tool Use, however, extends well beyond web searches or basic calculations. As the technology has evolved, developers have implemented a wide array of functions, enabling LLMs to interface with multiple external resources. These functions may include accessing specialized databases, interacting with productivity tools like email and calendar applications, generating or interpreting images, and engaging with multiple data sources such as Search (via Google/Bing Search APIs), Wikipedia, and academic repositories like arXiv.</li>\n  <li>Systems now prompt LLMs with detailed descriptions of available functions, specifying their capabilities and parameters. With these cues, an LLM can autonomously select the appropriate function to fulfill the user’s request. In settings where hundreds of tools are accessible, developers often employ heuristics to streamline function selection, prioritizing the tools most relevant to the current context—a strategy analogous to the subset selection techniques used in retrieval-augmented generation (RAG) systems.</li>\n  <li>The development of large multimodal models (LMMs) such as LLaVa, GPT-4V, and Gemini marked another milestone in Tool Use. Prior to these models, LLMs could not process or manipulate images directly, and any image-related tasks had to be offloaded to specific computer vision functions, such as object recognition or scene analysis. The introduction of GPT-4’s function-calling capabilities in 2023 further advanced Tool Use by establishing a more general-purpose function interface, laying the groundwork for a versatile, multimodal AI ecosystem where models seamlessly integrate text, image, and other data types. This new functionality has subsequently led to a proliferation of LLMs designed to exploit Tool Use, broadening the range of applications and enhancing overall adaptability.</li>\n  <li>For instance, below is a prompt from <a href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct\">SmolLM2</a> for function calling, where the model is prompted to choose relevant functions based on specific user inquiries is as follows. The prompt template guides SmolLM2 in structuring its function calls precisely and prompts it to assess the relevance and sufficiency of available parameters before executing a function.</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\">You are an expert in composing functions. You are given a question and a set of possible functions. \nBased on the question, you will need to make one or more function/tool calls to achieve the purpose. \nIf none of the functions can be used, point it out and refuse to answer. \nIf the given question lacks the parameters required by the function, also point it out.\n\nYou have access to the following tools:\n&lt;tools&gt;&lt;/tools&gt;\n\nThe output MUST strictly adhere to the following format, and NO other text MUST be included.\nThe example format is as follows. Please make sure the parameter type is correct. If no function call is needed, please make the tool calls an empty list '[]'.\n&lt;tool_call&gt;[\n{\"name\": \"func_name1\", \"arguments\": {\"argument1\": \"value1\", \"argument2\": \"value2\"}},\n(more tool calls as required)\n]&lt;/tool_call&gt;\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\">You are an expert in composing functions. You are given a question and a set of possible functions. \nBased on the question, you will need to make one or more function/tool calls to achieve the purpose. \nIf none of the functions can be used, point it out and refuse to answer. \nIf the given question lacks the parameters required by the function, also point it out.\n\nYou have access to the following tools:\n&lt;tools&gt;&lt;/tools&gt;\n\nThe output MUST strictly adhere to the following format, and NO other text MUST be included.\nThe example format is as follows. Please make sure the parameter type is correct. If no function call is needed, please make the tool calls an empty list '[]'.\n&lt;tool_call&gt;[\n{\"name\": \"func_name1\", \"arguments\": {\"argument1\": \"value1\", \"argument2\": \"value2\"}},\n(more tool calls as required)\n]&lt;/tool_call&gt;\n</code></pre>\n<ul>\n  <li>The evolution of Tool Use and function-calling capabilities in LLMs demonstrates the significant strides taken toward realizing general-purpose, agentic AI workflows. By enabling LLMs to autonomously utilize specialized tools across various contexts, developers have transformed these models from static text generators into dynamic, multifunctional systems capable of addressing a vast array of user needs. As the field advances, we can expect further innovations that expand the breadth and depth of Tool Use, pushing the boundaries of what LLMs can achieve in an integrated, agentic environment.</li>\n</ul>\n<h4 id=\"tool-calling-examples-web-search-and-code-execution\">Tool Calling Examples: Web Search and Code Execution</h4>\n<ul>\n  <li>LLMs can leverage tools such as web search to provide current product recommendations and code execution to handle complex calculations, showcasing their adeptness at choosing and using the right tools based on user input.</li>\n  <li>The image below (<a href=\"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use\">source</a>) showcases practical examples of tool calling in LLMs, highlighting two specific tools: a web search tool and a code execution tool. In the left panel, an example from Bing Copilot illustrates how an LLM can utilize a web search tool. When a user asks, “What is the best coffee maker according to reviewers?”, the model initiates a web search to gather relevant information from current online reviews. This allows the LLM to provide an informed answer based on up-to-date data.</li>\n  <li>The right panel demonstrates an example from ChatGPT using a code execution tool. When a user asks, “If I invest $100 at compound 7% interest for 12 years, what do I have at the end?”, the LLM responds by generating a Python command to calculate the compounded interest. The code snippet, <code class=\"language-plaintext highlighter-rouge\">principal = 100; interest_rate = 0.07; years = 12; value = principal * (1 + interest_rate) ** years</code>, is executed, providing an accurate financial calculation rather than relying solely on text-based reasoning.</li>\n  <li>These examples illustrate the model’s ability to identify and select the appropriate tool based on the user’s query, further demonstrating the flexibility and enhanced capabilities of Tool Use in agentic LLM workflows.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/agents/ToolCalling.jpg\" alt=\"\"></p>\n<h4 id=\"function-calling-datasets\">Function Calling Datasets</h4>\n<h5 id=\"hermes-function-calling-v1\"><a href=\"https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1\">Hermes Function-Calling V1</a></h5>\n<ul>\n  <li>The <a href=\"https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1\">Hermes Function-Calling V1</a> dataset is designed for training language models to perform structured function calls and return structured outputs based on natural language instructions.</li>\n  <li>It includes function-calling conversations, json-mode samples, agentic json-mode, and structured extraction examples, showcasing various scenarios where AI agents interpret queries and execute relevant function calls.</li>\n  <li>The Hermes Function-Calling Standard enables language models to execute API calls based on user requests, improving AI’s practical utility by allowing direct API interactions.</li>\n</ul>\n<h5 id=\"glaive-function-calling-v2\">Glaive Function Calling V2</h5>\n<ul>\n  <li>The <a href=\"https://huggingface.co/datasets/glaiveai/glaive-function-calling\">Glaive Function Calling (52K)</a> and <a href=\"https://huggingface.co/datasets/vietgpt/glaive-function-calling-v2?row=0\">Glaive Function Calling v2 (113K)</a> are datasets generated through <a href=\"https://glaive.ai/\">Glaive</a> for the task of function calling, in the following format:</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\">SYSTEM: You are an helpful assistant who has access to the following functions to help the user, you can use the functions if needed-\n{\n  JSON function definiton\n}\nUSER: user message\nASSISTANT: assistant message\nFunction call invocations are formatted as-\nASSISTANT: &lt;functioncall&gt; {json function call}\nResponse to the function call is formatted as-\nFUNCTION RESPONSE: {json function response}\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\">SYSTEM: You are an helpful assistant who has access to the following functions to help the user, you can use the functions if needed-\n{\n  JSON function definiton\n}\nUSER: user message\nASSISTANT: assistant message\nFunction call invocations are formatted as-\nASSISTANT: &lt;functioncall&gt; {json function call}\nResponse to the function call is formatted as-\nFUNCTION RESPONSE: {json function response}\n</code></pre>\n<ul>\n  <li>There are also samples which do not have any function invocations, multiple invocations and samples with no functions presented and invoked to keep the data balanced.</li>\n</ul>\n<h5 id=\"salesforces-xlam-function-calling-60k\"><a href=\"https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k?row=0\">Salesforce’s Xlam-function-calling-60k</a></h5>\n<ul>\n  <li>Salesforce’s <a href=\"https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k?row=0\">xlam-function-calling-60k</a> contains 60,000 data samples collected by APIGen, an automated data generation pipeline designed to produce verifiable high-quality datasets for function-calling applications. Each data in our dataset is verified through three hierarchical stages: format checking, actual function executions, and semantic verification, ensuring its reliability and correctness.</li>\n  <li>Also, <a href=\"https://huggingface.co/Salesforce/xLAM-1b-fc-r\">xLAM-1b-fc-r</a> and <a href=\"https://huggingface.co/Salesforce/xLAM-7b-fc-r\">xLAM-7b-fc-r</a>.</li>\n</ul>\n<h6 id=\"json-data-format-for-query-and-answers\">JSON Data Format for Query and Answers</h6>\n<ul>\n  <li>This JSON data format is used to represent a query along with the available tools and the corresponding answers. Here’s a description of the JSON format which consists of the following key-value pairs:\n```</li>\n  <li>\n    <p><strong>query</strong> (string): The query or problem statement.</p>\n  </li>\n  <li>\n    <p><strong>tools</strong> (array): An array of available tools that can be used to solve the query.</p>\n\n    <ul>\n      <li>\n        <p>Each tool is represented as an object with the following properties:</p>\n\n        <ul>\n          <li>\n            <p><strong>name</strong> (string): The name of the tool.</p>\n          </li>\n          <li>\n            <p><strong>description</strong> (string): A brief description of what the tool does.</p>\n          </li>\n          <li>\n            <p><strong>parameters</strong> (object): An object representing the parameters required by the tool.</p>\n\n            <ul>\n              <li>\n                <p>Each parameter is represented as a key-value pair, where the key is the parameter name and the value is an object with the following properties:</p>\n\n                <ul>\n                  <li>\n                    <p><strong>type</strong> (string): The data type of the parameter (e.g., “int”, “float”, “list”).</p>\n                  </li>\n                  <li>\n                    <p><strong>description</strong> (string): A brief description of the parameter.</p>\n                  </li>\n                  <li>\n                    <p><strong>required</strong> (boolean): Indicates whether the parameter is required or optional.</p>\n                  </li>\n                </ul>\n              </li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>answers</strong> (array): An array of answers corresponding to the query.</p>\n\n    <ul>\n      <li>\n        <p>Each answer is represented as an object with the following properties:</p>\n\n        <ul>\n          <li>\n            <p><strong>name</strong> (string): The name of the tool used to generate the answer.</p>\n          </li>\n          <li>\n            <p><strong>arguments</strong> (object): An object representing the arguments passed to the tool to generate the answer.</p>\n\n            <ul>\n              <li>Each argument is represented as a key-value pair, where the key is the parameter name and the value is the corresponding value.\n```</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>Note that they format the <strong>query</strong>, <strong>tools</strong>, and <strong>answers</strong> as a string, but you can easily recover each entry to the JSON object via <code class=\"language-plaintext highlighter-rouge\">json.loads(...)</code>.</li>\n</ul>\n<p><strong>query</strong> (string): The query or problem statement.</p>\n<p><strong>tools</strong> (array): An array of available tools that can be used to solve the query.</p>\n<ul>\n      <li>\n        <p>Each tool is represented as an object with the following properties:</p>\n\n        <ul>\n          <li>\n            <p><strong>name</strong> (string): The name of the tool.</p>\n          </li>\n          <li>\n            <p><strong>description</strong> (string): A brief description of what the tool does.</p>\n          </li>\n          <li>\n            <p><strong>parameters</strong> (object): An object representing the parameters required by the tool.</p>\n\n            <ul>\n              <li>\n                <p>Each parameter is represented as a key-value pair, where the key is the parameter name and the value is an object with the following properties:</p>\n\n                <ul>\n                  <li>\n                    <p><strong>type</strong> (string): The data type of the parameter (e.g., “int”, “float”, “list”).</p>\n                  </li>\n                  <li>\n                    <p><strong>description</strong> (string): A brief description of the parameter.</p>\n                  </li>\n                  <li>\n                    <p><strong>required</strong> (boolean): Indicates whether the parameter is required or optional.</p>\n                  </li>\n                </ul>\n              </li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n    </ul>\n<p>Each tool is represented as an object with the following properties:</p>\n<ul>\n          <li>\n            <p><strong>name</strong> (string): The name of the tool.</p>\n          </li>\n          <li>\n            <p><strong>description</strong> (string): A brief description of what the tool does.</p>\n          </li>\n          <li>\n            <p><strong>parameters</strong> (object): An object representing the parameters required by the tool.</p>\n\n            <ul>\n              <li>\n                <p>Each parameter is represented as a key-value pair, where the key is the parameter name and the value is an object with the following properties:</p>\n\n                <ul>\n                  <li>\n                    <p><strong>type</strong> (string): The data type of the parameter (e.g., “int”, “float”, “list”).</p>\n                  </li>\n                  <li>\n                    <p><strong>description</strong> (string): A brief description of the parameter.</p>\n                  </li>\n                  <li>\n                    <p><strong>required</strong> (boolean): Indicates whether the parameter is required or optional.</p>\n                  </li>\n                </ul>\n              </li>\n            </ul>\n          </li>\n        </ul>\n<p><strong>name</strong> (string): The name of the tool.</p>\n<p><strong>description</strong> (string): A brief description of what the tool does.</p>\n<p><strong>parameters</strong> (object): An object representing the parameters required by the tool.</p>\n<ul>\n              <li>\n                <p>Each parameter is represented as a key-value pair, where the key is the parameter name and the value is an object with the following properties:</p>\n\n                <ul>\n                  <li>\n                    <p><strong>type</strong> (string): The data type of the parameter (e.g., “int”, “float”, “list”).</p>\n                  </li>\n                  <li>\n                    <p><strong>description</strong> (string): A brief description of the parameter.</p>\n                  </li>\n                  <li>\n                    <p><strong>required</strong> (boolean): Indicates whether the parameter is required or optional.</p>\n                  </li>\n                </ul>\n              </li>\n            </ul>\n<p>Each parameter is represented as a key-value pair, where the key is the parameter name and the value is an object with the following properties:</p>\n<ul>\n                  <li>\n                    <p><strong>type</strong> (string): The data type of the parameter (e.g., “int”, “float”, “list”).</p>\n                  </li>\n                  <li>\n                    <p><strong>description</strong> (string): A brief description of the parameter.</p>\n                  </li>\n                  <li>\n                    <p><strong>required</strong> (boolean): Indicates whether the parameter is required or optional.</p>\n                  </li>\n                </ul>\n<p><strong>type</strong> (string): The data type of the parameter (e.g., “int”, “float”, “list”).</p>\n<p><strong>description</strong> (string): A brief description of the parameter.</p>\n<p><strong>required</strong> (boolean): Indicates whether the parameter is required or optional.</p>\n<p><strong>answers</strong> (array): An array of answers corresponding to the query.</p>\n<ul>\n      <li>\n        <p>Each answer is represented as an object with the following properties:</p>\n\n        <ul>\n          <li>\n            <p><strong>name</strong> (string): The name of the tool used to generate the answer.</p>\n          </li>\n          <li>\n            <p><strong>arguments</strong> (object): An object representing the arguments passed to the tool to generate the answer.</p>\n\n            <ul>\n              <li>Each argument is represented as a key-value pair, where the key is the parameter name and the value is the corresponding value.\n```</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n    </ul>\n<p>Each answer is represented as an object with the following properties:</p>\n<ul>\n          <li>\n            <p><strong>name</strong> (string): The name of the tool used to generate the answer.</p>\n          </li>\n          <li>\n            <p><strong>arguments</strong> (object): An object representing the arguments passed to the tool to generate the answer.</p>\n\n            <ul>\n              <li>Each argument is represented as a key-value pair, where the key is the parameter name and the value is the corresponding value.\n```</li>\n            </ul>\n          </li>\n        </ul>\n<p><strong>name</strong> (string): The name of the tool used to generate the answer.</p>\n<p><strong>arguments</strong> (object): An object representing the arguments passed to the tool to generate the answer.</p>\n<ul>\n              <li>Each argument is represented as a key-value pair, where the key is the parameter name and the value is the corresponding value.\n```</li>\n            </ul>\n<h6 id=\"example\">Example</h6>\n<ul>\n  <li>Here’s an example JSON data:</li>\n</ul>\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\"><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"query\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Find the sum of all the multiples of 3 and 5 between 1 and 1000. Also find the product of the first five prime numbers.\"</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"tools\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">\n    </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"math_toolkit.sum_of_multiples\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"description\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Find the sum of all multiples of specified numbers within a specified range.\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"parameters\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"lower_limit\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n          </span><span class=\"nl\">\"type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"int\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"description\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"The start of the range (inclusive).\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"required\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"w\">\n        </span><span class=\"p\">},</span><span class=\"w\">\n        </span><span class=\"nl\">\"upper_limit\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n          </span><span class=\"nl\">\"type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"int\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"description\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"The end of the range (inclusive).\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"required\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"w\">\n        </span><span class=\"p\">},</span><span class=\"w\">\n        </span><span class=\"nl\">\"multiples\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n          </span><span class=\"nl\">\"type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"list\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"description\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"The numbers to find multiples of.\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"required\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"w\">\n        </span><span class=\"p\">}</span><span class=\"w\">\n      </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"p\">},</span><span class=\"w\">\n    </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"math_toolkit.product_of_primes\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"description\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Find the product of the first n prime numbers.\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"parameters\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"count\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n          </span><span class=\"nl\">\"type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"int\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"description\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"The number of prime numbers to multiply together.\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"required\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"w\">\n        </span><span class=\"p\">}</span><span class=\"w\">\n      </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"p\">],</span><span class=\"w\">\n  </span><span class=\"nl\">\"answers\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">\n    </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"math_toolkit.sum_of_multiples\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"arguments\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"lower_limit\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"upper_limit\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1000</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"multiples\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">5</span><span class=\"p\">]</span><span class=\"w\">\n      </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"p\">},</span><span class=\"w\">\n    </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"math_toolkit.product_of_primes\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"arguments\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"count\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">5</span><span class=\"w\">\n      </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"p\">]</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\"><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"query\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Find the sum of all the multiples of 3 and 5 between 1 and 1000. Also find the product of the first five prime numbers.\"</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"tools\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">\n    </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"math_toolkit.sum_of_multiples\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"description\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Find the sum of all multiples of specified numbers within a specified range.\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"parameters\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"lower_limit\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n          </span><span class=\"nl\">\"type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"int\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"description\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"The start of the range (inclusive).\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"required\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"w\">\n        </span><span class=\"p\">},</span><span class=\"w\">\n        </span><span class=\"nl\">\"upper_limit\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n          </span><span class=\"nl\">\"type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"int\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"description\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"The end of the range (inclusive).\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"required\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"w\">\n        </span><span class=\"p\">},</span><span class=\"w\">\n        </span><span class=\"nl\">\"multiples\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n          </span><span class=\"nl\">\"type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"list\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"description\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"The numbers to find multiples of.\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"required\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"w\">\n        </span><span class=\"p\">}</span><span class=\"w\">\n      </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"p\">},</span><span class=\"w\">\n    </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"math_toolkit.product_of_primes\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"description\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Find the product of the first n prime numbers.\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"parameters\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"count\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n          </span><span class=\"nl\">\"type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"int\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"description\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"The number of prime numbers to multiply together.\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"required\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"w\">\n        </span><span class=\"p\">}</span><span class=\"w\">\n      </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"p\">],</span><span class=\"w\">\n  </span><span class=\"nl\">\"answers\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">\n    </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"math_toolkit.sum_of_multiples\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"arguments\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"lower_limit\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"upper_limit\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1000</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"multiples\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"mi\">5</span><span class=\"p\">]</span><span class=\"w\">\n      </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"p\">},</span><span class=\"w\">\n    </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"math_toolkit.product_of_primes\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"arguments\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"count\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">5</span><span class=\"w\">\n      </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"p\">]</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre>\n<ul>\n  <li>In this example, the query asks to find the sum of multiples of 3 and 5 between 1 and 1000, and also find the product of the first five prime numbers. The available tools are <code class=\"language-plaintext highlighter-rouge\">math_toolkit.sum_of_multiples</code> and <code class=\"language-plaintext highlighter-rouge\">math_toolkit.product_of_primes</code>, along with their parameter descriptions. The answers array provides the specific tool and arguments used to generate each answer.</li>\n</ul>\n<h5 id=\"synth-apigen-v01\"><a href=\"https://huggingface.co/datasets/argilla/Synth-APIGen-v0.1\">Synth-APIGen-v0.1</a></h5>\n<ul>\n  <li>A dataset of 50k samples by <a href=\"https://huggingface.co/argilla\">Argilla</a>.</li>\n</ul>\n<h6 id=\"example-1\">Example</h6>\n<ul>\n  <li>This example demonstrates the use of the <code class=\"language-plaintext highlighter-rouge\">complex_to_polar</code> function, which is designed to convert complex numbers into their polar coordinate representations. The input query requests conversions for two specific complex numbers, <code class=\"language-plaintext highlighter-rouge\">3 + 4j</code> and <code class=\"language-plaintext highlighter-rouge\">1 - 2j</code>, showcasing how the function can be called with different arguments to obtain their polar forms.</li>\n</ul>\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\"><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"func_name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"complex_to_polar\"</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"func_desc\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Converts a complex number to its polar coordinate representation.\"</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"tools\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"[{</span><span class=\"se\">\\\"</span><span class=\"s2\">type</span><span class=\"se\">\\\"</span><span class=\"s2\">:</span><span class=\"se\">\\\"</span><span class=\"s2\">function</span><span class=\"se\">\\\"</span><span class=\"s2\">,</span><span class=\"se\">\\\"</span><span class=\"s2\">function</span><span class=\"se\">\\\"</span><span class=\"s2\">:{</span><span class=\"se\">\\\"</span><span class=\"s2\">name</span><span class=\"se\">\\\"</span><span class=\"s2\">:</span><span class=\"se\">\\\"</span><span class=\"s2\">complex_to_polar</span><span class=\"se\">\\\"</span><span class=\"s2\">,</span><span class=\"se\">\\\"</span><span class=\"s2\">description</span><span class=\"se\">\\\"</span><span class=\"s2\">:</span><span class=\"se\">\\\"</span><span class=\"s2\">Converts a complex number to its polar coordinate representation.</span><span class=\"se\">\\\"</span><span class=\"s2\">,</span><span class=\"se\">\\\"</span><span class=\"s2\">parameters</span><span class=\"se\">\\\"</span><span class=\"s2\">:{</span><span class=\"se\">\\\"</span><span class=\"s2\">type</span><span class=\"se\">\\\"</span><span class=\"s2\">:</span><span class=\"se\">\\\"</span><span class=\"s2\">object</span><span class=\"se\">\\\"</span><span class=\"s2\">,</span><span class=\"se\">\\\"</span><span class=\"s2\">properties</span><span class=\"se\">\\\"</span><span class=\"s2\">:{</span><span class=\"se\">\\\"</span><span class=\"s2\">complex_number</span><span class=\"se\">\\\"</span><span class=\"s2\">:{</span><span class=\"se\">\\\"</span><span class=\"s2\">type</span><span class=\"se\">\\\"</span><span class=\"s2\">:</span><span class=\"se\">\\\"</span><span class=\"s2\">object</span><span class=\"se\">\\\"</span><span class=\"s2\">,</span><span class=\"se\">\\\"</span><span class=\"s2\">description</span><span class=\"se\">\\\"</span><span class=\"s2\">:</span><span class=\"se\">\\\"</span><span class=\"s2\">A complex number in the form of `real + imaginary * 1j`.</span><span class=\"se\">\\\"</span><span class=\"s2\">}},</span><span class=\"se\">\\\"</span><span class=\"s2\">required</span><span class=\"se\">\\\"</span><span class=\"s2\">:[</span><span class=\"se\">\\\"</span><span class=\"s2\">complex_number</span><span class=\"se\">\\\"</span><span class=\"s2\">]}}}]\"</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"query\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"I'd like to convert the complex number 3 + 4j and 1 - 2j to polar coordinates.\"</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"answers\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"[{</span><span class=\"se\">\\\"</span><span class=\"s2\">name</span><span class=\"se\">\\\"</span><span class=\"s2\">: </span><span class=\"se\">\\\"</span><span class=\"s2\">complex_to_polar</span><span class=\"se\">\\\"</span><span class=\"s2\">, </span><span class=\"se\">\\\"</span><span class=\"s2\">arguments</span><span class=\"se\">\\\"</span><span class=\"s2\">: {</span><span class=\"se\">\\\"</span><span class=\"s2\">complex_number</span><span class=\"se\">\\\"</span><span class=\"s2\">: </span><span class=\"se\">\\\"</span><span class=\"s2\">3 + 4j</span><span class=\"se\">\\\"</span><span class=\"s2\">}}, {</span><span class=\"se\">\\\"</span><span class=\"s2\">name</span><span class=\"se\">\\\"</span><span class=\"s2\">: </span><span class=\"se\">\\\"</span><span class=\"s2\">complex_to_polar</span><span class=\"se\">\\\"</span><span class=\"s2\">, </span><span class=\"se\">\\\"</span><span class=\"s2\">arguments</span><span class=\"se\">\\\"</span><span class=\"s2\">: {</span><span class=\"se\">\\\"</span><span class=\"s2\">complex_number</span><span class=\"se\">\\\"</span><span class=\"s2\">: </span><span class=\"se\">\\\"</span><span class=\"s2\">1 - 2j</span><span class=\"se\">\\\"</span><span class=\"s2\">}}]\"</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"model_name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"meta-llama/Meta-Llama-3.1-70B-Instruct\"</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"hash_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"f873783c04bbddd9d79f47287fa3b6705b3eaea0e5bc126fba91366f7b8b07e9\"</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"p\">]</span><span class=\"err\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"category\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"E-commerce Platforms\"</span><span class=\"err\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"subcategory\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Kayak\"</span><span class=\"err\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"task\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Flight Search\"</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\"><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"func_name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"complex_to_polar\"</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"func_desc\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Converts a complex number to its polar coordinate representation.\"</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"tools\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"[{</span><span class=\"se\">\\\"</span><span class=\"s2\">type</span><span class=\"se\">\\\"</span><span class=\"s2\">:</span><span class=\"se\">\\\"</span><span class=\"s2\">function</span><span class=\"se\">\\\"</span><span class=\"s2\">,</span><span class=\"se\">\\\"</span><span class=\"s2\">function</span><span class=\"se\">\\\"</span><span class=\"s2\">:{</span><span class=\"se\">\\\"</span><span class=\"s2\">name</span><span class=\"se\">\\\"</span><span class=\"s2\">:</span><span class=\"se\">\\\"</span><span class=\"s2\">complex_to_polar</span><span class=\"se\">\\\"</span><span class=\"s2\">,</span><span class=\"se\">\\\"</span><span class=\"s2\">description</span><span class=\"se\">\\\"</span><span class=\"s2\">:</span><span class=\"se\">\\\"</span><span class=\"s2\">Converts a complex number to its polar coordinate representation.</span><span class=\"se\">\\\"</span><span class=\"s2\">,</span><span class=\"se\">\\\"</span><span class=\"s2\">parameters</span><span class=\"se\">\\\"</span><span class=\"s2\">:{</span><span class=\"se\">\\\"</span><span class=\"s2\">type</span><span class=\"se\">\\\"</span><span class=\"s2\">:</span><span class=\"se\">\\\"</span><span class=\"s2\">object</span><span class=\"se\">\\\"</span><span class=\"s2\">,</span><span class=\"se\">\\\"</span><span class=\"s2\">properties</span><span class=\"se\">\\\"</span><span class=\"s2\">:{</span><span class=\"se\">\\\"</span><span class=\"s2\">complex_number</span><span class=\"se\">\\\"</span><span class=\"s2\">:{</span><span class=\"se\">\\\"</span><span class=\"s2\">type</span><span class=\"se\">\\\"</span><span class=\"s2\">:</span><span class=\"se\">\\\"</span><span class=\"s2\">object</span><span class=\"se\">\\\"</span><span class=\"s2\">,</span><span class=\"se\">\\\"</span><span class=\"s2\">description</span><span class=\"se\">\\\"</span><span class=\"s2\">:</span><span class=\"se\">\\\"</span><span class=\"s2\">A complex number in the form of `real + imaginary * 1j`.</span><span class=\"se\">\\\"</span><span class=\"s2\">}},</span><span class=\"se\">\\\"</span><span class=\"s2\">required</span><span class=\"se\">\\\"</span><span class=\"s2\">:[</span><span class=\"se\">\\\"</span><span class=\"s2\">complex_number</span><span class=\"se\">\\\"</span><span class=\"s2\">]}}}]\"</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"query\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"I'd like to convert the complex number 3 + 4j and 1 - 2j to polar coordinates.\"</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"answers\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"[{</span><span class=\"se\">\\\"</span><span class=\"s2\">name</span><span class=\"se\">\\\"</span><span class=\"s2\">: </span><span class=\"se\">\\\"</span><span class=\"s2\">complex_to_polar</span><span class=\"se\">\\\"</span><span class=\"s2\">, </span><span class=\"se\">\\\"</span><span class=\"s2\">arguments</span><span class=\"se\">\\\"</span><span class=\"s2\">: {</span><span class=\"se\">\\\"</span><span class=\"s2\">complex_number</span><span class=\"se\">\\\"</span><span class=\"s2\">: </span><span class=\"se\">\\\"</span><span class=\"s2\">3 + 4j</span><span class=\"se\">\\\"</span><span class=\"s2\">}}, {</span><span class=\"se\">\\\"</span><span class=\"s2\">name</span><span class=\"se\">\\\"</span><span class=\"s2\">: </span><span class=\"se\">\\\"</span><span class=\"s2\">complex_to_polar</span><span class=\"se\">\\\"</span><span class=\"s2\">, </span><span class=\"se\">\\\"</span><span class=\"s2\">arguments</span><span class=\"se\">\\\"</span><span class=\"s2\">: {</span><span class=\"se\">\\\"</span><span class=\"s2\">complex_number</span><span class=\"se\">\\\"</span><span class=\"s2\">: </span><span class=\"se\">\\\"</span><span class=\"s2\">1 - 2j</span><span class=\"se\">\\\"</span><span class=\"s2\">}}]\"</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"model_name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"meta-llama/Meta-Llama-3.1-70B-Instruct\"</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"hash_id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"f873783c04bbddd9d79f47287fa3b6705b3eaea0e5bc126fba91366f7b8b07e9\"</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"p\">]</span><span class=\"err\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"category\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"E-commerce Platforms\"</span><span class=\"err\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"subcategory\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Kayak\"</span><span class=\"err\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"task\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Flight Search\"</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre>\n<h4 id=\"evaluation\">Evaluation</h4>\n<ul>\n  <li>To ensure that Tool Use capabilities meet the demands of diverse real-world scenarios, it is crucial to evaluate the function-calling performance of LLMs rigorously. This evaluation encompasses assessing model performance across both Python and non-Python programming environments, with a focus on how effectively the model can execute functions, select the appropriate tools, and discern when a function is necessary within a conversational context. An essential aspect of this evaluation is testing the model’s ability to invoke functions accurately based on user prompts and determine whether certain functions are applicable or needed.</li>\n  <li>This structured evaluation methodology enables a holistic understanding of the model’s function-calling performance, combining both syntactic accuracy and real-world execution fidelity. By examining the model’s ability to navigate various programming contexts and detect relevance in function invocation, this approach underscores the practical reliability of LLMs in diverse applications.</li>\n</ul>\n<h5 id=\"berkeley-function-calling-leaderboard\"><a href=\"https://gorilla.cs.berkeley.edu/leaderboard.html\">Berkeley Function-Calling Leaderboard</a></h5>\n<ul>\n  <li>Berkeley Function-Calling Leaderboard (BFCL) assesses the function-calling capabilities of various LLMs. It consists of 2,000 question-function-answer pairs across multiple programming languages (Python, Java, JavaScript, REST API, SQL).</li>\n  <li>The evaluation covers complex use cases, including simple, multiple, and parallel function calls, requiring the selection and simultaneous execution of functions.</li>\n  <li>BFCL tests function relevance detection to see how models handle irrelevant functions, expecting them to return an error message.</li>\n  <li>Both proprietary and open-source models perform similarly in simple scenarios, but GPT-series models excel in more complex function-calling tasks.</li>\n  <li>The Gorilla OpenFunctions dataset has expanded from 100 to 2,000 data points, increasing diversity and complexity in evaluations. The dataset includes functions from varied fields such as Mathematics, Sports, Finance, and more, covering 40 sub-domains.</li>\n  <li>Evaluations are divided into Python (simple, multiple, parallel, parallel multiple functions) and Non-Python (chatting capability, function relevance, REST API, SQL, Java, JavaScript) categories.</li>\n  <li>Python evaluations cover scenarios from single function calls to complex parallel multiple function calls.</li>\n  <li>Non-Python evaluations test models on general-purpose chat, relevance detection, and specific API and language scenarios.</li>\n  <li>Function relevance detection is a key focus, evaluating whether models avoid using irrelevant functions and highlighting their potential for hallucination.</li>\n  <li>REST API testing involves real-world GET requests with parameters in URLs and headers, assessing models’ ability to generate executable API calls.</li>\n  <li>SQL evaluation includes basic SQL queries, while Java and JavaScript testing focus on language-specific function-calling abilities.</li>\n  <li>BFCL uses AST evaluation to check syntax and structural accuracy, and executable evaluation to verify real-world function execution.</li>\n  <li>AST evaluation ensures function matching, parameter consistency, and type/value accuracy.</li>\n  <li>Executable function evaluation runs generated functions to verify response accuracy and consistency, particularly for REST APIs.</li>\n  <li>The evaluation approach requires complete matching of model outputs to expected results; partial matches are considered failures.</li>\n  <li>Ongoing development includes continuous updates and community feedback to refine evaluation methods, especially for SQL and chat capabilities.</li>\n</ul>\n<p><a href=\"https://gorilla.cs.berkeley.edu/leaderboard.html\"><img src=\"/primers/ai/assets/LLM/BFCL.png\" alt=\"\"></a></p>\n<h5 id=\"python-evaluation\">Python Evaluation</h5>\n<ul>\n  <li>Inspired by the <a href=\"https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html\">Berkeley Function-Calling Leaderboard</a> and <a href=\"https://arxiv.org/abs/2406.18518\">APIGen</a>, the evaluation framework can be organized by function type (simple, multiple, parallel, parallel multiple) or evaluation method (AST or execution of APIs). This categorization helps in comparing model performances on standard function-calling scenarios and assessing their accuracy and efficiency. By structuring the evaluation in this way, it provides a comprehensive view of how well the model performs across different types of function calls and under varying conditions. More on the section on <a href=\"#evaluation-methods\">Evaluation Methods</a>.</li>\n  <li>The Python evaluation categories, listed below, assess the model’s ability to handle single and multiple function calls, both sequentially and in parallel. These tests simulate realistic scenarios where the model must interpret user queries, select appropriate functions, and execute them accurately, mimicking real-world applications. By testing these different scenarios, the evaluation can highlight the model’s proficiency in using Python-based function calls under varying degrees of complexity and concurrency.</li>\n</ul>\n<ol>\n  <li>\n    <p><strong>Simple Function:</strong> In this category, the evaluation involves a single, straightforward function call. The user provides a JSON function document, and the model is expected to invoke only one function call. This test examines the model’s ability to handle the most common and basic type of function call correctly.</p>\n  </li>\n  <li>\n    <p><strong>Parallel Function:</strong> This evaluation scenario requires the model to make multiple function calls in parallel in response to a single user query. The model must identify how many function calls are necessary and initiate them simultaneously, regardless of the complexity or length of the user query.</p>\n  </li>\n  <li>\n    <p><strong>Multiple Function:</strong> This category involves scenarios where the user input can be matched to one function call out of two to four available JSON function documentations. The model must accurately select the most appropriate function to call based on the given context.</p>\n  </li>\n  <li>\n    <p><strong>Parallel Multiple Function:</strong> This is a complex evaluation combining both parallel and multiple function categories. The model is presented with multiple function documentations, and each relevant function may need to be invoked zero or more times in parallel.</p>\n  </li>\n</ol>\n<p><strong>Simple Function:</strong> In this category, the evaluation involves a single, straightforward function call. The user provides a JSON function document, and the model is expected to invoke only one function call. This test examines the model’s ability to handle the most common and basic type of function call correctly.</p>\n<p><strong>Parallel Function:</strong> This evaluation scenario requires the model to make multiple function calls in parallel in response to a single user query. The model must identify how many function calls are necessary and initiate them simultaneously, regardless of the complexity or length of the user query.</p>\n<p><strong>Multiple Function:</strong> This category involves scenarios where the user input can be matched to one function call out of two to four available JSON function documentations. The model must accurately select the most appropriate function to call based on the given context.</p>\n<p><strong>Parallel Multiple Function:</strong> This is a complex evaluation combining both parallel and multiple function categories. The model is presented with multiple function documentations, and each relevant function may need to be invoked zero or more times in parallel.</p>\n<ul>\n  <li>As mentioned earlier, each Python evaluation category includes both Abstract Syntax Tree (AST) and executable evaluations. A significant limitation of AST evaluation is the variety of methods available to construct function calls that achieve the same result, leading to challenges in consistency and accuracy. In these cases, executable evaluations provide a more reliable alternative by directly running the code to verify outcomes, allowing for precise and practical validation of functionality across different coding approaches.</li>\n</ul>\n<h5 id=\"non-python-evaluation\">Non-Python Evaluation</h5>\n<ul>\n  <li>The non-Python evaluation categories, listed below, test the model’s ability to handle diverse scenarios involving conversation, relevance detection, and the use of different programming languages and technologies. These evaluations provide insights into the model’s adaptability to various contexts beyond Python. By including these diverse categories, the evaluation aims to ensure that the model is versatile and capable of handling various use cases, making it applicable in a broad range of applications.</li>\n</ul>\n<ol>\n  <li>\n    <p><strong>Chatting Capability:</strong> This category evaluates the model’s general conversational abilities without invoking functions. The goal is to see if the model can maintain coherent dialogue and recognize when function calls are unnecessary. This is distinct from function relevance detection, which involves determining the suitability of invoking any provided functions.</p>\n  </li>\n  <li>\n    <p><strong>Function Relevance Detection:</strong> This tests whether the model can discern when none of the provided functions are relevant. The ideal outcome is that the model refrains from making any function calls, demonstrating an understanding of when it lacks the required function information or user instruction.</p>\n  </li>\n  <li>\n    <p><strong>REST API:</strong> This evaluation focuses on the model’s ability to generate and execute realistic REST API calls using Python’s requests library. It tests the model’s understanding of GET requests, including path and query parameters, and its ability to generate calls that match real-world API documentation.</p>\n  </li>\n  <li>\n    <p><strong>SQL:</strong> This category assesses the model’s ability to construct simple SQL queries using custom <code class=\"language-plaintext highlighter-rouge\">sql.execute</code> functions. The evaluation is limited to basic SQL operations like SELECT, INSERT, UPDATE, DELETE, and CREATE, testing whether the model can generalize function-calling capabilities beyond Python.</p>\n  </li>\n  <li>\n    <p><strong>Java + JavaScript:</strong> Despite the uniformity in function-calling formats across languages, this evaluation examines how well the model adapts to language-specific types and syntax, such as Java’s HashMap. It includes examples that test the model’s handling of Java and JavaScript, emphasizing the need for language-specific adaptations.</p>\n  </li>\n</ol>\n<p><strong>Chatting Capability:</strong> This category evaluates the model’s general conversational abilities without invoking functions. The goal is to see if the model can maintain coherent dialogue and recognize when function calls are unnecessary. This is distinct from function relevance detection, which involves determining the suitability of invoking any provided functions.</p>\n<p><strong>Function Relevance Detection:</strong> This tests whether the model can discern when none of the provided functions are relevant. The ideal outcome is that the model refrains from making any function calls, demonstrating an understanding of when it lacks the required function information or user instruction.</p>\n<p><strong>REST API:</strong> This evaluation focuses on the model’s ability to generate and execute realistic REST API calls using Python’s requests library. It tests the model’s understanding of GET requests, including path and query parameters, and its ability to generate calls that match real-world API documentation.</p>\n<p><strong>SQL:</strong> This category assesses the model’s ability to construct simple SQL queries using custom <code class=\"language-plaintext highlighter-rouge\">sql.execute</code> functions. The evaluation is limited to basic SQL operations like SELECT, INSERT, UPDATE, DELETE, and CREATE, testing whether the model can generalize function-calling capabilities beyond Python.</p>\n<p><strong>Java + JavaScript:</strong> Despite the uniformity in function-calling formats across languages, this evaluation examines how well the model adapts to language-specific types and syntax, such as Java’s HashMap. It includes examples that test the model’s handling of Java and JavaScript, emphasizing the need for language-specific adaptations.</p>\n<h5 id=\"evaluation-methods\">Evaluation Methods</h5>\n<ul>\n  <li>\n    <p>Two primary methods are used to evaluate model performance:</p>\n\n    <ol>\n      <li>\n        <p><strong>Abstract Syntax Tree (AST) Evaluation:</strong> AST evaluation involves parsing the model-generated function calls to check their structure against expected outputs. It verifies the function name, parameter presence, and type correctness. AST evaluation is ideal for cases where execution isn’t feasible due to language constraints or when the result cannot be easily executed.</p>\n\n        <ul>\n          <li><strong>Simple Function AST Evaluation</strong>\n            <ul>\n              <li>The AST evaluation process focuses on comparing a single model output function against its function doc and possible answers. Here is a flow chart (<a href=\"https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html\">source</a>) that shows the step-by-step evaluation process.\n  <img src=\"/primers/ai/assets/agents/AST.jpg\" alt=\"\"></li>\n            </ul>\n          </li>\n          <li><strong>Multiple/Parallel/Parallel-Multiple Functions AST Evaluation</strong>\n            <ul>\n              <li>The multiple, parallel, or parallel-multiple function AST evaluation process extends the idea in the simple function evaluation to support multiple model outputs and possible answers.\n                <ul>\n                  <li>The evaluation process first associates each possible answer with its function doc. Then it iterates over the model outputs and calls the simple function evaluation on each function (which takes in one model output, one possible answer, and one function doc).\n                    <ul>\n                      <li>The order of model outputs relative to possible answers is not required. A model output can match with any possible answer.</li>\n                    </ul>\n                  </li>\n                </ul>\n              </li>\n              <li>The evaluation employs an all-or-nothing approach to evaluation. Failure to find a match across all model outputs for any given possible answer results in a failed evaluation.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Executable Function Evaluation:</strong> This metric assesses the model by executing the function calls it generates and comparing the outputs against expected results. This evaluation is crucial for testing real-world applicability, focusing on whether the function calls run successfully, produce the correct types of responses, and maintain structural consistency in their outputs.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p>The combination of AST and executable evaluations ensures a comprehensive assessment, providing insights into both the syntactic and functional correctness of the model’s output.</p>\n  </li>\n</ul>\n<p>Two primary methods are used to evaluate model performance:</p>\n<ol>\n      <li>\n        <p><strong>Abstract Syntax Tree (AST) Evaluation:</strong> AST evaluation involves parsing the model-generated function calls to check their structure against expected outputs. It verifies the function name, parameter presence, and type correctness. AST evaluation is ideal for cases where execution isn’t feasible due to language constraints or when the result cannot be easily executed.</p>\n\n        <ul>\n          <li><strong>Simple Function AST Evaluation</strong>\n            <ul>\n              <li>The AST evaluation process focuses on comparing a single model output function against its function doc and possible answers. Here is a flow chart (<a href=\"https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html\">source</a>) that shows the step-by-step evaluation process.\n  <img src=\"/primers/ai/assets/agents/AST.jpg\" alt=\"\"></li>\n            </ul>\n          </li>\n          <li><strong>Multiple/Parallel/Parallel-Multiple Functions AST Evaluation</strong>\n            <ul>\n              <li>The multiple, parallel, or parallel-multiple function AST evaluation process extends the idea in the simple function evaluation to support multiple model outputs and possible answers.\n                <ul>\n                  <li>The evaluation process first associates each possible answer with its function doc. Then it iterates over the model outputs and calls the simple function evaluation on each function (which takes in one model output, one possible answer, and one function doc).\n                    <ul>\n                      <li>The order of model outputs relative to possible answers is not required. A model output can match with any possible answer.</li>\n                    </ul>\n                  </li>\n                </ul>\n              </li>\n              <li>The evaluation employs an all-or-nothing approach to evaluation. Failure to find a match across all model outputs for any given possible answer results in a failed evaluation.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Executable Function Evaluation:</strong> This metric assesses the model by executing the function calls it generates and comparing the outputs against expected results. This evaluation is crucial for testing real-world applicability, focusing on whether the function calls run successfully, produce the correct types of responses, and maintain structural consistency in their outputs.</p>\n      </li>\n    </ol>\n<p><strong>Abstract Syntax Tree (AST) Evaluation:</strong> AST evaluation involves parsing the model-generated function calls to check their structure against expected outputs. It verifies the function name, parameter presence, and type correctness. AST evaluation is ideal for cases where execution isn’t feasible due to language constraints or when the result cannot be easily executed.</p>\n<ul>\n          <li><strong>Simple Function AST Evaluation</strong>\n            <ul>\n              <li>The AST evaluation process focuses on comparing a single model output function against its function doc and possible answers. Here is a flow chart (<a href=\"https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html\">source</a>) that shows the step-by-step evaluation process.\n  <img src=\"/primers/ai/assets/agents/AST.jpg\" alt=\"\"></li>\n            </ul>\n          </li>\n          <li><strong>Multiple/Parallel/Parallel-Multiple Functions AST Evaluation</strong>\n            <ul>\n              <li>The multiple, parallel, or parallel-multiple function AST evaluation process extends the idea in the simple function evaluation to support multiple model outputs and possible answers.\n                <ul>\n                  <li>The evaluation process first associates each possible answer with its function doc. Then it iterates over the model outputs and calls the simple function evaluation on each function (which takes in one model output, one possible answer, and one function doc).\n                    <ul>\n                      <li>The order of model outputs relative to possible answers is not required. A model output can match with any possible answer.</li>\n                    </ul>\n                  </li>\n                </ul>\n              </li>\n              <li>The evaluation employs an all-or-nothing approach to evaluation. Failure to find a match across all model outputs for any given possible answer results in a failed evaluation.</li>\n            </ul>\n          </li>\n        </ul>\n<ul>\n              <li>The AST evaluation process focuses on comparing a single model output function against its function doc and possible answers. Here is a flow chart (<a href=\"https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html\">source</a>) that shows the step-by-step evaluation process.\n  <img src=\"/primers/ai/assets/agents/AST.jpg\" alt=\"\"></li>\n            </ul>\n<ul>\n              <li>The multiple, parallel, or parallel-multiple function AST evaluation process extends the idea in the simple function evaluation to support multiple model outputs and possible answers.\n                <ul>\n                  <li>The evaluation process first associates each possible answer with its function doc. Then it iterates over the model outputs and calls the simple function evaluation on each function (which takes in one model output, one possible answer, and one function doc).\n                    <ul>\n                      <li>The order of model outputs relative to possible answers is not required. A model output can match with any possible answer.</li>\n                    </ul>\n                  </li>\n                </ul>\n              </li>\n              <li>The evaluation employs an all-or-nothing approach to evaluation. Failure to find a match across all model outputs for any given possible answer results in a failed evaluation.</li>\n            </ul>\n<ul>\n                  <li>The evaluation process first associates each possible answer with its function doc. Then it iterates over the model outputs and calls the simple function evaluation on each function (which takes in one model output, one possible answer, and one function doc).\n                    <ul>\n                      <li>The order of model outputs relative to possible answers is not required. A model output can match with any possible answer.</li>\n                    </ul>\n                  </li>\n                </ul>\n<ul>\n                      <li>The order of model outputs relative to possible answers is not required. A model output can match with any possible answer.</li>\n                    </ul>\n<p><strong>Executable Function Evaluation:</strong> This metric assesses the model by executing the function calls it generates and comparing the outputs against expected results. This evaluation is crucial for testing real-world applicability, focusing on whether the function calls run successfully, produce the correct types of responses, and maintain structural consistency in their outputs.</p>\n<p>The combination of AST and executable evaluations ensures a comprehensive assessment, providing insights into both the syntactic and functional correctness of the model’s output.</p>\n<h5 id=\"inference-example-output-with-xml-and-json\">Inference Example Output with XML and JSON</h5>\n<ul>\n  <li>\n    <p>Typical function calling datasets uses a combination of both XML and JSON elements (cf. inference output sample below), as detailed below.</p>\n\n    <ol>\n      <li>\n        <p><strong>XML Structure</strong>: Elements like <code class=\"language-plaintext highlighter-rouge\">&lt;|im_start|&gt;</code>, <code class=\"language-plaintext highlighter-rouge\">&lt;tool_call&gt;</code>, and <code class=\"language-plaintext highlighter-rouge\">&lt;tool_response&gt;</code> resemble XML-like tags, which help demarcate different parts of the communication.</p>\n      </li>\n      <li>\n        <p><strong>Dictionary/JSON Structure</strong>: Within the <code class=\"language-plaintext highlighter-rouge\">&lt;tool_call&gt;</code> and <code class=\"language-plaintext highlighter-rouge\">&lt;tool_response&gt;</code> tags, the data for the function arguments and the stock fundamentals is formatted as Python-style dictionaries (or JSON-like key-value pairs), such as <code class=\"language-plaintext highlighter-rouge\">{'symbol': 'TSLA'}</code> and <code class=\"language-plaintext highlighter-rouge\">{\"name\": \"get_stock_fundamentals\", \"content\": {'symbol': 'TSLA', 'company_name': 'Tesla, Inc.' ...}}</code>.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p>This combination provides an XML-like structure for message flow and JSON for data representation, allowing for structured, nested data representation and demarcation of sections.</p>\n  </li>\n  <li>\n    <p>Here’s an example of the inference output from <a href=\"https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1\">Hermes Function-Calling V1</a>:</p>\n  </li>\n</ul>\n<p>Typical function calling datasets uses a combination of both XML and JSON elements (cf. inference output sample below), as detailed below.</p>\n<ol>\n      <li>\n        <p><strong>XML Structure</strong>: Elements like <code class=\"language-plaintext highlighter-rouge\">&lt;|im_start|&gt;</code>, <code class=\"language-plaintext highlighter-rouge\">&lt;tool_call&gt;</code>, and <code class=\"language-plaintext highlighter-rouge\">&lt;tool_response&gt;</code> resemble XML-like tags, which help demarcate different parts of the communication.</p>\n      </li>\n      <li>\n        <p><strong>Dictionary/JSON Structure</strong>: Within the <code class=\"language-plaintext highlighter-rouge\">&lt;tool_call&gt;</code> and <code class=\"language-plaintext highlighter-rouge\">&lt;tool_response&gt;</code> tags, the data for the function arguments and the stock fundamentals is formatted as Python-style dictionaries (or JSON-like key-value pairs), such as <code class=\"language-plaintext highlighter-rouge\">{'symbol': 'TSLA'}</code> and <code class=\"language-plaintext highlighter-rouge\">{\"name\": \"get_stock_fundamentals\", \"content\": {'symbol': 'TSLA', 'company_name': 'Tesla, Inc.' ...}}</code>.</p>\n      </li>\n    </ol>\n<p><strong>XML Structure</strong>: Elements like <code class=\"language-plaintext highlighter-rouge\">&lt;|im_start|&gt;</code>, <code class=\"language-plaintext highlighter-rouge\">&lt;tool_call&gt;</code>, and <code class=\"language-plaintext highlighter-rouge\">&lt;tool_response&gt;</code> resemble XML-like tags, which help demarcate different parts of the communication.</p>\n<p><strong>Dictionary/JSON Structure</strong>: Within the <code class=\"language-plaintext highlighter-rouge\">&lt;tool_call&gt;</code> and <code class=\"language-plaintext highlighter-rouge\">&lt;tool_response&gt;</code> tags, the data for the function arguments and the stock fundamentals is formatted as Python-style dictionaries (or JSON-like key-value pairs), such as <code class=\"language-plaintext highlighter-rouge\">{'symbol': 'TSLA'}</code> and <code class=\"language-plaintext highlighter-rouge\">{\"name\": \"get_stock_fundamentals\", \"content\": {'symbol': 'TSLA', 'company_name': 'Tesla, Inc.' ...}}</code>.</p>\n<p>This combination provides an XML-like structure for message flow and JSON for data representation, allowing for structured, nested data representation and demarcation of sections.</p>\n<p>Here’s an example of the inference output from <a href=\"https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1\">Hermes Function-Calling V1</a>:</p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\">&lt;|im_start|&gt;user\nFetch the stock fundamentals data for Tesla (TSLA)&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;tool_call&gt;\n{'arguments': {'symbol': 'TSLA'}, 'name': 'get_stock_fundamentals'}\n&lt;/tool_call&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;tool\n&lt;tool_response&gt;\n{\"name\": \"get_stock_fundamentals\", \"content\": {'symbol': 'TSLA', 'company_name': 'Tesla, Inc.', 'sector': 'Consumer Cyclical', 'industry': 'Auto Manufacturers', 'market_cap': 611384164352, 'pe_ratio': 49.604652, 'pb_ratio': 9.762013, 'dividend_yield': None, 'eps': 4.3, 'beta': 2.427, '52_week_high': 299.29, '52_week_low': 152.37}}\n&lt;/tool_response&gt;\n&lt;|im_end|&gt;\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\">&lt;|im_start|&gt;user\nFetch the stock fundamentals data for Tesla (TSLA)&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;tool_call&gt;\n{'arguments': {'symbol': 'TSLA'}, 'name': 'get_stock_fundamentals'}\n&lt;/tool_call&gt;&lt;|im_end|&gt;\n&lt;|im_start|&gt;tool\n&lt;tool_response&gt;\n{\"name\": \"get_stock_fundamentals\", \"content\": {'symbol': 'TSLA', 'company_name': 'Tesla, Inc.', 'sector': 'Consumer Cyclical', 'industry': 'Auto Manufacturers', 'market_cap': 611384164352, 'pe_ratio': 49.604652, 'pb_ratio': 9.762013, 'dividend_yield': None, 'eps': 4.3, 'beta': 2.427, '52_week_high': 299.29, '52_week_low': 152.37}}\n&lt;/tool_response&gt;\n&lt;|im_end|&gt;\n</code></pre>\n<h5 id=\"jsonstructured-outputs\">JSON/Structured Outputs</h5>\n<ul>\n  <li>Once a model is trained on a system prompt that asks for JSON-based structured outputs (below), the model should respond with only a JSON object response, based on the specific JSON schema provided.</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\">&lt;|im_start|&gt;system\nYou are a helpful assistant that answers in JSON. Here's the JSON schema you must adhere to:\\n&lt;schema&gt;\\n{schema}\\n&lt;/schema&gt;&lt;|im_end|&gt;\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\">&lt;|im_start|&gt;system\nYou are a helpful assistant that answers in JSON. Here's the JSON schema you must adhere to:\\n&lt;schema&gt;\\n{schema}\\n&lt;/schema&gt;&lt;|im_end|&gt;\n</code></pre>\n<ul>\n  <li>\n    <p>The schema can be made from a pydantic object using (e.g., a standalone script available is here from <a href=\"https://github.com/NousResearch/Hermes-Function-Calling/blob/main/jsonmode.py\">Hermes Function-Calling V1</a>).</p>\n  </li>\n  <li>\n    <p>As an example from <a href=\"https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1\">Hermes Function-Calling V1</a>:</p>\n  </li>\n</ul>\n<p>The schema can be made from a pydantic object using (e.g., a standalone script available is here from <a href=\"https://github.com/NousResearch/Hermes-Function-Calling/blob/main/jsonmode.py\">Hermes Function-Calling V1</a>).</p>\n<p>As an example from <a href=\"https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1\">Hermes Function-Calling V1</a>:</p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code6\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code6\">{\n  \"id\": \"753d8365-0e54-43b1-9514-3f9b819fd31c\",\n  \"conversations\": [\n    {\n      \"from\": \"system\",\n      \"value\": \"You are a function calling AI model. You are provided with function signatures within &lt;tools&gt; &lt;/tools&gt; XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.\\n&lt;tools&gt;\\n[{'type': 'function', 'function': {'name': 'search_flights', 'description': 'Searches for flights based on departure and destination cities, dates, class, and other preferences.', 'parameters': {'type': 'object', 'properties': {'departure_city': {'type': 'string', 'description': 'The city from which the flight will depart.'}, 'destination_city': {'type': 'string', 'description': 'The destination city for the flight.'}, 'departure_date': {'type': 'string', 'description': 'The departure date for the flight.', 'format': 'date'}, 'return_date': {'type': 'string', 'description': 'The return date for the flight.', 'format': 'date'}, 'class': {'type': 'string', 'description': 'The class of the flight ticket.', 'enum': ['economy', 'business', 'first']}, 'flexible_cancellation': {'type': 'boolean', 'description': 'Indicates if the search should filter for flights with flexible cancellation policies.'}}, 'required': ['departure_city', 'destination_city', 'departure_date', 'return_date', 'class']}}}]\\n&lt;/tools&gt;\\nFor each function call return a json object with function name and arguments within &lt;tool_call&gt; &lt;/tool_call&gt; tags with the following schema:\\n&lt;tool_call&gt;\\n{'arguments': &lt;args-dict&gt;, 'name': &lt;function-name&gt;}\\n&lt;/tool_call&gt;\\n\"\n    },\n    {\n      \"from\": \"human\",\n      \"value\": \"I'm planning a kayaking trip and looking to book flights from Los Angeles to Auckland. My departure is scheduled for July 10th, 2023, and I intend to return on July 24th, 2023. I would prefer to travel in economy class and would also like the option to have flexible cancellation policies for the tickets due to the uncertain nature of outdoor activities. Could you please search for flights that meet these criteria and provide me with the available options?\"\n    },\n    {\n      \"from\": \"gpt\",\n      \"value\": \"&lt;tool_call&gt;\\n{'arguments': {'departure_city': 'Los Angeles', 'destination_city': 'Auckland', 'departure_date': '2023-07-10', 'return_date': '2023-07-24', 'class': 'economy', 'flexible_cancellation': True}, 'name': 'search_flights'}\\n&lt;/tool_call&gt;\\n\"\n    },\n    {\n      \"from\": \"tool\",\n      \"value\": \"&lt;tool_response&gt;\\n{\\\"name\\\": \\\"search_flights\\\", \\\"content\\\": {\\\"flights\\\": [{\\\"flight_number\\\": \\\"AA118\\\", \\\"airline\\\": \\\"American Airlines\\\", \\\"departure_airport\\\": \\\"LAX\\\", \\\"departure_city\\\": \\\"Los Angeles\\\", \\\"departure_date\\\": \\\"2023-07-10T08:00:00Z\\\", \\\"departure_terminal\\\": \\\"4\\\", \\\"arrival_airport\\\": \\\"AKL\\\", \\\"arrival_city\\\": \\\"Auckland\\\", \\\"arrival_date\\\": \\\"2023-07-11T14:25:00Z\\\", \\\"arrival_terminal\\\": \\\"I\\\", \\\"duration\\\": \\\"17h 25m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$950\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"UA796\\\", \\\"airline\\\": \\\"United Airlines\\\", \\\"departure_airport\\\": \\\"LAX\\\", \\\"departure_city\\\": \\\"Los Angeles\\\", \\\"departure_date\\\": \\\"2023-07-10T11:30:00Z\\\", \\\"departure_terminal\\\": \\\"7\\\", \\\"arrival_airport\\\": \\\"AKL\\\", \\\"arrival_city\\\": \\\"Auckland\\\", \\\"arrival_date\\\": \\\"2023-07-11T18:30:00Z\\\", \\\"arrival_terminal\\\": \\\"I\\\", \\\"duration\\\": \\\"16h 0m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$975\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"NZ6\\\", \\\"airline\\\": \\\"Air New Zealand\\\", \\\"departure_airport\\\": \\\"LAX\\\", \\\"departure_city\\\": \\\"Los Angeles\\\", \\\"departure_date\\\": \\\"2023-07-10T21:00:00Z\\\", \\\"departure_terminal\\\": \\\"B\\\", \\\"arrival_airport\\\": \\\"AKL\\\", \\\"arrival_city\\\": \\\"Auckland\\\", \\\"arrival_date\\\": \\\"2023-07-12T05:20:00Z\\\", \\\"arrival_terminal\\\": \\\"I\\\", \\\"duration\\\": \\\"13h 20m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$1020\\\", \\\"flexible_cancellation\\\": true}], \\\"return_flights\\\": [{\\\"flight_number\\\": \\\"AA117\\\", \\\"airline\\\": \\\"American Airlines\\\", \\\"departure_airport\\\": \\\"AKL\\\", \\\"departure_city\\\": \\\"Auckland\\\", \\\"departure_date\\\": \\\"2023-07-24T14:10:00Z\\\", \\\"departure_terminal\\\": \\\"I\\\", \\\"arrival_airport\\\": \\\"LAX\\\", \\\"arrival_city\\\": \\\"Los Angeles\\\", \\\"arrival_date\\\": \\\"2023-07-24T06:00:00Z\\\", \\\"arrival_terminal\\\": \\\"4\\\", \\\"duration\\\": \\\"12h 50m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$950\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"UA797\\\", \\\"airline\\\": \\\"United Airlines\\\", \\\"departure_airport\\\": \\\"AKL\\\", \\\"departure_city\\\": \\\"Auckland\\\", \\\"departure_date\\\": \\\"2023-07-24T19:15:00Z\\\", \\\"departure_terminal\\\": \\\"I\\\", \\\"arrival_airport\\\": \\\"LAX\\\", \\\"arrival_city\\\": \\\"Los Angeles\\\", \\\"arrival_date\\\": \\\"2023-07-24T10:15:00Z\\\", \\\"arrival_terminal\\\": \\\"7\\\", \\\"duration\\\": \\\"11h 0m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$975\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"NZ5\\\", \\\"airline\\\": \\\"Air New Zealand\\\", \\\"departure_airport\\\": \\\"AKL\\\", \\\"departure_city\\\": \\\"Auckland\\\", \\\"departure_date\\\": \\\"2023-07-24T19:30:00Z\\\", \\\"departure_terminal\\\": \\\"I\\\", \\\"arrival_airport\\\": \\\"LAX\\\", \\\"arrival_city\\\": \\\"Los Angeles\\\", \\\"arrival_date\\\": \\\"2023-07-24T10:30:00Z\\\", \\\"arrival_terminal\\\": \\\"B\\\", \\\"duration\\\": \\\"12h 0m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$1020\\\", \\\"flexible_cancellation\\\": true}]}}\\n&lt;/tool_response&gt;\\n\"\n    },\n    {\n      \"from\": \"gpt\",\n      \"value\": \"Here are the available flight options with flexible cancellation policies for your kayaking trip from Los Angeles to Auckland:\\n\\n**Outbound Flights (Los Angeles to Auckland):**\\n1. **American Airlines AA118**\\n   - Departure: July 10, 2023, 8:00 AM from LAX Terminal 4\\n   - Arrival: July 11, 2023, 2:25 PM at AKL Terminal I\\n   - Duration: 17h 25m\\n   - Price: $950\\n\\n2. **United Airlines UA796**\\n   - Departure: July 10, 2023, 11:30 AM from LAX Terminal 7\\n   - Arrival: July 11, 2023, 6:30 PM at AKL Terminal I\\n   - Duration: 16h 0m\\n   - Price: $975\\n\\n3. **Air New Zealand NZ6**\\n   - Departure: July 10, 2023, 9:00 PM from LAX Terminal B\\n   - Arrival: July 12, 2023, 5:20 AM at AKL Terminal I\\n   - Duration: 13h 20m\\n   - Price: $1020\\n\\n**Return Flights (Auckland to Los Angeles):**\\n1. **American Airlines AA117**\\n   - Departure: July 24, 2023, 2:10 PM from AKL Terminal I\\n   - Arrival: July 24, 2023, 6:00 AM at LAX Terminal 4\\n   - Duration: 12h 50m\\n   - Price: $950\\n\\n2. **United Airlines UA797**\\n   - Departure: July 24, 2023, 7:15 PM from AKL Terminal I\\n   - Arrival: July 24, 2023, 10:15 AM at LAX Terminal 7\\n   - Duration: 11h 0m\\n   - Price: $975\\n\\n3. **Air New Zealand NZ5**\\n   - Departure: July 24, 2023, 7:30 PM from AKL Terminal I\\n   - Arrival: July 24, 2023, 10:30 AM at LAX Terminal B\\n   - Duration: 12h 0m\\n   - Price: $1020\\n\\nAll the listed prices are in economy class with flexible cancellation policies. Please review the options and let me know if you would like to proceed with booking any of these flights or if you need further assistance.\"\n    }\n  ],\n  \"category\": \"E-commerce Platforms\",\n  \"subcategory\": \"Kayak\",\n  \"task\": \"Flight Search\"\n}\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code6\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code6\">{\n  \"id\": \"753d8365-0e54-43b1-9514-3f9b819fd31c\",\n  \"conversations\": [\n    {\n      \"from\": \"system\",\n      \"value\": \"You are a function calling AI model. You are provided with function signatures within &lt;tools&gt; &lt;/tools&gt; XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.\\n&lt;tools&gt;\\n[{'type': 'function', 'function': {'name': 'search_flights', 'description': 'Searches for flights based on departure and destination cities, dates, class, and other preferences.', 'parameters': {'type': 'object', 'properties': {'departure_city': {'type': 'string', 'description': 'The city from which the flight will depart.'}, 'destination_city': {'type': 'string', 'description': 'The destination city for the flight.'}, 'departure_date': {'type': 'string', 'description': 'The departure date for the flight.', 'format': 'date'}, 'return_date': {'type': 'string', 'description': 'The return date for the flight.', 'format': 'date'}, 'class': {'type': 'string', 'description': 'The class of the flight ticket.', 'enum': ['economy', 'business', 'first']}, 'flexible_cancellation': {'type': 'boolean', 'description': 'Indicates if the search should filter for flights with flexible cancellation policies.'}}, 'required': ['departure_city', 'destination_city', 'departure_date', 'return_date', 'class']}}}]\\n&lt;/tools&gt;\\nFor each function call return a json object with function name and arguments within &lt;tool_call&gt; &lt;/tool_call&gt; tags with the following schema:\\n&lt;tool_call&gt;\\n{'arguments': &lt;args-dict&gt;, 'name': &lt;function-name&gt;}\\n&lt;/tool_call&gt;\\n\"\n    },\n    {\n      \"from\": \"human\",\n      \"value\": \"I'm planning a kayaking trip and looking to book flights from Los Angeles to Auckland. My departure is scheduled for July 10th, 2023, and I intend to return on July 24th, 2023. I would prefer to travel in economy class and would also like the option to have flexible cancellation policies for the tickets due to the uncertain nature of outdoor activities. Could you please search for flights that meet these criteria and provide me with the available options?\"\n    },\n    {\n      \"from\": \"gpt\",\n      \"value\": \"&lt;tool_call&gt;\\n{'arguments': {'departure_city': 'Los Angeles', 'destination_city': 'Auckland', 'departure_date': '2023-07-10', 'return_date': '2023-07-24', 'class': 'economy', 'flexible_cancellation': True}, 'name': 'search_flights'}\\n&lt;/tool_call&gt;\\n\"\n    },\n    {\n      \"from\": \"tool\",\n      \"value\": \"&lt;tool_response&gt;\\n{\\\"name\\\": \\\"search_flights\\\", \\\"content\\\": {\\\"flights\\\": [{\\\"flight_number\\\": \\\"AA118\\\", \\\"airline\\\": \\\"American Airlines\\\", \\\"departure_airport\\\": \\\"LAX\\\", \\\"departure_city\\\": \\\"Los Angeles\\\", \\\"departure_date\\\": \\\"2023-07-10T08:00:00Z\\\", \\\"departure_terminal\\\": \\\"4\\\", \\\"arrival_airport\\\": \\\"AKL\\\", \\\"arrival_city\\\": \\\"Auckland\\\", \\\"arrival_date\\\": \\\"2023-07-11T14:25:00Z\\\", \\\"arrival_terminal\\\": \\\"I\\\", \\\"duration\\\": \\\"17h 25m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$950\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"UA796\\\", \\\"airline\\\": \\\"United Airlines\\\", \\\"departure_airport\\\": \\\"LAX\\\", \\\"departure_city\\\": \\\"Los Angeles\\\", \\\"departure_date\\\": \\\"2023-07-10T11:30:00Z\\\", \\\"departure_terminal\\\": \\\"7\\\", \\\"arrival_airport\\\": \\\"AKL\\\", \\\"arrival_city\\\": \\\"Auckland\\\", \\\"arrival_date\\\": \\\"2023-07-11T18:30:00Z\\\", \\\"arrival_terminal\\\": \\\"I\\\", \\\"duration\\\": \\\"16h 0m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$975\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"NZ6\\\", \\\"airline\\\": \\\"Air New Zealand\\\", \\\"departure_airport\\\": \\\"LAX\\\", \\\"departure_city\\\": \\\"Los Angeles\\\", \\\"departure_date\\\": \\\"2023-07-10T21:00:00Z\\\", \\\"departure_terminal\\\": \\\"B\\\", \\\"arrival_airport\\\": \\\"AKL\\\", \\\"arrival_city\\\": \\\"Auckland\\\", \\\"arrival_date\\\": \\\"2023-07-12T05:20:00Z\\\", \\\"arrival_terminal\\\": \\\"I\\\", \\\"duration\\\": \\\"13h 20m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$1020\\\", \\\"flexible_cancellation\\\": true}], \\\"return_flights\\\": [{\\\"flight_number\\\": \\\"AA117\\\", \\\"airline\\\": \\\"American Airlines\\\", \\\"departure_airport\\\": \\\"AKL\\\", \\\"departure_city\\\": \\\"Auckland\\\", \\\"departure_date\\\": \\\"2023-07-24T14:10:00Z\\\", \\\"departure_terminal\\\": \\\"I\\\", \\\"arrival_airport\\\": \\\"LAX\\\", \\\"arrival_city\\\": \\\"Los Angeles\\\", \\\"arrival_date\\\": \\\"2023-07-24T06:00:00Z\\\", \\\"arrival_terminal\\\": \\\"4\\\", \\\"duration\\\": \\\"12h 50m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$950\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"UA797\\\", \\\"airline\\\": \\\"United Airlines\\\", \\\"departure_airport\\\": \\\"AKL\\\", \\\"departure_city\\\": \\\"Auckland\\\", \\\"departure_date\\\": \\\"2023-07-24T19:15:00Z\\\", \\\"departure_terminal\\\": \\\"I\\\", \\\"arrival_airport\\\": \\\"LAX\\\", \\\"arrival_city\\\": \\\"Los Angeles\\\", \\\"arrival_date\\\": \\\"2023-07-24T10:15:00Z\\\", \\\"arrival_terminal\\\": \\\"7\\\", \\\"duration\\\": \\\"11h 0m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$975\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"NZ5\\\", \\\"airline\\\": \\\"Air New Zealand\\\", \\\"departure_airport\\\": \\\"AKL\\\", \\\"departure_city\\\": \\\"Auckland\\\", \\\"departure_date\\\": \\\"2023-07-24T19:30:00Z\\\", \\\"departure_terminal\\\": \\\"I\\\", \\\"arrival_airport\\\": \\\"LAX\\\", \\\"arrival_city\\\": \\\"Los Angeles\\\", \\\"arrival_date\\\": \\\"2023-07-24T10:30:00Z\\\", \\\"arrival_terminal\\\": \\\"B\\\", \\\"duration\\\": \\\"12h 0m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$1020\\\", \\\"flexible_cancellation\\\": true}]}}\\n&lt;/tool_response&gt;\\n\"\n    },\n    {\n      \"from\": \"gpt\",\n      \"value\": \"Here are the available flight options with flexible cancellation policies for your kayaking trip from Los Angeles to Auckland:\\n\\n**Outbound Flights (Los Angeles to Auckland):**\\n1. **American Airlines AA118**\\n   - Departure: July 10, 2023, 8:00 AM from LAX Terminal 4\\n   - Arrival: July 11, 2023, 2:25 PM at AKL Terminal I\\n   - Duration: 17h 25m\\n   - Price: $950\\n\\n2. **United Airlines UA796**\\n   - Departure: July 10, 2023, 11:30 AM from LAX Terminal 7\\n   - Arrival: July 11, 2023, 6:30 PM at AKL Terminal I\\n   - Duration: 16h 0m\\n   - Price: $975\\n\\n3. **Air New Zealand NZ6**\\n   - Departure: July 10, 2023, 9:00 PM from LAX Terminal B\\n   - Arrival: July 12, 2023, 5:20 AM at AKL Terminal I\\n   - Duration: 13h 20m\\n   - Price: $1020\\n\\n**Return Flights (Auckland to Los Angeles):**\\n1. **American Airlines AA117**\\n   - Departure: July 24, 2023, 2:10 PM from AKL Terminal I\\n   - Arrival: July 24, 2023, 6:00 AM at LAX Terminal 4\\n   - Duration: 12h 50m\\n   - Price: $950\\n\\n2. **United Airlines UA797**\\n   - Departure: July 24, 2023, 7:15 PM from AKL Terminal I\\n   - Arrival: July 24, 2023, 10:15 AM at LAX Terminal 7\\n   - Duration: 11h 0m\\n   - Price: $975\\n\\n3. **Air New Zealand NZ5**\\n   - Departure: July 24, 2023, 7:30 PM from AKL Terminal I\\n   - Arrival: July 24, 2023, 10:30 AM at LAX Terminal B\\n   - Duration: 12h 0m\\n   - Price: $1020\\n\\nAll the listed prices are in economy class with flexible cancellation policies. Please review the options and let me know if you would like to proceed with booking any of these flights or if you need further assistance.\"\n    }\n  ],\n  \"category\": \"E-commerce Platforms\",\n  \"subcategory\": \"Kayak\",\n  \"task\": \"Flight Search\"\n}\n</code></pre>\n<h4 id=\"gorilla-openfunctions-v2-llm\"><a href=\"https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2\">Gorilla OpenFunctions-v2 LLM</a></h4>\n<ul>\n  <li>\n    <p><strong>Overview</strong>:</p>\n\n    <ul>\n      <li><strong>State-of-the-Art Performance:</strong> Gorilla OpenFunctions-v2 is an open-source Large Language Model (LLM) that offers advanced function-calling capabilities, comparable to GPT-4.</li>\n      <li><strong>Extended Chat Completion:</strong> Extends LLM chat completion with the ability to generate executable API calls from natural language instructions and relevant API contexts.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Key Features</strong>:</p>\n\n    <ul>\n      <li><strong>Multi-Function Support:</strong>\n        <ul>\n          <li>Allows selection from multiple available functions based on user instructions, offering flexibility and adaptability within a single prompt.</li>\n        </ul>\n      </li>\n      <li><strong>Parallel Function Calling:</strong>\n        <ul>\n          <li>Supports executing the same function multiple times with different parameter values, streamlining workflows needing simultaneous function calls.</li>\n        </ul>\n      </li>\n      <li><strong>Combined Multi &amp; Parallel Functionality:</strong>\n        <ul>\n          <li>Executes both multi-function and parallel function calls in one chat completion call, handling complex API call scenarios in a single prompt for efficient, high-capability outputs.</li>\n        </ul>\n      </li>\n      <li><strong>Expanded Data Type Support:</strong>\n        <ul>\n          <li>Enhanced compatibility with diverse programming languages by supporting extensive data types:\n            <ul>\n              <li><strong>Python:</strong> Supports <code class=\"language-plaintext highlighter-rouge\">string</code>, <code class=\"language-plaintext highlighter-rouge\">number</code>, <code class=\"language-plaintext highlighter-rouge\">boolean</code>, <code class=\"language-plaintext highlighter-rouge\">list</code>, <code class=\"language-plaintext highlighter-rouge\">tuple</code>, <code class=\"language-plaintext highlighter-rouge\">dict</code>, and <code class=\"language-plaintext highlighter-rouge\">Any</code>.</li>\n              <li><strong>Java:</strong> Includes support for <code class=\"language-plaintext highlighter-rouge\">byte</code>, <code class=\"language-plaintext highlighter-rouge\">short</code>, <code class=\"language-plaintext highlighter-rouge\">int</code>, <code class=\"language-plaintext highlighter-rouge\">float</code>, <code class=\"language-plaintext highlighter-rouge\">double</code>, <code class=\"language-plaintext highlighter-rouge\">long</code>, <code class=\"language-plaintext highlighter-rouge\">boolean</code>, <code class=\"language-plaintext highlighter-rouge\">char</code>, and complex types like <code class=\"language-plaintext highlighter-rouge\">ArrayList</code>, <code class=\"language-plaintext highlighter-rouge\">Set</code>, <code class=\"language-plaintext highlighter-rouge\">HashMap</code>, and <code class=\"language-plaintext highlighter-rouge\">Stack</code>.</li>\n              <li><strong>JavaScript:</strong> Covers <code class=\"language-plaintext highlighter-rouge\">String</code>, <code class=\"language-plaintext highlighter-rouge\">Number</code>, <code class=\"language-plaintext highlighter-rouge\">BigInt</code>, <code class=\"language-plaintext highlighter-rouge\">Boolean</code>, <code class=\"language-plaintext highlighter-rouge\">Array</code>, <code class=\"language-plaintext highlighter-rouge\">Date</code>, <code class=\"language-plaintext highlighter-rouge\">dict (object)</code>, and <code class=\"language-plaintext highlighter-rouge\">Any</code>.</li>\n            </ul>\n          </li>\n          <li>Extending beyond typical JSON schema limits, this feature allows users to leverage OpenFunctions-v2 in a straightforward plug-and-play fashion without intricate data handling or reliance on string literals.</li>\n        </ul>\n      </li>\n      <li><strong>Function Relevance Detection:</strong>\n        <ul>\n          <li>Minimizes irrelevant function calls by detecting whether the user’s prompt is conversational or function-oriented.</li>\n          <li>If no function is relevant, the model raises an “Error” message with additional guidance, helping refine requests and reducing hallucinations.</li>\n        </ul>\n      </li>\n      <li><strong>Enhanced RESTful API Capabilities:</strong>\n        <ul>\n          <li>Specially trained to handle RESTful API calls, Gorilla OpenFunctions-v2 optimizes interactions with widely-used services, such as Slack and PayPal.</li>\n          <li>This high-quality support for REST API execution boosts compatibility across a broad range of applications and services.</li>\n        </ul>\n      </li>\n      <li><strong>Pioneering Open-Source Model with Seamless Integration:</strong>\n        <ul>\n          <li>As the first open-source model to support multi-language, multi-function, and parallel function calls, Gorilla OpenFunctions-v2 stands at the forefront of function calling in LLMs.</li>\n          <li>Integrates effortlessly into diverse applications, making it a seamless drop-in replacement that requires minimal setup.</li>\n        </ul>\n      </li>\n      <li><strong>Broad Application Compatibility:</strong>\n        <ul>\n          <li>Gorilla OpenFunctions-v2’s versatility supports a wide range of platforms, from social media like Instagram to delivery and utility services such as Google Calendar, Stripe, and DoorDash.</li>\n          <li>Its adaptability makes it a top choice for developers aiming to expand functional capabilities across multiple sectors with ease.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>The figure below (<a href=\"https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2\">source</a>) highlights some of the key features of OpenFunctions-v2:</p>\n  </li>\n</ul>\n<p><strong>Overview</strong>:</p>\n<ul>\n      <li><strong>State-of-the-Art Performance:</strong> Gorilla OpenFunctions-v2 is an open-source Large Language Model (LLM) that offers advanced function-calling capabilities, comparable to GPT-4.</li>\n      <li><strong>Extended Chat Completion:</strong> Extends LLM chat completion with the ability to generate executable API calls from natural language instructions and relevant API contexts.</li>\n    </ul>\n<p><strong>Key Features</strong>:</p>\n<ul>\n      <li><strong>Multi-Function Support:</strong>\n        <ul>\n          <li>Allows selection from multiple available functions based on user instructions, offering flexibility and adaptability within a single prompt.</li>\n        </ul>\n      </li>\n      <li><strong>Parallel Function Calling:</strong>\n        <ul>\n          <li>Supports executing the same function multiple times with different parameter values, streamlining workflows needing simultaneous function calls.</li>\n        </ul>\n      </li>\n      <li><strong>Combined Multi &amp; Parallel Functionality:</strong>\n        <ul>\n          <li>Executes both multi-function and parallel function calls in one chat completion call, handling complex API call scenarios in a single prompt for efficient, high-capability outputs.</li>\n        </ul>\n      </li>\n      <li><strong>Expanded Data Type Support:</strong>\n        <ul>\n          <li>Enhanced compatibility with diverse programming languages by supporting extensive data types:\n            <ul>\n              <li><strong>Python:</strong> Supports <code class=\"language-plaintext highlighter-rouge\">string</code>, <code class=\"language-plaintext highlighter-rouge\">number</code>, <code class=\"language-plaintext highlighter-rouge\">boolean</code>, <code class=\"language-plaintext highlighter-rouge\">list</code>, <code class=\"language-plaintext highlighter-rouge\">tuple</code>, <code class=\"language-plaintext highlighter-rouge\">dict</code>, and <code class=\"language-plaintext highlighter-rouge\">Any</code>.</li>\n              <li><strong>Java:</strong> Includes support for <code class=\"language-plaintext highlighter-rouge\">byte</code>, <code class=\"language-plaintext highlighter-rouge\">short</code>, <code class=\"language-plaintext highlighter-rouge\">int</code>, <code class=\"language-plaintext highlighter-rouge\">float</code>, <code class=\"language-plaintext highlighter-rouge\">double</code>, <code class=\"language-plaintext highlighter-rouge\">long</code>, <code class=\"language-plaintext highlighter-rouge\">boolean</code>, <code class=\"language-plaintext highlighter-rouge\">char</code>, and complex types like <code class=\"language-plaintext highlighter-rouge\">ArrayList</code>, <code class=\"language-plaintext highlighter-rouge\">Set</code>, <code class=\"language-plaintext highlighter-rouge\">HashMap</code>, and <code class=\"language-plaintext highlighter-rouge\">Stack</code>.</li>\n              <li><strong>JavaScript:</strong> Covers <code class=\"language-plaintext highlighter-rouge\">String</code>, <code class=\"language-plaintext highlighter-rouge\">Number</code>, <code class=\"language-plaintext highlighter-rouge\">BigInt</code>, <code class=\"language-plaintext highlighter-rouge\">Boolean</code>, <code class=\"language-plaintext highlighter-rouge\">Array</code>, <code class=\"language-plaintext highlighter-rouge\">Date</code>, <code class=\"language-plaintext highlighter-rouge\">dict (object)</code>, and <code class=\"language-plaintext highlighter-rouge\">Any</code>.</li>\n            </ul>\n          </li>\n          <li>Extending beyond typical JSON schema limits, this feature allows users to leverage OpenFunctions-v2 in a straightforward plug-and-play fashion without intricate data handling or reliance on string literals.</li>\n        </ul>\n      </li>\n      <li><strong>Function Relevance Detection:</strong>\n        <ul>\n          <li>Minimizes irrelevant function calls by detecting whether the user’s prompt is conversational or function-oriented.</li>\n          <li>If no function is relevant, the model raises an “Error” message with additional guidance, helping refine requests and reducing hallucinations.</li>\n        </ul>\n      </li>\n      <li><strong>Enhanced RESTful API Capabilities:</strong>\n        <ul>\n          <li>Specially trained to handle RESTful API calls, Gorilla OpenFunctions-v2 optimizes interactions with widely-used services, such as Slack and PayPal.</li>\n          <li>This high-quality support for REST API execution boosts compatibility across a broad range of applications and services.</li>\n        </ul>\n      </li>\n      <li><strong>Pioneering Open-Source Model with Seamless Integration:</strong>\n        <ul>\n          <li>As the first open-source model to support multi-language, multi-function, and parallel function calls, Gorilla OpenFunctions-v2 stands at the forefront of function calling in LLMs.</li>\n          <li>Integrates effortlessly into diverse applications, making it a seamless drop-in replacement that requires minimal setup.</li>\n        </ul>\n      </li>\n      <li><strong>Broad Application Compatibility:</strong>\n        <ul>\n          <li>Gorilla OpenFunctions-v2’s versatility supports a wide range of platforms, from social media like Instagram to delivery and utility services such as Google Calendar, Stripe, and DoorDash.</li>\n          <li>Its adaptability makes it a top choice for developers aiming to expand functional capabilities across multiple sectors with ease.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Allows selection from multiple available functions based on user instructions, offering flexibility and adaptability within a single prompt.</li>\n        </ul>\n<ul>\n          <li>Supports executing the same function multiple times with different parameter values, streamlining workflows needing simultaneous function calls.</li>\n        </ul>\n<ul>\n          <li>Executes both multi-function and parallel function calls in one chat completion call, handling complex API call scenarios in a single prompt for efficient, high-capability outputs.</li>\n        </ul>\n<ul>\n          <li>Enhanced compatibility with diverse programming languages by supporting extensive data types:\n            <ul>\n              <li><strong>Python:</strong> Supports <code class=\"language-plaintext highlighter-rouge\">string</code>, <code class=\"language-plaintext highlighter-rouge\">number</code>, <code class=\"language-plaintext highlighter-rouge\">boolean</code>, <code class=\"language-plaintext highlighter-rouge\">list</code>, <code class=\"language-plaintext highlighter-rouge\">tuple</code>, <code class=\"language-plaintext highlighter-rouge\">dict</code>, and <code class=\"language-plaintext highlighter-rouge\">Any</code>.</li>\n              <li><strong>Java:</strong> Includes support for <code class=\"language-plaintext highlighter-rouge\">byte</code>, <code class=\"language-plaintext highlighter-rouge\">short</code>, <code class=\"language-plaintext highlighter-rouge\">int</code>, <code class=\"language-plaintext highlighter-rouge\">float</code>, <code class=\"language-plaintext highlighter-rouge\">double</code>, <code class=\"language-plaintext highlighter-rouge\">long</code>, <code class=\"language-plaintext highlighter-rouge\">boolean</code>, <code class=\"language-plaintext highlighter-rouge\">char</code>, and complex types like <code class=\"language-plaintext highlighter-rouge\">ArrayList</code>, <code class=\"language-plaintext highlighter-rouge\">Set</code>, <code class=\"language-plaintext highlighter-rouge\">HashMap</code>, and <code class=\"language-plaintext highlighter-rouge\">Stack</code>.</li>\n              <li><strong>JavaScript:</strong> Covers <code class=\"language-plaintext highlighter-rouge\">String</code>, <code class=\"language-plaintext highlighter-rouge\">Number</code>, <code class=\"language-plaintext highlighter-rouge\">BigInt</code>, <code class=\"language-plaintext highlighter-rouge\">Boolean</code>, <code class=\"language-plaintext highlighter-rouge\">Array</code>, <code class=\"language-plaintext highlighter-rouge\">Date</code>, <code class=\"language-plaintext highlighter-rouge\">dict (object)</code>, and <code class=\"language-plaintext highlighter-rouge\">Any</code>.</li>\n            </ul>\n          </li>\n          <li>Extending beyond typical JSON schema limits, this feature allows users to leverage OpenFunctions-v2 in a straightforward plug-and-play fashion without intricate data handling or reliance on string literals.</li>\n        </ul>\n<ul>\n              <li><strong>Python:</strong> Supports <code class=\"language-plaintext highlighter-rouge\">string</code>, <code class=\"language-plaintext highlighter-rouge\">number</code>, <code class=\"language-plaintext highlighter-rouge\">boolean</code>, <code class=\"language-plaintext highlighter-rouge\">list</code>, <code class=\"language-plaintext highlighter-rouge\">tuple</code>, <code class=\"language-plaintext highlighter-rouge\">dict</code>, and <code class=\"language-plaintext highlighter-rouge\">Any</code>.</li>\n              <li><strong>Java:</strong> Includes support for <code class=\"language-plaintext highlighter-rouge\">byte</code>, <code class=\"language-plaintext highlighter-rouge\">short</code>, <code class=\"language-plaintext highlighter-rouge\">int</code>, <code class=\"language-plaintext highlighter-rouge\">float</code>, <code class=\"language-plaintext highlighter-rouge\">double</code>, <code class=\"language-plaintext highlighter-rouge\">long</code>, <code class=\"language-plaintext highlighter-rouge\">boolean</code>, <code class=\"language-plaintext highlighter-rouge\">char</code>, and complex types like <code class=\"language-plaintext highlighter-rouge\">ArrayList</code>, <code class=\"language-plaintext highlighter-rouge\">Set</code>, <code class=\"language-plaintext highlighter-rouge\">HashMap</code>, and <code class=\"language-plaintext highlighter-rouge\">Stack</code>.</li>\n              <li><strong>JavaScript:</strong> Covers <code class=\"language-plaintext highlighter-rouge\">String</code>, <code class=\"language-plaintext highlighter-rouge\">Number</code>, <code class=\"language-plaintext highlighter-rouge\">BigInt</code>, <code class=\"language-plaintext highlighter-rouge\">Boolean</code>, <code class=\"language-plaintext highlighter-rouge\">Array</code>, <code class=\"language-plaintext highlighter-rouge\">Date</code>, <code class=\"language-plaintext highlighter-rouge\">dict (object)</code>, and <code class=\"language-plaintext highlighter-rouge\">Any</code>.</li>\n            </ul>\n<ul>\n          <li>Minimizes irrelevant function calls by detecting whether the user’s prompt is conversational or function-oriented.</li>\n          <li>If no function is relevant, the model raises an “Error” message with additional guidance, helping refine requests and reducing hallucinations.</li>\n        </ul>\n<ul>\n          <li>Specially trained to handle RESTful API calls, Gorilla OpenFunctions-v2 optimizes interactions with widely-used services, such as Slack and PayPal.</li>\n          <li>This high-quality support for REST API execution boosts compatibility across a broad range of applications and services.</li>\n        </ul>\n<ul>\n          <li>As the first open-source model to support multi-language, multi-function, and parallel function calls, Gorilla OpenFunctions-v2 stands at the forefront of function calling in LLMs.</li>\n          <li>Integrates effortlessly into diverse applications, making it a seamless drop-in replacement that requires minimal setup.</li>\n        </ul>\n<ul>\n          <li>Gorilla OpenFunctions-v2’s versatility supports a wide range of platforms, from social media like Instagram to delivery and utility services such as Google Calendar, Stripe, and DoorDash.</li>\n          <li>Its adaptability makes it a top choice for developers aiming to expand functional capabilities across multiple sectors with ease.</li>\n        </ul>\n<p>The figure below (<a href=\"https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2\">source</a>) highlights some of the key features of OpenFunctions-v2:</p>\n<p><img src=\"/primers/ai/assets/agents/OpenFunctionsV2.jpg\" alt=\"\"></p>\n<h6 id=\"example-2\">Example</h6>\n<ul>\n  <li>The example below demonstrates function calling, where the LLM interprets a natural language prompt to generate an API request. Given the user’s request for weather data at specific coordinates, the model formulates an API call with precise parameters, enabling automated data retrieval.</li>\n</ul>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code7\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code7\">\"User\": \"Can you fetch me the weather data for the coordinates \n37.8651 N, 119.5383 W, including the hourly forecast for temperature, \nwind speed, and precipitation for the next 10 days?\"\n\n\"Function\": \n{\n    ...\n    \"parameters\": \n    {\n        \"type\": \"object\", \n        \"properties\": \n        {\n            \"url\": \n            {\n                \"type\": \"string\", \n                \"description\": \"The API endpoint for fetching weather\n                data from the Open-Meteo API for the given latitude\n                and longitude, default \n                https://api.open-meteo.com/v1/forecast\"\n            }\n            ...\n        }\n    }\n}\n\n\"GPT-4 output\":\n{\n    \"name\": \"requests.get\",\n    \"parameters\": {\n        \"params\": \n        {\n            \"latitude\": \"37.8651\",\n            \"longitude\": \"-119.5383\",\n            \"forecast_days\": 10\n        },\n    }\n}\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code7\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code7\">\"User\": \"Can you fetch me the weather data for the coordinates \n37.8651 N, 119.5383 W, including the hourly forecast for temperature, \nwind speed, and precipitation for the next 10 days?\"\n\n\"Function\": \n{\n    ...\n    \"parameters\": \n    {\n        \"type\": \"object\", \n        \"properties\": \n        {\n            \"url\": \n            {\n                \"type\": \"string\", \n                \"description\": \"The API endpoint for fetching weather\n                data from the Open-Meteo API for the given latitude\n                and longitude, default \n                https://api.open-meteo.com/v1/forecast\"\n            }\n            ...\n        }\n    }\n}\n\n\"GPT-4 output\":\n{\n    \"name\": \"requests.get\",\n    \"parameters\": {\n        \"params\": \n        {\n            \"latitude\": \"37.8651\",\n            \"longitude\": \"-119.5383\",\n            \"forecast_days\": 10\n        },\n    }\n}\n</code></pre>\n<h4 id=\"best-practices-guidelines-and-limitations\">Best Practices, Guidelines, and Limitations</h4>\n<ul>\n  <li>Per <a href=\"https://x.com/swyx/status/1775993946935906645\">Anthropic</a>, when using tools with Claude, it’s important to follow best practices, understand limitations, and optimize tool design to ensure effective interactions.</li>\n</ul>\n<h5 id=\"choosing-the-right-model-for-tool-use\">Choosing the Right Model for Tool Use</h5>\n<ul>\n  <li><strong>Claude 3 Opus</strong> is best for complex tool use, as it can handle multiple tools simultaneously and detect missing arguments. It will ask for clarification when necessary.</li>\n  <li><strong>Claude 3 Haiku</strong> is better suited for simple tool use but defaults to using tools more frequently—even when unnecessary. It will also infer missing parameters rather than asking for clarification.</li>\n</ul>\n<h5 id=\"tool-usage-limits\">Tool Usage Limits</h5>\n<ul>\n  <li><strong>Handling large toolsets</strong>\n    <ul>\n      <li>Claude can accurately select from over 250+ tools, as long as the user query includes all required parameters.</li>\n      <li>This limit applies regardless of tool complexity. Complex tools typically have numerous parameters or deeply nested schemas.</li>\n    </ul>\n  </li>\n  <li><strong>Optimizing tool complexity</strong>\n    <ul>\n      <li>Claude performs better with simpler tools.</li>\n      <li>To improve accuracy, avoid deeply nested JSON objects and reduce the number of required inputs.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Claude can accurately select from over 250+ tools, as long as the user query includes all required parameters.</li>\n      <li>This limit applies regardless of tool complexity. Complex tools typically have numerous parameters or deeply nested schemas.</li>\n    </ul>\n<ul>\n      <li>Claude performs better with simpler tools.</li>\n      <li>To improve accuracy, avoid deeply nested JSON objects and reduce the number of required inputs.</li>\n    </ul>\n<h5 id=\"sequential-vs-parallel-tool-execution\">Sequential vs. Parallel Tool Execution</h5>\n<ul>\n  <li>Claude generally prefers sequential tool execution—using one tool at a time, analyzing the output, and then deciding on the next step.</li>\n  <li>While parallel tool use is possible, it may lead to:\n    <ul>\n      <li>Missing dependencies (e.g., filling in placeholder values for parameters that depend on previous outputs).</li>\n      <li>Unnecessary tool invocations.</li>\n    </ul>\n  </li>\n  <li>Best practice: Design workflows that encourage sequential tool execution to improve accuracy.</li>\n</ul>\n<ul>\n      <li>Missing dependencies (e.g., filling in placeholder values for parameters that depend on previous outputs).</li>\n      <li>Unnecessary tool invocations.</li>\n    </ul>\n<h5 id=\"error-handling-and-retries\">Error Handling and Retries</h5>\n<ul>\n  <li>If Claude’s tool request is invalid, returning an error response will often prompt it to retry with missing parameters filled in.</li>\n  <li>However, after 2-3 failed attempts, Claude may stop retrying and instead return an apology message.</li>\n</ul>\n<h4 id=\"designing-effective-tool-interfaces\">Designing Effective Tool Interfaces</h4>\n<ul>\n  <li>Tools are an essential part of agentic systems, enabling Claude to interact with external services and APIs. However, tools should be carefully designed, just like prompts, to ensure clarity and usability. By thoughtfully designing your tools, you can reduce errors, improve model accuracy, and create more efficient agent workflows, per the below guidelines per <a href=\"https://www.anthropic.com/engineering/building-effective-agents\">Anthropic’s blog on building effective agents</a>.</li>\n</ul>\n<h5 id=\"choosing-the-right-tool-format\">Choosing the Right Tool Format</h5>\n<ul>\n  <li>The way tools are structured can significantly impact Claude’s accuracy. Some formats are harder for an LLM to generate correctly than others:</li>\n  <li><strong>Diff vs. Full Rewrite</strong>:\n    <ul>\n      <li>Writing a diff requires pre-determining line changes, which is error-prone.</li>\n      <li>A full rewrite avoids this complexity.</li>\n    </ul>\n  </li>\n  <li><strong>Markdown vs. JSON for structured output</strong>:\n    <ul>\n      <li>JSON requires escaping newlines and quotes, increasing error risks.</li>\n      <li>Markdown is often easier for Claude to handle accurately.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Writing a diff requires pre-determining line changes, which is error-prone.</li>\n      <li>A full rewrite avoids this complexity.</li>\n    </ul>\n<ul>\n      <li>JSON requires escaping newlines and quotes, increasing error risks.</li>\n      <li>Markdown is often easier for Claude to handle accurately.</li>\n    </ul>\n<h5 id=\"best-practices-for-tool-design\">Best Practices for Tool Design</h5>\n<ul>\n  <li>Give the model enough tokens to “think” before committing to an output.</li>\n  <li>Stick to familiar formats that naturally occur in publicly available text (e.g., Markdown over escaped JSON).</li>\n  <li>Minimize formatting overhead—avoid requiring Claude to track line counts or escape large text blocks.</li>\n</ul>\n<h5 id=\"improving-tool-usability\">Improving Tool Usability</h5>\n<ul>\n  <li>Make tool descriptions intuitive—think like a human: Would a developer immediately understand how to use this tool?</li>\n  <li>Refine parameter names and descriptions—write them as if documenting an API for a junior developer.</li>\n  <li>Test extensively—run real-world inputs in a sandbox environment to uncover edge cases and refine accordingly.</li>\n  <li>Poka-yoke (mistake-proof) your tools.\n    <ul>\n      <li><strong>Example:</strong> Instead of using relative file paths, require absolute paths to avoid errors when switching directories.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Example:</strong> Instead of using relative file paths, require absolute paths to avoid errors when switching directories.</li>\n    </ul>",
    "contentMarkdown": "*   The advent of Tool Use in LLMs represents a pivotal design pattern in agentic AI workflows, enabling LLMs to perform a diverse range of tasks beyond text generation. Tool Use refers to the capability of an LLM to utilize specific functions—such as executing code, conducting web searches, or interacting with productivity tools—within its responses, effectively expanding its utility far beyond conventional, language-based outputs. This approach allows LLMs to tackle more complex queries and execute multifaceted tasks by selectively invoking various external tools. From answering specific questions to performing calculations, the use of function calls empowers LLMs to provide highly accurate and contextually informed responses.\n*   A foundational example of Tool Use is seen in scenarios where users request information not available in the model’s pre-existing training data. For instance, if a user asks, “What is the best coffee maker according to reviewers?”, a model equipped with Tool Use may initiate a web search, fetching up-to-date information by generating a command string such as `{tool: web-search, query: \"coffee maker reviews\"}`. Upon processing, the model retrieves relevant pages, synthesizes the data, and delivers an informed response. This dynamic response mechanism emerged from early realizations that traditional transformer-based language models, reliant solely on pre-trained knowledge, were inherently limited. By integrating a web search tool, developers enabled the model to access and incorporate fresh information into its output, a capability now widely adopted across various LLMs in consumer-facing applications.\n*   Moreover, Tool Use enables LLMs to handle calculations and other tasks requiring precision that text generation alone cannot achieve. For example, when a user asks, “If I invest $100 at compound 7% interest for 12 years, what do I have at the end?”, an LLM could respond by executing a Python command like `100 * (1+0.07)**12`. The LLM generates a string such as `{tool: python-interpreter, code: \"100 * (1+0.07)**12\"}`, and then the calculation tool processes this command to deliver an accurate answer. This illustrates how Tool Use facilitates complex mathematical reasoning within conversational AI systems.\n*   The scope of Tool Use, however, extends well beyond web searches or basic calculations. As the technology has evolved, developers have implemented a wide array of functions, enabling LLMs to interface with multiple external resources. These functions may include accessing specialized databases, interacting with productivity tools like email and calendar applications, generating or interpreting images, and engaging with multiple data sources such as Search (via Google/Bing Search APIs), Wikipedia, and academic repositories like arXiv.\n*   Systems now prompt LLMs with detailed descriptions of available functions, specifying their capabilities and parameters. With these cues, an LLM can autonomously select the appropriate function to fulfill the user’s request. In settings where hundreds of tools are accessible, developers often employ heuristics to streamline function selection, prioritizing the tools most relevant to the current context—a strategy analogous to the subset selection techniques used in retrieval-augmented generation (RAG) systems.\n*   The development of large multimodal models (LMMs) such as LLaVa, GPT-4V, and Gemini marked another milestone in Tool Use. Prior to these models, LLMs could not process or manipulate images directly, and any image-related tasks had to be offloaded to specific computer vision functions, such as object recognition or scene analysis. The introduction of GPT-4’s function-calling capabilities in 2023 further advanced Tool Use by establishing a more general-purpose function interface, laying the groundwork for a versatile, multimodal AI ecosystem where models seamlessly integrate text, image, and other data types. This new functionality has subsequently led to a proliferation of LLMs designed to exploit Tool Use, broadening the range of applications and enhancing overall adaptability.\n*   For instance, below is a prompt from [SmolLM2](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct) for function calling, where the model is prompted to choose relevant functions based on specific user inquiries is as follows. The prompt template guides SmolLM2 in structuring its function calls precisely and prompts it to assess the relevance and sufficiency of available parameters before executing a function.\n\n![](https://aman.ai/images/copy.png)\n\n`You are an expert in composing functions. You are given a question and a set of possible functions.  Based on the question, you will need to make one or more function/tool calls to achieve the purpose.  If none of the functions can be used, point it out and refuse to answer.  If the given question lacks the parameters required by the function, also point it out.  You have access to the following tools: <tools></tools>  The output MUST strictly adhere to the following format, and NO other text MUST be included. The example format is as follows. Please make sure the parameter type is correct. If no function call is needed, please make the tool calls an empty list '[]'. <tool_call>[ {\"name\": \"func_name1\", \"arguments\": {\"argument1\": \"value1\", \"argument2\": \"value2\"}}, (more tool calls as required) ]</tool_call>`\n\n![](https://aman.ai/images/copy.png)\n\n`You are an expert in composing functions. You are given a question and a set of possible functions.  Based on the question, you will need to make one or more function/tool calls to achieve the purpose.  If none of the functions can be used, point it out and refuse to answer.  If the given question lacks the parameters required by the function, also point it out.  You have access to the following tools: <tools></tools>  The output MUST strictly adhere to the following format, and NO other text MUST be included. The example format is as follows. Please make sure the parameter type is correct. If no function call is needed, please make the tool calls an empty list '[]'. <tool_call>[ {\"name\": \"func_name1\", \"arguments\": {\"argument1\": \"value1\", \"argument2\": \"value2\"}}, (more tool calls as required) ]</tool_call>`\n\n*   The evolution of Tool Use and function-calling capabilities in LLMs demonstrates the significant strides taken toward realizing general-purpose, agentic AI workflows. By enabling LLMs to autonomously utilize specialized tools across various contexts, developers have transformed these models from static text generators into dynamic, multifunctional systems capable of addressing a vast array of user needs. As the field advances, we can expect further innovations that expand the breadth and depth of Tool Use, pushing the boundaries of what LLMs can achieve in an integrated, agentic environment.\n\n#### Tool Calling Examples: Web Search and Code Execution\n\n*   LLMs can leverage tools such as web search to provide current product recommendations and code execution to handle complex calculations, showcasing their adeptness at choosing and using the right tools based on user input.\n*   The image below ([source](https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use)) showcases practical examples of tool calling in LLMs, highlighting two specific tools: a web search tool and a code execution tool. In the left panel, an example from Bing Copilot illustrates how an LLM can utilize a web search tool. When a user asks, “What is the best coffee maker according to reviewers?”, the model initiates a web search to gather relevant information from current online reviews. This allows the LLM to provide an informed answer based on up-to-date data.\n*   The right panel demonstrates an example from ChatGPT using a code execution tool. When a user asks, “If I invest $100 at compound 7% interest for 12 years, what do I have at the end?”, the LLM responds by generating a Python command to calculate the compounded interest. The code snippet, `principal = 100; interest_rate = 0.07; years = 12; value = principal * (1 + interest_rate) ** years`, is executed, providing an accurate financial calculation rather than relying solely on text-based reasoning.\n*   These examples illustrate the model’s ability to identify and select the appropriate tool based on the user’s query, further demonstrating the flexibility and enhanced capabilities of Tool Use in agentic LLM workflows.\n\n![](/primers/ai/assets/agents/ToolCalling.jpg)\n\n#### Function Calling Datasets\n\n##### [Hermes Function-Calling V1](https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1)\n\n*   The [Hermes Function-Calling V1](https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1) dataset is designed for training language models to perform structured function calls and return structured outputs based on natural language instructions.\n*   It includes function-calling conversations, json-mode samples, agentic json-mode, and structured extraction examples, showcasing various scenarios where AI agents interpret queries and execute relevant function calls.\n*   The Hermes Function-Calling Standard enables language models to execute API calls based on user requests, improving AI’s practical utility by allowing direct API interactions.\n\n##### Glaive Function Calling V2\n\n*   The [Glaive Function Calling (52K)](https://huggingface.co/datasets/glaiveai/glaive-function-calling) and [Glaive Function Calling v2 (113K)](https://huggingface.co/datasets/vietgpt/glaive-function-calling-v2?row=0) are datasets generated through [Glaive](https://glaive.ai/) for the task of function calling, in the following format:\n\n![](https://aman.ai/images/copy.png)\n\n`SYSTEM: You are an helpful assistant who has access to the following functions to help the user, you can use the functions if needed- {   JSON function definiton } USER: user message ASSISTANT: assistant message Function call invocations are formatted as- ASSISTANT: <functioncall> {json function call} Response to the function call is formatted as- FUNCTION RESPONSE: {json function response}`\n\n![](https://aman.ai/images/copy.png)\n\n`SYSTEM: You are an helpful assistant who has access to the following functions to help the user, you can use the functions if needed- {   JSON function definiton } USER: user message ASSISTANT: assistant message Function call invocations are formatted as- ASSISTANT: <functioncall> {json function call} Response to the function call is formatted as- FUNCTION RESPONSE: {json function response}`\n\n*   There are also samples which do not have any function invocations, multiple invocations and samples with no functions presented and invoked to keep the data balanced.\n\n##### [Salesforce’s Xlam-function-calling-60k](https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k?row=0)\n\n*   Salesforce’s [xlam-function-calling-60k](https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k?row=0) contains 60,000 data samples collected by APIGen, an automated data generation pipeline designed to produce verifiable high-quality datasets for function-calling applications. Each data in our dataset is verified through three hierarchical stages: format checking, actual function executions, and semantic verification, ensuring its reliability and correctness.\n*   Also, [xLAM-1b-fc-r](https://huggingface.co/Salesforce/xLAM-1b-fc-r) and [xLAM-7b-fc-r](https://huggingface.co/Salesforce/xLAM-7b-fc-r).\n\n###### JSON Data Format for Query and Answers\n\n*   This JSON data format is used to represent a query along with the available tools and the corresponding answers. Here’s a description of the JSON format which consists of the following key-value pairs: \\`\\`\\`\n*   **query** (string): The query or problem statement.\n    \n*   **tools** (array): An array of available tools that can be used to solve the query.\n    \n    *   Each tool is represented as an object with the following properties:\n        \n        *   **name** (string): The name of the tool.\n            \n        *   **description** (string): A brief description of what the tool does.\n            \n        *   **parameters** (object): An object representing the parameters required by the tool.\n            \n            *   Each parameter is represented as a key-value pair, where the key is the parameter name and the value is an object with the following properties:\n                \n                *   **type** (string): The data type of the parameter (e.g., “int”, “float”, “list”).\n                    \n                *   **description** (string): A brief description of the parameter.\n                    \n                *   **required** (boolean): Indicates whether the parameter is required or optional.\n                    \n*   **answers** (array): An array of answers corresponding to the query.\n    \n    *   Each answer is represented as an object with the following properties:\n        \n        *   **name** (string): The name of the tool used to generate the answer.\n            \n        *   **arguments** (object): An object representing the arguments passed to the tool to generate the answer.\n            \n            *   Each argument is represented as a key-value pair, where the key is the parameter name and the value is the corresponding value. \\`\\`\\`\n*   Note that they format the **query**, **tools**, and **answers** as a string, but you can easily recover each entry to the JSON object via `json.loads(...)`.\n\n**query** (string): The query or problem statement.\n\n**tools** (array): An array of available tools that can be used to solve the query.\n\n*   Each tool is represented as an object with the following properties:\n    \n    *   **name** (string): The name of the tool.\n        \n    *   **description** (string): A brief description of what the tool does.\n        \n    *   **parameters** (object): An object representing the parameters required by the tool.\n        \n        *   Each parameter is represented as a key-value pair, where the key is the parameter name and the value is an object with the following properties:\n            \n            *   **type** (string): The data type of the parameter (e.g., “int”, “float”, “list”).\n                \n            *   **description** (string): A brief description of the parameter.\n                \n            *   **required** (boolean): Indicates whether the parameter is required or optional.\n                \n\nEach tool is represented as an object with the following properties:\n\n*   **name** (string): The name of the tool.\n    \n*   **description** (string): A brief description of what the tool does.\n    \n*   **parameters** (object): An object representing the parameters required by the tool.\n    \n    *   Each parameter is represented as a key-value pair, where the key is the parameter name and the value is an object with the following properties:\n        \n        *   **type** (string): The data type of the parameter (e.g., “int”, “float”, “list”).\n            \n        *   **description** (string): A brief description of the parameter.\n            \n        *   **required** (boolean): Indicates whether the parameter is required or optional.\n            \n\n**name** (string): The name of the tool.\n\n**description** (string): A brief description of what the tool does.\n\n**parameters** (object): An object representing the parameters required by the tool.\n\n*   Each parameter is represented as a key-value pair, where the key is the parameter name and the value is an object with the following properties:\n    \n    *   **type** (string): The data type of the parameter (e.g., “int”, “float”, “list”).\n        \n    *   **description** (string): A brief description of the parameter.\n        \n    *   **required** (boolean): Indicates whether the parameter is required or optional.\n        \n\nEach parameter is represented as a key-value pair, where the key is the parameter name and the value is an object with the following properties:\n\n*   **type** (string): The data type of the parameter (e.g., “int”, “float”, “list”).\n    \n*   **description** (string): A brief description of the parameter.\n    \n*   **required** (boolean): Indicates whether the parameter is required or optional.\n    \n\n**type** (string): The data type of the parameter (e.g., “int”, “float”, “list”).\n\n**description** (string): A brief description of the parameter.\n\n**required** (boolean): Indicates whether the parameter is required or optional.\n\n**answers** (array): An array of answers corresponding to the query.\n\n*   Each answer is represented as an object with the following properties:\n    \n    *   **name** (string): The name of the tool used to generate the answer.\n        \n    *   **arguments** (object): An object representing the arguments passed to the tool to generate the answer.\n        \n        *   Each argument is represented as a key-value pair, where the key is the parameter name and the value is the corresponding value. \\`\\`\\`\n\nEach answer is represented as an object with the following properties:\n\n*   **name** (string): The name of the tool used to generate the answer.\n    \n*   **arguments** (object): An object representing the arguments passed to the tool to generate the answer.\n    \n    *   Each argument is represented as a key-value pair, where the key is the parameter name and the value is the corresponding value. \\`\\`\\`\n\n**name** (string): The name of the tool used to generate the answer.\n\n**arguments** (object): An object representing the arguments passed to the tool to generate the answer.\n\n*   Each argument is represented as a key-value pair, where the key is the parameter name and the value is the corresponding value. \\`\\`\\`\n\n###### Example\n\n*   Here’s an example JSON data:\n\n![](https://aman.ai/images/copy.png)\n\n`{   \"query\": \"Find the sum of all the multiples of 3 and 5 between 1 and 1000. Also find the product of the first five prime numbers.\",   \"tools\": [     {       \"name\": \"math_toolkit.sum_of_multiples\",       \"description\": \"Find the sum of all multiples of specified numbers within a specified range.\",       \"parameters\": {         \"lower_limit\": {           \"type\": \"int\",           \"description\": \"The start of the range (inclusive).\",           \"required\": true         },         \"upper_limit\": {           \"type\": \"int\",           \"description\": \"The end of the range (inclusive).\",           \"required\": true         },         \"multiples\": {           \"type\": \"list\",           \"description\": \"The numbers to find multiples of.\",           \"required\": true         }       }     },     {       \"name\": \"math_toolkit.product_of_primes\",       \"description\": \"Find the product of the first n prime numbers.\",       \"parameters\": {         \"count\": {           \"type\": \"int\",           \"description\": \"The number of prime numbers to multiply together.\",           \"required\": true         }       }     }   ],   \"answers\": [     {       \"name\": \"math_toolkit.sum_of_multiples\",       \"arguments\": {         \"lower_limit\": 1,         \"upper_limit\": 1000,         \"multiples\": [3, 5]       }     },     {       \"name\": \"math_toolkit.product_of_primes\",       \"arguments\": {         \"count\": 5       }     }   ] }`\n\n![](https://aman.ai/images/copy.png)\n\n`{   \"query\": \"Find the sum of all the multiples of 3 and 5 between 1 and 1000. Also find the product of the first five prime numbers.\",   \"tools\": [     {       \"name\": \"math_toolkit.sum_of_multiples\",       \"description\": \"Find the sum of all multiples of specified numbers within a specified range.\",       \"parameters\": {         \"lower_limit\": {           \"type\": \"int\",           \"description\": \"The start of the range (inclusive).\",           \"required\": true         },         \"upper_limit\": {           \"type\": \"int\",           \"description\": \"The end of the range (inclusive).\",           \"required\": true         },         \"multiples\": {           \"type\": \"list\",           \"description\": \"The numbers to find multiples of.\",           \"required\": true         }       }     },     {       \"name\": \"math_toolkit.product_of_primes\",       \"description\": \"Find the product of the first n prime numbers.\",       \"parameters\": {         \"count\": {           \"type\": \"int\",           \"description\": \"The number of prime numbers to multiply together.\",           \"required\": true         }       }     }   ],   \"answers\": [     {       \"name\": \"math_toolkit.sum_of_multiples\",       \"arguments\": {         \"lower_limit\": 1,         \"upper_limit\": 1000,         \"multiples\": [3, 5]       }     },     {       \"name\": \"math_toolkit.product_of_primes\",       \"arguments\": {         \"count\": 5       }     }   ] }`\n\n*   In this example, the query asks to find the sum of multiples of 3 and 5 between 1 and 1000, and also find the product of the first five prime numbers. The available tools are `math_toolkit.sum_of_multiples` and `math_toolkit.product_of_primes`, along with their parameter descriptions. The answers array provides the specific tool and arguments used to generate each answer.\n\n##### [Synth-APIGen-v0.1](https://huggingface.co/datasets/argilla/Synth-APIGen-v0.1)\n\n*   A dataset of 50k samples by [Argilla](https://huggingface.co/argilla).\n\n###### Example\n\n*   This example demonstrates the use of the `complex_to_polar` function, which is designed to convert complex numbers into their polar coordinate representations. The input query requests conversions for two specific complex numbers, `3 + 4j` and `1 - 2j`, showcasing how the function can be called with different arguments to obtain their polar forms.\n\n![](https://aman.ai/images/copy.png)\n\n``{   \"func_name\": \"complex_to_polar\",   \"func_desc\": \"Converts a complex number to its polar coordinate representation.\",   \"tools\": \"[{\\\"type\\\":\\\"function\\\",\\\"function\\\":{\\\"name\\\":\\\"complex_to_polar\\\",\\\"description\\\":\\\"Converts a complex number to its polar coordinate representation.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"complex_number\\\":{\\\"type\\\":\\\"object\\\",\\\"description\\\":\\\"A complex number in the form of `real + imaginary * 1j`.\\\"}},\\\"required\\\":[\\\"complex_number\\\"]}}}]\",   \"query\": \"I'd like to convert the complex number 3 + 4j and 1 - 2j to polar coordinates.\",   \"answers\": \"[{\\\"name\\\": \\\"complex_to_polar\\\", \\\"arguments\\\": {\\\"complex_number\\\": \\\"3 + 4j\\\"}}, {\\\"name\\\": \\\"complex_to_polar\\\", \\\"arguments\\\": {\\\"complex_number\\\": \\\"1 - 2j\\\"}}]\",   \"model_name\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",   \"hash_id\": \"f873783c04bbddd9d79f47287fa3b6705b3eaea0e5bc126fba91366f7b8b07e9\",     }   ],   \"category\": \"E-commerce Platforms\",   \"subcategory\": \"Kayak\",   \"task\": \"Flight Search\" }``\n\n![](https://aman.ai/images/copy.png)\n\n``{   \"func_name\": \"complex_to_polar\",   \"func_desc\": \"Converts a complex number to its polar coordinate representation.\",   \"tools\": \"[{\\\"type\\\":\\\"function\\\",\\\"function\\\":{\\\"name\\\":\\\"complex_to_polar\\\",\\\"description\\\":\\\"Converts a complex number to its polar coordinate representation.\\\",\\\"parameters\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"complex_number\\\":{\\\"type\\\":\\\"object\\\",\\\"description\\\":\\\"A complex number in the form of `real + imaginary * 1j`.\\\"}},\\\"required\\\":[\\\"complex_number\\\"]}}}]\",   \"query\": \"I'd like to convert the complex number 3 + 4j and 1 - 2j to polar coordinates.\",   \"answers\": \"[{\\\"name\\\": \\\"complex_to_polar\\\", \\\"arguments\\\": {\\\"complex_number\\\": \\\"3 + 4j\\\"}}, {\\\"name\\\": \\\"complex_to_polar\\\", \\\"arguments\\\": {\\\"complex_number\\\": \\\"1 - 2j\\\"}}]\",   \"model_name\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",   \"hash_id\": \"f873783c04bbddd9d79f47287fa3b6705b3eaea0e5bc126fba91366f7b8b07e9\",     }   ],   \"category\": \"E-commerce Platforms\",   \"subcategory\": \"Kayak\",   \"task\": \"Flight Search\" }``\n\n#### Evaluation\n\n*   To ensure that Tool Use capabilities meet the demands of diverse real-world scenarios, it is crucial to evaluate the function-calling performance of LLMs rigorously. This evaluation encompasses assessing model performance across both Python and non-Python programming environments, with a focus on how effectively the model can execute functions, select the appropriate tools, and discern when a function is necessary within a conversational context. An essential aspect of this evaluation is testing the model’s ability to invoke functions accurately based on user prompts and determine whether certain functions are applicable or needed.\n*   This structured evaluation methodology enables a holistic understanding of the model’s function-calling performance, combining both syntactic accuracy and real-world execution fidelity. By examining the model’s ability to navigate various programming contexts and detect relevance in function invocation, this approach underscores the practical reliability of LLMs in diverse applications.\n\n##### [Berkeley Function-Calling Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html)\n\n*   Berkeley Function-Calling Leaderboard (BFCL) assesses the function-calling capabilities of various LLMs. It consists of 2,000 question-function-answer pairs across multiple programming languages (Python, Java, JavaScript, REST API, SQL).\n*   The evaluation covers complex use cases, including simple, multiple, and parallel function calls, requiring the selection and simultaneous execution of functions.\n*   BFCL tests function relevance detection to see how models handle irrelevant functions, expecting them to return an error message.\n*   Both proprietary and open-source models perform similarly in simple scenarios, but GPT-series models excel in more complex function-calling tasks.\n*   The Gorilla OpenFunctions dataset has expanded from 100 to 2,000 data points, increasing diversity and complexity in evaluations. The dataset includes functions from varied fields such as Mathematics, Sports, Finance, and more, covering 40 sub-domains.\n*   Evaluations are divided into Python (simple, multiple, parallel, parallel multiple functions) and Non-Python (chatting capability, function relevance, REST API, SQL, Java, JavaScript) categories.\n*   Python evaluations cover scenarios from single function calls to complex parallel multiple function calls.\n*   Non-Python evaluations test models on general-purpose chat, relevance detection, and specific API and language scenarios.\n*   Function relevance detection is a key focus, evaluating whether models avoid using irrelevant functions and highlighting their potential for hallucination.\n*   REST API testing involves real-world GET requests with parameters in URLs and headers, assessing models’ ability to generate executable API calls.\n*   SQL evaluation includes basic SQL queries, while Java and JavaScript testing focus on language-specific function-calling abilities.\n*   BFCL uses AST evaluation to check syntax and structural accuracy, and executable evaluation to verify real-world function execution.\n*   AST evaluation ensures function matching, parameter consistency, and type/value accuracy.\n*   Executable function evaluation runs generated functions to verify response accuracy and consistency, particularly for REST APIs.\n*   The evaluation approach requires complete matching of model outputs to expected results; partial matches are considered failures.\n*   Ongoing development includes continuous updates and community feedback to refine evaluation methods, especially for SQL and chat capabilities.\n\n[![](/primers/ai/assets/LLM/BFCL.png)](https://gorilla.cs.berkeley.edu/leaderboard.html)\n\n##### Python Evaluation\n\n*   Inspired by the [Berkeley Function-Calling Leaderboard](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html) and [APIGen](https://arxiv.org/abs/2406.18518), the evaluation framework can be organized by function type (simple, multiple, parallel, parallel multiple) or evaluation method (AST or execution of APIs). This categorization helps in comparing model performances on standard function-calling scenarios and assessing their accuracy and efficiency. By structuring the evaluation in this way, it provides a comprehensive view of how well the model performs across different types of function calls and under varying conditions. More on the section on [Evaluation Methods](#evaluation-methods).\n*   The Python evaluation categories, listed below, assess the model’s ability to handle single and multiple function calls, both sequentially and in parallel. These tests simulate realistic scenarios where the model must interpret user queries, select appropriate functions, and execute them accurately, mimicking real-world applications. By testing these different scenarios, the evaluation can highlight the model’s proficiency in using Python-based function calls under varying degrees of complexity and concurrency.\n\n1.  **Simple Function:** In this category, the evaluation involves a single, straightforward function call. The user provides a JSON function document, and the model is expected to invoke only one function call. This test examines the model’s ability to handle the most common and basic type of function call correctly.\n    \n2.  **Parallel Function:** This evaluation scenario requires the model to make multiple function calls in parallel in response to a single user query. The model must identify how many function calls are necessary and initiate them simultaneously, regardless of the complexity or length of the user query.\n    \n3.  **Multiple Function:** This category involves scenarios where the user input can be matched to one function call out of two to four available JSON function documentations. The model must accurately select the most appropriate function to call based on the given context.\n    \n4.  **Parallel Multiple Function:** This is a complex evaluation combining both parallel and multiple function categories. The model is presented with multiple function documentations, and each relevant function may need to be invoked zero or more times in parallel.\n    \n\n**Simple Function:** In this category, the evaluation involves a single, straightforward function call. The user provides a JSON function document, and the model is expected to invoke only one function call. This test examines the model’s ability to handle the most common and basic type of function call correctly.\n\n**Parallel Function:** This evaluation scenario requires the model to make multiple function calls in parallel in response to a single user query. The model must identify how many function calls are necessary and initiate them simultaneously, regardless of the complexity or length of the user query.\n\n**Multiple Function:** This category involves scenarios where the user input can be matched to one function call out of two to four available JSON function documentations. The model must accurately select the most appropriate function to call based on the given context.\n\n**Parallel Multiple Function:** This is a complex evaluation combining both parallel and multiple function categories. The model is presented with multiple function documentations, and each relevant function may need to be invoked zero or more times in parallel.\n\n*   As mentioned earlier, each Python evaluation category includes both Abstract Syntax Tree (AST) and executable evaluations. A significant limitation of AST evaluation is the variety of methods available to construct function calls that achieve the same result, leading to challenges in consistency and accuracy. In these cases, executable evaluations provide a more reliable alternative by directly running the code to verify outcomes, allowing for precise and practical validation of functionality across different coding approaches.\n\n##### Non-Python Evaluation\n\n*   The non-Python evaluation categories, listed below, test the model’s ability to handle diverse scenarios involving conversation, relevance detection, and the use of different programming languages and technologies. These evaluations provide insights into the model’s adaptability to various contexts beyond Python. By including these diverse categories, the evaluation aims to ensure that the model is versatile and capable of handling various use cases, making it applicable in a broad range of applications.\n\n1.  **Chatting Capability:** This category evaluates the model’s general conversational abilities without invoking functions. The goal is to see if the model can maintain coherent dialogue and recognize when function calls are unnecessary. This is distinct from function relevance detection, which involves determining the suitability of invoking any provided functions.\n    \n2.  **Function Relevance Detection:** This tests whether the model can discern when none of the provided functions are relevant. The ideal outcome is that the model refrains from making any function calls, demonstrating an understanding of when it lacks the required function information or user instruction.\n    \n3.  **REST API:** This evaluation focuses on the model’s ability to generate and execute realistic REST API calls using Python’s requests library. It tests the model’s understanding of GET requests, including path and query parameters, and its ability to generate calls that match real-world API documentation.\n    \n4.  **SQL:** This category assesses the model’s ability to construct simple SQL queries using custom `sql.execute` functions. The evaluation is limited to basic SQL operations like SELECT, INSERT, UPDATE, DELETE, and CREATE, testing whether the model can generalize function-calling capabilities beyond Python.\n    \n5.  **Java + JavaScript:** Despite the uniformity in function-calling formats across languages, this evaluation examines how well the model adapts to language-specific types and syntax, such as Java’s HashMap. It includes examples that test the model’s handling of Java and JavaScript, emphasizing the need for language-specific adaptations.\n    \n\n**Chatting Capability:** This category evaluates the model’s general conversational abilities without invoking functions. The goal is to see if the model can maintain coherent dialogue and recognize when function calls are unnecessary. This is distinct from function relevance detection, which involves determining the suitability of invoking any provided functions.\n\n**Function Relevance Detection:** This tests whether the model can discern when none of the provided functions are relevant. The ideal outcome is that the model refrains from making any function calls, demonstrating an understanding of when it lacks the required function information or user instruction.\n\n**REST API:** This evaluation focuses on the model’s ability to generate and execute realistic REST API calls using Python’s requests library. It tests the model’s understanding of GET requests, including path and query parameters, and its ability to generate calls that match real-world API documentation.\n\n**SQL:** This category assesses the model’s ability to construct simple SQL queries using custom `sql.execute` functions. The evaluation is limited to basic SQL operations like SELECT, INSERT, UPDATE, DELETE, and CREATE, testing whether the model can generalize function-calling capabilities beyond Python.\n\n**Java + JavaScript:** Despite the uniformity in function-calling formats across languages, this evaluation examines how well the model adapts to language-specific types and syntax, such as Java’s HashMap. It includes examples that test the model’s handling of Java and JavaScript, emphasizing the need for language-specific adaptations.\n\n##### Evaluation Methods\n\n*   Two primary methods are used to evaluate model performance:\n    \n    1.  **Abstract Syntax Tree (AST) Evaluation:** AST evaluation involves parsing the model-generated function calls to check their structure against expected outputs. It verifies the function name, parameter presence, and type correctness. AST evaluation is ideal for cases where execution isn’t feasible due to language constraints or when the result cannot be easily executed.\n        \n        *   **Simple Function AST Evaluation**\n            *   The AST evaluation process focuses on comparing a single model output function against its function doc and possible answers. Here is a flow chart ([source](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html)) that shows the step-by-step evaluation process. ![](/primers/ai/assets/agents/AST.jpg)\n        *   **Multiple/Parallel/Parallel-Multiple Functions AST Evaluation**\n            *   The multiple, parallel, or parallel-multiple function AST evaluation process extends the idea in the simple function evaluation to support multiple model outputs and possible answers.\n                *   The evaluation process first associates each possible answer with its function doc. Then it iterates over the model outputs and calls the simple function evaluation on each function (which takes in one model output, one possible answer, and one function doc).\n                    *   The order of model outputs relative to possible answers is not required. A model output can match with any possible answer.\n            *   The evaluation employs an all-or-nothing approach to evaluation. Failure to find a match across all model outputs for any given possible answer results in a failed evaluation.\n    2.  **Executable Function Evaluation:** This metric assesses the model by executing the function calls it generates and comparing the outputs against expected results. This evaluation is crucial for testing real-world applicability, focusing on whether the function calls run successfully, produce the correct types of responses, and maintain structural consistency in their outputs.\n        \n*   The combination of AST and executable evaluations ensures a comprehensive assessment, providing insights into both the syntactic and functional correctness of the model’s output.\n    \n\nTwo primary methods are used to evaluate model performance:\n\n1.  **Abstract Syntax Tree (AST) Evaluation:** AST evaluation involves parsing the model-generated function calls to check their structure against expected outputs. It verifies the function name, parameter presence, and type correctness. AST evaluation is ideal for cases where execution isn’t feasible due to language constraints or when the result cannot be easily executed.\n    \n    *   **Simple Function AST Evaluation**\n        *   The AST evaluation process focuses on comparing a single model output function against its function doc and possible answers. Here is a flow chart ([source](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html)) that shows the step-by-step evaluation process. ![](/primers/ai/assets/agents/AST.jpg)\n    *   **Multiple/Parallel/Parallel-Multiple Functions AST Evaluation**\n        *   The multiple, parallel, or parallel-multiple function AST evaluation process extends the idea in the simple function evaluation to support multiple model outputs and possible answers.\n            *   The evaluation process first associates each possible answer with its function doc. Then it iterates over the model outputs and calls the simple function evaluation on each function (which takes in one model output, one possible answer, and one function doc).\n                *   The order of model outputs relative to possible answers is not required. A model output can match with any possible answer.\n        *   The evaluation employs an all-or-nothing approach to evaluation. Failure to find a match across all model outputs for any given possible answer results in a failed evaluation.\n2.  **Executable Function Evaluation:** This metric assesses the model by executing the function calls it generates and comparing the outputs against expected results. This evaluation is crucial for testing real-world applicability, focusing on whether the function calls run successfully, produce the correct types of responses, and maintain structural consistency in their outputs.\n    \n\n**Abstract Syntax Tree (AST) Evaluation:** AST evaluation involves parsing the model-generated function calls to check their structure against expected outputs. It verifies the function name, parameter presence, and type correctness. AST evaluation is ideal for cases where execution isn’t feasible due to language constraints or when the result cannot be easily executed.\n\n*   **Simple Function AST Evaluation**\n    *   The AST evaluation process focuses on comparing a single model output function against its function doc and possible answers. Here is a flow chart ([source](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html)) that shows the step-by-step evaluation process. ![](/primers/ai/assets/agents/AST.jpg)\n*   **Multiple/Parallel/Parallel-Multiple Functions AST Evaluation**\n    *   The multiple, parallel, or parallel-multiple function AST evaluation process extends the idea in the simple function evaluation to support multiple model outputs and possible answers.\n        *   The evaluation process first associates each possible answer with its function doc. Then it iterates over the model outputs and calls the simple function evaluation on each function (which takes in one model output, one possible answer, and one function doc).\n            *   The order of model outputs relative to possible answers is not required. A model output can match with any possible answer.\n    *   The evaluation employs an all-or-nothing approach to evaluation. Failure to find a match across all model outputs for any given possible answer results in a failed evaluation.\n\n*   The AST evaluation process focuses on comparing a single model output function against its function doc and possible answers. Here is a flow chart ([source](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html)) that shows the step-by-step evaluation process. ![](/primers/ai/assets/agents/AST.jpg)\n\n*   The multiple, parallel, or parallel-multiple function AST evaluation process extends the idea in the simple function evaluation to support multiple model outputs and possible answers.\n    *   The evaluation process first associates each possible answer with its function doc. Then it iterates over the model outputs and calls the simple function evaluation on each function (which takes in one model output, one possible answer, and one function doc).\n        *   The order of model outputs relative to possible answers is not required. A model output can match with any possible answer.\n*   The evaluation employs an all-or-nothing approach to evaluation. Failure to find a match across all model outputs for any given possible answer results in a failed evaluation.\n\n*   The evaluation process first associates each possible answer with its function doc. Then it iterates over the model outputs and calls the simple function evaluation on each function (which takes in one model output, one possible answer, and one function doc).\n    *   The order of model outputs relative to possible answers is not required. A model output can match with any possible answer.\n\n*   The order of model outputs relative to possible answers is not required. A model output can match with any possible answer.\n\n**Executable Function Evaluation:** This metric assesses the model by executing the function calls it generates and comparing the outputs against expected results. This evaluation is crucial for testing real-world applicability, focusing on whether the function calls run successfully, produce the correct types of responses, and maintain structural consistency in their outputs.\n\nThe combination of AST and executable evaluations ensures a comprehensive assessment, providing insights into both the syntactic and functional correctness of the model’s output.\n\n##### Inference Example Output with XML and JSON\n\n*   Typical function calling datasets uses a combination of both XML and JSON elements (cf. inference output sample below), as detailed below.\n    \n    1.  **XML Structure**: Elements like `<|im_start|>`, `<tool_call>`, and `<tool_response>` resemble XML-like tags, which help demarcate different parts of the communication.\n        \n    2.  **Dictionary/JSON Structure**: Within the `<tool_call>` and `<tool_response>` tags, the data for the function arguments and the stock fundamentals is formatted as Python-style dictionaries (or JSON-like key-value pairs), such as `{'symbol': 'TSLA'}` and `{\"name\": \"get_stock_fundamentals\", \"content\": {'symbol': 'TSLA', 'company_name': 'Tesla, Inc.' ...}}`.\n        \n*   This combination provides an XML-like structure for message flow and JSON for data representation, allowing for structured, nested data representation and demarcation of sections.\n    \n*   Here’s an example of the inference output from [Hermes Function-Calling V1](https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1):\n    \n\nTypical function calling datasets uses a combination of both XML and JSON elements (cf. inference output sample below), as detailed below.\n\n1.  **XML Structure**: Elements like `<|im_start|>`, `<tool_call>`, and `<tool_response>` resemble XML-like tags, which help demarcate different parts of the communication.\n    \n2.  **Dictionary/JSON Structure**: Within the `<tool_call>` and `<tool_response>` tags, the data for the function arguments and the stock fundamentals is formatted as Python-style dictionaries (or JSON-like key-value pairs), such as `{'symbol': 'TSLA'}` and `{\"name\": \"get_stock_fundamentals\", \"content\": {'symbol': 'TSLA', 'company_name': 'Tesla, Inc.' ...}}`.\n    \n\n**XML Structure**: Elements like `<|im_start|>`, `<tool_call>`, and `<tool_response>` resemble XML-like tags, which help demarcate different parts of the communication.\n\n**Dictionary/JSON Structure**: Within the `<tool_call>` and `<tool_response>` tags, the data for the function arguments and the stock fundamentals is formatted as Python-style dictionaries (or JSON-like key-value pairs), such as `{'symbol': 'TSLA'}` and `{\"name\": \"get_stock_fundamentals\", \"content\": {'symbol': 'TSLA', 'company_name': 'Tesla, Inc.' ...}}`.\n\nThis combination provides an XML-like structure for message flow and JSON for data representation, allowing for structured, nested data representation and demarcation of sections.\n\nHere’s an example of the inference output from [Hermes Function-Calling V1](https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1):\n\n![](https://aman.ai/images/copy.png)\n\n`<|im_start|>user Fetch the stock fundamentals data for Tesla (TSLA)<|im_end|> <|im_start|>assistant <tool_call> {'arguments': {'symbol': 'TSLA'}, 'name': 'get_stock_fundamentals'} </tool_call><|im_end|> <|im_start|>tool <tool_response> {\"name\": \"get_stock_fundamentals\", \"content\": {'symbol': 'TSLA', 'company_name': 'Tesla, Inc.', 'sector': 'Consumer Cyclical', 'industry': 'Auto Manufacturers', 'market_cap': 611384164352, 'pe_ratio': 49.604652, 'pb_ratio': 9.762013, 'dividend_yield': None, 'eps': 4.3, 'beta': 2.427, '52_week_high': 299.29, '52_week_low': 152.37}} </tool_response> <|im_end|>`\n\n![](https://aman.ai/images/copy.png)\n\n`<|im_start|>user Fetch the stock fundamentals data for Tesla (TSLA)<|im_end|> <|im_start|>assistant <tool_call> {'arguments': {'symbol': 'TSLA'}, 'name': 'get_stock_fundamentals'} </tool_call><|im_end|> <|im_start|>tool <tool_response> {\"name\": \"get_stock_fundamentals\", \"content\": {'symbol': 'TSLA', 'company_name': 'Tesla, Inc.', 'sector': 'Consumer Cyclical', 'industry': 'Auto Manufacturers', 'market_cap': 611384164352, 'pe_ratio': 49.604652, 'pb_ratio': 9.762013, 'dividend_yield': None, 'eps': 4.3, 'beta': 2.427, '52_week_high': 299.29, '52_week_low': 152.37}} </tool_response> <|im_end|>`\n\n##### JSON/Structured Outputs\n\n*   Once a model is trained on a system prompt that asks for JSON-based structured outputs (below), the model should respond with only a JSON object response, based on the specific JSON schema provided.\n\n![](https://aman.ai/images/copy.png)\n\n`<|im_start|>system You are a helpful assistant that answers in JSON. Here's the JSON schema you must adhere to:\\n<schema>\\n{schema}\\n</schema><|im_end|>`\n\n![](https://aman.ai/images/copy.png)\n\n`<|im_start|>system You are a helpful assistant that answers in JSON. Here's the JSON schema you must adhere to:\\n<schema>\\n{schema}\\n</schema><|im_end|>`\n\n*   The schema can be made from a pydantic object using (e.g., a standalone script available is here from [Hermes Function-Calling V1](https://github.com/NousResearch/Hermes-Function-Calling/blob/main/jsonmode.py)).\n    \n*   As an example from [Hermes Function-Calling V1](https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1):\n    \n\nThe schema can be made from a pydantic object using (e.g., a standalone script available is here from [Hermes Function-Calling V1](https://github.com/NousResearch/Hermes-Function-Calling/blob/main/jsonmode.py)).\n\nAs an example from [Hermes Function-Calling V1](https://huggingface.co/datasets/NousResearch/hermes-function-calling-v1):\n\n![](https://aman.ai/images/copy.png)\n\n`{   \"id\": \"753d8365-0e54-43b1-9514-3f9b819fd31c\",   \"conversations\": [     {       \"from\": \"system\",       \"value\": \"You are a function calling AI model. You are provided with function signatures within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.\\n<tools>\\n[{'type': 'function', 'function': {'name': 'search_flights', 'description': 'Searches for flights based on departure and destination cities, dates, class, and other preferences.', 'parameters': {'type': 'object', 'properties': {'departure_city': {'type': 'string', 'description': 'The city from which the flight will depart.'}, 'destination_city': {'type': 'string', 'description': 'The destination city for the flight.'}, 'departure_date': {'type': 'string', 'description': 'The departure date for the flight.', 'format': 'date'}, 'return_date': {'type': 'string', 'description': 'The return date for the flight.', 'format': 'date'}, 'class': {'type': 'string', 'description': 'The class of the flight ticket.', 'enum': ['economy', 'business', 'first']}, 'flexible_cancellation': {'type': 'boolean', 'description': 'Indicates if the search should filter for flights with flexible cancellation policies.'}}, 'required': ['departure_city', 'destination_city', 'departure_date', 'return_date', 'class']}}}]\\n</tools>\\nFor each function call return a json object with function name and arguments within <tool_call> </tool_call> tags with the following schema:\\n<tool_call>\\n{'arguments': <args-dict>, 'name': <function-name>}\\n</tool_call>\\n\"     },     {       \"from\": \"human\",       \"value\": \"I'm planning a kayaking trip and looking to book flights from Los Angeles to Auckland. My departure is scheduled for July 10th, 2023, and I intend to return on July 24th, 2023. I would prefer to travel in economy class and would also like the option to have flexible cancellation policies for the tickets due to the uncertain nature of outdoor activities. Could you please search for flights that meet these criteria and provide me with the available options?\"     },     {       \"from\": \"gpt\",       \"value\": \"<tool_call>\\n{'arguments': {'departure_city': 'Los Angeles', 'destination_city': 'Auckland', 'departure_date': '2023-07-10', 'return_date': '2023-07-24', 'class': 'economy', 'flexible_cancellation': True}, 'name': 'search_flights'}\\n</tool_call>\\n\"     },     {       \"from\": \"tool\",       \"value\": \"<tool_response>\\n{\\\"name\\\": \\\"search_flights\\\", \\\"content\\\": {\\\"flights\\\": [{\\\"flight_number\\\": \\\"AA118\\\", \\\"airline\\\": \\\"American Airlines\\\", \\\"departure_airport\\\": \\\"LAX\\\", \\\"departure_city\\\": \\\"Los Angeles\\\", \\\"departure_date\\\": \\\"2023-07-10T08:00:00Z\\\", \\\"departure_terminal\\\": \\\"4\\\", \\\"arrival_airport\\\": \\\"AKL\\\", \\\"arrival_city\\\": \\\"Auckland\\\", \\\"arrival_date\\\": \\\"2023-07-11T14:25:00Z\\\", \\\"arrival_terminal\\\": \\\"I\\\", \\\"duration\\\": \\\"17h 25m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$950\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"UA796\\\", \\\"airline\\\": \\\"United Airlines\\\", \\\"departure_airport\\\": \\\"LAX\\\", \\\"departure_city\\\": \\\"Los Angeles\\\", \\\"departure_date\\\": \\\"2023-07-10T11:30:00Z\\\", \\\"departure_terminal\\\": \\\"7\\\", \\\"arrival_airport\\\": \\\"AKL\\\", \\\"arrival_city\\\": \\\"Auckland\\\", \\\"arrival_date\\\": \\\"2023-07-11T18:30:00Z\\\", \\\"arrival_terminal\\\": \\\"I\\\", \\\"duration\\\": \\\"16h 0m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$975\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"NZ6\\\", \\\"airline\\\": \\\"Air New Zealand\\\", \\\"departure_airport\\\": \\\"LAX\\\", \\\"departure_city\\\": \\\"Los Angeles\\\", \\\"departure_date\\\": \\\"2023-07-10T21:00:00Z\\\", \\\"departure_terminal\\\": \\\"B\\\", \\\"arrival_airport\\\": \\\"AKL\\\", \\\"arrival_city\\\": \\\"Auckland\\\", \\\"arrival_date\\\": \\\"2023-07-12T05:20:00Z\\\", \\\"arrival_terminal\\\": \\\"I\\\", \\\"duration\\\": \\\"13h 20m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$1020\\\", \\\"flexible_cancellation\\\": true}], \\\"return_flights\\\": [{\\\"flight_number\\\": \\\"AA117\\\", \\\"airline\\\": \\\"American Airlines\\\", \\\"departure_airport\\\": \\\"AKL\\\", \\\"departure_city\\\": \\\"Auckland\\\", \\\"departure_date\\\": \\\"2023-07-24T14:10:00Z\\\", \\\"departure_terminal\\\": \\\"I\\\", \\\"arrival_airport\\\": \\\"LAX\\\", \\\"arrival_city\\\": \\\"Los Angeles\\\", \\\"arrival_date\\\": \\\"2023-07-24T06:00:00Z\\\", \\\"arrival_terminal\\\": \\\"4\\\", \\\"duration\\\": \\\"12h 50m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$950\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"UA797\\\", \\\"airline\\\": \\\"United Airlines\\\", \\\"departure_airport\\\": \\\"AKL\\\", \\\"departure_city\\\": \\\"Auckland\\\", \\\"departure_date\\\": \\\"2023-07-24T19:15:00Z\\\", \\\"departure_terminal\\\": \\\"I\\\", \\\"arrival_airport\\\": \\\"LAX\\\", \\\"arrival_city\\\": \\\"Los Angeles\\\", \\\"arrival_date\\\": \\\"2023-07-24T10:15:00Z\\\", \\\"arrival_terminal\\\": \\\"7\\\", \\\"duration\\\": \\\"11h 0m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$975\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"NZ5\\\", \\\"airline\\\": \\\"Air New Zealand\\\", \\\"departure_airport\\\": \\\"AKL\\\", \\\"departure_city\\\": \\\"Auckland\\\", \\\"departure_date\\\": \\\"2023-07-24T19:30:00Z\\\", \\\"departure_terminal\\\": \\\"I\\\", \\\"arrival_airport\\\": \\\"LAX\\\", \\\"arrival_city\\\": \\\"Los Angeles\\\", \\\"arrival_date\\\": \\\"2023-07-24T10:30:00Z\\\", \\\"arrival_terminal\\\": \\\"B\\\", \\\"duration\\\": \\\"12h 0m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$1020\\\", \\\"flexible_cancellation\\\": true}]}}\\n</tool_response>\\n\"     },     {       \"from\": \"gpt\",       \"value\": \"Here are the available flight options with flexible cancellation policies for your kayaking trip from Los Angeles to Auckland:\\n\\n**Outbound Flights (Los Angeles to Auckland):**\\n1. **American Airlines AA118**\\n   - Departure: July 10, 2023, 8:00 AM from LAX Terminal 4\\n   - Arrival: July 11, 2023, 2:25 PM at AKL Terminal I\\n   - Duration: 17h 25m\\n   - Price: $950\\n\\n2. **United Airlines UA796**\\n   - Departure: July 10, 2023, 11:30 AM from LAX Terminal 7\\n   - Arrival: July 11, 2023, 6:30 PM at AKL Terminal I\\n   - Duration: 16h 0m\\n   - Price: $975\\n\\n3. **Air New Zealand NZ6**\\n   - Departure: July 10, 2023, 9:00 PM from LAX Terminal B\\n   - Arrival: July 12, 2023, 5:20 AM at AKL Terminal I\\n   - Duration: 13h 20m\\n   - Price: $1020\\n\\n**Return Flights (Auckland to Los Angeles):**\\n1. **American Airlines AA117**\\n   - Departure: July 24, 2023, 2:10 PM from AKL Terminal I\\n   - Arrival: July 24, 2023, 6:00 AM at LAX Terminal 4\\n   - Duration: 12h 50m\\n   - Price: $950\\n\\n2. **United Airlines UA797**\\n   - Departure: July 24, 2023, 7:15 PM from AKL Terminal I\\n   - Arrival: July 24, 2023, 10:15 AM at LAX Terminal 7\\n   - Duration: 11h 0m\\n   - Price: $975\\n\\n3. **Air New Zealand NZ5**\\n   - Departure: July 24, 2023, 7:30 PM from AKL Terminal I\\n   - Arrival: July 24, 2023, 10:30 AM at LAX Terminal B\\n   - Duration: 12h 0m\\n   - Price: $1020\\n\\nAll the listed prices are in economy class with flexible cancellation policies. Please review the options and let me know if you would like to proceed with booking any of these flights or if you need further assistance.\"     }   ],   \"category\": \"E-commerce Platforms\",   \"subcategory\": \"Kayak\",   \"task\": \"Flight Search\" }`\n\n![](https://aman.ai/images/copy.png)\n\n`{   \"id\": \"753d8365-0e54-43b1-9514-3f9b819fd31c\",   \"conversations\": [     {       \"from\": \"system\",       \"value\": \"You are a function calling AI model. You are provided with function signatures within <tools> </tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.\\n<tools>\\n[{'type': 'function', 'function': {'name': 'search_flights', 'description': 'Searches for flights based on departure and destination cities, dates, class, and other preferences.', 'parameters': {'type': 'object', 'properties': {'departure_city': {'type': 'string', 'description': 'The city from which the flight will depart.'}, 'destination_city': {'type': 'string', 'description': 'The destination city for the flight.'}, 'departure_date': {'type': 'string', 'description': 'The departure date for the flight.', 'format': 'date'}, 'return_date': {'type': 'string', 'description': 'The return date for the flight.', 'format': 'date'}, 'class': {'type': 'string', 'description': 'The class of the flight ticket.', 'enum': ['economy', 'business', 'first']}, 'flexible_cancellation': {'type': 'boolean', 'description': 'Indicates if the search should filter for flights with flexible cancellation policies.'}}, 'required': ['departure_city', 'destination_city', 'departure_date', 'return_date', 'class']}}}]\\n</tools>\\nFor each function call return a json object with function name and arguments within <tool_call> </tool_call> tags with the following schema:\\n<tool_call>\\n{'arguments': <args-dict>, 'name': <function-name>}\\n</tool_call>\\n\"     },     {       \"from\": \"human\",       \"value\": \"I'm planning a kayaking trip and looking to book flights from Los Angeles to Auckland. My departure is scheduled for July 10th, 2023, and I intend to return on July 24th, 2023. I would prefer to travel in economy class and would also like the option to have flexible cancellation policies for the tickets due to the uncertain nature of outdoor activities. Could you please search for flights that meet these criteria and provide me with the available options?\"     },     {       \"from\": \"gpt\",       \"value\": \"<tool_call>\\n{'arguments': {'departure_city': 'Los Angeles', 'destination_city': 'Auckland', 'departure_date': '2023-07-10', 'return_date': '2023-07-24', 'class': 'economy', 'flexible_cancellation': True}, 'name': 'search_flights'}\\n</tool_call>\\n\"     },     {       \"from\": \"tool\",       \"value\": \"<tool_response>\\n{\\\"name\\\": \\\"search_flights\\\", \\\"content\\\": {\\\"flights\\\": [{\\\"flight_number\\\": \\\"AA118\\\", \\\"airline\\\": \\\"American Airlines\\\", \\\"departure_airport\\\": \\\"LAX\\\", \\\"departure_city\\\": \\\"Los Angeles\\\", \\\"departure_date\\\": \\\"2023-07-10T08:00:00Z\\\", \\\"departure_terminal\\\": \\\"4\\\", \\\"arrival_airport\\\": \\\"AKL\\\", \\\"arrival_city\\\": \\\"Auckland\\\", \\\"arrival_date\\\": \\\"2023-07-11T14:25:00Z\\\", \\\"arrival_terminal\\\": \\\"I\\\", \\\"duration\\\": \\\"17h 25m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$950\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"UA796\\\", \\\"airline\\\": \\\"United Airlines\\\", \\\"departure_airport\\\": \\\"LAX\\\", \\\"departure_city\\\": \\\"Los Angeles\\\", \\\"departure_date\\\": \\\"2023-07-10T11:30:00Z\\\", \\\"departure_terminal\\\": \\\"7\\\", \\\"arrival_airport\\\": \\\"AKL\\\", \\\"arrival_city\\\": \\\"Auckland\\\", \\\"arrival_date\\\": \\\"2023-07-11T18:30:00Z\\\", \\\"arrival_terminal\\\": \\\"I\\\", \\\"duration\\\": \\\"16h 0m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$975\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"NZ6\\\", \\\"airline\\\": \\\"Air New Zealand\\\", \\\"departure_airport\\\": \\\"LAX\\\", \\\"departure_city\\\": \\\"Los Angeles\\\", \\\"departure_date\\\": \\\"2023-07-10T21:00:00Z\\\", \\\"departure_terminal\\\": \\\"B\\\", \\\"arrival_airport\\\": \\\"AKL\\\", \\\"arrival_city\\\": \\\"Auckland\\\", \\\"arrival_date\\\": \\\"2023-07-12T05:20:00Z\\\", \\\"arrival_terminal\\\": \\\"I\\\", \\\"duration\\\": \\\"13h 20m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$1020\\\", \\\"flexible_cancellation\\\": true}], \\\"return_flights\\\": [{\\\"flight_number\\\": \\\"AA117\\\", \\\"airline\\\": \\\"American Airlines\\\", \\\"departure_airport\\\": \\\"AKL\\\", \\\"departure_city\\\": \\\"Auckland\\\", \\\"departure_date\\\": \\\"2023-07-24T14:10:00Z\\\", \\\"departure_terminal\\\": \\\"I\\\", \\\"arrival_airport\\\": \\\"LAX\\\", \\\"arrival_city\\\": \\\"Los Angeles\\\", \\\"arrival_date\\\": \\\"2023-07-24T06:00:00Z\\\", \\\"arrival_terminal\\\": \\\"4\\\", \\\"duration\\\": \\\"12h 50m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$950\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"UA797\\\", \\\"airline\\\": \\\"United Airlines\\\", \\\"departure_airport\\\": \\\"AKL\\\", \\\"departure_city\\\": \\\"Auckland\\\", \\\"departure_date\\\": \\\"2023-07-24T19:15:00Z\\\", \\\"departure_terminal\\\": \\\"I\\\", \\\"arrival_airport\\\": \\\"LAX\\\", \\\"arrival_city\\\": \\\"Los Angeles\\\", \\\"arrival_date\\\": \\\"2023-07-24T10:15:00Z\\\", \\\"arrival_terminal\\\": \\\"7\\\", \\\"duration\\\": \\\"11h 0m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$975\\\", \\\"flexible_cancellation\\\": true}, {\\\"flight_number\\\": \\\"NZ5\\\", \\\"airline\\\": \\\"Air New Zealand\\\", \\\"departure_airport\\\": \\\"AKL\\\", \\\"departure_city\\\": \\\"Auckland\\\", \\\"departure_date\\\": \\\"2023-07-24T19:30:00Z\\\", \\\"departure_terminal\\\": \\\"I\\\", \\\"arrival_airport\\\": \\\"LAX\\\", \\\"arrival_city\\\": \\\"Los Angeles\\\", \\\"arrival_date\\\": \\\"2023-07-24T10:30:00Z\\\", \\\"arrival_terminal\\\": \\\"B\\\", \\\"duration\\\": \\\"12h 0m\\\", \\\"class\\\": \\\"Economy\\\", \\\"price\\\": \\\"$1020\\\", \\\"flexible_cancellation\\\": true}]}}\\n</tool_response>\\n\"     },     {       \"from\": \"gpt\",       \"value\": \"Here are the available flight options with flexible cancellation policies for your kayaking trip from Los Angeles to Auckland:\\n\\n**Outbound Flights (Los Angeles to Auckland):**\\n1. **American Airlines AA118**\\n   - Departure: July 10, 2023, 8:00 AM from LAX Terminal 4\\n   - Arrival: July 11, 2023, 2:25 PM at AKL Terminal I\\n   - Duration: 17h 25m\\n   - Price: $950\\n\\n2. **United Airlines UA796**\\n   - Departure: July 10, 2023, 11:30 AM from LAX Terminal 7\\n   - Arrival: July 11, 2023, 6:30 PM at AKL Terminal I\\n   - Duration: 16h 0m\\n   - Price: $975\\n\\n3. **Air New Zealand NZ6**\\n   - Departure: July 10, 2023, 9:00 PM from LAX Terminal B\\n   - Arrival: July 12, 2023, 5:20 AM at AKL Terminal I\\n   - Duration: 13h 20m\\n   - Price: $1020\\n\\n**Return Flights (Auckland to Los Angeles):**\\n1. **American Airlines AA117**\\n   - Departure: July 24, 2023, 2:10 PM from AKL Terminal I\\n   - Arrival: July 24, 2023, 6:00 AM at LAX Terminal 4\\n   - Duration: 12h 50m\\n   - Price: $950\\n\\n2. **United Airlines UA797**\\n   - Departure: July 24, 2023, 7:15 PM from AKL Terminal I\\n   - Arrival: July 24, 2023, 10:15 AM at LAX Terminal 7\\n   - Duration: 11h 0m\\n   - Price: $975\\n\\n3. **Air New Zealand NZ5**\\n   - Departure: July 24, 2023, 7:30 PM from AKL Terminal I\\n   - Arrival: July 24, 2023, 10:30 AM at LAX Terminal B\\n   - Duration: 12h 0m\\n   - Price: $1020\\n\\nAll the listed prices are in economy class with flexible cancellation policies. Please review the options and let me know if you would like to proceed with booking any of these flights or if you need further assistance.\"     }   ],   \"category\": \"E-commerce Platforms\",   \"subcategory\": \"Kayak\",   \"task\": \"Flight Search\" }`\n\n#### [Gorilla OpenFunctions-v2 LLM](https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2)\n\n*   **Overview**:\n    \n    *   **State-of-the-Art Performance:** Gorilla OpenFunctions-v2 is an open-source Large Language Model (LLM) that offers advanced function-calling capabilities, comparable to GPT-4.\n    *   **Extended Chat Completion:** Extends LLM chat completion with the ability to generate executable API calls from natural language instructions and relevant API contexts.\n*   **Key Features**:\n    \n    *   **Multi-Function Support:**\n        *   Allows selection from multiple available functions based on user instructions, offering flexibility and adaptability within a single prompt.\n    *   **Parallel Function Calling:**\n        *   Supports executing the same function multiple times with different parameter values, streamlining workflows needing simultaneous function calls.\n    *   **Combined Multi & Parallel Functionality:**\n        *   Executes both multi-function and parallel function calls in one chat completion call, handling complex API call scenarios in a single prompt for efficient, high-capability outputs.\n    *   **Expanded Data Type Support:**\n        *   Enhanced compatibility with diverse programming languages by supporting extensive data types:\n            *   **Python:** Supports `string`, `number`, `boolean`, `list`, `tuple`, `dict`, and `Any`.\n            *   **Java:** Includes support for `byte`, `short`, `int`, `float`, `double`, `long`, `boolean`, `char`, and complex types like `ArrayList`, `Set`, `HashMap`, and `Stack`.\n            *   **JavaScript:** Covers `String`, `Number`, `BigInt`, `Boolean`, `Array`, `Date`, `dict (object)`, and `Any`.\n        *   Extending beyond typical JSON schema limits, this feature allows users to leverage OpenFunctions-v2 in a straightforward plug-and-play fashion without intricate data handling or reliance on string literals.\n    *   **Function Relevance Detection:**\n        *   Minimizes irrelevant function calls by detecting whether the user’s prompt is conversational or function-oriented.\n        *   If no function is relevant, the model raises an “Error” message with additional guidance, helping refine requests and reducing hallucinations.\n    *   **Enhanced RESTful API Capabilities:**\n        *   Specially trained to handle RESTful API calls, Gorilla OpenFunctions-v2 optimizes interactions with widely-used services, such as Slack and PayPal.\n        *   This high-quality support for REST API execution boosts compatibility across a broad range of applications and services.\n    *   **Pioneering Open-Source Model with Seamless Integration:**\n        *   As the first open-source model to support multi-language, multi-function, and parallel function calls, Gorilla OpenFunctions-v2 stands at the forefront of function calling in LLMs.\n        *   Integrates effortlessly into diverse applications, making it a seamless drop-in replacement that requires minimal setup.\n    *   **Broad Application Compatibility:**\n        *   Gorilla OpenFunctions-v2’s versatility supports a wide range of platforms, from social media like Instagram to delivery and utility services such as Google Calendar, Stripe, and DoorDash.\n        *   Its adaptability makes it a top choice for developers aiming to expand functional capabilities across multiple sectors with ease.\n*   The figure below ([source](https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2)) highlights some of the key features of OpenFunctions-v2:\n    \n\n**Overview**:\n\n*   **State-of-the-Art Performance:** Gorilla OpenFunctions-v2 is an open-source Large Language Model (LLM) that offers advanced function-calling capabilities, comparable to GPT-4.\n*   **Extended Chat Completion:** Extends LLM chat completion with the ability to generate executable API calls from natural language instructions and relevant API contexts.\n\n**Key Features**:\n\n*   **Multi-Function Support:**\n    *   Allows selection from multiple available functions based on user instructions, offering flexibility and adaptability within a single prompt.\n*   **Parallel Function Calling:**\n    *   Supports executing the same function multiple times with different parameter values, streamlining workflows needing simultaneous function calls.\n*   **Combined Multi & Parallel Functionality:**\n    *   Executes both multi-function and parallel function calls in one chat completion call, handling complex API call scenarios in a single prompt for efficient, high-capability outputs.\n*   **Expanded Data Type Support:**\n    *   Enhanced compatibility with diverse programming languages by supporting extensive data types:\n        *   **Python:** Supports `string`, `number`, `boolean`, `list`, `tuple`, `dict`, and `Any`.\n        *   **Java:** Includes support for `byte`, `short`, `int`, `float`, `double`, `long`, `boolean`, `char`, and complex types like `ArrayList`, `Set`, `HashMap`, and `Stack`.\n        *   **JavaScript:** Covers `String`, `Number`, `BigInt`, `Boolean`, `Array`, `Date`, `dict (object)`, and `Any`.\n    *   Extending beyond typical JSON schema limits, this feature allows users to leverage OpenFunctions-v2 in a straightforward plug-and-play fashion without intricate data handling or reliance on string literals.\n*   **Function Relevance Detection:**\n    *   Minimizes irrelevant function calls by detecting whether the user’s prompt is conversational or function-oriented.\n    *   If no function is relevant, the model raises an “Error” message with additional guidance, helping refine requests and reducing hallucinations.\n*   **Enhanced RESTful API Capabilities:**\n    *   Specially trained to handle RESTful API calls, Gorilla OpenFunctions-v2 optimizes interactions with widely-used services, such as Slack and PayPal.\n    *   This high-quality support for REST API execution boosts compatibility across a broad range of applications and services.\n*   **Pioneering Open-Source Model with Seamless Integration:**\n    *   As the first open-source model to support multi-language, multi-function, and parallel function calls, Gorilla OpenFunctions-v2 stands at the forefront of function calling in LLMs.\n    *   Integrates effortlessly into diverse applications, making it a seamless drop-in replacement that requires minimal setup.\n*   **Broad Application Compatibility:**\n    *   Gorilla OpenFunctions-v2’s versatility supports a wide range of platforms, from social media like Instagram to delivery and utility services such as Google Calendar, Stripe, and DoorDash.\n    *   Its adaptability makes it a top choice for developers aiming to expand functional capabilities across multiple sectors with ease.\n\n*   Allows selection from multiple available functions based on user instructions, offering flexibility and adaptability within a single prompt.\n\n*   Supports executing the same function multiple times with different parameter values, streamlining workflows needing simultaneous function calls.\n\n*   Executes both multi-function and parallel function calls in one chat completion call, handling complex API call scenarios in a single prompt for efficient, high-capability outputs.\n\n*   Enhanced compatibility with diverse programming languages by supporting extensive data types:\n    *   **Python:** Supports `string`, `number`, `boolean`, `list`, `tuple`, `dict`, and `Any`.\n    *   **Java:** Includes support for `byte`, `short`, `int`, `float`, `double`, `long`, `boolean`, `char`, and complex types like `ArrayList`, `Set`, `HashMap`, and `Stack`.\n    *   **JavaScript:** Covers `String`, `Number`, `BigInt`, `Boolean`, `Array`, `Date`, `dict (object)`, and `Any`.\n*   Extending beyond typical JSON schema limits, this feature allows users to leverage OpenFunctions-v2 in a straightforward plug-and-play fashion without intricate data handling or reliance on string literals.\n\n*   **Python:** Supports `string`, `number`, `boolean`, `list`, `tuple`, `dict`, and `Any`.\n*   **Java:** Includes support for `byte`, `short`, `int`, `float`, `double`, `long`, `boolean`, `char`, and complex types like `ArrayList`, `Set`, `HashMap`, and `Stack`.\n*   **JavaScript:** Covers `String`, `Number`, `BigInt`, `Boolean`, `Array`, `Date`, `dict (object)`, and `Any`.\n\n*   Minimizes irrelevant function calls by detecting whether the user’s prompt is conversational or function-oriented.\n*   If no function is relevant, the model raises an “Error” message with additional guidance, helping refine requests and reducing hallucinations.\n\n*   Specially trained to handle RESTful API calls, Gorilla OpenFunctions-v2 optimizes interactions with widely-used services, such as Slack and PayPal.\n*   This high-quality support for REST API execution boosts compatibility across a broad range of applications and services.\n\n*   As the first open-source model to support multi-language, multi-function, and parallel function calls, Gorilla OpenFunctions-v2 stands at the forefront of function calling in LLMs.\n*   Integrates effortlessly into diverse applications, making it a seamless drop-in replacement that requires minimal setup.\n\n*   Gorilla OpenFunctions-v2’s versatility supports a wide range of platforms, from social media like Instagram to delivery and utility services such as Google Calendar, Stripe, and DoorDash.\n*   Its adaptability makes it a top choice for developers aiming to expand functional capabilities across multiple sectors with ease.\n\nThe figure below ([source](https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2)) highlights some of the key features of OpenFunctions-v2:\n\n![](/primers/ai/assets/agents/OpenFunctionsV2.jpg)\n\n###### Example\n\n*   The example below demonstrates function calling, where the LLM interprets a natural language prompt to generate an API request. Given the user’s request for weather data at specific coordinates, the model formulates an API call with precise parameters, enabling automated data retrieval.\n\n![](https://aman.ai/images/copy.png)\n\n`\"User\": \"Can you fetch me the weather data for the coordinates  37.8651 N, 119.5383 W, including the hourly forecast for temperature,  wind speed, and precipitation for the next 10 days?\"  \"Function\":  {     ...     \"parameters\":      {         \"type\": \"object\",          \"properties\":          {             \"url\":              {                 \"type\": \"string\",                  \"description\": \"The API endpoint for fetching weather                 data from the Open-Meteo API for the given latitude                 and longitude, default                  https://api.open-meteo.com/v1/forecast\"             }             ...         }     } }  \"GPT-4 output\": {     \"name\": \"requests.get\",     \"parameters\": {         \"params\":          {             \"latitude\": \"37.8651\",             \"longitude\": \"-119.5383\",             \"forecast_days\": 10         },     } }`\n\n![](https://aman.ai/images/copy.png)\n\n`\"User\": \"Can you fetch me the weather data for the coordinates  37.8651 N, 119.5383 W, including the hourly forecast for temperature,  wind speed, and precipitation for the next 10 days?\"  \"Function\":  {     ...     \"parameters\":      {         \"type\": \"object\",          \"properties\":          {             \"url\":              {                 \"type\": \"string\",                  \"description\": \"The API endpoint for fetching weather                 data from the Open-Meteo API for the given latitude                 and longitude, default                  https://api.open-meteo.com/v1/forecast\"             }             ...         }     } }  \"GPT-4 output\": {     \"name\": \"requests.get\",     \"parameters\": {         \"params\":          {             \"latitude\": \"37.8651\",             \"longitude\": \"-119.5383\",             \"forecast_days\": 10         },     } }`\n\n#### Best Practices, Guidelines, and Limitations\n\n*   Per [Anthropic](https://x.com/swyx/status/1775993946935906645), when using tools with Claude, it’s important to follow best practices, understand limitations, and optimize tool design to ensure effective interactions.\n\n##### Choosing the Right Model for Tool Use\n\n*   **Claude 3 Opus** is best for complex tool use, as it can handle multiple tools simultaneously and detect missing arguments. It will ask for clarification when necessary.\n*   **Claude 3 Haiku** is better suited for simple tool use but defaults to using tools more frequently—even when unnecessary. It will also infer missing parameters rather than asking for clarification.\n\n##### Tool Usage Limits\n\n*   **Handling large toolsets**\n    *   Claude can accurately select from over 250+ tools, as long as the user query includes all required parameters.\n    *   This limit applies regardless of tool complexity. Complex tools typically have numerous parameters or deeply nested schemas.\n*   **Optimizing tool complexity**\n    *   Claude performs better with simpler tools.\n    *   To improve accuracy, avoid deeply nested JSON objects and reduce the number of required inputs.\n\n*   Claude can accurately select from over 250+ tools, as long as the user query includes all required parameters.\n*   This limit applies regardless of tool complexity. Complex tools typically have numerous parameters or deeply nested schemas.\n\n*   Claude performs better with simpler tools.\n*   To improve accuracy, avoid deeply nested JSON objects and reduce the number of required inputs.\n\n##### Sequential vs. Parallel Tool Execution\n\n*   Claude generally prefers sequential tool execution—using one tool at a time, analyzing the output, and then deciding on the next step.\n*   While parallel tool use is possible, it may lead to:\n    *   Missing dependencies (e.g., filling in placeholder values for parameters that depend on previous outputs).\n    *   Unnecessary tool invocations.\n*   Best practice: Design workflows that encourage sequential tool execution to improve accuracy.\n\n*   Missing dependencies (e.g., filling in placeholder values for parameters that depend on previous outputs).\n*   Unnecessary tool invocations.\n\n##### Error Handling and Retries\n\n*   If Claude’s tool request is invalid, returning an error response will often prompt it to retry with missing parameters filled in.\n*   However, after 2-3 failed attempts, Claude may stop retrying and instead return an apology message.\n\n#### Designing Effective Tool Interfaces\n\n*   Tools are an essential part of agentic systems, enabling Claude to interact with external services and APIs. However, tools should be carefully designed, just like prompts, to ensure clarity and usability. By thoughtfully designing your tools, you can reduce errors, improve model accuracy, and create more efficient agent workflows, per the below guidelines per [Anthropic’s blog on building effective agents](https://www.anthropic.com/engineering/building-effective-agents).\n\n##### Choosing the Right Tool Format\n\n*   The way tools are structured can significantly impact Claude’s accuracy. Some formats are harder for an LLM to generate correctly than others:\n*   **Diff vs. Full Rewrite**:\n    *   Writing a diff requires pre-determining line changes, which is error-prone.\n    *   A full rewrite avoids this complexity.\n*   **Markdown vs. JSON for structured output**:\n    *   JSON requires escaping newlines and quotes, increasing error risks.\n    *   Markdown is often easier for Claude to handle accurately.\n\n*   Writing a diff requires pre-determining line changes, which is error-prone.\n*   A full rewrite avoids this complexity.\n\n*   JSON requires escaping newlines and quotes, increasing error risks.\n*   Markdown is often easier for Claude to handle accurately.\n\n##### Best Practices for Tool Design\n\n*   Give the model enough tokens to “think” before committing to an output.\n*   Stick to familiar formats that naturally occur in publicly available text (e.g., Markdown over escaped JSON).\n*   Minimize formatting overhead—avoid requiring Claude to track line counts or escape large text blocks.\n\n##### Improving Tool Usability\n\n*   Make tool descriptions intuitive—think like a human: Would a developer immediately understand how to use this tool?\n*   Refine parameter names and descriptions—write them as if documenting an API for a junior developer.\n*   Test extensively—run real-world inputs in a sandbox environment to uncover edge cases and refine accordingly.\n*   Poka-yoke (mistake-proof) your tools.\n    *   **Example:** Instead of using relative file paths, require absolute paths to avoid errors when switching directories.\n\n*   **Example:** Instead of using relative file paths, require absolute paths to avoid errors when switching directories.",
    "order": 6,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 50,
    "tags": [
      "algorithmsarchitecture",
      "transformer",
      "gpt",
      "llm",
      "computer vision"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 9878,
      "contentLength": 122276
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#function/tool/api-calling",
    "scrapedAt": "2025-12-28T11:47:30.114Z"
  },
  {
    "id": "ai-agents-planning-7",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Design Patterns",
    "title": "Planning",
    "subtitle": "Agentic Design Patterns",
    "contentHtml": "<ul>\n  <li>Planning is a foundational design pattern that empowers an AI system, typically a LLM, to autonomously determine a sequence of actions or steps needed to accomplish complex tasks. Through this dynamic decision-making process, the AI breaks down broad objectives into smaller, manageable steps, executing them in a structured sequence to produce coherent, often intricate outputs. This document delves into the importance of Planning in agentic AI design, illustrating its function with examples and examining its current capabilities alongside its limitations.</li>\n  <li>As a transformative design pattern, Planning grants LLMs the ability to autonomously devise and execute plans/strategies for completing tasks. Although current implementations can still exhibit unpredictability, Planning can empower an AI agent with enhanced creative problem-solving capability that enables it to navigate tasks in unforeseen, innovative ways.</li>\n  <li>The power of Planning lies in its flexibility and adaptability. When effectively implemented, Planning enables an AI to respond to unforeseen conditions, make informed decisions about task progression, and select tools best suited to each step. This autonomy, however, introduces unpredictability in the agent’s behavior and outcomes.</li>\n</ul>\n<h4 id=\"overview-2\">Overview</h4>\n<ul>\n  <li>Planning in agentic AI refers to the AI’s ability to autonomously design a task plan, selecting the steps necessary to achieve a given goal. Unlike more deterministic processes, Planning involves a level of adaptability, allowing the AI to adjust its approach based on available tools, task requirements, and unforeseen constraints.</li>\n  <li>For example, if an AI agent is tasked with conducting online research on a specific topic, it can independently generate a series of subtasks. These might include identifying key subtopics, gathering relevant information from reputable sources, synthesizing findings, and compiling the research into a cohesive report. Through Planning, the agent does not simply execute pre-programmed instructions but rather determines the optimal sequence of actions to meet the objective.</li>\n</ul>\n<h4 id=\"example-3\">Example</h4>\n<ul>\n  <li>\n    <p>Agentic Planning becomes especially critical when tasks are multifaceted and cannot be completed in a single step. In such cases, an LLM-driven agent dynamically designs a sequence of steps to accomplish the overarching goal. An example from the HuggingGPT paper illustrates this approach: if the objective is to render a picture of a girl in the same pose as a boy in an initial image, the AI might decompose the task as follows:</p>\n\n    <ol>\n      <li><strong>Step 1:</strong> Detect the pose in the initial picture of the boy using a pose-detection tool, producing a temporary output file (e.g., <code class=\"language-plaintext highlighter-rouge\">temp1</code>).</li>\n      <li><strong>Step 2:</strong> Use a pose-to-image tool to generate an image of a girl in the detected pose from <code class=\"language-plaintext highlighter-rouge\">temp1</code>, yielding the final output.</li>\n    </ol>\n  </li>\n  <li>In this structured format, the AI specifies each action step, defining the tool to use, the input file, and the expected output. This process then triggers software that invokes the necessary tools in the designated sequence to complete the task successfully. The agent’s autonomous Planning ability facilitates this multi-step workflow, demonstrating its capacity to tackle intricate, non-linear tasks.</li>\n  <li>The following figure (<a href=\"https://arxiv.org/abs/2303.17580\">source</a>) offers a visual overview of the above process:</li>\n</ul>\n<p>Agentic Planning becomes especially critical when tasks are multifaceted and cannot be completed in a single step. In such cases, an LLM-driven agent dynamically designs a sequence of steps to accomplish the overarching goal. An example from the HuggingGPT paper illustrates this approach: if the objective is to render a picture of a girl in the same pose as a boy in an initial image, the AI might decompose the task as follows:</p>\n<ol>\n      <li><strong>Step 1:</strong> Detect the pose in the initial picture of the boy using a pose-detection tool, producing a temporary output file (e.g., <code class=\"language-plaintext highlighter-rouge\">temp1</code>).</li>\n      <li><strong>Step 2:</strong> Use a pose-to-image tool to generate an image of a girl in the detected pose from <code class=\"language-plaintext highlighter-rouge\">temp1</code>, yielding the final output.</li>\n    </ol>\n<p><img src=\"assets/agents/Planning.png\" alt=\"\"></p>\n<h4 id=\"planning-vs-deterministic-approaches\">Planning vs. Deterministic Approaches</h4>\n<ul>\n  <li>Planning is not required in every agentic workflow. For simpler tasks or those that follow a predefined sequence, a deterministic, step-by-step approach may suffice. For instance, if an agent is programmed to reflect on and revise its output a fixed number of times, it can execute this series of steps without needing adaptive planning.</li>\n  <li>However, for complex or open-ended tasks where it is difficult to predefine the necessary sequence, Planning allows the AI to dynamically decide on the appropriate steps. This adaptive approach is especially valuable for tasks that may involve unexpected challenges or require the agent to select from a range of tools and methods to reach the best outcome.</li>\n</ul>",
    "contentMarkdown": "*   Planning is a foundational design pattern that empowers an AI system, typically a LLM, to autonomously determine a sequence of actions or steps needed to accomplish complex tasks. Through this dynamic decision-making process, the AI breaks down broad objectives into smaller, manageable steps, executing them in a structured sequence to produce coherent, often intricate outputs. This document delves into the importance of Planning in agentic AI design, illustrating its function with examples and examining its current capabilities alongside its limitations.\n*   As a transformative design pattern, Planning grants LLMs the ability to autonomously devise and execute plans/strategies for completing tasks. Although current implementations can still exhibit unpredictability, Planning can empower an AI agent with enhanced creative problem-solving capability that enables it to navigate tasks in unforeseen, innovative ways.\n*   The power of Planning lies in its flexibility and adaptability. When effectively implemented, Planning enables an AI to respond to unforeseen conditions, make informed decisions about task progression, and select tools best suited to each step. This autonomy, however, introduces unpredictability in the agent’s behavior and outcomes.\n\n#### Overview\n\n*   Planning in agentic AI refers to the AI’s ability to autonomously design a task plan, selecting the steps necessary to achieve a given goal. Unlike more deterministic processes, Planning involves a level of adaptability, allowing the AI to adjust its approach based on available tools, task requirements, and unforeseen constraints.\n*   For example, if an AI agent is tasked with conducting online research on a specific topic, it can independently generate a series of subtasks. These might include identifying key subtopics, gathering relevant information from reputable sources, synthesizing findings, and compiling the research into a cohesive report. Through Planning, the agent does not simply execute pre-programmed instructions but rather determines the optimal sequence of actions to meet the objective.\n\n#### Example\n\n*   Agentic Planning becomes especially critical when tasks are multifaceted and cannot be completed in a single step. In such cases, an LLM-driven agent dynamically designs a sequence of steps to accomplish the overarching goal. An example from the HuggingGPT paper illustrates this approach: if the objective is to render a picture of a girl in the same pose as a boy in an initial image, the AI might decompose the task as follows:\n    \n    1.  **Step 1:** Detect the pose in the initial picture of the boy using a pose-detection tool, producing a temporary output file (e.g., `temp1`).\n    2.  **Step 2:** Use a pose-to-image tool to generate an image of a girl in the detected pose from `temp1`, yielding the final output.\n*   In this structured format, the AI specifies each action step, defining the tool to use, the input file, and the expected output. This process then triggers software that invokes the necessary tools in the designated sequence to complete the task successfully. The agent’s autonomous Planning ability facilitates this multi-step workflow, demonstrating its capacity to tackle intricate, non-linear tasks.\n*   The following figure ([source](https://arxiv.org/abs/2303.17580)) offers a visual overview of the above process:\n\nAgentic Planning becomes especially critical when tasks are multifaceted and cannot be completed in a single step. In such cases, an LLM-driven agent dynamically designs a sequence of steps to accomplish the overarching goal. An example from the HuggingGPT paper illustrates this approach: if the objective is to render a picture of a girl in the same pose as a boy in an initial image, the AI might decompose the task as follows:\n\n1.  **Step 1:** Detect the pose in the initial picture of the boy using a pose-detection tool, producing a temporary output file (e.g., `temp1`).\n2.  **Step 2:** Use a pose-to-image tool to generate an image of a girl in the detected pose from `temp1`, yielding the final output.\n\n![](assets/agents/Planning.png)\n\n#### Planning vs. Deterministic Approaches\n\n*   Planning is not required in every agentic workflow. For simpler tasks or those that follow a predefined sequence, a deterministic, step-by-step approach may suffice. For instance, if an agent is programmed to reflect on and revise its output a fixed number of times, it can execute this series of steps without needing adaptive planning.\n*   However, for complex or open-ended tasks where it is difficult to predefine the necessary sequence, Planning allows the AI to dynamically decide on the appropriate steps. This adaptive approach is especially valuable for tasks that may involve unexpected challenges or require the agent to select from a range of tools and methods to reach the best outcome.",
    "order": 7,
    "orderInChapter": 3,
    "difficulty": 3,
    "estimatedMinutes": 4,
    "tags": [
      "algorithmsarchitecture",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 735,
      "contentLength": 5450
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#planning",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-multi-agent-collaboration-8",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Design Patterns",
    "title": "Multi-agent Collaboration",
    "subtitle": "Agentic Design Patterns",
    "contentHtml": "<h4 id=\"background\">Background</h4>\n<ul>\n  <li>Multi-agent collaboration has emerged as a pivotal AI design pattern for executing complex tasks by breaking them down into manageable subtasks. By assigning these subtasks to specialized agents—each acting as a software engineer, product manager, designer, QA engineer, etc.—multi-agent collaboration mirrors the structure of a well-coordinated team, where each agent performs specific, designated roles. These agents, whether built by prompting a single LLM in various ways or by employing multiple LLMs, can carry out their assigned tasks with tailored capabilities. For instance, prompting an LLM to act as a “software engineer” by instructing it to “write clear, efficient code” enables it to focus solely on that aspect, thereby honing its output to the requirements of the software engineering subtask.</li>\n  <li>This approach has strong parallels in multi-threading, where complex programs are divided across multiple processors or threads to be executed concurrently, improving efficiency and performance. The agentic model thus offers a divide-and-conquer structure that enables AI systems to manage intricate workflows by breaking them into smaller, role-based actions.</li>\n</ul>\n<h4 id=\"motivation\">Motivation</h4>\n<ul>\n  <li>The adoption of multi-agent systems in AI is driven by several key factors:</li>\n</ul>\n<ol>\n  <li>\n    <p><strong>Demonstrated Effectiveness:</strong> The multi-agent approach has consistently produced positive results across various projects. Ablation studies, such as those presented in the AutoGen paper, have confirmed that multi-agent systems often yield superior performance compared to single-agent configurations for complex tasks. The multi-agent structure allows each agent to focus narrowly on a specific subtask, which is conducive to better performance than attempting to accomplish the entire task in a monolithic approach.</p>\n  </li>\n  <li>\n    <p><strong>Enhanced Task Focus and Optimization:</strong> Despite recent advancements allowing some LLMs to accept extensive input contexts (e.g., Gemini 1.5 Pro with 1 million tokens), a multi-agent system still holds distinct advantages. Each agent can be directed to focus on one isolated subtask at a time, enhancing its ability to execute that task with precision. By setting tailored expectations—such as prioritizing code clarity for a “software engineer” agent over scalability or security—developers can optimize the output of each subtask according to specific project requirements.</p>\n  </li>\n  <li>\n    <p><strong>Decomposition of Complex Tasks:</strong> Beyond immediate efficiency gains, multi-agent systems offer a powerful conceptual framework for managing complex tasks by breaking them down into smaller, more manageable subtasks. This design pattern enables developers to simplify workflows while simultaneously enhancing communication and task alignment among agents. Much like a manager in a company would assign tasks to specialized employees to address different facets of a project, multi-agent systems use this human organizational structure as a blueprint for assigning AI tasks.</p>\n  </li>\n</ol>\n<p><strong>Demonstrated Effectiveness:</strong> The multi-agent approach has consistently produced positive results across various projects. Ablation studies, such as those presented in the AutoGen paper, have confirmed that multi-agent systems often yield superior performance compared to single-agent configurations for complex tasks. The multi-agent structure allows each agent to focus narrowly on a specific subtask, which is conducive to better performance than attempting to accomplish the entire task in a monolithic approach.</p>\n<p><strong>Enhanced Task Focus and Optimization:</strong> Despite recent advancements allowing some LLMs to accept extensive input contexts (e.g., Gemini 1.5 Pro with 1 million tokens), a multi-agent system still holds distinct advantages. Each agent can be directed to focus on one isolated subtask at a time, enhancing its ability to execute that task with precision. By setting tailored expectations—such as prioritizing code clarity for a “software engineer” agent over scalability or security—developers can optimize the output of each subtask according to specific project requirements.</p>\n<p><strong>Decomposition of Complex Tasks:</strong> Beyond immediate efficiency gains, multi-agent systems offer a powerful conceptual framework for managing complex tasks by breaking them down into smaller, more manageable subtasks. This design pattern enables developers to simplify workflows while simultaneously enhancing communication and task alignment among agents. Much like a manager in a company would assign tasks to specialized employees to address different facets of a project, multi-agent systems use this human organizational structure as a blueprint for assigning AI tasks.</p>\n<ul>\n  <li>This design abstraction supports developers in “hiring” agents for distinct roles and assigning tasks according to their “specializations,” with each agent independently executing its workflow, utilizing memory to track interactions, and potentially collaborating with other agents as necessary. Multi-agent workflows can involve dynamic elements like planning and tool use, enabling agents to respond adaptively and collectively in complex scenarios through interconnected calls and message passing.</li>\n</ul>",
    "contentMarkdown": "#### Background\n\n*   Multi-agent collaboration has emerged as a pivotal AI design pattern for executing complex tasks by breaking them down into manageable subtasks. By assigning these subtasks to specialized agents—each acting as a software engineer, product manager, designer, QA engineer, etc.—multi-agent collaboration mirrors the structure of a well-coordinated team, where each agent performs specific, designated roles. These agents, whether built by prompting a single LLM in various ways or by employing multiple LLMs, can carry out their assigned tasks with tailored capabilities. For instance, prompting an LLM to act as a “software engineer” by instructing it to “write clear, efficient code” enables it to focus solely on that aspect, thereby honing its output to the requirements of the software engineering subtask.\n*   This approach has strong parallels in multi-threading, where complex programs are divided across multiple processors or threads to be executed concurrently, improving efficiency and performance. The agentic model thus offers a divide-and-conquer structure that enables AI systems to manage intricate workflows by breaking them into smaller, role-based actions.\n\n#### Motivation\n\n*   The adoption of multi-agent systems in AI is driven by several key factors:\n\n1.  **Demonstrated Effectiveness:** The multi-agent approach has consistently produced positive results across various projects. Ablation studies, such as those presented in the AutoGen paper, have confirmed that multi-agent systems often yield superior performance compared to single-agent configurations for complex tasks. The multi-agent structure allows each agent to focus narrowly on a specific subtask, which is conducive to better performance than attempting to accomplish the entire task in a monolithic approach.\n    \n2.  **Enhanced Task Focus and Optimization:** Despite recent advancements allowing some LLMs to accept extensive input contexts (e.g., Gemini 1.5 Pro with 1 million tokens), a multi-agent system still holds distinct advantages. Each agent can be directed to focus on one isolated subtask at a time, enhancing its ability to execute that task with precision. By setting tailored expectations—such as prioritizing code clarity for a “software engineer” agent over scalability or security—developers can optimize the output of each subtask according to specific project requirements.\n    \n3.  **Decomposition of Complex Tasks:** Beyond immediate efficiency gains, multi-agent systems offer a powerful conceptual framework for managing complex tasks by breaking them down into smaller, more manageable subtasks. This design pattern enables developers to simplify workflows while simultaneously enhancing communication and task alignment among agents. Much like a manager in a company would assign tasks to specialized employees to address different facets of a project, multi-agent systems use this human organizational structure as a blueprint for assigning AI tasks.\n    \n\n**Demonstrated Effectiveness:** The multi-agent approach has consistently produced positive results across various projects. Ablation studies, such as those presented in the AutoGen paper, have confirmed that multi-agent systems often yield superior performance compared to single-agent configurations for complex tasks. The multi-agent structure allows each agent to focus narrowly on a specific subtask, which is conducive to better performance than attempting to accomplish the entire task in a monolithic approach.\n\n**Enhanced Task Focus and Optimization:** Despite recent advancements allowing some LLMs to accept extensive input contexts (e.g., Gemini 1.5 Pro with 1 million tokens), a multi-agent system still holds distinct advantages. Each agent can be directed to focus on one isolated subtask at a time, enhancing its ability to execute that task with precision. By setting tailored expectations—such as prioritizing code clarity for a “software engineer” agent over scalability or security—developers can optimize the output of each subtask according to specific project requirements.\n\n**Decomposition of Complex Tasks:** Beyond immediate efficiency gains, multi-agent systems offer a powerful conceptual framework for managing complex tasks by breaking them down into smaller, more manageable subtasks. This design pattern enables developers to simplify workflows while simultaneously enhancing communication and task alignment among agents. Much like a manager in a company would assign tasks to specialized employees to address different facets of a project, multi-agent systems use this human organizational structure as a blueprint for assigning AI tasks.\n\n*   This design abstraction supports developers in “hiring” agents for distinct roles and assigning tasks according to their “specializations,” with each agent independently executing its workflow, utilizing memory to track interactions, and potentially collaborating with other agents as necessary. Multi-agent workflows can involve dynamic elements like planning and tool use, enabling agents to respond adaptively and collectively in complex scenarios through interconnected calls and message passing.",
    "order": 8,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 4,
    "tags": [
      "algorithmsarchitecture",
      "llm",
      "optimization"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 718,
      "contentLength": 5426
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#multi-agent-collaboration",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-implementation-9",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Design Patterns",
    "title": "Implementation",
    "subtitle": "Agentic Design Patterns",
    "contentHtml": "<ul>\n  <li>\n    <p>While managing human teams has inherent challenges, applying similar organizational strategies to multi-agent AI systems is not only manageable but also offers low-risk flexibility; any issues in an AI agent’s performance are easily rectified. Emerging frameworks such as AutoGen, CrewAI, and LangGraph provide robust platforms for developing and implementing multi-agent systems tailored to diverse applications. Additionally, open-source projects like <a href=\"#chatdev-communicative-agents-for-software-development\">ChatDev</a> allow developers to experiment with multi-agent setups in a virtual “software company” environment, offering valuable insights into the collaborative potential of AI agents. Such tools represent the leading edge of multi-agent technology, providing a foundation for AI-driven task decomposition and collaboration.</p>\n  </li>\n  <li>\n    <p>In summary, multi-agent collaboration is a compelling and effective AI design pattern that leverages agent specialization, task decomposition, and focused prompting to enable more efficient handling of complex tasks. As multi-agent frameworks continue to advance, they are likely to become foundational in AI-driven workflows, providing developers with both the structure and flexibility to tackle increasingly sophisticated projects.</p>\n  </li>\n</ul>\n<p>While managing human teams has inherent challenges, applying similar organizational strategies to multi-agent AI systems is not only manageable but also offers low-risk flexibility; any issues in an AI agent’s performance are easily rectified. Emerging frameworks such as AutoGen, CrewAI, and LangGraph provide robust platforms for developing and implementing multi-agent systems tailored to diverse applications. Additionally, open-source projects like <a href=\"#chatdev-communicative-agents-for-software-development\">ChatDev</a> allow developers to experiment with multi-agent setups in a virtual “software company” environment, offering valuable insights into the collaborative potential of AI agents. Such tools represent the leading edge of multi-agent technology, providing a foundation for AI-driven task decomposition and collaboration.</p>\n<p>In summary, multi-agent collaboration is a compelling and effective AI design pattern that leverages agent specialization, task decomposition, and focused prompting to enable more efficient handling of complex tasks. As multi-agent frameworks continue to advance, they are likely to become foundational in AI-driven workflows, providing developers with both the structure and flexibility to tackle increasingly sophisticated projects.</p>",
    "contentMarkdown": "*   While managing human teams has inherent challenges, applying similar organizational strategies to multi-agent AI systems is not only manageable but also offers low-risk flexibility; any issues in an AI agent’s performance are easily rectified. Emerging frameworks such as AutoGen, CrewAI, and LangGraph provide robust platforms for developing and implementing multi-agent systems tailored to diverse applications. Additionally, open-source projects like [ChatDev](#chatdev-communicative-agents-for-software-development) allow developers to experiment with multi-agent setups in a virtual “software company” environment, offering valuable insights into the collaborative potential of AI agents. Such tools represent the leading edge of multi-agent technology, providing a foundation for AI-driven task decomposition and collaboration.\n    \n*   In summary, multi-agent collaboration is a compelling and effective AI design pattern that leverages agent specialization, task decomposition, and focused prompting to enable more efficient handling of complex tasks. As multi-agent frameworks continue to advance, they are likely to become foundational in AI-driven workflows, providing developers with both the structure and flexibility to tackle increasingly sophisticated projects.\n    \n\nWhile managing human teams has inherent challenges, applying similar organizational strategies to multi-agent AI systems is not only manageable but also offers low-risk flexibility; any issues in an AI agent’s performance are easily rectified. Emerging frameworks such as AutoGen, CrewAI, and LangGraph provide robust platforms for developing and implementing multi-agent systems tailored to diverse applications. Additionally, open-source projects like [ChatDev](#chatdev-communicative-agents-for-software-development) allow developers to experiment with multi-agent setups in a virtual “software company” environment, offering valuable insights into the collaborative potential of AI agents. Such tools represent the leading edge of multi-agent technology, providing a foundation for AI-driven task decomposition and collaboration.\n\nIn summary, multi-agent collaboration is a compelling and effective AI design pattern that leverages agent specialization, task decomposition, and focused prompting to enable more efficient handling of complex tasks. As multi-agent frameworks continue to advance, they are likely to become foundational in AI-driven workflows, providing developers with both the structure and flexibility to tackle increasingly sophisticated projects.",
    "order": 9,
    "orderInChapter": 5,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 318,
      "contentLength": 2636
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#implementation",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-prompt-chaining-10",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Workflow Patterns",
    "title": "Prompt Chaining",
    "subtitle": "Agentic Workflow Patterns",
    "contentHtml": "<ul>\n  <li>Prompt chaining involves decomposing a task into a sequence of steps, where each LLM call processes the output of the preceding step. Programmatic validation mechanisms (referred to as “gates”) may be applied at intermediate stages to ensure procedural accuracy.</li>\n  <li><strong>Optimal Use Cases:</strong>\n    <ul>\n      <li>This workflow is beneficial when a task can be clearly divided into structured subtasks. It prioritizes improved accuracy over latency by simplifying each LLM call.</li>\n    </ul>\n  </li>\n  <li><strong>Examples:</strong>\n    <ul>\n      <li>Generating marketing copy and subsequently translating it into another language.</li>\n      <li>Creating an outline for a document, verifying its adherence to specific criteria, and then generating the final document based on the outline.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>This workflow is beneficial when a task can be clearly divided into structured subtasks. It prioritizes improved accuracy over latency by simplifying each LLM call.</li>\n    </ul>\n<ul>\n      <li>Generating marketing copy and subsequently translating it into another language.</li>\n      <li>Creating an outline for a document, verifying its adherence to specific criteria, and then generating the final document based on the outline.</li>\n    </ul>",
    "contentMarkdown": "*   Prompt chaining involves decomposing a task into a sequence of steps, where each LLM call processes the output of the preceding step. Programmatic validation mechanisms (referred to as “gates”) may be applied at intermediate stages to ensure procedural accuracy.\n*   **Optimal Use Cases:**\n    *   This workflow is beneficial when a task can be clearly divided into structured subtasks. It prioritizes improved accuracy over latency by simplifying each LLM call.\n*   **Examples:**\n    *   Generating marketing copy and subsequently translating it into another language.\n    *   Creating an outline for a document, verifying its adherence to specific criteria, and then generating the final document based on the outline.\n\n*   This workflow is beneficial when a task can be clearly divided into structured subtasks. It prioritizes improved accuracy over latency by simplifying each LLM call.\n\n*   Generating marketing copy and subsequently translating it into another language.\n*   Creating an outline for a document, verifying its adherence to specific criteria, and then generating the final document based on the outline.",
    "order": 10,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 166,
      "contentLength": 1311
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#prompt-chaining",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-routing-11",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Workflow Patterns",
    "title": "Routing",
    "subtitle": "Agentic Workflow Patterns",
    "contentHtml": "<ul>\n  <li>\n    <p>Routing involves classifying an input and directing it to an appropriate specialized task. This approach facilitates the separation of concerns and enables more precise prompts. Without routing, optimizing for one input type may degrade performance for others.</p>\n  </li>\n  <li><strong>Optimal Use Cases:</strong>\n    <ul>\n      <li>Routing is effective for tasks with distinct categories that require specialized handling. Classification can be performed by either an LLM or a traditional classification model.</li>\n    </ul>\n  </li>\n  <li><strong>Examples:</strong>\n    <ul>\n      <li>Categorizing customer service queries (e.g., general inquiries, refund requests, technical support) and routing them to appropriate downstream processes.</li>\n      <li>Assigning simple queries to lightweight models (e.g., Claude 3.5 Haiku) while directing complex queries to more capable models (e.g., Claude 3.5 Sonnet) to balance efficiency and cost.</li>\n    </ul>\n  </li>\n</ul>\n<p>Routing involves classifying an input and directing it to an appropriate specialized task. This approach facilitates the separation of concerns and enables more precise prompts. Without routing, optimizing for one input type may degrade performance for others.</p>\n<ul>\n      <li>Routing is effective for tasks with distinct categories that require specialized handling. Classification can be performed by either an LLM or a traditional classification model.</li>\n    </ul>\n<ul>\n      <li>Categorizing customer service queries (e.g., general inquiries, refund requests, technical support) and routing them to appropriate downstream processes.</li>\n      <li>Assigning simple queries to lightweight models (e.g., Claude 3.5 Haiku) while directing complex queries to more capable models (e.g., Claude 3.5 Sonnet) to balance efficiency and cost.</li>\n    </ul>\n<h4 id=\"parallelization\">Parallelization</h4>\n<ul>\n  <li>Parallelization entails multiple LLM instances working on a task simultaneously, with results aggregated programmatically. It manifests in two key forms:\n    <ul>\n      <li><strong>Sectioning:</strong> Dividing a task into independent subtasks that can be processed in parallel.</li>\n      <li><strong>Voting:</strong> Running the same task multiple times to obtain diverse outputs.</li>\n    </ul>\n  </li>\n  <li><strong>Optimal Use Cases:</strong>\n    <ul>\n      <li>Parallelization is advantageous when tasks can be effectively subdivided to enhance speed or when multiple perspectives improve reliability.</li>\n    </ul>\n  </li>\n  <li><strong>Examples:</strong>\n    <ul>\n      <li><strong>Sectioning:</strong>\n        <ul>\n          <li>Implementing safeguards where one LLM processes user queries while another screens for inappropriate content.</li>\n          <li>Automating model performance evaluations by assigning different evaluation criteria to separate LLM calls.</li>\n        </ul>\n      </li>\n      <li><strong>Voting:</strong>\n        <ul>\n          <li>Conducting code reviews for security vulnerabilities, with multiple prompts assessing different aspects of the code.</li>\n          <li>Evaluating content for appropriateness by using multiple assessments to balance false positives and negatives.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Sectioning:</strong> Dividing a task into independent subtasks that can be processed in parallel.</li>\n      <li><strong>Voting:</strong> Running the same task multiple times to obtain diverse outputs.</li>\n    </ul>\n<ul>\n      <li>Parallelization is advantageous when tasks can be effectively subdivided to enhance speed or when multiple perspectives improve reliability.</li>\n    </ul>\n<ul>\n      <li><strong>Sectioning:</strong>\n        <ul>\n          <li>Implementing safeguards where one LLM processes user queries while another screens for inappropriate content.</li>\n          <li>Automating model performance evaluations by assigning different evaluation criteria to separate LLM calls.</li>\n        </ul>\n      </li>\n      <li><strong>Voting:</strong>\n        <ul>\n          <li>Conducting code reviews for security vulnerabilities, with multiple prompts assessing different aspects of the code.</li>\n          <li>Evaluating content for appropriateness by using multiple assessments to balance false positives and negatives.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Implementing safeguards where one LLM processes user queries while another screens for inappropriate content.</li>\n          <li>Automating model performance evaluations by assigning different evaluation criteria to separate LLM calls.</li>\n        </ul>\n<ul>\n          <li>Conducting code reviews for security vulnerabilities, with multiple prompts assessing different aspects of the code.</li>\n          <li>Evaluating content for appropriateness by using multiple assessments to balance false positives and negatives.</li>\n        </ul>\n<h4 id=\"orchestrator-workers\">Orchestrator-Workers</h4>\n<ul>\n  <li>\n    <p>This workflow features a central LLM that dynamically decomposes tasks, assigns them to worker LLMs, and synthesizes their results.</p>\n  </li>\n  <li><strong>Optimal Use Cases:</strong>\n    <ul>\n      <li>This approach is suited for complex tasks where the required subtasks are not predefined but must be determined dynamically. Unlike parallelization, which follows a fixed structure, this workflow offers greater adaptability.</li>\n    </ul>\n  </li>\n  <li><strong>Examples:</strong>\n    <ul>\n      <li>Software development tools that implement complex modifications across multiple files.</li>\n      <li>Research tasks requiring information retrieval and synthesis from diverse sources.</li>\n    </ul>\n  </li>\n</ul>\n<p>This workflow features a central LLM that dynamically decomposes tasks, assigns them to worker LLMs, and synthesizes their results.</p>\n<ul>\n      <li>This approach is suited for complex tasks where the required subtasks are not predefined but must be determined dynamically. Unlike parallelization, which follows a fixed structure, this workflow offers greater adaptability.</li>\n    </ul>\n<ul>\n      <li>Software development tools that implement complex modifications across multiple files.</li>\n      <li>Research tasks requiring information retrieval and synthesis from diverse sources.</li>\n    </ul>\n<h4 id=\"evaluator-optimizer\">Evaluator-Optimizer</h4>\n<ul>\n  <li>\n    <p>In this iterative workflow, one LLM generates responses while another evaluates and refines them in a continuous loop.</p>\n  </li>\n  <li><strong>Optimal Use Cases:</strong>\n    <ul>\n      <li>This pattern is particularly effective when clear evaluation criteria exist and iterative refinement adds measurable value. It is most beneficial when:\n        <ul>\n          <li>Human feedback has been shown to enhance LLM-generated outputs.</li>\n          <li>The LLM itself is capable of providing constructive feedback.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Examples:</strong>\n    <ul>\n      <li>Literary translation, where an evaluator LLM can provide nuanced critiques that improve translation quality.</li>\n      <li>Complex search tasks requiring iterative refinement, where an evaluator assesses whether additional searches are necessary to obtain comprehensive information.</li>\n    </ul>\n  </li>\n</ul>\n<p>In this iterative workflow, one LLM generates responses while another evaluates and refines them in a continuous loop.</p>\n<ul>\n      <li>This pattern is particularly effective when clear evaluation criteria exist and iterative refinement adds measurable value. It is most beneficial when:\n        <ul>\n          <li>Human feedback has been shown to enhance LLM-generated outputs.</li>\n          <li>The LLM itself is capable of providing constructive feedback.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Human feedback has been shown to enhance LLM-generated outputs.</li>\n          <li>The LLM itself is capable of providing constructive feedback.</li>\n        </ul>\n<ul>\n      <li>Literary translation, where an evaluator LLM can provide nuanced critiques that improve translation quality.</li>\n      <li>Complex search tasks requiring iterative refinement, where an evaluator assesses whether additional searches are necessary to obtain comprehensive information.</li>\n    </ul>",
    "contentMarkdown": "*   Routing involves classifying an input and directing it to an appropriate specialized task. This approach facilitates the separation of concerns and enables more precise prompts. Without routing, optimizing for one input type may degrade performance for others.\n    \n*   **Optimal Use Cases:**\n    *   Routing is effective for tasks with distinct categories that require specialized handling. Classification can be performed by either an LLM or a traditional classification model.\n*   **Examples:**\n    *   Categorizing customer service queries (e.g., general inquiries, refund requests, technical support) and routing them to appropriate downstream processes.\n    *   Assigning simple queries to lightweight models (e.g., Claude 3.5 Haiku) while directing complex queries to more capable models (e.g., Claude 3.5 Sonnet) to balance efficiency and cost.\n\nRouting involves classifying an input and directing it to an appropriate specialized task. This approach facilitates the separation of concerns and enables more precise prompts. Without routing, optimizing for one input type may degrade performance for others.\n\n*   Routing is effective for tasks with distinct categories that require specialized handling. Classification can be performed by either an LLM or a traditional classification model.\n\n*   Categorizing customer service queries (e.g., general inquiries, refund requests, technical support) and routing them to appropriate downstream processes.\n*   Assigning simple queries to lightweight models (e.g., Claude 3.5 Haiku) while directing complex queries to more capable models (e.g., Claude 3.5 Sonnet) to balance efficiency and cost.\n\n#### Parallelization\n\n*   Parallelization entails multiple LLM instances working on a task simultaneously, with results aggregated programmatically. It manifests in two key forms:\n    *   **Sectioning:** Dividing a task into independent subtasks that can be processed in parallel.\n    *   **Voting:** Running the same task multiple times to obtain diverse outputs.\n*   **Optimal Use Cases:**\n    *   Parallelization is advantageous when tasks can be effectively subdivided to enhance speed or when multiple perspectives improve reliability.\n*   **Examples:**\n    *   **Sectioning:**\n        *   Implementing safeguards where one LLM processes user queries while another screens for inappropriate content.\n        *   Automating model performance evaluations by assigning different evaluation criteria to separate LLM calls.\n    *   **Voting:**\n        *   Conducting code reviews for security vulnerabilities, with multiple prompts assessing different aspects of the code.\n        *   Evaluating content for appropriateness by using multiple assessments to balance false positives and negatives.\n\n*   **Sectioning:** Dividing a task into independent subtasks that can be processed in parallel.\n*   **Voting:** Running the same task multiple times to obtain diverse outputs.\n\n*   Parallelization is advantageous when tasks can be effectively subdivided to enhance speed or when multiple perspectives improve reliability.\n\n*   **Sectioning:**\n    *   Implementing safeguards where one LLM processes user queries while another screens for inappropriate content.\n    *   Automating model performance evaluations by assigning different evaluation criteria to separate LLM calls.\n*   **Voting:**\n    *   Conducting code reviews for security vulnerabilities, with multiple prompts assessing different aspects of the code.\n    *   Evaluating content for appropriateness by using multiple assessments to balance false positives and negatives.\n\n*   Implementing safeguards where one LLM processes user queries while another screens for inappropriate content.\n*   Automating model performance evaluations by assigning different evaluation criteria to separate LLM calls.\n\n*   Conducting code reviews for security vulnerabilities, with multiple prompts assessing different aspects of the code.\n*   Evaluating content for appropriateness by using multiple assessments to balance false positives and negatives.\n\n#### Orchestrator-Workers\n\n*   This workflow features a central LLM that dynamically decomposes tasks, assigns them to worker LLMs, and synthesizes their results.\n    \n*   **Optimal Use Cases:**\n    *   This approach is suited for complex tasks where the required subtasks are not predefined but must be determined dynamically. Unlike parallelization, which follows a fixed structure, this workflow offers greater adaptability.\n*   **Examples:**\n    *   Software development tools that implement complex modifications across multiple files.\n    *   Research tasks requiring information retrieval and synthesis from diverse sources.\n\nThis workflow features a central LLM that dynamically decomposes tasks, assigns them to worker LLMs, and synthesizes their results.\n\n*   This approach is suited for complex tasks where the required subtasks are not predefined but must be determined dynamically. Unlike parallelization, which follows a fixed structure, this workflow offers greater adaptability.\n\n*   Software development tools that implement complex modifications across multiple files.\n*   Research tasks requiring information retrieval and synthesis from diverse sources.\n\n#### Evaluator-Optimizer\n\n*   In this iterative workflow, one LLM generates responses while another evaluates and refines them in a continuous loop.\n    \n*   **Optimal Use Cases:**\n    *   This pattern is particularly effective when clear evaluation criteria exist and iterative refinement adds measurable value. It is most beneficial when:\n        *   Human feedback has been shown to enhance LLM-generated outputs.\n        *   The LLM itself is capable of providing constructive feedback.\n*   **Examples:**\n    *   Literary translation, where an evaluator LLM can provide nuanced critiques that improve translation quality.\n    *   Complex search tasks requiring iterative refinement, where an evaluator assesses whether additional searches are necessary to obtain comprehensive information.\n\nIn this iterative workflow, one LLM generates responses while another evaluates and refines them in a continuous loop.\n\n*   This pattern is particularly effective when clear evaluation criteria exist and iterative refinement adds measurable value. It is most beneficial when:\n    *   Human feedback has been shown to enhance LLM-generated outputs.\n    *   The LLM itself is capable of providing constructive feedback.\n\n*   Human feedback has been shown to enhance LLM-generated outputs.\n*   The LLM itself is capable of providing constructive feedback.\n\n*   Literary translation, where an evaluator LLM can provide nuanced critiques that improve translation quality.\n*   Complex search tasks requiring iterative refinement, where an evaluator assesses whether additional searches are necessary to obtain comprehensive information.",
    "order": 11,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 5,
    "tags": [
      "algorithmsarchitecture",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 908,
      "contentLength": 8344
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#routing",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-preservation-of-context-12",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Single-Agent Systems vs. Multi-Agent Systems",
    "title": "Preservation of Context",
    "subtitle": "Single-Agent Systems vs. Multi-Agent Systems",
    "contentHtml": "<ul>\n  <li>While multi-agent systems offer structural advantages in dividing and distributing tasks, there are compelling reasons why single-agent systems are often the preferred choice in particular contexts. These advantages stem largely from considerations of system coherence, efficiency, and maintainability.</li>\n  <li>First, single-agent systems mitigate the risk of context fragmentation. In multi-agent setups, information must be passed between distinct agents, each with its own prompt and action space. This process can lead to significant information loss or misinterpretation, especially when intermediate outputs are condensed or abstracted. In contrast, a single-agent system maintains a unified internal context throughout its operation, thereby preserving the continuity and depth of understanding necessary for complex reasoning or nuanced decision-making.</li>\n</ul>",
    "contentMarkdown": "*   While multi-agent systems offer structural advantages in dividing and distributing tasks, there are compelling reasons why single-agent systems are often the preferred choice in particular contexts. These advantages stem largely from considerations of system coherence, efficiency, and maintainability.\n*   First, single-agent systems mitigate the risk of context fragmentation. In multi-agent setups, information must be passed between distinct agents, each with its own prompt and action space. This process can lead to significant information loss or misinterpretation, especially when intermediate outputs are condensed or abstracted. In contrast, a single-agent system maintains a unified internal context throughout its operation, thereby preserving the continuity and depth of understanding necessary for complex reasoning or nuanced decision-making.",
    "order": 12,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 113,
      "contentLength": 886
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#preservation-of-context",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-simplicity-and-maintainability-13",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Single-Agent Systems vs. Multi-Agent Systems",
    "title": "Simplicity and Maintainability",
    "subtitle": "Single-Agent Systems vs. Multi-Agent Systems",
    "contentHtml": "<ul>\n  <li>System design and maintenance are notably simplified in single-agent architectures. Each additional agent in a multi-agent framework typically requires a separate codebase or configuration, increasing the burden of integration and long-term upkeep. A single-agent system, by consolidating all functionality within a unified framework, allows for a leaner development cycle and reduces the cognitive load for developers maintaining the system.</li>\n</ul>",
    "contentMarkdown": "*   System design and maintenance are notably simplified in single-agent architectures. Each additional agent in a multi-agent framework typically requires a separate codebase or configuration, increasing the burden of integration and long-term upkeep. A single-agent system, by consolidating all functionality within a unified framework, allows for a leaner development cycle and reduces the cognitive load for developers maintaining the system.",
    "order": 13,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 60,
      "contentLength": 464
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#simplicity-and-maintainability",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-flexibility-in-problem-solving-14",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Single-Agent Systems vs. Multi-Agent Systems",
    "title": "Flexibility in Problem Solving",
    "subtitle": "Single-Agent Systems vs. Multi-Agent Systems",
    "contentHtml": "<ul>\n  <li>Single-agent systems offer enhanced flexibility in problem-solving. Unlike multi-agent systems, where each agent is constrained by a specialized role and toolset, a well-configured single agent can dynamically apply a broad set of tools and methods as needed. This adaptability is particularly valuable when real-world tasks deviate from predefined structures or require interleaving of capabilities that would otherwise be siloed across agents.</li>\n</ul>",
    "contentMarkdown": "*   Single-agent systems offer enhanced flexibility in problem-solving. Unlike multi-agent systems, where each agent is constrained by a specialized role and toolset, a well-configured single agent can dynamically apply a broad set of tools and methods as needed. This adaptability is particularly valuable when real-world tasks deviate from predefined structures or require interleaving of capabilities that would otherwise be siloed across agents.",
    "order": 14,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 62,
      "contentLength": 467
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#flexibility-in-problem-solving",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-feasibility-with-modern-tools-15",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Single-Agent Systems vs. Multi-Agent Systems",
    "title": "Feasibility with Modern Tools",
    "subtitle": "Single-Agent Systems vs. Multi-Agent Systems",
    "contentHtml": "<ul>\n  <li>Advances in LLMs and prompt engineering have made it increasingly feasible to replicate the effectiveness of multi-agent coordination within a single-agent framework. With access to long-context models and robust prompting techniques, a single agent can internalize and execute complex workflows that previously necessitated distributed architectures.</li>\n</ul>",
    "contentMarkdown": "*   Advances in LLMs and prompt engineering have made it increasingly feasible to replicate the effectiveness of multi-agent coordination within a single-agent framework. With access to long-context models and robust prompting techniques, a single agent can internalize and execute complex workflows that previously necessitated distributed architectures.",
    "order": 15,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 46,
      "contentLength": 373
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#feasibility-with-modern-tools",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-overview-16",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Model Context Protocol (MCP)",
    "title": "Overview",
    "subtitle": "Model Context Protocol (MCP)",
    "contentHtml": "<ul>\n  <li>The <a href=\"https://modelcontextprotocol.io\">Model Context Protocol (MCP)</a> is an open protocol designed to standardize how applications provide context to LLMs. Think of MCP as the AI equivalent of a USB-C port: just as USB-C creates a standardized way for devices to connect with various peripherals, MCP provides a uniform approach to integrating AI models with different data sources and tools. The following figure (<a href=\"https://norahsakal.com/blog/mcp-vs-api-model-context-protocol-explained/\">source</a>) illustrates MCP’s client-server architecture: A standardized bridge connecting AI applications with local and remote data sources akin to how a USB-C port enables seamless connectivity between multiple peripherals without requiring separate adapters.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/agents/mcp_usbc.png\" alt=\"Placeholder for The Model Context Protocol (MCP) Image\"></p>\n<ul>\n  <li>\n    <p>MCP has rapidly become the de facto standard for integrating LLMs with external tools and data sources. Its AI-native approach, strong industry backing, and pragmatic design choices have positioned it as the most viable open protocol for agent-based applications. As adoption continues to grow, MCP is likely to overtake existing standards like OpenAPI, solidifying its role in the future of AI-powered workflows.</p>\n  </li>\n  <li>\n    <p>The following figure (<a href=\"https://www.philschmid.de/mcp-introduction\">source</a>) contrasts LLM tool integrations before and after MCP, highlighting how MCP simplifies interactions through a unified API layer.</p>\n  </li>\n</ul>\n<p>MCP has rapidly become the de facto standard for integrating LLMs with external tools and data sources. Its AI-native approach, strong industry backing, and pragmatic design choices have positioned it as the most viable open protocol for agent-based applications. As adoption continues to grow, MCP is likely to overtake existing standards like OpenAPI, solidifying its role in the future of AI-powered workflows.</p>\n<p>The following figure (<a href=\"https://www.philschmid.de/mcp-introduction\">source</a>) contrasts LLM tool integrations before and after MCP, highlighting how MCP simplifies interactions through a unified API layer.</p>\n<p><img src=\"/primers/ai/assets/agents/mcp_before_after.jpg\" alt=\"Placeholder for Before and After MCP Image\"></p>",
    "contentMarkdown": "*   The [Model Context Protocol (MCP)](https://modelcontextprotocol.io) is an open protocol designed to standardize how applications provide context to LLMs. Think of MCP as the AI equivalent of a USB-C port: just as USB-C creates a standardized way for devices to connect with various peripherals, MCP provides a uniform approach to integrating AI models with different data sources and tools. The following figure ([source](https://norahsakal.com/blog/mcp-vs-api-model-context-protocol-explained/)) illustrates MCP’s client-server architecture: A standardized bridge connecting AI applications with local and remote data sources akin to how a USB-C port enables seamless connectivity between multiple peripherals without requiring separate adapters.\n\n![Placeholder for The Model Context Protocol (MCP) Image](/primers/ai/assets/agents/mcp_usbc.png)\n\n*   MCP has rapidly become the de facto standard for integrating LLMs with external tools and data sources. Its AI-native approach, strong industry backing, and pragmatic design choices have positioned it as the most viable open protocol for agent-based applications. As adoption continues to grow, MCP is likely to overtake existing standards like OpenAPI, solidifying its role in the future of AI-powered workflows.\n    \n*   The following figure ([source](https://www.philschmid.de/mcp-introduction)) contrasts LLM tool integrations before and after MCP, highlighting how MCP simplifies interactions through a unified API layer.\n    \n\nMCP has rapidly become the de facto standard for integrating LLMs with external tools and data sources. Its AI-native approach, strong industry backing, and pragmatic design choices have positioned it as the most viable open protocol for agent-based applications. As adoption continues to grow, MCP is likely to overtake existing standards like OpenAPI, solidifying its role in the future of AI-powered workflows.\n\nThe following figure ([source](https://www.philschmid.de/mcp-introduction)) contrasts LLM tool integrations before and after MCP, highlighting how MCP simplifies interactions through a unified API layer.\n\n![Placeholder for Before and After MCP Image](/primers/ai/assets/agents/mcp_before_after.jpg)",
    "order": 16,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 280,
      "contentLength": 2360
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#overview",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-why-mcp-17",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Model Context Protocol (MCP)",
    "title": "Why MCP?",
    "subtitle": "Model Context Protocol (MCP)",
    "contentHtml": "<ul>\n  <li>MCP is essential for building AI-powered agents and complex workflows. LLMs often need access to external data and tools, and MCP facilitates this by offering:\n    <ul>\n      <li>A growing library of pre-built integrations that LLMs can seamlessly interact with.</li>\n      <li>Flexibility to switch between different LLM providers and vendors without reconfiguring integrations.</li>\n      <li>Best practices for securing data while keeping it within a controlled infrastructure.</li>\n    </ul>\n  </li>\n  <li>Beyond these practical advantages, MCP has also emerged as the dominant open standard for AI agent integrations due to several key factors:\n    <ul>\n      <li><strong>MCP is an AI-Native adaptation of an existing idea:</strong> Unlike older interoperability standards such as OpenAPI, OData, and GraphQL, MCP was built specifically for LLM-based applications, making it more effective in AI-driven workflows.</li>\n      <li><strong>MCP is backed by a major player (Anthropic):</strong> Large organizations supporting a protocol significantly boost its chances of success, and Anthropic’s involvement has given MCP a strong foundation.</li>\n      <li><strong>Anthropic has a strong developer AI brand:</strong> The company’s focus on high-quality developer tooling has positioned MCP as the leading choice among AI engineers and application developers.</li>\n      <li><strong>MCP is built on the <a href=\"https://learn.microsoft.com/en-us/visualstudio/extensibility/language-server-protocol?view=vs-2022\">Language Server Protocol (LSP)</a>:</strong> By leveraging an already successful and widely adopted architecture, MCP avoids the pitfalls of entirely new protocol designs and benefits from proven best practices.</li>\n      <li><strong>MCP was dogfooded with complete tooling:</strong> From its launch, MCP was tested and refined with first-party clients like Claude Desktop, 19 reference server implementations, and essential development tools such as MCP Inspector and SDKs in Python and TypeScript.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>A growing library of pre-built integrations that LLMs can seamlessly interact with.</li>\n      <li>Flexibility to switch between different LLM providers and vendors without reconfiguring integrations.</li>\n      <li>Best practices for securing data while keeping it within a controlled infrastructure.</li>\n    </ul>\n<ul>\n      <li><strong>MCP is an AI-Native adaptation of an existing idea:</strong> Unlike older interoperability standards such as OpenAPI, OData, and GraphQL, MCP was built specifically for LLM-based applications, making it more effective in AI-driven workflows.</li>\n      <li><strong>MCP is backed by a major player (Anthropic):</strong> Large organizations supporting a protocol significantly boost its chances of success, and Anthropic’s involvement has given MCP a strong foundation.</li>\n      <li><strong>Anthropic has a strong developer AI brand:</strong> The company’s focus on high-quality developer tooling has positioned MCP as the leading choice among AI engineers and application developers.</li>\n      <li><strong>MCP is built on the <a href=\"https://learn.microsoft.com/en-us/visualstudio/extensibility/language-server-protocol?view=vs-2022\">Language Server Protocol (LSP)</a>:</strong> By leveraging an already successful and widely adopted architecture, MCP avoids the pitfalls of entirely new protocol designs and benefits from proven best practices.</li>\n      <li><strong>MCP was dogfooded with complete tooling:</strong> From its launch, MCP was tested and refined with first-party clients like Claude Desktop, 19 reference server implementations, and essential development tools such as MCP Inspector and SDKs in Python and TypeScript.</li>\n    </ul>",
    "contentMarkdown": "*   MCP is essential for building AI-powered agents and complex workflows. LLMs often need access to external data and tools, and MCP facilitates this by offering:\n    *   A growing library of pre-built integrations that LLMs can seamlessly interact with.\n    *   Flexibility to switch between different LLM providers and vendors without reconfiguring integrations.\n    *   Best practices for securing data while keeping it within a controlled infrastructure.\n*   Beyond these practical advantages, MCP has also emerged as the dominant open standard for AI agent integrations due to several key factors:\n    *   **MCP is an AI-Native adaptation of an existing idea:** Unlike older interoperability standards such as OpenAPI, OData, and GraphQL, MCP was built specifically for LLM-based applications, making it more effective in AI-driven workflows.\n    *   **MCP is backed by a major player (Anthropic):** Large organizations supporting a protocol significantly boost its chances of success, and Anthropic’s involvement has given MCP a strong foundation.\n    *   **Anthropic has a strong developer AI brand:** The company’s focus on high-quality developer tooling has positioned MCP as the leading choice among AI engineers and application developers.\n    *   **MCP is built on the [Language Server Protocol (LSP)](https://learn.microsoft.com/en-us/visualstudio/extensibility/language-server-protocol?view=vs-2022):** By leveraging an already successful and widely adopted architecture, MCP avoids the pitfalls of entirely new protocol designs and benefits from proven best practices.\n    *   **MCP was dogfooded with complete tooling:** From its launch, MCP was tested and refined with first-party clients like Claude Desktop, 19 reference server implementations, and essential development tools such as MCP Inspector and SDKs in Python and TypeScript.\n\n*   A growing library of pre-built integrations that LLMs can seamlessly interact with.\n*   Flexibility to switch between different LLM providers and vendors without reconfiguring integrations.\n*   Best practices for securing data while keeping it within a controlled infrastructure.\n\n*   **MCP is an AI-Native adaptation of an existing idea:** Unlike older interoperability standards such as OpenAPI, OData, and GraphQL, MCP was built specifically for LLM-based applications, making it more effective in AI-driven workflows.\n*   **MCP is backed by a major player (Anthropic):** Large organizations supporting a protocol significantly boost its chances of success, and Anthropic’s involvement has given MCP a strong foundation.\n*   **Anthropic has a strong developer AI brand:** The company’s focus on high-quality developer tooling has positioned MCP as the leading choice among AI engineers and application developers.\n*   **MCP is built on the [Language Server Protocol (LSP)](https://learn.microsoft.com/en-us/visualstudio/extensibility/language-server-protocol?view=vs-2022):** By leveraging an already successful and widely adopted architecture, MCP avoids the pitfalls of entirely new protocol designs and benefits from proven best practices.\n*   **MCP was dogfooded with complete tooling:** From its launch, MCP was tested and refined with first-party clients like Claude Desktop, 19 reference server implementations, and essential development tools such as MCP Inspector and SDKs in Python and TypeScript.",
    "order": 17,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 3,
    "tags": [
      "algorithmsarchitecture",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 455,
      "contentLength": 3762
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#why-mcp?",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-general-architecture-18",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Model Context Protocol (MCP)",
    "title": "General Architecture",
    "subtitle": "Model Context Protocol (MCP)",
    "contentHtml": "<ul>\n  <li>At its core, MCP follows a client-server architecture, enabling a host application to connect to multiple MCP servers. Below is an overview of the key components:\n    <ul>\n      <li><strong>MCP Hosts:</strong> Programs such as Claude Desktop, IDEs, and AI tools that need to access data through MCP.</li>\n      <li><strong>MCP Clients:</strong> Protocol clients that maintain one-to-one connections with MCP servers.</li>\n      <li><strong>MCP Servers:</strong> Lightweight programs that expose specific capabilities using the standardized MCP framework.</li>\n      <li><strong>Local Data Sources:</strong> Files, databases, and services on a user’s computer that MCP servers can securely access.</li>\n      <li><strong>Remote Services:</strong> External systems, such as APIs, that MCP servers can connect to over the internet.</li>\n    </ul>\n  </li>\n  <li><strong>Client Example Using Python:</strong> The following code snippet (<a href=\"https://www.philschmid.de/mcp-introduction\">source</a>) shows how to create a basic MCP client that initializes a session, lists available tools, and calls one of the server’s tools using standard I/O transport.</li>\n</ul>\n<ul>\n      <li><strong>MCP Hosts:</strong> Programs such as Claude Desktop, IDEs, and AI tools that need to access data through MCP.</li>\n      <li><strong>MCP Clients:</strong> Protocol clients that maintain one-to-one connections with MCP servers.</li>\n      <li><strong>MCP Servers:</strong> Lightweight programs that expose specific capabilities using the standardized MCP framework.</li>\n      <li><strong>Local Data Sources:</strong> Files, databases, and services on a user’s computer that MCP servers can securely access.</li>\n      <li><strong>Remote Services:</strong> External systems, such as APIs, that MCP servers can connect to over the internet.</li>\n    </ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code8\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code8\"><span class=\"kn\">from</span> <span class=\"nn\">mcp</span> <span class=\"kn\">import</span> <span class=\"n\">ClientSession</span><span class=\"p\">,</span> <span class=\"n\">StdioServerParameters</span>\n<span class=\"kn\">from</span> <span class=\"nn\">mcp.client.stdio</span> <span class=\"kn\">import</span> <span class=\"n\">stdio_client</span>\n\n<span class=\"n\">server_params</span> <span class=\"o\">=</span> <span class=\"n\">StdioServerParameters</span><span class=\"p\">(</span>\n    <span class=\"n\">command</span><span class=\"o\">=</span><span class=\"s\">\"python\"</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">\"example_server.py\"</span><span class=\"p\">]</span>\n<span class=\"p\">)</span>\n\n<span class=\"k\">async</span> <span class=\"k\">with</span> <span class=\"n\">stdio_client</span><span class=\"p\">(</span><span class=\"n\">server_params</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"p\">(</span><span class=\"n\">read</span><span class=\"p\">,</span> <span class=\"n\">write</span><span class=\"p\">):</span>\n    <span class=\"k\">async</span> <span class=\"k\">with</span> <span class=\"n\">ClientSession</span><span class=\"p\">(</span><span class=\"n\">read</span><span class=\"p\">,</span> <span class=\"n\">write</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">session</span><span class=\"p\">:</span>\n        <span class=\"k\">await</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">initialize</span><span class=\"p\">()</span>\n        <span class=\"n\">tools</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">list_tools</span><span class=\"p\">()</span>\n        <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">call_tool</span><span class=\"p\">(</span><span class=\"s\">\"tool-name\"</span><span class=\"p\">,</span> <span class=\"n\">arguments</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">\"arg1\"</span><span class=\"p\">:</span> <span class=\"s\">\"value\"</span><span class=\"p\">})</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code8\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code8\"><span class=\"kn\">from</span> <span class=\"nn\">mcp</span> <span class=\"kn\">import</span> <span class=\"n\">ClientSession</span><span class=\"p\">,</span> <span class=\"n\">StdioServerParameters</span>\n<span class=\"kn\">from</span> <span class=\"nn\">mcp.client.stdio</span> <span class=\"kn\">import</span> <span class=\"n\">stdio_client</span>\n\n<span class=\"n\">server_params</span> <span class=\"o\">=</span> <span class=\"n\">StdioServerParameters</span><span class=\"p\">(</span>\n    <span class=\"n\">command</span><span class=\"o\">=</span><span class=\"s\">\"python\"</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">\"example_server.py\"</span><span class=\"p\">]</span>\n<span class=\"p\">)</span>\n\n<span class=\"k\">async</span> <span class=\"k\">with</span> <span class=\"n\">stdio_client</span><span class=\"p\">(</span><span class=\"n\">server_params</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"p\">(</span><span class=\"n\">read</span><span class=\"p\">,</span> <span class=\"n\">write</span><span class=\"p\">):</span>\n    <span class=\"k\">async</span> <span class=\"k\">with</span> <span class=\"n\">ClientSession</span><span class=\"p\">(</span><span class=\"n\">read</span><span class=\"p\">,</span> <span class=\"n\">write</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">session</span><span class=\"p\">:</span>\n        <span class=\"k\">await</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">initialize</span><span class=\"p\">()</span>\n        <span class=\"n\">tools</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">list_tools</span><span class=\"p\">()</span>\n        <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">call_tool</span><span class=\"p\">(</span><span class=\"s\">\"tool-name\"</span><span class=\"p\">,</span> <span class=\"n\">arguments</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">\"arg1\"</span><span class=\"p\">:</span> <span class=\"s\">\"value\"</span><span class=\"p\">})</span>\n</code></pre>\n<ul>\n  <li><strong>Server Example Using FastMCP (Python):</strong> The following code snippet (<a href=\"https://www.philschmid.de/mcp-introduction\">source</a>) illustrates how to create a simple MCP server using FastMCP, which exposes a tool, a data resource, and a reusable prompt template.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code9\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code9\"><span class=\"kn\">from</span> <span class=\"nn\">fastmcp</span> <span class=\"kn\">import</span> <span class=\"n\">FastMCP</span>\n\n<span class=\"n\">mcp</span> <span class=\"o\">=</span> <span class=\"n\">FastMCP</span><span class=\"p\">(</span><span class=\"s\">\"Demo\"</span><span class=\"p\">)</span>\n\n<span class=\"o\">@</span><span class=\"n\">mcp</span><span class=\"p\">.</span><span class=\"n\">tool</span><span class=\"p\">()</span>\n<span class=\"k\">def</span> <span class=\"nf\">add</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">int</span><span class=\"p\">:</span>\n    <span class=\"k\">return</span> <span class=\"n\">a</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n\n<span class=\"o\">@</span><span class=\"n\">mcp</span><span class=\"p\">.</span><span class=\"n\">resource</span><span class=\"p\">(</span><span class=\"s\">\"greeting://{name}\"</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_greeting</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"k\">return</span> <span class=\"sa\">f</span><span class=\"s\">\"Hello, </span><span class=\"si\">{</span><span class=\"n\">name</span><span class=\"si\">}</span><span class=\"s\">!\"</span>\n\n<span class=\"o\">@</span><span class=\"n\">mcp</span><span class=\"p\">.</span><span class=\"n\">prompt</span><span class=\"p\">()</span>\n<span class=\"k\">def</span> <span class=\"nf\">review_code</span><span class=\"p\">(</span><span class=\"n\">code</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"k\">return</span> <span class=\"sa\">f</span><span class=\"s\">\"Please review this code:</span><span class=\"se\">\\n\\n</span><span class=\"si\">{</span><span class=\"n\">code</span><span class=\"si\">}</span><span class=\"s\">\"</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code9\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code9\"><span class=\"kn\">from</span> <span class=\"nn\">fastmcp</span> <span class=\"kn\">import</span> <span class=\"n\">FastMCP</span>\n\n<span class=\"n\">mcp</span> <span class=\"o\">=</span> <span class=\"n\">FastMCP</span><span class=\"p\">(</span><span class=\"s\">\"Demo\"</span><span class=\"p\">)</span>\n\n<span class=\"o\">@</span><span class=\"n\">mcp</span><span class=\"p\">.</span><span class=\"n\">tool</span><span class=\"p\">()</span>\n<span class=\"k\">def</span> <span class=\"nf\">add</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">int</span><span class=\"p\">:</span>\n    <span class=\"k\">return</span> <span class=\"n\">a</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n\n<span class=\"o\">@</span><span class=\"n\">mcp</span><span class=\"p\">.</span><span class=\"n\">resource</span><span class=\"p\">(</span><span class=\"s\">\"greeting://{name}\"</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_greeting</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"k\">return</span> <span class=\"sa\">f</span><span class=\"s\">\"Hello, </span><span class=\"si\">{</span><span class=\"n\">name</span><span class=\"si\">}</span><span class=\"s\">!\"</span>\n\n<span class=\"o\">@</span><span class=\"n\">mcp</span><span class=\"p\">.</span><span class=\"n\">prompt</span><span class=\"p\">()</span>\n<span class=\"k\">def</span> <span class=\"nf\">review_code</span><span class=\"p\">(</span><span class=\"n\">code</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"k\">return</span> <span class=\"sa\">f</span><span class=\"s\">\"Please review this code:</span><span class=\"se\">\\n\\n</span><span class=\"si\">{</span><span class=\"n\">code</span><span class=\"si\">}</span><span class=\"s\">\"</span>\n</code></pre>",
    "contentMarkdown": "*   At its core, MCP follows a client-server architecture, enabling a host application to connect to multiple MCP servers. Below is an overview of the key components:\n    *   **MCP Hosts:** Programs such as Claude Desktop, IDEs, and AI tools that need to access data through MCP.\n    *   **MCP Clients:** Protocol clients that maintain one-to-one connections with MCP servers.\n    *   **MCP Servers:** Lightweight programs that expose specific capabilities using the standardized MCP framework.\n    *   **Local Data Sources:** Files, databases, and services on a user’s computer that MCP servers can securely access.\n    *   **Remote Services:** External systems, such as APIs, that MCP servers can connect to over the internet.\n*   **Client Example Using Python:** The following code snippet ([source](https://www.philschmid.de/mcp-introduction)) shows how to create a basic MCP client that initializes a session, lists available tools, and calls one of the server’s tools using standard I/O transport.\n\n*   **MCP Hosts:** Programs such as Claude Desktop, IDEs, and AI tools that need to access data through MCP.\n*   **MCP Clients:** Protocol clients that maintain one-to-one connections with MCP servers.\n*   **MCP Servers:** Lightweight programs that expose specific capabilities using the standardized MCP framework.\n*   **Local Data Sources:** Files, databases, and services on a user’s computer that MCP servers can securely access.\n*   **Remote Services:** External systems, such as APIs, that MCP servers can connect to over the internet.\n\n![](https://aman.ai/images/copy.png)\n\n`from mcp import ClientSession, StdioServerParameters from mcp.client.stdio import stdio_client  server_params = StdioServerParameters(     command=\"python\", args=[\"example_server.py\"] )  async with stdio_client(server_params) as (read, write):     async with ClientSession(read, write) as session:         await session.initialize()         tools = await session.list_tools()         result = await session.call_tool(\"tool-name\", arguments={\"arg1\": \"value\"})`\n\n![](https://aman.ai/images/copy.png)\n\n`from mcp import ClientSession, StdioServerParameters from mcp.client.stdio import stdio_client  server_params = StdioServerParameters(     command=\"python\", args=[\"example_server.py\"] )  async with stdio_client(server_params) as (read, write):     async with ClientSession(read, write) as session:         await session.initialize()         tools = await session.list_tools()         result = await session.call_tool(\"tool-name\", arguments={\"arg1\": \"value\"})`\n\n*   **Server Example Using FastMCP (Python):** The following code snippet ([source](https://www.philschmid.de/mcp-introduction)) illustrates how to create a simple MCP server using FastMCP, which exposes a tool, a data resource, and a reusable prompt template.\n\n![](https://aman.ai/images/copy.png)\n\n`from fastmcp import FastMCP  mcp = FastMCP(\"Demo\")  @mcp.tool() def add(a: int, b: int) -> int:     return a + b  @mcp.resource(\"greeting://{name}\") def get_greeting(name: str) -> str:     return f\"Hello, {name}!\"  @mcp.prompt() def review_code(code: str) -> str:     return f\"Please review this code:\\n\\n{code}\"`\n\n![](https://aman.ai/images/copy.png)\n\n`from fastmcp import FastMCP  mcp = FastMCP(\"Demo\")  @mcp.tool() def add(a: int, b: int) -> int:     return a + b  @mcp.resource(\"greeting://{name}\") def get_greeting(name: str) -> str:     return f\"Hello, {name}!\"  @mcp.prompt() def review_code(code: str) -> str:     return f\"Please review this code:\\n\\n{code}\"`",
    "order": 18,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 3,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 416,
      "contentLength": 12210
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#general-architecture",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-how-mcp-works-19",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Model Context Protocol (MCP)",
    "title": "How MCP Works",
    "subtitle": "Model Context Protocol (MCP)",
    "contentHtml": "<ul>\n  <li>The following figure (<a href=\"https://blog.dailydoseofds.com/p/visual-guide-to-model-context-protocol\">source</a>) provides an animated overview of the MCP Flow.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/agents/overview.gif\" alt=\"Placeholder for Animated Overview of MCP Flow\"></p>\n<ul>\n  <li>MCP operates through a structured client-server interaction that involves capability exchange and bidirectional communication:\n    <ol>\n      <li><strong>Capability Exchange:</strong> The MCP client initializes a request to the server to learn about its available capabilities.</li>\n      <li><strong>Server Response:</strong> The server provides details about its available tools, resources, and prompts.</li>\n      <li><strong>Notification &amp; Acknowledgment:</strong> The client acknowledges the connection and facilitates further communication.</li>\n    </ol>\n  </li>\n  <li>The following figure (<a href=\"https://www.philschmid.de/mcp-introduction\">source</a>) the division of responsibilities between the MCP client and server, along with examples of Tools, Resources, and Prompts.</li>\n</ul>\n<ol>\n      <li><strong>Capability Exchange:</strong> The MCP client initializes a request to the server to learn about its available capabilities.</li>\n      <li><strong>Server Response:</strong> The server provides details about its available tools, resources, and prompts.</li>\n      <li><strong>Notification &amp; Acknowledgment:</strong> The client acknowledges the connection and facilitates further communication.</li>\n    </ol>\n<p><img src=\"/primers/ai/assets/agents/client_server_capabilities.jpg\" alt=\"Placeholder for MCP Client-Server Capability Breakdown\"></p>\n<ul>\n  <li>Unlike traditional APIs, MCP communication is two-way, allowing:\n    <ul>\n      <li>Servers to leverage client-side AI capabilities (such as LLM-based completions) without requiring API keys.</li>\n      <li>Clients to maintain control over model access and permissions while enabling interoperability with various services.</li>\n    </ul>\n  </li>\n  <li><strong>Practical Example with Gemini + MCP Client:</strong> The following code snippet (<a href=\"https://www.philschmid.de/mcp-introduction\">source</a>) shows how an AI model like Gemini can dynamically discover available MCP tools and send function calls, making tool invocation seamless within natural language interactions.</li>\n</ul>\n<ul>\n      <li>Servers to leverage client-side AI capabilities (such as LLM-based completions) without requiring API keys.</li>\n      <li>Clients to maintain control over model access and permissions while enabling interoperability with various services.</li>\n    </ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code10\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code10\"><span class=\"kn\">from</span> <span class=\"nn\">google</span> <span class=\"kn\">import</span> <span class=\"n\">genai</span>\n<span class=\"kn\">from</span> <span class=\"nn\">google.genai</span> <span class=\"kn\">import</span> <span class=\"n\">types</span>\n\n<span class=\"c1\"># Assume session is an initialized MCP ClientSession\n</span><span class=\"n\">mcp_tools</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">list_tools</span><span class=\"p\">()</span>\n<span class=\"n\">tools</span> <span class=\"o\">=</span> <span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Tool</span><span class=\"p\">(</span><span class=\"n\">function_declarations</span><span class=\"o\">=</span><span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n        <span class=\"s\">\"name\"</span><span class=\"p\">:</span> <span class=\"n\">tool</span><span class=\"p\">.</span><span class=\"n\">name</span><span class=\"p\">,</span>\n        <span class=\"s\">\"description\"</span><span class=\"p\">:</span> <span class=\"n\">tool</span><span class=\"p\">.</span><span class=\"n\">description</span><span class=\"p\">,</span>\n        <span class=\"s\">\"parameters\"</span><span class=\"p\">:</span> <span class=\"n\">tool</span><span class=\"p\">.</span><span class=\"n\">inputSchema</span><span class=\"p\">,</span>\n    <span class=\"p\">}</span>\n    <span class=\"k\">for</span> <span class=\"n\">tool</span> <span class=\"ow\">in</span> <span class=\"n\">mcp_tools</span><span class=\"p\">.</span><span class=\"n\">tools</span>\n<span class=\"p\">])</span>\n\n<span class=\"c1\"># Send tool schema to Gemini model\n</span><span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">aio</span><span class=\"p\">.</span><span class=\"n\">models</span><span class=\"p\">.</span><span class=\"n\">generate_content</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s\">\"gemini-2.0-flash\"</span><span class=\"p\">,</span>\n    <span class=\"n\">contents</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Content</span><span class=\"p\">(</span><span class=\"n\">role</span><span class=\"o\">=</span><span class=\"s\">\"user\"</span><span class=\"p\">,</span> <span class=\"n\">parts</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Part</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"o\">=</span><span class=\"s\">\"Show me available Airbnb listings in Paris.\"</span><span class=\"p\">)])],</span>\n    <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">GenerateContentConfig</span><span class=\"p\">(</span><span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">tools</span><span class=\"p\">])</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code10\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code10\"><span class=\"kn\">from</span> <span class=\"nn\">google</span> <span class=\"kn\">import</span> <span class=\"n\">genai</span>\n<span class=\"kn\">from</span> <span class=\"nn\">google.genai</span> <span class=\"kn\">import</span> <span class=\"n\">types</span>\n\n<span class=\"c1\"># Assume session is an initialized MCP ClientSession\n</span><span class=\"n\">mcp_tools</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">list_tools</span><span class=\"p\">()</span>\n<span class=\"n\">tools</span> <span class=\"o\">=</span> <span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Tool</span><span class=\"p\">(</span><span class=\"n\">function_declarations</span><span class=\"o\">=</span><span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n        <span class=\"s\">\"name\"</span><span class=\"p\">:</span> <span class=\"n\">tool</span><span class=\"p\">.</span><span class=\"n\">name</span><span class=\"p\">,</span>\n        <span class=\"s\">\"description\"</span><span class=\"p\">:</span> <span class=\"n\">tool</span><span class=\"p\">.</span><span class=\"n\">description</span><span class=\"p\">,</span>\n        <span class=\"s\">\"parameters\"</span><span class=\"p\">:</span> <span class=\"n\">tool</span><span class=\"p\">.</span><span class=\"n\">inputSchema</span><span class=\"p\">,</span>\n    <span class=\"p\">}</span>\n    <span class=\"k\">for</span> <span class=\"n\">tool</span> <span class=\"ow\">in</span> <span class=\"n\">mcp_tools</span><span class=\"p\">.</span><span class=\"n\">tools</span>\n<span class=\"p\">])</span>\n\n<span class=\"c1\"># Send tool schema to Gemini model\n</span><span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">aio</span><span class=\"p\">.</span><span class=\"n\">models</span><span class=\"p\">.</span><span class=\"n\">generate_content</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s\">\"gemini-2.0-flash\"</span><span class=\"p\">,</span>\n    <span class=\"n\">contents</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Content</span><span class=\"p\">(</span><span class=\"n\">role</span><span class=\"o\">=</span><span class=\"s\">\"user\"</span><span class=\"p\">,</span> <span class=\"n\">parts</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Part</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"o\">=</span><span class=\"s\">\"Show me available Airbnb listings in Paris.\"</span><span class=\"p\">)])],</span>\n    <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">GenerateContentConfig</span><span class=\"p\">(</span><span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">tools</span><span class=\"p\">])</span>\n<span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li><strong>Practical Example with Gemini and Python uSDK:</strong> The following end-to-end implementation (<a href=\"https://www.philschmid.de/mcp-introduction\">source</a>) demonstrates an AI agent loop that connects to an MCP server via stdio, retrieves tool specs, sends them to Gemini 2.0 Flash, handles function calls from the model, and completes the interaction using the Google Python SDK.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code11\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code11\"><span class=\"kn\">from</span> <span class=\"nn\">typing</span> <span class=\"kn\">import</span> <span class=\"n\">List</span>\n<span class=\"kn\">from</span> <span class=\"nn\">google</span> <span class=\"kn\">import</span> <span class=\"n\">genai</span>\n<span class=\"kn\">from</span> <span class=\"nn\">google.genai</span> <span class=\"kn\">import</span> <span class=\"n\">types</span>\n<span class=\"kn\">from</span> <span class=\"nn\">mcp</span> <span class=\"kn\">import</span> <span class=\"n\">ClientSession</span><span class=\"p\">,</span> <span class=\"n\">StdioServerParameters</span>\n<span class=\"kn\">from</span> <span class=\"nn\">mcp.client.stdio</span> <span class=\"kn\">import</span> <span class=\"n\">stdio_client</span>\n<span class=\"kn\">import</span> <span class=\"nn\">os</span>\n\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">genai</span><span class=\"p\">.</span><span class=\"n\">Client</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">getenv</span><span class=\"p\">(</span><span class=\"s\">\"GEMINI_API_KEY\"</span><span class=\"p\">))</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"s\">\"gemini-2.0-flash\"</span>\n\n<span class=\"n\">server_params</span> <span class=\"o\">=</span> <span class=\"n\">StdioServerParameters</span><span class=\"p\">(</span>\n    <span class=\"n\">command</span><span class=\"o\">=</span><span class=\"s\">\"npx\"</span><span class=\"p\">,</span>\n    <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">\"-y\"</span><span class=\"p\">,</span> <span class=\"s\">\"@openbnb/mcp-server-airbnb\"</span><span class=\"p\">],</span>\n    <span class=\"n\">env</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">agent_loop</span><span class=\"p\">(</span><span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">client</span><span class=\"p\">:</span> <span class=\"n\">genai</span><span class=\"p\">.</span><span class=\"n\">Client</span><span class=\"p\">,</span> <span class=\"n\">session</span><span class=\"p\">:</span> <span class=\"n\">ClientSession</span><span class=\"p\">):</span>\n    <span class=\"n\">contents</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Content</span><span class=\"p\">(</span><span class=\"n\">role</span><span class=\"o\">=</span><span class=\"s\">\"user\"</span><span class=\"p\">,</span> <span class=\"n\">parts</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Part</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"o\">=</span><span class=\"n\">prompt</span><span class=\"p\">)])]</span>\n    <span class=\"k\">await</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">initialize</span><span class=\"p\">()</span>\n\n    <span class=\"n\">mcp_tools</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">list_tools</span><span class=\"p\">()</span>\n    <span class=\"n\">tools</span> <span class=\"o\">=</span> <span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Tool</span><span class=\"p\">(</span><span class=\"n\">function_declarations</span><span class=\"o\">=</span><span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s\">\"name\"</span><span class=\"p\">:</span> <span class=\"n\">tool</span><span class=\"p\">.</span><span class=\"n\">name</span><span class=\"p\">,</span>\n            <span class=\"s\">\"description\"</span><span class=\"p\">:</span> <span class=\"n\">tool</span><span class=\"p\">.</span><span class=\"n\">description</span><span class=\"p\">,</span>\n            <span class=\"s\">\"parameters\"</span><span class=\"p\">:</span> <span class=\"n\">tool</span><span class=\"p\">.</span><span class=\"n\">inputSchema</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n        <span class=\"k\">for</span> <span class=\"n\">tool</span> <span class=\"ow\">in</span> <span class=\"n\">mcp_tools</span><span class=\"p\">.</span><span class=\"n\">tools</span>\n    <span class=\"p\">])</span>\n\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">aio</span><span class=\"p\">.</span><span class=\"n\">models</span><span class=\"p\">.</span><span class=\"n\">generate_content</span><span class=\"p\">(</span>\n        <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span>\n        <span class=\"n\">contents</span><span class=\"o\">=</span><span class=\"n\">contents</span><span class=\"p\">,</span>\n        <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">GenerateContentConfig</span><span class=\"p\">(</span>\n            <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n            <span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">tools</span><span class=\"p\">],</span>\n        <span class=\"p\">),</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"n\">contents</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">candidates</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">content</span><span class=\"p\">)</span>\n\n    <span class=\"n\">turn_count</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n    <span class=\"n\">max_tool_turns</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n    <span class=\"k\">while</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">function_calls</span> <span class=\"ow\">and</span> <span class=\"n\">turn_count</span> <span class=\"o\">&lt;</span> <span class=\"n\">max_tool_turns</span><span class=\"p\">:</span>\n        <span class=\"n\">turn_count</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n        <span class=\"n\">tool_response_parts</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Part</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n        <span class=\"k\">for</span> <span class=\"n\">fc_part</span> <span class=\"ow\">in</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">function_calls</span><span class=\"p\">:</span>\n            <span class=\"n\">tool_name</span> <span class=\"o\">=</span> <span class=\"n\">fc_part</span><span class=\"p\">.</span><span class=\"n\">name</span>\n            <span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"n\">fc_part</span><span class=\"p\">.</span><span class=\"n\">args</span> <span class=\"ow\">or</span> <span class=\"p\">{}</span>\n            <span class=\"k\">try</span><span class=\"p\">:</span>\n                <span class=\"n\">tool_result</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">call_tool</span><span class=\"p\">(</span><span class=\"n\">tool_name</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">)</span>\n                <span class=\"k\">if</span> <span class=\"n\">tool_result</span><span class=\"p\">.</span><span class=\"n\">isError</span><span class=\"p\">:</span>\n                    <span class=\"n\">tool_response</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">\"error\"</span><span class=\"p\">:</span> <span class=\"n\">tool_result</span><span class=\"p\">.</span><span class=\"n\">content</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">text</span><span class=\"p\">}</span>\n                <span class=\"k\">else</span><span class=\"p\">:</span>\n                    <span class=\"n\">tool_response</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">\"result\"</span><span class=\"p\">:</span> <span class=\"n\">tool_result</span><span class=\"p\">.</span><span class=\"n\">content</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">text</span><span class=\"p\">}</span>\n            <span class=\"k\">except</span> <span class=\"nb\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n                <span class=\"n\">tool_response</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">\"error\"</span><span class=\"p\">:</span> <span class=\"sa\">f</span><span class=\"s\">\"Tool execution failed: </span><span class=\"si\">{</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">).</span><span class=\"n\">__name__</span><span class=\"si\">}</span><span class=\"s\">: </span><span class=\"si\">{</span><span class=\"n\">e</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">}</span>\n\n            <span class=\"n\">tool_response_parts</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span>\n                <span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Part</span><span class=\"p\">.</span><span class=\"n\">from_function_response</span><span class=\"p\">(</span>\n                    <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"n\">tool_name</span><span class=\"p\">,</span> <span class=\"n\">response</span><span class=\"o\">=</span><span class=\"n\">tool_response</span>\n                <span class=\"p\">)</span>\n            <span class=\"p\">)</span>\n\n        <span class=\"n\">contents</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Content</span><span class=\"p\">(</span><span class=\"n\">role</span><span class=\"o\">=</span><span class=\"s\">\"user\"</span><span class=\"p\">,</span> <span class=\"n\">parts</span><span class=\"o\">=</span><span class=\"n\">tool_response_parts</span><span class=\"p\">))</span>\n\n        <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">aio</span><span class=\"p\">.</span><span class=\"n\">models</span><span class=\"p\">.</span><span class=\"n\">generate_content</span><span class=\"p\">(</span>\n            <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span>\n            <span class=\"n\">contents</span><span class=\"o\">=</span><span class=\"n\">contents</span><span class=\"p\">,</span>\n            <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">GenerateContentConfig</span><span class=\"p\">(</span>\n                <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">,</span>\n                <span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">tools</span><span class=\"p\">],</span>\n            <span class=\"p\">),</span>\n        <span class=\"p\">)</span>\n        <span class=\"n\">contents</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">candidates</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">content</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">response</span>\n\n<span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">run</span><span class=\"p\">():</span>\n    <span class=\"k\">async</span> <span class=\"k\">with</span> <span class=\"n\">stdio_client</span><span class=\"p\">(</span><span class=\"n\">server_params</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"p\">(</span><span class=\"n\">read</span><span class=\"p\">,</span> <span class=\"n\">write</span><span class=\"p\">):</span>\n        <span class=\"k\">async</span> <span class=\"k\">with</span> <span class=\"n\">ClientSession</span><span class=\"p\">(</span><span class=\"n\">read</span><span class=\"p\">,</span> <span class=\"n\">write</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">session</span><span class=\"p\">:</span>\n            <span class=\"n\">prompt</span> <span class=\"o\">=</span> <span class=\"s\">\"I want to book an apartment in Paris for 2 nights. 03/28 - 03/30\"</span>\n            <span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">agent_loop</span><span class=\"p\">(</span><span class=\"n\">prompt</span><span class=\"p\">,</span> <span class=\"n\">client</span><span class=\"p\">,</span> <span class=\"n\">session</span><span class=\"p\">)</span>\n            <span class=\"k\">return</span> <span class=\"n\">res</span>\n\n<span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">run</span><span class=\"p\">()</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">res</span><span class=\"p\">.</span><span class=\"n\">text</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code11\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code11\"><span class=\"kn\">from</span> <span class=\"nn\">typing</span> <span class=\"kn\">import</span> <span class=\"n\">List</span>\n<span class=\"kn\">from</span> <span class=\"nn\">google</span> <span class=\"kn\">import</span> <span class=\"n\">genai</span>\n<span class=\"kn\">from</span> <span class=\"nn\">google.genai</span> <span class=\"kn\">import</span> <span class=\"n\">types</span>\n<span class=\"kn\">from</span> <span class=\"nn\">mcp</span> <span class=\"kn\">import</span> <span class=\"n\">ClientSession</span><span class=\"p\">,</span> <span class=\"n\">StdioServerParameters</span>\n<span class=\"kn\">from</span> <span class=\"nn\">mcp.client.stdio</span> <span class=\"kn\">import</span> <span class=\"n\">stdio_client</span>\n<span class=\"kn\">import</span> <span class=\"nn\">os</span>\n\n<span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">genai</span><span class=\"p\">.</span><span class=\"n\">Client</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">getenv</span><span class=\"p\">(</span><span class=\"s\">\"GEMINI_API_KEY\"</span><span class=\"p\">))</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"s\">\"gemini-2.0-flash\"</span>\n\n<span class=\"n\">server_params</span> <span class=\"o\">=</span> <span class=\"n\">StdioServerParameters</span><span class=\"p\">(</span>\n    <span class=\"n\">command</span><span class=\"o\">=</span><span class=\"s\">\"npx\"</span><span class=\"p\">,</span>\n    <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">\"-y\"</span><span class=\"p\">,</span> <span class=\"s\">\"@openbnb/mcp-server-airbnb\"</span><span class=\"p\">],</span>\n    <span class=\"n\">env</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">agent_loop</span><span class=\"p\">(</span><span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">client</span><span class=\"p\">:</span> <span class=\"n\">genai</span><span class=\"p\">.</span><span class=\"n\">Client</span><span class=\"p\">,</span> <span class=\"n\">session</span><span class=\"p\">:</span> <span class=\"n\">ClientSession</span><span class=\"p\">):</span>\n    <span class=\"n\">contents</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Content</span><span class=\"p\">(</span><span class=\"n\">role</span><span class=\"o\">=</span><span class=\"s\">\"user\"</span><span class=\"p\">,</span> <span class=\"n\">parts</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Part</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"o\">=</span><span class=\"n\">prompt</span><span class=\"p\">)])]</span>\n    <span class=\"k\">await</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">initialize</span><span class=\"p\">()</span>\n\n    <span class=\"n\">mcp_tools</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">list_tools</span><span class=\"p\">()</span>\n    <span class=\"n\">tools</span> <span class=\"o\">=</span> <span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Tool</span><span class=\"p\">(</span><span class=\"n\">function_declarations</span><span class=\"o\">=</span><span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"s\">\"name\"</span><span class=\"p\">:</span> <span class=\"n\">tool</span><span class=\"p\">.</span><span class=\"n\">name</span><span class=\"p\">,</span>\n            <span class=\"s\">\"description\"</span><span class=\"p\">:</span> <span class=\"n\">tool</span><span class=\"p\">.</span><span class=\"n\">description</span><span class=\"p\">,</span>\n            <span class=\"s\">\"parameters\"</span><span class=\"p\">:</span> <span class=\"n\">tool</span><span class=\"p\">.</span><span class=\"n\">inputSchema</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n        <span class=\"k\">for</span> <span class=\"n\">tool</span> <span class=\"ow\">in</span> <span class=\"n\">mcp_tools</span><span class=\"p\">.</span><span class=\"n\">tools</span>\n    <span class=\"p\">])</span>\n\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">aio</span><span class=\"p\">.</span><span class=\"n\">models</span><span class=\"p\">.</span><span class=\"n\">generate_content</span><span class=\"p\">(</span>\n        <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span>\n        <span class=\"n\">contents</span><span class=\"o\">=</span><span class=\"n\">contents</span><span class=\"p\">,</span>\n        <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">GenerateContentConfig</span><span class=\"p\">(</span>\n            <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n            <span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">tools</span><span class=\"p\">],</span>\n        <span class=\"p\">),</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"n\">contents</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">candidates</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">content</span><span class=\"p\">)</span>\n\n    <span class=\"n\">turn_count</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n    <span class=\"n\">max_tool_turns</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n    <span class=\"k\">while</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">function_calls</span> <span class=\"ow\">and</span> <span class=\"n\">turn_count</span> <span class=\"o\">&lt;</span> <span class=\"n\">max_tool_turns</span><span class=\"p\">:</span>\n        <span class=\"n\">turn_count</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n        <span class=\"n\">tool_response_parts</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Part</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n        <span class=\"k\">for</span> <span class=\"n\">fc_part</span> <span class=\"ow\">in</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">function_calls</span><span class=\"p\">:</span>\n            <span class=\"n\">tool_name</span> <span class=\"o\">=</span> <span class=\"n\">fc_part</span><span class=\"p\">.</span><span class=\"n\">name</span>\n            <span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"n\">fc_part</span><span class=\"p\">.</span><span class=\"n\">args</span> <span class=\"ow\">or</span> <span class=\"p\">{}</span>\n            <span class=\"k\">try</span><span class=\"p\">:</span>\n                <span class=\"n\">tool_result</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">call_tool</span><span class=\"p\">(</span><span class=\"n\">tool_name</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"p\">)</span>\n                <span class=\"k\">if</span> <span class=\"n\">tool_result</span><span class=\"p\">.</span><span class=\"n\">isError</span><span class=\"p\">:</span>\n                    <span class=\"n\">tool_response</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">\"error\"</span><span class=\"p\">:</span> <span class=\"n\">tool_result</span><span class=\"p\">.</span><span class=\"n\">content</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">text</span><span class=\"p\">}</span>\n                <span class=\"k\">else</span><span class=\"p\">:</span>\n                    <span class=\"n\">tool_response</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">\"result\"</span><span class=\"p\">:</span> <span class=\"n\">tool_result</span><span class=\"p\">.</span><span class=\"n\">content</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">text</span><span class=\"p\">}</span>\n            <span class=\"k\">except</span> <span class=\"nb\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n                <span class=\"n\">tool_response</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">\"error\"</span><span class=\"p\">:</span> <span class=\"sa\">f</span><span class=\"s\">\"Tool execution failed: </span><span class=\"si\">{</span><span class=\"nb\">type</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">).</span><span class=\"n\">__name__</span><span class=\"si\">}</span><span class=\"s\">: </span><span class=\"si\">{</span><span class=\"n\">e</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">}</span>\n\n            <span class=\"n\">tool_response_parts</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span>\n                <span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Part</span><span class=\"p\">.</span><span class=\"n\">from_function_response</span><span class=\"p\">(</span>\n                    <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"n\">tool_name</span><span class=\"p\">,</span> <span class=\"n\">response</span><span class=\"o\">=</span><span class=\"n\">tool_response</span>\n                <span class=\"p\">)</span>\n            <span class=\"p\">)</span>\n\n        <span class=\"n\">contents</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">Content</span><span class=\"p\">(</span><span class=\"n\">role</span><span class=\"o\">=</span><span class=\"s\">\"user\"</span><span class=\"p\">,</span> <span class=\"n\">parts</span><span class=\"o\">=</span><span class=\"n\">tool_response_parts</span><span class=\"p\">))</span>\n\n        <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">aio</span><span class=\"p\">.</span><span class=\"n\">models</span><span class=\"p\">.</span><span class=\"n\">generate_content</span><span class=\"p\">(</span>\n            <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model</span><span class=\"p\">,</span>\n            <span class=\"n\">contents</span><span class=\"o\">=</span><span class=\"n\">contents</span><span class=\"p\">,</span>\n            <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"n\">types</span><span class=\"p\">.</span><span class=\"n\">GenerateContentConfig</span><span class=\"p\">(</span>\n                <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">,</span>\n                <span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">tools</span><span class=\"p\">],</span>\n            <span class=\"p\">),</span>\n        <span class=\"p\">)</span>\n        <span class=\"n\">contents</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">candidates</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">content</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">response</span>\n\n<span class=\"k\">async</span> <span class=\"k\">def</span> <span class=\"nf\">run</span><span class=\"p\">():</span>\n    <span class=\"k\">async</span> <span class=\"k\">with</span> <span class=\"n\">stdio_client</span><span class=\"p\">(</span><span class=\"n\">server_params</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"p\">(</span><span class=\"n\">read</span><span class=\"p\">,</span> <span class=\"n\">write</span><span class=\"p\">):</span>\n        <span class=\"k\">async</span> <span class=\"k\">with</span> <span class=\"n\">ClientSession</span><span class=\"p\">(</span><span class=\"n\">read</span><span class=\"p\">,</span> <span class=\"n\">write</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">session</span><span class=\"p\">:</span>\n            <span class=\"n\">prompt</span> <span class=\"o\">=</span> <span class=\"s\">\"I want to book an apartment in Paris for 2 nights. 03/28 - 03/30\"</span>\n            <span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">agent_loop</span><span class=\"p\">(</span><span class=\"n\">prompt</span><span class=\"p\">,</span> <span class=\"n\">client</span><span class=\"p\">,</span> <span class=\"n\">session</span><span class=\"p\">)</span>\n            <span class=\"k\">return</span> <span class=\"n\">res</span>\n\n<span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">run</span><span class=\"p\">()</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">res</span><span class=\"p\">.</span><span class=\"n\">text</span><span class=\"p\">)</span>\n</code></pre>",
    "contentMarkdown": "*   The following figure ([source](https://blog.dailydoseofds.com/p/visual-guide-to-model-context-protocol)) provides an animated overview of the MCP Flow.\n\n![Placeholder for Animated Overview of MCP Flow](/primers/ai/assets/agents/overview.gif)\n\n*   MCP operates through a structured client-server interaction that involves capability exchange and bidirectional communication:\n    1.  **Capability Exchange:** The MCP client initializes a request to the server to learn about its available capabilities.\n    2.  **Server Response:** The server provides details about its available tools, resources, and prompts.\n    3.  **Notification & Acknowledgment:** The client acknowledges the connection and facilitates further communication.\n*   The following figure ([source](https://www.philschmid.de/mcp-introduction)) the division of responsibilities between the MCP client and server, along with examples of Tools, Resources, and Prompts.\n\n1.  **Capability Exchange:** The MCP client initializes a request to the server to learn about its available capabilities.\n2.  **Server Response:** The server provides details about its available tools, resources, and prompts.\n3.  **Notification & Acknowledgment:** The client acknowledges the connection and facilitates further communication.\n\n![Placeholder for MCP Client-Server Capability Breakdown](/primers/ai/assets/agents/client_server_capabilities.jpg)\n\n*   Unlike traditional APIs, MCP communication is two-way, allowing:\n    *   Servers to leverage client-side AI capabilities (such as LLM-based completions) without requiring API keys.\n    *   Clients to maintain control over model access and permissions while enabling interoperability with various services.\n*   **Practical Example with Gemini + MCP Client:** The following code snippet ([source](https://www.philschmid.de/mcp-introduction)) shows how an AI model like Gemini can dynamically discover available MCP tools and send function calls, making tool invocation seamless within natural language interactions.\n\n*   Servers to leverage client-side AI capabilities (such as LLM-based completions) without requiring API keys.\n*   Clients to maintain control over model access and permissions while enabling interoperability with various services.\n\n![](https://aman.ai/images/copy.png)\n\n`from google import genai from google.genai import types  # Assume session is an initialized MCP ClientSession mcp_tools = await session.list_tools() tools = types.Tool(function_declarations=[     {         \"name\": tool.name,         \"description\": tool.description,         \"parameters\": tool.inputSchema,     }     for tool in mcp_tools.tools ])  # Send tool schema to Gemini model response = await client.aio.models.generate_content(     model=\"gemini-2.0-flash\",     contents=[types.Content(role=\"user\", parts=[types.Part(text=\"Show me available Airbnb listings in Paris.\")])],     config=types.GenerateContentConfig(tools=[tools]) )`\n\n![](https://aman.ai/images/copy.png)\n\n`from google import genai from google.genai import types  # Assume session is an initialized MCP ClientSession mcp_tools = await session.list_tools() tools = types.Tool(function_declarations=[     {         \"name\": tool.name,         \"description\": tool.description,         \"parameters\": tool.inputSchema,     }     for tool in mcp_tools.tools ])  # Send tool schema to Gemini model response = await client.aio.models.generate_content(     model=\"gemini-2.0-flash\",     contents=[types.Content(role=\"user\", parts=[types.Part(text=\"Show me available Airbnb listings in Paris.\")])],     config=types.GenerateContentConfig(tools=[tools]) )`\n\n*   **Practical Example with Gemini and Python uSDK:** The following end-to-end implementation ([source](https://www.philschmid.de/mcp-introduction)) demonstrates an AI agent loop that connects to an MCP server via stdio, retrieves tool specs, sends them to Gemini 2.0 Flash, handles function calls from the model, and completes the interaction using the Google Python SDK.\n\n![](https://aman.ai/images/copy.png)\n\n`from typing import List from google import genai from google.genai import types from mcp import ClientSession, StdioServerParameters from mcp.client.stdio import stdio_client import os  client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) model = \"gemini-2.0-flash\"  server_params = StdioServerParameters(     command=\"npx\",     args=[\"-y\", \"@openbnb/mcp-server-airbnb\"],     env=None, )  async def agent_loop(prompt: str, client: genai.Client, session: ClientSession):     contents = [types.Content(role=\"user\", parts=[types.Part(text=prompt)])]     await session.initialize()      mcp_tools = await session.list_tools()     tools = types.Tool(function_declarations=[         {             \"name\": tool.name,             \"description\": tool.description,             \"parameters\": tool.inputSchema,         }         for tool in mcp_tools.tools     ])      response = await client.aio.models.generate_content(         model=model,         contents=contents,         config=types.GenerateContentConfig(             temperature=0,             tools=[tools],         ),     )      contents.append(response.candidates[0].content)      turn_count = 0     max_tool_turns = 5     while response.function_calls and turn_count < max_tool_turns:         turn_count += 1         tool_response_parts: List[types.Part] = []          for fc_part in response.function_calls:             tool_name = fc_part.name             args = fc_part.args or {}             try:                 tool_result = await session.call_tool(tool_name, args)                 if tool_result.isError:                     tool_response = {\"error\": tool_result.content[0].text}                 else:                     tool_response = {\"result\": tool_result.content[0].text}             except Exception as e:                 tool_response = {\"error\": f\"Tool execution failed: {type(e).__name__}: {e}\"}              tool_response_parts.append(                 types.Part.from_function_response(                     name=tool_name, response=tool_response                 )             )          contents.append(types.Content(role=\"user\", parts=tool_response_parts))          response = await client.aio.models.generate_content(             model=model,             contents=contents,             config=types.GenerateContentConfig(                 temperature=1.0,                 tools=[tools],             ),         )         contents.append(response.candidates[0].content)      return response  async def run():     async with stdio_client(server_params) as (read, write):         async with ClientSession(read, write) as session:             prompt = \"I want to book an apartment in Paris for 2 nights. 03/28 - 03/30\"             res = await agent_loop(prompt, client, session)             return res  res = await run() print(res.text)`\n\n![](https://aman.ai/images/copy.png)\n\n`from typing import List from google import genai from google.genai import types from mcp import ClientSession, StdioServerParameters from mcp.client.stdio import stdio_client import os  client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) model = \"gemini-2.0-flash\"  server_params = StdioServerParameters(     command=\"npx\",     args=[\"-y\", \"@openbnb/mcp-server-airbnb\"],     env=None, )  async def agent_loop(prompt: str, client: genai.Client, session: ClientSession):     contents = [types.Content(role=\"user\", parts=[types.Part(text=prompt)])]     await session.initialize()      mcp_tools = await session.list_tools()     tools = types.Tool(function_declarations=[         {             \"name\": tool.name,             \"description\": tool.description,             \"parameters\": tool.inputSchema,         }         for tool in mcp_tools.tools     ])      response = await client.aio.models.generate_content(         model=model,         contents=contents,         config=types.GenerateContentConfig(             temperature=0,             tools=[tools],         ),     )      contents.append(response.candidates[0].content)      turn_count = 0     max_tool_turns = 5     while response.function_calls and turn_count < max_tool_turns:         turn_count += 1         tool_response_parts: List[types.Part] = []          for fc_part in response.function_calls:             tool_name = fc_part.name             args = fc_part.args or {}             try:                 tool_result = await session.call_tool(tool_name, args)                 if tool_result.isError:                     tool_response = {\"error\": tool_result.content[0].text}                 else:                     tool_response = {\"result\": tool_result.content[0].text}             except Exception as e:                 tool_response = {\"error\": f\"Tool execution failed: {type(e).__name__}: {e}\"}              tool_response_parts.append(                 types.Part.from_function_response(                     name=tool_name, response=tool_response                 )             )          contents.append(types.Content(role=\"user\", parts=tool_response_parts))          response = await client.aio.models.generate_content(             model=model,             contents=contents,             config=types.GenerateContentConfig(                 temperature=1.0,                 tools=[tools],             ),         )         contents.append(response.candidates[0].content)      return response  async def run():     async with stdio_client(server_params) as (read, write):         async with ClientSession(read, write) as session:             prompt = \"I want to book an apartment in Paris for 2 nights. 03/28 - 03/30\"             res = await agent_loop(prompt, client, session)             return res  res = await run() print(res.text)`",
    "order": 19,
    "orderInChapter": 4,
    "difficulty": 4,
    "estimatedMinutes": 5,
    "tags": [
      "algorithmsarchitecture",
      "llm"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 851,
      "contentLength": 36958
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#how-mcp-works",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-mcp-vs-api-20",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Model Context Protocol (MCP)",
    "title": "MCP vs. API",
    "subtitle": "Model Context Protocol (MCP)",
    "contentHtml": "<ul>\n  <li><strong>Single Protocol:</strong> Traditional APIs require distinct authentication and integration for each service, akin to managing multiple keys for different locks. This approach increases complexity, maintenance overhead, and integration effort for developers. MCP acts as a standardized “connector,” meaning integrating one MCP provides potential access to multiple tools and services. The following figure (<a href=\"https://norahsakal.com/blog/mcp-vs-api-model-context-protocol-explained/\">source</a>) illustrates how traditional APIs require separate integrations for each tool, whereas MCP provides a unified, standardized connection for seamless interoperability.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/agents/mcp_apikey.png\" alt=\"Placeholder for The Model Context Protocol (MCP) Image\"></p>\n<ul>\n  <li><strong>Dynamic Discovery:</strong> MCP allows AI models to dynamically discover and interact with available tools without requiring hard-coded knowledge of each integration.</li>\n  <li><strong>Two-Way Communication:</strong> MCP supports persistent, real-time two-way communication, similar to WebSockets. The AI model can both retrieve information and trigger actions dynamically.</li>\n</ul>\n<h4 id=\"comparative-analysis\">Comparative Analysis</h4>\n<div align=\"center\">  \n<table class=\"tg\">  \n<thead>  \n<tr>  \n<th class=\"tg-hcenter-valign-first\"><strong>Feature</strong></th>  \n<th class=\"tg-hcenter-valign-first\"><strong>MCP</strong></th>  \n<th class=\"tg-hcenter-valign-second\"><strong>Traditional API</strong></th>  \n</tr>  \n</thead>  \n<tbody>  \n<tr>  \n<td class=\"tg-tleft-valign-first\">Integration Effort</td>  \n<td class=\"tg-tleft-valign-first\">Single, standardized integration</td>  \n<td class=\"tg-tleft-valign-second\">Separate integration per API</td>  \n</tr>  \n<tr>  \n<td class=\"tg-tleft-valign-first\">Real-Time Communication</td>  \n<td class=\"tg-tleft-valign-first\">✅ Yes</td>  \n<td class=\"tg-tleft-valign-second\">❌ No</td>  \n</tr>  \n<tr>  \n<td class=\"tg-tleft-valign-first\">Dynamic Discovery</td>  \n<td class=\"tg-tleft-valign-first\">✅ Yes</td>  \n<td class=\"tg-tleft-valign-second\">❌ No</td>  \n</tr>  \n<tr>  \n<td class=\"tg-tleft-valign-first\">Scalability</td>  \n<td class=\"tg-tleft-valign-first\">Easy (plug-and-play)</td>  \n<td class=\"tg-tleft-valign-second\">Requires additional integrations</td>  \n</tr>  \n<tr>  \n<td class=\"tg-tleft-valign-first\">Security &amp; Control</td>  \n<td class=\"tg-tleft-valign-first\">Consistent across tools</td>  \n<td class=\"tg-tleft-valign-second\">Varies by API</td>  \n</tr>  \n<tr>  \n<td class=\"tg-tleft-valign-first\">Two-Way Communication</td>  \n<td class=\"tg-tleft-valign-first\">✅ Yes</td>  \n<td class=\"tg-tleft-valign-second\">❌ No</td>  \n</tr>  \n</tbody>  \n</table>  \n</div>\n<table class=\"tg\">  \n<thead>  \n<tr>  \n<th class=\"tg-hcenter-valign-first\"><strong>Feature</strong></th>  \n<th class=\"tg-hcenter-valign-first\"><strong>MCP</strong></th>  \n<th class=\"tg-hcenter-valign-second\"><strong>Traditional API</strong></th>  \n</tr>  \n</thead>  \n<tbody>  \n<tr>  \n<td class=\"tg-tleft-valign-first\">Integration Effort</td>  \n<td class=\"tg-tleft-valign-first\">Single, standardized integration</td>  \n<td class=\"tg-tleft-valign-second\">Separate integration per API</td>  \n</tr>  \n<tr>  \n<td class=\"tg-tleft-valign-first\">Real-Time Communication</td>  \n<td class=\"tg-tleft-valign-first\">✅ Yes</td>  \n<td class=\"tg-tleft-valign-second\">❌ No</td>  \n</tr>  \n<tr>  \n<td class=\"tg-tleft-valign-first\">Dynamic Discovery</td>  \n<td class=\"tg-tleft-valign-first\">✅ Yes</td>  \n<td class=\"tg-tleft-valign-second\">❌ No</td>  \n</tr>  \n<tr>  \n<td class=\"tg-tleft-valign-first\">Scalability</td>  \n<td class=\"tg-tleft-valign-first\">Easy (plug-and-play)</td>  \n<td class=\"tg-tleft-valign-second\">Requires additional integrations</td>  \n</tr>  \n<tr>  \n<td class=\"tg-tleft-valign-first\">Security &amp; Control</td>  \n<td class=\"tg-tleft-valign-first\">Consistent across tools</td>  \n<td class=\"tg-tleft-valign-second\">Varies by API</td>  \n</tr>  \n<tr>  \n<td class=\"tg-tleft-valign-first\">Two-Way Communication</td>  \n<td class=\"tg-tleft-valign-first\">✅ Yes</td>  \n<td class=\"tg-tleft-valign-second\">❌ No</td>  \n</tr>  \n</tbody>  \n</table>\n<h4 id=\"when-to-use-mcp-vs-traditional-apis\">When to Use MCP vs. Traditional APIs</h4>\n<ul>\n  <li><strong>Use MCP when:</strong>\n    <ul>\n      <li>You need flexible, AI-native integration with multiple tools and data sources.</li>\n      <li>Your AI model benefits from real-time capability discovery.</li>\n      <li>You want a scalable, standardized way to manage AI interactions.</li>\n    </ul>\n  </li>\n  <li><strong>Stick with traditional APIs when:</strong>\n    <ul>\n      <li>Your use case requires precise, deterministic behavior with strict limits.</li>\n      <li>You need maximum predictability with minimal context autonomy.</li>\n      <li>Fine-grained control over interactions is essential.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>You need flexible, AI-native integration with multiple tools and data sources.</li>\n      <li>Your AI model benefits from real-time capability discovery.</li>\n      <li>You want a scalable, standardized way to manage AI interactions.</li>\n    </ul>\n<ul>\n      <li>Your use case requires precise, deterministic behavior with strict limits.</li>\n      <li>You need maximum predictability with minimal context autonomy.</li>\n      <li>Fine-grained control over interactions is essential.</li>\n    </ul>",
    "contentMarkdown": "*   **Single Protocol:** Traditional APIs require distinct authentication and integration for each service, akin to managing multiple keys for different locks. This approach increases complexity, maintenance overhead, and integration effort for developers. MCP acts as a standardized “connector,” meaning integrating one MCP provides potential access to multiple tools and services. The following figure ([source](https://norahsakal.com/blog/mcp-vs-api-model-context-protocol-explained/)) illustrates how traditional APIs require separate integrations for each tool, whereas MCP provides a unified, standardized connection for seamless interoperability.\n\n![Placeholder for The Model Context Protocol (MCP) Image](/primers/ai/assets/agents/mcp_apikey.png)\n\n*   **Dynamic Discovery:** MCP allows AI models to dynamically discover and interact with available tools without requiring hard-coded knowledge of each integration.\n*   **Two-Way Communication:** MCP supports persistent, real-time two-way communication, similar to WebSockets. The AI model can both retrieve information and trigger actions dynamically.\n\n#### Comparative Analysis\n\n**Feature**\n\n**MCP**\n\n**Traditional API**\n\nIntegration Effort\n\nSingle, standardized integration\n\nSeparate integration per API\n\nReal-Time Communication\n\n✅ Yes\n\n❌ No\n\nDynamic Discovery\n\n✅ Yes\n\n❌ No\n\nScalability\n\nEasy (plug-and-play)\n\nRequires additional integrations\n\nSecurity & Control\n\nConsistent across tools\n\nVaries by API\n\nTwo-Way Communication\n\n✅ Yes\n\n❌ No\n\n**Feature**\n\n**MCP**\n\n**Traditional API**\n\nIntegration Effort\n\nSingle, standardized integration\n\nSeparate integration per API\n\nReal-Time Communication\n\n✅ Yes\n\n❌ No\n\nDynamic Discovery\n\n✅ Yes\n\n❌ No\n\nScalability\n\nEasy (plug-and-play)\n\nRequires additional integrations\n\nSecurity & Control\n\nConsistent across tools\n\nVaries by API\n\nTwo-Way Communication\n\n✅ Yes\n\n❌ No\n\n#### When to Use MCP vs. Traditional APIs\n\n*   **Use MCP when:**\n    *   You need flexible, AI-native integration with multiple tools and data sources.\n    *   Your AI model benefits from real-time capability discovery.\n    *   You want a scalable, standardized way to manage AI interactions.\n*   **Stick with traditional APIs when:**\n    *   Your use case requires precise, deterministic behavior with strict limits.\n    *   You need maximum predictability with minimal context autonomy.\n    *   Fine-grained control over interactions is essential.\n\n*   You need flexible, AI-native integration with multiple tools and data sources.\n*   Your AI model benefits from real-time capability discovery.\n*   You want a scalable, standardized way to manage AI interactions.\n\n*   Your use case requires precise, deterministic behavior with strict limits.\n*   You need maximum predictability with minimal context autonomy.\n*   Fine-grained control over interactions is essential.",
    "order": 20,
    "orderInChapter": 5,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 358,
      "contentLength": 5425
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#mcp-vs.-api",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-security-updates-and-authentication-21",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Model Context Protocol (MCP)",
    "title": "Security, Updates, and Authentication",
    "subtitle": "Model Context Protocol (MCP)",
    "contentHtml": "<ul>\n  <li>MCP is a living protocol, actively evolving. Key updates include:\n    <ul>\n      <li><strong>OAuth 2.1 Authentication:</strong> The protocol mandates OAuth 2.1 for secure HTTP server authentication.</li>\n      <li><strong>Transport Upgrades:</strong> A shift from SSE to Streamable HTTP improves efficiency and enables JSON-RPC batching.</li>\n      <li><strong>Richer Tool Metadata:</strong> Tools now support annotations such as side-effect classification (read-only vs. write operations), helping AI reason better about tool usage.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>OAuth 2.1 Authentication:</strong> The protocol mandates OAuth 2.1 for secure HTTP server authentication.</li>\n      <li><strong>Transport Upgrades:</strong> A shift from SSE to Streamable HTTP improves efficiency and enables JSON-RPC batching.</li>\n      <li><strong>Richer Tool Metadata:</strong> Tools now support annotations such as side-effect classification (read-only vs. write operations), helping AI reason better about tool usage.</li>\n    </ul>",
    "contentMarkdown": "*   MCP is a living protocol, actively evolving. Key updates include:\n    *   **OAuth 2.1 Authentication:** The protocol mandates OAuth 2.1 for secure HTTP server authentication.\n    *   **Transport Upgrades:** A shift from SSE to Streamable HTTP improves efficiency and enables JSON-RPC batching.\n    *   **Richer Tool Metadata:** Tools now support annotations such as side-effect classification (read-only vs. write operations), helping AI reason better about tool usage.\n\n*   **OAuth 2.1 Authentication:** The protocol mandates OAuth 2.1 for secure HTTP server authentication.\n*   **Transport Upgrades:** A shift from SSE to Streamable HTTP improves efficiency and enables JSON-RPC batching.\n*   **Richer Tool Metadata:** Tools now support annotations such as side-effect classification (read-only vs. write operations), helping AI reason better about tool usage.",
    "order": 21,
    "orderInChapter": 6,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 117,
      "contentLength": 1052
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#security,-updates,-and-authentication",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-getting-started-with-mcp-high-level-steps-22",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Model Context Protocol (MCP)",
    "title": "Getting Started with MCP: High-Level Steps",
    "subtitle": "Model Context Protocol (MCP)",
    "contentHtml": "<ul>\n  <li>\n    <p>Integrating MCP into an AI-driven workflow involves several key steps:</p>\n\n    <ol>\n      <li><strong>Define Capabilities:</strong> Clearly outline what your MCP server will offer, including available tools, data sources, and functionalities.</li>\n      <li><strong>Implement MCP Layer:</strong> Develop the MCP integration layer by adhering to the standardized MCP protocol specifications to ensure seamless interoperability.</li>\n      <li><strong>Choose Transport Method:</strong> Decide on the communication method between MCP clients and servers, using either local transport (stdio) or remote transport (Server-Sent Events/WebSockets) based on your requirements.</li>\n      <li><strong>Create Resources and Tools:</strong> Develop the specific data sources, services, and functionalities that your MCP server will expose for AI-driven applications.</li>\n      <li><strong>Set Up Clients:</strong> Establish secure and stable connections between MCP servers and clients, ensuring smooth data exchange and integration with AI agents.</li>\n    </ol>\n  </li>\n  <li>\n    <p>By following these steps, developers can create powerful, flexible, and AI-enhanced systems that leverage MCP for dynamic and intelligent interactions with external tools and data sources.</p>\n  </li>\n</ul>\n<p>Integrating MCP into an AI-driven workflow involves several key steps:</p>\n<ol>\n      <li><strong>Define Capabilities:</strong> Clearly outline what your MCP server will offer, including available tools, data sources, and functionalities.</li>\n      <li><strong>Implement MCP Layer:</strong> Develop the MCP integration layer by adhering to the standardized MCP protocol specifications to ensure seamless interoperability.</li>\n      <li><strong>Choose Transport Method:</strong> Decide on the communication method between MCP clients and servers, using either local transport (stdio) or remote transport (Server-Sent Events/WebSockets) based on your requirements.</li>\n      <li><strong>Create Resources and Tools:</strong> Develop the specific data sources, services, and functionalities that your MCP server will expose for AI-driven applications.</li>\n      <li><strong>Set Up Clients:</strong> Establish secure and stable connections between MCP servers and clients, ensuring smooth data exchange and integration with AI agents.</li>\n    </ol>\n<p>By following these steps, developers can create powerful, flexible, and AI-enhanced systems that leverage MCP for dynamic and intelligent interactions with external tools and data sources.</p>",
    "contentMarkdown": "*   Integrating MCP into an AI-driven workflow involves several key steps:\n    \n    1.  **Define Capabilities:** Clearly outline what your MCP server will offer, including available tools, data sources, and functionalities.\n    2.  **Implement MCP Layer:** Develop the MCP integration layer by adhering to the standardized MCP protocol specifications to ensure seamless interoperability.\n    3.  **Choose Transport Method:** Decide on the communication method between MCP clients and servers, using either local transport (stdio) or remote transport (Server-Sent Events/WebSockets) based on your requirements.\n    4.  **Create Resources and Tools:** Develop the specific data sources, services, and functionalities that your MCP server will expose for AI-driven applications.\n    5.  **Set Up Clients:** Establish secure and stable connections between MCP servers and clients, ensuring smooth data exchange and integration with AI agents.\n*   By following these steps, developers can create powerful, flexible, and AI-enhanced systems that leverage MCP for dynamic and intelligent interactions with external tools and data sources.\n    \n\nIntegrating MCP into an AI-driven workflow involves several key steps:\n\n1.  **Define Capabilities:** Clearly outline what your MCP server will offer, including available tools, data sources, and functionalities.\n2.  **Implement MCP Layer:** Develop the MCP integration layer by adhering to the standardized MCP protocol specifications to ensure seamless interoperability.\n3.  **Choose Transport Method:** Decide on the communication method between MCP clients and servers, using either local transport (stdio) or remote transport (Server-Sent Events/WebSockets) based on your requirements.\n4.  **Create Resources and Tools:** Develop the specific data sources, services, and functionalities that your MCP server will expose for AI-driven applications.\n5.  **Set Up Clients:** Establish secure and stable connections between MCP servers and clients, ensuring smooth data exchange and integration with AI agents.\n\nBy following these steps, developers can create powerful, flexible, and AI-enhanced systems that leverage MCP for dynamic and intelligent interactions with external tools and data sources.",
    "order": 22,
    "orderInChapter": 7,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 298,
      "contentLength": 2549
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#getting-started-with-mcp:-high-level-steps",
    "scrapedAt": "2025-12-28T11:47:30.115Z"
  },
  {
    "id": "ai-agents-use-cases-of-mcp-in-real-world-development-scenari-23",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Model Context Protocol (MCP)",
    "title": "Use-Cases of MCP in Real-World Development Scenarios",
    "subtitle": "Model Context Protocol (MCP)",
    "contentHtml": "<ul>\n  <li>Per this Reddit <a href=\"https://www.reddit.com/r/cursor/comments/1j3nnbz/comment/mg1p89i/?context=3&amp;share_id=qcdVjC99AK-08LRsVqsfk&amp;utm_medium=ios_app&amp;utm_name=ioscss&amp;utm_source=share&amp;utm_term=1\">post</a>, MCP is particularly powerful in software development workflows, where engineers often juggle multiple tasks beyond just writing code. From ticket retrieval to automated commits and intelligent pull request creation, MCP eliminates the friction of routine development tasks, allowing engineers to focus on coding rather than administrative overhead.</li>\n  <li>Here’s how MCP streamlines the entire development lifecycle:</li>\n</ul>\n<h4 id=\"automating-feature-development-from-ticket-to-implementation\">Automating Feature Development from Ticket to Implementation</h4>\n<ul>\n  <li>When starting a new feature, developers typically need to reference multiple sources—project management tools for assigned tickets, design files for UI specifications, and documentation for technical requirements. MCP simplifies this process by:\n    <ul>\n      <li>Fetching currently assigned tickets from Jira, GitHub Issues, or Linear, automatically understanding which ticket is being worked on.</li>\n      <li>Extracting relevant requirements from the ticket without manual searching.</li>\n      <li>Retrieving design specifications from Figma, analyzing images and text descriptions to provide necessary dimensions, fonts, and layouts.</li>\n      <li>Allowing an AI-powered coding assistant like Cursor to immediately start implementing the feature based on all retrieved inputs.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Fetching currently assigned tickets from Jira, GitHub Issues, or Linear, automatically understanding which ticket is being worked on.</li>\n      <li>Extracting relevant requirements from the ticket without manual searching.</li>\n      <li>Retrieving design specifications from Figma, analyzing images and text descriptions to provide necessary dimensions, fonts, and layouts.</li>\n      <li>Allowing an AI-powered coding assistant like Cursor to immediately start implementing the feature based on all retrieved inputs.</li>\n    </ul>\n<h4 id=\"intelligent-ticket-and-task-management\">Intelligent Ticket and Task Management</h4>\n<ul>\n  <li>During implementation, developers often discover inconsistencies or missing details in tickets. Instead of manually updating them, MCP-enabled servers allow:\n    <ul>\n      <li>Cursor to update tickets dynamically, correct issues, and create new subtasks when necessary.</li>\n      <li>Auto-detection of missing requirements and automatic messaging to the ticket’s author for clarifications.</li>\n      <li>Seamless linking of related issues and tasks across different project management systems.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Cursor to update tickets dynamically, correct issues, and create new subtasks when necessary.</li>\n      <li>Auto-detection of missing requirements and automatic messaging to the ticket’s author for clarifications.</li>\n      <li>Seamless linking of related issues and tasks across different project management systems.</li>\n    </ul>\n<h4 id=\"automated-communication-with-relevant-stakeholders\">Automated Communication with Relevant Stakeholders</h4>\n<ul>\n  <li>If questions arise about the ticket or design, MCP servers determine the relevant author (e.g., a designer or product manager) and send automated messages via Slack or other communication tools. This eliminates delays caused by manually tracking down stakeholders.</li>\n</ul>\n<h4 id=\"smart-staging-commit-messages-and-pr-creation\">Smart Staging, Commit Messages, and PR Creation</h4>\n<ul>\n  <li>Once development is complete, MCP enhances the process of committing code and opening pull requests by:\n    <ul>\n      <li>Staging all modified files and generating an intelligent commit message based on the staged changes.</li>\n      <li>Creating a pull request on platforms like GitHub or GitLab, automatically linking it to the corresponding Jira ticket.</li>\n      <li>Adding necessary labels, titles, and descriptions to the PR, ensuring proper documentation without manual intervention.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Staging all modified files and generating an intelligent commit message based on the staged changes.</li>\n      <li>Creating a pull request on platforms like GitHub or GitLab, automatically linking it to the corresponding Jira ticket.</li>\n      <li>Adding necessary labels, titles, and descriptions to the PR, ensuring proper documentation without manual intervention.</li>\n    </ul>\n<h4 id=\"automated-debugging-and-console-log-access\">Automated Debugging and Console Log Access</h4>\n<ul>\n  <li>Developers frequently encounter errors while testing their implementations in a browser. MCP can:\n    <ul>\n      <li>Grant AI assistants real-time access to console logs, so they can analyze issues without requiring manual copy-pasting.</li>\n      <li>Provide automated suggestions or fixes based on detected errors.</li>\n      <li>Iterate on solutions in the background, accelerating the debugging process.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Grant AI assistants real-time access to console logs, so they can analyze issues without requiring manual copy-pasting.</li>\n      <li>Provide automated suggestions or fixes based on detected errors.</li>\n      <li>Iterate on solutions in the background, accelerating the debugging process.</li>\n    </ul>\n<h4 id=\"integration-with-personal-task-management-tools\">Integration with Personal Task Management Tools</h4>\n<ul>\n  <li>Many developers maintain personal to-do lists to track daily tasks. MCP allows seamless synchronization between personal task managers and project tracking tools:\n    <ul>\n      <li>A local to-do app can be MCP-enabled, automatically pulling relevant tickets and work items.</li>\n      <li>Developers can use natural language to prioritize tasks, delegate them, or get reminders.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>A local to-do app can be MCP-enabled, automatically pulling relevant tickets and work items.</li>\n      <li>Developers can use natural language to prioritize tasks, delegate them, or get reminders.</li>\n    </ul>\n<h4 id=\"automated-project-announcements\">Automated Project Announcements</h4>\n<ul>\n  <li>After completing a feature, developers often need to announce it to the team. MCP can:\n    <ul>\n      <li>Automatically draft and send announcements in Slack, summarizing the completed work.</li>\n      <li>Include links to the PR, Jira ticket, and relevant documentation.</li>\n      <li>Ensure communication is structured, timely, and contains all necessary context.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Automatically draft and send announcements in Slack, summarizing the completed work.</li>\n      <li>Include links to the PR, Jira ticket, and relevant documentation.</li>\n      <li>Ensure communication is structured, timely, and contains all necessary context.</li>\n    </ul>",
    "contentMarkdown": "*   Per this Reddit [post](https://www.reddit.com/r/cursor/comments/1j3nnbz/comment/mg1p89i/?context=3&share_id=qcdVjC99AK-08LRsVqsfk&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1), MCP is particularly powerful in software development workflows, where engineers often juggle multiple tasks beyond just writing code. From ticket retrieval to automated commits and intelligent pull request creation, MCP eliminates the friction of routine development tasks, allowing engineers to focus on coding rather than administrative overhead.\n*   Here’s how MCP streamlines the entire development lifecycle:\n\n#### Automating Feature Development from Ticket to Implementation\n\n*   When starting a new feature, developers typically need to reference multiple sources—project management tools for assigned tickets, design files for UI specifications, and documentation for technical requirements. MCP simplifies this process by:\n    *   Fetching currently assigned tickets from Jira, GitHub Issues, or Linear, automatically understanding which ticket is being worked on.\n    *   Extracting relevant requirements from the ticket without manual searching.\n    *   Retrieving design specifications from Figma, analyzing images and text descriptions to provide necessary dimensions, fonts, and layouts.\n    *   Allowing an AI-powered coding assistant like Cursor to immediately start implementing the feature based on all retrieved inputs.\n\n*   Fetching currently assigned tickets from Jira, GitHub Issues, or Linear, automatically understanding which ticket is being worked on.\n*   Extracting relevant requirements from the ticket without manual searching.\n*   Retrieving design specifications from Figma, analyzing images and text descriptions to provide necessary dimensions, fonts, and layouts.\n*   Allowing an AI-powered coding assistant like Cursor to immediately start implementing the feature based on all retrieved inputs.\n\n#### Intelligent Ticket and Task Management\n\n*   During implementation, developers often discover inconsistencies or missing details in tickets. Instead of manually updating them, MCP-enabled servers allow:\n    *   Cursor to update tickets dynamically, correct issues, and create new subtasks when necessary.\n    *   Auto-detection of missing requirements and automatic messaging to the ticket’s author for clarifications.\n    *   Seamless linking of related issues and tasks across different project management systems.\n\n*   Cursor to update tickets dynamically, correct issues, and create new subtasks when necessary.\n*   Auto-detection of missing requirements and automatic messaging to the ticket’s author for clarifications.\n*   Seamless linking of related issues and tasks across different project management systems.\n\n#### Automated Communication with Relevant Stakeholders\n\n*   If questions arise about the ticket or design, MCP servers determine the relevant author (e.g., a designer or product manager) and send automated messages via Slack or other communication tools. This eliminates delays caused by manually tracking down stakeholders.\n\n#### Smart Staging, Commit Messages, and PR Creation\n\n*   Once development is complete, MCP enhances the process of committing code and opening pull requests by:\n    *   Staging all modified files and generating an intelligent commit message based on the staged changes.\n    *   Creating a pull request on platforms like GitHub or GitLab, automatically linking it to the corresponding Jira ticket.\n    *   Adding necessary labels, titles, and descriptions to the PR, ensuring proper documentation without manual intervention.\n\n*   Staging all modified files and generating an intelligent commit message based on the staged changes.\n*   Creating a pull request on platforms like GitHub or GitLab, automatically linking it to the corresponding Jira ticket.\n*   Adding necessary labels, titles, and descriptions to the PR, ensuring proper documentation without manual intervention.\n\n#### Automated Debugging and Console Log Access\n\n*   Developers frequently encounter errors while testing their implementations in a browser. MCP can:\n    *   Grant AI assistants real-time access to console logs, so they can analyze issues without requiring manual copy-pasting.\n    *   Provide automated suggestions or fixes based on detected errors.\n    *   Iterate on solutions in the background, accelerating the debugging process.\n\n*   Grant AI assistants real-time access to console logs, so they can analyze issues without requiring manual copy-pasting.\n*   Provide automated suggestions or fixes based on detected errors.\n*   Iterate on solutions in the background, accelerating the debugging process.\n\n#### Integration with Personal Task Management Tools\n\n*   Many developers maintain personal to-do lists to track daily tasks. MCP allows seamless synchronization between personal task managers and project tracking tools:\n    *   A local to-do app can be MCP-enabled, automatically pulling relevant tickets and work items.\n    *   Developers can use natural language to prioritize tasks, delegate them, or get reminders.\n\n*   A local to-do app can be MCP-enabled, automatically pulling relevant tickets and work items.\n*   Developers can use natural language to prioritize tasks, delegate them, or get reminders.\n\n#### Automated Project Announcements\n\n*   After completing a feature, developers often need to announce it to the team. MCP can:\n    *   Automatically draft and send announcements in Slack, summarizing the completed work.\n    *   Include links to the PR, Jira ticket, and relevant documentation.\n    *   Ensure communication is structured, timely, and contains all necessary context.\n\n*   Automatically draft and send announcements in Slack, summarizing the completed work.\n*   Include links to the PR, Jira ticket, and relevant documentation.\n*   Ensure communication is structured, timely, and contains all necessary context.",
    "order": 23,
    "orderInChapter": 8,
    "difficulty": 3,
    "estimatedMinutes": 4,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 790,
      "contentLength": 7005
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#use-cases-of-mcp-in-real-world-development-scenarios",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-mcp-servers-list-24",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Model Context Protocol (MCP)",
    "title": "MCP Servers List",
    "subtitle": "Model Context Protocol (MCP)",
    "contentHtml": "<ul>\n  <li>A curated list of ready-to-use and community-maintained servers:</li>\n</ul>\n<h4 id=\"awesome-mcp-servers\"><a href=\"https://github.com/punkpeye/awesome-mcp-servers\">Awesome MCP Servers</a></h4>\n<ul>\n  <li>A curated list of MCP servers that enable AI models to securely interact with local and remote resources through standardized implementations.</li>\n</ul>\n<h4 id=\"model-context-protocol-servers\"><a href=\"https://github.com/modelcontextprotocol/servers\">Model Context Protocol Servers</a></h4>\n<ul>\n  <li>A collection of reference implementations for the Model Context Protocol (MCP), showcasing how Large Language Models (LLMs) can securely access tools and data sources.</li>\n</ul>\n<h4 id=\"composio-mcp-servers\"><a href=\"https://mcp.composio.dev/\">Composio MCP Servers</a></h4>\n<ul>\n  <li>Composio offers over 100 fully managed MCP server implementations with built-in authentication, facilitating seamless integration of AI agents and LLMs with various tools and services.</li>\n</ul>\n<h4 id=\"smithery\"><a href=\"https://smithery.ai/\">Smithery</a></h4>\n<ul>\n  <li>Smithery is a platform to help developers find and ship agentic services that follow the Model Context Protocols (MCP) specification.</li>\n  <li>Smithery’s mission is to make agentic services accessible and accelerate the development of agentic AI.</li>\n</ul>",
    "contentMarkdown": "*   A curated list of ready-to-use and community-maintained servers:\n\n#### [Awesome MCP Servers](https://github.com/punkpeye/awesome-mcp-servers)\n\n*   A curated list of MCP servers that enable AI models to securely interact with local and remote resources through standardized implementations.\n\n#### [Model Context Protocol Servers](https://github.com/modelcontextprotocol/servers)\n\n*   A collection of reference implementations for the Model Context Protocol (MCP), showcasing how Large Language Models (LLMs) can securely access tools and data sources.\n\n#### [Composio MCP Servers](https://mcp.composio.dev/)\n\n*   Composio offers over 100 fully managed MCP server implementations with built-in authentication, facilitating seamless integration of AI agents and LLMs with various tools and services.\n\n#### [Smithery](https://smithery.ai/)\n\n*   Smithery is a platform to help developers find and ship agentic services that follow the Model Context Protocols (MCP) specification.\n*   Smithery’s mission is to make agentic services accessible and accelerate the development of agentic AI.",
    "order": 24,
    "orderInChapter": 9,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 134,
      "contentLength": 1336
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#mcp-servers-list",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-a2a-design-principles-25",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agent2Agent (A2A) Protocol",
    "title": "A2A Design Principles",
    "subtitle": "Agent2Agent (A2A) Protocol",
    "contentHtml": "<ul>\n  <li>\n    <p>The protocol’s architecture is grounded in five key principles:</p>\n\n    <ol>\n      <li><strong>Embrace Agentic Capabilities</strong>\n        <ul>\n          <li>A2A supports agents operating in unstructured, autonomous modes—even when agents do not share memory or toolsets. This enables robust, tool-agnostic agent collaboration that respects the diversity of agent implementations and modalities.</li>\n        </ul>\n      </li>\n      <li><strong>Build on Existing Standards</strong>\n        <ul>\n          <li>A2A leverages widely used, mature web technologies such as:\n            <ul>\n              <li>HTTP for transport</li>\n              <li>Server-Sent Events (SSE) for real-time messaging</li>\n              <li>JSON-RPC for structured, language-agnostic function calls</li>\n            </ul>\n          </li>\n          <li>This alignment ensures compatibility with existing enterprise systems and lowers the barrier to adoption.</li>\n        </ul>\n      </li>\n      <li><strong>Secure by Default</strong>\n        <ul>\n          <li>A2A supports enterprise-grade authentication and authorization protocols, achieving parity with OpenAPI authentication schemes (e.g., OAuth2, API keys, bearer tokens). This allows agents to authenticate both themselves and each other within secure, policy-governed environments.</li>\n        </ul>\n      </li>\n      <li><strong>Support for Long-Running Tasks</strong>\n        <ul>\n          <li>A2A is designed to support both ephemeral and extended workflows. Agents can initiate tasks that persist over hours or days, with real-time status updates, notifications, and progress messages shared throughout the lifecycle. This supports human-in-the-loop workflows, asynchronous task completion, and progressive state reporting.</li>\n        </ul>\n      </li>\n      <li><strong>Modality Agnostic</strong>\n        <ul>\n          <li>The protocol supports multiple modalities beyond text, including images, video, and audio. It accommodates multimodal data streams and user experiences by enabling agents to negotiate and exchange content in forms suited to downstream UI capabilities.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n</ul>\n<p>The protocol’s architecture is grounded in five key principles:</p>\n<ol>\n      <li><strong>Embrace Agentic Capabilities</strong>\n        <ul>\n          <li>A2A supports agents operating in unstructured, autonomous modes—even when agents do not share memory or toolsets. This enables robust, tool-agnostic agent collaboration that respects the diversity of agent implementations and modalities.</li>\n        </ul>\n      </li>\n      <li><strong>Build on Existing Standards</strong>\n        <ul>\n          <li>A2A leverages widely used, mature web technologies such as:\n            <ul>\n              <li>HTTP for transport</li>\n              <li>Server-Sent Events (SSE) for real-time messaging</li>\n              <li>JSON-RPC for structured, language-agnostic function calls</li>\n            </ul>\n          </li>\n          <li>This alignment ensures compatibility with existing enterprise systems and lowers the barrier to adoption.</li>\n        </ul>\n      </li>\n      <li><strong>Secure by Default</strong>\n        <ul>\n          <li>A2A supports enterprise-grade authentication and authorization protocols, achieving parity with OpenAPI authentication schemes (e.g., OAuth2, API keys, bearer tokens). This allows agents to authenticate both themselves and each other within secure, policy-governed environments.</li>\n        </ul>\n      </li>\n      <li><strong>Support for Long-Running Tasks</strong>\n        <ul>\n          <li>A2A is designed to support both ephemeral and extended workflows. Agents can initiate tasks that persist over hours or days, with real-time status updates, notifications, and progress messages shared throughout the lifecycle. This supports human-in-the-loop workflows, asynchronous task completion, and progressive state reporting.</li>\n        </ul>\n      </li>\n      <li><strong>Modality Agnostic</strong>\n        <ul>\n          <li>The protocol supports multiple modalities beyond text, including images, video, and audio. It accommodates multimodal data streams and user experiences by enabling agents to negotiate and exchange content in forms suited to downstream UI capabilities.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>A2A supports agents operating in unstructured, autonomous modes—even when agents do not share memory or toolsets. This enables robust, tool-agnostic agent collaboration that respects the diversity of agent implementations and modalities.</li>\n        </ul>\n<ul>\n          <li>A2A leverages widely used, mature web technologies such as:\n            <ul>\n              <li>HTTP for transport</li>\n              <li>Server-Sent Events (SSE) for real-time messaging</li>\n              <li>JSON-RPC for structured, language-agnostic function calls</li>\n            </ul>\n          </li>\n          <li>This alignment ensures compatibility with existing enterprise systems and lowers the barrier to adoption.</li>\n        </ul>\n<ul>\n              <li>HTTP for transport</li>\n              <li>Server-Sent Events (SSE) for real-time messaging</li>\n              <li>JSON-RPC for structured, language-agnostic function calls</li>\n            </ul>\n<ul>\n          <li>A2A supports enterprise-grade authentication and authorization protocols, achieving parity with OpenAPI authentication schemes (e.g., OAuth2, API keys, bearer tokens). This allows agents to authenticate both themselves and each other within secure, policy-governed environments.</li>\n        </ul>\n<ul>\n          <li>A2A is designed to support both ephemeral and extended workflows. Agents can initiate tasks that persist over hours or days, with real-time status updates, notifications, and progress messages shared throughout the lifecycle. This supports human-in-the-loop workflows, asynchronous task completion, and progressive state reporting.</li>\n        </ul>\n<ul>\n          <li>The protocol supports multiple modalities beyond text, including images, video, and audio. It accommodates multimodal data streams and user experiences by enabling agents to negotiate and exchange content in forms suited to downstream UI capabilities.</li>\n        </ul>",
    "contentMarkdown": "*   The protocol’s architecture is grounded in five key principles:\n    \n    1.  **Embrace Agentic Capabilities**\n        *   A2A supports agents operating in unstructured, autonomous modes—even when agents do not share memory or toolsets. This enables robust, tool-agnostic agent collaboration that respects the diversity of agent implementations and modalities.\n    2.  **Build on Existing Standards**\n        *   A2A leverages widely used, mature web technologies such as:\n            *   HTTP for transport\n            *   Server-Sent Events (SSE) for real-time messaging\n            *   JSON-RPC for structured, language-agnostic function calls\n        *   This alignment ensures compatibility with existing enterprise systems and lowers the barrier to adoption.\n    3.  **Secure by Default**\n        *   A2A supports enterprise-grade authentication and authorization protocols, achieving parity with OpenAPI authentication schemes (e.g., OAuth2, API keys, bearer tokens). This allows agents to authenticate both themselves and each other within secure, policy-governed environments.\n    4.  **Support for Long-Running Tasks**\n        *   A2A is designed to support both ephemeral and extended workflows. Agents can initiate tasks that persist over hours or days, with real-time status updates, notifications, and progress messages shared throughout the lifecycle. This supports human-in-the-loop workflows, asynchronous task completion, and progressive state reporting.\n    5.  **Modality Agnostic**\n        *   The protocol supports multiple modalities beyond text, including images, video, and audio. It accommodates multimodal data streams and user experiences by enabling agents to negotiate and exchange content in forms suited to downstream UI capabilities.\n\nThe protocol’s architecture is grounded in five key principles:\n\n1.  **Embrace Agentic Capabilities**\n    *   A2A supports agents operating in unstructured, autonomous modes—even when agents do not share memory or toolsets. This enables robust, tool-agnostic agent collaboration that respects the diversity of agent implementations and modalities.\n2.  **Build on Existing Standards**\n    *   A2A leverages widely used, mature web technologies such as:\n        *   HTTP for transport\n        *   Server-Sent Events (SSE) for real-time messaging\n        *   JSON-RPC for structured, language-agnostic function calls\n    *   This alignment ensures compatibility with existing enterprise systems and lowers the barrier to adoption.\n3.  **Secure by Default**\n    *   A2A supports enterprise-grade authentication and authorization protocols, achieving parity with OpenAPI authentication schemes (e.g., OAuth2, API keys, bearer tokens). This allows agents to authenticate both themselves and each other within secure, policy-governed environments.\n4.  **Support for Long-Running Tasks**\n    *   A2A is designed to support both ephemeral and extended workflows. Agents can initiate tasks that persist over hours or days, with real-time status updates, notifications, and progress messages shared throughout the lifecycle. This supports human-in-the-loop workflows, asynchronous task completion, and progressive state reporting.\n5.  **Modality Agnostic**\n    *   The protocol supports multiple modalities beyond text, including images, video, and audio. It accommodates multimodal data streams and user experiences by enabling agents to negotiate and exchange content in forms suited to downstream UI capabilities.\n\n*   A2A supports agents operating in unstructured, autonomous modes—even when agents do not share memory or toolsets. This enables robust, tool-agnostic agent collaboration that respects the diversity of agent implementations and modalities.\n\n*   A2A leverages widely used, mature web technologies such as:\n    *   HTTP for transport\n    *   Server-Sent Events (SSE) for real-time messaging\n    *   JSON-RPC for structured, language-agnostic function calls\n*   This alignment ensures compatibility with existing enterprise systems and lowers the barrier to adoption.\n\n*   HTTP for transport\n*   Server-Sent Events (SSE) for real-time messaging\n*   JSON-RPC for structured, language-agnostic function calls\n\n*   A2A supports enterprise-grade authentication and authorization protocols, achieving parity with OpenAPI authentication schemes (e.g., OAuth2, API keys, bearer tokens). This allows agents to authenticate both themselves and each other within secure, policy-governed environments.\n\n*   A2A is designed to support both ephemeral and extended workflows. Agents can initiate tasks that persist over hours or days, with real-time status updates, notifications, and progress messages shared throughout the lifecycle. This supports human-in-the-loop workflows, asynchronous task completion, and progressive state reporting.\n\n*   The protocol supports multiple modalities beyond text, including images, video, and audio. It accommodates multimodal data streams and user experiences by enabling agents to negotiate and exchange content in forms suited to downstream UI capabilities.",
    "order": 25,
    "orderInChapter": 1,
    "difficulty": 3,
    "estimatedMinutes": 4,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 646,
      "contentLength": 6293
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#a2a-design-principles",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-how-a2a-works-26",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agent2Agent (A2A) Protocol",
    "title": "How A2A Works",
    "subtitle": "Agent2Agent (A2A) Protocol",
    "contentHtml": "<p><img src=\"/primers/ai/assets/agents/A2A.jpg\" alt=\"\"></p>\n<ul>\n  <li>\n    <p>The illustration above (<a href=\"https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/\">source</a>) offers a visual overview of Agent2Agent communication and depicts the core dynamic between a client agent and a remote agent in an A2A-compliant system. It highlights the secure bidirectional exchange of structured tasks and content, and how mismatches or incompatibilities (like unsupported formats) are negotiated during collaboration.</p>\n  </li>\n  <li>\n    <p>A2A facilitates communication between two primary roles:</p>\n\n    <ul>\n      <li><strong>Client Agent</strong>: Initiates the interaction, discovers the remote agent’s capabilities, and submits task requests.</li>\n      <li><strong>Remote Agent</strong>: Responds to the request, executes tasks, and delivers artifacts or messages back to the client.</li>\n    </ul>\n  </li>\n  <li>\n    <p>This interaction is governed by four key capabilities:</p>\n\n    <ul>\n      <li><strong>Secure Collaboration</strong>: All messages and artifacts exchanged are protected with enterprise-grade authentication and authorization mechanisms.</li>\n      <li><strong>Task and State Management</strong>: Tasks have lifecycle states and can be monitored or updated across long-running workflows.</li>\n      <li><strong>User Experience Negotiation</strong>: Agents can communicate about UI and modality expectations, ensuring compatibility across interfaces.</li>\n      <li><strong>Capability Discovery</strong>: Agents advertise their skills, tools, and modalities using Agent Cards, enabling intelligent routing of tasks.</li>\n    </ul>\n  </li>\n</ul>\n<p>The illustration above (<a href=\"https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/\">source</a>) offers a visual overview of Agent2Agent communication and depicts the core dynamic between a client agent and a remote agent in an A2A-compliant system. It highlights the secure bidirectional exchange of structured tasks and content, and how mismatches or incompatibilities (like unsupported formats) are negotiated during collaboration.</p>\n<p>A2A facilitates communication between two primary roles:</p>\n<ul>\n      <li><strong>Client Agent</strong>: Initiates the interaction, discovers the remote agent’s capabilities, and submits task requests.</li>\n      <li><strong>Remote Agent</strong>: Responds to the request, executes tasks, and delivers artifacts or messages back to the client.</li>\n    </ul>\n<p>This interaction is governed by four key capabilities:</p>\n<ul>\n      <li><strong>Secure Collaboration</strong>: All messages and artifacts exchanged are protected with enterprise-grade authentication and authorization mechanisms.</li>\n      <li><strong>Task and State Management</strong>: Tasks have lifecycle states and can be monitored or updated across long-running workflows.</li>\n      <li><strong>User Experience Negotiation</strong>: Agents can communicate about UI and modality expectations, ensuring compatibility across interfaces.</li>\n      <li><strong>Capability Discovery</strong>: Agents advertise their skills, tools, and modalities using Agent Cards, enabling intelligent routing of tasks.</li>\n    </ul>",
    "contentMarkdown": "![](/primers/ai/assets/agents/A2A.jpg)\n\n*   The illustration above ([source](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)) offers a visual overview of Agent2Agent communication and depicts the core dynamic between a client agent and a remote agent in an A2A-compliant system. It highlights the secure bidirectional exchange of structured tasks and content, and how mismatches or incompatibilities (like unsupported formats) are negotiated during collaboration.\n    \n*   A2A facilitates communication between two primary roles:\n    \n    *   **Client Agent**: Initiates the interaction, discovers the remote agent’s capabilities, and submits task requests.\n    *   **Remote Agent**: Responds to the request, executes tasks, and delivers artifacts or messages back to the client.\n*   This interaction is governed by four key capabilities:\n    \n    *   **Secure Collaboration**: All messages and artifacts exchanged are protected with enterprise-grade authentication and authorization mechanisms.\n    *   **Task and State Management**: Tasks have lifecycle states and can be monitored or updated across long-running workflows.\n    *   **User Experience Negotiation**: Agents can communicate about UI and modality expectations, ensuring compatibility across interfaces.\n    *   **Capability Discovery**: Agents advertise their skills, tools, and modalities using Agent Cards, enabling intelligent routing of tasks.\n\nThe illustration above ([source](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)) offers a visual overview of Agent2Agent communication and depicts the core dynamic between a client agent and a remote agent in an A2A-compliant system. It highlights the secure bidirectional exchange of structured tasks and content, and how mismatches or incompatibilities (like unsupported formats) are negotiated during collaboration.\n\nA2A facilitates communication between two primary roles:\n\n*   **Client Agent**: Initiates the interaction, discovers the remote agent’s capabilities, and submits task requests.\n*   **Remote Agent**: Responds to the request, executes tasks, and delivers artifacts or messages back to the client.\n\nThis interaction is governed by four key capabilities:\n\n*   **Secure Collaboration**: All messages and artifacts exchanged are protected with enterprise-grade authentication and authorization mechanisms.\n*   **Task and State Management**: Tasks have lifecycle states and can be monitored or updated across long-running workflows.\n*   **User Experience Negotiation**: Agents can communicate about UI and modality expectations, ensuring compatibility across interfaces.\n*   **Capability Discovery**: Agents advertise their skills, tools, and modalities using Agent Cards, enabling intelligent routing of tasks.",
    "order": 26,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 338,
      "contentLength": 3255
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#how-a2a-works",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-core-protocol-mechanics-27",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agent2Agent (A2A) Protocol",
    "title": "Core Protocol Mechanics",
    "subtitle": "Agent2Agent (A2A) Protocol",
    "contentHtml": "<h4 id=\"capability-discovery\">Capability Discovery</h4>\n<ul>\n  <li>Every A2A-compliant agent publishes a JSON-based Agent Card, advertising its available capabilities, supported modalities, and service endpoints.</li>\n  <li>This enables the client agent to:\n    <ul>\n      <li>Programmatically discover agents relevant to a given task</li>\n      <li>Select remote agents based on advertised functions, tools, or domain expertise</li>\n    </ul>\n  </li>\n  <li>Agent Cards may include metadata such as:\n    <ul>\n      <li>Versioning info</li>\n      <li>Contact or fallback methods</li>\n      <li>Supported content types (e.g., <code class=\"language-plaintext highlighter-rouge\">text/plain</code>, <code class=\"language-plaintext highlighter-rouge\">image/png</code>, <code class=\"language-plaintext highlighter-rouge\">video/webm</code>)</li>\n      <li>Trust level or authentication requirements</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Programmatically discover agents relevant to a given task</li>\n      <li>Select remote agents based on advertised functions, tools, or domain expertise</li>\n    </ul>\n<ul>\n      <li>Versioning info</li>\n      <li>Contact or fallback methods</li>\n      <li>Supported content types (e.g., <code class=\"language-plaintext highlighter-rouge\">text/plain</code>, <code class=\"language-plaintext highlighter-rouge\">image/png</code>, <code class=\"language-plaintext highlighter-rouge\">video/webm</code>)</li>\n      <li>Trust level or authentication requirements</li>\n    </ul>\n<h4 id=\"task-management\">Task Management</h4>\n<ul>\n  <li>The core of the A2A interaction is the task object, a structured entity that captures:\n    <ul>\n      <li>Task description and parameters</li>\n      <li>Required capabilities or desired outputs</li>\n      <li>Task lifecycle state (created, in-progress, completed, failed)</li>\n      <li>Timestamps, identifiers, and optional user metadata</li>\n    </ul>\n  </li>\n  <li>Agents coordinate on task status updates through SSE or HTTP polling, ensuring synchronized task execution over long durations.</li>\n  <li>Upon task completion, the remote agent emits one or more artifacts, which are structured outputs (e.g., a file, message, API call result).</li>\n</ul>\n<ul>\n      <li>Task description and parameters</li>\n      <li>Required capabilities or desired outputs</li>\n      <li>Task lifecycle state (created, in-progress, completed, failed)</li>\n      <li>Timestamps, identifiers, and optional user metadata</li>\n    </ul>\n<h4 id=\"collaboration-and-messaging\">Collaboration and Messaging</h4>\n<ul>\n  <li>Agents exchange context, interim results, clarifications, and feedback via message objects. These messages may contain:\n    <ul>\n      <li>Plain or rich content (e.g., structured JSON, UI-renderable components)</li>\n      <li>Contextual replies or updates (e.g., clarification requests, validation errors)</li>\n      <li>User or agent instructions</li>\n    </ul>\n  </li>\n  <li>All messages are delivered using SSE or HTTP endpoints, ensuring timely delivery and minimal overhead.</li>\n</ul>\n<ul>\n      <li>Plain or rich content (e.g., structured JSON, UI-renderable components)</li>\n      <li>Contextual replies or updates (e.g., clarification requests, validation errors)</li>\n      <li>User or agent instructions</li>\n    </ul>\n<h4 id=\"user-experience-ux-negotiation\">User Experience (UX) Negotiation</h4>\n<ul>\n  <li>Each message can contain multiple parts, each with a specified content type.</li>\n  <li>Example parts might include:\n    <ul>\n      <li>A generated image (<code class=\"language-plaintext highlighter-rouge\">image/png</code>)</li>\n      <li>A previewable webpage (<code class=\"language-plaintext highlighter-rouge\">text/html</code>)</li>\n      <li>An embeddable iframe (<code class=\"language-plaintext highlighter-rouge\">application/vnd.ui.iframe</code>)</li>\n    </ul>\n  </li>\n  <li>Client and remote agents negotiate these parts to align with UI capabilities of the requesting environment, ensuring that output is rendered in a format compatible with the user interface—whether it’s a chat window, dashboard, or mobile device.</li>\n</ul>\n<ul>\n      <li>A generated image (<code class=\"language-plaintext highlighter-rouge\">image/png</code>)</li>\n      <li>A previewable webpage (<code class=\"language-plaintext highlighter-rouge\">text/html</code>)</li>\n      <li>An embeddable iframe (<code class=\"language-plaintext highlighter-rouge\">application/vnd.ui.iframe</code>)</li>\n    </ul>\n<h4 id=\"content-routing-and-privacy\">Content Routing and Privacy</h4>\n<ul>\n  <li>A2A supports fine-grained control over which agents receive what content. This is essential for:\n    <ul>\n      <li>Multi-agent task orchestration</li>\n      <li>Privacy-preserving collaboration</li>\n      <li>Compliance with enterprise data protection rules</li>\n    </ul>\n  </li>\n  <li>Payloads may be routed through secure proxies, or encrypted per agent-to-agent key exchange.</li>\n</ul>\n<ul>\n      <li>Multi-agent task orchestration</li>\n      <li>Privacy-preserving collaboration</li>\n      <li>Compliance with enterprise data protection rules</li>\n    </ul>",
    "contentMarkdown": "#### Capability Discovery\n\n*   Every A2A-compliant agent publishes a JSON-based Agent Card, advertising its available capabilities, supported modalities, and service endpoints.\n*   This enables the client agent to:\n    *   Programmatically discover agents relevant to a given task\n    *   Select remote agents based on advertised functions, tools, or domain expertise\n*   Agent Cards may include metadata such as:\n    *   Versioning info\n    *   Contact or fallback methods\n    *   Supported content types (e.g., `text/plain`, `image/png`, `video/webm`)\n    *   Trust level or authentication requirements\n\n*   Programmatically discover agents relevant to a given task\n*   Select remote agents based on advertised functions, tools, or domain expertise\n\n*   Versioning info\n*   Contact or fallback methods\n*   Supported content types (e.g., `text/plain`, `image/png`, `video/webm`)\n*   Trust level or authentication requirements\n\n#### Task Management\n\n*   The core of the A2A interaction is the task object, a structured entity that captures:\n    *   Task description and parameters\n    *   Required capabilities or desired outputs\n    *   Task lifecycle state (created, in-progress, completed, failed)\n    *   Timestamps, identifiers, and optional user metadata\n*   Agents coordinate on task status updates through SSE or HTTP polling, ensuring synchronized task execution over long durations.\n*   Upon task completion, the remote agent emits one or more artifacts, which are structured outputs (e.g., a file, message, API call result).\n\n*   Task description and parameters\n*   Required capabilities or desired outputs\n*   Task lifecycle state (created, in-progress, completed, failed)\n*   Timestamps, identifiers, and optional user metadata\n\n#### Collaboration and Messaging\n\n*   Agents exchange context, interim results, clarifications, and feedback via message objects. These messages may contain:\n    *   Plain or rich content (e.g., structured JSON, UI-renderable components)\n    *   Contextual replies or updates (e.g., clarification requests, validation errors)\n    *   User or agent instructions\n*   All messages are delivered using SSE or HTTP endpoints, ensuring timely delivery and minimal overhead.\n\n*   Plain or rich content (e.g., structured JSON, UI-renderable components)\n*   Contextual replies or updates (e.g., clarification requests, validation errors)\n*   User or agent instructions\n\n#### User Experience (UX) Negotiation\n\n*   Each message can contain multiple parts, each with a specified content type.\n*   Example parts might include:\n    *   A generated image (`image/png`)\n    *   A previewable webpage (`text/html`)\n    *   An embeddable iframe (`application/vnd.ui.iframe`)\n*   Client and remote agents negotiate these parts to align with UI capabilities of the requesting environment, ensuring that output is rendered in a format compatible with the user interface—whether it’s a chat window, dashboard, or mobile device.\n\n*   A generated image (`image/png`)\n*   A previewable webpage (`text/html`)\n*   An embeddable iframe (`application/vnd.ui.iframe`)\n\n#### Content Routing and Privacy\n\n*   A2A supports fine-grained control over which agents receive what content. This is essential for:\n    *   Multi-agent task orchestration\n    *   Privacy-preserving collaboration\n    *   Compliance with enterprise data protection rules\n*   Payloads may be routed through secure proxies, or encrypted per agent-to-agent key exchange.\n\n*   Multi-agent task orchestration\n*   Privacy-preserving collaboration\n*   Compliance with enterprise data protection rules",
    "order": 27,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 3,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 474,
      "contentLength": 5111
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#core-protocol-mechanics",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-real-world-scenario-candidate-sourcing-28",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agent2Agent (A2A) Protocol",
    "title": "Real-World Scenario: Candidate Sourcing",
    "subtitle": "Agent2Agent (A2A) Protocol",
    "contentHtml": "<ul>\n  <li>In a hiring workflow, a hiring manager tasks an agent to find suitable engineering candidates. This agent:\n    <ol>\n      <li>Consults the job description and location</li>\n      <li>Uses A2A to identify and query remote agents specialized in resume parsing, candidate databases, or social graph insights</li>\n      <li>Aggregates candidate artifacts and presents them via a unified UI</li>\n      <li>Further agents handle interview scheduling, background checks, or onboarding</li>\n    </ol>\n  </li>\n  <li>This coordinated agent ecosystem—built atop A2A—enables seamless, scalable execution of complex enterprise workflows, combining capabilities from multiple vendors and systems.</li>\n</ul>\n<ol>\n      <li>Consults the job description and location</li>\n      <li>Uses A2A to identify and query remote agents specialized in resume parsing, candidate databases, or social graph insights</li>\n      <li>Aggregates candidate artifacts and presents them via a unified UI</li>\n      <li>Further agents handle interview scheduling, background checks, or onboarding</li>\n    </ol>",
    "contentMarkdown": "*   In a hiring workflow, a hiring manager tasks an agent to find suitable engineering candidates. This agent:\n    1.  Consults the job description and location\n    2.  Uses A2A to identify and query remote agents specialized in resume parsing, candidate databases, or social graph insights\n    3.  Aggregates candidate artifacts and presents them via a unified UI\n    4.  Further agents handle interview scheduling, background checks, or onboarding\n*   This coordinated agent ecosystem—built atop A2A—enables seamless, scalable execution of complex enterprise workflows, combining capabilities from multiple vendors and systems.\n\n1.  Consults the job description and location\n2.  Uses A2A to identify and query remote agents specialized in resume parsing, candidate databases, or social graph insights\n3.  Aggregates candidate artifacts and presents them via a unified UI\n4.  Further agents handle interview scheduling, background checks, or onboarding",
    "order": 28,
    "orderInChapter": 4,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 133,
      "contentLength": 1086
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#real-world-scenario:-candidate-sourcing",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-implementation-architecture-29",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agent2Agent (A2A) Protocol",
    "title": "Implementation Architecture",
    "subtitle": "Agent2Agent (A2A) Protocol",
    "contentHtml": "<ul>\n  <li>\n    <p>A2A-compliant agents are typically composed of the following components:</p>\n\n    <ul>\n      <li><strong>Agent Card Endpoint</strong>\n        <ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">GET /agent/card</code></li>\n          <li>Returns metadata about the agent, capabilities, and supported modalities</li>\n        </ul>\n      </li>\n      <li><strong>Task Endpoint</strong>\n        <ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">POST /agent/tasks</code></li>\n          <li>Accepts new task requests from a client agent</li>\n        </ul>\n      </li>\n      <li><strong>Task State Endpoint</strong>\n        <ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">GET /agent/tasks/{task_id}</code></li>\n          <li>Returns current status of a task (e.g., “in_progress”, “completed”)</li>\n        </ul>\n      </li>\n      <li><strong>Artifact Retrieval Endpoint</strong>\n        <ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">GET /agent/tasks/{task_id}/artifact</code></li>\n          <li>Downloads final output(s) for the task</li>\n        </ul>\n      </li>\n      <li><strong>Message Stream Endpoint</strong>\n        <ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">GET /agent/messages/stream</code></li>\n          <li>Uses SSE to deliver messages (contextual updates, feedback, etc.)</li>\n        </ul>\n      </li>\n      <li><strong>Authentication</strong>\n        <ul>\n          <li>OAuth 2.0 Bearer Tokens or API keys passed in headers</li>\n          <li>Supports mutual TLS or signed requests for sensitive data exchange</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p>A2A-compliant agents are typically composed of the following components:</p>\n<ul>\n      <li><strong>Agent Card Endpoint</strong>\n        <ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">GET /agent/card</code></li>\n          <li>Returns metadata about the agent, capabilities, and supported modalities</li>\n        </ul>\n      </li>\n      <li><strong>Task Endpoint</strong>\n        <ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">POST /agent/tasks</code></li>\n          <li>Accepts new task requests from a client agent</li>\n        </ul>\n      </li>\n      <li><strong>Task State Endpoint</strong>\n        <ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">GET /agent/tasks/{task_id}</code></li>\n          <li>Returns current status of a task (e.g., “in_progress”, “completed”)</li>\n        </ul>\n      </li>\n      <li><strong>Artifact Retrieval Endpoint</strong>\n        <ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">GET /agent/tasks/{task_id}/artifact</code></li>\n          <li>Downloads final output(s) for the task</li>\n        </ul>\n      </li>\n      <li><strong>Message Stream Endpoint</strong>\n        <ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">GET /agent/messages/stream</code></li>\n          <li>Uses SSE to deliver messages (contextual updates, feedback, etc.)</li>\n        </ul>\n      </li>\n      <li><strong>Authentication</strong>\n        <ul>\n          <li>OAuth 2.0 Bearer Tokens or API keys passed in headers</li>\n          <li>Supports mutual TLS or signed requests for sensitive data exchange</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">GET /agent/card</code></li>\n          <li>Returns metadata about the agent, capabilities, and supported modalities</li>\n        </ul>\n<ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">POST /agent/tasks</code></li>\n          <li>Accepts new task requests from a client agent</li>\n        </ul>\n<ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">GET /agent/tasks/{task_id}</code></li>\n          <li>Returns current status of a task (e.g., “in_progress”, “completed”)</li>\n        </ul>\n<ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">GET /agent/tasks/{task_id}/artifact</code></li>\n          <li>Downloads final output(s) for the task</li>\n        </ul>\n<ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">GET /agent/messages/stream</code></li>\n          <li>Uses SSE to deliver messages (contextual updates, feedback, etc.)</li>\n        </ul>\n<ul>\n          <li>OAuth 2.0 Bearer Tokens or API keys passed in headers</li>\n          <li>Supports mutual TLS or signed requests for sensitive data exchange</li>\n        </ul>",
    "contentMarkdown": "*   A2A-compliant agents are typically composed of the following components:\n    \n    *   **Agent Card Endpoint**\n        *   `GET /agent/card`\n        *   Returns metadata about the agent, capabilities, and supported modalities\n    *   **Task Endpoint**\n        *   `POST /agent/tasks`\n        *   Accepts new task requests from a client agent\n    *   **Task State Endpoint**\n        *   `GET /agent/tasks/{task_id}`\n        *   Returns current status of a task (e.g., “in\\_progress”, “completed”)\n    *   **Artifact Retrieval Endpoint**\n        *   `GET /agent/tasks/{task_id}/artifact`\n        *   Downloads final output(s) for the task\n    *   **Message Stream Endpoint**\n        *   `GET /agent/messages/stream`\n        *   Uses SSE to deliver messages (contextual updates, feedback, etc.)\n    *   **Authentication**\n        *   OAuth 2.0 Bearer Tokens or API keys passed in headers\n        *   Supports mutual TLS or signed requests for sensitive data exchange\n\nA2A-compliant agents are typically composed of the following components:\n\n*   **Agent Card Endpoint**\n    *   `GET /agent/card`\n    *   Returns metadata about the agent, capabilities, and supported modalities\n*   **Task Endpoint**\n    *   `POST /agent/tasks`\n    *   Accepts new task requests from a client agent\n*   **Task State Endpoint**\n    *   `GET /agent/tasks/{task_id}`\n    *   Returns current status of a task (e.g., “in\\_progress”, “completed”)\n*   **Artifact Retrieval Endpoint**\n    *   `GET /agent/tasks/{task_id}/artifact`\n    *   Downloads final output(s) for the task\n*   **Message Stream Endpoint**\n    *   `GET /agent/messages/stream`\n    *   Uses SSE to deliver messages (contextual updates, feedback, etc.)\n*   **Authentication**\n    *   OAuth 2.0 Bearer Tokens or API keys passed in headers\n    *   Supports mutual TLS or signed requests for sensitive data exchange\n\n*   `GET /agent/card`\n*   Returns metadata about the agent, capabilities, and supported modalities\n\n*   `POST /agent/tasks`\n*   Accepts new task requests from a client agent\n\n*   `GET /agent/tasks/{task_id}`\n*   Returns current status of a task (e.g., “in\\_progress”, “completed”)\n\n*   `GET /agent/tasks/{task_id}/artifact`\n*   Downloads final output(s) for the task\n\n*   `GET /agent/messages/stream`\n*   Uses SSE to deliver messages (contextual updates, feedback, etc.)\n\n*   OAuth 2.0 Bearer Tokens or API keys passed in headers\n*   Supports mutual TLS or signed requests for sensitive data exchange",
    "order": 29,
    "orderInChapter": 5,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 310,
      "contentLength": 4537
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#implementation-architecture",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-integration-and-future-roadmap-30",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agent2Agent (A2A) Protocol",
    "title": "Integration and Future Roadmap",
    "subtitle": "Agent2Agent (A2A) Protocol",
    "contentHtml": "<ul>\n  <li>A2A is being released as open source, with contributions from major cloud vendors, enterprise software providers, and consulting firms.</li>\n  <li>A full draft specification is available online, with example implementations using:\n    <ul>\n      <li>Python and FastAPI</li>\n      <li>Node.js and Express</li>\n      <li>Langchain-based agents</li>\n    </ul>\n  </li>\n  <li>Development kits, validators, and sandbox agents are available to accelerate adoption and testing.</li>\n  <li>A production-ready version of the protocol is scheduled for release later this year, with backward compatibility and feature expansion focused on:\n    <ul>\n      <li>Peer discovery</li>\n      <li>Intent routing</li>\n      <li>Secure agent marketplaces</li>\n      <li>UI auto-generation from message part schemas</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Python and FastAPI</li>\n      <li>Node.js and Express</li>\n      <li>Langchain-based agents</li>\n    </ul>\n<ul>\n      <li>Peer discovery</li>\n      <li>Intent routing</li>\n      <li>Secure agent marketplaces</li>\n      <li>UI auto-generation from message part schemas</li>\n    </ul>",
    "contentMarkdown": "*   A2A is being released as open source, with contributions from major cloud vendors, enterprise software providers, and consulting firms.\n*   A full draft specification is available online, with example implementations using:\n    *   Python and FastAPI\n    *   Node.js and Express\n    *   Langchain-based agents\n*   Development kits, validators, and sandbox agents are available to accelerate adoption and testing.\n*   A production-ready version of the protocol is scheduled for release later this year, with backward compatibility and feature expansion focused on:\n    *   Peer discovery\n    *   Intent routing\n    *   Secure agent marketplaces\n    *   UI auto-generation from message part schemas\n\n*   Python and FastAPI\n*   Node.js and Express\n*   Langchain-based agents\n\n*   Peer discovery\n*   Intent routing\n*   Secure agent marketplaces\n*   UI auto-generation from message part schemas",
    "order": 30,
    "orderInChapter": 6,
    "difficulty": 3,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 124,
      "contentLength": 1130
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#integration-and-future-roadmap",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-how-agentic-rag-works-31",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Retrieval-Augmented Generation (RAG)",
    "title": "How Agentic RAG Works",
    "subtitle": "Agentic Retrieval-Augmented Generation (RAG)",
    "contentHtml": "<ul>\n  <li>\n    <p>In an agentic RAG system, AI agents play key roles in the retrieval process, using specialized tools to retrieve context-sensitive information. Unlike traditional RAG, where retrieval functions are static, agentic RAG allows dynamic selection and operation of tools based on query requirements. Retrieval agents may utilize tools such as:</p>\n\n    <ol>\n      <li><strong>Vector Search Engines</strong>: Retrieve information from vectorized data in databases.</li>\n      <li><strong>Web Search Tools</strong>: Access live web data for up-to-date, contextually relevant information.</li>\n      <li><strong>Calculators</strong>: Perform computations for queries that require accurate calculation.</li>\n      <li><strong>APIs for Software Programs</strong>: Programmatically retrieve information from applications like email or chat programs to access user-specific data.</li>\n    </ol>\n  </li>\n  <li>\n    <p>In the context of Agentic RAG, the retrieval process is “agentic,” meaning agents are capable of reasoning and decision-making regarding which sources and tools to use, based on the specific requirements of the query. This flexibility elevates their tool usage beyond simple retrieval, allowing for a more dynamic and adaptive response.</p>\n  </li>\n</ul>\n<p>In an agentic RAG system, AI agents play key roles in the retrieval process, using specialized tools to retrieve context-sensitive information. Unlike traditional RAG, where retrieval functions are static, agentic RAG allows dynamic selection and operation of tools based on query requirements. Retrieval agents may utilize tools such as:</p>\n<ol>\n      <li><strong>Vector Search Engines</strong>: Retrieve information from vectorized data in databases.</li>\n      <li><strong>Web Search Tools</strong>: Access live web data for up-to-date, contextually relevant information.</li>\n      <li><strong>Calculators</strong>: Perform computations for queries that require accurate calculation.</li>\n      <li><strong>APIs for Software Programs</strong>: Programmatically retrieve information from applications like email or chat programs to access user-specific data.</li>\n    </ol>\n<p>In the context of Agentic RAG, the retrieval process is “agentic,” meaning agents are capable of reasoning and decision-making regarding which sources and tools to use, based on the specific requirements of the query. This flexibility elevates their tool usage beyond simple retrieval, allowing for a more dynamic and adaptive response.</p>",
    "contentMarkdown": "*   In an agentic RAG system, AI agents play key roles in the retrieval process, using specialized tools to retrieve context-sensitive information. Unlike traditional RAG, where retrieval functions are static, agentic RAG allows dynamic selection and operation of tools based on query requirements. Retrieval agents may utilize tools such as:\n    \n    1.  **Vector Search Engines**: Retrieve information from vectorized data in databases.\n    2.  **Web Search Tools**: Access live web data for up-to-date, contextually relevant information.\n    3.  **Calculators**: Perform computations for queries that require accurate calculation.\n    4.  **APIs for Software Programs**: Programmatically retrieve information from applications like email or chat programs to access user-specific data.\n*   In the context of Agentic RAG, the retrieval process is “agentic,” meaning agents are capable of reasoning and decision-making regarding which sources and tools to use, based on the specific requirements of the query. This flexibility elevates their tool usage beyond simple retrieval, allowing for a more dynamic and adaptive response.\n    \n\nIn an agentic RAG system, AI agents play key roles in the retrieval process, using specialized tools to retrieve context-sensitive information. Unlike traditional RAG, where retrieval functions are static, agentic RAG allows dynamic selection and operation of tools based on query requirements. Retrieval agents may utilize tools such as:\n\n1.  **Vector Search Engines**: Retrieve information from vectorized data in databases.\n2.  **Web Search Tools**: Access live web data for up-to-date, contextually relevant information.\n3.  **Calculators**: Perform computations for queries that require accurate calculation.\n4.  **APIs for Software Programs**: Programmatically retrieve information from applications like email or chat programs to access user-specific data.\n\nIn the context of Agentic RAG, the retrieval process is “agentic,” meaning agents are capable of reasoning and decision-making regarding which sources and tools to use, based on the specific requirements of the query. This flexibility elevates their tool usage beyond simple retrieval, allowing for a more dynamic and adaptive response.",
    "order": 31,
    "orderInChapter": 1,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 308,
      "contentLength": 2503
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#how-agentic-rag-works",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-agentic-decision-making-in-retrieval-32",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Retrieval-Augmented Generation (RAG)",
    "title": "Agentic Decision-Making in Retrieval",
    "subtitle": "Agentic Retrieval-Augmented Generation (RAG)",
    "contentHtml": "<ul>\n  <li>\n    <p>The decision-making process of retrieval agents encompasses several key actions, including:</p>\n\n    <ul>\n      <li><strong>Deciding Whether to Retrieve</strong>: Assessing if additional information is necessary for the query.</li>\n      <li><strong>Choosing the Appropriate Tool</strong>: Selecting the most suitable tool (e.g., a vector search engine or web search) based on the query.</li>\n      <li><strong>Query Formulation</strong>: Refining or rephrasing the query to enhance retrieval accuracy.</li>\n      <li><strong>Evaluating Retrieved Results</strong>: Reviewing the retrieved information to determine sufficiency, and whether further retrieval is needed.</li>\n    </ul>\n  </li>\n</ul>\n<p>The decision-making process of retrieval agents encompasses several key actions, including:</p>\n<ul>\n      <li><strong>Deciding Whether to Retrieve</strong>: Assessing if additional information is necessary for the query.</li>\n      <li><strong>Choosing the Appropriate Tool</strong>: Selecting the most suitable tool (e.g., a vector search engine or web search) based on the query.</li>\n      <li><strong>Query Formulation</strong>: Refining or rephrasing the query to enhance retrieval accuracy.</li>\n      <li><strong>Evaluating Retrieved Results</strong>: Reviewing the retrieved information to determine sufficiency, and whether further retrieval is needed.</li>\n    </ul>",
    "contentMarkdown": "*   The decision-making process of retrieval agents encompasses several key actions, including:\n    \n    *   **Deciding Whether to Retrieve**: Assessing if additional information is necessary for the query.\n    *   **Choosing the Appropriate Tool**: Selecting the most suitable tool (e.g., a vector search engine or web search) based on the query.\n    *   **Query Formulation**: Refining or rephrasing the query to enhance retrieval accuracy.\n    *   **Evaluating Retrieved Results**: Reviewing the retrieved information to determine sufficiency, and whether further retrieval is needed.\n\nThe decision-making process of retrieval agents encompasses several key actions, including:\n\n*   **Deciding Whether to Retrieve**: Assessing if additional information is necessary for the query.\n*   **Choosing the Appropriate Tool**: Selecting the most suitable tool (e.g., a vector search engine or web search) based on the query.\n*   **Query Formulation**: Refining or rephrasing the query to enhance retrieval accuracy.\n*   **Evaluating Retrieved Results**: Reviewing the retrieved information to determine sufficiency, and whether further retrieval is needed.",
    "order": 32,
    "orderInChapter": 2,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 153,
      "contentLength": 1396
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#agentic-decision-making-in-retrieval",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-agentic-rag-architectures-single-agent-vs-multi-ag-33",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Retrieval-Augmented Generation (RAG)",
    "title": "Agentic RAG Architectures: Single-Agent vs. Multi-Agent Systems",
    "subtitle": "Agentic Retrieval-Augmented Generation (RAG)",
    "contentHtml": "<ul>\n  <li>Agentic RAG can be implemented with a single agent or multiple agents, each offering unique strengths.</li>\n</ul>\n<h4 id=\"single-agent-rag-router\">Single-Agent RAG (Router)</h4>\n<ul>\n  <li>The simplest implementation of agentic RAG involves a single agent functioning as a “router.” This agent determines the appropriate source or tool for retrieving information based on the query. The single agent toggles between different options, such as a vector database, web search, or an API. This setup provides a versatile retrieval process, enabling access to multiple data sources beyond a single vector search tool.</li>\n  <li>As shown in the figure below (<a href=\"https://weaviate.io/blog/what-is-agentic-rag\">source</a>), the single-Agent RAG system (router) architecture involves a single agent serving as a “router,” dynamically selecting the best tool or source based on the query, enabling efficient information retrieval across multiple data channels.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/agents/Single_Agent_RAG_System_Router.png\" alt=\"Agentic RAG Visual Summary\"></p>\n<h4 id=\"multi-agent-rag-systems\">Multi-Agent RAG Systems</h4>\n<ul>\n  <li>\n    <p>For more complex queries, multi-agent RAG systems provide additional flexibility. These systems feature a “master agent” that coordinates several specialized retrieval agents, such as:</p>\n\n    <ul>\n      <li><strong>Internal Data Retrieval Agent</strong>: Retrieves information from proprietary, internal databases.</li>\n      <li><strong>Personal Data Retrieval Agent</strong>: Accesses user-specific information, such as emails or chat history.</li>\n      <li><strong>Public Data Retrieval Agent</strong>: Conducts web searches for up-to-date public information.</li>\n    </ul>\n  </li>\n  <li>\n    <p>By utilizing multiple agents tailored to specific sources or tasks, multi-agent RAG systems can deliver comprehensive, accurate responses across diverse channels.</p>\n  </li>\n  <li>\n    <p>As shown in the figure below (<a href=\"https://weaviate.io/blog/what-is-agentic-rag\">source</a>), the multi-agent RAG system architecture utilizes multiple specialized retrieval agents to access different sources and tools, offering a flexible and comprehensive approach to complex queries.</p>\n  </li>\n</ul>\n<p>For more complex queries, multi-agent RAG systems provide additional flexibility. These systems feature a “master agent” that coordinates several specialized retrieval agents, such as:</p>\n<ul>\n      <li><strong>Internal Data Retrieval Agent</strong>: Retrieves information from proprietary, internal databases.</li>\n      <li><strong>Personal Data Retrieval Agent</strong>: Accesses user-specific information, such as emails or chat history.</li>\n      <li><strong>Public Data Retrieval Agent</strong>: Conducts web searches for up-to-date public information.</li>\n    </ul>\n<p>By utilizing multiple agents tailored to specific sources or tasks, multi-agent RAG systems can deliver comprehensive, accurate responses across diverse channels.</p>\n<p>As shown in the figure below (<a href=\"https://weaviate.io/blog/what-is-agentic-rag\">source</a>), the multi-agent RAG system architecture utilizes multiple specialized retrieval agents to access different sources and tools, offering a flexible and comprehensive approach to complex queries.</p>\n<p><img src=\"/primers/ai/assets/agents/Multi_Agent_RAG_System.png\" alt=\"Agentic RAG Visual Summary\"></p>",
    "contentMarkdown": "*   Agentic RAG can be implemented with a single agent or multiple agents, each offering unique strengths.\n\n#### Single-Agent RAG (Router)\n\n*   The simplest implementation of agentic RAG involves a single agent functioning as a “router.” This agent determines the appropriate source or tool for retrieving information based on the query. The single agent toggles between different options, such as a vector database, web search, or an API. This setup provides a versatile retrieval process, enabling access to multiple data sources beyond a single vector search tool.\n*   As shown in the figure below ([source](https://weaviate.io/blog/what-is-agentic-rag)), the single-Agent RAG system (router) architecture involves a single agent serving as a “router,” dynamically selecting the best tool or source based on the query, enabling efficient information retrieval across multiple data channels.\n\n![Agentic RAG Visual Summary](/primers/ai/assets/agents/Single_Agent_RAG_System_Router.png)\n\n#### Multi-Agent RAG Systems\n\n*   For more complex queries, multi-agent RAG systems provide additional flexibility. These systems feature a “master agent” that coordinates several specialized retrieval agents, such as:\n    \n    *   **Internal Data Retrieval Agent**: Retrieves information from proprietary, internal databases.\n    *   **Personal Data Retrieval Agent**: Accesses user-specific information, such as emails or chat history.\n    *   **Public Data Retrieval Agent**: Conducts web searches for up-to-date public information.\n*   By utilizing multiple agents tailored to specific sources or tasks, multi-agent RAG systems can deliver comprehensive, accurate responses across diverse channels.\n    \n*   As shown in the figure below ([source](https://weaviate.io/blog/what-is-agentic-rag)), the multi-agent RAG system architecture utilizes multiple specialized retrieval agents to access different sources and tools, offering a flexible and comprehensive approach to complex queries.\n    \n\nFor more complex queries, multi-agent RAG systems provide additional flexibility. These systems feature a “master agent” that coordinates several specialized retrieval agents, such as:\n\n*   **Internal Data Retrieval Agent**: Retrieves information from proprietary, internal databases.\n*   **Personal Data Retrieval Agent**: Accesses user-specific information, such as emails or chat history.\n*   **Public Data Retrieval Agent**: Conducts web searches for up-to-date public information.\n\nBy utilizing multiple agents tailored to specific sources or tasks, multi-agent RAG systems can deliver comprehensive, accurate responses across diverse channels.\n\nAs shown in the figure below ([source](https://weaviate.io/blog/what-is-agentic-rag)), the multi-agent RAG system architecture utilizes multiple specialized retrieval agents to access different sources and tools, offering a flexible and comprehensive approach to complex queries.\n\n![Agentic RAG Visual Summary](/primers/ai/assets/agents/Multi_Agent_RAG_System.png)",
    "order": 33,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 371,
      "contentLength": 3425
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#agentic-rag-architectures:-single-agent-vs.-multi-agent-systems",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-beyond-retrieval-expanding-agentic-rags-capabiliti-34",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Retrieval-Augmented Generation (RAG)",
    "title": "Beyond Retrieval: Expanding Agentic RAG’s Capabilities",
    "subtitle": "Agentic Retrieval-Augmented Generation (RAG)",
    "contentHtml": "<ul>\n  <li>\n    <p>Agentic RAG systems can incorporate agents for tasks beyond retrieval, including:</p>\n\n    <ul>\n      <li><strong>Validating Information</strong>: Cross-referencing data across sources to ensure accuracy.</li>\n      <li><strong>Performing Multi-step Reasoning</strong>: Following logical steps to address complex queries before generating responses.</li>\n      <li><strong>Updating System Memory</strong>: Tracking and retaining user-specific preferences or past queries, enabling personalized and context-aware responses.</li>\n    </ul>\n  </li>\n  <li>\n    <p>By expanding its capabilities beyond simple retrieval, Agentic RAG delivers a powerful, context-sensitive AI solution capable of handling intricate, real-world applications.</p>\n  </li>\n</ul>\n<p>Agentic RAG systems can incorporate agents for tasks beyond retrieval, including:</p>\n<ul>\n      <li><strong>Validating Information</strong>: Cross-referencing data across sources to ensure accuracy.</li>\n      <li><strong>Performing Multi-step Reasoning</strong>: Following logical steps to address complex queries before generating responses.</li>\n      <li><strong>Updating System Memory</strong>: Tracking and retaining user-specific preferences or past queries, enabling personalized and context-aware responses.</li>\n    </ul>\n<p>By expanding its capabilities beyond simple retrieval, Agentic RAG delivers a powerful, context-sensitive AI solution capable of handling intricate, real-world applications.</p>",
    "contentMarkdown": "*   Agentic RAG systems can incorporate agents for tasks beyond retrieval, including:\n    \n    *   **Validating Information**: Cross-referencing data across sources to ensure accuracy.\n    *   **Performing Multi-step Reasoning**: Following logical steps to address complex queries before generating responses.\n    *   **Updating System Memory**: Tracking and retaining user-specific preferences or past queries, enabling personalized and context-aware responses.\n*   By expanding its capabilities beyond simple retrieval, Agentic RAG delivers a powerful, context-sensitive AI solution capable of handling intricate, real-world applications.\n    \n\nAgentic RAG systems can incorporate agents for tasks beyond retrieval, including:\n\n*   **Validating Information**: Cross-referencing data across sources to ensure accuracy.\n*   **Performing Multi-step Reasoning**: Following logical steps to address complex queries before generating responses.\n*   **Updating System Memory**: Tracking and retaining user-specific preferences or past queries, enabling personalized and context-aware responses.\n\nBy expanding its capabilities beyond simple retrieval, Agentic RAG delivers a powerful, context-sensitive AI solution capable of handling intricate, real-world applications.",
    "order": 34,
    "orderInChapter": 4,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 148,
      "contentLength": 1487
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#beyond-retrieval:-expanding-agentic-rag’s-capabilities",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-agentic-rag-vs-vanilla-rag-key-differences-35",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Retrieval-Augmented Generation (RAG)",
    "title": "Agentic RAG vs. Vanilla RAG: Key Differences",
    "subtitle": "Agentic Retrieval-Augmented Generation (RAG)",
    "contentHtml": "<ul>\n  <li>While both vanilla and agentic RAG systems aim to retrieve information and generate responses, agentic RAG introduces several significant enhancements:</li>\n</ul>\n<div align=\"center\">\n<table class=\"tg\">\n <thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Feature</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Vanilla RAG</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Agentic RAG</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Access to External Tools</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">Yes – Utilizes external tools like vector search engines, web search tools, calculators, and APIs.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Query Pre-processing</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">Yes – Agents dynamically refine, rephrase, and adapt queries for optimized retrieval.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Decision-making in Retrieval</td>\n<td class=\"tg-tleft-valign-first\">Limited to direct retrieval from knowledge base</td>\n<td class=\"tg-tleft-valign-second\">Agents autonomously decide if retrieval is needed, select tools, and adapt based on query complexity and source type.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Multi-step Retrieval Process</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">Yes – Agents perform multi-step, adaptive retrieval processes involving various sources or tool combinations.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Data Validation</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">Yes – Information is cross-referenced across sources to validate accuracy, supporting complex, real-world responses.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Dynamic Tool Selection</td>\n<td class=\"tg-tleft-valign-first\">Static retrieval tools only</td>\n<td class=\"tg-tleft-valign-second\">Dynamic – Agents choose specific tools (e.g., vector search, APIs) based on query needs.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Adaptability to Query</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-second\">Highly adaptive – Agents select and operate tools based on real-time assessment of query requirements.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Types of Agents</td>\n<td class=\"tg-tleft-valign-first\">Not applicable</td>\n<td class=\"tg-tleft-valign-second\">Multiple specialized agents, such as internal data retrieval, personal data retrieval, public data retrieval.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Single-Agent vs. Multi-Agent System</td>\n<td class=\"tg-tleft-valign-first\">Not applicable</td>\n<td class=\"tg-tleft-valign-second\">Single-agent router or multi-agent systems, with “master” and specialized agents for complex queries.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Reasoning and Logic Capability</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">Yes – Supports multi-step reasoning, allowing logical sequence handling before generating responses.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Memory and Personalization</td>\n<td class=\"tg-tleft-valign-first\">Limited to immediate query</td>\n<td class=\"tg-tleft-valign-second\">Yes – Capable of updating memory to retain user preferences or history, allowing personalized responses.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Real-world Applications</td>\n<td class=\"tg-tleft-valign-first\">Primarily static responses from a fixed database</td>\n<td class=\"tg-tleft-valign-second\">Supports a wide range of real-world applications by responding to complex, nuanced inquiries with context sensitivity.</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n <thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Feature</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Vanilla RAG</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Agentic RAG</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Access to External Tools</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">Yes – Utilizes external tools like vector search engines, web search tools, calculators, and APIs.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Query Pre-processing</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">Yes – Agents dynamically refine, rephrase, and adapt queries for optimized retrieval.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Decision-making in Retrieval</td>\n<td class=\"tg-tleft-valign-first\">Limited to direct retrieval from knowledge base</td>\n<td class=\"tg-tleft-valign-second\">Agents autonomously decide if retrieval is needed, select tools, and adapt based on query complexity and source type.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Multi-step Retrieval Process</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">Yes – Agents perform multi-step, adaptive retrieval processes involving various sources or tool combinations.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Data Validation</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">Yes – Information is cross-referenced across sources to validate accuracy, supporting complex, real-world responses.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Dynamic Tool Selection</td>\n<td class=\"tg-tleft-valign-first\">Static retrieval tools only</td>\n<td class=\"tg-tleft-valign-second\">Dynamic – Agents choose specific tools (e.g., vector search, APIs) based on query needs.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Adaptability to Query</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-second\">Highly adaptive – Agents select and operate tools based on real-time assessment of query requirements.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Types of Agents</td>\n<td class=\"tg-tleft-valign-first\">Not applicable</td>\n<td class=\"tg-tleft-valign-second\">Multiple specialized agents, such as internal data retrieval, personal data retrieval, public data retrieval.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Single-Agent vs. Multi-Agent System</td>\n<td class=\"tg-tleft-valign-first\">Not applicable</td>\n<td class=\"tg-tleft-valign-second\">Single-agent router or multi-agent systems, with “master” and specialized agents for complex queries.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Reasoning and Logic Capability</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">Yes – Supports multi-step reasoning, allowing logical sequence handling before generating responses.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Memory and Personalization</td>\n<td class=\"tg-tleft-valign-first\">Limited to immediate query</td>\n<td class=\"tg-tleft-valign-second\">Yes – Capable of updating memory to retain user preferences or history, allowing personalized responses.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Real-world Applications</td>\n<td class=\"tg-tleft-valign-first\">Primarily static responses from a fixed database</td>\n<td class=\"tg-tleft-valign-second\">Supports a wide range of real-world applications by responding to complex, nuanced inquiries with context sensitivity.</td>\n</tr>\n</tbody>\n</table>\n<ul>\n  <li>Drawing a parallel with problem-solving, agentic RAG offers capabilities akin to having a smartphone in hand—equipped with multiple apps and tools to help answer a question—whereas vanilla RAG is akin to being in a library with limited resources.</li>\n</ul>",
    "contentMarkdown": "*   While both vanilla and agentic RAG systems aim to retrieve information and generate responses, agentic RAG introduces several significant enhancements:\n\n**Feature**\n\n**Vanilla RAG**\n\n**Agentic RAG**\n\nAccess to External Tools\n\nNo\n\nYes – Utilizes external tools like vector search engines, web search tools, calculators, and APIs.\n\nQuery Pre-processing\n\nNo\n\nYes – Agents dynamically refine, rephrase, and adapt queries for optimized retrieval.\n\nDecision-making in Retrieval\n\nLimited to direct retrieval from knowledge base\n\nAgents autonomously decide if retrieval is needed, select tools, and adapt based on query complexity and source type.\n\nMulti-step Retrieval Process\n\nNo\n\nYes – Agents perform multi-step, adaptive retrieval processes involving various sources or tool combinations.\n\nData Validation\n\nNo\n\nYes – Information is cross-referenced across sources to validate accuracy, supporting complex, real-world responses.\n\nDynamic Tool Selection\n\nStatic retrieval tools only\n\nDynamic – Agents choose specific tools (e.g., vector search, APIs) based on query needs.\n\nAdaptability to Query\n\nLimited\n\nHighly adaptive – Agents select and operate tools based on real-time assessment of query requirements.\n\nTypes of Agents\n\nNot applicable\n\nMultiple specialized agents, such as internal data retrieval, personal data retrieval, public data retrieval.\n\nSingle-Agent vs. Multi-Agent System\n\nNot applicable\n\nSingle-agent router or multi-agent systems, with “master” and specialized agents for complex queries.\n\nReasoning and Logic Capability\n\nNo\n\nYes – Supports multi-step reasoning, allowing logical sequence handling before generating responses.\n\nMemory and Personalization\n\nLimited to immediate query\n\nYes – Capable of updating memory to retain user preferences or history, allowing personalized responses.\n\nReal-world Applications\n\nPrimarily static responses from a fixed database\n\nSupports a wide range of real-world applications by responding to complex, nuanced inquiries with context sensitivity.\n\n**Feature**\n\n**Vanilla RAG**\n\n**Agentic RAG**\n\nAccess to External Tools\n\nNo\n\nYes – Utilizes external tools like vector search engines, web search tools, calculators, and APIs.\n\nQuery Pre-processing\n\nNo\n\nYes – Agents dynamically refine, rephrase, and adapt queries for optimized retrieval.\n\nDecision-making in Retrieval\n\nLimited to direct retrieval from knowledge base\n\nAgents autonomously decide if retrieval is needed, select tools, and adapt based on query complexity and source type.\n\nMulti-step Retrieval Process\n\nNo\n\nYes – Agents perform multi-step, adaptive retrieval processes involving various sources or tool combinations.\n\nData Validation\n\nNo\n\nYes – Information is cross-referenced across sources to validate accuracy, supporting complex, real-world responses.\n\nDynamic Tool Selection\n\nStatic retrieval tools only\n\nDynamic – Agents choose specific tools (e.g., vector search, APIs) based on query needs.\n\nAdaptability to Query\n\nLimited\n\nHighly adaptive – Agents select and operate tools based on real-time assessment of query requirements.\n\nTypes of Agents\n\nNot applicable\n\nMultiple specialized agents, such as internal data retrieval, personal data retrieval, public data retrieval.\n\nSingle-Agent vs. Multi-Agent System\n\nNot applicable\n\nSingle-agent router or multi-agent systems, with “master” and specialized agents for complex queries.\n\nReasoning and Logic Capability\n\nNo\n\nYes – Supports multi-step reasoning, allowing logical sequence handling before generating responses.\n\nMemory and Personalization\n\nLimited to immediate query\n\nYes – Capable of updating memory to retain user preferences or history, allowing personalized responses.\n\nReal-world Applications\n\nPrimarily static responses from a fixed database\n\nSupports a wide range of real-world applications by responding to complex, nuanced inquiries with context sensitivity.\n\n*   Drawing a parallel with problem-solving, agentic RAG offers capabilities akin to having a smartphone in hand—equipped with multiple apps and tools to help answer a question—whereas vanilla RAG is akin to being in a library with limited resources.",
    "order": 35,
    "orderInChapter": 5,
    "difficulty": 4,
    "estimatedMinutes": 3,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 550,
      "contentLength": 7654
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#agentic-rag-vs.-vanilla-rag:-key-differences",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-implementing-agentic-rag-key-approaches-36",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Retrieval-Augmented Generation (RAG)",
    "title": "Implementing Agentic RAG: Key Approaches",
    "subtitle": "Agentic Retrieval-Augmented Generation (RAG)",
    "contentHtml": "<ul>\n  <li>To implement agentic RAG, developers can use either language models with function calling or agent frameworks, each providing specific advantages in terms of flexibility and control.</li>\n  <li>Both methods—function calling in language models and agent frameworks—enable agentic RAG, though each has unique benefits:\n    <ul>\n      <li><strong>Function Calling</strong> provides control over each tool interaction, suitable for cases with specific tool chains or simple agent setups.</li>\n      <li><strong>Agent Frameworks</strong> offer pre-built integrations and routing logic, ideal for larger, multi-agent architectures.</li>\n    </ul>\n  </li>\n  <li>Using these implementations, developers can build flexible and adaptive agentic RAG pipelines, enhancing retrieval, reasoning, and response generation capabilities for AI-driven applications.</li>\n</ul>\n<ul>\n      <li><strong>Function Calling</strong> provides control over each tool interaction, suitable for cases with specific tool chains or simple agent setups.</li>\n      <li><strong>Agent Frameworks</strong> offer pre-built integrations and routing logic, ideal for larger, multi-agent architectures.</li>\n    </ul>\n<h4 id=\"language-models-with-function-calling\">Language Models with Function Calling</h4>\n<ul>\n  <li>Function calling allows language models to interact directly with external tools. For example, OpenAI’s function calling for GPT-4 or Cohere’s connectors API lets developers connect language models to databases, calculators, and other services. This interaction involves defining a function (such as querying a database), passing it to the model via a schema, and routing the model’s queries through the defined functions. This approach enables the model to leverage specific tools as needed, based on the query.</li>\n</ul>\n<h4 id=\"agent-frameworks\">Agent Frameworks</h4>\n<ul>\n  <li>Several agent frameworks—such as LangChain, LlamaIndex, CrewAI—simplify agentic RAG implementation by providing pre-built templates and tool integrations. Key features include:\n    <ul>\n      <li><strong>LangChain</strong>: Offers support for language model tools, and its LCEL and LangGraph frameworks integrate these tools seamlessly.</li>\n      <li><strong>LlamaIndex</strong>: Provides a QueryEngineTool to streamline retrieval tasks.</li>\n      <li><strong>CrewAI</strong>: A leading framework for multi-agent setups, which supports shared tool access among agents.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>LangChain</strong>: Offers support for language model tools, and its LCEL and LangGraph frameworks integrate these tools seamlessly.</li>\n      <li><strong>LlamaIndex</strong>: Provides a QueryEngineTool to streamline retrieval tasks.</li>\n      <li><strong>CrewAI</strong>: A leading framework for multi-agent setups, which supports shared tool access among agents.</li>\n    </ul>",
    "contentMarkdown": "*   To implement agentic RAG, developers can use either language models with function calling or agent frameworks, each providing specific advantages in terms of flexibility and control.\n*   Both methods—function calling in language models and agent frameworks—enable agentic RAG, though each has unique benefits:\n    *   **Function Calling** provides control over each tool interaction, suitable for cases with specific tool chains or simple agent setups.\n    *   **Agent Frameworks** offer pre-built integrations and routing logic, ideal for larger, multi-agent architectures.\n*   Using these implementations, developers can build flexible and adaptive agentic RAG pipelines, enhancing retrieval, reasoning, and response generation capabilities for AI-driven applications.\n\n*   **Function Calling** provides control over each tool interaction, suitable for cases with specific tool chains or simple agent setups.\n*   **Agent Frameworks** offer pre-built integrations and routing logic, ideal for larger, multi-agent architectures.\n\n#### Language Models with Function Calling\n\n*   Function calling allows language models to interact directly with external tools. For example, OpenAI’s function calling for GPT-4 or Cohere’s connectors API lets developers connect language models to databases, calculators, and other services. This interaction involves defining a function (such as querying a database), passing it to the model via a schema, and routing the model’s queries through the defined functions. This approach enables the model to leverage specific tools as needed, based on the query.\n\n#### Agent Frameworks\n\n*   Several agent frameworks—such as LangChain, LlamaIndex, CrewAI—simplify agentic RAG implementation by providing pre-built templates and tool integrations. Key features include:\n    *   **LangChain**: Offers support for language model tools, and its LCEL and LangGraph frameworks integrate these tools seamlessly.\n    *   **LlamaIndex**: Provides a QueryEngineTool to streamline retrieval tasks.\n    *   **CrewAI**: A leading framework for multi-agent setups, which supports shared tool access among agents.\n\n*   **LangChain**: Offers support for language model tools, and its LCEL and LangGraph frameworks integrate these tools seamlessly.\n*   **LlamaIndex**: Provides a QueryEngineTool to streamline retrieval tasks.\n*   **CrewAI**: A leading framework for multi-agent setups, which supports shared tool access among agents.",
    "order": 36,
    "orderInChapter": 6,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture",
      "gpt"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 326,
      "contentLength": 2877
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#implementing-agentic-rag:-key-approaches",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-enterprise-driven-adoption-37",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Retrieval-Augmented Generation (RAG)",
    "title": "Enterprise-driven Adoption",
    "subtitle": "Agentic Retrieval-Augmented Generation (RAG)",
    "contentHtml": "<ul>\n  <li>Organizations are increasingly transitioning to agentic RAG to gain more autonomous and accurate AI-driven systems. Enterprises such as Microsoft and Replit have introduced agents to enhance task completion and software development assistance. With agentic RAG, companies can build AI applications capable of handling diverse, real-time data sources, providing robust and adaptable responses for complex queries and tasks.</li>\n</ul>",
    "contentMarkdown": "*   Organizations are increasingly transitioning to agentic RAG to gain more autonomous and accurate AI-driven systems. Enterprises such as Microsoft and Replit have introduced agents to enhance task completion and software development assistance. With agentic RAG, companies can build AI applications capable of handling diverse, real-time data sources, providing robust and adaptable responses for complex queries and tasks.",
    "order": 37,
    "orderInChapter": 7,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 58,
      "contentLength": 444
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#enterprise-driven-adoption",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-benefits-38",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Retrieval-Augmented Generation (RAG)",
    "title": "Benefits",
    "subtitle": "Agentic Retrieval-Augmented Generation (RAG)",
    "contentHtml": "<ul>\n  <li>The primary benefits of agentic RAG include:\n    <ul>\n      <li><strong>Enhanced Retrieval Accuracy</strong>: By routing queries through specialized agents, agentic RAG can provide more accurate responses.</li>\n      <li><strong>Autonomous Task Performance</strong>: Agents can perform multi-step reasoning, independently solving complex problems.</li>\n      <li><strong>Improved Collaboration</strong>: These systems can better assist users by handling more varied and personalized queries.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Enhanced Retrieval Accuracy</strong>: By routing queries through specialized agents, agentic RAG can provide more accurate responses.</li>\n      <li><strong>Autonomous Task Performance</strong>: Agents can perform multi-step reasoning, independently solving complex problems.</li>\n      <li><strong>Improved Collaboration</strong>: These systems can better assist users by handling more varied and personalized queries.</li>\n    </ul>",
    "contentMarkdown": "*   The primary benefits of agentic RAG include:\n    *   **Enhanced Retrieval Accuracy**: By routing queries through specialized agents, agentic RAG can provide more accurate responses.\n    *   **Autonomous Task Performance**: Agents can perform multi-step reasoning, independently solving complex problems.\n    *   **Improved Collaboration**: These systems can better assist users by handling more varied and personalized queries.\n\n*   **Enhanced Retrieval Accuracy**: By routing queries through specialized agents, agentic RAG can provide more accurate responses.\n*   **Autonomous Task Performance**: Agents can perform multi-step reasoning, independently solving complex problems.\n*   **Improved Collaboration**: These systems can better assist users by handling more varied and personalized queries.",
    "order": 38,
    "orderInChapter": 8,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 100,
      "contentLength": 989
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#benefits",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-limitations-39",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Retrieval-Augmented Generation (RAG)",
    "title": "Limitations",
    "subtitle": "Agentic Retrieval-Augmented Generation (RAG)",
    "contentHtml": "<ul>\n  <li>Agentic RAG does present challenges, such as:\n    <ul>\n      <li><strong>Increased Latency</strong>: Running multiple agents and interacting with tools can add delays to the response.</li>\n      <li><strong>Reliability of Agents</strong>: Depending on the LLM’s reasoning capabilities, agents may fail to complete certain tasks accurately.</li>\n      <li><strong>Complexity in Error Handling</strong>: Systems need robust fallback mechanisms to recover if an agent fails to retrieve or process data.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Increased Latency</strong>: Running multiple agents and interacting with tools can add delays to the response.</li>\n      <li><strong>Reliability of Agents</strong>: Depending on the LLM’s reasoning capabilities, agents may fail to complete certain tasks accurately.</li>\n      <li><strong>Complexity in Error Handling</strong>: Systems need robust fallback mechanisms to recover if an agent fails to retrieve or process data.</li>\n    </ul>",
    "contentMarkdown": "*   Agentic RAG does present challenges, such as:\n    *   **Increased Latency**: Running multiple agents and interacting with tools can add delays to the response.\n    *   **Reliability of Agents**: Depending on the LLM’s reasoning capabilities, agents may fail to complete certain tasks accurately.\n    *   **Complexity in Error Handling**: Systems need robust fallback mechanisms to recover if an agent fails to retrieve or process data.\n\n*   **Increased Latency**: Running multiple agents and interacting with tools can add delays to the response.\n*   **Reliability of Agents**: Depending on the LLM’s reasoning capabilities, agents may fail to complete certain tasks accurately.\n*   **Complexity in Error Handling**: Systems need robust fallback mechanisms to recover if an agent fails to retrieve or process data.",
    "order": 39,
    "orderInChapter": 9,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 118,
      "contentLength": 1004
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#limitations",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-code-40",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Retrieval-Augmented Generation (RAG)",
    "title": "Code",
    "subtitle": "Agentic Retrieval-Augmented Generation (RAG)",
    "contentHtml": "<ul>\n  <li>Implementing agentic RAG requires setting up an agent framework capable of handling tool integrations and coordinating retrieval processes. This section walks through an example code setup, demonstrating both language models with function calling and agent frameworks for building an agentic RAG pipeline.</li>\n</ul>\n<h4 id=\"implementing-agentic-rag-with-function-calling\">Implementing Agentic RAG with Function Calling</h4>\n<ul>\n  <li>\n    <p>Function calling in language models allows them to interact with tools by defining functions that retrieve data from external sources. This method leverages API calls, database queries, and computation tools to enrich the response with dynamic data.</p>\n  </li>\n  <li>\n    <p>Here’s an example implementation using a function for retrieval from a database via the Weaviate vector search API.</p>\n  </li>\n</ul>\n<p>Function calling in language models allows them to interact with tools by defining functions that retrieve data from external sources. This method leverages API calls, database queries, and computation tools to enrich the response with dynamic data.</p>\n<p>Here’s an example implementation using a function for retrieval from a database via the Weaviate vector search API.</p>\n<h5 id=\"define-the-function-for-retrieval\">Define the Function for Retrieval</h5>\n<ul>\n  <li>To start, we define a function that uses Weaviate’s hybrid search to query a database and retrieve relevant results.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code12\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code12\"><span class=\"k\">def</span> <span class=\"nf\">get_search_results</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"s\">\"\"\"Sends a query to Weaviate's Hybrid Search. Parses the response into a formatted string.\"\"\"</span>\n\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">blogs</span><span class=\"p\">.</span><span class=\"n\">query</span><span class=\"p\">.</span><span class=\"n\">hybrid</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">,</span> <span class=\"n\">limit</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>  <span class=\"c1\"># Retrieve top 5 results based on the query\n</span>    <span class=\"n\">stringified_response</span> <span class=\"o\">=</span> <span class=\"s\">\"\"</span>\n    <span class=\"k\">for</span> <span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">o</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">objects</span><span class=\"p\">):</span>\n        <span class=\"n\">stringified_response</span> <span class=\"o\">+=</span> <span class=\"sa\">f</span><span class=\"s\">\"Search Result </span><span class=\"si\">{</span><span class=\"n\">idx</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s\">:</span><span class=\"se\">\\n</span><span class=\"s\">\"</span>\n        <span class=\"k\">for</span> <span class=\"n\">prop</span> <span class=\"ow\">in</span> <span class=\"n\">o</span><span class=\"p\">.</span><span class=\"n\">properties</span><span class=\"p\">:</span>\n            <span class=\"n\">stringified_response</span> <span class=\"o\">+=</span> <span class=\"sa\">f</span><span class=\"s\">\"</span><span class=\"si\">{</span><span class=\"n\">prop</span><span class=\"si\">}</span><span class=\"s\">: </span><span class=\"si\">{</span><span class=\"n\">o</span><span class=\"p\">.</span><span class=\"n\">properties</span><span class=\"p\">[</span><span class=\"n\">prop</span><span class=\"p\">]</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"s\">\"</span>\n        <span class=\"n\">stringified_response</span> <span class=\"o\">+=</span> <span class=\"s\">\"</span><span class=\"se\">\\n</span><span class=\"s\">\"</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">stringified_response</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code12\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code12\"><span class=\"k\">def</span> <span class=\"nf\">get_search_results</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"s\">\"\"\"Sends a query to Weaviate's Hybrid Search. Parses the response into a formatted string.\"\"\"</span>\n\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">blogs</span><span class=\"p\">.</span><span class=\"n\">query</span><span class=\"p\">.</span><span class=\"n\">hybrid</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">,</span> <span class=\"n\">limit</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>  <span class=\"c1\"># Retrieve top 5 results based on the query\n</span>    <span class=\"n\">stringified_response</span> <span class=\"o\">=</span> <span class=\"s\">\"\"</span>\n    <span class=\"k\">for</span> <span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">o</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">objects</span><span class=\"p\">):</span>\n        <span class=\"n\">stringified_response</span> <span class=\"o\">+=</span> <span class=\"sa\">f</span><span class=\"s\">\"Search Result </span><span class=\"si\">{</span><span class=\"n\">idx</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s\">:</span><span class=\"se\">\\n</span><span class=\"s\">\"</span>\n        <span class=\"k\">for</span> <span class=\"n\">prop</span> <span class=\"ow\">in</span> <span class=\"n\">o</span><span class=\"p\">.</span><span class=\"n\">properties</span><span class=\"p\">:</span>\n            <span class=\"n\">stringified_response</span> <span class=\"o\">+=</span> <span class=\"sa\">f</span><span class=\"s\">\"</span><span class=\"si\">{</span><span class=\"n\">prop</span><span class=\"si\">}</span><span class=\"s\">: </span><span class=\"si\">{</span><span class=\"n\">o</span><span class=\"p\">.</span><span class=\"n\">properties</span><span class=\"p\">[</span><span class=\"n\">prop</span><span class=\"p\">]</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"s\">\"</span>\n        <span class=\"n\">stringified_response</span> <span class=\"o\">+=</span> <span class=\"s\">\"</span><span class=\"se\">\\n</span><span class=\"s\">\"</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">stringified_response</span>\n</code></pre>\n<h5 id=\"define-the-tools-schema\">Define the Tools Schema</h5>\n<ul>\n  <li>Next, we define a tools schema that connects the function to the language model. This schema tells the model how to use the function for retrieving data.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code13\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code13\"><span class=\"n\">tools_schema</span> <span class=\"o\">=</span> <span class=\"p\">[{</span>\n    <span class=\"s\">'type'</span><span class=\"p\">:</span> <span class=\"s\">'function'</span><span class=\"p\">,</span>\n    <span class=\"s\">'function'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s\">'name'</span><span class=\"p\">:</span> <span class=\"s\">'get_search_results'</span><span class=\"p\">,</span>\n        <span class=\"s\">'description'</span><span class=\"p\">:</span> <span class=\"s\">'Get search results for a provided query.'</span><span class=\"p\">,</span>\n        <span class=\"s\">'parameters'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n          <span class=\"s\">'type'</span><span class=\"p\">:</span> <span class=\"s\">'object'</span><span class=\"p\">,</span>\n          <span class=\"s\">'properties'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s\">'query'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n              <span class=\"s\">'type'</span><span class=\"p\">:</span> <span class=\"s\">'string'</span><span class=\"p\">,</span>\n              <span class=\"s\">'description'</span><span class=\"p\">:</span> <span class=\"s\">'The search query.'</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n          <span class=\"p\">},</span>\n          <span class=\"s\">'required'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'query'</span><span class=\"p\">],</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}]</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code13\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code13\"><span class=\"n\">tools_schema</span> <span class=\"o\">=</span> <span class=\"p\">[{</span>\n    <span class=\"s\">'type'</span><span class=\"p\">:</span> <span class=\"s\">'function'</span><span class=\"p\">,</span>\n    <span class=\"s\">'function'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"s\">'name'</span><span class=\"p\">:</span> <span class=\"s\">'get_search_results'</span><span class=\"p\">,</span>\n        <span class=\"s\">'description'</span><span class=\"p\">:</span> <span class=\"s\">'Get search results for a provided query.'</span><span class=\"p\">,</span>\n        <span class=\"s\">'parameters'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n          <span class=\"s\">'type'</span><span class=\"p\">:</span> <span class=\"s\">'object'</span><span class=\"p\">,</span>\n          <span class=\"s\">'properties'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n            <span class=\"s\">'query'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n              <span class=\"s\">'type'</span><span class=\"p\">:</span> <span class=\"s\">'string'</span><span class=\"p\">,</span>\n              <span class=\"s\">'description'</span><span class=\"p\">:</span> <span class=\"s\">'The search query.'</span><span class=\"p\">,</span>\n            <span class=\"p\">},</span>\n          <span class=\"p\">},</span>\n          <span class=\"s\">'required'</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"s\">'query'</span><span class=\"p\">],</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">},</span>\n<span class=\"p\">}]</span>\n</code></pre>\n<h5 id=\"setting-up-the-interaction-loop\">Setting up the Interaction Loop</h5>\n<ul>\n  <li>To ensure the model can call the tool multiple times (if needed), we set up a loop that enables the model to interact with tools and retrieve data iteratively until it has all necessary information.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code14\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code14\"><span class=\"k\">def</span> <span class=\"nf\">ollama_generation_with_tools</span><span class=\"p\">(</span><span class=\"n\">user_message</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">tools_schema</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">tool_mapping</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">,</span> <span class=\"n\">model_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s\">\"llama3.1\"</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"n\">messages</span> <span class=\"o\">=</span> <span class=\"p\">[{</span><span class=\"s\">\"role\"</span><span class=\"p\">:</span> <span class=\"s\">\"user\"</span><span class=\"p\">,</span> <span class=\"s\">\"content\"</span><span class=\"p\">:</span> <span class=\"n\">user_message</span><span class=\"p\">}]</span>\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">ollama</span><span class=\"p\">.</span><span class=\"n\">chat</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"n\">messages</span><span class=\"o\">=</span><span class=\"n\">messages</span><span class=\"p\">,</span> <span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"n\">tools_schema</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Check if the model needs to use a tool\n</span>    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s\">\"message\"</span><span class=\"p\">].</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s\">\"tool_calls\"</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s\">\"message\"</span><span class=\"p\">][</span><span class=\"s\">\"content\"</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Handle tool calls and retrieve information\n</span>    <span class=\"k\">for</span> <span class=\"n\">tool</span> <span class=\"ow\">in</span> <span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s\">\"message\"</span><span class=\"p\">][</span><span class=\"s\">\"tool_calls\"</span><span class=\"p\">]:</span>\n        <span class=\"n\">function_to_call</span> <span class=\"o\">=</span> <span class=\"n\">tool_mapping</span><span class=\"p\">[</span><span class=\"n\">tool</span><span class=\"p\">[</span><span class=\"s\">\"function\"</span><span class=\"p\">][</span><span class=\"s\">\"name\"</span><span class=\"p\">]]</span>\n        <span class=\"n\">function_response</span> <span class=\"o\">=</span> <span class=\"n\">function_to_call</span><span class=\"p\">(</span><span class=\"n\">tool</span><span class=\"p\">[</span><span class=\"s\">\"function\"</span><span class=\"p\">][</span><span class=\"s\">\"arguments\"</span><span class=\"p\">][</span><span class=\"s\">\"query\"</span><span class=\"p\">])</span>\n        <span class=\"n\">messages</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">({</span><span class=\"s\">\"role\"</span><span class=\"p\">:</span> <span class=\"s\">\"tool\"</span><span class=\"p\">,</span> <span class=\"s\">\"content\"</span><span class=\"p\">:</span> <span class=\"n\">function_response</span><span class=\"p\">})</span>\n    \n    <span class=\"c1\"># Generate final response after tool calls\n</span>    <span class=\"n\">final_response</span> <span class=\"o\">=</span> <span class=\"n\">ollama</span><span class=\"p\">.</span><span class=\"n\">chat</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"n\">messages</span><span class=\"o\">=</span><span class=\"n\">messages</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">final_response</span><span class=\"p\">[</span><span class=\"s\">\"message\"</span><span class=\"p\">][</span><span class=\"s\">\"content\"</span><span class=\"p\">]</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code14\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code14\"><span class=\"k\">def</span> <span class=\"nf\">ollama_generation_with_tools</span><span class=\"p\">(</span><span class=\"n\">user_message</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">tools_schema</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">tool_mapping</span><span class=\"p\">:</span> <span class=\"nb\">dict</span><span class=\"p\">,</span> <span class=\"n\">model_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s\">\"llama3.1\"</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"n\">messages</span> <span class=\"o\">=</span> <span class=\"p\">[{</span><span class=\"s\">\"role\"</span><span class=\"p\">:</span> <span class=\"s\">\"user\"</span><span class=\"p\">,</span> <span class=\"s\">\"content\"</span><span class=\"p\">:</span> <span class=\"n\">user_message</span><span class=\"p\">}]</span>\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">ollama</span><span class=\"p\">.</span><span class=\"n\">chat</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"n\">messages</span><span class=\"o\">=</span><span class=\"n\">messages</span><span class=\"p\">,</span> <span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"n\">tools_schema</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Check if the model needs to use a tool\n</span>    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s\">\"message\"</span><span class=\"p\">].</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s\">\"tool_calls\"</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s\">\"message\"</span><span class=\"p\">][</span><span class=\"s\">\"content\"</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Handle tool calls and retrieve information\n</span>    <span class=\"k\">for</span> <span class=\"n\">tool</span> <span class=\"ow\">in</span> <span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s\">\"message\"</span><span class=\"p\">][</span><span class=\"s\">\"tool_calls\"</span><span class=\"p\">]:</span>\n        <span class=\"n\">function_to_call</span> <span class=\"o\">=</span> <span class=\"n\">tool_mapping</span><span class=\"p\">[</span><span class=\"n\">tool</span><span class=\"p\">[</span><span class=\"s\">\"function\"</span><span class=\"p\">][</span><span class=\"s\">\"name\"</span><span class=\"p\">]]</span>\n        <span class=\"n\">function_response</span> <span class=\"o\">=</span> <span class=\"n\">function_to_call</span><span class=\"p\">(</span><span class=\"n\">tool</span><span class=\"p\">[</span><span class=\"s\">\"function\"</span><span class=\"p\">][</span><span class=\"s\">\"arguments\"</span><span class=\"p\">][</span><span class=\"s\">\"query\"</span><span class=\"p\">])</span>\n        <span class=\"n\">messages</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">({</span><span class=\"s\">\"role\"</span><span class=\"p\">:</span> <span class=\"s\">\"tool\"</span><span class=\"p\">,</span> <span class=\"s\">\"content\"</span><span class=\"p\">:</span> <span class=\"n\">function_response</span><span class=\"p\">})</span>\n    \n    <span class=\"c1\"># Generate final response after tool calls\n</span>    <span class=\"n\">final_response</span> <span class=\"o\">=</span> <span class=\"n\">ollama</span><span class=\"p\">.</span><span class=\"n\">chat</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"n\">messages</span><span class=\"o\">=</span><span class=\"n\">messages</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">final_response</span><span class=\"p\">[</span><span class=\"s\">\"message\"</span><span class=\"p\">][</span><span class=\"s\">\"content\"</span><span class=\"p\">]</span>\n</code></pre>\n<h5 id=\"executing-the-agentic-rag-query\">Executing the Agentic RAG Query</h5>\n<ul>\n  <li>Finally, we run the function, allowing the language model to interact with the <code class=\"language-plaintext highlighter-rouge\">get_search_results</code> tool.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code15\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code15\"><span class=\"n\">tool_mapping</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">\"get_search_results\"</span><span class=\"p\">:</span> <span class=\"n\">get_search_results</span><span class=\"p\">}</span>  <span class=\"c1\"># Maps tool name to function\n</span><span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">ollama_generation_with_tools</span><span class=\"p\">(</span>\n    <span class=\"s\">\"How is HNSW different from DiskANN?\"</span><span class=\"p\">,</span>\n    <span class=\"n\">tools_schema</span><span class=\"o\">=</span><span class=\"n\">tools_schema</span><span class=\"p\">,</span>\n    <span class=\"n\">tool_mapping</span><span class=\"o\">=</span><span class=\"n\">tool_mapping</span>\n<span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code15\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code15\"><span class=\"n\">tool_mapping</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s\">\"get_search_results\"</span><span class=\"p\">:</span> <span class=\"n\">get_search_results</span><span class=\"p\">}</span>  <span class=\"c1\"># Maps tool name to function\n</span><span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">ollama_generation_with_tools</span><span class=\"p\">(</span>\n    <span class=\"s\">\"How is HNSW different from DiskANN?\"</span><span class=\"p\">,</span>\n    <span class=\"n\">tools_schema</span><span class=\"o\">=</span><span class=\"n\">tools_schema</span><span class=\"p\">,</span>\n    <span class=\"n\">tool_mapping</span><span class=\"o\">=</span><span class=\"n\">tool_mapping</span>\n<span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>This setup enables the language model to retrieve dynamic information and perform tool-based retrievals as needed.</li>\n</ul>\n<h4 id=\"implementing-agentic-rag-with-agent-frameworks\">Implementing Agentic RAG with Agent Frameworks</h4>\n<ul>\n  <li>Using agent frameworks streamlines the implementation process by providing templates and pre-built modules for multi-agent orchestration. Here’s how to set up an agentic RAG pipeline using LangChain as an example.</li>\n</ul>\n<h5 id=\"step-1-define-agents-and-tools\">Step 1: Define Agents and Tools</h5>\n<ul>\n  <li>LangChain simplifies agentic RAG by managing tools and routing tasks. First, define the agents and register the tools they will use.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code16\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code16\"><span class=\"kn\">from</span> <span class=\"nn\">langchain.tools</span> <span class=\"kn\">import</span> <span class=\"n\">WebSearchTool</span><span class=\"p\">,</span> <span class=\"n\">DatabaseTool</span><span class=\"p\">,</span> <span class=\"n\">CalculatorTool</span>\n<span class=\"kn\">from</span> <span class=\"nn\">langchain.agents</span> <span class=\"kn\">import</span> <span class=\"n\">Agent</span>\n\n<span class=\"c1\"># Define tools for retrieval\n</span><span class=\"n\">web_search_tool</span> <span class=\"o\">=</span> <span class=\"n\">WebSearchTool</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"s\">\"YOUR_WEB_SEARCH_API_KEY\"</span><span class=\"p\">)</span>\n<span class=\"n\">database_tool</span> <span class=\"o\">=</span> <span class=\"n\">DatabaseTool</span><span class=\"p\">(</span><span class=\"n\">db_client</span><span class=\"o\">=</span><span class=\"s\">\"your_database_client\"</span><span class=\"p\">)</span>\n<span class=\"n\">calculator_tool</span> <span class=\"o\">=</span> <span class=\"n\">CalculatorTool</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Set up an agent with a routing function\n</span><span class=\"n\">retrieval_agent</span> <span class=\"o\">=</span> <span class=\"n\">Agent</span><span class=\"p\">(</span>\n    <span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">web_search_tool</span><span class=\"p\">,</span> <span class=\"n\">database_tool</span><span class=\"p\">,</span> <span class=\"n\">calculator_tool</span><span class=\"p\">],</span>\n    <span class=\"n\">routing_function</span><span class=\"o\">=</span><span class=\"s\">\"retrieve_and_select_tool\"</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code16\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code16\"><span class=\"kn\">from</span> <span class=\"nn\">langchain.tools</span> <span class=\"kn\">import</span> <span class=\"n\">WebSearchTool</span><span class=\"p\">,</span> <span class=\"n\">DatabaseTool</span><span class=\"p\">,</span> <span class=\"n\">CalculatorTool</span>\n<span class=\"kn\">from</span> <span class=\"nn\">langchain.agents</span> <span class=\"kn\">import</span> <span class=\"n\">Agent</span>\n\n<span class=\"c1\"># Define tools for retrieval\n</span><span class=\"n\">web_search_tool</span> <span class=\"o\">=</span> <span class=\"n\">WebSearchTool</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"s\">\"YOUR_WEB_SEARCH_API_KEY\"</span><span class=\"p\">)</span>\n<span class=\"n\">database_tool</span> <span class=\"o\">=</span> <span class=\"n\">DatabaseTool</span><span class=\"p\">(</span><span class=\"n\">db_client</span><span class=\"o\">=</span><span class=\"s\">\"your_database_client\"</span><span class=\"p\">)</span>\n<span class=\"n\">calculator_tool</span> <span class=\"o\">=</span> <span class=\"n\">CalculatorTool</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Set up an agent with a routing function\n</span><span class=\"n\">retrieval_agent</span> <span class=\"o\">=</span> <span class=\"n\">Agent</span><span class=\"p\">(</span>\n    <span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">web_search_tool</span><span class=\"p\">,</span> <span class=\"n\">database_tool</span><span class=\"p\">,</span> <span class=\"n\">calculator_tool</span><span class=\"p\">],</span>\n    <span class=\"n\">routing_function</span><span class=\"o\">=</span><span class=\"s\">\"retrieve_and_select_tool\"</span>\n<span class=\"p\">)</span>\n</code></pre>\n<h5 id=\"step-2-configure-agent-routing\">Step 2: Configure Agent Routing</h5>\n<ul>\n  <li>Set up the routing function to let the agent decide which tool to use based on the input query.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code17\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code17\"><span class=\"k\">def</span> <span class=\"nf\">retrieve_and_select_tool</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"s\">\"calculate\"</span> <span class=\"ow\">in</span> <span class=\"n\">query</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">calculator_tool</span>\n    <span class=\"k\">elif</span> <span class=\"s\">\"web\"</span> <span class=\"ow\">in</span> <span class=\"n\">query</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">web_search_tool</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">database_tool</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code17\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code17\"><span class=\"k\">def</span> <span class=\"nf\">retrieve_and_select_tool</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"s\">\"calculate\"</span> <span class=\"ow\">in</span> <span class=\"n\">query</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">calculator_tool</span>\n    <span class=\"k\">elif</span> <span class=\"s\">\"web\"</span> <span class=\"ow\">in</span> <span class=\"n\">query</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">web_search_tool</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">database_tool</span>\n</code></pre>\n<h5 id=\"step-3-chain-agents-for-multi-agent-rag\">Step 3: Chain Agents for Multi-Agent RAG</h5>\n<ul>\n  <li>In multi-agent RAG, you might have a “master agent” that routes queries to specialized agents based on query type. Here’s how to set up a master agent to coordinate multiple agents.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code18\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code18\"><span class=\"kn\">from</span> <span class=\"nn\">langchain.agents</span> <span class=\"kn\">import</span> <span class=\"n\">MultiAgent</span>\n\n<span class=\"c1\"># Define specialized agents\n</span><span class=\"n\">internal_agent</span> <span class=\"o\">=</span> <span class=\"n\">Agent</span><span class=\"p\">(</span><span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">database_tool</span><span class=\"p\">],</span> <span class=\"n\">routing_function</span><span class=\"o\">=</span><span class=\"s\">\"database_retrieval\"</span><span class=\"p\">)</span>\n<span class=\"n\">public_agent</span> <span class=\"o\">=</span> <span class=\"n\">Agent</span><span class=\"p\">(</span><span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">web_search_tool</span><span class=\"p\">],</span> <span class=\"n\">routing_function</span><span class=\"o\">=</span><span class=\"s\">\"web_retrieval\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Create a master agent to coordinate retrieval\n</span><span class=\"n\">master_agent</span> <span class=\"o\">=</span> <span class=\"n\">MultiAgent</span><span class=\"p\">(</span><span class=\"n\">agents</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">internal_agent</span><span class=\"p\">,</span> <span class=\"n\">public_agent</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Function to handle a query using master agent\n</span><span class=\"k\">def</span> <span class=\"nf\">handle_query_with_master_agent</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">master_agent</span><span class=\"p\">.</span><span class=\"n\">handle_query</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code18\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code18\"><span class=\"kn\">from</span> <span class=\"nn\">langchain.agents</span> <span class=\"kn\">import</span> <span class=\"n\">MultiAgent</span>\n\n<span class=\"c1\"># Define specialized agents\n</span><span class=\"n\">internal_agent</span> <span class=\"o\">=</span> <span class=\"n\">Agent</span><span class=\"p\">(</span><span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">database_tool</span><span class=\"p\">],</span> <span class=\"n\">routing_function</span><span class=\"o\">=</span><span class=\"s\">\"database_retrieval\"</span><span class=\"p\">)</span>\n<span class=\"n\">public_agent</span> <span class=\"o\">=</span> <span class=\"n\">Agent</span><span class=\"p\">(</span><span class=\"n\">tools</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">web_search_tool</span><span class=\"p\">],</span> <span class=\"n\">routing_function</span><span class=\"o\">=</span><span class=\"s\">\"web_retrieval\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Create a master agent to coordinate retrieval\n</span><span class=\"n\">master_agent</span> <span class=\"o\">=</span> <span class=\"n\">MultiAgent</span><span class=\"p\">(</span><span class=\"n\">agents</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">internal_agent</span><span class=\"p\">,</span> <span class=\"n\">public_agent</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Function to handle a query using master agent\n</span><span class=\"k\">def</span> <span class=\"nf\">handle_query_with_master_agent</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">master_agent</span><span class=\"p\">.</span><span class=\"n\">handle_query</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">)</span>\n</code></pre>\n<h5 id=\"running-the-multi-agent-query\">Running the Multi-Agent Query</h5>\n<ul>\n  <li>Finally, to test the system, input a query and let the master agent route it appropriately:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code19\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code19\"><span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">handle_query_with_master_agent</span><span class=\"p\">(</span><span class=\"s\">\"Find recent studies on neural networks\"</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code19\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code19\"><span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">handle_query_with_master_agent</span><span class=\"p\">(</span><span class=\"s\">\"Find recent studies on neural networks\"</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">)</span>\n</code></pre>",
    "contentMarkdown": "*   Implementing agentic RAG requires setting up an agent framework capable of handling tool integrations and coordinating retrieval processes. This section walks through an example code setup, demonstrating both language models with function calling and agent frameworks for building an agentic RAG pipeline.\n\n#### Implementing Agentic RAG with Function Calling\n\n*   Function calling in language models allows them to interact with tools by defining functions that retrieve data from external sources. This method leverages API calls, database queries, and computation tools to enrich the response with dynamic data.\n    \n*   Here’s an example implementation using a function for retrieval from a database via the Weaviate vector search API.\n    \n\nFunction calling in language models allows them to interact with tools by defining functions that retrieve data from external sources. This method leverages API calls, database queries, and computation tools to enrich the response with dynamic data.\n\nHere’s an example implementation using a function for retrieval from a database via the Weaviate vector search API.\n\n##### Define the Function for Retrieval\n\n*   To start, we define a function that uses Weaviate’s hybrid search to query a database and retrieve relevant results.\n\n![](https://aman.ai/images/copy.png)\n\n`def get_search_results(query: str) -> str:     \"\"\"Sends a query to Weaviate's Hybrid Search. Parses the response into a formatted string.\"\"\"      response = blogs.query.hybrid(query, limit=5)  # Retrieve top 5 results based on the query     stringified_response = \"\"     for idx, o in enumerate(response.objects):         stringified_response += f\"Search Result {idx+1}:\\n\"         for prop in o.properties:             stringified_response += f\"{prop}: {o.properties[prop]}\\n\"         stringified_response += \"\\n\"      return stringified_response`\n\n![](https://aman.ai/images/copy.png)\n\n`def get_search_results(query: str) -> str:     \"\"\"Sends a query to Weaviate's Hybrid Search. Parses the response into a formatted string.\"\"\"      response = blogs.query.hybrid(query, limit=5)  # Retrieve top 5 results based on the query     stringified_response = \"\"     for idx, o in enumerate(response.objects):         stringified_response += f\"Search Result {idx+1}:\\n\"         for prop in o.properties:             stringified_response += f\"{prop}: {o.properties[prop]}\\n\"         stringified_response += \"\\n\"      return stringified_response`\n\n##### Define the Tools Schema\n\n*   Next, we define a tools schema that connects the function to the language model. This schema tells the model how to use the function for retrieving data.\n\n![](https://aman.ai/images/copy.png)\n\n`tools_schema = [{     'type': 'function',     'function': {         'name': 'get_search_results',         'description': 'Get search results for a provided query.',         'parameters': {           'type': 'object',           'properties': {             'query': {               'type': 'string',               'description': 'The search query.',             },           },           'required': ['query'],         },     }, }]`\n\n![](https://aman.ai/images/copy.png)\n\n`tools_schema = [{     'type': 'function',     'function': {         'name': 'get_search_results',         'description': 'Get search results for a provided query.',         'parameters': {           'type': 'object',           'properties': {             'query': {               'type': 'string',               'description': 'The search query.',             },           },           'required': ['query'],         },     }, }]`\n\n##### Setting up the Interaction Loop\n\n*   To ensure the model can call the tool multiple times (if needed), we set up a loop that enables the model to interact with tools and retrieve data iteratively until it has all necessary information.\n\n![](https://aman.ai/images/copy.png)\n\n`def ollama_generation_with_tools(user_message: str, tools_schema: list, tool_mapping: dict, model_name: str = \"llama3.1\") -> str:     messages = [{\"role\": \"user\", \"content\": user_message}]     response = ollama.chat(model=model_name, messages=messages, tools=tools_schema)          # Check if the model needs to use a tool     if not response[\"message\"].get(\"tool_calls\"):         return response[\"message\"][\"content\"]          # Handle tool calls and retrieve information     for tool in response[\"message\"][\"tool_calls\"]:         function_to_call = tool_mapping[tool[\"function\"][\"name\"]]         function_response = function_to_call(tool[\"function\"][\"arguments\"][\"query\"])         messages.append({\"role\": \"tool\", \"content\": function_response})          # Generate final response after tool calls     final_response = ollama.chat(model=model_name, messages=messages)     return final_response[\"message\"][\"content\"]`\n\n![](https://aman.ai/images/copy.png)\n\n`def ollama_generation_with_tools(user_message: str, tools_schema: list, tool_mapping: dict, model_name: str = \"llama3.1\") -> str:     messages = [{\"role\": \"user\", \"content\": user_message}]     response = ollama.chat(model=model_name, messages=messages, tools=tools_schema)          # Check if the model needs to use a tool     if not response[\"message\"].get(\"tool_calls\"):         return response[\"message\"][\"content\"]          # Handle tool calls and retrieve information     for tool in response[\"message\"][\"tool_calls\"]:         function_to_call = tool_mapping[tool[\"function\"][\"name\"]]         function_response = function_to_call(tool[\"function\"][\"arguments\"][\"query\"])         messages.append({\"role\": \"tool\", \"content\": function_response})          # Generate final response after tool calls     final_response = ollama.chat(model=model_name, messages=messages)     return final_response[\"message\"][\"content\"]`\n\n##### Executing the Agentic RAG Query\n\n*   Finally, we run the function, allowing the language model to interact with the `get_search_results` tool.\n\n![](https://aman.ai/images/copy.png)\n\n`tool_mapping = {\"get_search_results\": get_search_results}  # Maps tool name to function response = ollama_generation_with_tools(     \"How is HNSW different from DiskANN?\",     tools_schema=tools_schema,     tool_mapping=tool_mapping ) print(response)`\n\n![](https://aman.ai/images/copy.png)\n\n`tool_mapping = {\"get_search_results\": get_search_results}  # Maps tool name to function response = ollama_generation_with_tools(     \"How is HNSW different from DiskANN?\",     tools_schema=tools_schema,     tool_mapping=tool_mapping ) print(response)`\n\n*   This setup enables the language model to retrieve dynamic information and perform tool-based retrievals as needed.\n\n#### Implementing Agentic RAG with Agent Frameworks\n\n*   Using agent frameworks streamlines the implementation process by providing templates and pre-built modules for multi-agent orchestration. Here’s how to set up an agentic RAG pipeline using LangChain as an example.\n\n##### Step 1: Define Agents and Tools\n\n*   LangChain simplifies agentic RAG by managing tools and routing tasks. First, define the agents and register the tools they will use.\n\n![](https://aman.ai/images/copy.png)\n\n`from langchain.tools import WebSearchTool, DatabaseTool, CalculatorTool from langchain.agents import Agent  # Define tools for retrieval web_search_tool = WebSearchTool(api_key=\"YOUR_WEB_SEARCH_API_KEY\") database_tool = DatabaseTool(db_client=\"your_database_client\") calculator_tool = CalculatorTool()  # Set up an agent with a routing function retrieval_agent = Agent(     tools=[web_search_tool, database_tool, calculator_tool],     routing_function=\"retrieve_and_select_tool\" )`\n\n![](https://aman.ai/images/copy.png)\n\n`from langchain.tools import WebSearchTool, DatabaseTool, CalculatorTool from langchain.agents import Agent  # Define tools for retrieval web_search_tool = WebSearchTool(api_key=\"YOUR_WEB_SEARCH_API_KEY\") database_tool = DatabaseTool(db_client=\"your_database_client\") calculator_tool = CalculatorTool()  # Set up an agent with a routing function retrieval_agent = Agent(     tools=[web_search_tool, database_tool, calculator_tool],     routing_function=\"retrieve_and_select_tool\" )`\n\n##### Step 2: Configure Agent Routing\n\n*   Set up the routing function to let the agent decide which tool to use based on the input query.\n\n![](https://aman.ai/images/copy.png)\n\n`def retrieve_and_select_tool(query):     if \"calculate\" in query:         return calculator_tool     elif \"web\" in query:         return web_search_tool     else:         return database_tool`\n\n![](https://aman.ai/images/copy.png)\n\n`def retrieve_and_select_tool(query):     if \"calculate\" in query:         return calculator_tool     elif \"web\" in query:         return web_search_tool     else:         return database_tool`\n\n##### Step 3: Chain Agents for Multi-Agent RAG\n\n*   In multi-agent RAG, you might have a “master agent” that routes queries to specialized agents based on query type. Here’s how to set up a master agent to coordinate multiple agents.\n\n![](https://aman.ai/images/copy.png)\n\n`from langchain.agents import MultiAgent  # Define specialized agents internal_agent = Agent(tools=[database_tool], routing_function=\"database_retrieval\") public_agent = Agent(tools=[web_search_tool], routing_function=\"web_retrieval\")  # Create a master agent to coordinate retrieval master_agent = MultiAgent(agents=[internal_agent, public_agent])  # Function to handle a query using master agent def handle_query_with_master_agent(query):     return master_agent.handle_query(query)`\n\n![](https://aman.ai/images/copy.png)\n\n`from langchain.agents import MultiAgent  # Define specialized agents internal_agent = Agent(tools=[database_tool], routing_function=\"database_retrieval\") public_agent = Agent(tools=[web_search_tool], routing_function=\"web_retrieval\")  # Create a master agent to coordinate retrieval master_agent = MultiAgent(agents=[internal_agent, public_agent])  # Function to handle a query using master agent def handle_query_with_master_agent(query):     return master_agent.handle_query(query)`\n\n##### Running the Multi-Agent Query\n\n*   Finally, to test the system, input a query and let the master agent route it appropriately:\n\n![](https://aman.ai/images/copy.png)\n\n`response = handle_query_with_master_agent(\"Find recent studies on neural networks\") print(response)`\n\n![](https://aman.ai/images/copy.png)\n\n`response = handle_query_with_master_agent(\"Find recent studies on neural networks\") print(response)`",
    "order": 40,
    "orderInChapter": 10,
    "difficulty": 5,
    "estimatedMinutes": 6,
    "tags": [
      "algorithmsarchitecture",
      "neural network"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 1073,
      "contentLength": 35913
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#code",
    "scrapedAt": "2025-12-28T11:47:30.116Z"
  },
  {
    "id": "ai-agents-disadvantages-of-agentic-rag-41",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Retrieval-Augmented Generation (RAG)",
    "title": "Disadvantages of Agentic RAG",
    "subtitle": "Agentic Retrieval-Augmented Generation (RAG)",
    "contentHtml": "<ul>\n  <li>\n    <p>Despite its advantages, agentic RAG comes with several limitations that should be carefully considered, particularly for time-sensitive applications:</p>\n\n    <ol>\n      <li>\n        <p><strong>Increased Latency</strong>: The inherent complexity of agentic RAG often translates to longer response times. Each query may require multiple tool interactions and sequential retrieval steps, which increase the latency significantly. This can hinder the system’s usability in environments where quick responses are crucial, such as real-time support systems or conversational interfaces.</p>\n      </li>\n      <li>\n        <p><strong>Higher Computational Cost</strong>: Agentic RAG systems often involve multiple calls to LLMs and other external tools. These calls cumulatively drive up computational costs, making it less efficient and potentially prohibitive for high-traffic applications. This expense adds to operational concerns, especially if the system must process large volumes of queries.</p>\n      </li>\n      <li>\n        <p><strong>Production Feasibility</strong>: Due to the latency and cost concerns, agentic RAG may not be ideal for production applications requiring rapid and continuous output. In such cases, vanilla RAG, which offers more direct and faster response generation, might be more suitable.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p>While these drawbacks limit agentic RAG’s use in certain scenarios, its capability to generate high-quality, well-researched responses can make it worthwhile in contexts where response time is less critical and information accuracy is paramount.</p>\n  </li>\n</ul>\n<p>Despite its advantages, agentic RAG comes with several limitations that should be carefully considered, particularly for time-sensitive applications:</p>\n<ol>\n      <li>\n        <p><strong>Increased Latency</strong>: The inherent complexity of agentic RAG often translates to longer response times. Each query may require multiple tool interactions and sequential retrieval steps, which increase the latency significantly. This can hinder the system’s usability in environments where quick responses are crucial, such as real-time support systems or conversational interfaces.</p>\n      </li>\n      <li>\n        <p><strong>Higher Computational Cost</strong>: Agentic RAG systems often involve multiple calls to LLMs and other external tools. These calls cumulatively drive up computational costs, making it less efficient and potentially prohibitive for high-traffic applications. This expense adds to operational concerns, especially if the system must process large volumes of queries.</p>\n      </li>\n      <li>\n        <p><strong>Production Feasibility</strong>: Due to the latency and cost concerns, agentic RAG may not be ideal for production applications requiring rapid and continuous output. In such cases, vanilla RAG, which offers more direct and faster response generation, might be more suitable.</p>\n      </li>\n    </ol>\n<p><strong>Increased Latency</strong>: The inherent complexity of agentic RAG often translates to longer response times. Each query may require multiple tool interactions and sequential retrieval steps, which increase the latency significantly. This can hinder the system’s usability in environments where quick responses are crucial, such as real-time support systems or conversational interfaces.</p>\n<p><strong>Higher Computational Cost</strong>: Agentic RAG systems often involve multiple calls to LLMs and other external tools. These calls cumulatively drive up computational costs, making it less efficient and potentially prohibitive for high-traffic applications. This expense adds to operational concerns, especially if the system must process large volumes of queries.</p>\n<p><strong>Production Feasibility</strong>: Due to the latency and cost concerns, agentic RAG may not be ideal for production applications requiring rapid and continuous output. In such cases, vanilla RAG, which offers more direct and faster response generation, might be more suitable.</p>\n<p>While these drawbacks limit agentic RAG’s use in certain scenarios, its capability to generate high-quality, well-researched responses can make it worthwhile in contexts where response time is less critical and information accuracy is paramount.</p>",
    "contentMarkdown": "*   Despite its advantages, agentic RAG comes with several limitations that should be carefully considered, particularly for time-sensitive applications:\n    \n    1.  **Increased Latency**: The inherent complexity of agentic RAG often translates to longer response times. Each query may require multiple tool interactions and sequential retrieval steps, which increase the latency significantly. This can hinder the system’s usability in environments where quick responses are crucial, such as real-time support systems or conversational interfaces.\n        \n    2.  **Higher Computational Cost**: Agentic RAG systems often involve multiple calls to LLMs and other external tools. These calls cumulatively drive up computational costs, making it less efficient and potentially prohibitive for high-traffic applications. This expense adds to operational concerns, especially if the system must process large volumes of queries.\n        \n    3.  **Production Feasibility**: Due to the latency and cost concerns, agentic RAG may not be ideal for production applications requiring rapid and continuous output. In such cases, vanilla RAG, which offers more direct and faster response generation, might be more suitable.\n        \n*   While these drawbacks limit agentic RAG’s use in certain scenarios, its capability to generate high-quality, well-researched responses can make it worthwhile in contexts where response time is less critical and information accuracy is paramount.\n    \n\nDespite its advantages, agentic RAG comes with several limitations that should be carefully considered, particularly for time-sensitive applications:\n\n1.  **Increased Latency**: The inherent complexity of agentic RAG often translates to longer response times. Each query may require multiple tool interactions and sequential retrieval steps, which increase the latency significantly. This can hinder the system’s usability in environments where quick responses are crucial, such as real-time support systems or conversational interfaces.\n    \n2.  **Higher Computational Cost**: Agentic RAG systems often involve multiple calls to LLMs and other external tools. These calls cumulatively drive up computational costs, making it less efficient and potentially prohibitive for high-traffic applications. This expense adds to operational concerns, especially if the system must process large volumes of queries.\n    \n3.  **Production Feasibility**: Due to the latency and cost concerns, agentic RAG may not be ideal for production applications requiring rapid and continuous output. In such cases, vanilla RAG, which offers more direct and faster response generation, might be more suitable.\n    \n\n**Increased Latency**: The inherent complexity of agentic RAG often translates to longer response times. Each query may require multiple tool interactions and sequential retrieval steps, which increase the latency significantly. This can hinder the system’s usability in environments where quick responses are crucial, such as real-time support systems or conversational interfaces.\n\n**Higher Computational Cost**: Agentic RAG systems often involve multiple calls to LLMs and other external tools. These calls cumulatively drive up computational costs, making it less efficient and potentially prohibitive for high-traffic applications. This expense adds to operational concerns, especially if the system must process large volumes of queries.\n\n**Production Feasibility**: Due to the latency and cost concerns, agentic RAG may not be ideal for production applications requiring rapid and continuous output. In such cases, vanilla RAG, which offers more direct and faster response generation, might be more suitable.\n\nWhile these drawbacks limit agentic RAG’s use in certain scenarios, its capability to generate high-quality, well-researched responses can make it worthwhile in contexts where response time is less critical and information accuracy is paramount.",
    "order": 41,
    "orderInChapter": 11,
    "difficulty": 4,
    "estimatedMinutes": 3,
    "tags": [
      "algorithmsarchitecture",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 532,
      "contentLength": 4311
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#disadvantages-of-agentic-rag",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-summary-42",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Agentic Retrieval-Augmented Generation (RAG)",
    "title": "Summary",
    "subtitle": "Agentic Retrieval-Augmented Generation (RAG)",
    "contentHtml": "<ul>\n  <li>Agentic RAG refers to an agent-based implementation of RAG. AI agents are entities tasked with accomplishing specific objectives. These agents are often equipped with memory and tools, which they can utilize to carry out their tasks effectively. Among these tools, one significant capability is the ability to retrieve information from various sources, such as web searches or internal documents.</li>\n  <li>In the context of agentic RAG, the “retrieval becomes agentic.” This implies that the AI agent is capable of reasoning and making decisions regarding which sources are most appropriate for retrieving the required information. The agent’s tool usage evolves beyond simple information retrieval, becoming more flexible and dynamic.</li>\n  <li>The distinction between standard and agentic RAG can be summarized as follows:\n    <ul>\n      <li><strong>Common RAG</strong>: The user input prompts a single call to a database, retrieving additional information in response to the query.</li>\n      <li><strong>Agentic RAG</strong>: The agent is able to deliberate on which source is the most suitable for retrieving information based on the query, providing a more sophisticated and adaptable approach.</li>\n    </ul>\n  </li>\n  <li>The following figure (<a href=\"https://www.linkedin.com/in/804250ab/\">source</a>) offers a visual summary of Agentic RAG:</li>\n</ul>\n<ul>\n      <li><strong>Common RAG</strong>: The user input prompts a single call to a database, retrieving additional information in response to the query.</li>\n      <li><strong>Agentic RAG</strong>: The agent is able to deliberate on which source is the most suitable for retrieving information based on the query, providing a more sophisticated and adaptable approach.</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/RAG/AgenticRAG.jpg\" alt=\"\"></p>",
    "contentMarkdown": "*   Agentic RAG refers to an agent-based implementation of RAG. AI agents are entities tasked with accomplishing specific objectives. These agents are often equipped with memory and tools, which they can utilize to carry out their tasks effectively. Among these tools, one significant capability is the ability to retrieve information from various sources, such as web searches or internal documents.\n*   In the context of agentic RAG, the “retrieval becomes agentic.” This implies that the AI agent is capable of reasoning and making decisions regarding which sources are most appropriate for retrieving the required information. The agent’s tool usage evolves beyond simple information retrieval, becoming more flexible and dynamic.\n*   The distinction between standard and agentic RAG can be summarized as follows:\n    *   **Common RAG**: The user input prompts a single call to a database, retrieving additional information in response to the query.\n    *   **Agentic RAG**: The agent is able to deliberate on which source is the most suitable for retrieving information based on the query, providing a more sophisticated and adaptable approach.\n*   The following figure ([source](https://www.linkedin.com/in/804250ab/)) offers a visual summary of Agentic RAG:\n\n*   **Common RAG**: The user input prompts a single call to a database, retrieving additional information in response to the query.\n*   **Agentic RAG**: The agent is able to deliberate on which source is the most suitable for retrieving information based on the query, providing a more sophisticated and adaptable approach.\n\n![](/primers/ai/assets/RAG/AgenticRAG.jpg)",
    "order": 42,
    "orderInChapter": 12,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 237,
      "contentLength": 1827
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#summary",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-customer-support-43",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Case Studies",
    "title": "Customer Support",
    "subtitle": "Case Studies",
    "contentHtml": "<ul>\n  <li>\n    <p>Customer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because:</p>\n\n    <ul>\n      <li>Support interactions naturally follow a conversation flow while requiring access to external information and actions;</li>\n      <li>Tools can be integrated to pull customer data, order history, and knowledge base articles;</li>\n      <li>Actions such as issuing refunds or updating tickets can be handled programmatically; and</li>\n      <li>Success can be clearly measured through user-defined resolutions.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Several companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents’ effectiveness.</p>\n  </li>\n</ul>\n<p>Customer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because:</p>\n<ul>\n      <li>Support interactions naturally follow a conversation flow while requiring access to external information and actions;</li>\n      <li>Tools can be integrated to pull customer data, order history, and knowledge base articles;</li>\n      <li>Actions such as issuing refunds or updating tickets can be handled programmatically; and</li>\n      <li>Success can be clearly measured through user-defined resolutions.</li>\n    </ul>\n<p>Several companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents’ effectiveness.</p>",
    "contentMarkdown": "*   Customer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because:\n    \n    *   Support interactions naturally follow a conversation flow while requiring access to external information and actions;\n    *   Tools can be integrated to pull customer data, order history, and knowledge base articles;\n    *   Actions such as issuing refunds or updating tickets can be handled programmatically; and\n    *   Success can be clearly measured through user-defined resolutions.\n*   Several companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents’ effectiveness.\n    \n\nCustomer support combines familiar chatbot interfaces with enhanced capabilities through tool integration. This is a natural fit for more open-ended agents because:\n\n*   Support interactions naturally follow a conversation flow while requiring access to external information and actions;\n*   Tools can be integrated to pull customer data, order history, and knowledge base articles;\n*   Actions such as issuing refunds or updating tickets can be handled programmatically; and\n*   Success can be clearly measured through user-defined resolutions.\n\nSeveral companies have demonstrated the viability of this approach through usage-based pricing models that charge only for successful resolutions, showing confidence in their agents’ effectiveness.",
    "order": 43,
    "orderInChapter": 1,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 204,
      "contentLength": 1685
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#customer-support",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-software-development-44",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Case Studies",
    "title": "Software Development",
    "subtitle": "Case Studies",
    "contentHtml": "<ul>\n  <li>\n    <p>The software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because:</p>\n\n    <ul>\n      <li>Code solutions are verifiable through automated tests;</li>\n      <li>Agents can iterate on solutions using test results as feedback;</li>\n      <li>The problem space is well-defined and structured; and</li>\n      <li>Output quality can be measured objectively.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Agentic workflows demonstrate significant performance improvements in programming tasks. The figure below (<a href=\"https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/\">source</a>) offers an analysis of various AI models’ coding capabilities on the HumanEval benchmark revealed stark differences when using iterative agentic approaches. Zero-shot attempts by GPT-3.5 and GPT-4 achieved correctness rates of 48.1% and 67.0%, respectively. However, when GPT-3.5 was embedded within an agent loop, it achieved up to 95.1% accuracy, highlighting the profound impact of iterative processing.</p>\n  </li>\n</ul>\n<p>The software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because:</p>\n<ul>\n      <li>Code solutions are verifiable through automated tests;</li>\n      <li>Agents can iterate on solutions using test results as feedback;</li>\n      <li>The problem space is well-defined and structured; and</li>\n      <li>Output quality can be measured objectively.</li>\n    </ul>\n<p>Agentic workflows demonstrate significant performance improvements in programming tasks. The figure below (<a href=\"https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/\">source</a>) offers an analysis of various AI models’ coding capabilities on the HumanEval benchmark revealed stark differences when using iterative agentic approaches. Zero-shot attempts by GPT-3.5 and GPT-4 achieved correctness rates of 48.1% and 67.0%, respectively. However, when GPT-3.5 was embedded within an agent loop, it achieved up to 95.1% accuracy, highlighting the profound impact of iterative processing.</p>\n<p><img src=\"/primers/ai/assets/agents/HumanEval.jpg\" alt=\"diagram illustrating HumanEval benchmark results\"></p>\n<ul>\n  <li>\n    <p>In summary, the integration of agentic and iterative workflows marks a significant evolution in AI design patterns, enhancing the ability of LLM-powered agents to handle increasingly sophisticated tasks. As these frameworks and tools continue to mature, they are poised to become essential components in AI-driven task management and automation, offering unparalleled efficiency and adaptability.</p>\n  </li>\n  <li>\n    <p>In our own implementation, agents can now solve real GitHub issues in the SWE-bench Verified benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements.</p>\n  </li>\n</ul>\n<p>In summary, the integration of agentic and iterative workflows marks a significant evolution in AI design patterns, enhancing the ability of LLM-powered agents to handle increasingly sophisticated tasks. As these frameworks and tools continue to mature, they are poised to become essential components in AI-driven task management and automation, offering unparalleled efficiency and adaptability.</p>\n<p>In our own implementation, agents can now solve real GitHub issues in the SWE-bench Verified benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements.</p>\n<h4 id=\"devin\">Devin</h4>\n<ul>\n  <li>A notable example of an agentic approach applied to software development is <a href=\"https://www.cognition.ai/\">Devin</a>, an autonomous agent optimized for software engineering tasks. Devin exemplifies the effectiveness of agentic workflows, achieving state-of-the-art results on the <a href=\"https://www.swebench.com/\">SWE-Bench coding benchmark</a>, passing rigorous practical engineering interviews at top AI firms, and successfully executing real-world assignments on platforms like Upwork. Devin operates by leveraging its own shell, code editor, and web browser, enabling it to autonomously solve engineering problems and refine its work iteratively.</li>\n</ul>",
    "contentMarkdown": "*   The software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because:\n    \n    *   Code solutions are verifiable through automated tests;\n    *   Agents can iterate on solutions using test results as feedback;\n    *   The problem space is well-defined and structured; and\n    *   Output quality can be measured objectively.\n*   Agentic workflows demonstrate significant performance improvements in programming tasks. The figure below ([source](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/)) offers an analysis of various AI models’ coding capabilities on the HumanEval benchmark revealed stark differences when using iterative agentic approaches. Zero-shot attempts by GPT-3.5 and GPT-4 achieved correctness rates of 48.1% and 67.0%, respectively. However, when GPT-3.5 was embedded within an agent loop, it achieved up to 95.1% accuracy, highlighting the profound impact of iterative processing.\n    \n\nThe software development space has shown remarkable potential for LLM features, with capabilities evolving from code completion to autonomous problem-solving. Agents are particularly effective because:\n\n*   Code solutions are verifiable through automated tests;\n*   Agents can iterate on solutions using test results as feedback;\n*   The problem space is well-defined and structured; and\n*   Output quality can be measured objectively.\n\nAgentic workflows demonstrate significant performance improvements in programming tasks. The figure below ([source](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/)) offers an analysis of various AI models’ coding capabilities on the HumanEval benchmark revealed stark differences when using iterative agentic approaches. Zero-shot attempts by GPT-3.5 and GPT-4 achieved correctness rates of 48.1% and 67.0%, respectively. However, when GPT-3.5 was embedded within an agent loop, it achieved up to 95.1% accuracy, highlighting the profound impact of iterative processing.\n\n![diagram illustrating HumanEval benchmark results](/primers/ai/assets/agents/HumanEval.jpg)\n\n*   In summary, the integration of agentic and iterative workflows marks a significant evolution in AI design patterns, enhancing the ability of LLM-powered agents to handle increasingly sophisticated tasks. As these frameworks and tools continue to mature, they are poised to become essential components in AI-driven task management and automation, offering unparalleled efficiency and adaptability.\n    \n*   In our own implementation, agents can now solve real GitHub issues in the SWE-bench Verified benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements.\n    \n\nIn summary, the integration of agentic and iterative workflows marks a significant evolution in AI design patterns, enhancing the ability of LLM-powered agents to handle increasingly sophisticated tasks. As these frameworks and tools continue to mature, they are poised to become essential components in AI-driven task management and automation, offering unparalleled efficiency and adaptability.\n\nIn our own implementation, agents can now solve real GitHub issues in the SWE-bench Verified benchmark based on the pull request description alone. However, whereas automated testing helps verify functionality, human review remains crucial for ensuring solutions align with broader system requirements.\n\n#### Devin\n\n*   A notable example of an agentic approach applied to software development is [Devin](https://www.cognition.ai/), an autonomous agent optimized for software engineering tasks. Devin exemplifies the effectiveness of agentic workflows, achieving state-of-the-art results on the [SWE-Bench coding benchmark](https://www.swebench.com/), passing rigorous practical engineering interviews at top AI firms, and successfully executing real-world assignments on platforms like Upwork. Devin operates by leveraging its own shell, code editor, and web browser, enabling it to autonomously solve engineering problems and refine its work iteratively.",
    "order": 44,
    "orderInChapter": 2,
    "difficulty": 4,
    "estimatedMinutes": 3,
    "tags": [
      "algorithmsarchitecture",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 542,
      "contentLength": 4592
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#software-development",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-autogen-studio-45",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "AutoGen Studio",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li>Microsoft Research’s <a href=\"https://microsoft.github.io/autogen/docs/autogen-studio/getting-started/\">AutoGen Studio</a> is a low-code interface for rapidly prototyping AI agents. It’s built on top of the AutoGen framework and can also be used for debugging and evaluating multi-agent workflows.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/agents/ags.png\" alt=\"\"></p>",
    "contentMarkdown": "*   Microsoft Research’s [AutoGen Studio](https://microsoft.github.io/autogen/docs/autogen-studio/getting-started/) is a low-code interface for rapidly prototyping AI agents. It’s built on top of the AutoGen framework and can also be used for debugging and evaluating multi-agent workflows.\n\n![](/primers/ai/assets/agents/ags.png)",
    "order": 45,
    "orderInChapter": 1,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 34,
      "contentLength": 379
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#autogen-studio",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-autogen-46",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "AutoGen",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li>AutoGen is an open-source framework by Microsoft for building AI agent systems. It simplifies the creation of event-driven, distributed, scalable, and resilient agentic applications.</li>\n  <li><a href=\"https://github.com/microsoft/autogen\">Github</a>; <a href=\"https://microsoft.github.io/autogen/0.2/docs/Getting-Started\">Docs</a>; <a href=\"https://microsoft.github.io/autogen/0.2/docs/notebooks\">Examples</a></li>\n</ul>",
    "contentMarkdown": "*   AutoGen is an open-source framework by Microsoft for building AI agent systems. It simplifies the creation of event-driven, distributed, scalable, and resilient agentic applications.\n*   [Github](https://github.com/microsoft/autogen); [Docs](https://microsoft.github.io/autogen/0.2/docs/Getting-Started); [Examples](https://microsoft.github.io/autogen/0.2/docs/notebooks)",
    "order": 46,
    "orderInChapter": 2,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 29,
      "contentLength": 433
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#autogen",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-swarm-47",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "Swarm",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li>Swarm by OpenAI’s is a framework exploring ergonomic, lightweight multi-agent orchestration.</li>\n  <li><a href=\"https://github.com/openai/swarm\">Github</a></li>\n</ul>",
    "contentMarkdown": "*   Swarm by OpenAI’s is a framework exploring ergonomic, lightweight multi-agent orchestration.\n*   [Github](https://github.com/openai/swarm)",
    "order": 47,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 14,
      "contentLength": 178
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#swarm",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-crewai-48",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "CrewAI",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li>Cutting-edge framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.</li>\n  <li><a href=\"https://github.com/crewAIInc/crewAI\">Github</a>; <a href=\"https://docs.crewai.com/introduction\">Docs</a>; <a href=\"https://docs.crewai.com/examples/\">Examples</a></li>\n</ul>",
    "contentMarkdown": "*   Cutting-edge framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.\n*   [Github](https://github.com/crewAIInc/crewAI); [Docs](https://docs.crewai.com/introduction); [Examples](https://docs.crewai.com/examples/)",
    "order": 48,
    "orderInChapter": 4,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 27,
      "contentLength": 398
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#crewai",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-letta-49",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "Letta",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li><a href=\"https://www.letta.com/\">Letta</a> is an open source framework for building stateful LLM applications powered by agents with advanced reasoning capabilities and transparent long-term memory.</li>\n  <li><a href=\"https://github.com/cpacker/MemGPT\">Github</a>; <a href=\"https://docs.letta.com/introduction\">Docs</a>; <a href=\"https://github.com/cpacker/MemGPT/tree/main/examples\">Examples</a></li>\n</ul>",
    "contentMarkdown": "*   [Letta](https://www.letta.com/) is an open source framework for building stateful LLM applications powered by agents with advanced reasoning capabilities and transparent long-term memory.\n*   [Github](https://github.com/cpacker/MemGPT); [Docs](https://docs.letta.com/introduction); [Examples](https://github.com/cpacker/MemGPT/tree/main/examples)",
    "order": 49,
    "orderInChapter": 5,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "gpt",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 27,
      "contentLength": 419
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#letta",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-llama-stack-50",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "Llama Stack",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li><a href=\"https://github.com/meta-llama/llama-stack\">Llama Stack</a> from Meta defines and standardizes the building blocks needed to bring generative AI applications to market. These blocks span the entire development lifecycle: from model training and fine-tuning, through product evaluation, to building and running AI agents in production.</li>\n  <li><a href=\"https://github.com/meta-llama/llama-stack\">Github</a>; <a href=\"https://github.com/meta-llama/llama-stack/tree/main/docs\">Docs</a>; <a href=\"https://github.com/meta-llama/llama-stack-apps/tree/main/examples\">Examples</a></li>\n</ul>",
    "contentMarkdown": "*   [Llama Stack](https://github.com/meta-llama/llama-stack) from Meta defines and standardizes the building blocks needed to bring generative AI applications to market. These blocks span the entire development lifecycle: from model training and fine-tuning, through product evaluation, to building and running AI agents in production.\n*   [Github](https://github.com/meta-llama/llama-stack); [Docs](https://github.com/meta-llama/llama-stack/tree/main/docs); [Examples](https://github.com/meta-llama/llama-stack-apps/tree/main/examples)",
    "order": 50,
    "orderInChapter": 6,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 46,
      "contentLength": 605
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#llama-stack",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-autorag-51",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "AutoRAG",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li><a href=\"https://auto-rag.com/\">AutoRAG</a> is a tool for finding the optimal RAG pipeline for “your data”, including Agentic RAG approaches. You can evaluate various RAG modules automatically with your own evaluation data and find the best RAG pipeline for your own use-case.</li>\n  <li><a href=\"https://github.com/Marker-Inc-Korea/AutoRAG\">Github</a>; <a href=\"https://docs.auto-rag.com/\">Docs</a>; <a href=\"https://github.com/Marker-Inc-Korea/AutoRAG-tutorial\">Examples</a></li>\n</ul>",
    "contentMarkdown": "*   [AutoRAG](https://auto-rag.com/) is a tool for finding the optimal RAG pipeline for “your data”, including Agentic RAG approaches. You can evaluate various RAG modules automatically with your own evaluation data and find the best RAG pipeline for your own use-case.\n*   [Github](https://github.com/Marker-Inc-Korea/AutoRAG); [Docs](https://docs.auto-rag.com/); [Examples](https://github.com/Marker-Inc-Korea/AutoRAG-tutorial)",
    "order": 51,
    "orderInChapter": 7,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 44,
      "contentLength": 498
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#autorag",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-beam-52",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "Beam",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li><a href=\"https://beam.ai/\">Beam</a> is the leading platform for Agentic Process Automation.</li>\n</ul>",
    "contentMarkdown": "*   [Beam](https://beam.ai/) is the leading platform for Agentic Process Automation.",
    "order": 52,
    "orderInChapter": 8,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 10,
      "contentLength": 113
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#beam",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-autoagents-53",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "AutoAgents",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li><a href=\"https://github.com/Link-AGI/AutoAgents\">AutoAgents</a> is a novel framework designed for dynamic multi-agent generation and coordination, enabling language models to construct adaptive AI teams for a wide range of tasks. Unlike traditional systems that rely on static, predefined agents, AutoAgents generates task-specific agents autonomously, allowing for flexible collaboration across varied domains. The framework introduces a drafting and execution stage to handle complex task environments and facilitate effective role assignment and solution planning.</li>\n</ul>",
    "contentMarkdown": "*   [AutoAgents](https://github.com/Link-AGI/AutoAgents) is a novel framework designed for dynamic multi-agent generation and coordination, enabling language models to construct adaptive AI teams for a wide range of tasks. Unlike traditional systems that rely on static, predefined agents, AutoAgents generates task-specific agents autonomously, allowing for flexible collaboration across varied domains. The framework introduces a drafting and execution stage to handle complex task environments and facilitate effective role assignment and solution planning.",
    "order": 53,
    "orderInChapter": 9,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 69,
      "contentLength": 589
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#autoagents",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-amazon-bedrocks-ai-agent-framework-54",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "Amazon Bedrock’s AI Agent Framework",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li><a href=\"https://aws.amazon.com/bedrock/agents/\">Amazon Bedrock Agents</a> automate complex tasks by integrating with company systems, using foundation models for reasoning, memory retention, and secure execution. They support multi-agent collaboration, retrieval-augmented generation, code execution, and customizable prompt engineering for efficient workflows.</li>\n</ul>",
    "contentMarkdown": "*   [Amazon Bedrock Agents](https://aws.amazon.com/bedrock/agents/) automate complex tasks by integrating with company systems, using foundation models for reasoning, memory retention, and secure execution. They support multi-agent collaboration, retrieval-augmented generation, code execution, and customizable prompt engineering for efficient workflows.",
    "order": 54,
    "orderInChapter": 10,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 37,
      "contentLength": 384
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#amazon-bedrock’s-ai-agent-framework",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-rivet-55",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "Rivet",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li><a href=\"https://rivet.ironcladapp.com/\">Rivet</a> is a drag and drop GUI LLM workflow builder.</li>\n</ul>",
    "contentMarkdown": "*   [Rivet](https://rivet.ironcladapp.com/) is a drag and drop GUI LLM workflow builder.",
    "order": 55,
    "orderInChapter": 11,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "llm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 11,
      "contentLength": 117
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#rivet",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-vellum-56",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "Vellum",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li><a href=\"https://www.vellum.ai/\">Vellum</a> is a GUI tool for building and testing complex workflows.</li>\n</ul>",
    "contentMarkdown": "*   [Vellum](https://www.vellum.ai/) is a GUI tool for building and testing complex workflows.",
    "order": 56,
    "orderInChapter": 12,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 12,
      "contentLength": 123
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#vellum",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-babyagi-57",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "BabyAGI",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li>BabyAGI is a widely used, task-driven autonomous agent built to tackle a variety of tasks across multiple domains. It leverages advanced technologies, including OpenAI’s GPT-4 language model, the Pinecone vector search platform, and the LangChain framework. A breakdown of its core components is <a href=\"https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/\">here</a> and code is <a href=\"https://github.com/yoheinakajima/babyagi\">here</a>.</li>\n  <li>\n    <p>BabyAGI’s workflow involves the following operational steps:</p>\n\n    <ol>\n      <li><strong>Task Completion:</strong> The system processes the task at the forefront of the task list, utilizing GPT-4 in conjunction with LangChain’s chain and agent functionalities to produce a result. This result is subsequently refined, if necessary, and stored in Pinecone for future reference.</li>\n      <li><strong>Task Generation:</strong> Following the completion of a task, the system leverages GPT-4 to create new tasks, ensuring that these do not duplicate any existing tasks.</li>\n      <li><strong>Task Prioritization:</strong> The system reprioritizes the task list by evaluating the newly generated tasks and their relative importance, with GPT-4 facilitating the prioritization process.</li>\n    </ol>\n  </li>\n  <li>The following figure (<a href=\"https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/\">source</a>) illustrates the workflow of an AI agent system comprising multiple GPT-4-based agents. The user initiates the process by providing an objective and task, which is added to the Task Queue. The Execution Agent completes the task and sends the result back, which is stored in Memory for future context. The Task Creation Agent can create additional tasks based on the task results, and the Task Prioritization Agent organizes the Task Queue by prioritizing tasks to ensure an optimized flow. Memory is accessed throughout to maintain contextual relevance.</li>\n</ul>\n<p>BabyAGI’s workflow involves the following operational steps:</p>\n<ol>\n      <li><strong>Task Completion:</strong> The system processes the task at the forefront of the task list, utilizing GPT-4 in conjunction with LangChain’s chain and agent functionalities to produce a result. This result is subsequently refined, if necessary, and stored in Pinecone for future reference.</li>\n      <li><strong>Task Generation:</strong> Following the completion of a task, the system leverages GPT-4 to create new tasks, ensuring that these do not duplicate any existing tasks.</li>\n      <li><strong>Task Prioritization:</strong> The system reprioritizes the task list by evaluating the newly generated tasks and their relative importance, with GPT-4 facilitating the prioritization process.</li>\n    </ol>\n<p><img src=\"/primers/ai/assets/agents/babyAGI.png\" alt=\"\"></p>",
    "contentMarkdown": "*   BabyAGI is a widely used, task-driven autonomous agent built to tackle a variety of tasks across multiple domains. It leverages advanced technologies, including OpenAI’s GPT-4 language model, the Pinecone vector search platform, and the LangChain framework. A breakdown of its core components is [here](https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/) and code is [here](https://github.com/yoheinakajima/babyagi).\n*   BabyAGI’s workflow involves the following operational steps:\n    \n    1.  **Task Completion:** The system processes the task at the forefront of the task list, utilizing GPT-4 in conjunction with LangChain’s chain and agent functionalities to produce a result. This result is subsequently refined, if necessary, and stored in Pinecone for future reference.\n    2.  **Task Generation:** Following the completion of a task, the system leverages GPT-4 to create new tasks, ensuring that these do not duplicate any existing tasks.\n    3.  **Task Prioritization:** The system reprioritizes the task list by evaluating the newly generated tasks and their relative importance, with GPT-4 facilitating the prioritization process.\n*   The following figure ([source](https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/)) illustrates the workflow of an AI agent system comprising multiple GPT-4-based agents. The user initiates the process by providing an objective and task, which is added to the Task Queue. The Execution Agent completes the task and sends the result back, which is stored in Memory for future context. The Task Creation Agent can create additional tasks based on the task results, and the Task Prioritization Agent organizes the Task Queue by prioritizing tasks to ensure an optimized flow. Memory is accessed throughout to maintain contextual relevance.\n\nBabyAGI’s workflow involves the following operational steps:\n\n1.  **Task Completion:** The system processes the task at the forefront of the task list, utilizing GPT-4 in conjunction with LangChain’s chain and agent functionalities to produce a result. This result is subsequently refined, if necessary, and stored in Pinecone for future reference.\n2.  **Task Generation:** Following the completion of a task, the system leverages GPT-4 to create new tasks, ensuring that these do not duplicate any existing tasks.\n3.  **Task Prioritization:** The system reprioritizes the task list by evaluating the newly generated tasks and their relative importance, with GPT-4 facilitating the prioritization process.\n\n![](/primers/ai/assets/agents/babyAGI.png)",
    "order": 57,
    "orderInChapter": 13,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture",
      "gpt"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 345,
      "contentLength": 2943
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#babyagi",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-smolagents-58",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "smolagents",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li><a href=\"https://huggingface.co/blog/smolagents\">smolagents</a> by Hugging Face is a simple library to build powerful agents. The main logic for agents fits in ~1000 lines of code. So it’s really dead simple.</li>\n  <li>The main agent class is <code class=\"language-plaintext highlighter-rouge\">CodeAgent</code>, an agent that writes its actions in code. That means, contrary to the standard set by OpenAI of writing JSON blobs that call tools, this agent writes code blobs. It’s much more natural for LLMs to write actions this way, and as a result performance is much better.</li>\n  <li><a href=\"https://github.com/huggingface/smolagents\">Code</a></li>\n</ul>",
    "contentMarkdown": "*   [smolagents](https://huggingface.co/blog/smolagents) by Hugging Face is a simple library to build powerful agents. The main logic for agents fits in ~1000 lines of code. So it’s really dead simple.\n*   The main agent class is `CodeAgent`, an agent that writes its actions in code. That means, contrary to the standard set by OpenAI of writing JSON blobs that call tools, this agent writes code blobs. It’s much more natural for LLMs to write actions this way, and as a result performance is much better.\n*   [Code](https://github.com/huggingface/smolagents)",
    "order": 58,
    "orderInChapter": 14,
    "difficulty": 5,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture",
      "llm"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 86,
      "contentLength": 671
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#smolagents",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-agent-s2-59",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "Agent S2",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li>Agent S2 is an <a href=\"https://github.com/simular-ai/agent-s\">open source</a>, autonomous AI framework that offers improved performance, modularity, and scalability by combining frontier and specialized models.</li>\n  <li><a href=\"https://github.com/simular-ai/agent-s\">Code</a></li>\n</ul>",
    "contentMarkdown": "*   Agent S2 is an [open source](https://github.com/simular-ai/agent-s), autonomous AI framework that offers improved performance, modularity, and scalability by combining frontier and specialized models.\n*   [Code](https://github.com/simular-ai/agent-s)",
    "order": 59,
    "orderInChapter": 15,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 25,
      "contentLength": 301
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#agent-s2",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-open-operator-60",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Frameworks/Libraries",
    "title": "Open Operator",
    "subtitle": "Frameworks/Libraries",
    "contentHtml": "<ul>\n  <li><a href=\"https://github.com/browserbase/open-operator\">Open Operator</a> is inspired by OpenAI’s Operator feature and builds upon various open source technologies such as <a href=\"https://www.browserbase.com/\">Browserbase</a> powers the core browser automation and interaction capabilities and <a href=\"https://www.stagehand.dev/\">Stagehand</a> which handles precise DOM manipulation and state management</li>\n</ul>",
    "contentMarkdown": "*   [Open Operator](https://github.com/browserbase/open-operator) is inspired by OpenAI’s Operator feature and builds upon various open source technologies such as [Browserbase](https://www.browserbase.com/) powers the core browser automation and interaction capabilities and [Stagehand](https://www.stagehand.dev/) which handles precise DOM manipulation and state management",
    "order": 60,
    "orderInChapter": 16,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 37,
      "contentLength": 426
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#open-operator",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-data-agent-for-data-analysis-61",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Use Cases",
    "title": "Data Agent for Data Analysis",
    "subtitle": "Use Cases",
    "contentHtml": "<p><img src=\"/primers/ai/assets/agents/4.png\" alt=\"\"></p>\n<ul>\n  <li>\n    <p>The image above <a href=\"https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/\">(source)</a> visually illustrates the flow we will use below.</p>\n\n    <ol>\n      <li><strong>Identify the Use Case:</strong>\n        <ul>\n          <li>Define specific data analysis tasks, such as querying databases or analyzing financial reports.</li>\n        </ul>\n      </li>\n      <li><strong>Select the Appropriate LLM:</strong>\n        <ul>\n          <li>Choose an LLM that handles the complexity of data queries and analysis.</li>\n        </ul>\n      </li>\n      <li><strong>Agent Components:</strong>\n        <ul>\n          <li>Develop the agent with tools for data handling, a memory module for tracking interactions, and a planning module for strategic execution of tasks.</li>\n        </ul>\n      </li>\n      <li><strong>Design the Data Interaction Tools:</strong>\n        <ul>\n          <li>Implement tools for interacting with databases or other data sources.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n</ul>\n<p>The image above <a href=\"https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/\">(source)</a> visually illustrates the flow we will use below.</p>\n<ol>\n      <li><strong>Identify the Use Case:</strong>\n        <ul>\n          <li>Define specific data analysis tasks, such as querying databases or analyzing financial reports.</li>\n        </ul>\n      </li>\n      <li><strong>Select the Appropriate LLM:</strong>\n        <ul>\n          <li>Choose an LLM that handles the complexity of data queries and analysis.</li>\n        </ul>\n      </li>\n      <li><strong>Agent Components:</strong>\n        <ul>\n          <li>Develop the agent with tools for data handling, a memory module for tracking interactions, and a planning module for strategic execution of tasks.</li>\n        </ul>\n      </li>\n      <li><strong>Design the Data Interaction Tools:</strong>\n        <ul>\n          <li>Implement tools for interacting with databases or other data sources.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>Define specific data analysis tasks, such as querying databases or analyzing financial reports.</li>\n        </ul>\n<ul>\n          <li>Choose an LLM that handles the complexity of data queries and analysis.</li>\n        </ul>\n<ul>\n          <li>Develop the agent with tools for data handling, a memory module for tracking interactions, and a planning module for strategic execution of tasks.</li>\n        </ul>\n<ul>\n          <li>Implement tools for interacting with databases or other data sources.</li>\n        </ul>\n<h4 id=\"tools-setup\">Tools Setup</h4>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code20\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code20\"><span class=\"k\">class</span> <span class=\"nc\">SQLExecutor</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">database_url</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">database_url</span> <span class=\"o\">=</span> <span class=\"n\">database_url</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">execute_query</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">query</span><span class=\"p\">):</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Executing SQL query: </span><span class=\"si\">{</span><span class=\"n\">query</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"s\">\"Query results\"</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">Calculator</span><span class=\"p\">:</span>\n    <span class=\"o\">@</span><span class=\"nb\">staticmethod</span>\n    <span class=\"k\">def</span> <span class=\"nf\">perform_calculation</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">):</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Performing calculation on data: </span><span class=\"si\">{</span><span class=\"n\">data</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"s\">\"Calculation results\"</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code20\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code20\"><span class=\"k\">class</span> <span class=\"nc\">SQLExecutor</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">database_url</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">database_url</span> <span class=\"o\">=</span> <span class=\"n\">database_url</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">execute_query</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">query</span><span class=\"p\">):</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Executing SQL query: </span><span class=\"si\">{</span><span class=\"n\">query</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"s\">\"Query results\"</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">Calculator</span><span class=\"p\">:</span>\n    <span class=\"o\">@</span><span class=\"nb\">staticmethod</span>\n    <span class=\"k\">def</span> <span class=\"nf\">perform_calculation</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">):</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Performing calculation on data: </span><span class=\"si\">{</span><span class=\"n\">data</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"s\">\"Calculation results\"</span>\n</code></pre>\n<h4 id=\"agent-core-logic\">Agent Core Logic</h4>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code21\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code21\"><span class=\"k\">class</span> <span class=\"nc\">DataAgent</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">sql_executor</span><span class=\"p\">,</span> <span class=\"n\">calculator</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">sql_executor</span> <span class=\"o\">=</span> <span class=\"n\">sql_executor</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">calculator</span> <span class=\"o\">=</span> <span class=\"n\">calculator</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">memory</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">analyze_data</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">query</span><span class=\"p\">,</span> <span class=\"n\">calculation_needed</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">):</span>\n        <span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">sql_executor</span><span class=\"p\">.</span><span class=\"n\">execute_query</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">memory</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">)</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">calculation_needed</span><span class=\"p\">:</span>\n            <span class=\"n\">calculation_results</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">calculator</span><span class=\"p\">.</span><span class=\"n\">perform_calculation</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">)</span>\n            <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">memory</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">calculation_results</span><span class=\"p\">)</span>\n            <span class=\"k\">return</span> <span class=\"n\">calculation_results</span>\n        \n        <span class=\"k\">return</span> <span class=\"n\">results</span>\n\n<span class=\"n\">database_url</span> <span class=\"o\">=</span> <span class=\"s\">\"your_database_url_here\"</span>\n<span class=\"n\">sql_executor</span> <span class=\"o\">=</span> <span class=\"n\">SQLExecutor</span><span class=\"p\">(</span><span class=\"n\">database_url</span><span class=\"p\">)</span>\n<span class=\"n\">calculator</span> <span class=\"o\">=</span> <span class=\"n\">Calculator</span><span class=\"p\">()</span>\n\n<span class=\"n\">agent</span> <span class=\"o\">=</span> <span class=\"n\">DataAgent</span><span class=\"p\">(</span><span class=\"n\">sql_executor</span><span class=\"p\">,</span> <span class=\"n\">calculator</span><span class=\"p\">)</span>\n<span class=\"n\">query</span> <span class=\"o\">=</span> <span class=\"s\">\"SELECT * FROM sales_data WHERE year = 2021\"</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">agent</span><span class=\"p\">.</span><span class=\"n\">analyze_data</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">))</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code21\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code21\"><span class=\"k\">class</span> <span class=\"nc\">DataAgent</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">sql_executor</span><span class=\"p\">,</span> <span class=\"n\">calculator</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">sql_executor</span> <span class=\"o\">=</span> <span class=\"n\">sql_executor</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">calculator</span> <span class=\"o\">=</span> <span class=\"n\">calculator</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">memory</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">analyze_data</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">query</span><span class=\"p\">,</span> <span class=\"n\">calculation_needed</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">):</span>\n        <span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">sql_executor</span><span class=\"p\">.</span><span class=\"n\">execute_query</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">memory</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">)</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">calculation_needed</span><span class=\"p\">:</span>\n            <span class=\"n\">calculation_results</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">calculator</span><span class=\"p\">.</span><span class=\"n\">perform_calculation</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">)</span>\n            <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">memory</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">calculation_results</span><span class=\"p\">)</span>\n            <span class=\"k\">return</span> <span class=\"n\">calculation_results</span>\n        \n        <span class=\"k\">return</span> <span class=\"n\">results</span>\n\n<span class=\"n\">database_url</span> <span class=\"o\">=</span> <span class=\"s\">\"your_database_url_here\"</span>\n<span class=\"n\">sql_executor</span> <span class=\"o\">=</span> <span class=\"n\">SQLExecutor</span><span class=\"p\">(</span><span class=\"n\">database_url</span><span class=\"p\">)</span>\n<span class=\"n\">calculator</span> <span class=\"o\">=</span> <span class=\"n\">Calculator</span><span class=\"p\">()</span>\n\n<span class=\"n\">agent</span> <span class=\"o\">=</span> <span class=\"n\">DataAgent</span><span class=\"p\">(</span><span class=\"n\">sql_executor</span><span class=\"p\">,</span> <span class=\"n\">calculator</span><span class=\"p\">)</span>\n<span class=\"n\">query</span> <span class=\"o\">=</span> <span class=\"s\">\"SELECT * FROM sales_data WHERE year = 2021\"</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">agent</span><span class=\"p\">.</span><span class=\"n\">analyze_data</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">))</span>\n</code></pre>",
    "contentMarkdown": "![](/primers/ai/assets/agents/4.png)\n\n*   The image above [(source)](https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/) visually illustrates the flow we will use below.\n    \n    1.  **Identify the Use Case:**\n        *   Define specific data analysis tasks, such as querying databases or analyzing financial reports.\n    2.  **Select the Appropriate LLM:**\n        *   Choose an LLM that handles the complexity of data queries and analysis.\n    3.  **Agent Components:**\n        *   Develop the agent with tools for data handling, a memory module for tracking interactions, and a planning module for strategic execution of tasks.\n    4.  **Design the Data Interaction Tools:**\n        *   Implement tools for interacting with databases or other data sources.\n\nThe image above [(source)](https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-data-analysis/) visually illustrates the flow we will use below.\n\n1.  **Identify the Use Case:**\n    *   Define specific data analysis tasks, such as querying databases or analyzing financial reports.\n2.  **Select the Appropriate LLM:**\n    *   Choose an LLM that handles the complexity of data queries and analysis.\n3.  **Agent Components:**\n    *   Develop the agent with tools for data handling, a memory module for tracking interactions, and a planning module for strategic execution of tasks.\n4.  **Design the Data Interaction Tools:**\n    *   Implement tools for interacting with databases or other data sources.\n\n*   Define specific data analysis tasks, such as querying databases or analyzing financial reports.\n\n*   Choose an LLM that handles the complexity of data queries and analysis.\n\n*   Develop the agent with tools for data handling, a memory module for tracking interactions, and a planning module for strategic execution of tasks.\n\n*   Implement tools for interacting with databases or other data sources.\n\n#### Tools Setup\n\n![](https://aman.ai/images/copy.png)\n\n`class SQLExecutor:     def __init__(self, database_url):         self.database_url = database_url      def execute_query(self, query):         print(f\"Executing SQL query: {query}\")         return \"Query results\"  class Calculator:     @staticmethod     def perform_calculation(data):         print(f\"Performing calculation on data: {data}\")         return \"Calculation results\"`\n\n![](https://aman.ai/images/copy.png)\n\n`class SQLExecutor:     def __init__(self, database_url):         self.database_url = database_url      def execute_query(self, query):         print(f\"Executing SQL query: {query}\")         return \"Query results\"  class Calculator:     @staticmethod     def perform_calculation(data):         print(f\"Performing calculation on data: {data}\")         return \"Calculation results\"`\n\n#### Agent Core Logic\n\n![](https://aman.ai/images/copy.png)\n\n`class DataAgent:     def __init__(self, sql_executor, calculator):         self.sql_executor = sql_executor         self.calculator = calculator         self.memory = []      def analyze_data(self, query, calculation_needed=True):         results = self.sql_executor.execute_query(query)         self.memory.append(results)          if calculation_needed:             calculation_results = self.calculator.perform_calculation(results)             self.memory.append(calculation_results)             return calculation_results                  return results  database_url = \"your_database_url_here\" sql_executor = SQLExecutor(database_url) calculator = Calculator()  agent = DataAgent(sql_executor, calculator) query = \"SELECT * FROM sales_data WHERE year = 2021\" print(agent.analyze_data(query))`\n\n![](https://aman.ai/images/copy.png)\n\n`class DataAgent:     def __init__(self, sql_executor, calculator):         self.sql_executor = sql_executor         self.calculator = calculator         self.memory = []      def analyze_data(self, query, calculation_needed=True):         results = self.sql_executor.execute_query(query)         self.memory.append(results)          if calculation_needed:             calculation_results = self.calculator.perform_calculation(results)             self.memory.append(calculation_results)             return calculation_results                  return results  database_url = \"your_database_url_here\" sql_executor = SQLExecutor(database_url) calculator = Calculator()  agent = DataAgent(sql_executor, calculator) query = \"SELECT * FROM sales_data WHERE year = 2021\" print(agent.analyze_data(query))`",
    "order": 61,
    "orderInChapter": 1,
    "difficulty": 5,
    "estimatedMinutes": 3,
    "tags": [
      "algorithmsarchitecture",
      "llm"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 437,
      "contentLength": 14173
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#data-agent-for-data-analysis",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-llm-powered-api-agent-for-task-execution-62",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Use Cases",
    "title": "LLM-Powered API Agent for Task Execution",
    "subtitle": "Use Cases",
    "contentHtml": "<ol>\n  <li><strong>Choose an LLM:</strong>\n    <ul>\n      <li>Select a suitable LLM for handling task execution.</li>\n    </ul>\n  </li>\n  <li><strong>Select a Use Case:</strong>\n    <ul>\n      <li>Define the tasks the agent will execute.</li>\n    </ul>\n  </li>\n  <li><strong>Build the Agent:</strong>\n    <ul>\n      <li>Develop the components required for the API agent: tools, planning module, and agent core.</li>\n    </ul>\n  </li>\n  <li><strong>Define API Functions:</strong>\n    <ul>\n      <li>Create classes for each API call to the models.</li>\n    </ul>\n  </li>\n</ol>\n<ul>\n      <li>Select a suitable LLM for handling task execution.</li>\n    </ul>\n<ul>\n      <li>Define the tasks the agent will execute.</li>\n    </ul>\n<ul>\n      <li>Develop the components required for the API agent: tools, planning module, and agent core.</li>\n    </ul>\n<ul>\n      <li>Create classes for each API call to the models.</li>\n    </ul>\n<h4 id=\"python-code-example\">Python Code Example</h4>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code22\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code22\"><span class=\"k\">class</span> <span class=\"nc\">ImageGenerator</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">api_key</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">api_key</span> <span class=\"o\">=</span> <span class=\"n\">api_key</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">generate_image</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"p\">,</span> <span class=\"n\">negative_prompt</span><span class=\"o\">=</span><span class=\"s\">\"\"</span><span class=\"p\">):</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Generating image with description: </span><span class=\"si\">{</span><span class=\"n\">description</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"s\">\"Image URL or data\"</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">TextGenerator</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">api_key</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">api_key</span> <span class=\"o\">=</span> <span class=\"n\">api_key</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">generate_text</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">text_prompt</span><span class=\"p\">):</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Generating text with prompt: </span><span class=\"si\">{</span><span class=\"n\">text_prompt</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"s\">\"Generated text\"</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">CodeGenerator</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">api_key</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">api_key</span> <span class=\"o\">=</span> <span class=\"n\">api_key</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">generate_code</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">problem_description</span><span class=\"p\">):</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Generating code for: </span><span class=\"si\">{</span><span class=\"n\">problem_description</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"s\">\"Generated code\"</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code22\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code22\"><span class=\"k\">class</span> <span class=\"nc\">ImageGenerator</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">api_key</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">api_key</span> <span class=\"o\">=</span> <span class=\"n\">api_key</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">generate_image</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"p\">,</span> <span class=\"n\">negative_prompt</span><span class=\"o\">=</span><span class=\"s\">\"\"</span><span class=\"p\">):</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Generating image with description: </span><span class=\"si\">{</span><span class=\"n\">description</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"s\">\"Image URL or data\"</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">TextGenerator</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">api_key</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">api_key</span> <span class=\"o\">=</span> <span class=\"n\">api_key</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">generate_text</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">text_prompt</span><span class=\"p\">):</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Generating text with prompt: </span><span class=\"si\">{</span><span class=\"n\">text_prompt</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"s\">\"Generated text\"</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">CodeGenerator</span><span class=\"p\">:</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">api_key</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">api_key</span> <span class=\"o\">=</span> <span class=\"n\">api_key</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">generate_code</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">problem_description</span><span class=\"p\">):</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Generating code for: </span><span class=\"si\">{</span><span class=\"n\">problem_description</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"s\">\"Generated code\"</span>\n</code></pre>\n<h4 id=\"plan-and-execute-approach\">Plan-and-Execute Approach</h4>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code23\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code23\"><span class=\"k\">def</span> <span class=\"nf\">plan_and_execute</span><span class=\"p\">(</span><span class=\"n\">question</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"s\">'marketing'</span> <span class=\"ow\">in</span> <span class=\"n\">question</span><span class=\"p\">:</span>\n        <span class=\"n\">plan</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n            <span class=\"p\">{</span>\n                <span class=\"s\">\"function\"</span><span class=\"p\">:</span> <span class=\"s\">\"ImageGenerator\"</span><span class=\"p\">,</span>\n                <span class=\"s\">\"arguments\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                    <span class=\"s\">\"description\"</span><span class=\"p\">:</span> <span class=\"s\">\"A bright and clean laundry room with a large bottle of WishyWash detergent, featuring the new UltraClean formula and softener, placed prominently.\"</span><span class=\"p\">,</span>\n                    <span class=\"s\">\"negative_prompt\"</span><span class=\"p\">:</span> <span class=\"s\">\"No clutter, no other brands, only WishyWash.\"</span>\n                <span class=\"p\">}</span>\n            <span class=\"p\">},</span>\n            <span class=\"p\">{</span>\n                <span class=\"s\">\"function\"</span><span class=\"p\">:</span> <span class=\"s\">\"TextGenerator\"</span><span class=\"p\">,</span>\n                <span class=\"s\">\"arguments\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                    <span class=\"s\">\"text_prompt\"</span><span class=\"p\">:</span> <span class=\"s\">\"Compose a tweet to promote the new WishyWash detergent with the UltraClean formula and softener at $4.99. Highlight its benefits and competitive pricing.\"</span>\n                <span class=\"p\">}</span>\n            <span class=\"p\">},</span>\n            <span class=\"p\">{</span>\n                <span class=\"s\">\"function\"</span><span class=\"p\">:</span> <span class=\"s\">\"TextGenerator\"</span><span class=\"p\">,</span>\n                <span class=\"s\">\"arguments\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                    <span class=\"s\">\"text_prompt\"</span><span class=\"p\">:</span> <span class=\"s\">\"Generate ideas for marketing campaigns to increase WishyWash detergent sales, focusing on the new UltraClean formula and softener.\"</span>\n                <span class=\"p\">}</span>\n            <span class=\"p\">}</span>\n        <span class=\"p\">]</span>\n        <span class=\"k\">return</span> <span class=\"n\">plan</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">pass</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">execute_plan</span><span class=\"p\">(</span><span class=\"n\">plan</span><span class=\"p\">):</span>\n    <span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">step</span> <span class=\"ow\">in</span> <span class=\"n\">plan</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"n\">step</span><span class=\"p\">[</span><span class=\"s\">\"function\"</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"s\">\"ImageGenerator\"</span><span class=\"p\">:</span>\n            <span class=\"n\">generator</span> <span class=\"o\">=</span> <span class=\"n\">ImageGenerator</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"s\">\"your_api_key\"</span><span class=\"p\">)</span>\n            <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">generator</span><span class=\"p\">.</span><span class=\"n\">generate_image</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">step</span><span class=\"p\">[</span><span class=\"s\">\"arguments\"</span><span class=\"p\">])</span>\n            <span class=\"n\">results</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n        <span class=\"k\">elif</span> <span class=\"n\">step</span><span class=\"p\">[</span><span class=\"s\">\"function\"</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"s\">\"TextGenerator\"</span><span class=\"p\">:</span>\n            <span class=\"n\">generator</span> <span class=\"o\">=</span> <span class=\"n\">TextGenerator</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"s\">\"your_api_key\"</span><span class=\"p\">)</span>\n            <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">generator</span><span class=\"p\">.</span><span class=\"n\">generate_text</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">step</span><span class=\"p\">[</span><span class=\"s\">\"arguments\"</span><span class=\"p\">])</span>\n            <span class=\"n\">results</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n        <span class=\"k\">elif</span> <span class=\"n\">step</span><span class=\"p\">[</span><span class=\"s\">\"function\"</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"s\">\"CodeGenerator\"</span><span class=\"p\">:</span>\n            <span class=\"n\">generator</span> <span class=\"o\">=</span> <span class=\"n\">CodeGenerator</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"s\">\"your_api_key\"</span><span class=\"p\">)</span>\n            <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">generator</span><span class=\"p\">.</span><span class=\"n\">generate_code</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">step</span><span class=\"p\">[</span><span class=\"s\">\"arguments\"</span><span class=\"p\">])</span>\n            <span class=\"n\">results</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">results</span>\n\n<span class=\"n\">question</span> <span class=\"o\">=</span> <span class=\"s\">\"How can we create a marketing campaign for our new detergent?\"</span>\n<span class=\"n\">plan</span> <span class=\"o\">=</span> <span class=\"n\">plan_and_execute</span><span class=\"p\">(</span><span class=\"n\">question</span><span class=\"p\">)</span>\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"n\">execute_plan</span><span class=\"p\">(</span><span class=\"n\">plan</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">result</span> <span class=\"ow\">in</span> <span class=\"n\">results</span><span class=\"p\">:</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code23\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code23\"><span class=\"k\">def</span> <span class=\"nf\">plan_and_execute</span><span class=\"p\">(</span><span class=\"n\">question</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"s\">'marketing'</span> <span class=\"ow\">in</span> <span class=\"n\">question</span><span class=\"p\">:</span>\n        <span class=\"n\">plan</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n            <span class=\"p\">{</span>\n                <span class=\"s\">\"function\"</span><span class=\"p\">:</span> <span class=\"s\">\"ImageGenerator\"</span><span class=\"p\">,</span>\n                <span class=\"s\">\"arguments\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                    <span class=\"s\">\"description\"</span><span class=\"p\">:</span> <span class=\"s\">\"A bright and clean laundry room with a large bottle of WishyWash detergent, featuring the new UltraClean formula and softener, placed prominently.\"</span><span class=\"p\">,</span>\n                    <span class=\"s\">\"negative_prompt\"</span><span class=\"p\">:</span> <span class=\"s\">\"No clutter, no other brands, only WishyWash.\"</span>\n                <span class=\"p\">}</span>\n            <span class=\"p\">},</span>\n            <span class=\"p\">{</span>\n                <span class=\"s\">\"function\"</span><span class=\"p\">:</span> <span class=\"s\">\"TextGenerator\"</span><span class=\"p\">,</span>\n                <span class=\"s\">\"arguments\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                    <span class=\"s\">\"text_prompt\"</span><span class=\"p\">:</span> <span class=\"s\">\"Compose a tweet to promote the new WishyWash detergent with the UltraClean formula and softener at $4.99. Highlight its benefits and competitive pricing.\"</span>\n                <span class=\"p\">}</span>\n            <span class=\"p\">},</span>\n            <span class=\"p\">{</span>\n                <span class=\"s\">\"function\"</span><span class=\"p\">:</span> <span class=\"s\">\"TextGenerator\"</span><span class=\"p\">,</span>\n                <span class=\"s\">\"arguments\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                    <span class=\"s\">\"text_prompt\"</span><span class=\"p\">:</span> <span class=\"s\">\"Generate ideas for marketing campaigns to increase WishyWash detergent sales, focusing on the new UltraClean formula and softener.\"</span>\n                <span class=\"p\">}</span>\n            <span class=\"p\">}</span>\n        <span class=\"p\">]</span>\n        <span class=\"k\">return</span> <span class=\"n\">plan</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">pass</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">execute_plan</span><span class=\"p\">(</span><span class=\"n\">plan</span><span class=\"p\">):</span>\n    <span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">step</span> <span class=\"ow\">in</span> <span class=\"n\">plan</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"n\">step</span><span class=\"p\">[</span><span class=\"s\">\"function\"</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"s\">\"ImageGenerator\"</span><span class=\"p\">:</span>\n            <span class=\"n\">generator</span> <span class=\"o\">=</span> <span class=\"n\">ImageGenerator</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"s\">\"your_api_key\"</span><span class=\"p\">)</span>\n            <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">generator</span><span class=\"p\">.</span><span class=\"n\">generate_image</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">step</span><span class=\"p\">[</span><span class=\"s\">\"arguments\"</span><span class=\"p\">])</span>\n            <span class=\"n\">results</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n        <span class=\"k\">elif</span> <span class=\"n\">step</span><span class=\"p\">[</span><span class=\"s\">\"function\"</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"s\">\"TextGenerator\"</span><span class=\"p\">:</span>\n            <span class=\"n\">generator</span> <span class=\"o\">=</span> <span class=\"n\">TextGenerator</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"s\">\"your_api_key\"</span><span class=\"p\">)</span>\n            <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">generator</span><span class=\"p\">.</span><span class=\"n\">generate_text</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">step</span><span class=\"p\">[</span><span class=\"s\">\"arguments\"</span><span class=\"p\">])</span>\n            <span class=\"n\">results</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n        <span class=\"k\">elif</span> <span class=\"n\">step</span><span class=\"p\">[</span><span class=\"s\">\"function\"</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"s\">\"CodeGenerator\"</span><span class=\"p\">:</span>\n            <span class=\"n\">generator</span> <span class=\"o\">=</span> <span class=\"n\">CodeGenerator</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"s\">\"your_api_key\"</span><span class=\"p\">)</span>\n            <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">generator</span><span class=\"p\">.</span><span class=\"n\">generate_code</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">step</span><span class=\"p\">[</span><span class=\"s\">\"arguments\"</span><span class=\"p\">])</span>\n            <span class=\"n\">results</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">results</span>\n\n<span class=\"n\">question</span> <span class=\"o\">=</span> <span class=\"s\">\"How can we create a marketing campaign for our new detergent?\"</span>\n<span class=\"n\">plan</span> <span class=\"o\">=</span> <span class=\"n\">plan_and_execute</span><span class=\"p\">(</span><span class=\"n\">question</span><span class=\"p\">)</span>\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"n\">execute_plan</span><span class=\"p\">(</span><span class=\"n\">plan</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">result</span> <span class=\"ow\">in</span> <span class=\"n\">results</span><span class=\"p\">:</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n</code></pre>",
    "contentMarkdown": "1.  **Choose an LLM:**\n    *   Select a suitable LLM for handling task execution.\n2.  **Select a Use Case:**\n    *   Define the tasks the agent will execute.\n3.  **Build the Agent:**\n    *   Develop the components required for the API agent: tools, planning module, and agent core.\n4.  **Define API Functions:**\n    *   Create classes for each API call to the models.\n\n*   Select a suitable LLM for handling task execution.\n\n*   Define the tasks the agent will execute.\n\n*   Develop the components required for the API agent: tools, planning module, and agent core.\n\n*   Create classes for each API call to the models.\n\n#### Python Code Example\n\n![](https://aman.ai/images/copy.png)\n\n`class ImageGenerator:     def __init__(self, api_key):         self.api_key = api_key      def generate_image(self, description, negative_prompt=\"\"):         print(f\"Generating image with description: {description}\")         return \"Image URL or data\"  class TextGenerator:     def __init__(self, api_key):         self.api_key = api_key      def generate_text(self, text_prompt):         print(f\"Generating text with prompt: {text_prompt}\")         return \"Generated text\"  class CodeGenerator:     def __init__(self, api_key):         self.api_key = api_key      def generate_code(self, problem_description):         print(f\"Generating code for: {problem_description}\")         return \"Generated code\"`\n\n![](https://aman.ai/images/copy.png)\n\n`class ImageGenerator:     def __init__(self, api_key):         self.api_key = api_key      def generate_image(self, description, negative_prompt=\"\"):         print(f\"Generating image with description: {description}\")         return \"Image URL or data\"  class TextGenerator:     def __init__(self, api_key):         self.api_key = api_key      def generate_text(self, text_prompt):         print(f\"Generating text with prompt: {text_prompt}\")         return \"Generated text\"  class CodeGenerator:     def __init__(self, api_key):         self.api_key = api_key      def generate_code(self, problem_description):         print(f\"Generating code for: {problem_description}\")         return \"Generated code\"`\n\n#### Plan-and-Execute Approach\n\n![](https://aman.ai/images/copy.png)\n\n`def plan_and_execute(question):     if 'marketing' in question:         plan = [             {                 \"function\": \"ImageGenerator\",                 \"arguments\": {                     \"description\": \"A bright and clean laundry room with a large bottle of WishyWash detergent, featuring the new UltraClean formula and softener, placed prominently.\",                     \"negative_prompt\": \"No clutter, no other brands, only WishyWash.\"                 }             },             {                 \"function\": \"TextGenerator\",                 \"arguments\": {                     \"text_prompt\": \"Compose a tweet to promote the new WishyWash detergent with the UltraClean formula and softener at $4.99. Highlight its benefits and competitive pricing.\"                 }             },             {                 \"function\": \"TextGenerator\",                 \"arguments\": {                     \"text_prompt\": \"Generate ideas for marketing campaigns to increase WishyWash detergent sales, focusing on the new UltraClean formula and softener.\"                 }             }         ]         return plan     else:         pass  def execute_plan(plan):     results = []     for step in plan:         if step[\"function\"] == \"ImageGenerator\":             generator = ImageGenerator(api_key=\"your_api_key\")             result = generator.generate_image(**step[\"arguments\"])             results.append(result)         elif step[\"function\"] == \"TextGenerator\":             generator = TextGenerator(api_key=\"your_api_key\")             result = generator.generate_text(**step[\"arguments\"])             results.append(result)         elif step[\"function\"] == \"CodeGenerator\":             generator = CodeGenerator(api_key=\"your_api_key\")             result = generator.generate_code(**step[\"arguments\"])             results.append(result)     return results  question = \"How can we create a marketing campaign for our new detergent?\" plan = plan_and_execute(question) results = execute_plan(plan) for result in results:     print(result)`\n\n![](https://aman.ai/images/copy.png)\n\n`def plan_and_execute(question):     if 'marketing' in question:         plan = [             {                 \"function\": \"ImageGenerator\",                 \"arguments\": {                     \"description\": \"A bright and clean laundry room with a large bottle of WishyWash detergent, featuring the new UltraClean formula and softener, placed prominently.\",                     \"negative_prompt\": \"No clutter, no other brands, only WishyWash.\"                 }             },             {                 \"function\": \"TextGenerator\",                 \"arguments\": {                     \"text_prompt\": \"Compose a tweet to promote the new WishyWash detergent with the UltraClean formula and softener at $4.99. Highlight its benefits and competitive pricing.\"                 }             },             {                 \"function\": \"TextGenerator\",                 \"arguments\": {                     \"text_prompt\": \"Generate ideas for marketing campaigns to increase WishyWash detergent sales, focusing on the new UltraClean formula and softener.\"                 }             }         ]         return plan     else:         pass  def execute_plan(plan):     results = []     for step in plan:         if step[\"function\"] == \"ImageGenerator\":             generator = ImageGenerator(api_key=\"your_api_key\")             result = generator.generate_image(**step[\"arguments\"])             results.append(result)         elif step[\"function\"] == \"TextGenerator\":             generator = TextGenerator(api_key=\"your_api_key\")             result = generator.generate_text(**step[\"arguments\"])             results.append(result)         elif step[\"function\"] == \"CodeGenerator\":             generator = CodeGenerator(api_key=\"your_api_key\")             result = generator.generate_code(**step[\"arguments\"])             results.append(result)     return results  question = \"How can we create a marketing campaign for our new detergent?\" plan = plan_and_execute(question) results = execute_plan(plan) for result in results:     print(result)`",
    "order": 62,
    "orderInChapter": 2,
    "difficulty": 5,
    "estimatedMinutes": 3,
    "tags": [
      "algorithmsarchitecture",
      "llm"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 584,
      "contentLength": 21847
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#llm-powered-api-agent-for-task-execution",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-core-dimensions-of-responsible-ai-agents-63",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Responsible AI Agents",
    "title": "Core Dimensions of Responsible AI Agents",
    "subtitle": "Responsible AI Agents",
    "contentHtml": "<ul>\n  <li><strong>Safety</strong>: Agents must avoid harmful actions, even under adversarial inputs or environmental changes.</li>\n  <li><strong>Alignment</strong>: Agent behaviors must reflect human intentions and values, especially as agents pursue independent goals.</li>\n  <li><strong>Reliability</strong>: Agents must perform predictably across diverse conditions and handle uncertainty gracefully.</li>\n  <li><strong>Oversight and Accountability</strong>: There must be clear mechanisms for monitoring, auditing, and intervening in agent behavior.</li>\n  <li><strong>Fairness</strong>: Agents should avoid perpetuating biases and should treat users equitably.</li>\n  <li><strong>Privacy and Data Protection</strong>: Agents must process user data securely and comply with privacy standards.</li>\n</ul>",
    "contentMarkdown": "*   **Safety**: Agents must avoid harmful actions, even under adversarial inputs or environmental changes.\n*   **Alignment**: Agent behaviors must reflect human intentions and values, especially as agents pursue independent goals.\n*   **Reliability**: Agents must perform predictably across diverse conditions and handle uncertainty gracefully.\n*   **Oversight and Accountability**: There must be clear mechanisms for monitoring, auditing, and intervening in agent behavior.\n*   **Fairness**: Agents should avoid perpetuating biases and should treat users equitably.\n*   **Privacy and Data Protection**: Agents must process user data securely and comply with privacy standards.",
    "order": 63,
    "orderInChapter": 1,
    "difficulty": 4,
    "estimatedMinutes": 1,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 88,
      "contentLength": 808
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#core-dimensions-of-responsible-ai-agents",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-responsible-architectures-for-agentic-systems-64",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Responsible AI Agents",
    "title": "Responsible Architectures for Agentic Systems",
    "subtitle": "Responsible AI Agents",
    "contentHtml": "<ul>\n  <li>\n    <p><a href=\"https://arxiv.org/pdf/2311.13148\">Lu et al. (2024)</a> propose a modular architecture for responsible agent design, emphasizing the need for built-in constraints, safe memory structures, planning checks, and execution controls.</p>\n  </li>\n  <li>\n    <p>The image below <a href=\"https://arxiv.org/abs/2311.13148\">(source)</a> illustrates the architecture of a responsible agent-based ecosystem proposed by <a href=\"https://arxiv.org/pdf/2311.13148\">Lu et al. (2024)</a>, highlighting key components like interaction engineering, memory safety, planning constraints, and RAI plugins.</p>\n  </li>\n</ul>\n<p><a href=\"https://arxiv.org/pdf/2311.13148\">Lu et al. (2024)</a> propose a modular architecture for responsible agent design, emphasizing the need for built-in constraints, safe memory structures, planning checks, and execution controls.</p>\n<p>The image below <a href=\"https://arxiv.org/abs/2311.13148\">(source)</a> illustrates the architecture of a responsible agent-based ecosystem proposed by <a href=\"https://arxiv.org/pdf/2311.13148\">Lu et al. (2024)</a>, highlighting key components like interaction engineering, memory safety, planning constraints, and RAI plugins.</p>\n<p><img src=\"/primers/ai/assets/agents/agent_ecosystem_architecture.jpg\" alt=\"\"></p>\n<ul>\n  <li>\n    <p>Their architecture includes:</p>\n\n    <ul>\n      <li><strong>Interaction Engineering</strong>: Designing goal creation mechanisms that limit dangerous autonomy, such as using passive goal generators or human-in-the-loop goal acceptance.</li>\n      <li><strong>Memory Systems</strong>: Structuring memory with safe retrieval mechanisms, separating working memory from permanent storage to minimize risky hallucinations.</li>\n      <li><strong>Planning Module Constraints</strong>: Enabling multi-agent planning through voting-based, debate-based, or role-based cooperation, improving robustness before critical actions.</li>\n      <li><strong>Execution Engines with Guardrails</strong>: Adding middleware between the planner and actuators, incorporating filters like RAG-based retrieval, multimodal validation, and affordance estimation to screen unsafe actions.</li>\n      <li><strong>Responsible AI (RAI) Plugins</strong>: Plug-and-play modules such as black-box recorders, explainers, ethical rule checkers, and self-critique evaluators, integrated into every agent cycle.</li>\n    </ul>\n  </li>\n</ul>\n<p>Their architecture includes:</p>\n<ul>\n      <li><strong>Interaction Engineering</strong>: Designing goal creation mechanisms that limit dangerous autonomy, such as using passive goal generators or human-in-the-loop goal acceptance.</li>\n      <li><strong>Memory Systems</strong>: Structuring memory with safe retrieval mechanisms, separating working memory from permanent storage to minimize risky hallucinations.</li>\n      <li><strong>Planning Module Constraints</strong>: Enabling multi-agent planning through voting-based, debate-based, or role-based cooperation, improving robustness before critical actions.</li>\n      <li><strong>Execution Engines with Guardrails</strong>: Adding middleware between the planner and actuators, incorporating filters like RAG-based retrieval, multimodal validation, and affordance estimation to screen unsafe actions.</li>\n      <li><strong>Responsible AI (RAI) Plugins</strong>: Plug-and-play modules such as black-box recorders, explainers, ethical rule checkers, and self-critique evaluators, integrated into every agent cycle.</li>\n    </ul>",
    "contentMarkdown": "*   [Lu et al. (2024)](https://arxiv.org/pdf/2311.13148) propose a modular architecture for responsible agent design, emphasizing the need for built-in constraints, safe memory structures, planning checks, and execution controls.\n    \n*   The image below [(source)](https://arxiv.org/abs/2311.13148) illustrates the architecture of a responsible agent-based ecosystem proposed by [Lu et al. (2024)](https://arxiv.org/pdf/2311.13148), highlighting key components like interaction engineering, memory safety, planning constraints, and RAI plugins.\n    \n\n[Lu et al. (2024)](https://arxiv.org/pdf/2311.13148) propose a modular architecture for responsible agent design, emphasizing the need for built-in constraints, safe memory structures, planning checks, and execution controls.\n\nThe image below [(source)](https://arxiv.org/abs/2311.13148) illustrates the architecture of a responsible agent-based ecosystem proposed by [Lu et al. (2024)](https://arxiv.org/pdf/2311.13148), highlighting key components like interaction engineering, memory safety, planning constraints, and RAI plugins.\n\n![](/primers/ai/assets/agents/agent_ecosystem_architecture.jpg)\n\n*   Their architecture includes:\n    \n    *   **Interaction Engineering**: Designing goal creation mechanisms that limit dangerous autonomy, such as using passive goal generators or human-in-the-loop goal acceptance.\n    *   **Memory Systems**: Structuring memory with safe retrieval mechanisms, separating working memory from permanent storage to minimize risky hallucinations.\n    *   **Planning Module Constraints**: Enabling multi-agent planning through voting-based, debate-based, or role-based cooperation, improving robustness before critical actions.\n    *   **Execution Engines with Guardrails**: Adding middleware between the planner and actuators, incorporating filters like RAG-based retrieval, multimodal validation, and affordance estimation to screen unsafe actions.\n    *   **Responsible AI (RAI) Plugins**: Plug-and-play modules such as black-box recorders, explainers, ethical rule checkers, and self-critique evaluators, integrated into every agent cycle.\n\nTheir architecture includes:\n\n*   **Interaction Engineering**: Designing goal creation mechanisms that limit dangerous autonomy, such as using passive goal generators or human-in-the-loop goal acceptance.\n*   **Memory Systems**: Structuring memory with safe retrieval mechanisms, separating working memory from permanent storage to minimize risky hallucinations.\n*   **Planning Module Constraints**: Enabling multi-agent planning through voting-based, debate-based, or role-based cooperation, improving robustness before critical actions.\n*   **Execution Engines with Guardrails**: Adding middleware between the planner and actuators, incorporating filters like RAG-based retrieval, multimodal validation, and affordance estimation to screen unsafe actions.\n*   **Responsible AI (RAI) Plugins**: Plug-and-play modules such as black-box recorders, explainers, ethical rule checkers, and self-critique evaluators, integrated into every agent cycle.",
    "order": 64,
    "orderInChapter": 2,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 338,
      "contentLength": 3504
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#responsible-architectures-for-agentic-systems",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-governance-practices-for-agentic-ai-systems-65",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Responsible AI Agents",
    "title": "Governance Practices for Agentic AI Systems",
    "subtitle": "Responsible AI Agents",
    "contentHtml": "<ul>\n  <li>\n    <p><a href=\"https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf\">Shavit et al. (2024)</a> outline a full life-cycle governance model for agentic AI systems, covering developers, deployers, users, and regulators. Key practices include:</p>\n\n    <ul>\n      <li><strong>Evaluate Task Suitability</strong>: Assess if an agentic system is appropriate for a given use case or if simpler automation would be safer.</li>\n      <li><strong>Constrain Action Spaces</strong>: Strictly limit the agent’s available actions and tool access by default, only expanding capabilities based on demonstrated safety.</li>\n      <li><strong>Default Safe Behaviors</strong>: Configure safe fallback plans, such as agent shutdown or user escalation, when agents encounter uncertainty.</li>\n      <li><strong>Ensure Legibility</strong>: Design agents to generate explanations, self-reports, or summaries of their goals, plans, and actions to enhance transparency for users and overseers.</li>\n      <li><strong>Continuous Monitoring and Logging</strong>: Implement robust observability frameworks to track agent reasoning steps, tool calls, and environmental interactions.</li>\n      <li><strong>Establish Accountability Mechanisms</strong>: Assign responsibility for agent actions through contracts, insurance, and internal auditing systems.</li>\n    </ul>\n  </li>\n</ul>\n<p><a href=\"https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf\">Shavit et al. (2024)</a> outline a full life-cycle governance model for agentic AI systems, covering developers, deployers, users, and regulators. Key practices include:</p>\n<ul>\n      <li><strong>Evaluate Task Suitability</strong>: Assess if an agentic system is appropriate for a given use case or if simpler automation would be safer.</li>\n      <li><strong>Constrain Action Spaces</strong>: Strictly limit the agent’s available actions and tool access by default, only expanding capabilities based on demonstrated safety.</li>\n      <li><strong>Default Safe Behaviors</strong>: Configure safe fallback plans, such as agent shutdown or user escalation, when agents encounter uncertainty.</li>\n      <li><strong>Ensure Legibility</strong>: Design agents to generate explanations, self-reports, or summaries of their goals, plans, and actions to enhance transparency for users and overseers.</li>\n      <li><strong>Continuous Monitoring and Logging</strong>: Implement robust observability frameworks to track agent reasoning steps, tool calls, and environmental interactions.</li>\n      <li><strong>Establish Accountability Mechanisms</strong>: Assign responsibility for agent actions through contracts, insurance, and internal auditing systems.</li>\n    </ul>",
    "contentMarkdown": "*   [Shavit et al. (2024)](https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf) outline a full life-cycle governance model for agentic AI systems, covering developers, deployers, users, and regulators. Key practices include:\n    \n    *   **Evaluate Task Suitability**: Assess if an agentic system is appropriate for a given use case or if simpler automation would be safer.\n    *   **Constrain Action Spaces**: Strictly limit the agent’s available actions and tool access by default, only expanding capabilities based on demonstrated safety.\n    *   **Default Safe Behaviors**: Configure safe fallback plans, such as agent shutdown or user escalation, when agents encounter uncertainty.\n    *   **Ensure Legibility**: Design agents to generate explanations, self-reports, or summaries of their goals, plans, and actions to enhance transparency for users and overseers.\n    *   **Continuous Monitoring and Logging**: Implement robust observability frameworks to track agent reasoning steps, tool calls, and environmental interactions.\n    *   **Establish Accountability Mechanisms**: Assign responsibility for agent actions through contracts, insurance, and internal auditing systems.\n\n[Shavit et al. (2024)](https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf) outline a full life-cycle governance model for agentic AI systems, covering developers, deployers, users, and regulators. Key practices include:\n\n*   **Evaluate Task Suitability**: Assess if an agentic system is appropriate for a given use case or if simpler automation would be safer.\n*   **Constrain Action Spaces**: Strictly limit the agent’s available actions and tool access by default, only expanding capabilities based on demonstrated safety.\n*   **Default Safe Behaviors**: Configure safe fallback plans, such as agent shutdown or user escalation, when agents encounter uncertainty.\n*   **Ensure Legibility**: Design agents to generate explanations, self-reports, or summaries of their goals, plans, and actions to enhance transparency for users and overseers.\n*   **Continuous Monitoring and Logging**: Implement robust observability frameworks to track agent reasoning steps, tool calls, and environmental interactions.\n*   **Establish Accountability Mechanisms**: Assign responsibility for agent actions through contracts, insurance, and internal auditing systems.",
    "order": 65,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 293,
      "contentLength": 2740
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#governance-practices-for-agentic-ai-systems",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-implementation-details-for-responsible-agents-66",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Responsible AI Agents",
    "title": "Implementation Details for Responsible Agents",
    "subtitle": "Responsible AI Agents",
    "contentHtml": "<ul>\n  <li>\n    <p>Based on <a href=\"https://arxiv.org/pdf/2311.13148\">Lu et al. (2024)</a> and <a href=\"https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf\">Shavit et al. (2024)</a>, responsible agent development can be operationalized through the following techniques:</p>\n\n    <ul>\n      <li><strong>Goal Creation Interfaces</strong>: Implement passive goal creators that require explicit user approval before initiating complex workflows.</li>\n      <li><strong>Memory Safety Filters</strong>: Deploy retrieval guards to prevent the agent from inserting unverified external information into planning stages.</li>\n      <li><strong>Constrained Planning Algorithms</strong>: Restrict planning algorithms to verifiable tool chains and avoid open-ended execution flows unless carefully sandboxed.</li>\n      <li><strong>Pre-Execution Checks</strong>: Use RAI Plugins like ethics checkers or adversarial simulators to validate plans before critical execution steps.</li>\n      <li><strong>Post-Hoc Auditing Tools</strong>: Record entire agent trajectories using black-box recorders for external auditors to review and investigate.</li>\n      <li><strong>Adjustable Autonomy Levels</strong>: Enable dynamic risk scaling, where agents can operate semi-autonomously during low-risk tasks but require human approval for high-risk operations.</li>\n      <li><strong>Feedback Loops</strong>: Incorporate human feedback in deployment via reinforcement learning or preference modeling to refine agent behaviors over time.</li>\n    </ul>\n  </li>\n</ul>\n<p>Based on <a href=\"https://arxiv.org/pdf/2311.13148\">Lu et al. (2024)</a> and <a href=\"https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf\">Shavit et al. (2024)</a>, responsible agent development can be operationalized through the following techniques:</p>\n<ul>\n      <li><strong>Goal Creation Interfaces</strong>: Implement passive goal creators that require explicit user approval before initiating complex workflows.</li>\n      <li><strong>Memory Safety Filters</strong>: Deploy retrieval guards to prevent the agent from inserting unverified external information into planning stages.</li>\n      <li><strong>Constrained Planning Algorithms</strong>: Restrict planning algorithms to verifiable tool chains and avoid open-ended execution flows unless carefully sandboxed.</li>\n      <li><strong>Pre-Execution Checks</strong>: Use RAI Plugins like ethics checkers or adversarial simulators to validate plans before critical execution steps.</li>\n      <li><strong>Post-Hoc Auditing Tools</strong>: Record entire agent trajectories using black-box recorders for external auditors to review and investigate.</li>\n      <li><strong>Adjustable Autonomy Levels</strong>: Enable dynamic risk scaling, where agents can operate semi-autonomously during low-risk tasks but require human approval for high-risk operations.</li>\n      <li><strong>Feedback Loops</strong>: Incorporate human feedback in deployment via reinforcement learning or preference modeling to refine agent behaviors over time.</li>\n    </ul>",
    "contentMarkdown": "*   Based on [Lu et al. (2024)](https://arxiv.org/pdf/2311.13148) and [Shavit et al. (2024)](https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf), responsible agent development can be operationalized through the following techniques:\n    \n    *   **Goal Creation Interfaces**: Implement passive goal creators that require explicit user approval before initiating complex workflows.\n    *   **Memory Safety Filters**: Deploy retrieval guards to prevent the agent from inserting unverified external information into planning stages.\n    *   **Constrained Planning Algorithms**: Restrict planning algorithms to verifiable tool chains and avoid open-ended execution flows unless carefully sandboxed.\n    *   **Pre-Execution Checks**: Use RAI Plugins like ethics checkers or adversarial simulators to validate plans before critical execution steps.\n    *   **Post-Hoc Auditing Tools**: Record entire agent trajectories using black-box recorders for external auditors to review and investigate.\n    *   **Adjustable Autonomy Levels**: Enable dynamic risk scaling, where agents can operate semi-autonomously during low-risk tasks but require human approval for high-risk operations.\n    *   **Feedback Loops**: Incorporate human feedback in deployment via reinforcement learning or preference modeling to refine agent behaviors over time.\n\nBased on [Lu et al. (2024)](https://arxiv.org/pdf/2311.13148) and [Shavit et al. (2024)](https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf), responsible agent development can be operationalized through the following techniques:\n\n*   **Goal Creation Interfaces**: Implement passive goal creators that require explicit user approval before initiating complex workflows.\n*   **Memory Safety Filters**: Deploy retrieval guards to prevent the agent from inserting unverified external information into planning stages.\n*   **Constrained Planning Algorithms**: Restrict planning algorithms to verifiable tool chains and avoid open-ended execution flows unless carefully sandboxed.\n*   **Pre-Execution Checks**: Use RAI Plugins like ethics checkers or adversarial simulators to validate plans before critical execution steps.\n*   **Post-Hoc Auditing Tools**: Record entire agent trajectories using black-box recorders for external auditors to review and investigate.\n*   **Adjustable Autonomy Levels**: Enable dynamic risk scaling, where agents can operate semi-autonomously during low-risk tasks but require human approval for high-risk operations.\n*   **Feedback Loops**: Incorporate human feedback in deployment via reinforcement learning or preference modeling to refine agent behaviors over time.",
    "order": 66,
    "orderInChapter": 4,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture",
      "reinforcement learning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 313,
      "contentLength": 3098
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#implementation-details-for-responsible-agents",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-challenges-and-open-research-questions-67",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Responsible AI Agents",
    "title": "Challenges and Open Research Questions",
    "subtitle": "Responsible AI Agents",
    "contentHtml": "<ul>\n  <li>\n    <p><a href=\"https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf\">Shavit et al. (2024)</a> note that agentic systems introduce novel governance challenges compared to traditional software, including:</p>\n\n    <ul>\n      <li><strong>Scaling Oversight</strong>: How to supervise agents operating faster and more broadly than human monitors.</li>\n      <li><strong>Value Drift</strong>: How to ensure agents’ goals remain aligned over long timescales and diverse environments.</li>\n      <li><strong>Emergent Cooperation</strong>: How to prevent networks of agents from colluding or misbehaving in unintended ways.</li>\n      <li><strong>Cross-Organizational Accountability</strong>: How to allocate liability between developers, deployers, and users when harms occur.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Addressing these issues will require deeper collaboration between AI developers, ethicists, regulators, and society at large to build safe and trustworthy agentic ecosystems.</p>\n  </li>\n</ul>\n<p><a href=\"https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf\">Shavit et al. (2024)</a> note that agentic systems introduce novel governance challenges compared to traditional software, including:</p>\n<ul>\n      <li><strong>Scaling Oversight</strong>: How to supervise agents operating faster and more broadly than human monitors.</li>\n      <li><strong>Value Drift</strong>: How to ensure agents’ goals remain aligned over long timescales and diverse environments.</li>\n      <li><strong>Emergent Cooperation</strong>: How to prevent networks of agents from colluding or misbehaving in unintended ways.</li>\n      <li><strong>Cross-Organizational Accountability</strong>: How to allocate liability between developers, deployers, and users when harms occur.</li>\n    </ul>\n<p>Addressing these issues will require deeper collaboration between AI developers, ethicists, regulators, and society at large to build safe and trustworthy agentic ecosystems.</p>",
    "contentMarkdown": "*   [Shavit et al. (2024)](https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf) note that agentic systems introduce novel governance challenges compared to traditional software, including:\n    \n    *   **Scaling Oversight**: How to supervise agents operating faster and more broadly than human monitors.\n    *   **Value Drift**: How to ensure agents’ goals remain aligned over long timescales and diverse environments.\n    *   **Emergent Cooperation**: How to prevent networks of agents from colluding or misbehaving in unintended ways.\n    *   **Cross-Organizational Accountability**: How to allocate liability between developers, deployers, and users when harms occur.\n*   Addressing these issues will require deeper collaboration between AI developers, ethicists, regulators, and society at large to build safe and trustworthy agentic ecosystems.\n    \n\n[Shavit et al. (2024)](https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf) note that agentic systems introduce novel governance challenges compared to traditional software, including:\n\n*   **Scaling Oversight**: How to supervise agents operating faster and more broadly than human monitors.\n*   **Value Drift**: How to ensure agents’ goals remain aligned over long timescales and diverse environments.\n*   **Emergent Cooperation**: How to prevent networks of agents from colluding or misbehaving in unintended ways.\n*   **Cross-Organizational Accountability**: How to allocate liability between developers, deployers, and users when harms occur.\n\nAddressing these issues will require deeper collaboration between AI developers, ethicists, regulators, and society at large to build safe and trustworthy agentic ecosystems.",
    "order": 67,
    "orderInChapter": 5,
    "difficulty": 4,
    "estimatedMinutes": 2,
    "tags": [
      "algorithmsarchitecture"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 206,
      "contentLength": 2017
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#challenges-and-open-research-questions",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-reflection-68",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Related Papers",
    "title": "Reflection",
    "subtitle": "Related Papers",
    "contentHtml": "<h4 id=\"self-refine-iterative-refinement-with-self-feedback\"><a href=\"https://arxiv.org/abs/2303.17651\">Self-Refine: Iterative Refinement with Self-Feedback</a></h4>\n<ul>\n  <li>Like humans, large language models (LLMs) do not always generate the best output on their first try.</li>\n  <li>This paper by Madaan et al. from CMU, Allen AI, UW, NVIDIA, UC San Diego, and Google Research introduces a novel approach for enhancing outputs from large language models (LLMs) like GPT-3.5 and GPT-4 through self-generated iterative feedback and refinement, without the need for additional training data or supervised learning – similar to how humans refine their written text.</li>\n  <li>Put simply, the main idea is to generate an initial output using an LLM; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. This process repeats until a predefined condition is met. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider.</li>\n  <li>The figure below from the paper shows that given an input (step 0), Self-Refine starts by generating an output and passing it back to the same model M to get feedback (step 1). The feedback is passed back to M, which refines the previously generated output (step 2). Steps (step 1) and (step 2) iterate until a stopping condition is met. SELF-REFINE is instantiated with a language model such as GPT-3.5 and does not involve human assistance.</li>\n</ul>\n<p><img src=\"../../../images/papers/Self-Refine.jpg\" alt=\"\"></p>\n<ul>\n  <li>The approach is evaluated across seven diverse tasks, including dialogue response and code optimization, demonstrating significant improvements over conventional one-step generation methods. This method leverages few-shot prompting for guiding the LLM to generate feedback and incorporate it for output refinement.</li>\n  <li>The results show that Self-Refine significantly enhances output quality in terms of human preference and task-specific metrics, indicating its potential to improve LLM-generated content across a range of applications.</li>\n  <li><a href=\"https://selfrefine.info/\">Code</a></li>\n</ul>\n<h4 id=\"reflexion-language-agents-with-verbal-reinforcement-learning\"><a href=\"https://arxiv.org/abs/2303.11366\">Reflexion: Language Agents with Verbal Reinforcement Learning</a></h4>\n<ul>\n  <li>This paper by Shinn et al. from Northeastern University, MIT, and Princeton University introduces Reflexion, a reinforcement learning framework for large language model (LLM)-based agents, enabling them to improve task performance using self-reflective verbal feedback instead of traditional weight updates. Reflexion processes external feedback, transforming it into actionable self-reflections stored in episodic memory, enhancing agents’ decision-making over successive trials in tasks such as sequential action selection, reasoning, and code generation.</li>\n  <li><strong>Framework Overview</strong>: Reflexion involves three models: an Actor (task action generation), an Evaluator (performance assessment), and a Self-Reflection model (produces verbal guidance for improvement). The Actor, built on LLMs, generates actions based on both state observations and past experiences. The Evaluator assigns task-specific rewards, and the Self-Reflection model formulates verbal feedback based on task failures, guiding future attempts.</li>\n  <li><strong>Memory Structure</strong>: Reflexion employs short-term memory (trajectory history) and long-term memory (aggregated self-reflections), which the Actor consults during action generation. This structure allows the agent to remember specific past mistakes while retaining broader learnings across episodes, which aids in complex decision-making tasks.</li>\n  <li>The following figure from the paper shows that Reflexion works on decision-making, programming, and reasoning tasks.</li>\n</ul>\n<p><img src=\"../../../images/papers/Reflexion.jpg\" alt=\"\"></p>\n<ul>\n  <li><strong>Experimentation and Results</strong>:\n    <ul>\n      <li><strong>Decision-Making in AlfWorld</strong>: Reflexion significantly improved performance on multi-step tasks in AlfWorld by 22%, using heuristics to detect repetitive failures and adapt action choices based on memory. Reflexion enables effective backtracking and context recall, with a notable reduction in inefficient planning errors compared to baseline.</li>\n      <li><strong>Reasoning with HotPotQA</strong>: Reflexion enhanced reasoning on HotPotQA, achieving a 20% improvement by refining answers through Chain-of-Thought and episodic memory. Reflexion agents retained task-specific strategies across trials, outpacing baseline approaches in correctly navigating long contextual questions.</li>\n      <li><strong>Programming in HumanEval and LeetcodeHardGym</strong>: Reflexion set new state-of-the-art scores, achieving 91% on HumanEval, aided by self-generated test suites and continuous error-checking through self-reflection. This iterative testing allowed Reflexion agents to refine code output by addressing both syntactical and logical errors.</li>\n    </ul>\n  </li>\n  <li><strong>Implementation Details</strong>:\n    <ul>\n      <li>Reflexion agents use Chain-of-Thought and ReAct generation techniques, with self-reflective prompts implemented through few-shot examples tailored for each task type. For programming, Reflexion employs syntactically validated test suites, filtered to retain only valid abstract syntax tree representations, ensuring comprehensive error handling in code generation.</li>\n      <li>Self-reflection feedback is stored in memory limited to the last three experiences to maintain efficiency within LLM context limits. Reflexion’s feedback loop iterates until the Evaluator confirms task success, effectively combining reinforcement with natural language memory for performance gains.</li>\n    </ul>\n  </li>\n  <li><strong>Ablation Studies and Analysis</strong>: Tests on compromised versions of Reflexion, such as without test generation or self-reflection, showed marked performance drops, underscoring the importance of verbal self-reflection in driving task success. This highlights Reflexion’s effectiveness in environments requiring high interpretability and actionable feedback.</li>\n  <li>Reflexion exemplifies a low-compute yet adaptive approach to reinforcement learning for LLM agents, showing potential for expansive applications in autonomous decision-making where interpretable and incremental learning are essential.</li>\n</ul>\n<ul>\n      <li><strong>Decision-Making in AlfWorld</strong>: Reflexion significantly improved performance on multi-step tasks in AlfWorld by 22%, using heuristics to detect repetitive failures and adapt action choices based on memory. Reflexion enables effective backtracking and context recall, with a notable reduction in inefficient planning errors compared to baseline.</li>\n      <li><strong>Reasoning with HotPotQA</strong>: Reflexion enhanced reasoning on HotPotQA, achieving a 20% improvement by refining answers through Chain-of-Thought and episodic memory. Reflexion agents retained task-specific strategies across trials, outpacing baseline approaches in correctly navigating long contextual questions.</li>\n      <li><strong>Programming in HumanEval and LeetcodeHardGym</strong>: Reflexion set new state-of-the-art scores, achieving 91% on HumanEval, aided by self-generated test suites and continuous error-checking through self-reflection. This iterative testing allowed Reflexion agents to refine code output by addressing both syntactical and logical errors.</li>\n    </ul>\n<ul>\n      <li>Reflexion agents use Chain-of-Thought and ReAct generation techniques, with self-reflective prompts implemented through few-shot examples tailored for each task type. For programming, Reflexion employs syntactically validated test suites, filtered to retain only valid abstract syntax tree representations, ensuring comprehensive error handling in code generation.</li>\n      <li>Self-reflection feedback is stored in memory limited to the last three experiences to maintain efficiency within LLM context limits. Reflexion’s feedback loop iterates until the Evaluator confirms task success, effectively combining reinforcement with natural language memory for performance gains.</li>\n    </ul>\n<h4 id=\"critic-large-language-models-can-self-correct-with-tool-interactive-critiquing\"><a href=\"https://arxiv.org/abs/2305.11738\">CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</a></h4>\n<ul>\n  <li>The paper by Gou et al. from Tsinghua University, Microsoft Research Asia, and Microsoft Azure AI, published in ICLR 2024, introduces a novel framework called CRITIC, designed to enhance the reliability of LLM outputs by allowing the models to interact with external tools to critique and amend their own responses.</li>\n  <li>CRITIC’s approach diverges from traditional model fine-tuning, focusing instead on a verify-and-correct process where LLMs generate initial outputs and subsequently engage with tools such as search engines, code interpreters, and calculators to verify aspects of these outputs, like truthfulness or accuracy. Based on feedback from these tools, the model then refines its response, iteratively improving until a specified condition (e.g., sufficient accuracy) is achieved. This method sidesteps the need for extensive additional training or data annotation and is structured to work with black-box LLMs through in-context learning and few-shot demonstrations.</li>\n  <li>In terms of <strong>implementation</strong>, CRITIC operates in two main phases: verification and correction. An initial output is generated using a few-shot prompt-based approach. This output is then scrutinized with tools tailored to the task at hand. For instance, for fact-checking in question answering, CRITIC uses a Google-based search API to retrieve relevant web snippets, while for mathematical problem-solving, a Python interpreter verifies code execution and provides debugging information if errors occur. The verification feedback, structured as critiques, is appended to the prompt, enabling the LLM to correct its initial output. This verify-then-correct cycle is repeated iteratively, with the maximum number of interactions set per task or until stability in the output is observed.</li>\n  <li>The following figure from the paper shows that the CRITIC framework consists of two steps: (1) verifying the output by interacting with external tools to generate critiques and (2) correcting the output based on the received critiques. We can iterate over such verify-then-correct process to enable continuous improvements.</li>\n</ul>\n<p><img src=\"../../../images/papers/CRITIC.jpg\" alt=\"\"></p>\n<ul>\n  <li>CRITIC’s <strong>performance evaluation</strong> demonstrates its effectiveness across three types of tasks:\n    <ol>\n      <li><strong>Free-form Question Answering</strong> - Here, CRITIC leverages web search to validate answers, achieving notable improvements in F1 scores over baseline methods, such as chain-of-thought (CoT) prompting and retrieval-augmented techniques.</li>\n      <li><strong>Mathematical Program Synthesis</strong> - By utilizing an interpreter for mathematical validation, CRITIC substantially enhances solve rates for datasets like GSM8k, surpassing program-of-thought (PoT) strategies.</li>\n      <li><strong>Toxicity Reduction</strong> - CRITIC employs the PERSPECTIVE API to monitor and reduce toxic content, achieving higher fluency and diversity in outputs while significantly reducing toxicity probabilities.</li>\n    </ol>\n  </li>\n  <li>Experimental results indicate that CRITIC consistently improves model performance, especially in cases requiring high factual accuracy or computational precision. The paper concludes by emphasizing that the inclusion of external feedback mechanisms allows LLMs to perform self-corrections that would be challenging through self-refinement alone.</li>\n  <li><a href=\"https://github.com/microsoft/ProphetNet/tree/master/CRITIC\">Code</a></li>\n</ul>\n<ol>\n      <li><strong>Free-form Question Answering</strong> - Here, CRITIC leverages web search to validate answers, achieving notable improvements in F1 scores over baseline methods, such as chain-of-thought (CoT) prompting and retrieval-augmented techniques.</li>\n      <li><strong>Mathematical Program Synthesis</strong> - By utilizing an interpreter for mathematical validation, CRITIC substantially enhances solve rates for datasets like GSM8k, surpassing program-of-thought (PoT) strategies.</li>\n      <li><strong>Toxicity Reduction</strong> - CRITIC employs the PERSPECTIVE API to monitor and reduce toxic content, achieving higher fluency and diversity in outputs while significantly reducing toxicity probabilities.</li>\n    </ol>",
    "contentMarkdown": "#### [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/abs/2303.17651)\n\n*   Like humans, large language models (LLMs) do not always generate the best output on their first try.\n*   This paper by Madaan et al. from CMU, Allen AI, UW, NVIDIA, UC San Diego, and Google Research introduces a novel approach for enhancing outputs from large language models (LLMs) like GPT-3.5 and GPT-4 through self-generated iterative feedback and refinement, without the need for additional training data or supervised learning – similar to how humans refine their written text.\n*   Put simply, the main idea is to generate an initial output using an LLM; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. This process repeats until a predefined condition is met. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider.\n*   The figure below from the paper shows that given an input (step 0), Self-Refine starts by generating an output and passing it back to the same model M to get feedback (step 1). The feedback is passed back to M, which refines the previously generated output (step 2). Steps (step 1) and (step 2) iterate until a stopping condition is met. SELF-REFINE is instantiated with a language model such as GPT-3.5 and does not involve human assistance.\n\n![](../../../images/papers/Self-Refine.jpg)\n\n*   The approach is evaluated across seven diverse tasks, including dialogue response and code optimization, demonstrating significant improvements over conventional one-step generation methods. This method leverages few-shot prompting for guiding the LLM to generate feedback and incorporate it for output refinement.\n*   The results show that Self-Refine significantly enhances output quality in terms of human preference and task-specific metrics, indicating its potential to improve LLM-generated content across a range of applications.\n*   [Code](https://selfrefine.info/)\n\n#### [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/abs/2303.11366)\n\n*   This paper by Shinn et al. from Northeastern University, MIT, and Princeton University introduces Reflexion, a reinforcement learning framework for large language model (LLM)-based agents, enabling them to improve task performance using self-reflective verbal feedback instead of traditional weight updates. Reflexion processes external feedback, transforming it into actionable self-reflections stored in episodic memory, enhancing agents’ decision-making over successive trials in tasks such as sequential action selection, reasoning, and code generation.\n*   **Framework Overview**: Reflexion involves three models: an Actor (task action generation), an Evaluator (performance assessment), and a Self-Reflection model (produces verbal guidance for improvement). The Actor, built on LLMs, generates actions based on both state observations and past experiences. The Evaluator assigns task-specific rewards, and the Self-Reflection model formulates verbal feedback based on task failures, guiding future attempts.\n*   **Memory Structure**: Reflexion employs short-term memory (trajectory history) and long-term memory (aggregated self-reflections), which the Actor consults during action generation. This structure allows the agent to remember specific past mistakes while retaining broader learnings across episodes, which aids in complex decision-making tasks.\n*   The following figure from the paper shows that Reflexion works on decision-making, programming, and reasoning tasks.\n\n![](../../../images/papers/Reflexion.jpg)\n\n*   **Experimentation and Results**:\n    *   **Decision-Making in AlfWorld**: Reflexion significantly improved performance on multi-step tasks in AlfWorld by 22%, using heuristics to detect repetitive failures and adapt action choices based on memory. Reflexion enables effective backtracking and context recall, with a notable reduction in inefficient planning errors compared to baseline.\n    *   **Reasoning with HotPotQA**: Reflexion enhanced reasoning on HotPotQA, achieving a 20% improvement by refining answers through Chain-of-Thought and episodic memory. Reflexion agents retained task-specific strategies across trials, outpacing baseline approaches in correctly navigating long contextual questions.\n    *   **Programming in HumanEval and LeetcodeHardGym**: Reflexion set new state-of-the-art scores, achieving 91% on HumanEval, aided by self-generated test suites and continuous error-checking through self-reflection. This iterative testing allowed Reflexion agents to refine code output by addressing both syntactical and logical errors.\n*   **Implementation Details**:\n    *   Reflexion agents use Chain-of-Thought and ReAct generation techniques, with self-reflective prompts implemented through few-shot examples tailored for each task type. For programming, Reflexion employs syntactically validated test suites, filtered to retain only valid abstract syntax tree representations, ensuring comprehensive error handling in code generation.\n    *   Self-reflection feedback is stored in memory limited to the last three experiences to maintain efficiency within LLM context limits. Reflexion’s feedback loop iterates until the Evaluator confirms task success, effectively combining reinforcement with natural language memory for performance gains.\n*   **Ablation Studies and Analysis**: Tests on compromised versions of Reflexion, such as without test generation or self-reflection, showed marked performance drops, underscoring the importance of verbal self-reflection in driving task success. This highlights Reflexion’s effectiveness in environments requiring high interpretability and actionable feedback.\n*   Reflexion exemplifies a low-compute yet adaptive approach to reinforcement learning for LLM agents, showing potential for expansive applications in autonomous decision-making where interpretable and incremental learning are essential.\n\n*   **Decision-Making in AlfWorld**: Reflexion significantly improved performance on multi-step tasks in AlfWorld by 22%, using heuristics to detect repetitive failures and adapt action choices based on memory. Reflexion enables effective backtracking and context recall, with a notable reduction in inefficient planning errors compared to baseline.\n*   **Reasoning with HotPotQA**: Reflexion enhanced reasoning on HotPotQA, achieving a 20% improvement by refining answers through Chain-of-Thought and episodic memory. Reflexion agents retained task-specific strategies across trials, outpacing baseline approaches in correctly navigating long contextual questions.\n*   **Programming in HumanEval and LeetcodeHardGym**: Reflexion set new state-of-the-art scores, achieving 91% on HumanEval, aided by self-generated test suites and continuous error-checking through self-reflection. This iterative testing allowed Reflexion agents to refine code output by addressing both syntactical and logical errors.\n\n*   Reflexion agents use Chain-of-Thought and ReAct generation techniques, with self-reflective prompts implemented through few-shot examples tailored for each task type. For programming, Reflexion employs syntactically validated test suites, filtered to retain only valid abstract syntax tree representations, ensuring comprehensive error handling in code generation.\n*   Self-reflection feedback is stored in memory limited to the last three experiences to maintain efficiency within LLM context limits. Reflexion’s feedback loop iterates until the Evaluator confirms task success, effectively combining reinforcement with natural language memory for performance gains.\n\n#### [CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing](https://arxiv.org/abs/2305.11738)\n\n*   The paper by Gou et al. from Tsinghua University, Microsoft Research Asia, and Microsoft Azure AI, published in ICLR 2024, introduces a novel framework called CRITIC, designed to enhance the reliability of LLM outputs by allowing the models to interact with external tools to critique and amend their own responses.\n*   CRITIC’s approach diverges from traditional model fine-tuning, focusing instead on a verify-and-correct process where LLMs generate initial outputs and subsequently engage with tools such as search engines, code interpreters, and calculators to verify aspects of these outputs, like truthfulness or accuracy. Based on feedback from these tools, the model then refines its response, iteratively improving until a specified condition (e.g., sufficient accuracy) is achieved. This method sidesteps the need for extensive additional training or data annotation and is structured to work with black-box LLMs through in-context learning and few-shot demonstrations.\n*   In terms of **implementation**, CRITIC operates in two main phases: verification and correction. An initial output is generated using a few-shot prompt-based approach. This output is then scrutinized with tools tailored to the task at hand. For instance, for fact-checking in question answering, CRITIC uses a Google-based search API to retrieve relevant web snippets, while for mathematical problem-solving, a Python interpreter verifies code execution and provides debugging information if errors occur. The verification feedback, structured as critiques, is appended to the prompt, enabling the LLM to correct its initial output. This verify-then-correct cycle is repeated iteratively, with the maximum number of interactions set per task or until stability in the output is observed.\n*   The following figure from the paper shows that the CRITIC framework consists of two steps: (1) verifying the output by interacting with external tools to generate critiques and (2) correcting the output based on the received critiques. We can iterate over such verify-then-correct process to enable continuous improvements.\n\n![](../../../images/papers/CRITIC.jpg)\n\n*   CRITIC’s **performance evaluation** demonstrates its effectiveness across three types of tasks:\n    1.  **Free-form Question Answering** - Here, CRITIC leverages web search to validate answers, achieving notable improvements in F1 scores over baseline methods, such as chain-of-thought (CoT) prompting and retrieval-augmented techniques.\n    2.  **Mathematical Program Synthesis** - By utilizing an interpreter for mathematical validation, CRITIC substantially enhances solve rates for datasets like GSM8k, surpassing program-of-thought (PoT) strategies.\n    3.  **Toxicity Reduction** - CRITIC employs the PERSPECTIVE API to monitor and reduce toxic content, achieving higher fluency and diversity in outputs while significantly reducing toxicity probabilities.\n*   Experimental results indicate that CRITIC consistently improves model performance, especially in cases requiring high factual accuracy or computational precision. The paper concludes by emphasizing that the inclusion of external feedback mechanisms allows LLMs to perform self-corrections that would be challenging through self-refinement alone.\n*   [Code](https://github.com/microsoft/ProphetNet/tree/master/CRITIC)\n\n1.  **Free-form Question Answering** - Here, CRITIC leverages web search to validate answers, achieving notable improvements in F1 scores over baseline methods, such as chain-of-thought (CoT) prompting and retrieval-augmented techniques.\n2.  **Mathematical Program Synthesis** - By utilizing an interpreter for mathematical validation, CRITIC substantially enhances solve rates for datasets like GSM8k, surpassing program-of-thought (PoT) strategies.\n3.  **Toxicity Reduction** - CRITIC employs the PERSPECTIVE API to monitor and reduce toxic content, achieving higher fluency and diversity in outputs while significantly reducing toxicity probabilities.",
    "order": 68,
    "orderInChapter": 1,
    "difficulty": 4,
    "estimatedMinutes": 8,
    "tags": [
      "algorithmsarchitecture",
      "gpt",
      "llm",
      "reinforcement learning",
      "supervised learning",
      "optimization",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 1540,
      "contentLength": 12910
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#reflection",
    "scrapedAt": "2025-12-28T11:47:30.117Z"
  },
  {
    "id": "ai-agents-tool-calling-69",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Related Papers",
    "title": "Tool Calling",
    "subtitle": "Related Papers",
    "contentHtml": "<h4 id=\"gorilla-large-language-model-connected-with-massive-apis\"><a href=\"https://arxiv.org/abs/2305.15334\">Gorilla: Large Language Model Connected with Massive APIs</a></h4>\n<ul>\n  <li>Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today’s state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call.</li>\n  <li>This paper by Patil et al. from UC Berkeley and Microsoft Research in 2023 introduces Gorilla, a finetuned LLaMA-based model that generates APIs to complete tasks by interacting with external tools and surpasses the performance of GPT-4 while writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly.</li>\n  <li>To evaluate the model’s ability, they introduce APIBench, a comprehensive dataset consisting of Weights, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs.</li>\n  <li>The following figure from the paper shows (top) the training procedure for Gorilla using the most exhaustive API dataset for ML to the best of their knowledge; (bottom) during inference Gorilla supports two modes - with retrieval, and zero-shot. In this example, it is able to suggest the right API call for generating the image from the user’s natural language query.</li>\n</ul>\n<p><img src=\"../../../images/papers/Gorilla.jpg\" alt=\"\"></p>\n<ul>\n  <li><a href=\"https://gorilla.cs.berkeley.edu/\">Project page</a></li>\n</ul>\n<h4 id=\"mm-react-prompting-chatgpt-for-multimodal-reasoning-and-action\"><a href=\"https://arxiv.org/abs/2303.11381\">MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action</a></h4>\n<ul>\n  <li>This paper by Yang et al. from proposes MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action.</li>\n  <li>They define and explore a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models. To achieve such advanced visual intelligence, MM-REACT introduces a textual prompt design that can represent text descriptions, textualized spatial coordinates, and aligned file names for dense visual signals such as images and videos.</li>\n  <li>MM-REACT’s prompt design allows language models to accept, associate, and process multimodal information, thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate MM-REACT’s effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require advanced visual understanding.</li>\n  <li>Furthermore, they discuss and compare MM-REACT’s system paradigm with an alternative approach that extends language models for multimodal scenarios through joint finetuning.</li>\n  <li>The following figure from the paper shows that MM-REACT allocates specialized vision experts with ChatGPT to solve challenging visual understanding tasks through multimodal reasoning and action. For example, the system could associate information from multiple uploaded receipts and calculate the total travel cost (“Multi-Image Reasoning”).</li>\n</ul>\n<p><img src=\"../../../images/papers/MM-REACT1.jpg\" alt=\"\"></p>\n<ul>\n  <li>The following figure from the paper shows the flowchart of MM-REACT for enhanced visual understanding with ChatGPT. The user input can be in the form of text, images, or videos, with the latter two represented as file path strings. ChatGPT is instructed to say specific watchwords in action request if a vision expert is required to interpret the visual inputs. Regular expression matching is applied to parse the expert’s name and the file path, which are then used to call the vision expert (action execution). The expert’s output (observation) is serialized as text and combined with the history to further activate ChatGPT. If no extra experts are needed, MM-REACT would return the final response to the user. The right figure shows a single-round vision expert execution, which is the component that constructs the full execution flow.</li>\n</ul>\n<p><img src=\"../../../images/papers/MM-REACT2.jpg\" alt=\"\"></p>\n<h4 id=\"efficient-tool-use-with-chain-of-abstraction-reasoning\"><a href=\"https://arxiv.org/abs/2401.17464?\">Efficient Tool Use with Chain-of-Abstraction Reasoning</a></h4>\n<ul>\n  <li>\n    <p>This paper by Gao et al. from EPFL and FAIR introduces a novel method called Chain-of-Abstraction (CoA) reasoning, which optimizes LLMs for multi-step reasoning by using tools to access external knowledge. CoA decouples general reasoning from domain-specific information, which is later retrieved using specialized tools, enhancing LLM accuracy and efficiency in domains that require complex reasoning chains, such as mathematics and Wikipedia-based question answering (Wiki QA). The CoA method trains LLMs to generate reasoning chains with placeholders for domain knowledge, allowing parallel tool usage and reducing the lag introduced by interleaving LLM outputs with tool responses.</p>\n  </li>\n  <li>\n    <p>Implementation of CoA reasoning involves a two-stage fine-tuning process: first, LLMs are trained to produce abstract reasoning chains that include placeholders for required operations or knowledge, such as calculations or article references. Then, these chains are reified by filling placeholders with actual knowledge retrieved from tools like an equation solver or a Wikipedia search engine. This approach enables parallel processing, as tools fill in specific information after generating the complete reasoning chain, speeding up inference significantly compared to sequential tool-augmented approaches.</p>\n  </li>\n  <li>For the training pipeline, the authors fine-tune LLMs by re-writing gold-standard answers into abstract reasoning chains labeled with placeholders (e.g., “[20 + 35 = y1]”) for mathematical derivations or search queries for Wiki QA. Verification with domain-specific tools ensures that placeholders align with expected outcomes. For instance, in math, an equation solver calculates final results for placeholders. For Wiki QA, a combination of Wikipedia search (BM25 retriever) and NER extracts relevant articles and entities, which are then matched against gold references.</li>\n  <li>The following figure from the paper shows an overview of chain-of-abstraction reasoning with tools. Given a domain question (green scroll), a LLM is fine-tuned to first generate an abstract multi-step reasoning chain (blue bubble), and then call external tools to reify the chain with domain-specific knowledge (orange label). The final answer (yellow bubble) is obtained based on the reified chain of reasoning.</li>\n</ul>\n<p>This paper by Gao et al. from EPFL and FAIR introduces a novel method called Chain-of-Abstraction (CoA) reasoning, which optimizes LLMs for multi-step reasoning by using tools to access external knowledge. CoA decouples general reasoning from domain-specific information, which is later retrieved using specialized tools, enhancing LLM accuracy and efficiency in domains that require complex reasoning chains, such as mathematics and Wikipedia-based question answering (Wiki QA). The CoA method trains LLMs to generate reasoning chains with placeholders for domain knowledge, allowing parallel tool usage and reducing the lag introduced by interleaving LLM outputs with tool responses.</p>\n<p>Implementation of CoA reasoning involves a two-stage fine-tuning process: first, LLMs are trained to produce abstract reasoning chains that include placeholders for required operations or knowledge, such as calculations or article references. Then, these chains are reified by filling placeholders with actual knowledge retrieved from tools like an equation solver or a Wikipedia search engine. This approach enables parallel processing, as tools fill in specific information after generating the complete reasoning chain, speeding up inference significantly compared to sequential tool-augmented approaches.</p>\n<p><img src=\"../../../images/papers/CoA.jpg\" alt=\"\"></p>\n<ul>\n  <li>Evaluation on GSM8K and HotpotQA datasets showed CoA’s significant improvements in both accuracy and inference speed, outperforming baselines such as Toolformer and traditional chain-of-thought (CoT) methods by 6-7.5% accuracy in mathematics and Wiki QA. Additionally, CoA demonstrated robust generalization in zero-shot settings on other datasets like SVAMP and Natural Questions, with human evaluations confirming reduced reasoning and arithmetic errors.</li>\n</ul>",
    "contentMarkdown": "#### [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/abs/2305.15334)\n\n*   Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today’s state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call.\n*   This paper by Patil et al. from UC Berkeley and Microsoft Research in 2023 introduces Gorilla, a finetuned LLaMA-based model that generates APIs to complete tasks by interacting with external tools and surpasses the performance of GPT-4 while writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly.\n*   To evaluate the model’s ability, they introduce APIBench, a comprehensive dataset consisting of Weights, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs.\n*   The following figure from the paper shows (top) the training procedure for Gorilla using the most exhaustive API dataset for ML to the best of their knowledge; (bottom) during inference Gorilla supports two modes - with retrieval, and zero-shot. In this example, it is able to suggest the right API call for generating the image from the user’s natural language query.\n\n![](../../../images/papers/Gorilla.jpg)\n\n*   [Project page](https://gorilla.cs.berkeley.edu/)\n\n#### [MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action](https://arxiv.org/abs/2303.11381)\n\n*   This paper by Yang et al. from proposes MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action.\n*   They define and explore a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models. To achieve such advanced visual intelligence, MM-REACT introduces a textual prompt design that can represent text descriptions, textualized spatial coordinates, and aligned file names for dense visual signals such as images and videos.\n*   MM-REACT’s prompt design allows language models to accept, associate, and process multimodal information, thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate MM-REACT’s effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require advanced visual understanding.\n*   Furthermore, they discuss and compare MM-REACT’s system paradigm with an alternative approach that extends language models for multimodal scenarios through joint finetuning.\n*   The following figure from the paper shows that MM-REACT allocates specialized vision experts with ChatGPT to solve challenging visual understanding tasks through multimodal reasoning and action. For example, the system could associate information from multiple uploaded receipts and calculate the total travel cost (“Multi-Image Reasoning”).\n\n![](../../../images/papers/MM-REACT1.jpg)\n\n*   The following figure from the paper shows the flowchart of MM-REACT for enhanced visual understanding with ChatGPT. The user input can be in the form of text, images, or videos, with the latter two represented as file path strings. ChatGPT is instructed to say specific watchwords in action request if a vision expert is required to interpret the visual inputs. Regular expression matching is applied to parse the expert’s name and the file path, which are then used to call the vision expert (action execution). The expert’s output (observation) is serialized as text and combined with the history to further activate ChatGPT. If no extra experts are needed, MM-REACT would return the final response to the user. The right figure shows a single-round vision expert execution, which is the component that constructs the full execution flow.\n\n![](../../../images/papers/MM-REACT2.jpg)\n\n#### [Efficient Tool Use with Chain-of-Abstraction Reasoning](https://arxiv.org/abs/2401.17464?)\n\n*   This paper by Gao et al. from EPFL and FAIR introduces a novel method called Chain-of-Abstraction (CoA) reasoning, which optimizes LLMs for multi-step reasoning by using tools to access external knowledge. CoA decouples general reasoning from domain-specific information, which is later retrieved using specialized tools, enhancing LLM accuracy and efficiency in domains that require complex reasoning chains, such as mathematics and Wikipedia-based question answering (Wiki QA). The CoA method trains LLMs to generate reasoning chains with placeholders for domain knowledge, allowing parallel tool usage and reducing the lag introduced by interleaving LLM outputs with tool responses.\n    \n*   Implementation of CoA reasoning involves a two-stage fine-tuning process: first, LLMs are trained to produce abstract reasoning chains that include placeholders for required operations or knowledge, such as calculations or article references. Then, these chains are reified by filling placeholders with actual knowledge retrieved from tools like an equation solver or a Wikipedia search engine. This approach enables parallel processing, as tools fill in specific information after generating the complete reasoning chain, speeding up inference significantly compared to sequential tool-augmented approaches.\n    \n*   For the training pipeline, the authors fine-tune LLMs by re-writing gold-standard answers into abstract reasoning chains labeled with placeholders (e.g., “\\[20 + 35 = y1\\]”) for mathematical derivations or search queries for Wiki QA. Verification with domain-specific tools ensures that placeholders align with expected outcomes. For instance, in math, an equation solver calculates final results for placeholders. For Wiki QA, a combination of Wikipedia search (BM25 retriever) and NER extracts relevant articles and entities, which are then matched against gold references.\n*   The following figure from the paper shows an overview of chain-of-abstraction reasoning with tools. Given a domain question (green scroll), a LLM is fine-tuned to first generate an abstract multi-step reasoning chain (blue bubble), and then call external tools to reify the chain with domain-specific knowledge (orange label). The final answer (yellow bubble) is obtained based on the reified chain of reasoning.\n\nThis paper by Gao et al. from EPFL and FAIR introduces a novel method called Chain-of-Abstraction (CoA) reasoning, which optimizes LLMs for multi-step reasoning by using tools to access external knowledge. CoA decouples general reasoning from domain-specific information, which is later retrieved using specialized tools, enhancing LLM accuracy and efficiency in domains that require complex reasoning chains, such as mathematics and Wikipedia-based question answering (Wiki QA). The CoA method trains LLMs to generate reasoning chains with placeholders for domain knowledge, allowing parallel tool usage and reducing the lag introduced by interleaving LLM outputs with tool responses.\n\nImplementation of CoA reasoning involves a two-stage fine-tuning process: first, LLMs are trained to produce abstract reasoning chains that include placeholders for required operations or knowledge, such as calculations or article references. Then, these chains are reified by filling placeholders with actual knowledge retrieved from tools like an equation solver or a Wikipedia search engine. This approach enables parallel processing, as tools fill in specific information after generating the complete reasoning chain, speeding up inference significantly compared to sequential tool-augmented approaches.\n\n![](../../../images/papers/CoA.jpg)\n\n*   Evaluation on GSM8K and HotpotQA datasets showed CoA’s significant improvements in both accuracy and inference speed, outperforming baselines such as Toolformer and traditional chain-of-thought (CoT) methods by 6-7.5% accuracy in mathematics and Wiki QA. Additionally, CoA demonstrated robust generalization in zero-shot settings on other datasets like SVAMP and Natural Questions, with human evaluations confirming reduced reasoning and arithmetic errors.",
    "order": 69,
    "orderInChapter": 2,
    "difficulty": 4,
    "estimatedMinutes": 7,
    "tags": [
      "algorithmsarchitecture",
      "gpt",
      "llm",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 1209,
      "contentLength": 9283
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#tool-calling",
    "scrapedAt": "2025-12-28T11:47:30.118Z"
  },
  {
    "id": "ai-agents-planning-70",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Related Papers",
    "title": "Planning",
    "subtitle": "Related Papers",
    "contentHtml": "<h4 id=\"chain-of-thought-prompting-elicits-reasoning-in-large-language-models\"><a href=\"https://arxiv.org/abs/2201.11903\">Chain of Thought Prompting Elicits Reasoning in Large Language Models</a></h4>\n<ul>\n  <li>Although scaling up language model size has reliably improved performance on a range of NLP tasks, even the largest models currently struggle with certain reasoning tasks such as arithmetic reasoning, math word problems, symbolic manipulation, and commonsense reasoning.</li>\n  <li>This paper by Wei et al. from Google in 2022 explores the ability of language models to generate a coherent chain of thought – a series of short sentences that mimic the reasoning process a person might have when responding to a question. The idea is strikingly simple: instead of being terse while prompting show the model a few examples of a multi-step reasoning process (the like of which a human would use). Couple this with LLMs (the larger the better) and magic happens! Check out the below image from the paper.</li>\n</ul>\n<p><img src=\"../../../images/papers/cot.jpeg\" alt=\"\"></p>\n<ul>\n  <li>They have explored chain of thought prompting as a simple and broadly applicable method for enhancing reasoning in language models. The superb results you can elucidate via this method are an emergent property of model scale (surprise surprise) - bigger models benefit more from this, and the conclusion holds across models (LaMDA, GPT, PaLM).</li>\n  <li>Interestingly enough, the more complex the task of interest is (in the sense of requiring multi-step reasoning approach), the bigger the boost from the chain of thought prompting!</li>\n  <li>In order to make sure that the performance boost comes from this multi-step approach and not simply because of e.g. more compute, the authors have done a couple of ablations: (i) outputting a terse equation instead of a multi-step reasoning description, (ii) outputting the answer and only then the chain of thought, etc. None of these experiments yielded good results.</li>\n  <li>The method also proved to be fairly robust (always outperforms standard prompting) to the choice of exact few shot exemplars. Despite different annotators, different styles, etc. the method is always better than standard prompting.</li>\n  <li>Through experiments on arithmetic, symbolic, and commonsense reasoning, they find that chain of thought processing is an emergent property of model scale that can be induced via prompting and can enable sufficiently large language models to better perform reasoning tasks that otherwise have flat scaling curves.</li>\n</ul>\n<h4 id=\"hugginggpt-solving-ai-tasks-with-chatgpt-and-its-friends-in-huggingface\"><a href=\"https://arxiv.org/abs/2303.17580\">HuggingGPT: Solving AI Tasks with ChatGPT and Its Friends in HuggingFace</a></h4>\n<ul>\n  <li>Solving complicated AI tasks with different domains and modalities is a key step toward advanced artificial intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks.</li>\n  <li>This paper by Shen et al. from Zhejiang University and Microsoft Research Asia in 2023 advocates that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this, considering the exceptional ability large language models (LLMs) have exhibited in language understanding, generation, interaction, and reasoning, etc. Based on this philosophy, they present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Weights) to solve AI tasks.</li>\n  <li>Specifically, they use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Weights, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Weights, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards advanced artificial intelligence.</li>\n  <li>Summary:\n    <ol>\n      <li>HuggingGPT is recently introduced as a suitable middleware to bridge the connections between Large Language Models (LLMs) and AI models. The workflow goes as follows.</li>\n      <li>Users can send a request (multimodal for sure) which will be processed by an LLM controller. The LLM analyzes the request, understands the intention of the user, and generates possible solvable sub-tasks.</li>\n      <li>ChatGPT selects and invokes the corresponding models hosted on Weights to solve each subtask.</li>\n      <li>Once tasks are executed, the invoked model returns the results to the ChatGPT controller.</li>\n      <li>Finally, ChatGPT integrates the prediction of all models and generates the response.</li>\n      <li>It is amazing how HuggingGPT can show its reasoning and point to its in-context task-model assignment as intermediary steps before generating the output.</li>\n    </ol>\n  </li>\n  <li>The following figure from the paper shows that language serves as an interface for LLMs (e.g., ChatGPT) to connect numerous AI models (e.g., those in Weights) for solving complicated AI tasks. In this concept, an LLM acts as a controller, managing and organizing the cooperation of expert models. The LLM first plans a list of tasks based on the user request and then assigns expert models to each task. After the experts execute the tasks, the LLM collects the results and responds to the user.</li>\n</ul>\n<ol>\n      <li>HuggingGPT is recently introduced as a suitable middleware to bridge the connections between Large Language Models (LLMs) and AI models. The workflow goes as follows.</li>\n      <li>Users can send a request (multimodal for sure) which will be processed by an LLM controller. The LLM analyzes the request, understands the intention of the user, and generates possible solvable sub-tasks.</li>\n      <li>ChatGPT selects and invokes the corresponding models hosted on Weights to solve each subtask.</li>\n      <li>Once tasks are executed, the invoked model returns the results to the ChatGPT controller.</li>\n      <li>Finally, ChatGPT integrates the prediction of all models and generates the response.</li>\n      <li>It is amazing how HuggingGPT can show its reasoning and point to its in-context task-model assignment as intermediary steps before generating the output.</li>\n    </ol>\n<p><img src=\"../../../images/papers/HuggingGPT.jpg\" alt=\"\"></p>\n<ul>\n  <li><a href=\"https://github.com/microsoft/JARVIS\">Code</a></li>\n</ul>\n<h4 id=\"understanding-the-planning-of-llm-agents-a-survey\"><a href=\"https://arxiv.org/abs/2402.02716\">Understanding the Planning of LLM Agents: a Survey</a></h4>\n<ul>\n  <li>This paper by Xu Huang et al. from the USTC and Huawei Noah’s Ark Lab presents a comprehensive survey of the planning capabilities of Large Language Model (LLM)-based agents, systematically categorizing recent approaches and identifying challenges in leveraging LLMs as planning modules. The authors define a taxonomy that classifies existing LLM-agent planning methods into five main categories: Task Decomposition, Multi-Plan Selection, External Planner-Aided Planning, Reflection and Refinement, and Memory-Augmented Planning. This taxonomy serves as a framework for understanding how various methods address the complexity of planning tasks in autonomous agents and highlights key developments in each category.</li>\n  <li>The paper explores Task Decomposition, where complex tasks are divided into manageable sub-tasks, categorized further into Decomposition-First and Interleaved Decomposition. Techniques like Chain-of-Thought (CoT) and ReAct are examined for their methods in guiding LLMs to sequentially reason through tasks, emphasizing the benefits of sub-task correlation while noting limitations in handling lengthy planning sequences due to memory constraints. Multi-Plan Selection is detailed with methods like Tree-of-Thought (ToT) and Graph-of-Thought (GoT), where LLMs generate multiple candidate plans and then employ search algorithms (e.g., Monte Carlo Tree Search) to choose optimal paths, addressing the stochastic nature of LLM planning but also noting challenges with computational overhead.</li>\n  <li>External Planner-Aided Planning is reviewed, dividing approaches into symbolic and neural planners, where LLMs act primarily as intermediaries, structuring tasks for external systems like PDDL or reinforcement learning-based neural planners. The symbolic planners enhance task formalization, while neural planners like DRRN model LLM-aided planning as Markov decision processes, showing efficiency in domain-specific scenarios. Reflection and Refinement strategies, such as Self-Refine and Reflexion, use iterative planning and feedback mechanisms to allow LLM agents to self-correct based on previous errors, resembling reinforcement learning updates but emphasizing textual feedback over parameter adjustments.</li>\n  <li>This paper by Xu Huang et al. from the USTC and Huawei Noah’s Ark Lab presents a comprehensive survey of the planning capabilities of Large Language Model (LLM)-based agents, systematically categorizing recent approaches and identifying challenges in leveraging LLMs as planning modules. The authors define a taxonomy that classifies existing LLM-agent planning methods into five main categories: Task Decomposition, Multi-Plan Selection, External Planner-Aided Planning, Reflection and Refinement, and Memory-Augmented Planning. This taxonomy serves as a framework for understanding how various methods address the complexity of planning tasks in autonomous agents and highlights key developments in each category.</li>\n  <li>Memory-Augmented Planning is discussed through RAG-based memory, which retrieves task-relevant information to support planning, and embodied memory, where agents fine-tune on experiential data, embedding learned knowledge into model parameters. Examples like MemGPT and TDT illustrate how different memory types enhance planning capabilities, balancing between update costs and memorization capacity.</li>\n  <li>The following figure from the paper shows a taxonomy on LLM-Agent planning.</li>\n</ul>\n<p><img src=\"../../../images/papers/LLM-Agent_Planning.jpg\" alt=\"\"></p>\n<ul>\n  <li>The paper evaluates the effectiveness of these approaches on four benchmarks, demonstrating that strategies involving task decomposition, multi-path selection, and reflection significantly improve performance, albeit at higher computational costs. Challenges identified include LLM hallucinations, plan feasibility under complex constraints, efficiency, and limitations in handling multi-modal feedback. Future directions suggested include incorporating symbolic models for constraint handling, optimizing for planning efficiency, and developing realistic evaluation environments to more closely simulate real-world agent interactions. This survey provides a foundational overview of LLM planning, guiding future work toward robust and adaptable planning agents.</li>\n</ul>",
    "contentMarkdown": "#### [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n\n*   Although scaling up language model size has reliably improved performance on a range of NLP tasks, even the largest models currently struggle with certain reasoning tasks such as arithmetic reasoning, math word problems, symbolic manipulation, and commonsense reasoning.\n*   This paper by Wei et al. from Google in 2022 explores the ability of language models to generate a coherent chain of thought – a series of short sentences that mimic the reasoning process a person might have when responding to a question. The idea is strikingly simple: instead of being terse while prompting show the model a few examples of a multi-step reasoning process (the like of which a human would use). Couple this with LLMs (the larger the better) and magic happens! Check out the below image from the paper.\n\n![](../../../images/papers/cot.jpeg)\n\n*   They have explored chain of thought prompting as a simple and broadly applicable method for enhancing reasoning in language models. The superb results you can elucidate via this method are an emergent property of model scale (surprise surprise) - bigger models benefit more from this, and the conclusion holds across models (LaMDA, GPT, PaLM).\n*   Interestingly enough, the more complex the task of interest is (in the sense of requiring multi-step reasoning approach), the bigger the boost from the chain of thought prompting!\n*   In order to make sure that the performance boost comes from this multi-step approach and not simply because of e.g. more compute, the authors have done a couple of ablations: (i) outputting a terse equation instead of a multi-step reasoning description, (ii) outputting the answer and only then the chain of thought, etc. None of these experiments yielded good results.\n*   The method also proved to be fairly robust (always outperforms standard prompting) to the choice of exact few shot exemplars. Despite different annotators, different styles, etc. the method is always better than standard prompting.\n*   Through experiments on arithmetic, symbolic, and commonsense reasoning, they find that chain of thought processing is an emergent property of model scale that can be induced via prompting and can enable sufficiently large language models to better perform reasoning tasks that otherwise have flat scaling curves.\n\n#### [HuggingGPT: Solving AI Tasks with ChatGPT and Its Friends in HuggingFace](https://arxiv.org/abs/2303.17580)\n\n*   Solving complicated AI tasks with different domains and modalities is a key step toward advanced artificial intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks.\n*   This paper by Shen et al. from Zhejiang University and Microsoft Research Asia in 2023 advocates that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this, considering the exceptional ability large language models (LLMs) have exhibited in language understanding, generation, interaction, and reasoning, etc. Based on this philosophy, they present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Weights) to solve AI tasks.\n*   Specifically, they use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Weights, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Weights, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards advanced artificial intelligence.\n*   Summary:\n    1.  HuggingGPT is recently introduced as a suitable middleware to bridge the connections between Large Language Models (LLMs) and AI models. The workflow goes as follows.\n    2.  Users can send a request (multimodal for sure) which will be processed by an LLM controller. The LLM analyzes the request, understands the intention of the user, and generates possible solvable sub-tasks.\n    3.  ChatGPT selects and invokes the corresponding models hosted on Weights to solve each subtask.\n    4.  Once tasks are executed, the invoked model returns the results to the ChatGPT controller.\n    5.  Finally, ChatGPT integrates the prediction of all models and generates the response.\n    6.  It is amazing how HuggingGPT can show its reasoning and point to its in-context task-model assignment as intermediary steps before generating the output.\n*   The following figure from the paper shows that language serves as an interface for LLMs (e.g., ChatGPT) to connect numerous AI models (e.g., those in Weights) for solving complicated AI tasks. In this concept, an LLM acts as a controller, managing and organizing the cooperation of expert models. The LLM first plans a list of tasks based on the user request and then assigns expert models to each task. After the experts execute the tasks, the LLM collects the results and responds to the user.\n\n1.  HuggingGPT is recently introduced as a suitable middleware to bridge the connections between Large Language Models (LLMs) and AI models. The workflow goes as follows.\n2.  Users can send a request (multimodal for sure) which will be processed by an LLM controller. The LLM analyzes the request, understands the intention of the user, and generates possible solvable sub-tasks.\n3.  ChatGPT selects and invokes the corresponding models hosted on Weights to solve each subtask.\n4.  Once tasks are executed, the invoked model returns the results to the ChatGPT controller.\n5.  Finally, ChatGPT integrates the prediction of all models and generates the response.\n6.  It is amazing how HuggingGPT can show its reasoning and point to its in-context task-model assignment as intermediary steps before generating the output.\n\n![](../../../images/papers/HuggingGPT.jpg)\n\n*   [Code](https://github.com/microsoft/JARVIS)\n\n#### [Understanding the Planning of LLM Agents: a Survey](https://arxiv.org/abs/2402.02716)\n\n*   This paper by Xu Huang et al. from the USTC and Huawei Noah’s Ark Lab presents a comprehensive survey of the planning capabilities of Large Language Model (LLM)-based agents, systematically categorizing recent approaches and identifying challenges in leveraging LLMs as planning modules. The authors define a taxonomy that classifies existing LLM-agent planning methods into five main categories: Task Decomposition, Multi-Plan Selection, External Planner-Aided Planning, Reflection and Refinement, and Memory-Augmented Planning. This taxonomy serves as a framework for understanding how various methods address the complexity of planning tasks in autonomous agents and highlights key developments in each category.\n*   The paper explores Task Decomposition, where complex tasks are divided into manageable sub-tasks, categorized further into Decomposition-First and Interleaved Decomposition. Techniques like Chain-of-Thought (CoT) and ReAct are examined for their methods in guiding LLMs to sequentially reason through tasks, emphasizing the benefits of sub-task correlation while noting limitations in handling lengthy planning sequences due to memory constraints. Multi-Plan Selection is detailed with methods like Tree-of-Thought (ToT) and Graph-of-Thought (GoT), where LLMs generate multiple candidate plans and then employ search algorithms (e.g., Monte Carlo Tree Search) to choose optimal paths, addressing the stochastic nature of LLM planning but also noting challenges with computational overhead.\n*   External Planner-Aided Planning is reviewed, dividing approaches into symbolic and neural planners, where LLMs act primarily as intermediaries, structuring tasks for external systems like PDDL or reinforcement learning-based neural planners. The symbolic planners enhance task formalization, while neural planners like DRRN model LLM-aided planning as Markov decision processes, showing efficiency in domain-specific scenarios. Reflection and Refinement strategies, such as Self-Refine and Reflexion, use iterative planning and feedback mechanisms to allow LLM agents to self-correct based on previous errors, resembling reinforcement learning updates but emphasizing textual feedback over parameter adjustments.\n*   This paper by Xu Huang et al. from the USTC and Huawei Noah’s Ark Lab presents a comprehensive survey of the planning capabilities of Large Language Model (LLM)-based agents, systematically categorizing recent approaches and identifying challenges in leveraging LLMs as planning modules. The authors define a taxonomy that classifies existing LLM-agent planning methods into five main categories: Task Decomposition, Multi-Plan Selection, External Planner-Aided Planning, Reflection and Refinement, and Memory-Augmented Planning. This taxonomy serves as a framework for understanding how various methods address the complexity of planning tasks in autonomous agents and highlights key developments in each category.\n*   Memory-Augmented Planning is discussed through RAG-based memory, which retrieves task-relevant information to support planning, and embodied memory, where agents fine-tune on experiential data, embedding learned knowledge into model parameters. Examples like MemGPT and TDT illustrate how different memory types enhance planning capabilities, balancing between update costs and memorization capacity.\n*   The following figure from the paper shows a taxonomy on LLM-Agent planning.\n\n![](../../../images/papers/LLM-Agent_Planning.jpg)\n\n*   The paper evaluates the effectiveness of these approaches on four benchmarks, demonstrating that strategies involving task decomposition, multi-path selection, and reflection significantly improve performance, albeit at higher computational costs. Challenges identified include LLM hallucinations, plan feasibility under complex constraints, efficiency, and limitations in handling multi-modal feedback. Future directions suggested include incorporating symbolic models for constraint handling, optimizing for planning efficiency, and developing realistic evaluation environments to more closely simulate real-world agent interactions. This survey provides a foundational overview of LLM planning, guiding future work toward robust and adaptable planning agents.",
    "order": 70,
    "orderInChapter": 3,
    "difficulty": 4,
    "estimatedMinutes": 8,
    "tags": [
      "algorithmsarchitecture",
      "machine learning",
      "embedding",
      "gpt",
      "llm",
      "nlp",
      "reinforcement learning"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 1497,
      "contentLength": 11279
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#planning",
    "scrapedAt": "2025-12-28T11:47:30.118Z"
  },
  {
    "id": "ai-agents-multi-agent-collaboration-71",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Related Papers",
    "title": "Multi-Agent Collaboration",
    "subtitle": "Related Papers",
    "contentHtml": "<h4 id=\"chatdev-communicative-agents-for-software-development\"><a href=\"https://arxiv.org/abs/2307.07924\">ChatDev: Communicative Agents for Software Development</a></h4>\n<ul>\n  <li>The paper by Qian et al. from Tsinghua, The University of Sydney, BUPT, and Modelbest Inc., published in ACL 2024, presents ChatDev, a software development framework using multiple agents powered by large language models (LLMs) to facilitate collaborative tasks within the software development lifecycle, including design, coding, and testing. The framework is designed to streamline multi-agent communication for more coherent, effective problem-solving across these phases. Key innovations include a structured “chat chain” approach and a “communicative dehallucination” mechanism, both aimed at enhancing the quality and executability of the generated code.</li>\n  <li>ChatDev’s chat chain organizes the workflow into sequential phases—design, coding (subdivided into code writing and completion), and testing (split between code review and system testing)—with each phase containing sequential subtasks. Each subtask is addressed by an “instructor” and an “assistant” agent, who engage in multi-turn dialogues to collaboratively develop solutions, making it easier to handle complex requirements through natural language exchanges in design and programming dialogues in development. The framework thus maintains a coherent flow across phases, facilitating effective transitions and linking subtasks while offering transparency for monitoring intermediate solutions and issues.</li>\n  <li>The communicative dehallucination mechanism addresses the problem of LLM-induced hallucinations in code by prompting agents to seek additional details from the instructor before finalizing responses. This enables agents to achieve more precise task outcomes, reducing instances of incomplete or unexecutable code. In this pattern, the assistant initially requests further guidance, which the instructor then clarifies, allowing the assistant to proceed with an optimized response. This iterative approach helps minimize errors and increases the quality and reliability of the generated code.</li>\n  <li>The following figure from the paper shows that upon receiving a preliminary task requirement (e.g., “develop a Gomoku game”), these software agents engage in multi-turn communication and perform instruction-following along a chain-structured workflow, collaborating to execute a series of subtasks autonomously to craft a comprehensive solution.</li>\n</ul>\n<p><img src=\"../../../images/papers/MAC.jpg\" alt=\"\"></p>\n<ul>\n  <li><strong>Implementation Details</strong>:\n    <ul>\n      <li><strong>Role Assignment and Memory Management</strong>: Each agent is assigned a specific role tailored to different stages in the workflow, like CEO, CTO, programmer, reviewer, and tester. These roles ensure that agents contribute specialized insights at each phase. ChatDev employs short-term memory to maintain contextual continuity within a phase and long-term memory for cross-phase coherence, selectively transmitting only key solutions rather than entire dialogues to avoid memory overload.</li>\n      <li><strong>Subtask Termination Criteria</strong>: To streamline communication, a subtask concludes after two consecutive unchanged code updates or after 10 rounds of communication. This rule optimizes resource use and prevents redundant iterations.</li>\n      <li><strong>Prompt Engineering and LLM Integration</strong>: In each subtask, prompt engineering is applied at the onset, followed by automated exchanges. ChatGPT-3.5, with a low temperature of 0.2, supports task-specific response generation, while Python-3.11.4 integration enables real-time feedback on executable code.</li>\n    </ul>\n  </li>\n  <li><strong>Evaluation</strong>:\n    <ul>\n      <li>ChatDev was evaluated against baseline models GPT-Engineer and MetaGPT using metrics including completeness, executability, consistency, and overall quality. Results show ChatDev’s significant improvements in generating more executable and complete code, largely due to its structured chat chain and communicative dehallucination mechanisms. An ablation study highlights the importance of specific roles and the dehallucination mechanism in boosting software quality.</li>\n    </ul>\n  </li>\n  <li><a href=\"https://github.com/OpenBMB/ChatDev\">Code</a></li>\n</ul>\n<ul>\n      <li><strong>Role Assignment and Memory Management</strong>: Each agent is assigned a specific role tailored to different stages in the workflow, like CEO, CTO, programmer, reviewer, and tester. These roles ensure that agents contribute specialized insights at each phase. ChatDev employs short-term memory to maintain contextual continuity within a phase and long-term memory for cross-phase coherence, selectively transmitting only key solutions rather than entire dialogues to avoid memory overload.</li>\n      <li><strong>Subtask Termination Criteria</strong>: To streamline communication, a subtask concludes after two consecutive unchanged code updates or after 10 rounds of communication. This rule optimizes resource use and prevents redundant iterations.</li>\n      <li><strong>Prompt Engineering and LLM Integration</strong>: In each subtask, prompt engineering is applied at the onset, followed by automated exchanges. ChatGPT-3.5, with a low temperature of 0.2, supports task-specific response generation, while Python-3.11.4 integration enables real-time feedback on executable code.</li>\n    </ul>\n<ul>\n      <li>ChatDev was evaluated against baseline models GPT-Engineer and MetaGPT using metrics including completeness, executability, consistency, and overall quality. Results show ChatDev’s significant improvements in generating more executable and complete code, largely due to its structured chat chain and communicative dehallucination mechanisms. An ablation study highlights the importance of specific roles and the dehallucination mechanism in boosting software quality.</li>\n    </ul>\n<h4 id=\"autogen-enabling-next-gen-llm-applications-via-multi-agent-conversation\"><a href=\"https://arxiv.org/abs/2308.08155\">AutoGen: Enabling Next-Gen LLM Applications Via Multi-Agent Conversation</a></h4>\n<ul>\n  <li>This paper by Wu et al. from Microsoft Research, Pennsylvania State University, University of Washington, and Xidian University, introduces AutoGen, an open-source framework designed to facilitate the development of multi-agent large language model (LLM) applications. The framework allows the creation of customizable, conversable agents that can operate in various modes combining LLMs, human inputs, and tools.</li>\n  <li>AutoGen agents can be easily programmed using both natural language and computer code to define flexible conversation patterns for different applications. The framework supports hierarchical chat, joint chat, and other conversation patterns, enabling agents to converse and cooperate to solve tasks. The agents can hold multiple-turn conversations with other agents or solicit human inputs, enhancing their ability to solve complex tasks.</li>\n</ul>\n<p><img src=\"../../../images/papers/AutoGen_1.jpg\" alt=\"\"></p>\n<ul>\n  <li>Key technical details include the design of conversable agents and conversation programming. Conversable agents can send and receive messages, maintain internal context, and be configured with various capabilities such as LLMs, human inputs, and tools. These agents can also be extended to include more custom behaviors. Conversation programming involves defining agent roles and capabilities and programming their interactions using a combination of natural and programming languages. This approach simplifies complex workflows into intuitive multi-agent conversations.</li>\n  <li><strong>Implementation details:</strong>\n    <ol>\n      <li><strong>Conversable Agents</strong>: AutoGen provides a generic design for agents, enabling them to leverage LLMs, human inputs, tools, or a combination. The agents can autonomously hold conversations and solicit human inputs at certain stages. Developers can easily create specialized agents with different roles by configuring built-in capabilities and extending agent backends.</li>\n      <li><strong>Conversation Programming</strong>: AutoGen adopts a conversation programming paradigm to streamline LLM application workflows. This involves defining conversable agents and programming their interactions via conversation-centric computation and control. The framework supports various conversation patterns, including static and dynamic flows, allowing for flexible agent interactions.</li>\n      <li><strong>Unified Interfaces and Auto-Reply Mechanisms</strong>: Agents in AutoGen have unified interfaces for sending, receiving, and generating replies. An auto-reply mechanism enables conversation-driven control, where agents automatically generate and send replies based on received messages unless a termination condition is met. Custom reply functions can also be registered to define specific behavior patterns.</li>\n      <li><strong>Control Flow</strong>: AutoGen allows control over conversations using both natural language and programming languages. Natural language prompts guide LLM-backed agents, while Python code specifies conditions for human input, tool execution, and termination. This flexibility supports diverse multi-agent conversation patterns, including dynamic group chats managed by the <code class=\"language-plaintext highlighter-rouge\">GroupChatManager</code> class.</li>\n    </ol>\n  </li>\n</ul>\n<ol>\n      <li><strong>Conversable Agents</strong>: AutoGen provides a generic design for agents, enabling them to leverage LLMs, human inputs, tools, or a combination. The agents can autonomously hold conversations and solicit human inputs at certain stages. Developers can easily create specialized agents with different roles by configuring built-in capabilities and extending agent backends.</li>\n      <li><strong>Conversation Programming</strong>: AutoGen adopts a conversation programming paradigm to streamline LLM application workflows. This involves defining conversable agents and programming their interactions via conversation-centric computation and control. The framework supports various conversation patterns, including static and dynamic flows, allowing for flexible agent interactions.</li>\n      <li><strong>Unified Interfaces and Auto-Reply Mechanisms</strong>: Agents in AutoGen have unified interfaces for sending, receiving, and generating replies. An auto-reply mechanism enables conversation-driven control, where agents automatically generate and send replies based on received messages unless a termination condition is met. Custom reply functions can also be registered to define specific behavior patterns.</li>\n      <li><strong>Control Flow</strong>: AutoGen allows control over conversations using both natural language and programming languages. Natural language prompts guide LLM-backed agents, while Python code specifies conditions for human input, tool execution, and termination. This flexibility supports diverse multi-agent conversation patterns, including dynamic group chats managed by the <code class=\"language-plaintext highlighter-rouge\">GroupChatManager</code> class.</li>\n    </ol>\n<p><img src=\"../../../images/papers/AutoGen.jpg\" alt=\"\"></p>\n<ul>\n  <li>The framework’s architecture defines agents with specific roles and capabilities, interacting through structured conversations to process tasks efficiently. This approach improves task performance, reduces development effort, and enhances application flexibility. Key technical aspects include using a unified interface for agent interaction, conversation-centric computation for defining agent behaviors, and conversation-driven control flows that manage interactions among agents.</li>\n  <li><strong>Applications demonstrate AutoGen’s capabilities in various domains:</strong>\n    <ul>\n      <li><strong>Math Problem Solving</strong>: AutoGen builds systems for autonomous and human-in-the-loop math problem solving, outperforming other approaches on the MATH dataset.</li>\n      <li><strong>Retrieval-Augmented Code Generation and Question Answering</strong>: The framework enhances retrieval-augmented generation systems, improving performance on question-answering tasks through interactive retrieval mechanisms.</li>\n      <li><strong>Decision Making in Text World Environments</strong>: AutoGen implements effective interactive decision-making applications using benchmarks like ALFWorld.</li>\n      <li><strong>Multi-Agent Coding</strong>: The framework simplifies coding tasks by dividing responsibilities among agents, improving code safety and efficiency.</li>\n      <li><strong>Dynamic Group Chat</strong>: AutoGen supports dynamic group chats, enabling collaborative problem-solving without predefined communication orders.</li>\n      <li><strong>Conversational Chess</strong>: The framework creates engaging chess games with natural language interfaces, ensuring valid moves through a board agent.</li>\n    </ul>\n  </li>\n  <li>The empirical results indicate that AutoGen significantly outperforms existing single-agent and some multi-agent systems in complex task environments by effectively integrating and managing multiple agents’ capabilities. The paper includes a figure illustrating the use of AutoGen to program a multi-agent conversation, showing built-in agents, a two-agent system with a custom reply function, and the resulting automated agent chat.</li>\n  <li>The authors highlight the potential for AutoGen to improve LLM applications by reducing development effort, enhancing performance, and enabling innovative uses of LLMs. Future work will explore optimal multi-agent workflows, agent capabilities, scaling, safety, and human involvement in multi-agent conversations. The open-source library invites contributions from the broader community to further develop and refine AutoGen.</li>\n</ul>\n<ul>\n      <li><strong>Math Problem Solving</strong>: AutoGen builds systems for autonomous and human-in-the-loop math problem solving, outperforming other approaches on the MATH dataset.</li>\n      <li><strong>Retrieval-Augmented Code Generation and Question Answering</strong>: The framework enhances retrieval-augmented generation systems, improving performance on question-answering tasks through interactive retrieval mechanisms.</li>\n      <li><strong>Decision Making in Text World Environments</strong>: AutoGen implements effective interactive decision-making applications using benchmarks like ALFWorld.</li>\n      <li><strong>Multi-Agent Coding</strong>: The framework simplifies coding tasks by dividing responsibilities among agents, improving code safety and efficiency.</li>\n      <li><strong>Dynamic Group Chat</strong>: AutoGen supports dynamic group chats, enabling collaborative problem-solving without predefined communication orders.</li>\n      <li><strong>Conversational Chess</strong>: The framework creates engaging chess games with natural language interfaces, ensuring valid moves through a board agent.</li>\n    </ul>\n<h4 id=\"apigen-automated-pipeline-for-generating-verifiable-and-diverse-function-calling-datasets\"><a href=\"https://arxiv.org/abs/2406.18518\">APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets</a></h4>\n<ul>\n  <li>This paper introduces <strong>APIGen</strong>, a novel automated data generation pipeline designed by Salesforce AI Research to create reliable and diverse datasets specifically for function-calling applications in LLMs. The APIGen framework addresses challenges in training function-calling agents by producing a large-scale dataset with high-quality, verified function calls. The authors leverage a structured, multi-stage verification approach to generate a dataset that enables fine-tuning LLMs, which demonstrates significant improvements in performance on the Berkeley Function-Calling Benchmark (BFCL).</li>\n  <li>APIGen includes a dataset of 60,000 entries across 3,673 APIs from 21 categories, encompassing different query styles (simple, multiple, parallel, and parallel multiple). Notably, APIGen-trained models, even with relatively fewer parameters, achieved strong results in function-calling benchmarks, surpassing larger LLMs such as GPT-4 and Claude-3.</li>\n  <li><strong>APIGen Framework and Data Generation</strong>: The APIGen framework generates query-answer pairs through a structured pipeline, involving:\n    <ol>\n      <li><strong>Sampling</strong>: APIs and QA pairs are sampled from a comprehensive library and formatted in a unified JSON schema.</li>\n      <li><strong>Prompting</strong>: LLMs are prompted using diverse templates to generate function-call responses in JSON format, promoting response variability across different real-world scenarios.</li>\n      <li><strong>Multi-Stage Verification</strong>:\n        <ul>\n          <li><strong>Format Checker</strong>: Ensures generated data follows JSON specifications and filters poorly formatted entries.</li>\n          <li><strong>Execution Checker</strong>: Executes function calls to verify correctness against backend APIs, discarding calls with errors.</li>\n          <li><strong>Semantic Checker</strong>: A second LLM verifies alignment between generated answers and query intent, further refining data quality.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>This pipeline allows APIGen to create a dataset that supports diverse function-calling scenarios, enhancing model generalization and robustness. The figure below from the paper illustrates the post-process filters.</li>\n</ul>\n<ol>\n      <li><strong>Sampling</strong>: APIs and QA pairs are sampled from a comprehensive library and formatted in a unified JSON schema.</li>\n      <li><strong>Prompting</strong>: LLMs are prompted using diverse templates to generate function-call responses in JSON format, promoting response variability across different real-world scenarios.</li>\n      <li><strong>Multi-Stage Verification</strong>:\n        <ul>\n          <li><strong>Format Checker</strong>: Ensures generated data follows JSON specifications and filters poorly formatted entries.</li>\n          <li><strong>Execution Checker</strong>: Executes function calls to verify correctness against backend APIs, discarding calls with errors.</li>\n          <li><strong>Semantic Checker</strong>: A second LLM verifies alignment between generated answers and query intent, further refining data quality.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li><strong>Format Checker</strong>: Ensures generated data follows JSON specifications and filters poorly formatted entries.</li>\n          <li><strong>Execution Checker</strong>: Executes function calls to verify correctness against backend APIs, discarding calls with errors.</li>\n          <li><strong>Semantic Checker</strong>: A second LLM verifies alignment between generated answers and query intent, further refining data quality.</li>\n        </ul>\n<p><img src=\"../../../images/papers/APIGen1.jpg\" alt=\"\"></p>\n<ul>\n  <li>The figure below from the paper illustrates JSON data format examples.</li>\n</ul>\n<p><img src=\"../../../images/papers/APIGen2.png\" alt=\"\"></p>\n<ul>\n  <li><strong>Diversity and Scalability</strong>: APIGen emphasizes diversity by categorizing queries and sampling API descriptions from different sources. It includes four query types:\n    <ul>\n      <li><strong>Simple</strong>: A single API call per query.</li>\n      <li><strong>Multiple</strong>: Requires selecting the appropriate function from various APIs.</li>\n      <li><strong>Parallel</strong>: Executes multiple API calls in one response.</li>\n      <li><strong>Parallel Multiple</strong>: Combines both parallel and multiple query types, calling for intricate function handling.</li>\n    </ul>\n  </li>\n  <li>To ensure scalability, APIGen processes data from multiple API formats, such as REST and Python functions, adapting them into its JSON format. This modular approach accommodates a variety of API sources with minimal adjustments.</li>\n  <li><strong>Dataset and Implementation</strong>: The dataset generation involves filtering APIs for quality, executing requests to ensure validity, and regenerating API descriptions where necessary. The resulting dataset spans diverse categories, ensuring comprehensive coverage across fields like finance, technology, and social sciences.</li>\n  <li>Training experiments involved two model versions, <strong>xLAM-1B (FC)</strong> and <strong>xLAM-7B (FC)</strong>, demonstrating APIGen’s dataset’s efficacy. Models trained with APIGen data achieved remarkable accuracy on the BFCL, particularly in scenarios involving complex, parallel, and multi-call queries. The xLAM-7B model notably ranked 6th on the BFCL leaderboard, surpassing models like GPT-4 and Llama3.</li>\n  <li><strong>Human Evaluation and Future Work</strong>: APIGen’s effectiveness was validated through human evaluation, with over 95% of samples passing quality checks. The authors plan to extend APIGen to support more API types and incorporate multi-turn interaction capabilities for function-calling agents.</li>\n  <li>By providing a structured and scalable approach to high-quality dataset generation, APIGen sets a new standard for training robust function-calling LLMs, addressing gaps in current datasets and enhancing LLMs’ real-world applicability.</li>\n  <li><a href=\"https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k\">Hugging Face</a>; <a href=\"https://apigen-pipeline.github.io/\">Project page</a></li>\n</ul>\n<ul>\n      <li><strong>Simple</strong>: A single API call per query.</li>\n      <li><strong>Multiple</strong>: Requires selecting the appropriate function from various APIs.</li>\n      <li><strong>Parallel</strong>: Executes multiple API calls in one response.</li>\n      <li><strong>Parallel Multiple</strong>: Combines both parallel and multiple query types, calling for intricate function handling.</li>\n    </ul>\n<h4 id=\"autoagents-a-framework-for-automatic-agent-generation\"><a href=\"https://arxiv.org/abs/2309.17288\">AutoAgents: a Framework for Automatic Agent Generation</a></h4>\n<ul>\n  <li>This paper from Chen et al. published in IJCAI 2024 presents AutoAgents, a novel framework designed for dynamic multi-agent generation and coordination, enabling language models to construct adaptive AI teams for a wide range of tasks. Unlike traditional systems that rely on static, predefined agents, AutoAgents generates task-specific agents autonomously, allowing for flexible collaboration across varied domains. The framework introduces a drafting and execution stage to handle complex task environments and facilitate effective role assignment and solution planning.</li>\n  <li>In the drafting stage, three primary agents—Planner, Agent Observer, and Plan Observer—collaborate to define and refine an agent team and execution plan. The Planner generates specialized agents, each described by prompts detailing roles, objectives, constraints, and toolsets. The Agent Observer evaluates agents for task suitability, ensuring they are adequately diverse and relevant. Simultaneously, the Plan Observer assesses the execution plan, refining it to address any gaps and optimize agent collaboration.</li>\n  <li>The execution stage leverages two mechanisms for task completion: self-refinement and collaborative refinement. In self-refinement, individual agents iterate on their tasks, improving their outputs through cycles of reasoning and self-evaluation. Collaborative refinement allows agents to pool expertise, enhancing task execution through interdisciplinary dialogue. A predefined Action Observer oversees coordination, adjusting task allocations and managing memory across agents to maintain efficiency and coherence.</li>\n  <li>To enhance adaptability in complex tasks, AutoAgents incorporates three types of memory—short-term, long-term, and dynamic—to manage historical data and context for each action. Dynamic memory, in particular, facilitates the Action Observer’s access to essential prior actions, optimizing task-related decisions.</li>\n  <li>\n    <ul>\n      <li>The figure below from the paper illustrates a schematic diagram of AutoAgents. The system takes the user input as a starting point and generates a set of specialized agents for novel writing, along with a corresponding execution plan. The agents collaboratively carry out the tasks according to the plan and produce the final novel. Meanwhile, an observer monitors the generation and execution of the Agents and the plan, ensuring the quality and coherence of the process.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>The figure below from the paper illustrates a schematic diagram of AutoAgents. The system takes the user input as a starting point and generates a set of specialized agents for novel writing, along with a corresponding execution plan. The agents collaboratively carry out the tasks according to the plan and produce the final novel. Meanwhile, an observer monitors the generation and execution of the Agents and the plan, ensuring the quality and coherence of the process.</li>\n    </ul>\n<p><img src=\"../../../images/papers/AutoAgents.jpg\" alt=\"\"></p>\n<ul>\n  <li>Experiments across open-ended question-answering and trivia creative writing tasks validate AutoAgents’ superior performance. AutoAgents surpasses traditional models, including GPT-4, in both knowledge acquisition and reasoning quality. The system demonstrates a significant increase in knowledge integration, especially when handling tasks demanding extensive domain-specific information. A case study in software development shows how AutoAgents generates diverse expert roles (e.g., game designer, programmer, tester) for developing a Python-based Tetris game, highlighting the versatility of the agent team composition.</li>\n  <li>Implementation-wise, AutoAgents utilizes GPT-4 API with a controlled temperature setting for reproducibility. Experiment parameters include a maximum of three drafting discussions and five execution refinements, with dynamic prompts designed to guide each agent’s expertise and actions.</li>\n  <li>In summary, AutoAgents offers a significant advancement in adaptive, collaborative AI systems by automating agent generation and task planning, reinforcing the capabilities of LLMs in handling complex, domain-spanning tasks through self-organizing, expert-driven agent teams.</li>\n  <li><a href=\"https://github.com/Link-AGI/AutoAgents\">Code</a></li>\n</ul>\n<h4 id=\"metagpt-meta-programming-for-multi-agent-collaborative-framework\"><a href=\"https://arxiv.org/abs/2308.00352\">MetaGPT: Meta Programming for Multi-Agent Collaborative Framework</a></h4>\n<ul>\n  <li>Recently, remarkable progress has been made in automated task-solving through the use of multi-agents driven by large language models (LLMs). However, existing works primarily focuses on simple tasks lacking exploration and investigation in complicated tasks mainly due to the hallucination problem. This kind of hallucination gets amplified infinitely as multiple intelligent agents interact with each other, resulting in failures when tackling complicated problems.</li>\n  <li>This paper by Hong et al. from DeepWisdom, Xiamen University, CUHK, Shenzhen, Nanjing University, UPenn, Berkeley introduces MetaGPT, an innovative framework that infuses effective human workflows as a meta programming approach into LLM-driven multi-agent collaboration. In particular, MetaGPT first encodes Standardized Operating Procedures (SOPs) into prompts, fostering structured coordination. And then, it further mandates modular outputs, bestowing agents with domain expertise paralleling human professionals to validate outputs and reduce compounded errors.</li>\n  <li>In this way, MetaGPT leverages the assembly line work model to assign diverse roles to various agents, thus establishing a framework that can effectively and cohesively deconstruct complex multi-agent collaborative problems.</li>\n  <li>Their experiments conducted on collaborative software engineering tasks illustrate MetaGPT’s capability in producing comprehensive solutions with higher coherence relative to existing conversational and chat-based multi-agent systems. This underscores the potential of incorporating human domain knowledge into multi-agents, thus opening up novel avenues for grappling with intricate real-world challenges.</li>\n  <li>The following figure from the paper shows a comparative depiction of the software development SOP between MetaGPT and real-world human team. The MetaGPT approach showcases its ability to decompose high-level tasks into detailed actionable components handled by distinct roles (ProductManager, Architect, ProjectManager, Engineer), thereby facilitating role-specific expertise and coordination. This methodology mirrors human software development teams, but with the advantage of improved efficiency, precision, and consistency. The diagram illustrates how MetaGPT is designed to handle task complexity and promote clear role delineations, making it a valuable tool for complex software development scenarios.</li>\n</ul>\n<p><img src=\"../../../images/papers/MetaGPT.jpg\" alt=\"\"></p>\n<h4 id=\"chameleon-plug-and-play-compositional-reasoning-with-large-language-models\"><a href=\"https://arxiv.org/abs/2304.09842\">Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models</a></h4>\n<ul>\n  <li>Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning.</li>\n  <li>This paper by Lu et al. from UCLA and Microsoft Research presents Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks.</li>\n  <li>At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response.</li>\n  <li>They showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%.</li>\n  <li>Their analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner.</li>\n  <li>The following figures from the paper shows two examples from their Chameleon with GPT-4 on TabMWP, a mathematical reasoning benchmark with tabular contexts. Chameleon demonstrates flexibility and efficiency in adapting to different queries that require various reasoning abilities.</li>\n</ul>\n<p><img src=\"../../../images/papers/Chameleon.jpg\" alt=\"\"></p>",
    "contentMarkdown": "#### [ChatDev: Communicative Agents for Software Development](https://arxiv.org/abs/2307.07924)\n\n*   The paper by Qian et al. from Tsinghua, The University of Sydney, BUPT, and Modelbest Inc., published in ACL 2024, presents ChatDev, a software development framework using multiple agents powered by large language models (LLMs) to facilitate collaborative tasks within the software development lifecycle, including design, coding, and testing. The framework is designed to streamline multi-agent communication for more coherent, effective problem-solving across these phases. Key innovations include a structured “chat chain” approach and a “communicative dehallucination” mechanism, both aimed at enhancing the quality and executability of the generated code.\n*   ChatDev’s chat chain organizes the workflow into sequential phases—design, coding (subdivided into code writing and completion), and testing (split between code review and system testing)—with each phase containing sequential subtasks. Each subtask is addressed by an “instructor” and an “assistant” agent, who engage in multi-turn dialogues to collaboratively develop solutions, making it easier to handle complex requirements through natural language exchanges in design and programming dialogues in development. The framework thus maintains a coherent flow across phases, facilitating effective transitions and linking subtasks while offering transparency for monitoring intermediate solutions and issues.\n*   The communicative dehallucination mechanism addresses the problem of LLM-induced hallucinations in code by prompting agents to seek additional details from the instructor before finalizing responses. This enables agents to achieve more precise task outcomes, reducing instances of incomplete or unexecutable code. In this pattern, the assistant initially requests further guidance, which the instructor then clarifies, allowing the assistant to proceed with an optimized response. This iterative approach helps minimize errors and increases the quality and reliability of the generated code.\n*   The following figure from the paper shows that upon receiving a preliminary task requirement (e.g., “develop a Gomoku game”), these software agents engage in multi-turn communication and perform instruction-following along a chain-structured workflow, collaborating to execute a series of subtasks autonomously to craft a comprehensive solution.\n\n![](../../../images/papers/MAC.jpg)\n\n*   **Implementation Details**:\n    *   **Role Assignment and Memory Management**: Each agent is assigned a specific role tailored to different stages in the workflow, like CEO, CTO, programmer, reviewer, and tester. These roles ensure that agents contribute specialized insights at each phase. ChatDev employs short-term memory to maintain contextual continuity within a phase and long-term memory for cross-phase coherence, selectively transmitting only key solutions rather than entire dialogues to avoid memory overload.\n    *   **Subtask Termination Criteria**: To streamline communication, a subtask concludes after two consecutive unchanged code updates or after 10 rounds of communication. This rule optimizes resource use and prevents redundant iterations.\n    *   **Prompt Engineering and LLM Integration**: In each subtask, prompt engineering is applied at the onset, followed by automated exchanges. ChatGPT-3.5, with a low temperature of 0.2, supports task-specific response generation, while Python-3.11.4 integration enables real-time feedback on executable code.\n*   **Evaluation**:\n    *   ChatDev was evaluated against baseline models GPT-Engineer and MetaGPT using metrics including completeness, executability, consistency, and overall quality. Results show ChatDev’s significant improvements in generating more executable and complete code, largely due to its structured chat chain and communicative dehallucination mechanisms. An ablation study highlights the importance of specific roles and the dehallucination mechanism in boosting software quality.\n*   [Code](https://github.com/OpenBMB/ChatDev)\n\n*   **Role Assignment and Memory Management**: Each agent is assigned a specific role tailored to different stages in the workflow, like CEO, CTO, programmer, reviewer, and tester. These roles ensure that agents contribute specialized insights at each phase. ChatDev employs short-term memory to maintain contextual continuity within a phase and long-term memory for cross-phase coherence, selectively transmitting only key solutions rather than entire dialogues to avoid memory overload.\n*   **Subtask Termination Criteria**: To streamline communication, a subtask concludes after two consecutive unchanged code updates or after 10 rounds of communication. This rule optimizes resource use and prevents redundant iterations.\n*   **Prompt Engineering and LLM Integration**: In each subtask, prompt engineering is applied at the onset, followed by automated exchanges. ChatGPT-3.5, with a low temperature of 0.2, supports task-specific response generation, while Python-3.11.4 integration enables real-time feedback on executable code.\n\n*   ChatDev was evaluated against baseline models GPT-Engineer and MetaGPT using metrics including completeness, executability, consistency, and overall quality. Results show ChatDev’s significant improvements in generating more executable and complete code, largely due to its structured chat chain and communicative dehallucination mechanisms. An ablation study highlights the importance of specific roles and the dehallucination mechanism in boosting software quality.\n\n#### [AutoGen: Enabling Next-Gen LLM Applications Via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155)\n\n*   This paper by Wu et al. from Microsoft Research, Pennsylvania State University, University of Washington, and Xidian University, introduces AutoGen, an open-source framework designed to facilitate the development of multi-agent large language model (LLM) applications. The framework allows the creation of customizable, conversable agents that can operate in various modes combining LLMs, human inputs, and tools.\n*   AutoGen agents can be easily programmed using both natural language and computer code to define flexible conversation patterns for different applications. The framework supports hierarchical chat, joint chat, and other conversation patterns, enabling agents to converse and cooperate to solve tasks. The agents can hold multiple-turn conversations with other agents or solicit human inputs, enhancing their ability to solve complex tasks.\n\n![](../../../images/papers/AutoGen_1.jpg)\n\n*   Key technical details include the design of conversable agents and conversation programming. Conversable agents can send and receive messages, maintain internal context, and be configured with various capabilities such as LLMs, human inputs, and tools. These agents can also be extended to include more custom behaviors. Conversation programming involves defining agent roles and capabilities and programming their interactions using a combination of natural and programming languages. This approach simplifies complex workflows into intuitive multi-agent conversations.\n*   **Implementation details:**\n    1.  **Conversable Agents**: AutoGen provides a generic design for agents, enabling them to leverage LLMs, human inputs, tools, or a combination. The agents can autonomously hold conversations and solicit human inputs at certain stages. Developers can easily create specialized agents with different roles by configuring built-in capabilities and extending agent backends.\n    2.  **Conversation Programming**: AutoGen adopts a conversation programming paradigm to streamline LLM application workflows. This involves defining conversable agents and programming their interactions via conversation-centric computation and control. The framework supports various conversation patterns, including static and dynamic flows, allowing for flexible agent interactions.\n    3.  **Unified Interfaces and Auto-Reply Mechanisms**: Agents in AutoGen have unified interfaces for sending, receiving, and generating replies. An auto-reply mechanism enables conversation-driven control, where agents automatically generate and send replies based on received messages unless a termination condition is met. Custom reply functions can also be registered to define specific behavior patterns.\n    4.  **Control Flow**: AutoGen allows control over conversations using both natural language and programming languages. Natural language prompts guide LLM-backed agents, while Python code specifies conditions for human input, tool execution, and termination. This flexibility supports diverse multi-agent conversation patterns, including dynamic group chats managed by the `GroupChatManager` class.\n\n1.  **Conversable Agents**: AutoGen provides a generic design for agents, enabling them to leverage LLMs, human inputs, tools, or a combination. The agents can autonomously hold conversations and solicit human inputs at certain stages. Developers can easily create specialized agents with different roles by configuring built-in capabilities and extending agent backends.\n2.  **Conversation Programming**: AutoGen adopts a conversation programming paradigm to streamline LLM application workflows. This involves defining conversable agents and programming their interactions via conversation-centric computation and control. The framework supports various conversation patterns, including static and dynamic flows, allowing for flexible agent interactions.\n3.  **Unified Interfaces and Auto-Reply Mechanisms**: Agents in AutoGen have unified interfaces for sending, receiving, and generating replies. An auto-reply mechanism enables conversation-driven control, where agents automatically generate and send replies based on received messages unless a termination condition is met. Custom reply functions can also be registered to define specific behavior patterns.\n4.  **Control Flow**: AutoGen allows control over conversations using both natural language and programming languages. Natural language prompts guide LLM-backed agents, while Python code specifies conditions for human input, tool execution, and termination. This flexibility supports diverse multi-agent conversation patterns, including dynamic group chats managed by the `GroupChatManager` class.\n\n![](../../../images/papers/AutoGen.jpg)\n\n*   The framework’s architecture defines agents with specific roles and capabilities, interacting through structured conversations to process tasks efficiently. This approach improves task performance, reduces development effort, and enhances application flexibility. Key technical aspects include using a unified interface for agent interaction, conversation-centric computation for defining agent behaviors, and conversation-driven control flows that manage interactions among agents.\n*   **Applications demonstrate AutoGen’s capabilities in various domains:**\n    *   **Math Problem Solving**: AutoGen builds systems for autonomous and human-in-the-loop math problem solving, outperforming other approaches on the MATH dataset.\n    *   **Retrieval-Augmented Code Generation and Question Answering**: The framework enhances retrieval-augmented generation systems, improving performance on question-answering tasks through interactive retrieval mechanisms.\n    *   **Decision Making in Text World Environments**: AutoGen implements effective interactive decision-making applications using benchmarks like ALFWorld.\n    *   **Multi-Agent Coding**: The framework simplifies coding tasks by dividing responsibilities among agents, improving code safety and efficiency.\n    *   **Dynamic Group Chat**: AutoGen supports dynamic group chats, enabling collaborative problem-solving without predefined communication orders.\n    *   **Conversational Chess**: The framework creates engaging chess games with natural language interfaces, ensuring valid moves through a board agent.\n*   The empirical results indicate that AutoGen significantly outperforms existing single-agent and some multi-agent systems in complex task environments by effectively integrating and managing multiple agents’ capabilities. The paper includes a figure illustrating the use of AutoGen to program a multi-agent conversation, showing built-in agents, a two-agent system with a custom reply function, and the resulting automated agent chat.\n*   The authors highlight the potential for AutoGen to improve LLM applications by reducing development effort, enhancing performance, and enabling innovative uses of LLMs. Future work will explore optimal multi-agent workflows, agent capabilities, scaling, safety, and human involvement in multi-agent conversations. The open-source library invites contributions from the broader community to further develop and refine AutoGen.\n\n*   **Math Problem Solving**: AutoGen builds systems for autonomous and human-in-the-loop math problem solving, outperforming other approaches on the MATH dataset.\n*   **Retrieval-Augmented Code Generation and Question Answering**: The framework enhances retrieval-augmented generation systems, improving performance on question-answering tasks through interactive retrieval mechanisms.\n*   **Decision Making in Text World Environments**: AutoGen implements effective interactive decision-making applications using benchmarks like ALFWorld.\n*   **Multi-Agent Coding**: The framework simplifies coding tasks by dividing responsibilities among agents, improving code safety and efficiency.\n*   **Dynamic Group Chat**: AutoGen supports dynamic group chats, enabling collaborative problem-solving without predefined communication orders.\n*   **Conversational Chess**: The framework creates engaging chess games with natural language interfaces, ensuring valid moves through a board agent.\n\n#### [APIGen: Automated Pipeline for Generating Verifiable and Diverse Function-Calling Datasets](https://arxiv.org/abs/2406.18518)\n\n*   This paper introduces **APIGen**, a novel automated data generation pipeline designed by Salesforce AI Research to create reliable and diverse datasets specifically for function-calling applications in LLMs. The APIGen framework addresses challenges in training function-calling agents by producing a large-scale dataset with high-quality, verified function calls. The authors leverage a structured, multi-stage verification approach to generate a dataset that enables fine-tuning LLMs, which demonstrates significant improvements in performance on the Berkeley Function-Calling Benchmark (BFCL).\n*   APIGen includes a dataset of 60,000 entries across 3,673 APIs from 21 categories, encompassing different query styles (simple, multiple, parallel, and parallel multiple). Notably, APIGen-trained models, even with relatively fewer parameters, achieved strong results in function-calling benchmarks, surpassing larger LLMs such as GPT-4 and Claude-3.\n*   **APIGen Framework and Data Generation**: The APIGen framework generates query-answer pairs through a structured pipeline, involving:\n    1.  **Sampling**: APIs and QA pairs are sampled from a comprehensive library and formatted in a unified JSON schema.\n    2.  **Prompting**: LLMs are prompted using diverse templates to generate function-call responses in JSON format, promoting response variability across different real-world scenarios.\n    3.  **Multi-Stage Verification**:\n        *   **Format Checker**: Ensures generated data follows JSON specifications and filters poorly formatted entries.\n        *   **Execution Checker**: Executes function calls to verify correctness against backend APIs, discarding calls with errors.\n        *   **Semantic Checker**: A second LLM verifies alignment between generated answers and query intent, further refining data quality.\n*   This pipeline allows APIGen to create a dataset that supports diverse function-calling scenarios, enhancing model generalization and robustness. The figure below from the paper illustrates the post-process filters.\n\n1.  **Sampling**: APIs and QA pairs are sampled from a comprehensive library and formatted in a unified JSON schema.\n2.  **Prompting**: LLMs are prompted using diverse templates to generate function-call responses in JSON format, promoting response variability across different real-world scenarios.\n3.  **Multi-Stage Verification**:\n    *   **Format Checker**: Ensures generated data follows JSON specifications and filters poorly formatted entries.\n    *   **Execution Checker**: Executes function calls to verify correctness against backend APIs, discarding calls with errors.\n    *   **Semantic Checker**: A second LLM verifies alignment between generated answers and query intent, further refining data quality.\n\n*   **Format Checker**: Ensures generated data follows JSON specifications and filters poorly formatted entries.\n*   **Execution Checker**: Executes function calls to verify correctness against backend APIs, discarding calls with errors.\n*   **Semantic Checker**: A second LLM verifies alignment between generated answers and query intent, further refining data quality.\n\n![](../../../images/papers/APIGen1.jpg)\n\n*   The figure below from the paper illustrates JSON data format examples.\n\n![](../../../images/papers/APIGen2.png)\n\n*   **Diversity and Scalability**: APIGen emphasizes diversity by categorizing queries and sampling API descriptions from different sources. It includes four query types:\n    *   **Simple**: A single API call per query.\n    *   **Multiple**: Requires selecting the appropriate function from various APIs.\n    *   **Parallel**: Executes multiple API calls in one response.\n    *   **Parallel Multiple**: Combines both parallel and multiple query types, calling for intricate function handling.\n*   To ensure scalability, APIGen processes data from multiple API formats, such as REST and Python functions, adapting them into its JSON format. This modular approach accommodates a variety of API sources with minimal adjustments.\n*   **Dataset and Implementation**: The dataset generation involves filtering APIs for quality, executing requests to ensure validity, and regenerating API descriptions where necessary. The resulting dataset spans diverse categories, ensuring comprehensive coverage across fields like finance, technology, and social sciences.\n*   Training experiments involved two model versions, **xLAM-1B (FC)** and **xLAM-7B (FC)**, demonstrating APIGen’s dataset’s efficacy. Models trained with APIGen data achieved remarkable accuracy on the BFCL, particularly in scenarios involving complex, parallel, and multi-call queries. The xLAM-7B model notably ranked 6th on the BFCL leaderboard, surpassing models like GPT-4 and Llama3.\n*   **Human Evaluation and Future Work**: APIGen’s effectiveness was validated through human evaluation, with over 95% of samples passing quality checks. The authors plan to extend APIGen to support more API types and incorporate multi-turn interaction capabilities for function-calling agents.\n*   By providing a structured and scalable approach to high-quality dataset generation, APIGen sets a new standard for training robust function-calling LLMs, addressing gaps in current datasets and enhancing LLMs’ real-world applicability.\n*   [Hugging Face](https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k); [Project page](https://apigen-pipeline.github.io/)\n\n*   **Simple**: A single API call per query.\n*   **Multiple**: Requires selecting the appropriate function from various APIs.\n*   **Parallel**: Executes multiple API calls in one response.\n*   **Parallel Multiple**: Combines both parallel and multiple query types, calling for intricate function handling.\n\n#### [AutoAgents: a Framework for Automatic Agent Generation](https://arxiv.org/abs/2309.17288)\n\n*   This paper from Chen et al. published in IJCAI 2024 presents AutoAgents, a novel framework designed for dynamic multi-agent generation and coordination, enabling language models to construct adaptive AI teams for a wide range of tasks. Unlike traditional systems that rely on static, predefined agents, AutoAgents generates task-specific agents autonomously, allowing for flexible collaboration across varied domains. The framework introduces a drafting and execution stage to handle complex task environments and facilitate effective role assignment and solution planning.\n*   In the drafting stage, three primary agents—Planner, Agent Observer, and Plan Observer—collaborate to define and refine an agent team and execution plan. The Planner generates specialized agents, each described by prompts detailing roles, objectives, constraints, and toolsets. The Agent Observer evaluates agents for task suitability, ensuring they are adequately diverse and relevant. Simultaneously, the Plan Observer assesses the execution plan, refining it to address any gaps and optimize agent collaboration.\n*   The execution stage leverages two mechanisms for task completion: self-refinement and collaborative refinement. In self-refinement, individual agents iterate on their tasks, improving their outputs through cycles of reasoning and self-evaluation. Collaborative refinement allows agents to pool expertise, enhancing task execution through interdisciplinary dialogue. A predefined Action Observer oversees coordination, adjusting task allocations and managing memory across agents to maintain efficiency and coherence.\n*   To enhance adaptability in complex tasks, AutoAgents incorporates three types of memory—short-term, long-term, and dynamic—to manage historical data and context for each action. Dynamic memory, in particular, facilitates the Action Observer’s access to essential prior actions, optimizing task-related decisions.\n*   *   The figure below from the paper illustrates a schematic diagram of AutoAgents. The system takes the user input as a starting point and generates a set of specialized agents for novel writing, along with a corresponding execution plan. The agents collaboratively carry out the tasks according to the plan and produce the final novel. Meanwhile, an observer monitors the generation and execution of the Agents and the plan, ensuring the quality and coherence of the process.\n\n*   The figure below from the paper illustrates a schematic diagram of AutoAgents. The system takes the user input as a starting point and generates a set of specialized agents for novel writing, along with a corresponding execution plan. The agents collaboratively carry out the tasks according to the plan and produce the final novel. Meanwhile, an observer monitors the generation and execution of the Agents and the plan, ensuring the quality and coherence of the process.\n\n![](../../../images/papers/AutoAgents.jpg)\n\n*   Experiments across open-ended question-answering and trivia creative writing tasks validate AutoAgents’ superior performance. AutoAgents surpasses traditional models, including GPT-4, in both knowledge acquisition and reasoning quality. The system demonstrates a significant increase in knowledge integration, especially when handling tasks demanding extensive domain-specific information. A case study in software development shows how AutoAgents generates diverse expert roles (e.g., game designer, programmer, tester) for developing a Python-based Tetris game, highlighting the versatility of the agent team composition.\n*   Implementation-wise, AutoAgents utilizes GPT-4 API with a controlled temperature setting for reproducibility. Experiment parameters include a maximum of three drafting discussions and five execution refinements, with dynamic prompts designed to guide each agent’s expertise and actions.\n*   In summary, AutoAgents offers a significant advancement in adaptive, collaborative AI systems by automating agent generation and task planning, reinforcing the capabilities of LLMs in handling complex, domain-spanning tasks through self-organizing, expert-driven agent teams.\n*   [Code](https://github.com/Link-AGI/AutoAgents)\n\n#### [MetaGPT: Meta Programming for Multi-Agent Collaborative Framework](https://arxiv.org/abs/2308.00352)\n\n*   Recently, remarkable progress has been made in automated task-solving through the use of multi-agents driven by large language models (LLMs). However, existing works primarily focuses on simple tasks lacking exploration and investigation in complicated tasks mainly due to the hallucination problem. This kind of hallucination gets amplified infinitely as multiple intelligent agents interact with each other, resulting in failures when tackling complicated problems.\n*   This paper by Hong et al. from DeepWisdom, Xiamen University, CUHK, Shenzhen, Nanjing University, UPenn, Berkeley introduces MetaGPT, an innovative framework that infuses effective human workflows as a meta programming approach into LLM-driven multi-agent collaboration. In particular, MetaGPT first encodes Standardized Operating Procedures (SOPs) into prompts, fostering structured coordination. And then, it further mandates modular outputs, bestowing agents with domain expertise paralleling human professionals to validate outputs and reduce compounded errors.\n*   In this way, MetaGPT leverages the assembly line work model to assign diverse roles to various agents, thus establishing a framework that can effectively and cohesively deconstruct complex multi-agent collaborative problems.\n*   Their experiments conducted on collaborative software engineering tasks illustrate MetaGPT’s capability in producing comprehensive solutions with higher coherence relative to existing conversational and chat-based multi-agent systems. This underscores the potential of incorporating human domain knowledge into multi-agents, thus opening up novel avenues for grappling with intricate real-world challenges.\n*   The following figure from the paper shows a comparative depiction of the software development SOP between MetaGPT and real-world human team. The MetaGPT approach showcases its ability to decompose high-level tasks into detailed actionable components handled by distinct roles (ProductManager, Architect, ProjectManager, Engineer), thereby facilitating role-specific expertise and coordination. This methodology mirrors human software development teams, but with the advantage of improved efficiency, precision, and consistency. The diagram illustrates how MetaGPT is designed to handle task complexity and promote clear role delineations, making it a valuable tool for complex software development scenarios.\n\n![](../../../images/papers/MetaGPT.jpg)\n\n#### [Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models](https://arxiv.org/abs/2304.09842)\n\n*   Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning.\n*   This paper by Lu et al. from UCLA and Microsoft Research presents Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks.\n*   At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response.\n*   They showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%.\n*   Their analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner.\n*   The following figures from the paper shows two examples from their Chameleon with GPT-4 on TabMWP, a mathematical reasoning benchmark with tabular contexts. Chameleon demonstrates flexibility and efficiency in adapting to different queries that require various reasoning abilities.\n\n![](../../../images/papers/Chameleon.jpg)",
    "order": 71,
    "orderInChapter": 4,
    "difficulty": 5,
    "estimatedMinutes": 19,
    "tags": [
      "algorithmsarchitecture",
      "gpt",
      "llm",
      "fine-tuning"
    ],
    "metadata": {
      "hasCode": true,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 3613,
      "contentLength": 31394
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#multi-agent-collaboration",
    "scrapedAt": "2025-12-28T11:47:30.118Z"
  },
  {
    "id": "ai-agents-octotools-an-agentic-framework-with-extensible-too-72",
    "domain": "ai_primers",
    "category": "Algorithms/Architecture",
    "article": "Agents",
    "articleSlug": "agents",
    "chapter": "Related Papers",
    "title": "OctoTools: an Agentic Framework with Extensible Tools for Complex Reasoning",
    "subtitle": "Related Papers",
    "contentHtml": "<ul>\n  <li>This paper by Lu et al. from Stanford introduces OctoTools, a training-free, extensible agentic framework designed to enhance large language models (LLMs) with external tools for complex reasoning. Unlike prior approaches, OctoTools does not require additional training data or domain-specific tool constraints, making it adaptable across diverse reasoning tasks.</li>\n  <li><strong>Key contributions:</strong>\n    <ul>\n      <li>Tool cards: Standardized tool wrappers encapsulate various functionalities, enabling seamless integration and execution without framework modifications.</li>\n      <li>Planner-executor framework: Separates high-level task planning from tool execution. The planner formulates a structured problem-solving strategy, while the executor generates and executes tool commands dynamically.</li>\n      <li>Task-specific toolset optimization: A lightweight algorithm selects the most beneficial subset of tools for each task, improving both accuracy and efficiency.</li>\n      <li>Comprehensive benchmarking: OctoTools was evaluated on 16 reasoning benchmarks (MathVista, MMLU-Pro, MedQA, GAIA-Text, etc.), achieving a 9.3% accuracy improvement over GPT-4o and up to 10.6% over existing agent frameworks like AutoGen, GPT-Functions, and LangChain.</li>\n    </ul>\n  </li>\n  <li>The following figures from the paper shows the framework of OctoTools. (1) Tool cards define tool-usage metadata and encapsulate tools, enabling training-free integration of new tools without additional training or framework refinement. (2) The planner governs both high-level and low-level planning to address the global objective and refine actions step by step. (3) The executor instantiates tool calls by generating executable commands and save structured results in the context. The final answer is summarized from the full trajectory in the context. Furthermore, the task-specific toolset optimization algorithm learns to select a beneficial subset of tools for downstream tasks.</li>\n</ul>\n<ul>\n      <li>Tool cards: Standardized tool wrappers encapsulate various functionalities, enabling seamless integration and execution without framework modifications.</li>\n      <li>Planner-executor framework: Separates high-level task planning from tool execution. The planner formulates a structured problem-solving strategy, while the executor generates and executes tool commands dynamically.</li>\n      <li>Task-specific toolset optimization: A lightweight algorithm selects the most beneficial subset of tools for each task, improving both accuracy and efficiency.</li>\n      <li>Comprehensive benchmarking: OctoTools was evaluated on 16 reasoning benchmarks (MathVista, MMLU-Pro, MedQA, GAIA-Text, etc.), achieving a 9.3% accuracy improvement over GPT-4o and up to 10.6% over existing agent frameworks like AutoGen, GPT-Functions, and LangChain.</li>\n    </ul>\n<p><img src=\"../../../images/papers/OctoTools.jpg\" alt=\"\"></p>\n<ul>\n  <li><strong>Implementation details:</strong>\n    <ul>\n      <li>The system operates through three core modules:\n        <ol>\n          <li>Tool cards: Define metadata for each tool, including input-output formats, constraints, and best-use cases (e.g., Image Captioner, Object Detector, Python Calculator).</li>\n          <li>Planner: Determines which tools to invoke and structures problem decomposition into sub-goals.</li>\n          <li>Executor: Converts planner instructions into executable commands, runs tools, and updates context iteratively.</li>\n        </ol>\n      </li>\n      <li>Uses structured reasoning pipelines, where each step refines prior results to enhance multi-step problem-solving.</li>\n      <li>The optimization algorithm iteratively selects tools based on validation accuracy, ensuring efficiency without introducing unnecessary complexity.</li>\n    </ul>\n  </li>\n  <li><strong>Experimental results:</strong>\n    <ul>\n      <li>Achieved 58.5% accuracy across 16 diverse benchmarks, outperforming GPT-4o (49.2%), LangChain (51.2%), and AutoGen (47.9%).</li>\n      <li>Demonstrated superior multi-step reasoning and tool utilization, particularly in math, science, and medical domains.</li>\n      <li>Ablation studies confirmed that task planning, external tool calling, and multi-step problem solving each contribute significantly to performance improvements.</li>\n    </ul>\n  </li>\n  <li><strong>Conclusion:</strong>\n    <ul>\n      <li>OctoTools is a scalable, modular, and effective solution for enhancing LLMs with external tool integration.</li>\n      <li>It bridges the gap between generic LLM reasoning and domain-specific problem solving, making it a powerful alternative to traditional AI agent frameworks.</li>\n      <li>Future work includes real-time query-based tool selection, multi-agent collaboration, and expanding domain-specific functionalities.</li>\n    </ul>\n  </li>\n  <li><a href=\"https://octotools.github.io\">Website</a>; <a href=\"https://github.com/octotools/octotools\">GitHub Repository</a>; <a href=\"https://huggingface.co/spaces/OctoTools/octotools\">Hugging Face Demo</a></li>\n</ul>\n<ul>\n      <li>The system operates through three core modules:\n        <ol>\n          <li>Tool cards: Define metadata for each tool, including input-output formats, constraints, and best-use cases (e.g., Image Captioner, Object Detector, Python Calculator).</li>\n          <li>Planner: Determines which tools to invoke and structures problem decomposition into sub-goals.</li>\n          <li>Executor: Converts planner instructions into executable commands, runs tools, and updates context iteratively.</li>\n        </ol>\n      </li>\n      <li>Uses structured reasoning pipelines, where each step refines prior results to enhance multi-step problem-solving.</li>\n      <li>The optimization algorithm iteratively selects tools based on validation accuracy, ensuring efficiency without introducing unnecessary complexity.</li>\n    </ul>\n<ol>\n          <li>Tool cards: Define metadata for each tool, including input-output formats, constraints, and best-use cases (e.g., Image Captioner, Object Detector, Python Calculator).</li>\n          <li>Planner: Determines which tools to invoke and structures problem decomposition into sub-goals.</li>\n          <li>Executor: Converts planner instructions into executable commands, runs tools, and updates context iteratively.</li>\n        </ol>\n<ul>\n      <li>Achieved 58.5% accuracy across 16 diverse benchmarks, outperforming GPT-4o (49.2%), LangChain (51.2%), and AutoGen (47.9%).</li>\n      <li>Demonstrated superior multi-step reasoning and tool utilization, particularly in math, science, and medical domains.</li>\n      <li>Ablation studies confirmed that task planning, external tool calling, and multi-step problem solving each contribute significantly to performance improvements.</li>\n    </ul>\n<ul>\n      <li>OctoTools is a scalable, modular, and effective solution for enhancing LLMs with external tool integration.</li>\n      <li>It bridges the gap between generic LLM reasoning and domain-specific problem solving, making it a powerful alternative to traditional AI agent frameworks.</li>\n      <li>Future work includes real-time query-based tool selection, multi-agent collaboration, and expanding domain-specific functionalities.</li>\n    </ul>",
    "contentMarkdown": "*   This paper by Lu et al. from Stanford introduces OctoTools, a training-free, extensible agentic framework designed to enhance large language models (LLMs) with external tools for complex reasoning. Unlike prior approaches, OctoTools does not require additional training data or domain-specific tool constraints, making it adaptable across diverse reasoning tasks.\n*   **Key contributions:**\n    *   Tool cards: Standardized tool wrappers encapsulate various functionalities, enabling seamless integration and execution without framework modifications.\n    *   Planner-executor framework: Separates high-level task planning from tool execution. The planner formulates a structured problem-solving strategy, while the executor generates and executes tool commands dynamically.\n    *   Task-specific toolset optimization: A lightweight algorithm selects the most beneficial subset of tools for each task, improving both accuracy and efficiency.\n    *   Comprehensive benchmarking: OctoTools was evaluated on 16 reasoning benchmarks (MathVista, MMLU-Pro, MedQA, GAIA-Text, etc.), achieving a 9.3% accuracy improvement over GPT-4o and up to 10.6% over existing agent frameworks like AutoGen, GPT-Functions, and LangChain.\n*   The following figures from the paper shows the framework of OctoTools. (1) Tool cards define tool-usage metadata and encapsulate tools, enabling training-free integration of new tools without additional training or framework refinement. (2) The planner governs both high-level and low-level planning to address the global objective and refine actions step by step. (3) The executor instantiates tool calls by generating executable commands and save structured results in the context. The final answer is summarized from the full trajectory in the context. Furthermore, the task-specific toolset optimization algorithm learns to select a beneficial subset of tools for downstream tasks.\n\n*   Tool cards: Standardized tool wrappers encapsulate various functionalities, enabling seamless integration and execution without framework modifications.\n*   Planner-executor framework: Separates high-level task planning from tool execution. The planner formulates a structured problem-solving strategy, while the executor generates and executes tool commands dynamically.\n*   Task-specific toolset optimization: A lightweight algorithm selects the most beneficial subset of tools for each task, improving both accuracy and efficiency.\n*   Comprehensive benchmarking: OctoTools was evaluated on 16 reasoning benchmarks (MathVista, MMLU-Pro, MedQA, GAIA-Text, etc.), achieving a 9.3% accuracy improvement over GPT-4o and up to 10.6% over existing agent frameworks like AutoGen, GPT-Functions, and LangChain.\n\n![](../../../images/papers/OctoTools.jpg)\n\n*   **Implementation details:**\n    *   The system operates through three core modules:\n        1.  Tool cards: Define metadata for each tool, including input-output formats, constraints, and best-use cases (e.g., Image Captioner, Object Detector, Python Calculator).\n        2.  Planner: Determines which tools to invoke and structures problem decomposition into sub-goals.\n        3.  Executor: Converts planner instructions into executable commands, runs tools, and updates context iteratively.\n    *   Uses structured reasoning pipelines, where each step refines prior results to enhance multi-step problem-solving.\n    *   The optimization algorithm iteratively selects tools based on validation accuracy, ensuring efficiency without introducing unnecessary complexity.\n*   **Experimental results:**\n    *   Achieved 58.5% accuracy across 16 diverse benchmarks, outperforming GPT-4o (49.2%), LangChain (51.2%), and AutoGen (47.9%).\n    *   Demonstrated superior multi-step reasoning and tool utilization, particularly in math, science, and medical domains.\n    *   Ablation studies confirmed that task planning, external tool calling, and multi-step problem solving each contribute significantly to performance improvements.\n*   **Conclusion:**\n    *   OctoTools is a scalable, modular, and effective solution for enhancing LLMs with external tool integration.\n    *   It bridges the gap between generic LLM reasoning and domain-specific problem solving, making it a powerful alternative to traditional AI agent frameworks.\n    *   Future work includes real-time query-based tool selection, multi-agent collaboration, and expanding domain-specific functionalities.\n*   [Website](https://octotools.github.io); [GitHub Repository](https://github.com/octotools/octotools); [Hugging Face Demo](https://huggingface.co/spaces/OctoTools/octotools)\n\n*   The system operates through three core modules:\n    1.  Tool cards: Define metadata for each tool, including input-output formats, constraints, and best-use cases (e.g., Image Captioner, Object Detector, Python Calculator).\n    2.  Planner: Determines which tools to invoke and structures problem decomposition into sub-goals.\n    3.  Executor: Converts planner instructions into executable commands, runs tools, and updates context iteratively.\n*   Uses structured reasoning pipelines, where each step refines prior results to enhance multi-step problem-solving.\n*   The optimization algorithm iteratively selects tools based on validation accuracy, ensuring efficiency without introducing unnecessary complexity.\n\n1.  Tool cards: Define metadata for each tool, including input-output formats, constraints, and best-use cases (e.g., Image Captioner, Object Detector, Python Calculator).\n2.  Planner: Determines which tools to invoke and structures problem decomposition into sub-goals.\n3.  Executor: Converts planner instructions into executable commands, runs tools, and updates context iteratively.\n\n*   Achieved 58.5% accuracy across 16 diverse benchmarks, outperforming GPT-4o (49.2%), LangChain (51.2%), and AutoGen (47.9%).\n*   Demonstrated superior multi-step reasoning and tool utilization, particularly in math, science, and medical domains.\n*   Ablation studies confirmed that task planning, external tool calling, and multi-step problem solving each contribute significantly to performance improvements.\n\n*   OctoTools is a scalable, modular, and effective solution for enhancing LLMs with external tool integration.\n*   It bridges the gap between generic LLM reasoning and domain-specific problem solving, making it a powerful alternative to traditional AI agent frameworks.\n*   Future work includes real-time query-based tool selection, multi-agent collaboration, and expanding domain-specific functionalities.",
    "order": 72,
    "orderInChapter": 5,
    "difficulty": 4,
    "estimatedMinutes": 5,
    "tags": [
      "algorithmsarchitecture",
      "gpt",
      "llm",
      "optimization"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 803,
      "contentLength": 7279
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/agents/#octotools:-an-agentic-framework-with-extensible-tools-for-complex-reasoning",
    "scrapedAt": "2025-12-28T11:47:30.118Z"
  }
]