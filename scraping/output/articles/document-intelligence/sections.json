[
  {
    "id": "ai-document-intelligence-what-is-ai-system-for-documents-1",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "General Steps to Build Multimodal AI for Documents",
    "title": "What is AI System for Documents?",
    "order": 1,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Documents, specifically lets take an example of PDFs, are a collection of structured text, tables, figures, plots, graphs and diagrams.</li>\n  <li>We want a system which can explain us/user these contents in a PDF document.</li>\n  <li>We want to be able to ask questions about the document, and the system should be able to generate answers based on the content of the document.</li>\n  <li>We can say, we want a AI-system/chat-bot where we want to upload our documents and ask questions about it, and the system/chat-bot generates answers based on the documents (uploaded documents), internal learnt knowledge (trained content) and external knowledge (augmented tools).</li>\n</ul>\n<blockquote>\n  <p>You may ask, isn’t ChatGPT already capable of doing it?</p>\n</blockquote>\n<p>You may ask, isn’t ChatGPT already capable of doing it?</p>\n<ul>\n  <li>Not completely yet (Jan-2024)!. Like other AI-systems for documents, it primarily relies on the text content present in the document.</li>\n  <li>It will generate great answers for text content, good answers for tables and suboptimal answers for images (charts/plots/diagrams) in the PDF document.</li>\n  <li>You might have to crop image manually from document and upload for getting better answers on document images.</li>\n  <li>Also, not all PDFs are digitally-born or native-PDFs. Scanned pages containing text and images also exists in PDF formats.</li>\n  <li>OCR (Optical Character Recognition) task becomes crucial with these scanned PDFs.</li>\n  <li>The OCR capabilities of ChatGPT are not as advanced as those of specialized OCR models.</li>\n  <li>Here is the <a href=\"https://arxiv.org/abs/2310.16809\">link to paper</a> which studied OCR capabilities of ChatGPT (GPT-4V).</li>\n</ul>\n<p><strong>Now, let’s first understand the problems with current systems/models. We can address those challenges and build a better system.</strong></p>",
    "contentMarkdown": "*   Documents, specifically lets take an example of PDFs, are a collection of structured text, tables, figures, plots, graphs and diagrams.\n*   We want a system which can explain us/user these contents in a PDF document.\n*   We want to be able to ask questions about the document, and the system should be able to generate answers based on the content of the document.\n*   We can say, we want a AI-system/chat-bot where we want to upload our documents and ask questions about it, and the system/chat-bot generates answers based on the documents (uploaded documents), internal learnt knowledge (trained content) and external knowledge (augmented tools).\n\n> You may ask, isn’t ChatGPT already capable of doing it?\n\nYou may ask, isn’t ChatGPT already capable of doing it?\n\n*   Not completely yet (Jan-2024)!. Like other AI-systems for documents, it primarily relies on the text content present in the document.\n*   It will generate great answers for text content, good answers for tables and suboptimal answers for images (charts/plots/diagrams) in the PDF document.\n*   You might have to crop image manually from document and upload for getting better answers on document images.\n*   Also, not all PDFs are digitally-born or native-PDFs. Scanned pages containing text and images also exists in PDF formats.\n*   OCR (Optical Character Recognition) task becomes crucial with these scanned PDFs.\n*   The OCR capabilities of ChatGPT are not as advanced as those of specialized OCR models.\n*   Here is the [link to paper](https://arxiv.org/abs/2310.16809) which studied OCR capabilities of ChatGPT (GPT-4V).\n\n**Now, let’s first understand the problems with current systems/models. We can address those challenges and build a better system.**",
    "contentLength": 1901,
    "wordCount": 270,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/document-intelligence/#what-is-ai-system-for-documents?"
  },
  {
    "id": "ai-document-intelligence-problem-1-lack-of-end-to-end-multimodal-models-tha-2",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Problems with Current AI Document Processing Models/systems",
    "title": "Problem 1: Lack of End-to-end Multimodal Models That Can Comprehend All Components of Document",
    "order": 2,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Components of a PDF/document are text, tables, charts, plots, diagrams and layout.</li>\n  <li>There is a lack of models which can perform well on ALL the components of a PDF or document.</li>\n  <li>There are tons of models addressing parts of it. Different models exist for addressing text-table data pairs, text-chart, text-plot, text-diagrams and layout detection.</li>\n  <li>But few to none which can handle all (text-tables-charts-plots-diagrams-layout).</li>\n  <li>You may say, isn’t charts, plots, diagrams are all images? and if a model is able to understand images, shouldn’t it be able to comprehend all types of images?</li>\n  <li>Answer is, ideally it should. But most of the widely used pretrained models are trained on natural images and not necessarily on document-type images.</li>\n  <li>Natural images are the real-world object images. Like images of dogs, cats, trees and etc.</li>\n  <li>Below example figures 1 and 2, shows the difference between natural-images and document-type images.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/document-intelligence/natural_images.png\" alt=\"Natural Images\">\n<img src=\"/primers/ai/assets/document-intelligence/plotandchart.png\" alt=\"Document Images\"></p>\n<blockquote>\n  <p>Plot/charts like pie-chart, bar-graphs and others, have text and visual elements in it with a particular 2D layout/structure. The diversity of these 2D layout/structures is enormous.</p>\n</blockquote>\n<p>Plot/charts like pie-chart, bar-graphs and others, have text and visual elements in it with a particular 2D layout/structure. The diversity of these 2D layout/structures is enormous.</p>\n<ul>\n  <li>Hence it has been a challenging task to build a well-performing model for document-type images.</li>\n  <li>Many works like <a href=\"https://arxiv.org/abs/2212.09662\">MatCha</a>, <a href=\"https://arxiv.org/abs/2212.10505\">DePlot</a>, <a href=\"https://arxiv.org/abs/2111.15664\">DONUT</a>, <a href=\"https://arxiv.org/abs/2204.08387\">LayoutLMv3</a>, <a href=\"https://arxiv.org/abs/2305.14761\">UniChart</a> and <a href=\"https://arxiv.org/abs/2310.05126\">UReader</a> have tried addressing this issue by training models on large corpus of document-type images and are also open-source.</li>\n  <li>These models perform well when presented with two modalities at a time, like text-table/text-diagram/text-chart/text-plot.</li>\n  <li>But, performance degrades when presented with text-table-chart/plot/diagram at once. Overview of these SOTA models is added at the end of the article.</li>\n</ul>",
    "contentMarkdown": "*   Components of a PDF/document are text, tables, charts, plots, diagrams and layout.\n*   There is a lack of models which can perform well on ALL the components of a PDF or document.\n*   There are tons of models addressing parts of it. Different models exist for addressing text-table data pairs, text-chart, text-plot, text-diagrams and layout detection.\n*   But few to none which can handle all (text-tables-charts-plots-diagrams-layout).\n*   You may say, isn’t charts, plots, diagrams are all images? and if a model is able to understand images, shouldn’t it be able to comprehend all types of images?\n*   Answer is, ideally it should. But most of the widely used pretrained models are trained on natural images and not necessarily on document-type images.\n*   Natural images are the real-world object images. Like images of dogs, cats, trees and etc.\n*   Below example figures 1 and 2, shows the difference between natural-images and document-type images.\n\n![Natural Images](/primers/ai/assets/document-intelligence/natural_images.png) ![Document Images](/primers/ai/assets/document-intelligence/plotandchart.png)\n\n> Plot/charts like pie-chart, bar-graphs and others, have text and visual elements in it with a particular 2D layout/structure. The diversity of these 2D layout/structures is enormous.\n\nPlot/charts like pie-chart, bar-graphs and others, have text and visual elements in it with a particular 2D layout/structure. The diversity of these 2D layout/structures is enormous.\n\n*   Hence it has been a challenging task to build a well-performing model for document-type images.\n*   Many works like [MatCha](https://arxiv.org/abs/2212.09662), [DePlot](https://arxiv.org/abs/2212.10505), [DONUT](https://arxiv.org/abs/2111.15664), [LayoutLMv3](https://arxiv.org/abs/2204.08387), [UniChart](https://arxiv.org/abs/2305.14761) and [UReader](https://arxiv.org/abs/2310.05126) have tried addressing this issue by training models on large corpus of document-type images and are also open-source.\n*   These models perform well when presented with two modalities at a time, like text-table/text-diagram/text-chart/text-plot.\n*   But, performance degrades when presented with text-table-chart/plot/diagram at once. Overview of these SOTA models is added at the end of the article.",
    "contentLength": 2525,
    "wordCount": 293,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/document-intelligence/#problem-1:-lack-of-end-to-end-multimodal-models-that-can-comprehend-all-components-of-document"
  },
  {
    "id": "ai-document-intelligence-problem-2-cannot-capture-multipage-multimodal-cont-3",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Problems with Current AI Document Processing Models/systems",
    "title": "Problem 2: Cannot Capture Multipage Multimodal Context",
    "order": 3,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Text-based systems can handle context-text from multiple pages. But a document can have charts/plots, tables and text on one page or multiple pages. To properly encode charts/plots and tables, large number of tokens are required.</li>\n</ul>\n<h4 id=\"why-does-encoding-tables-and-plotscharts-require-large-number-of-tokens\">Why Does Encoding Tables and Plots/charts Require Large Number of Tokens?</h4>\n<ul>\n  <li>Table is a structured-text. To encode table with its 2D layout, special tokens are required to capture the layout.</li>\n  <li>Also, for chart/plots, a high-resolution chart/plot image is required. Why high resolution? can’t a simple 224x224 size suffice?. No, there are numbers, characters and symbols on the chart/plot, which are lost if the image is captured in low-resolution.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/document-intelligence/resizeissue.png\" alt=\"Figure 3\"></p>\n<ul>\n  <li>Most of the models described in previous section, can handle only one document-type image and one table as input along with textual input. But for a comprehensive answers, the model should be able to access multiple pages, to capture full context.</li>\n</ul>\n<h4 id=\"why-is-processing-multiple-pages-is-a-problem\">Why is Processing Multiple Pages is a Problem?</h4>\n<ul>\n  <li>Computations! In transformers, computations are quadratically related to input context length!. So increasing number of input pages is prohibitive in compute sensitive environment.</li>\n  <li>There have been works linearizing this relation, but the computations are still enormous for large context.</li>\n  <li>Hi-VT5 model is able to handle multiple document page images, but it uses OCR tokens along with the input page images. Including both OCR tokens and page-images tokens increases size of token length by a large amount which we want to avoid, and also OCR errors propogates and degrades the overall performance of the system.</li>\n  <li>Hence, there is a need to reduce computations further. In next section shows state-of-the-art multimodal model architectures, which can give us potential insights into addressing this issue.</li>\n</ul>",
    "contentMarkdown": "*   Text-based systems can handle context-text from multiple pages. But a document can have charts/plots, tables and text on one page or multiple pages. To properly encode charts/plots and tables, large number of tokens are required.\n\n#### Why Does Encoding Tables and Plots/charts Require Large Number of Tokens?\n\n*   Table is a structured-text. To encode table with its 2D layout, special tokens are required to capture the layout.\n*   Also, for chart/plots, a high-resolution chart/plot image is required. Why high resolution? can’t a simple 224x224 size suffice?. No, there are numbers, characters and symbols on the chart/plot, which are lost if the image is captured in low-resolution.\n\n![Figure 3](/primers/ai/assets/document-intelligence/resizeissue.png)\n\n*   Most of the models described in previous section, can handle only one document-type image and one table as input along with textual input. But for a comprehensive answers, the model should be able to access multiple pages, to capture full context.\n\n#### Why is Processing Multiple Pages is a Problem?\n\n*   Computations! In transformers, computations are quadratically related to input context length!. So increasing number of input pages is prohibitive in compute sensitive environment.\n*   There have been works linearizing this relation, but the computations are still enormous for large context.\n*   Hi-VT5 model is able to handle multiple document page images, but it uses OCR tokens along with the input page images. Including both OCR tokens and page-images tokens increases size of token length by a large amount which we want to avoid, and also OCR errors propogates and degrades the overall performance of the system.\n*   Hence, there is a need to reduce computations further. In next section shows state-of-the-art multimodal model architectures, which can give us potential insights into addressing this issue.",
    "contentLength": 2140,
    "wordCount": 285,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/document-intelligence/#problem-2:-cannot-capture-multipage-multimodal-context"
  },
  {
    "id": "ai-document-intelligence-step-1-a-suitable-multimodal-model-architecture-4",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Exploring Solutions",
    "title": "Step 1: a Suitable Multimodal Model Architecture",
    "order": 4,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li>Why is this the first step? This will help us address the above two problems (not completely, but to some extent). The type of training data will further help, but first need to fix a model architecture.</li>\n  <li>We want to capture the content and 2D structure in tables, plots, charts and diagrams. Vision-models are designed to capture such diverse 2D content and structure in images.</li>\n</ul>\n<blockquote>\n  <p>So, instead of text-input, vision-models can be used to capture this 2D content and structure by PRIMARILY using document-page-image as our input.</p>\n</blockquote>\n<p>So, instead of text-input, vision-models can be used to capture this 2D content and structure by PRIMARILY using document-page-image as our input.</p>\n<ul>\n  <li>This means we want to convert each document page to an image and provide the entire page as an image to the model. And, no text input to the model except the question from the user.</li>\n  <li>This will help us avoid OCR, special tokens and textual tokens. So instead of giving text tokens, table tokens, and chart/plot/diagram image tokens separately to capture multimodal context, we can now give a document-page-image which contains all this information.</li>\n  <li>This doesn’t necessarily decrease the number of tokens or computations? or can it?</li>\n</ul>\n<blockquote>\n  <p>If we study the transformer literature, we observe that, the vision-encoders are 4x to 10x smaller than LLMs. So, vision-encoders can be used to encode images with less computations when compared to directly giving image to LLM (or decoder. LLMs have decoder-only transformer architecture).</p>\n</blockquote>\n<p>If we study the transformer literature, we observe that, the vision-encoders are 4x to 10x smaller than LLMs. So, vision-encoders can be used to encode images with less computations when compared to directly giving image to LLM (or decoder. LLMs have decoder-only transformer architecture).</p>\n<ul>\n  <li>Ok, so let’s use an encoder. But there are different multimodal model architecture types to choose from. Let’s briefly understand them and choose based on our requirements.</li>\n</ul>\n<blockquote>\n  <p>Lot’s of new ideas can be generated once we understand these below four multimodal model architecture types.</p>\n</blockquote>\n<p>Lot’s of new ideas can be generated once we understand these below four multimodal model architecture types.</p>\n<h4 id=\"four-types-of-sota-multimodal-model-architectures\">Four Types of SOTA Multimodal Model Architectures</h4>\n<ol>\n  <li>Standard encoder-decoder transformer</li>\n</ol>\n<p><img src=\"/primers/ai/assets/document-intelligence/standard_encoder_decoder_textfont.png\" alt=\"Standard encoder decoder transformer\"></p>\n<ol>\n  <li>Vision-encoder + adapter + LLM (decoder)</li>\n</ol>\n<p><img src=\"/primers/ai/assets/document-intelligence/adapter_encoder_decoder.png\" alt=\"Transformer with adapter\"></p>\n<ol>\n  <li>Vision-encoder + projector + LLM (decoder)</li>\n</ol>\n<p><img src=\"/primers/ai/assets/document-intelligence/llava_encoder_decoder.png\" alt=\"Transformer with projector\"></p>\n<ol>\n  <li>Decoder-only (LLMs, GPT models)</li>\n</ol>\n<p><img src=\"/primers/ai/assets/document-intelligence/only_decoder.png\" alt=\"Transformer with \"></p>\n<ul>\n  <li>Architecture 2, 3 and 4 can leverage the SOTA LLM models by replacing its decoder with the best performing LLM. This is possible because, most of the LLMs are of decoder-only transformer architecture. So they can replace any decoder in encoder-decoder architectures except type 1.</li>\n  <li><strong>The architecture type 2 and 3 suits our application.</strong></li>\n  <li>The 2 and 3 tries to utilize latest best performing LLMs (decoders) and intergrates with best-performing vision-encoders.</li>\n  <li>Integration happens through a projector (one or multiple fully connected layers) or adapter (light-weight cross-attention with learnable embeddings). Future articles will be explaining these architectures in details</li>\n  <li>Why LLMs cannot replace the decoder in type 1 architecture?. Critical difference between 1 and rest of the architecture is, the cross attention layer in decoder of the type 1 architecture. LLMs typically are not trained with cross attention layer. The type 1 architectures utilizes cross attention to process language and image tokens. It is pretrained and finetuned with cross attention present in the architecture.</li>\n</ul>",
    "contentMarkdown": "*   Why is this the first step? This will help us address the above two problems (not completely, but to some extent). The type of training data will further help, but first need to fix a model architecture.\n*   We want to capture the content and 2D structure in tables, plots, charts and diagrams. Vision-models are designed to capture such diverse 2D content and structure in images.\n\n> So, instead of text-input, vision-models can be used to capture this 2D content and structure by PRIMARILY using document-page-image as our input.\n\nSo, instead of text-input, vision-models can be used to capture this 2D content and structure by PRIMARILY using document-page-image as our input.\n\n*   This means we want to convert each document page to an image and provide the entire page as an image to the model. And, no text input to the model except the question from the user.\n*   This will help us avoid OCR, special tokens and textual tokens. So instead of giving text tokens, table tokens, and chart/plot/diagram image tokens separately to capture multimodal context, we can now give a document-page-image which contains all this information.\n*   This doesn’t necessarily decrease the number of tokens or computations? or can it?\n\n> If we study the transformer literature, we observe that, the vision-encoders are 4x to 10x smaller than LLMs. So, vision-encoders can be used to encode images with less computations when compared to directly giving image to LLM (or decoder. LLMs have decoder-only transformer architecture).\n\nIf we study the transformer literature, we observe that, the vision-encoders are 4x to 10x smaller than LLMs. So, vision-encoders can be used to encode images with less computations when compared to directly giving image to LLM (or decoder. LLMs have decoder-only transformer architecture).\n\n*   Ok, so let’s use an encoder. But there are different multimodal model architecture types to choose from. Let’s briefly understand them and choose based on our requirements.\n\n> Lot’s of new ideas can be generated once we understand these below four multimodal model architecture types.\n\nLot’s of new ideas can be generated once we understand these below four multimodal model architecture types.\n\n#### Four Types of SOTA Multimodal Model Architectures\n\n1.  Standard encoder-decoder transformer\n\n![Standard encoder decoder transformer](/primers/ai/assets/document-intelligence/standard_encoder_decoder_textfont.png)\n\n1.  Vision-encoder + adapter + LLM (decoder)\n\n![Transformer with adapter](/primers/ai/assets/document-intelligence/adapter_encoder_decoder.png)\n\n1.  Vision-encoder + projector + LLM (decoder)\n\n![Transformer with projector](/primers/ai/assets/document-intelligence/llava_encoder_decoder.png)\n\n1.  Decoder-only (LLMs, GPT models)\n\n![Transformer with ](/primers/ai/assets/document-intelligence/only_decoder.png)\n\n*   Architecture 2, 3 and 4 can leverage the SOTA LLM models by replacing its decoder with the best performing LLM. This is possible because, most of the LLMs are of decoder-only transformer architecture. So they can replace any decoder in encoder-decoder architectures except type 1.\n*   **The architecture type 2 and 3 suits our application.**\n*   The 2 and 3 tries to utilize latest best performing LLMs (decoders) and intergrates with best-performing vision-encoders.\n*   Integration happens through a projector (one or multiple fully connected layers) or adapter (light-weight cross-attention with learnable embeddings). Future articles will be explaining these architectures in details\n*   Why LLMs cannot replace the decoder in type 1 architecture?. Critical difference between 1 and rest of the architecture is, the cross attention layer in decoder of the type 1 architecture. LLMs typically are not trained with cross attention layer. The type 1 architectures utilizes cross attention to process language and image tokens. It is pretrained and finetuned with cross attention present in the architecture.",
    "contentLength": 4409,
    "wordCount": 567,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/document-intelligence/#step-1:-a-suitable-multimodal-model-architecture"
  },
  {
    "id": "ai-document-intelligence-step-2-multimodal-pretraining-data-5",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Exploring Solutions",
    "title": "Step 2: Multimodal Pretraining Data",
    "order": 5,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>The Multimodal documet AI system should generate answer from the uploded documents, but the document doesn’t contain all the knowledge to generate a comprehensive answer. It needs background knowlege.</li>\n  <li>For example, in research papers (one type of document), the authors might refer an abbreviation, whose full definition might be present in some other papers or wikipedia.</li>\n  <li>We want this background knowledge in our model to be of multimodal in nature. Text-based general knowledge model exists. But a multimodal model, trained on large-scale open-source multimodal data doesn’t exists yet or is not open-source yet. So need to first pretrain our multimodal model with general-multimodal-knowledge.</li>\n  <li>Large scale multimodal data suitable for document related tasks is not available.</li>\n  <li><strong>We need paired-data, i.e, text-table-chart/plot/figure/diagrams should be present in one data sample, for the model to learn the correlation between these three document modalities.</strong></li>\n  <li>There are large scale datasets with text-table, text-chart, text-plot, but no large scale dataset which has text-table-chart/plot/figure/diagram in each data sample. Hence, we need to find new ways.</li>\n  <li><strong>Two possible sources: Wikipedia webpages as a PDF, and PDF of Arxiv papers.</strong></li>\n  <li>A Wikipedia webpage can be converted to a PDF. This PDF will have multiple pages. These webpages has multimodal data in them. So a multimodal multipage dataset can be created.</li>\n  <li><strong>Wikipedia can be a best source of multimodal knowledge. It consists of broad topics, has text, images and tables. Which is suitable for documents.</strong></li>\n  <li>The layout is not diverse, but the knowledge and multimodality of it is useful due to its scale.</li>\n  <li><strong>Arxiv paper are great source of paired-data of text-table-chart-plots-diagrams. Also they come in various layouts. Hence, this is a great source for multimodal pretraining data for document tasks.</strong></li>\n</ul>",
    "contentMarkdown": "*   The Multimodal documet AI system should generate answer from the uploded documents, but the document doesn’t contain all the knowledge to generate a comprehensive answer. It needs background knowlege.\n*   For example, in research papers (one type of document), the authors might refer an abbreviation, whose full definition might be present in some other papers or wikipedia.\n*   We want this background knowledge in our model to be of multimodal in nature. Text-based general knowledge model exists. But a multimodal model, trained on large-scale open-source multimodal data doesn’t exists yet or is not open-source yet. So need to first pretrain our multimodal model with general-multimodal-knowledge.\n*   Large scale multimodal data suitable for document related tasks is not available.\n*   **We need paired-data, i.e, text-table-chart/plot/figure/diagrams should be present in one data sample, for the model to learn the correlation between these three document modalities.**\n*   There are large scale datasets with text-table, text-chart, text-plot, but no large scale dataset which has text-table-chart/plot/figure/diagram in each data sample. Hence, we need to find new ways.\n*   **Two possible sources: Wikipedia webpages as a PDF, and PDF of Arxiv papers.**\n*   A Wikipedia webpage can be converted to a PDF. This PDF will have multiple pages. These webpages has multimodal data in them. So a multimodal multipage dataset can be created.\n*   **Wikipedia can be a best source of multimodal knowledge. It consists of broad topics, has text, images and tables. Which is suitable for documents.**\n*   The layout is not diverse, but the knowledge and multimodality of it is useful due to its scale.\n*   **Arxiv paper are great source of paired-data of text-table-chart-plots-diagrams. Also they come in various layouts. Hence, this is a great source for multimodal pretraining data for document tasks.**",
    "contentLength": 2051,
    "wordCount": 291,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/document-intelligence/#step-2:-multimodal-pretraining-data"
  },
  {
    "id": "ai-document-intelligence-step-3-multimodal-finetuning-data-6",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Exploring Solutions",
    "title": "Step 3: Multimodal Finetuning Data",
    "order": 6,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>Document related tasks: OCR, layout-detection, summarization, key-information extraction, Question-and-Answering. Lots of datasets are available for each of these tasks. <a href=\"https://www.docvqa.org/\">DocVQA</a>, <a href=\"https://www.docvqa.org/datasets/infographicvqa\">InfoVQA</a>, <a href=\"https://github.com/vis-nlp/ChartQA\">ChartQA</a>, <a href=\"https://textvqa.org\">TextVQA</a>, <a href=\"https://github.com/project-deepform/deepform\">DeepForm</a>, <a href=\"https://guillaumejaume.github.io/FUNSD/\">FUNSD</a>, <a href=\"https://github.com/clovaai/cord\">CORD</a>, <a href=\"https://paperswithcode.com/dataset/rvl-cdip\">RVL-CDIP</a>.</li>\n  <li>New or advanced way of generating finetuning data: LLaVA paper shows excellent ways to generate finetuning data by leveraging GPT-4. A small dataset with small real-world data can be converted to a larger dataset using GPT-4.</li>\n  <li>In multimodal domain, data generation through automation is key to fast development, because there is a lack of large real world multimodal data right now.</li>\n</ul>",
    "contentMarkdown": "*   Document related tasks: OCR, layout-detection, summarization, key-information extraction, Question-and-Answering. Lots of datasets are available for each of these tasks. [DocVQA](https://www.docvqa.org/), [InfoVQA](https://www.docvqa.org/datasets/infographicvqa), [ChartQA](https://github.com/vis-nlp/ChartQA), [TextVQA](https://textvqa.org), [DeepForm](https://github.com/project-deepform/deepform), [FUNSD](https://guillaumejaume.github.io/FUNSD/), [CORD](https://github.com/clovaai/cord), [RVL-CDIP](https://paperswithcode.com/dataset/rvl-cdip).\n*   New or advanced way of generating finetuning data: LLaVA paper shows excellent ways to generate finetuning data by leveraging GPT-4. A small dataset with small real-world data can be converted to a larger dataset using GPT-4.\n*   In multimodal domain, data generation through automation is key to fast development, because there is a lack of large real world multimodal data right now.",
    "contentLength": 1062,
    "wordCount": 91,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/document-intelligence/#step-3:-multimodal-finetuning-data"
  },
  {
    "id": "ai-document-intelligence-step-4-pretraining-7",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Exploring Solutions",
    "title": "Step 4: Pretraining",
    "order": 7,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>Typically used pretraining tasks are desgined such that the model is able to read text in the image in order to solve the complex tasks during finetuning.</li>\n  <li>If no OCR inputs are used, OCR task is used as pretraining task.</li>\n  <li>If OCR inputs are used, then Multimodal-masked-language-modeling is used.</li>\n</ul>",
    "contentMarkdown": "*   Typically used pretraining tasks are desgined such that the model is able to read text in the image in order to solve the complex tasks during finetuning.\n*   If no OCR inputs are used, OCR task is used as pretraining task.\n*   If OCR inputs are used, then Multimodal-masked-language-modeling is used.",
    "contentLength": 337,
    "wordCount": 52,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/document-intelligence/#step-4:-pretraining"
  },
  {
    "id": "ai-document-intelligence-step-5-finetuning-8",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Exploring Solutions",
    "title": "Step 5: Finetuning",
    "order": 8,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>Document related finetuning tasks: Layout-detection, summarization, key-information extraction, Question-and-Answering.</li>\n  <li>Use datasets listed in the multimodal-finetuning-data section to train on these tasks.</li>\n</ul>",
    "contentMarkdown": "*   Document related finetuning tasks: Layout-detection, summarization, key-information extraction, Question-and-Answering.\n*   Use datasets listed in the multimodal-finetuning-data section to train on these tasks.",
    "contentLength": 239,
    "wordCount": 23,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/document-intelligence/#step-5:-finetuning"
  },
  {
    "id": "ai-document-intelligence-step-6-instruction-tuning-aligning-model-for-chat--9",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Exploring Solutions",
    "title": "Step 6: Instruction Tuning (Aligning Model for Chat Interface)",
    "order": 9,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>\n    <p>Why is this required?</p>\n\n    <ul>\n      <li><strong>Reason 1</strong>:\n        <ul>\n          <li>Users need structured and natural language answers. Finetuned models may not necessarily generate structured natural language answers.</li>\n          <li>Example, model output for layout detection, may simply be coordinate numbers. But user might not understand it clearly. The coordinates must accompany with a text explaining the details in brief.</li>\n          <li>In order to achieve this, we need to train the model further to generate responses which are structured and look natural to the user. This training process is aligning the model for chat.</li>\n        </ul>\n      </li>\n      <li><strong>Reason 2</strong>:\n        <ul>\n          <li>Also, to answer complex reasoning questions, the model may not be able to generate answer in one interaction.</li>\n          <li>The instruction tuning process, trains/prompts the model to break the question into multiple parts and allows the models to iteratively answer the individual parts of the question, so that an comprehensive answer is generated at the end.</li>\n          <li><a href=\"https://arxiv.org/abs/2210.03629\">ReAct</a> framework is used to achieve this.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>This step can also be used to train/prompt the model to use external tools, but it is best practise to separate these two phases.</li>\n  <li>Why? We first want model to generate responses based on its internal (wikipedia, arxiv) and the document knowledge, so that we can understand limitations of the model for further improvement.</li>\n  <li>How to do this?\n    <ul>\n      <li>Generate data with ReAct format such that complex tasks or questions are broken down into small steps and model generate responses for each of the small step. Generate data manually or find automated process leveraging GPT-3.4 or GPT-4.</li>\n    </ul>\n  </li>\n</ul>\n<p>Why is this required?</p>\n<ul>\n      <li><strong>Reason 1</strong>:\n        <ul>\n          <li>Users need structured and natural language answers. Finetuned models may not necessarily generate structured natural language answers.</li>\n          <li>Example, model output for layout detection, may simply be coordinate numbers. But user might not understand it clearly. The coordinates must accompany with a text explaining the details in brief.</li>\n          <li>In order to achieve this, we need to train the model further to generate responses which are structured and look natural to the user. This training process is aligning the model for chat.</li>\n        </ul>\n      </li>\n      <li><strong>Reason 2</strong>:\n        <ul>\n          <li>Also, to answer complex reasoning questions, the model may not be able to generate answer in one interaction.</li>\n          <li>The instruction tuning process, trains/prompts the model to break the question into multiple parts and allows the models to iteratively answer the individual parts of the question, so that an comprehensive answer is generated at the end.</li>\n          <li><a href=\"https://arxiv.org/abs/2210.03629\">ReAct</a> framework is used to achieve this.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Users need structured and natural language answers. Finetuned models may not necessarily generate structured natural language answers.</li>\n          <li>Example, model output for layout detection, may simply be coordinate numbers. But user might not understand it clearly. The coordinates must accompany with a text explaining the details in brief.</li>\n          <li>In order to achieve this, we need to train the model further to generate responses which are structured and look natural to the user. This training process is aligning the model for chat.</li>\n        </ul>\n<ul>\n          <li>Also, to answer complex reasoning questions, the model may not be able to generate answer in one interaction.</li>\n          <li>The instruction tuning process, trains/prompts the model to break the question into multiple parts and allows the models to iteratively answer the individual parts of the question, so that an comprehensive answer is generated at the end.</li>\n          <li><a href=\"https://arxiv.org/abs/2210.03629\">ReAct</a> framework is used to achieve this.</li>\n        </ul>\n<ul>\n      <li>Generate data with ReAct format such that complex tasks or questions are broken down into small steps and model generate responses for each of the small step. Generate data manually or find automated process leveraging GPT-3.4 or GPT-4.</li>\n    </ul>",
    "contentMarkdown": "*   Why is this required?\n    \n    *   **Reason 1**:\n        *   Users need structured and natural language answers. Finetuned models may not necessarily generate structured natural language answers.\n        *   Example, model output for layout detection, may simply be coordinate numbers. But user might not understand it clearly. The coordinates must accompany with a text explaining the details in brief.\n        *   In order to achieve this, we need to train the model further to generate responses which are structured and look natural to the user. This training process is aligning the model for chat.\n    *   **Reason 2**:\n        *   Also, to answer complex reasoning questions, the model may not be able to generate answer in one interaction.\n        *   The instruction tuning process, trains/prompts the model to break the question into multiple parts and allows the models to iteratively answer the individual parts of the question, so that an comprehensive answer is generated at the end.\n        *   [ReAct](https://arxiv.org/abs/2210.03629) framework is used to achieve this.\n*   This step can also be used to train/prompt the model to use external tools, but it is best practise to separate these two phases.\n*   Why? We first want model to generate responses based on its internal (wikipedia, arxiv) and the document knowledge, so that we can understand limitations of the model for further improvement.\n*   How to do this?\n    *   Generate data with ReAct format such that complex tasks or questions are broken down into small steps and model generate responses for each of the small step. Generate data manually or find automated process leveraging GPT-3.4 or GPT-4.\n\nWhy is this required?\n\n*   **Reason 1**:\n    *   Users need structured and natural language answers. Finetuned models may not necessarily generate structured natural language answers.\n    *   Example, model output for layout detection, may simply be coordinate numbers. But user might not understand it clearly. The coordinates must accompany with a text explaining the details in brief.\n    *   In order to achieve this, we need to train the model further to generate responses which are structured and look natural to the user. This training process is aligning the model for chat.\n*   **Reason 2**:\n    *   Also, to answer complex reasoning questions, the model may not be able to generate answer in one interaction.\n    *   The instruction tuning process, trains/prompts the model to break the question into multiple parts and allows the models to iteratively answer the individual parts of the question, so that an comprehensive answer is generated at the end.\n    *   [ReAct](https://arxiv.org/abs/2210.03629) framework is used to achieve this.\n\n*   Users need structured and natural language answers. Finetuned models may not necessarily generate structured natural language answers.\n*   Example, model output for layout detection, may simply be coordinate numbers. But user might not understand it clearly. The coordinates must accompany with a text explaining the details in brief.\n*   In order to achieve this, we need to train the model further to generate responses which are structured and look natural to the user. This training process is aligning the model for chat.\n\n*   Also, to answer complex reasoning questions, the model may not be able to generate answer in one interaction.\n*   The instruction tuning process, trains/prompts the model to break the question into multiple parts and allows the models to iteratively answer the individual parts of the question, so that an comprehensive answer is generated at the end.\n*   [ReAct](https://arxiv.org/abs/2210.03629) framework is used to achieve this.\n\n*   Generate data with ReAct format such that complex tasks or questions are broken down into small steps and model generate responses for each of the small step. Generate data manually or find automated process leveraging GPT-3.4 or GPT-4.",
    "contentLength": 4583,
    "wordCount": 604,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/document-intelligence/#step-6:-instruction-tuning-(aligning-model-for-chat-interface)"
  },
  {
    "id": "ai-document-intelligence-step-7-tuning-model-for-it-to-use-external-tool-us-10",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Exploring Solutions",
    "title": "Step 7: Tuning Model for It to Use External Tool Use (Internet, Image-generation, Calculators, Custom-tasks, and Etc.)",
    "order": 10,
    "orderInChapter": 7,
    "contentHtml": "<ul>\n  <li>Why do we need this?\n    <ul>\n      <li>When the model is not able to generate confident response from its internal knowledge or the document, it should be able to access external knowledge and tools to generate better response. These tools, can be accessing internet, calculator, custom task and etc.</li>\n      <li>This stage also uses REACT framework. But here the training will be explicitly for training the model to use the augmented tools.</li>\n    </ul>\n  </li>\n  <li>How to do this?\n    <ul>\n      <li>Generate training data with ReAct format. The responses in each data sample should include request or keywords for indicating external tool use. Generate data manually or find automated process leveraging GPT-3.4 or GPT-4.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>When the model is not able to generate confident response from its internal knowledge or the document, it should be able to access external knowledge and tools to generate better response. These tools, can be accessing internet, calculator, custom task and etc.</li>\n      <li>This stage also uses REACT framework. But here the training will be explicitly for training the model to use the augmented tools.</li>\n    </ul>\n<ul>\n      <li>Generate training data with ReAct format. The responses in each data sample should include request or keywords for indicating external tool use. Generate data manually or find automated process leveraging GPT-3.4 or GPT-4.</li>\n    </ul>",
    "contentMarkdown": "*   Why do we need this?\n    *   When the model is not able to generate confident response from its internal knowledge or the document, it should be able to access external knowledge and tools to generate better response. These tools, can be accessing internet, calculator, custom task and etc.\n    *   This stage also uses REACT framework. But here the training will be explicitly for training the model to use the augmented tools.\n*   How to do this?\n    *   Generate training data with ReAct format. The responses in each data sample should include request or keywords for indicating external tool use. Generate data manually or find automated process leveraging GPT-3.4 or GPT-4.\n\n*   When the model is not able to generate confident response from its internal knowledge or the document, it should be able to access external knowledge and tools to generate better response. These tools, can be accessing internet, calculator, custom task and etc.\n*   This stage also uses REACT framework. But here the training will be explicitly for training the model to use the augmented tools.\n\n*   Generate training data with ReAct format. The responses in each data sample should include request or keywords for indicating external tool use. Generate data manually or find automated process leveraging GPT-3.4 or GPT-4.",
    "contentLength": 1463,
    "wordCount": 211,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/document-intelligence/#step-7:-tuning-model-for-it-to-use-external-tool-use-(internet,-image-generation,-calculators,-custom-tasks,-and-etc.)"
  },
  {
    "id": "ai-document-intelligence-step-8-building-a-chat-interface-11",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Exploring Solutions",
    "title": "Step 8: Building a Chat Interface",
    "order": 11,
    "orderInChapter": 8,
    "contentHtml": "<ul>\n  <li><a href=\"https://streamlit.io/\">Streamlit</a> and <a href=\"https://gradio.app/\">Gradio</a> allows us to quickly build chat interfaces for our LLMs and multimodal models.</li>\n</ul>\n<blockquote>\n  <p>The multimodal model we develop will always have a input-page-limit. For example 10 or 20 page-images can be given to the model at once. Then what happens when there is a document with more pages than that?</p>\n</blockquote>\n<p>The multimodal model we develop will always have a input-page-limit. For example 10 or 20 page-images can be given to the model at once. Then what happens when there is a document with more pages than that?</p>\n<ul>\n  <li>Vector database comes to rescue! It allows us to store and retrieve large number of document pages. For example, if we had to perform Q&amp;A on 300 page book, then first we encode each page and store it in vector database. Once user asks a question, based on the question relevance, 10 or 20 pages from the vector database is extracted. These 10 or 20 pages are now sent to our multimodal model for document processing and generating answer.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/document-intelligence/multimodal_chat_ing_ret_gen.png\" alt=\"A Multimodal document AI system which handles multiple document pages\"></p>\n<ul>\n  <li>\n    <p>There is a new wave of multimodal RAG system development for deploying multimodal chat-bots using <a href=\"https://python.langchain.com/docs/use_cases/chatbots\">LangChain</a>, <a href=\"https://docs.llamaindex.ai/en/stable/use_cases/multimodal.html\">LLamaIndex</a> and others.</p>\n  </li>\n  <li>\n    <p><strong>This article mainly focused on the multimodal model development which includes training and data details. There are lot of resources online to learn how to build a chat interfaces with vector databases and other frameworks.</strong></p>\n  </li>\n</ul>\n<p>There is a new wave of multimodal RAG system development for deploying multimodal chat-bots using <a href=\"https://python.langchain.com/docs/use_cases/chatbots\">LangChain</a>, <a href=\"https://docs.llamaindex.ai/en/stable/use_cases/multimodal.html\">LLamaIndex</a> and others.</p>\n<p><strong>This article mainly focused on the multimodal model development which includes training and data details. There are lot of resources online to learn how to build a chat interfaces with vector databases and other frameworks.</strong></p>",
    "contentMarkdown": "*   [Streamlit](https://streamlit.io/) and [Gradio](https://gradio.app/) allows us to quickly build chat interfaces for our LLMs and multimodal models.\n\n> The multimodal model we develop will always have a input-page-limit. For example 10 or 20 page-images can be given to the model at once. Then what happens when there is a document with more pages than that?\n\nThe multimodal model we develop will always have a input-page-limit. For example 10 or 20 page-images can be given to the model at once. Then what happens when there is a document with more pages than that?\n\n*   Vector database comes to rescue! It allows us to store and retrieve large number of document pages. For example, if we had to perform Q&A on 300 page book, then first we encode each page and store it in vector database. Once user asks a question, based on the question relevance, 10 or 20 pages from the vector database is extracted. These 10 or 20 pages are now sent to our multimodal model for document processing and generating answer.\n\n![A Multimodal document AI system which handles multiple document pages](/primers/ai/assets/document-intelligence/multimodal_chat_ing_ret_gen.png)\n\n*   There is a new wave of multimodal RAG system development for deploying multimodal chat-bots using [LangChain](https://python.langchain.com/docs/use_cases/chatbots), [LLamaIndex](https://docs.llamaindex.ai/en/stable/use_cases/multimodal.html) and others.\n    \n*   **This article mainly focused on the multimodal model development which includes training and data details. There are lot of resources online to learn how to build a chat interfaces with vector databases and other frameworks.**\n    \n\nThere is a new wave of multimodal RAG system development for deploying multimodal chat-bots using [LangChain](https://python.langchain.com/docs/use_cases/chatbots), [LLamaIndex](https://docs.llamaindex.ai/en/stable/use_cases/multimodal.html) and others.\n\n**This article mainly focused on the multimodal model development which includes training and data details. There are lot of resources online to learn how to build a chat interfaces with vector databases and other frameworks.**",
    "contentLength": 2392,
    "wordCount": 292,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/document-intelligence/#step-8:-building-a-chat-interface"
  },
  {
    "id": "ai-document-intelligence-categorizing-sota-models-12",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Overview of SOTA Document Processing Models",
    "title": "Categorizing SOTA Models",
    "order": 12,
    "orderInChapter": 1,
    "contentHtml": "<ul>\n  <li><strong>Multi-page DocVQA</strong>\n    <ul>\n      <li><strong>OCR-based Multi-page DocVQA</strong>\n        <ul>\n          <li><a href=\"https://arxiv.org/abs/2212.05935\">Hi-VT5</a>, <a href=\"https://arxiv.org/abs/2306.00526\">LATIN-prompt</a></li>\n        </ul>\n      </li>\n      <li><strong>OCR-free Multi-page DocVQA</strong>\n        <ul>\n          <li>This type of models are what we ideally need to build best performing Multimodal Document AI. As of this writing, currently there is only one work, <a href=\"https://arxiv.org/abs/2309.08872\">PDFTriage</a>. But this is ONLY for native PDF documents, hence is not general to work with different type of documents. The model architecture, data and training processes described at the start of the article can be used to build an OCR-free Multi-page DocVQA  models.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Single-page DocVQA</strong>\n    <ul>\n      <li><strong>OCR-based Single-page DocVQA models</strong>\n        <ul>\n          <li><a href=\"https://arxiv.org/abs/2204.08387\">LayoutLMv3</a>, <a href=\"https://arxiv.org/abs/2012.14740\">LayoutLMv2</a>, <a href=\"https://arxiv.org/abs/1912.13318\">LayoutLMv1</a>, <a href=\"https://arxiv.org/abs/2304.10759\">GeoLayoutLM</a>, <a href=\"https://arxiv.org/abs/2106.11539\">DocFormer</a>, <a href=\"https://arxiv.org/abs/2203.06947\">XYLayoutLM</a>, <a href=\"https://arxiv.org/abs/2105.11672\">ViBERTgrid</a>, <a href=\"https://arxiv.org/abs/2206.13155\">Bi-VLDoc</a> and <a href=\"https://arxiv.org/abs/2202.13669\">LiLT</a>.</li>\n        </ul>\n      </li>\n      <li><strong>OCR-free Single-page DocVQA models</strong>\n        <ul>\n          <li><a href=\"https://arxiv.org/abs/2309.11419\">Kosmos-2.5</a>, <a href=\"https://arxiv.org/abs/2303.00289\">StrucText-V2</a>, <a href=\"https://arxiv.org/abs/2111.15664\">DONUT</a>, <a href=\"https://arxiv.org/abs/2306.17107\">LLaVAR</a>, <a href=\"https://arxiv.org/abs/2308.11592\">UniDoc</a>, <a href=\"https://arxiv.org/abs/2310.05126\">UReader</a>, <a href=\"https://arxiv.org/abs/2307.02499\">mPLUG-DocOwl</a>, <a href=\"https://arxiv.org/abs/2311.09808\">PixT3</a>.</li>\n        </ul>\n      </li>\n      <li>Single-page DocVQA models for High resolution images\n        <ul>\n          <li><a href=\"https://arxiv.org/abs/2308.13418\">Nougat</a> and <a href=\"https://arxiv.org/abs/2311.11810\">DocPedia</a>.\\</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>Lets dive into few important models listed above.</li>\n</ul>\n<ul>\n      <li><strong>OCR-based Multi-page DocVQA</strong>\n        <ul>\n          <li><a href=\"https://arxiv.org/abs/2212.05935\">Hi-VT5</a>, <a href=\"https://arxiv.org/abs/2306.00526\">LATIN-prompt</a></li>\n        </ul>\n      </li>\n      <li><strong>OCR-free Multi-page DocVQA</strong>\n        <ul>\n          <li>This type of models are what we ideally need to build best performing Multimodal Document AI. As of this writing, currently there is only one work, <a href=\"https://arxiv.org/abs/2309.08872\">PDFTriage</a>. But this is ONLY for native PDF documents, hence is not general to work with different type of documents. The model architecture, data and training processes described at the start of the article can be used to build an OCR-free Multi-page DocVQA  models.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li><a href=\"https://arxiv.org/abs/2212.05935\">Hi-VT5</a>, <a href=\"https://arxiv.org/abs/2306.00526\">LATIN-prompt</a></li>\n        </ul>\n<ul>\n          <li>This type of models are what we ideally need to build best performing Multimodal Document AI. As of this writing, currently there is only one work, <a href=\"https://arxiv.org/abs/2309.08872\">PDFTriage</a>. But this is ONLY for native PDF documents, hence is not general to work with different type of documents. The model architecture, data and training processes described at the start of the article can be used to build an OCR-free Multi-page DocVQA  models.</li>\n        </ul>\n<ul>\n      <li><strong>OCR-based Single-page DocVQA models</strong>\n        <ul>\n          <li><a href=\"https://arxiv.org/abs/2204.08387\">LayoutLMv3</a>, <a href=\"https://arxiv.org/abs/2012.14740\">LayoutLMv2</a>, <a href=\"https://arxiv.org/abs/1912.13318\">LayoutLMv1</a>, <a href=\"https://arxiv.org/abs/2304.10759\">GeoLayoutLM</a>, <a href=\"https://arxiv.org/abs/2106.11539\">DocFormer</a>, <a href=\"https://arxiv.org/abs/2203.06947\">XYLayoutLM</a>, <a href=\"https://arxiv.org/abs/2105.11672\">ViBERTgrid</a>, <a href=\"https://arxiv.org/abs/2206.13155\">Bi-VLDoc</a> and <a href=\"https://arxiv.org/abs/2202.13669\">LiLT</a>.</li>\n        </ul>\n      </li>\n      <li><strong>OCR-free Single-page DocVQA models</strong>\n        <ul>\n          <li><a href=\"https://arxiv.org/abs/2309.11419\">Kosmos-2.5</a>, <a href=\"https://arxiv.org/abs/2303.00289\">StrucText-V2</a>, <a href=\"https://arxiv.org/abs/2111.15664\">DONUT</a>, <a href=\"https://arxiv.org/abs/2306.17107\">LLaVAR</a>, <a href=\"https://arxiv.org/abs/2308.11592\">UniDoc</a>, <a href=\"https://arxiv.org/abs/2310.05126\">UReader</a>, <a href=\"https://arxiv.org/abs/2307.02499\">mPLUG-DocOwl</a>, <a href=\"https://arxiv.org/abs/2311.09808\">PixT3</a>.</li>\n        </ul>\n      </li>\n      <li>Single-page DocVQA models for High resolution images\n        <ul>\n          <li><a href=\"https://arxiv.org/abs/2308.13418\">Nougat</a> and <a href=\"https://arxiv.org/abs/2311.11810\">DocPedia</a>.\\</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li><a href=\"https://arxiv.org/abs/2204.08387\">LayoutLMv3</a>, <a href=\"https://arxiv.org/abs/2012.14740\">LayoutLMv2</a>, <a href=\"https://arxiv.org/abs/1912.13318\">LayoutLMv1</a>, <a href=\"https://arxiv.org/abs/2304.10759\">GeoLayoutLM</a>, <a href=\"https://arxiv.org/abs/2106.11539\">DocFormer</a>, <a href=\"https://arxiv.org/abs/2203.06947\">XYLayoutLM</a>, <a href=\"https://arxiv.org/abs/2105.11672\">ViBERTgrid</a>, <a href=\"https://arxiv.org/abs/2206.13155\">Bi-VLDoc</a> and <a href=\"https://arxiv.org/abs/2202.13669\">LiLT</a>.</li>\n        </ul>\n<ul>\n          <li><a href=\"https://arxiv.org/abs/2309.11419\">Kosmos-2.5</a>, <a href=\"https://arxiv.org/abs/2303.00289\">StrucText-V2</a>, <a href=\"https://arxiv.org/abs/2111.15664\">DONUT</a>, <a href=\"https://arxiv.org/abs/2306.17107\">LLaVAR</a>, <a href=\"https://arxiv.org/abs/2308.11592\">UniDoc</a>, <a href=\"https://arxiv.org/abs/2310.05126\">UReader</a>, <a href=\"https://arxiv.org/abs/2307.02499\">mPLUG-DocOwl</a>, <a href=\"https://arxiv.org/abs/2311.09808\">PixT3</a>.</li>\n        </ul>\n<ul>\n          <li><a href=\"https://arxiv.org/abs/2308.13418\">Nougat</a> and <a href=\"https://arxiv.org/abs/2311.11810\">DocPedia</a>.\\</li>\n        </ul>",
    "contentMarkdown": "*   **Multi-page DocVQA**\n    *   **OCR-based Multi-page DocVQA**\n        *   [Hi-VT5](https://arxiv.org/abs/2212.05935), [LATIN-prompt](https://arxiv.org/abs/2306.00526)\n    *   **OCR-free Multi-page DocVQA**\n        *   This type of models are what we ideally need to build best performing Multimodal Document AI. As of this writing, currently there is only one work, [PDFTriage](https://arxiv.org/abs/2309.08872). But this is ONLY for native PDF documents, hence is not general to work with different type of documents. The model architecture, data and training processes described at the start of the article can be used to build an OCR-free Multi-page DocVQA models.\n*   **Single-page DocVQA**\n    *   **OCR-based Single-page DocVQA models**\n        *   [LayoutLMv3](https://arxiv.org/abs/2204.08387), [LayoutLMv2](https://arxiv.org/abs/2012.14740), [LayoutLMv1](https://arxiv.org/abs/1912.13318), [GeoLayoutLM](https://arxiv.org/abs/2304.10759), [DocFormer](https://arxiv.org/abs/2106.11539), [XYLayoutLM](https://arxiv.org/abs/2203.06947), [ViBERTgrid](https://arxiv.org/abs/2105.11672), [Bi-VLDoc](https://arxiv.org/abs/2206.13155) and [LiLT](https://arxiv.org/abs/2202.13669).\n    *   **OCR-free Single-page DocVQA models**\n        *   [Kosmos-2.5](https://arxiv.org/abs/2309.11419), [StrucText-V2](https://arxiv.org/abs/2303.00289), [DONUT](https://arxiv.org/abs/2111.15664), [LLaVAR](https://arxiv.org/abs/2306.17107), [UniDoc](https://arxiv.org/abs/2308.11592), [UReader](https://arxiv.org/abs/2310.05126), [mPLUG-DocOwl](https://arxiv.org/abs/2307.02499), [PixT3](https://arxiv.org/abs/2311.09808).\n    *   Single-page DocVQA models for High resolution images\n        *   [Nougat](https://arxiv.org/abs/2308.13418) and [DocPedia](https://arxiv.org/abs/2311.11810).\\\\\n*   Lets dive into few important models listed above.\n\n*   **OCR-based Multi-page DocVQA**\n    *   [Hi-VT5](https://arxiv.org/abs/2212.05935), [LATIN-prompt](https://arxiv.org/abs/2306.00526)\n*   **OCR-free Multi-page DocVQA**\n    *   This type of models are what we ideally need to build best performing Multimodal Document AI. As of this writing, currently there is only one work, [PDFTriage](https://arxiv.org/abs/2309.08872). But this is ONLY for native PDF documents, hence is not general to work with different type of documents. The model architecture, data and training processes described at the start of the article can be used to build an OCR-free Multi-page DocVQA models.\n\n*   [Hi-VT5](https://arxiv.org/abs/2212.05935), [LATIN-prompt](https://arxiv.org/abs/2306.00526)\n\n*   This type of models are what we ideally need to build best performing Multimodal Document AI. As of this writing, currently there is only one work, [PDFTriage](https://arxiv.org/abs/2309.08872). But this is ONLY for native PDF documents, hence is not general to work with different type of documents. The model architecture, data and training processes described at the start of the article can be used to build an OCR-free Multi-page DocVQA models.\n\n*   **OCR-based Single-page DocVQA models**\n    *   [LayoutLMv3](https://arxiv.org/abs/2204.08387), [LayoutLMv2](https://arxiv.org/abs/2012.14740), [LayoutLMv1](https://arxiv.org/abs/1912.13318), [GeoLayoutLM](https://arxiv.org/abs/2304.10759), [DocFormer](https://arxiv.org/abs/2106.11539), [XYLayoutLM](https://arxiv.org/abs/2203.06947), [ViBERTgrid](https://arxiv.org/abs/2105.11672), [Bi-VLDoc](https://arxiv.org/abs/2206.13155) and [LiLT](https://arxiv.org/abs/2202.13669).\n*   **OCR-free Single-page DocVQA models**\n    *   [Kosmos-2.5](https://arxiv.org/abs/2309.11419), [StrucText-V2](https://arxiv.org/abs/2303.00289), [DONUT](https://arxiv.org/abs/2111.15664), [LLaVAR](https://arxiv.org/abs/2306.17107), [UniDoc](https://arxiv.org/abs/2308.11592), [UReader](https://arxiv.org/abs/2310.05126), [mPLUG-DocOwl](https://arxiv.org/abs/2307.02499), [PixT3](https://arxiv.org/abs/2311.09808).\n*   Single-page DocVQA models for High resolution images\n    *   [Nougat](https://arxiv.org/abs/2308.13418) and [DocPedia](https://arxiv.org/abs/2311.11810).\\\\\n\n*   [LayoutLMv3](https://arxiv.org/abs/2204.08387), [LayoutLMv2](https://arxiv.org/abs/2012.14740), [LayoutLMv1](https://arxiv.org/abs/1912.13318), [GeoLayoutLM](https://arxiv.org/abs/2304.10759), [DocFormer](https://arxiv.org/abs/2106.11539), [XYLayoutLM](https://arxiv.org/abs/2203.06947), [ViBERTgrid](https://arxiv.org/abs/2105.11672), [Bi-VLDoc](https://arxiv.org/abs/2206.13155) and [LiLT](https://arxiv.org/abs/2202.13669).\n\n*   [Kosmos-2.5](https://arxiv.org/abs/2309.11419), [StrucText-V2](https://arxiv.org/abs/2303.00289), [DONUT](https://arxiv.org/abs/2111.15664), [LLaVAR](https://arxiv.org/abs/2306.17107), [UniDoc](https://arxiv.org/abs/2308.11592), [UReader](https://arxiv.org/abs/2310.05126), [mPLUG-DocOwl](https://arxiv.org/abs/2307.02499), [PixT3](https://arxiv.org/abs/2311.09808).\n\n*   [Nougat](https://arxiv.org/abs/2308.13418) and [DocPedia](https://arxiv.org/abs/2311.11810).\\\\",
    "contentLength": 6624,
    "wordCount": 361,
    "hasCode": false,
    "hasMath": false,
    "hasImages": false,
    "url": "https://aman.ai/primers/ai/document-intelligence/#categorizing-sota-models"
  },
  {
    "id": "ai-document-intelligence-nougat-13",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Overview of SOTA Document Processing Models",
    "title": "Nougat",
    "order": 13,
    "orderInChapter": 2,
    "contentHtml": "<ul>\n  <li>Nougat (Neural Optical Understanding for Academic Documents), is a multimodal transformer model.</li>\n  <li>This model is primarily build to handle PDF type documents. Specifically  for OCR task on academic documents. It is great for text and table extraction from the document images.</li>\n  <li>The input is an image of a page in PDF. That means, each PDF page is converted to an image of size 896x762 (HxW) and this image is provided as an input to the model. The output <a href=\"https://en.wikipedia.org/wiki/Lightweight_markup_language\">lightweight markup language</a> [markdown file (.md file)].</li>\n  <li>The model architecture is of encoder-decoder type transformer. The encoder is a <a href=\"https://arxiv.org/abs/2111.09883\">swin-transformer</a> and decoder is <a href=\"https://arxiv.org/abs/2001.08210\">mBART</a>.</li>\n  <li>It is trained for OCR task on datasets like arXiv, <a href=\"https://www.ncbi.nlm.nih.gov/pmc/\">PubMed Central (PMC)</a> and <a href=\"https://www.industrydocuments.ucsf.edu\">Industrial documents library (IDL)</a>. Total of ~1.7M PDFs from arXiv were used for training.</li>\n  <li>Below image shows the NOUGAT model architecture and results of OCR task on arXiv test data from the <a href=\"https://arxiv.org/PDF/2308.13418.PDF\">published research paper</a>.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/document-intelligence/Nougat_model_architecture.png\" alt=\"Nougat Model architecture\"></p>\n<p><img src=\"/primers/ai/assets/document-intelligence/Nougat_results_table.png\" alt=\"Nougat results table\"></p>",
    "contentMarkdown": "*   Nougat (Neural Optical Understanding for Academic Documents), is a multimodal transformer model.\n*   This model is primarily build to handle PDF type documents. Specifically for OCR task on academic documents. It is great for text and table extraction from the document images.\n*   The input is an image of a page in PDF. That means, each PDF page is converted to an image of size 896x762 (HxW) and this image is provided as an input to the model. The output [lightweight markup language](https://en.wikipedia.org/wiki/Lightweight_markup_language) \\[markdown file (.md file)\\].\n*   The model architecture is of encoder-decoder type transformer. The encoder is a [swin-transformer](https://arxiv.org/abs/2111.09883) and decoder is [mBART](https://arxiv.org/abs/2001.08210).\n*   It is trained for OCR task on datasets like arXiv, [PubMed Central (PMC)](https://www.ncbi.nlm.nih.gov/pmc/) and [Industrial documents library (IDL)](https://www.industrydocuments.ucsf.edu). Total of ~1.7M PDFs from arXiv were used for training.\n*   Below image shows the NOUGAT model architecture and results of OCR task on arXiv test data from the [published research paper](https://arxiv.org/PDF/2308.13418.PDF).\n\n![Nougat Model architecture](/primers/ai/assets/document-intelligence/Nougat_model_architecture.png)\n\n![Nougat results table](/primers/ai/assets/document-intelligence/Nougat_results_table.png)",
    "contentLength": 1548,
    "wordCount": 163,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/document-intelligence/#nougat"
  },
  {
    "id": "ai-document-intelligence-ureader-14",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Overview of SOTA Document Processing Models",
    "title": "UReader",
    "order": 14,
    "orderInChapter": 3,
    "contentHtml": "<ul>\n  <li>UReader is a multimodal model for OCR-free vision-language tasks.</li>\n  <li>It shows that, with low-cost instruction tuning, SOTA performance can be achieved on various vision-language tasks. No pretraining step is performed for this model.</li>\n  <li>All the tasks are converted into vision-language instruction tuning format before training.</li>\n  <li>It also introduces text reading and key points generation task during finetuning for enhancing text reading and semantic understanding across different tasks.</li>\n  <li>For processing high-resolution images, it introduces adaptive image cropping module to crop high resolution images into smaller ones. These smaller images are then encoded using frozen image encoder and learnable visual abstractor/projector layer.</li>\n  <li>This paper contributes 5 instruction-tuning datasets covering 5 domains of vision-language tasks. These 5 datasets include image of type document, table, chart, natural image, and webpage screenshot.</li>\n  <li>UReader model is built on top of <a href=\"https://arxiv.org/abs/2304.14178\">mPLUG-owl model</a>. mPLUG-owl model is a multimodal model, built using a modulerized learning technique, which enables combining pretrained LLM and pretrained encoders (image or other modalities) to create a multimodal model.</li>\n  <li>Below images show model architecture and results from the research paper.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/document-intelligence/UReader_model_architecture.png\" alt=\"UReader Model architecture\"></p>\n<p><img src=\"/primers/ai/assets/document-intelligence/UReader_results_table.png\" alt=\"UReader results table\"></p>",
    "contentMarkdown": "*   UReader is a multimodal model for OCR-free vision-language tasks.\n*   It shows that, with low-cost instruction tuning, SOTA performance can be achieved on various vision-language tasks. No pretraining step is performed for this model.\n*   All the tasks are converted into vision-language instruction tuning format before training.\n*   It also introduces text reading and key points generation task during finetuning for enhancing text reading and semantic understanding across different tasks.\n*   For processing high-resolution images, it introduces adaptive image cropping module to crop high resolution images into smaller ones. These smaller images are then encoded using frozen image encoder and learnable visual abstractor/projector layer.\n*   This paper contributes 5 instruction-tuning datasets covering 5 domains of vision-language tasks. These 5 datasets include image of type document, table, chart, natural image, and webpage screenshot.\n*   UReader model is built on top of [mPLUG-owl model](https://arxiv.org/abs/2304.14178). mPLUG-owl model is a multimodal model, built using a modulerized learning technique, which enables combining pretrained LLM and pretrained encoders (image or other modalities) to create a multimodal model.\n*   Below images show model architecture and results from the research paper.\n\n![UReader Model architecture](/primers/ai/assets/document-intelligence/UReader_model_architecture.png)\n\n![UReader results table](/primers/ai/assets/document-intelligence/UReader_results_table.png)",
    "contentLength": 1643,
    "wordCount": 190,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/document-intelligence/#ureader"
  },
  {
    "id": "ai-document-intelligence-layoutlmv3-15",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Overview of SOTA Document Processing Models",
    "title": "LayoutLMv3",
    "order": 15,
    "orderInChapter": 4,
    "contentHtml": "<ul>\n  <li>LayoutLMv3 comes from a legacy of LayoutLM models from Microsoft.</li>\n  <li>The first LayoutLM model (LayoutLMv1) introduced pretraining techniques, first of its kind to jointly model text and layout in scanned documents during pretraining.</li>\n  <li>Later, LayoutLMv2 introduced new pretraining tasks to jointly model interactions between text, layout and image in a single Multimodal framework.</li>\n  <li>LayoutLMv3 proposes to jointly use Masked-Language-Modeling (MLM) and Masked-Image-Modeling (MIM) for pretraining. It demonstrated SOTA performance on both text-centric and image-centric tasks.</li>\n  <li>Below images show the model architecture and results comparing its performance to previous LayoutLM models and other works.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/document-intelligence/LayoutLMv3_model_architecture.png\" alt=\"LayoutLMv3 Model architecture\"></p>\n<p><img src=\"/primers/ai/assets/document-intelligence/LayoutLMv3_results_table.png\" alt=\"LayoutLMv3 results table\"></p>",
    "contentMarkdown": "*   LayoutLMv3 comes from a legacy of LayoutLM models from Microsoft.\n*   The first LayoutLM model (LayoutLMv1) introduced pretraining techniques, first of its kind to jointly model text and layout in scanned documents during pretraining.\n*   Later, LayoutLMv2 introduced new pretraining tasks to jointly model interactions between text, layout and image in a single Multimodal framework.\n*   LayoutLMv3 proposes to jointly use Masked-Language-Modeling (MLM) and Masked-Image-Modeling (MIM) for pretraining. It demonstrated SOTA performance on both text-centric and image-centric tasks.\n*   Below images show the model architecture and results comparing its performance to previous LayoutLM models and other works.\n\n![LayoutLMv3 Model architecture](/primers/ai/assets/document-intelligence/LayoutLMv3_model_architecture.png)\n\n![LayoutLMv3 results table](/primers/ai/assets/document-intelligence/LayoutLMv3_results_table.png)",
    "contentLength": 1010,
    "wordCount": 104,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/document-intelligence/#layoutlmv3"
  },
  {
    "id": "ai-document-intelligence-unichart-16",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Overview of SOTA Document Processing Models",
    "title": "UniChart",
    "order": 16,
    "orderInChapter": 5,
    "contentHtml": "<ul>\n  <li>UniChart is a Universal Vision-language pretrained model for chart comprehension and reasoning.</li>\n  <li>This paper introduces a large-scale chart corpus containing a diverse range of visual styles and topics for pretraining.</li>\n  <li>Chart specific pretraining tasks are proposed, which enables this model to achieve SOTA performance. These pretraining techinques include both low-level and high-level chart related tasks.</li>\n  <li>The low-level pretraining tasks allows model to be used for information extraction.</li>\n  <li>The high-level pretraining tasks helps model to acquire complex reasoning skills.</li>\n  <li>Below images show the model architecture and results.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/document-intelligence/UniChart_model_architecture.png\" alt=\"UniChart Model architecture\"></p>\n<p><img src=\"/primers/ai/assets/document-intelligence/UniChart_results_table.png\" alt=\"UniChart results table\"></p>",
    "contentMarkdown": "*   UniChart is a Universal Vision-language pretrained model for chart comprehension and reasoning.\n*   This paper introduces a large-scale chart corpus containing a diverse range of visual styles and topics for pretraining.\n*   Chart specific pretraining tasks are proposed, which enables this model to achieve SOTA performance. These pretraining techinques include both low-level and high-level chart related tasks.\n*   The low-level pretraining tasks allows model to be used for information extraction.\n*   The high-level pretraining tasks helps model to acquire complex reasoning skills.\n*   Below images show the model architecture and results.\n\n![UniChart Model architecture](/primers/ai/assets/document-intelligence/UniChart_model_architecture.png)\n\n![UniChart results table](/primers/ai/assets/document-intelligence/UniChart_results_table.png)",
    "contentLength": 944,
    "wordCount": 98,
    "hasCode": false,
    "hasMath": false,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/document-intelligence/#unichart"
  },
  {
    "id": "ai-document-intelligence-mplug-docowl-15-17",
    "articleSlug": "document-intelligence",
    "articleTitle": "Document Intelligence",
    "category": "NLP/LLMs",
    "chapter": "Overview of SOTA Document Processing Models",
    "title": "mPLUG-DocOwl 1.5",
    "order": 17,
    "orderInChapter": 6,
    "contentHtml": "<ul>\n  <li>Proposed in <a href=\"https://arxiv.org/abs/2403.12895\">mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding</a> by Hu et al. from Alibaba Group and Renmin University of China.</li>\n  <li>mPLUG-DocOwl is a novel approach for Visual Document Understanding (VDU) that emphasizes the significance of structure information in interpreting text-rich images like documents, tables, and charts. Unlike existing Multimodal Large Language Models (MLLMs), which can recognize text but struggle with structural comprehension, the proposed model, DocOwl 1.5, introduces Unified Structure Learning to enhance MLLMs’ performance by focusing on structure-aware parsing and multi-grained text localization across five domains: document, webpage, table, chart, and natural image.</li>\n  <li>To efficiently encode structure information, the authors developed the H-Reducer, a vision-to-text module designed to maintain layout information while reducing the length of visual features by merging horizontally adjacent patches through convolution. This enables the model to process high-resolution images more efficiently by preserving their spatial relationships.</li>\n  <li>The Unified Structure Learning framework comprises two main components: structure-aware parsing tasks that instruct the model to parse text in images in a structure-aware manner (using line feeds, spaces, and extended Markdown syntax for different structures) and multi-grained text localization tasks that enhance the model’s ability to associate texts with specific image positions.</li>\n  <li>A comprehensive training set, DocStruct4M, was constructed to support this structure learning approach. It includes structure-aware text sequences and multi-grained pairs of texts and bounding boxes derived from publicly available text-rich images. Additionally, a smaller, high-quality reasoning dataset, DocReason25K, was created to further improve the model’s document domain explanation abilities.</li>\n  <li>The figure below from the paper shows: (a) the two-stage training framework and (b) overall architecture of DocOwl 1.5. The global image and cropped images are processed independently by the Visual Encoder and H-Reducer. <code class=\"language-plaintext highlighter-rouge\">&lt;rowx-coly&gt;</code> is the special textual token to indicate that the position of the cropped image in the original image is the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mi>h</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"msubsup\" id=\"MathJax-Span-3\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-5\"><span class=\"mrow\" id=\"MathJax-Span-6\"><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-8\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mi>h</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">x^{th}</script> row and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mi>h</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-9\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.1em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-10\"><span class=\"msubsup\" id=\"MathJax-Span-11\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-13\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-16\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mi>h</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">y^{th}</script> column.</li>\n</ul>\n<p><img src=\"../../../images/papers/mPLUG-DocOwl.jpg\" alt=\"\"></p>\n<ul>\n  <li>The paper reports that DocOwl 1.5 significantly outperforms state-of-the-art models on 10 visual document understanding benchmarks, improving the performance of MLLMs with a 7B LLM by over 10 points in half of the benchmarks. This is attributed to its effective use of Unified Structure Learning and the H-Reducer.</li>\n  <li><a href=\"https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5\">Code</a></li>\n</ul>",
    "contentMarkdown": "*   Proposed in [mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding](https://arxiv.org/abs/2403.12895) by Hu et al. from Alibaba Group and Renmin University of China.\n*   mPLUG-DocOwl is a novel approach for Visual Document Understanding (VDU) that emphasizes the significance of structure information in interpreting text-rich images like documents, tables, and charts. Unlike existing Multimodal Large Language Models (MLLMs), which can recognize text but struggle with structural comprehension, the proposed model, DocOwl 1.5, introduces Unified Structure Learning to enhance MLLMs’ performance by focusing on structure-aware parsing and multi-grained text localization across five domains: document, webpage, table, chart, and natural image.\n*   To efficiently encode structure information, the authors developed the H-Reducer, a vision-to-text module designed to maintain layout information while reducing the length of visual features by merging horizontally adjacent patches through convolution. This enables the model to process high-resolution images more efficiently by preserving their spatial relationships.\n*   The Unified Structure Learning framework comprises two main components: structure-aware parsing tasks that instruct the model to parse text in images in a structure-aware manner (using line feeds, spaces, and extended Markdown syntax for different structures) and multi-grained text localization tasks that enhance the model’s ability to associate texts with specific image positions.\n*   A comprehensive training set, DocStruct4M, was constructed to support this structure learning approach. It includes structure-aware text sequences and multi-grained pairs of texts and bounding boxes derived from publicly available text-rich images. Additionally, a smaller, high-quality reasoning dataset, DocReason25K, was created to further improve the model’s document domain explanation abilities.\n*   The figure below from the paper shows: (a) the two-stage training framework and (b) overall architecture of DocOwl 1.5. The global image and cropped images are processed independently by the Visual Encoder and H-Reducer. `<rowx-coly>` is the special textual token to indicate that the position of the cropped image in the original image is the xthxthx^{th} row and ythythy^{th} column.\n\n![](../../../images/papers/mPLUG-DocOwl.jpg)\n\n*   The paper reports that DocOwl 1.5 significantly outperforms state-of-the-art models on 10 visual document understanding benchmarks, improving the performance of MLLMs with a 7B LLM by over 10 points in half of the benchmarks. This is attributed to its effective use of Unified Structure Learning and the H-Reducer.\n*   [Code](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5)",
    "contentLength": 7423,
    "wordCount": 365,
    "hasCode": true,
    "hasMath": true,
    "hasImages": true,
    "url": "https://aman.ai/primers/ai/document-intelligence/#mplug-docowl-1.5"
  }
]