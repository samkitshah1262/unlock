[
  {
    "id": "ai-meena-metric-sensibleness-and-specificity-average-ssa-1",
    "domain": "ai_primers",
    "category": "Models",
    "article": "Meena",
    "articleSlug": "meena",
    "chapter": "Meena - Towards a Human-like Open-Domain Chatbot",
    "title": "Metric: Sensibleness and Specificity Average (SSA)",
    "subtitle": "Meena - Towards a Human-like Open-Domain Chatbot",
    "contentHtml": "<ul>\n  <li>SSA is designed to measure how sensible and specific Meena’s responses are in the context of a given conversation.</li>\n  <li>SSA measures the extent to which the language model produces sensible and specific responses to prompts.</li>\n  <li>To compute SSA, a set of prompts is given to the language model, and the generated responses are evaluated by human judges.</li>\n  <li>Each response is given a score between 0 and 1, representing its sensibleness and specificity.</li>\n  <li>Sensibleness refers to the degree to which the response is coherent and grammatically correct, while specificity refers to the degree to which the response is relevant and informative.</li>\n  <li>The average of the sensibleness and specificity scores for all responses is then calculated to obtain the SSA score.</li>\n  <li>A higher SSA score indicates that the language model produces more sensible and specific responses.</li>\n  <li>A quick note here about perplexity, which is a commonly used metric to evaluate language models,while perplexity is a useful metric for evaluating the quality of language models for tasks such as machine translation or text generation, it is not always a reliable measure of overall language model performance.</li>\n  <li>SSA is a metric that is specifically designed to evaluate the quality of generated text.</li>\n  <li>It provides a more direct measure of the language model’s ability to produce coherent and relevant responses to prompts.</li>\n</ul>",
    "contentMarkdown": "*   SSA is designed to measure how sensible and specific Meena’s responses are in the context of a given conversation.\n*   SSA measures the extent to which the language model produces sensible and specific responses to prompts.\n*   To compute SSA, a set of prompts is given to the language model, and the generated responses are evaluated by human judges.\n*   Each response is given a score between 0 and 1, representing its sensibleness and specificity.\n*   Sensibleness refers to the degree to which the response is coherent and grammatically correct, while specificity refers to the degree to which the response is relevant and informative.\n*   The average of the sensibleness and specificity scores for all responses is then calculated to obtain the SSA score.\n*   A higher SSA score indicates that the language model produces more sensible and specific responses.\n*   A quick note here about perplexity, which is a commonly used metric to evaluate language models,while perplexity is a useful metric for evaluating the quality of language models for tasks such as machine translation or text generation, it is not always a reliable measure of overall language model performance.\n*   SSA is a metric that is specifically designed to evaluate the quality of generated text.\n*   It provides a more direct measure of the language model’s ability to produce coherent and relevant responses to prompts.",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "models"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 227,
      "contentLength": 1482
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/meena/#metric:-sensibleness-and-specificity-average-(ssa)",
    "scrapedAt": "2025-12-28T11:50:26.758Z"
  },
  {
    "id": "ai-meena-training-2",
    "domain": "ai_primers",
    "category": "Models",
    "article": "Meena",
    "articleSlug": "meena",
    "chapter": "Meena - Towards a Human-like Open-Domain Chatbot",
    "title": "Training",
    "subtitle": "Meena - Towards a Human-like Open-Domain Chatbot",
    "contentHtml": "<ul>\n  <li>“The training objective is to minimize perplexity, the uncertainty of predicting the next token (in this case, the next word in a conversation). At its heart lies the Evolved Transformer seq2seq architecture, a Transformer architecture discovered by evolutionary neural architecture search to improve perplexity.” <a href=\"https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html\">Google AI</a></li>\n  <li>The data Meena used to train on was mined and filtered from public domain social media conversations.</li>\n  <li>The conversations with multiple speakers are stored in a tree structure where the root of the tree is the first message and the replies are the child nodes.</li>\n  <li>“The Meena model has 2.6 billion parameters and is trained on 341 GB of text, filtered from public domain social media conversations.</li>\n  <li>Compared to an existing state-of-the-art generative model, OpenAI GPT-2, Meena has 1.7x greater model capacity and was trained on 8.5x more data.” <a href=\"https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html\">Google AI</a></li>\n</ul>",
    "contentMarkdown": "*   “The training objective is to minimize perplexity, the uncertainty of predicting the next token (in this case, the next word in a conversation). At its heart lies the Evolved Transformer seq2seq architecture, a Transformer architecture discovered by evolutionary neural architecture search to improve perplexity.” [Google AI](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html)\n*   The data Meena used to train on was mined and filtered from public domain social media conversations.\n*   The conversations with multiple speakers are stored in a tree structure where the root of the tree is the first message and the replies are the child nodes.\n*   “The Meena model has 2.6 billion parameters and is trained on 341 GB of text, filtered from public domain social media conversations.\n*   Compared to an existing state-of-the-art generative model, OpenAI GPT-2, Meena has 1.7x greater model capacity and was trained on 8.5x more data.” [Google AI](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html)",
    "order": 2,
    "orderInChapter": 2,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models",
      "transformer",
      "gpt"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 142,
      "contentLength": 1121
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/meena/#training",
    "scrapedAt": "2025-12-28T11:50:26.758Z"
  },
  {
    "id": "ai-meena-architecture-3",
    "domain": "ai_primers",
    "category": "Models",
    "article": "Meena",
    "articleSlug": "meena",
    "chapter": "Meena - Towards a Human-like Open-Domain Chatbot",
    "title": "Architecture",
    "subtitle": "Meena - Towards a Human-like Open-Domain Chatbot",
    "contentHtml": "<ul>\n  <li>Meena’s architecture includes: an Evolved Transformer (ET) seq2seq model with 2.6B parameters with 1 ET encoder block and 13 ET decoder blocks as shown below.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/bard/1.png\" alt=\"\"></p>\n<ul>\n  <li>The “Evolved Transformer” (ET) is a neural architecture that combines evolutionary algorithms and the Transformer architecture for automatic neural architecture search.</li>\n  <li>The encoder, per usual, encodes the contextual meaning of the conversation while the decoder works on a sensible response.</li>\n  <li>“Through tuning the hyper-parameters, we discovered that a more powerful decoder was the key to higher conversational quality.” <a href=\"https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html\">Google AI</a></li>\n  <li>For decoding in particular, the paper found that a model with sufficiently low perplexity can achieve diverse and high-quality responses using a simple sample-and-rank decoding strategy.</li>\n  <li>In this strategy, they sampled N independent candidate responses using random sampling with a temperature T, and selected the candidate response with the highest probability as the final output.</li>\n  <li>The temperature T is a hyper-parameter that regulates the probability distribution of the next token during decoding.</li>\n</ul>",
    "contentMarkdown": "*   Meena’s architecture includes: an Evolved Transformer (ET) seq2seq model with 2.6B parameters with 1 ET encoder block and 13 ET decoder blocks as shown below.\n\n![](/primers/ai/assets/bard/1.png)\n\n*   The “Evolved Transformer” (ET) is a neural architecture that combines evolutionary algorithms and the Transformer architecture for automatic neural architecture search.\n*   The encoder, per usual, encodes the contextual meaning of the conversation while the decoder works on a sensible response.\n*   “Through tuning the hyper-parameters, we discovered that a more powerful decoder was the key to higher conversational quality.” [Google AI](https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html)\n*   For decoding in particular, the paper found that a model with sufficiently low perplexity can achieve diverse and high-quality responses using a simple sample-and-rank decoding strategy.\n*   In this strategy, they sampled N independent candidate responses using random sampling with a temperature T, and selected the candidate response with the highest probability as the final output.\n*   The temperature T is a hyper-parameter that regulates the probability distribution of the next token during decoding.",
    "order": 3,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models",
      "transformer"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 165,
      "contentLength": 1334
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/meena/#architecture",
    "scrapedAt": "2025-12-28T11:50:26.758Z"
  },
  {
    "id": "ai-meena-results-4",
    "domain": "ai_primers",
    "category": "Models",
    "article": "Meena",
    "articleSlug": "meena",
    "chapter": "Meena - Towards a Human-like Open-Domain Chatbot",
    "title": "Results",
    "subtitle": "Meena - Towards a Human-like Open-Domain Chatbot",
    "contentHtml": "<p><img src=\"/primers/ai/assets/bard/2.png\" alt=\"\"></p>\n<ul>\n  <li>Meena was able to achieve a perplexity of 10.2 (smaller is better) and that translates to an SSA score of 72% compared to 86% SSA achieved by the average person.</li>\n  <li>The full version of Meena a filtering mechanism and tuned decoding had further advances the SSA score to 79%.</li>\n</ul>",
    "contentMarkdown": "![](/primers/ai/assets/bard/2.png)\n\n*   Meena was able to achieve a perplexity of 10.2 (smaller is better) and that translates to an SSA score of 72% compared to 86% SSA achieved by the average person.\n*   The full version of Meena a filtering mechanism and tuned decoding had further advances the SSA score to 79%.",
    "order": 4,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "models"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 52,
      "contentLength": 360
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/meena/#results",
    "scrapedAt": "2025-12-28T11:50:26.758Z"
  }
]