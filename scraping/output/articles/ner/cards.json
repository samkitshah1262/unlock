[
  {
    "id": "ai-ner-bidirectional-lstm-crf-models-for-sequence-tagging-1",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Named Entity Recognition",
    "articleSlug": "ner",
    "chapter": "Neural Sequence Labelling Models",
    "title": "Bidirectional LSTM-CRF Models for Sequence Tagging (2015)",
    "subtitle": "Neural Sequence Labelling Models",
    "contentHtml": "<h4 id=\"architecture\">Architecture</h4>\n<ul>\n  <li>\n    <p>To the best of our knowledge, this was the first work to apply a bidirectional-LSTM-CRF architecture for sequence tagging. The idea is to use two LSTMs, one reading each word in a sentence from beginning to end and another reading the same but from end to beginning, producing for each word, a vector representation made from both the un-folded LSTM (i.e., forward and backward) read up to that word. There is this intuition that the vector for each word will take into account the words read/seen before, on both directions.</p>\n  </li>\n  <li>\n    <p>There is no explicit mention in the paper on how the vectors from each LSTM are combined to produce a single vector for each word, I will assume that they are just concatenated.</p>\n  </li>\n  <li>\n    <p>This bidirectional-LSTM architecture is then combined with a CRF layer at the top. A Conditional Random Field (CRF) layer has a state transition matrix as parameters, which can be used to efficiently use past attributed tags in predicting the current tag. The following diagram from Huang et al. (2015) shows a bi-LSTM-CRF model for NER.</p>\n  </li>\n</ul>\n<p>To the best of our knowledge, this was the first work to apply a bidirectional-LSTM-CRF architecture for sequence tagging. The idea is to use two LSTMs, one reading each word in a sentence from beginning to end and another reading the same but from end to beginning, producing for each word, a vector representation made from both the un-folded LSTM (i.e., forward and backward) read up to that word. There is this intuition that the vector for each word will take into account the words read/seen before, on both directions.</p>\n<p>There is no explicit mention in the paper on how the vectors from each LSTM are combined to produce a single vector for each word, I will assume that they are just concatenated.</p>\n<p>This bidirectional-LSTM architecture is then combined with a CRF layer at the top. A Conditional Random Field (CRF) layer has a state transition matrix as parameters, which can be used to efficiently use past attributed tags in predicting the current tag. The following diagram from Huang et al. (2015) shows a bi-LSTM-CRF model for NER.</p>\n<p><img src=\"/primers/ai/assets/ner/A_bi-LSTM-CRF_model.png\" alt=\"\"></p>\n<h4 id=\"features-and-embeddings\">Features and Embeddings</h4>\n<ul>\n  <li>Word embeddings generated from each state of the LSTM, are combined with hand-crafted features:\n    <ul>\n      <li>spelling, e.g.: capitalization, punctuation, word patters, etc.</li>\n      <li>context, e.g: uni-, bi- and tri-gram features</li>\n    </ul>\n  </li>\n  <li>The embeddings used are those produced by <a href=\"http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf\">Collobert et al., 2011</a> which has 130K vocabulary size and each word corresponds to a 50-dimensional embedding vector.</li>\n</ul>\n<ul>\n      <li>spelling, e.g.: capitalization, punctuation, word patters, etc.</li>\n      <li>context, e.g: uni-, bi- and tri-gram features</li>\n    </ul>\n<h5 id=\"features-connection-tricks\">Features Connection Tricks</h5>\n<ul>\n  <li>The input for the model include both word, spelling and context features, however, the authors suggest direct connecting the hand-crafted features to the output layer (i.e, CRF) which accelerates training and result in very similar tagging accuracy, when comparing without direct connections. That is, in my understanding, the vector representing the hand-crafted features are passed directly to the CRF and are not passed through the bidirectional-LSTM. The following diagram from Huang et al. (2015) shows a bi-LSTM-CRF model with Maximum Entropy features.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/ner/A_bi-LSTM-CRF_model_with_max_ent_features.png\" alt=\"\"></p>",
    "contentMarkdown": "#### Architecture\n\n*   To the best of our knowledge, this was the first work to apply a bidirectional-LSTM-CRF architecture for sequence tagging. The idea is to use two LSTMs, one reading each word in a sentence from beginning to end and another reading the same but from end to beginning, producing for each word, a vector representation made from both the un-folded LSTM (i.e., forward and backward) read up to that word. There is this intuition that the vector for each word will take into account the words read/seen before, on both directions.\n    \n*   There is no explicit mention in the paper on how the vectors from each LSTM are combined to produce a single vector for each word, I will assume that they are just concatenated.\n    \n*   This bidirectional-LSTM architecture is then combined with a CRF layer at the top. A Conditional Random Field (CRF) layer has a state transition matrix as parameters, which can be used to efficiently use past attributed tags in predicting the current tag. The following diagram from Huang et al. (2015) shows a bi-LSTM-CRF model for NER.\n    \n\nTo the best of our knowledge, this was the first work to apply a bidirectional-LSTM-CRF architecture for sequence tagging. The idea is to use two LSTMs, one reading each word in a sentence from beginning to end and another reading the same but from end to beginning, producing for each word, a vector representation made from both the un-folded LSTM (i.e., forward and backward) read up to that word. There is this intuition that the vector for each word will take into account the words read/seen before, on both directions.\n\nThere is no explicit mention in the paper on how the vectors from each LSTM are combined to produce a single vector for each word, I will assume that they are just concatenated.\n\nThis bidirectional-LSTM architecture is then combined with a CRF layer at the top. A Conditional Random Field (CRF) layer has a state transition matrix as parameters, which can be used to efficiently use past attributed tags in predicting the current tag. The following diagram from Huang et al. (2015) shows a bi-LSTM-CRF model for NER.\n\n![](/primers/ai/assets/ner/A_bi-LSTM-CRF_model.png)\n\n#### Features and Embeddings\n\n*   Word embeddings generated from each state of the LSTM, are combined with hand-crafted features:\n    *   spelling, e.g.: capitalization, punctuation, word patters, etc.\n    *   context, e.g: uni-, bi- and tri-gram features\n*   The embeddings used are those produced by [Collobert et al., 2011](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf) which has 130K vocabulary size and each word corresponds to a 50-dimensional embedding vector.\n\n*   spelling, e.g.: capitalization, punctuation, word patters, etc.\n*   context, e.g: uni-, bi- and tri-gram features\n\n##### Features Connection Tricks\n\n*   The input for the model include both word, spelling and context features, however, the authors suggest direct connecting the hand-crafted features to the output layer (i.e, CRF) which accelerates training and result in very similar tagging accuracy, when comparing without direct connections. That is, in my understanding, the vector representing the hand-crafted features are passed directly to the CRF and are not passed through the bidirectional-LSTM. The following diagram from Huang et al. (2015) shows a bi-LSTM-CRF model with Maximum Entropy features.\n\n![](/primers/ai/assets/ner/A_bi-LSTM-CRF_model_with_max_ent_features.png)",
    "order": 1,
    "orderInChapter": 1,
    "difficulty": 2,
    "estimatedMinutes": 3,
    "tags": [
      "nlpllms",
      "embedding",
      "lstm",
      "bert"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 527,
      "contentLength": 3798
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ner/#bidirectional-lstm-crf-models-for-sequence-tagging-(2015)",
    "scrapedAt": "2025-12-28T11:54:15.608Z"
  },
  {
    "id": "ai-ner-summary-2",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Named Entity Recognition",
    "articleSlug": "ner",
    "chapter": "Neural Sequence Labelling Models",
    "title": "Summary",
    "subtitle": "Neural Sequence Labelling Models",
    "contentHtml": "<p>In essence, I guess one can see this architecture as using the output of the bidirectional-LSTM, vector representations for each word in a sentence, together with a vector of features derived from spelling and context hand-crafted rules, these vectors are concatenated and passed to a CRF layer.</p>\n<h4 id=\"named-entity-recognition-with-bidirectional-lstm-cnns-2016\"><a href=\"https://www.aclweb.org/anthology/Q16-1026\">Named Entity Recognition with Bidirectional LSTM-CNNs (2016)</a></h4>\n<h4 id=\"architecture-1\">Architecture</h4>\n<ul>\n  <li>The authors propose a hybrid model combining bidirectional-LSTMs with a Convolutional Neural Network (CNN), the latter learns both character- and word-level features. So, this makes use of words-embeddings, additional hand-crafted word features, and CNN-extracted character-level features. All these features, for each word, are fed into a bidirectional-LSTM. The following diagram from Chiu and Nichols (2016) shows a bidirectional-LSTMs with CNNs.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/ner/CNN-Char-Embeddings.png\" alt=\"\"></p>\n<ul>\n  <li>The output vector of each LSTM (i.e., forward and backward) at each time step is decoded by a linear layer and a log-softmax layer into log-probabilities for each tag category, and These two vectors are then added together. The following diagram from Chiu and Nichols (2016) shows the output layer.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/ner/output_layer.png\" alt=\"\"></p>\n<ul>\n  <li>Character-level features are induced by a CNN architecture, which was successfully applied to Spanish and Portuguese NER by <a href=\"http://www.anthology.aclweb.org/W/W15/W15-3904.pdf\">Santos et al.</a> (2015) and German POS-tagging by <a href=\"http://www.aclweb.org/anthology/D15-1025\">Labeau et al.</a> (2015). For each word a convolution and a max layer are applied to extract a new feature vector from the per-character feature vectors such as character embeddings and character type. The following diagram from Chiu and Nichols (2016) shows the char-embeddings architecture.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/ner/bi-directional-LSTM-with-CNN-chars.png\" alt=\"\"></p>\n<h4 id=\"features-and-embeddings-1\">Features and Embeddings</h4>\n<ul>\n  <li><strong>Word Embeddings</strong>: 50-dimensional word embeddings by <a href=\"http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf\">Collobert et al.</a> (2011), all words are lower-cased, embeddings are allowed to be - modified during training.</li>\n  <li><strong>Character Embeddings</strong>: randomly initialized a lookup table with values drawn from a uniform distribution with range [−0.5,0.5] to output a character embedding of 25 dimensions. Two special tokens - are added: PADDING and UNKNOWN.</li>\n  <li><strong>Additional Char Features</strong> A lookup table was used to output a 4-dimensional vector representing the type of the character (<em>upper case</em>, <em>lower case</em>, <em>punctuation</em>, <em>other</em>).</li>\n  <li><strong>Additional Word Features</strong>: each words is tagged as <em>allCaps</em>, <em>upperInitial</em>, <em>lowercase</em>, <em>mixedCaps</em>, <em>noinfo</em>.</li>\n  <li><strong>Lexicons</strong>: partial lexicon matches using a list of known named-entities from DBpedia. The list is then used to perform <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-45\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-46\"><span class=\"mi\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">n</script>-gram matches against the words. A match is successful when the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-48\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-49\"><span class=\"mi\" id=\"MathJax-Span-50\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">n</script>-gram matches the prefix or suffix of an entry and is at least half the length of the entry.</li>\n</ul>",
    "contentMarkdown": "In essence, I guess one can see this architecture as using the output of the bidirectional-LSTM, vector representations for each word in a sentence, together with a vector of features derived from spelling and context hand-crafted rules, these vectors are concatenated and passed to a CRF layer.\n\n#### [Named Entity Recognition with Bidirectional LSTM-CNNs (2016)](https://www.aclweb.org/anthology/Q16-1026)\n\n#### Architecture\n\n*   The authors propose a hybrid model combining bidirectional-LSTMs with a Convolutional Neural Network (CNN), the latter learns both character- and word-level features. So, this makes use of words-embeddings, additional hand-crafted word features, and CNN-extracted character-level features. All these features, for each word, are fed into a bidirectional-LSTM. The following diagram from Chiu and Nichols (2016) shows a bidirectional-LSTMs with CNNs.\n\n![](/primers/ai/assets/ner/CNN-Char-Embeddings.png)\n\n*   The output vector of each LSTM (i.e., forward and backward) at each time step is decoded by a linear layer and a log-softmax layer into log-probabilities for each tag category, and These two vectors are then added together. The following diagram from Chiu and Nichols (2016) shows the output layer.\n\n![](/primers/ai/assets/ner/output_layer.png)\n\n*   Character-level features are induced by a CNN architecture, which was successfully applied to Spanish and Portuguese NER by [Santos et al.](http://www.anthology.aclweb.org/W/W15/W15-3904.pdf) (2015) and German POS-tagging by [Labeau et al.](http://www.aclweb.org/anthology/D15-1025) (2015). For each word a convolution and a max layer are applied to extract a new feature vector from the per-character feature vectors such as character embeddings and character type. The following diagram from Chiu and Nichols (2016) shows the char-embeddings architecture.\n\n![](/primers/ai/assets/ner/bi-directional-LSTM-with-CNN-chars.png)\n\n#### Features and Embeddings\n\n*   **Word Embeddings**: 50-dimensional word embeddings by [Collobert et al.](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf) (2011), all words are lower-cased, embeddings are allowed to be - modified during training.\n*   **Character Embeddings**: randomly initialized a lookup table with values drawn from a uniform distribution with range \\[−0.5,0.5\\] to output a character embedding of 25 dimensions. Two special tokens - are added: PADDING and UNKNOWN.\n*   **Additional Char Features** A lookup table was used to output a 4-dimensional vector representing the type of the character (_upper case_, _lower case_, _punctuation_, _other_).\n*   **Additional Word Features**: each words is tagged as _allCaps_, _upperInitial_, _lowercase_, _mixedCaps_, _noinfo_.\n*   **Lexicons**: partial lexicon matches using a list of known named-entities from DBpedia. The list is then used to perform nnn\\-gram matches against the words. A match is successful when the nnn\\-gram matches the prefix or suffix of an entry and is at least half the length of the entry.",
    "order": 2,
    "orderInChapter": 2,
    "difficulty": 3,
    "estimatedMinutes": 2,
    "tags": [
      "nlpllms",
      "neural network",
      "embedding",
      "convolution",
      "cnn",
      "lstm",
      "bert"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 397,
      "contentLength": 5816
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ner/#summary",
    "scrapedAt": "2025-12-28T11:54:15.608Z"
  },
  {
    "id": "ai-ner-summary-3",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Named Entity Recognition",
    "articleSlug": "ner",
    "chapter": "Neural Sequence Labelling Models",
    "title": "Summary",
    "subtitle": "Neural Sequence Labelling Models",
    "contentHtml": "<ul>\n  <li>\n    <p>The authors also explore several features, some hand-crafted:</p>\n\n    <ul>\n      <li>word embeddings</li>\n      <li>word shape features</li>\n      <li>character-level features (extracted with a CNN)</li>\n      <li>lexical features</li>\n    </ul>\n  </li>\n  <li>\n    <p>All these features are then concatenated, passed through a bi-LSTM and each time step is decoded by a linear layer and a log-softmax layer into log-probabilities for each tag category. The model also learns a tag transition matrix, and at inference time the Viterbi algorithm selects the sequence that maximizes the score all possible tag-sequences.</p>\n  </li>\n</ul>\n<p>The authors also explore several features, some hand-crafted:</p>\n<ul>\n      <li>word embeddings</li>\n      <li>word shape features</li>\n      <li>character-level features (extracted with a CNN)</li>\n      <li>lexical features</li>\n    </ul>\n<p>All these features are then concatenated, passed through a bi-LSTM and each time step is decoded by a linear layer and a log-softmax layer into log-probabilities for each tag category. The model also learns a tag transition matrix, and at inference time the Viterbi algorithm selects the sequence that maximizes the score all possible tag-sequences.</p>",
    "contentMarkdown": "*   The authors also explore several features, some hand-crafted:\n    \n    *   word embeddings\n    *   word shape features\n    *   character-level features (extracted with a CNN)\n    *   lexical features\n*   All these features are then concatenated, passed through a bi-LSTM and each time step is decoded by a linear layer and a log-softmax layer into log-probabilities for each tag category. The model also learns a tag transition matrix, and at inference time the Viterbi algorithm selects the sequence that maximizes the score all possible tag-sequences.\n    \n\nThe authors also explore several features, some hand-crafted:\n\n*   word embeddings\n*   word shape features\n*   character-level features (extracted with a CNN)\n*   lexical features\n\nAll these features are then concatenated, passed through a bi-LSTM and each time step is decoded by a linear layer and a log-softmax layer into log-probabilities for each tag category. The model also learns a tag transition matrix, and at inference time the Viterbi algorithm selects the sequence that maximizes the score all possible tag-sequences.",
    "order": 3,
    "orderInChapter": 3,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "embedding",
      "cnn",
      "lstm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 162,
      "contentLength": 1257
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ner/#summary",
    "scrapedAt": "2025-12-28T11:54:15.608Z"
  },
  {
    "id": "ai-ner-implementations-4",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Named Entity Recognition",
    "articleSlug": "ner",
    "chapter": "Neural Sequence Labelling Models",
    "title": "Implementations",
    "subtitle": "Neural Sequence Labelling Models",
    "contentHtml": "<ul>\n  <li><a href=\"https://github.com/kamalkraj/Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs\">https://github.com/kamalkraj/Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs</a></li>\n</ul>",
    "contentMarkdown": "*   [https://github.com/kamalkraj/Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs](https://github.com/kamalkraj/Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs)",
    "order": 4,
    "orderInChapter": 4,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "cnn",
      "lstm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 2,
      "contentLength": 201
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ner/#implementations",
    "scrapedAt": "2025-12-28T11:54:15.608Z"
  },
  {
    "id": "ai-ner-neural-architectures-for-named-entity-recognition--5",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Named Entity Recognition",
    "articleSlug": "ner",
    "chapter": "Neural Sequence Labelling Models",
    "title": "Neural Architectures for Named Entity Recognition (2016)",
    "subtitle": "Neural Sequence Labelling Models",
    "contentHtml": "<h4 id=\"architecture-2\">Architecture</h4>\n<ul>\n  <li>\n    <p>This was, to the best of my knowledge, the first work on NER to completely drop hand-crafted features, i.e., they use no language-specific resources or features beyond a small amount of supervised training data and unlabeled corpora.</p>\n  </li>\n  <li>\n    <p>Two architectures are proposed:</p>\n\n    <ul>\n      <li>Bidirectional LSTMs + Conditional Random Fields (CRF)</li>\n      <li>Generating labels segments using a transition-based approach inspired by shift-reduce parsers</li>\n    </ul>\n  </li>\n  <li>\n    <p>We shall focus on the first model, which follows a similar architecture as the other models presented in this post. The biggest selling point of this model is its simplicity.</p>\n  </li>\n  <li>\n    <p>As in the previous models, two LSTMs are used to generate a word representation by concatenating its left and right context. These are two distinct LSTMs with different parameters. The tagging decisions are modeled jointly using a CRF layer by <a href=\"https://repository.upenn.edu/cgi/viewcontent.cgi?article=116\">Lafferty et al.</a> (2001). The following diagram from Lample et al. (2016) shows the model architecture.</p>\n  </li>\n</ul>\n<p>This was, to the best of my knowledge, the first work on NER to completely drop hand-crafted features, i.e., they use no language-specific resources or features beyond a small amount of supervised training data and unlabeled corpora.</p>\n<p>Two architectures are proposed:</p>\n<ul>\n      <li>Bidirectional LSTMs + Conditional Random Fields (CRF)</li>\n      <li>Generating labels segments using a transition-based approach inspired by shift-reduce parsers</li>\n    </ul>\n<p>We shall focus on the first model, which follows a similar architecture as the other models presented in this post. The biggest selling point of this model is its simplicity.</p>\n<p>As in the previous models, two LSTMs are used to generate a word representation by concatenating its left and right context. These are two distinct LSTMs with different parameters. The tagging decisions are modeled jointly using a CRF layer by <a href=\"https://repository.upenn.edu/cgi/viewcontent.cgi?article=116\">Lafferty et al.</a> (2001). The following diagram from Lample et al. (2016) shows the model architecture.</p>\n<p><img src=\"/primers/ai/assets/ner/neural-arch.png\" alt=\"\"></p>\n<h4 id=\"embeddings\">Embeddings</h4>\n<ul>\n  <li>\n    <p>The authors generate words embeddings from both representations of the characters of the word and from the contexts where the word occurs.</p>\n  </li>\n  <li>\n    <p>The rational behinds this idea is that many languages have orthographic or morphological evidence that a word or sequence of words is a named-entity or not, so they use character-level embeddings to try to capture these evidences. Secondly, named-entities appear in somewhat regular contexts in large corpora, therefore they use embeddings learned from a large corpus that are sensitive to word order.</p>\n  </li>\n</ul>\n<p>The authors generate words embeddings from both representations of the characters of the word and from the contexts where the word occurs.</p>\n<p>The rational behinds this idea is that many languages have orthographic or morphological evidence that a word or sequence of words is a named-entity or not, so they use character-level embeddings to try to capture these evidences. Secondly, named-entities appear in somewhat regular contexts in large corpora, therefore they use embeddings learned from a large corpus that are sensitive to word order.</p>\n<h5 id=\"character-embeddings\">Character Embeddings</h5>\n<ul>\n  <li>A character lookup table is initialized randomly containing an embedding for every character. The character embeddings corresponding to every character in a word are given in direct and reverse order to a bidirectional-LSTM. The embedding for a word derived from its characters is the concatenation of its forward and backward representations from the bidirectional-LSTM. The hidden dimension of the forward and backward character LSTMs are 25 each. The following diagram from Lample et al. (2016) shows the character-embeddings Architecture.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/ner/nerual-arch-char-embeddings.png\" alt=\"\"></p>\n<h5 id=\"word-embeddings\">Word Embeddings</h5>\n<ul>\n  <li>\n    <p>This character-level representation is then concatenated with a word-level representation from pre-trained word embeddings. Embeddings are pre-trained using skip-n-gram <a href=\"http://www.aclweb.org/anthology/D15-1161\">(Ling et al., 2015)</a>, a variation of skip-gram that accounts for word order.</p>\n  </li>\n  <li>\n    <p>These embeddings are fine-tuned during training, and the authors claim that using pre-trained over randomly initialized ones results in performance improvements.</p>\n  </li>\n  <li>\n    <p>They also mention that they apply a dropout mask to the final embedding layer just before the input to the bidirectional LSTM observe a significant improvement in model’s performance after using dropout.</p>\n  </li>\n</ul>\n<p>This character-level representation is then concatenated with a word-level representation from pre-trained word embeddings. Embeddings are pre-trained using skip-n-gram <a href=\"http://www.aclweb.org/anthology/D15-1161\">(Ling et al., 2015)</a>, a variation of skip-gram that accounts for word order.</p>\n<p>These embeddings are fine-tuned during training, and the authors claim that using pre-trained over randomly initialized ones results in performance improvements.</p>\n<p>They also mention that they apply a dropout mask to the final embedding layer just before the input to the bidirectional LSTM observe a significant improvement in model’s performance after using dropout.</p>",
    "contentMarkdown": "#### Architecture\n\n*   This was, to the best of my knowledge, the first work on NER to completely drop hand-crafted features, i.e., they use no language-specific resources or features beyond a small amount of supervised training data and unlabeled corpora.\n    \n*   Two architectures are proposed:\n    \n    *   Bidirectional LSTMs + Conditional Random Fields (CRF)\n    *   Generating labels segments using a transition-based approach inspired by shift-reduce parsers\n*   We shall focus on the first model, which follows a similar architecture as the other models presented in this post. The biggest selling point of this model is its simplicity.\n    \n*   As in the previous models, two LSTMs are used to generate a word representation by concatenating its left and right context. These are two distinct LSTMs with different parameters. The tagging decisions are modeled jointly using a CRF layer by [Lafferty et al.](https://repository.upenn.edu/cgi/viewcontent.cgi?article=116) (2001). The following diagram from Lample et al. (2016) shows the model architecture.\n    \n\nThis was, to the best of my knowledge, the first work on NER to completely drop hand-crafted features, i.e., they use no language-specific resources or features beyond a small amount of supervised training data and unlabeled corpora.\n\nTwo architectures are proposed:\n\n*   Bidirectional LSTMs + Conditional Random Fields (CRF)\n*   Generating labels segments using a transition-based approach inspired by shift-reduce parsers\n\nWe shall focus on the first model, which follows a similar architecture as the other models presented in this post. The biggest selling point of this model is its simplicity.\n\nAs in the previous models, two LSTMs are used to generate a word representation by concatenating its left and right context. These are two distinct LSTMs with different parameters. The tagging decisions are modeled jointly using a CRF layer by [Lafferty et al.](https://repository.upenn.edu/cgi/viewcontent.cgi?article=116) (2001). The following diagram from Lample et al. (2016) shows the model architecture.\n\n![](/primers/ai/assets/ner/neural-arch.png)\n\n#### Embeddings\n\n*   The authors generate words embeddings from both representations of the characters of the word and from the contexts where the word occurs.\n    \n*   The rational behinds this idea is that many languages have orthographic or morphological evidence that a word or sequence of words is a named-entity or not, so they use character-level embeddings to try to capture these evidences. Secondly, named-entities appear in somewhat regular contexts in large corpora, therefore they use embeddings learned from a large corpus that are sensitive to word order.\n    \n\nThe authors generate words embeddings from both representations of the characters of the word and from the contexts where the word occurs.\n\nThe rational behinds this idea is that many languages have orthographic or morphological evidence that a word or sequence of words is a named-entity or not, so they use character-level embeddings to try to capture these evidences. Secondly, named-entities appear in somewhat regular contexts in large corpora, therefore they use embeddings learned from a large corpus that are sensitive to word order.\n\n##### Character Embeddings\n\n*   A character lookup table is initialized randomly containing an embedding for every character. The character embeddings corresponding to every character in a word are given in direct and reverse order to a bidirectional-LSTM. The embedding for a word derived from its characters is the concatenation of its forward and backward representations from the bidirectional-LSTM. The hidden dimension of the forward and backward character LSTMs are 25 each. The following diagram from Lample et al. (2016) shows the character-embeddings Architecture.\n\n![](/primers/ai/assets/ner/nerual-arch-char-embeddings.png)\n\n##### Word Embeddings\n\n*   This character-level representation is then concatenated with a word-level representation from pre-trained word embeddings. Embeddings are pre-trained using skip-n-gram [(Ling et al., 2015)](http://www.aclweb.org/anthology/D15-1161), a variation of skip-gram that accounts for word order.\n    \n*   These embeddings are fine-tuned during training, and the authors claim that using pre-trained over randomly initialized ones results in performance improvements.\n    \n*   They also mention that they apply a dropout mask to the final embedding layer just before the input to the bidirectional LSTM observe a significant improvement in model’s performance after using dropout.\n    \n\nThis character-level representation is then concatenated with a word-level representation from pre-trained word embeddings. Embeddings are pre-trained using skip-n-gram [(Ling et al., 2015)](http://www.aclweb.org/anthology/D15-1161), a variation of skip-gram that accounts for word order.\n\nThese embeddings are fine-tuned during training, and the authors claim that using pre-trained over randomly initialized ones results in performance improvements.\n\nThey also mention that they apply a dropout mask to the final embedding layer just before the input to the bidirectional LSTM observe a significant improvement in model’s performance after using dropout.",
    "order": 5,
    "orderInChapter": 5,
    "difficulty": 2,
    "estimatedMinutes": 4,
    "tags": [
      "nlpllms",
      "embedding",
      "lstm",
      "dropout"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 733,
      "contentLength": 5749
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ner/#neural-architectures-for-named-entity-recognition-(2016)",
    "scrapedAt": "2025-12-28T11:54:15.608Z"
  },
  {
    "id": "ai-ner-summary-6",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Named Entity Recognition",
    "articleSlug": "ner",
    "chapter": "Neural Sequence Labelling Models",
    "title": "Summary",
    "subtitle": "Neural Sequence Labelling Models",
    "contentHtml": "<ul>\n  <li>\n    <p>This model is relatively simple, the authors use no hand-crafted features, just embeddings. The word embeddings are the concatenation of two vectors, a vector made of character embeddings using two LSTMs, for each character in a word, and a vector corresponding to word embeddings trained on external data.</p>\n  </li>\n  <li>\n    <p>The embeddings for word each word in a sentence are then passed through a forward and backward LSTM, and the output for each word is then fed into a CRF layer.</p>\n  </li>\n</ul>\n<p>This model is relatively simple, the authors use no hand-crafted features, just embeddings. The word embeddings are the concatenation of two vectors, a vector made of character embeddings using two LSTMs, for each character in a word, and a vector corresponding to word embeddings trained on external data.</p>\n<p>The embeddings for word each word in a sentence are then passed through a forward and backward LSTM, and the output for each word is then fed into a CRF layer.</p>",
    "contentMarkdown": "*   This model is relatively simple, the authors use no hand-crafted features, just embeddings. The word embeddings are the concatenation of two vectors, a vector made of character embeddings using two LSTMs, for each character in a word, and a vector corresponding to word embeddings trained on external data.\n    \n*   The embeddings for word each word in a sentence are then passed through a forward and backward LSTM, and the output for each word is then fed into a CRF layer.\n    \n\nThis model is relatively simple, the authors use no hand-crafted features, just embeddings. The word embeddings are the concatenation of two vectors, a vector made of character embeddings using two LSTMs, for each character in a word, and a vector corresponding to word embeddings trained on external data.\n\nThe embeddings for word each word in a sentence are then passed through a forward and backward LSTM, and the output for each word is then fed into a CRF layer.",
    "order": 6,
    "orderInChapter": 6,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "embedding",
      "lstm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 160,
      "contentLength": 1010
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ner/#summary",
    "scrapedAt": "2025-12-28T11:54:15.608Z"
  },
  {
    "id": "ai-ner-implementations-7",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Named Entity Recognition",
    "articleSlug": "ner",
    "chapter": "Neural Sequence Labelling Models",
    "title": "Implementations",
    "subtitle": "Neural Sequence Labelling Models",
    "contentHtml": "<ul>\n  <li><a href=\"https://github.com/glample/tagger\">https://github.com/glample/tagger</a></li>\n  <li><a href=\"https://github.com/Hironsan/anago\">https://github.com/Hironsan/anago</a></li>\n  <li><a href=\"https://github.com/achernodub/bilstm-cnn-crf-tagger\">https://github.com/achernodub/bilstm-cnn-crf-tagger</a></li>\n</ul>\n<h4 id=\"end-to-end-sequence-labelling-via-bi-directional-lstm-cnns-crf-2016\"><a href=\"http://www.aclweb.org/anthology/P16-1101\">End-to-end Sequence Labelling Via Bi-directional LSTM-CNNs-CRF (2016)</a></h4>\n<h4 id=\"architecture-3\">Architecture</h4>\n<ul>\n  <li>This system is very similar to the previous one. The authors use a Convolutional Neural Networks (CNN) to encode character-level information of a word into its character-level representation. Then combine character- and word-level representations and feed them into bidirectional LSTM to model context information of each word. Finally, the output vectors of BLSTM are fed to the CRF layer to jointly decode the best label sequence. The following diagram from Ma and Hovy (2016) shows the model architecture.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/ner/end_to_ent2.png\" alt=\"\"></p>\n<h4 id=\"embeddings-1\">Embeddings</h4>\n<h5 id=\"character-embeddings-1\">Character Embeddings</h5>\n<ul>\n  <li>The CNN is similar to the one in <a href=\"https://www.aclweb.org/anthology/Q16-1026\">Chiu and Nichols (2015)</a>, the second system presented, except that they use only character embeddings as the inputs to CNN, without any character type features. A dropout layer is applied before character embeddings are input to CNN. The following diagram from Ma and Hovy (2016) shows the character-embeddings architecture.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/ner/end_to_ent1.png\" alt=\"\"></p>\n<h5 id=\"word-embeddings-1\">Word Embeddings</h5>\n<ul>\n  <li>The word embeddings are the publicly available GloVe 100-dimensional embeddings trained on 6 billion words from Wikipedia and web text.</li>\n</ul>",
    "contentMarkdown": "*   [https://github.com/glample/tagger](https://github.com/glample/tagger)\n*   [https://github.com/Hironsan/anago](https://github.com/Hironsan/anago)\n*   [https://github.com/achernodub/bilstm-cnn-crf-tagger](https://github.com/achernodub/bilstm-cnn-crf-tagger)\n\n#### [End-to-end Sequence Labelling Via Bi-directional LSTM-CNNs-CRF (2016)](http://www.aclweb.org/anthology/P16-1101)\n\n#### Architecture\n\n*   This system is very similar to the previous one. The authors use a Convolutional Neural Networks (CNN) to encode character-level information of a word into its character-level representation. Then combine character- and word-level representations and feed them into bidirectional LSTM to model context information of each word. Finally, the output vectors of BLSTM are fed to the CRF layer to jointly decode the best label sequence. The following diagram from Ma and Hovy (2016) shows the model architecture.\n\n![](/primers/ai/assets/ner/end_to_ent2.png)\n\n#### Embeddings\n\n##### Character Embeddings\n\n*   The CNN is similar to the one in [Chiu and Nichols (2015)](https://www.aclweb.org/anthology/Q16-1026), the second system presented, except that they use only character embeddings as the inputs to CNN, without any character type features. A dropout layer is applied before character embeddings are input to CNN. The following diagram from Ma and Hovy (2016) shows the character-embeddings architecture.\n\n![](/primers/ai/assets/ner/end_to_ent1.png)\n\n##### Word Embeddings\n\n*   The word embeddings are the publicly available GloVe 100-dimensional embeddings trained on 6 billion words from Wikipedia and web text.",
    "order": 7,
    "orderInChapter": 7,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "neural network",
      "embedding",
      "convolution",
      "cnn",
      "lstm",
      "dropout"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": true,
      "wordCount": 184,
      "contentLength": 1972
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ner/#implementations",
    "scrapedAt": "2025-12-28T11:54:15.608Z"
  },
  {
    "id": "ai-ner-summary-8",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Named Entity Recognition",
    "articleSlug": "ner",
    "chapter": "Neural Sequence Labelling Models",
    "title": "Summary",
    "subtitle": "Neural Sequence Labelling Models",
    "contentHtml": "<ul>\n  <li>This model follows basically the same architecture as the one presented before, being the only architecture change the fact that they use CNN to generate word-level char-embeddings instead of an LSTM.</li>\n</ul>",
    "contentMarkdown": "*   This model follows basically the same architecture as the one presented before, being the only architecture change the fact that they use CNN to generate word-level char-embeddings instead of an LSTM.",
    "order": 8,
    "orderInChapter": 8,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "embedding",
      "cnn",
      "lstm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 32,
      "contentLength": 222
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ner/#summary",
    "scrapedAt": "2025-12-28T11:54:15.608Z"
  },
  {
    "id": "ai-ner-implementations-9",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Named Entity Recognition",
    "articleSlug": "ner",
    "chapter": "Neural Sequence Labelling Models",
    "title": "Implementations",
    "subtitle": "Neural Sequence Labelling Models",
    "contentHtml": "<ul>\n  <li><a href=\"https://github.com/achernodub/bilstm-cnn-crf-tagger\">https://github.com/achernodub/bilstm-cnn-crf-tagger</a></li>\n</ul>",
    "contentMarkdown": "*   [https://github.com/achernodub/bilstm-cnn-crf-tagger](https://github.com/achernodub/bilstm-cnn-crf-tagger)",
    "order": 9,
    "orderInChapter": 9,
    "difficulty": 2,
    "estimatedMinutes": 1,
    "tags": [
      "nlpllms",
      "cnn",
      "lstm"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 2,
      "contentLength": 139
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ner/#implementations",
    "scrapedAt": "2025-12-28T11:54:15.608Z"
  },
  {
    "id": "ai-ner-key-takeaways-10",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Named Entity Recognition",
    "articleSlug": "ner",
    "chapter": "Neural Sequence Labelling Models",
    "title": "Key Takeaways",
    "subtitle": "Neural Sequence Labelling Models",
    "contentHtml": "<ul>\n  <li>\n    <p>The main lessons learned from these papers are:</p>\n\n    <ul>\n      <li>Use two LSTMs (forward and backward)</li>\n      <li>CRF on the top/final layer to model tag transitions</li>\n      <li>Final embeddings are a combinations of word and character embeddings</li>\n    </ul>\n  </li>\n  <li>\n    <p>The following table summarizes the main characteristics of each of the models:</p>\n  </li>\n</ul>\n<p>The main lessons learned from these papers are:</p>\n<ul>\n      <li>Use two LSTMs (forward and backward)</li>\n      <li>CRF on the top/final layer to model tag transitions</li>\n      <li>Final embeddings are a combinations of word and character embeddings</li>\n    </ul>\n<p>The following table summarizes the main characteristics of each of the models:</p>\n<div align=\"center\">\n<table class=\"tg\">\n <thead>\n<tr><th class=\"tg-hcenter-valign-first\">&nbsp;</th>\n<th class=\"tg-hcenter-valign-first\">Features</th>\n<th class=\"tg-hcenter-valign-first\">Architecture Resume</th>\n<th class=\"tg-hcenter-valign-first\">Structured Tagging</th>\n<th class=\"tg-hcenter-valign-second\">Embeddings</th>\n</tr></thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Huang et al. (2015)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">\nbi-LSTM output vectors +\n<br>\nfeatures vectors connected to CRF</td>\n<td class=\"tg-tleft-valign-first\">CRF</td>\n<td class=\"tg-tleft-valign-second\">Collobert et al. (2011)\n<br>\npre-trained\n<br>\n50-dimensions</td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\">Chiu and Nichols (2016)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">\nword embeddings + features vector\n<br>\ninput to a bi-LSTM the output\n<br>\nat each time step is decoded by a\n<br>\nlinear layer and a log-softmax layer\n<br>\ninto log-probabilities for each tag category\n<br>\n</td>\n<td class=\"tg-tleft-valign-first\">\nSentence-level log-likelihood\n</td>\n<td class=\"tg-tleft-valign-second\">\n- Collobert et al. 2011\n<br>\n- char-level embeddings\n<br>\nextracted with a CNN</td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\">Lample et al. (2016)</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">\nchars and word embeddings\n<br>\ninput for the bi-LSTM\n<br>\noutput vectors are fed to the CRF layer to  jointly decode the best label sequence\n</td>\n<td class=\"tg-tleft-valign-first\">CRF</td>\n<td class=\"tg-tleft-valign-second\">\n- char-level embeddings\n<br>\nextracted with a bi-LSTM\n<br>\n- pre-trained word embeddings\n<br>\nwith skip-n-gram</td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\">Ma and Hovy (2016)</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">\nchars and word embeddings\n<br>\ninput for the bi-LSTM\n<br>\noutput vectors are fed to the CRF layer to  jointly decode the best label sequence\n</td>\n<td class=\"tg-tleft-valign-first\">CRF</td>\n<td class=\"tg-tleft-valign-second\">\n- char embeddings extracted with a CNN\n<br>\n- word embeddings: GloVe 100-dimensions</td>\n</tr>\n\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n <thead>\n<tr><th class=\"tg-hcenter-valign-first\">&nbsp;</th>\n<th class=\"tg-hcenter-valign-first\">Features</th>\n<th class=\"tg-hcenter-valign-first\">Architecture Resume</th>\n<th class=\"tg-hcenter-valign-first\">Structured Tagging</th>\n<th class=\"tg-hcenter-valign-second\">Embeddings</th>\n</tr></thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Huang et al. (2015)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">\nbi-LSTM output vectors +\n<br>\nfeatures vectors connected to CRF</td>\n<td class=\"tg-tleft-valign-first\">CRF</td>\n<td class=\"tg-tleft-valign-second\">Collobert et al. (2011)\n<br>\npre-trained\n<br>\n50-dimensions</td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\">Chiu and Nichols (2016)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">\nword embeddings + features vector\n<br>\ninput to a bi-LSTM the output\n<br>\nat each time step is decoded by a\n<br>\nlinear layer and a log-softmax layer\n<br>\ninto log-probabilities for each tag category\n<br>\n</td>\n<td class=\"tg-tleft-valign-first\">\nSentence-level log-likelihood\n</td>\n<td class=\"tg-tleft-valign-second\">\n- Collobert et al. 2011\n<br>\n- char-level embeddings\n<br>\nextracted with a CNN</td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\">Lample et al. (2016)</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">\nchars and word embeddings\n<br>\ninput for the bi-LSTM\n<br>\noutput vectors are fed to the CRF layer to  jointly decode the best label sequence\n</td>\n<td class=\"tg-tleft-valign-first\">CRF</td>\n<td class=\"tg-tleft-valign-second\">\n- char-level embeddings\n<br>\nextracted with a bi-LSTM\n<br>\n- pre-trained word embeddings\n<br>\nwith skip-n-gram</td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\">Ma and Hovy (2016)</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">\nchars and word embeddings\n<br>\ninput for the bi-LSTM\n<br>\noutput vectors are fed to the CRF layer to  jointly decode the best label sequence\n</td>\n<td class=\"tg-tleft-valign-first\">CRF</td>\n<td class=\"tg-tleft-valign-second\">\n- char embeddings extracted with a CNN\n<br>\n- word embeddings: GloVe 100-dimensions</td>\n</tr>\n\n</tbody>\n</table>\n<h4 id=\"extra-why-a-conditional-random-field-at-the-top\">Extra: Why a Conditional Random Field at the Top?</h4>\n<ul>\n  <li>\n    <p>Having independent classification decisions is limiting when there are strong dependencies across output labels, since you decide the label for a word independently from the previous given tags.</p>\n  </li>\n  <li>\n    <p>For sequence labeling or general structured prediction tasks, it is beneficial to consider the correlations between labels in neighborhoods and jointly decode the best chain of labels for a given input sentence:</p>\n\n    <ul>\n      <li>\n        <p>NER is one such task, since interpretable sequences of tags have constraints, e.g.: I-PER cannot follow B-LOC that would be impossible to model with independence assumptions;</p>\n      </li>\n      <li>\n        <p>Another example is in POS tagging, an adjective is more likely to be followed by a noun than a verb;</p>\n      </li>\n    </ul>\n  </li>\n  <li>The idea of using a CRF at the top is to model tagging decisions jointly, that is the probability of a given label to a word depends on the features associated to that word (i.e., final word embedding) and the assigned tag the word before.</li>\n  <li>This means that the CRF layer could add constrains to the final predicted labels ensuring they are valid. The constrains are learned by the CRF layer automatically based on the annotated samples during the training process.</li>\n</ul>\n<p>Having independent classification decisions is limiting when there are strong dependencies across output labels, since you decide the label for a word independently from the previous given tags.</p>\n<p>For sequence labeling or general structured prediction tasks, it is beneficial to consider the correlations between labels in neighborhoods and jointly decode the best chain of labels for a given input sentence:</p>\n<ul>\n      <li>\n        <p>NER is one such task, since interpretable sequences of tags have constraints, e.g.: I-PER cannot follow B-LOC that would be impossible to model with independence assumptions;</p>\n      </li>\n      <li>\n        <p>Another example is in POS tagging, an adjective is more likely to be followed by a noun than a verb;</p>\n      </li>\n    </ul>\n<p>NER is one such task, since interpretable sequences of tags have constraints, e.g.: I-PER cannot follow B-LOC that would be impossible to model with independence assumptions;</p>\n<p>Another example is in POS tagging, an adjective is more likely to be followed by a noun than a verb;</p>\n<h5 id=\"emission-score-matrix\">Emission Score Matrix</h5>\n<ul>\n  <li>The output of the LSTM is given as input to the CRF layer, that is, a matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>P</mtext></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-51\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-52\"><span class=\"texatom\" id=\"MathJax-Span-53\"><span class=\"mrow\" id=\"MathJax-Span-54\"><span class=\"mtext\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Regular;\">P</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>P</mtext></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">\\textrm{P}</script> with the scores of the LSTM of size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi><mo>&amp;#x00D7;</mo><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-56\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-57\"><span class=\"mi\" id=\"MathJax-Span-58\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi><mo>×</mo><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">n \\times k</script>, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-61\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-62\"><span class=\"mi\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">n</script> is the number of words in the sentence and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-64\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-65\"><span class=\"mi\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">k</script> is the possible number of labels that each word can have, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>P</mtext></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-67\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-68\"><span class=\"msubsup\" id=\"MathJax-Span-69\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-70\"><span class=\"mrow\" id=\"MathJax-Span-71\"><span class=\"mtext\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Regular;\">P</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-73\"><span class=\"mrow\" id=\"MathJax-Span-74\"><span class=\"mi\" id=\"MathJax-Span-75\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-76\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mtext>P</mtext></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">\\textrm{P}_{i,j}</script> is the score of the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>j</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mi>h</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-78\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1000.99em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-79\"><span class=\"msubsup\" id=\"MathJax-Span-80\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-81\" style=\"font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-82\"><span class=\"mrow\" id=\"MathJax-Span-83\"><span class=\"mi\" id=\"MathJax-Span-84\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-85\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>j</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mi>h</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">j^{th}</script> tag of the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>i</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mi>h</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-86\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-87\"><span class=\"msubsup\" id=\"MathJax-Span-88\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-90\"><span class=\"mrow\" id=\"MathJax-Span-91\"><span class=\"mi\" id=\"MathJax-Span-92\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-93\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">h</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>i</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mi>h</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">i^{th}</script> word in the sentence. In the image below the matrix would be the concatenation of the yellow blocks coming out of each LSTM. The following diagram from https://createmomo.github.io shows the CRF Input Matrix.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/ner/LSTM_CRF_matrix.png\" alt=\"\"></p>\n<h5 id=\"transition-matrix\">Transition Matrix</h5>\n<ul>\n  <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>T</mtext></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-94\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-95\"><span class=\"texatom\" id=\"MathJax-Span-96\"><span class=\"mrow\" id=\"MathJax-Span-97\"><span class=\"mtext\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Regular;\">T</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>T</mtext></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">\\textrm{T}</script> is a matrix of transition scores such that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>P</mtext></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-99\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-100\"><span class=\"msubsup\" id=\"MathJax-Span-101\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-102\"><span class=\"mrow\" id=\"MathJax-Span-103\"><span class=\"mtext\" id=\"MathJax-Span-104\" style=\"font-family: STIXGeneral-Regular;\">P</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-105\"><span class=\"mrow\" id=\"MathJax-Span-106\"><span class=\"mi\" id=\"MathJax-Span-107\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-108\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-109\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mtext>P</mtext></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">\\textrm{P}_{i,j}</script> represents the score of a transition from the tag <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-110\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-111\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">i</script> to tag <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>j</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-113\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-114\"><span class=\"mi\" id=\"MathJax-Span-115\" style=\"font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>j</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">j</script>. Two extra tags are added, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>0</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-116\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.89em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-117\"><span class=\"msubsup\" id=\"MathJax-Span-118\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-119\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-120\"><span class=\"mrow\" id=\"MathJax-Span-121\"><span class=\"mn\" id=\"MathJax-Span-122\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>0</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">y_{0}</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-123\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.89em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-124\"><span class=\"msubsup\" id=\"MathJax-Span-125\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-126\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-127\"><span class=\"mrow\" id=\"MathJax-Span-128\"><span class=\"mi\" id=\"MathJax-Span-129\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">y_{n}</script> are the <em>start</em> and <em>end</em> tags of a sentence, that we add to the set of possible tags, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>T</mtext></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-130\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-131\"><span class=\"texatom\" id=\"MathJax-Span-132\"><span class=\"mrow\" id=\"MathJax-Span-133\"><span class=\"mtext\" id=\"MathJax-Span-134\" style=\"font-family: STIXGeneral-Regular;\">T</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>T</mtext></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">\\textrm{T}</script> is therefore a square matrix of size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>k</mtext></mrow><mo>+</mo><mn>2</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-135\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.398em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-136\"><span class=\"texatom\" id=\"MathJax-Span-137\"><span class=\"mrow\" id=\"MathJax-Span-138\"><span class=\"mtext\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Regular;\">k</span></span></span><span class=\"mo\" id=\"MathJax-Span-140\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mn\" id=\"MathJax-Span-141\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">2</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>k</mtext></mrow><mo>+</mo><mn>2</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">\\textrm{k}+2</script>. The following diagram from https://eli5.readthedocs.io shows the CRF State Transition Matrix.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/ner/transition_matrix.png\" alt=\"\"></p>\n<h5 id=\"score-of-a-prediction\">Score of a Prediction</h5>\n<ul>\n  <li>\n    <p>For a given sequence of predictions for a sequence of words <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-142\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-143\"><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">x</script>…</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>y</mtext></mrow><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-145\" style=\"width: 9.169em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1007.55em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-146\"><span class=\"texatom\" id=\"MathJax-Span-147\"><span class=\"mrow\" id=\"MathJax-Span-148\"><span class=\"mtext\" id=\"MathJax-Span-149\" style=\"font-family: STIXGeneral-Regular;\">y</span></span></span><span class=\"mo\" id=\"MathJax-Span-150\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-152\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-154\"><span class=\"mrow\" id=\"MathJax-Span-155\"><span class=\"mn\" id=\"MathJax-Span-156\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-157\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-158\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-159\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-160\"><span class=\"mrow\" id=\"MathJax-Span-161\"><span class=\"mn\" id=\"MathJax-Span-162\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-163\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-164\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-165\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-166\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-167\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-168\"><span class=\"mrow\" id=\"MathJax-Span-169\"><span class=\"mi\" id=\"MathJax-Span-170\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>y</mtext></mrow><mo>=</mo><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-27\">\\textrm{y} = (y_{1},y_{2},\\dots,y_{n})</script>\n  </li>\n  <li>\n    <p>…we can compute it’s score based on the <em>emission</em> and <em>transition</em> matrices:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>score</mtext></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>T</mtext></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></msub><mo>+</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>P</mtext></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-172\" style=\"width: 15.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.555em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.419em, 1012.55em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-173\"><span class=\"texatom\" id=\"MathJax-Span-174\"><span class=\"mrow\" id=\"MathJax-Span-175\"><span class=\"mtext\" id=\"MathJax-Span-176\" style=\"font-family: STIXGeneral-Regular;\">score</span></span></span><span class=\"mo\" id=\"MathJax-Span-177\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"mo\" id=\"MathJax-Span-179\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-180\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-181\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-182\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.04em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-183\"><span class=\"mrow\" id=\"MathJax-Span-184\"><span class=\"mi\" id=\"MathJax-Span-185\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-186\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-187\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-188\"><span class=\"mrow\" id=\"MathJax-Span-189\"><span class=\"mi\" id=\"MathJax-Span-190\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-191\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.451em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-192\"><span class=\"mrow\" id=\"MathJax-Span-193\"><span class=\"mtext\" id=\"MathJax-Span-194\" style=\"font-family: STIXGeneral-Regular;\">T</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-195\"><span class=\"mrow\" id=\"MathJax-Span-196\"><span class=\"msubsup\" id=\"MathJax-Span-197\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-198\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-199\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-200\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-201\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-202\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-203\"><span class=\"mrow\" id=\"MathJax-Span-204\"><span class=\"mi\" id=\"MathJax-Span-205\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-206\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-207\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-208\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"munderover\" id=\"MathJax-Span-209\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-210\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-211\"><span class=\"mrow\" id=\"MathJax-Span-212\"><span class=\"mi\" id=\"MathJax-Span-213\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-214\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-215\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-216\"><span class=\"mrow\" id=\"MathJax-Span-217\"><span class=\"mi\" id=\"MathJax-Span-218\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-219\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-220\"><span class=\"mrow\" id=\"MathJax-Span-221\"><span class=\"mtext\" id=\"MathJax-Span-222\" style=\"font-family: STIXGeneral-Regular;\">P</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-223\"><span class=\"mrow\" id=\"MathJax-Span-224\"><span class=\"mi\" id=\"MathJax-Span-225\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-226\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-227\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-228\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-229\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>score</mtext></mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msub><mrow class=\"MJX-TeXAtom-ORD\"><mtext>T</mtext></mrow><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></msub><mo>+</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msub><mrow class=\"MJX-TeXAtom-ORD\"><mtext>P</mtext></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-28\">\\textrm{score}(y) = \\sum_{i=0}^{n} \\textrm{T}_{y_i,y_{i+1}} + \\sum_{i=1}^{n} \\textrm{P}_{i,y_i}</script>\n  </li>\n  <li>\n    <p>So the score of a sequence of predictions is, for each word, the sum of the transition from the current assigned tag <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-230\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-231\"><span class=\"msubsup\" id=\"MathJax-Span-232\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-233\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-234\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">y_i</script> to next assigned tag <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-235\" style=\"width: 1.878em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.57em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-236\"><span class=\"msubsup\" id=\"MathJax-Span-237\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-238\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-239\"><span class=\"mrow\" id=\"MathJax-Span-240\"><span class=\"mi\" id=\"MathJax-Span-241\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-242\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-243\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">y_{i+1}</script> plus the probability given by the LSTM to the tag assigned for the current word <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-244\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-245\"><span class=\"mi\" id=\"MathJax-Span-246\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">i</script>.</p>\n  </li>\n</ul>\n<p>For a given sequence of predictions for a sequence of words <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-142\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-143\"><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">x</script>…</p>\n<p>…we can compute it’s score based on the <em>emission</em> and <em>transition</em> matrices:</p>\n<p>So the score of a sequence of predictions is, for each word, the sum of the transition from the current assigned tag <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-230\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-231\"><span class=\"msubsup\" id=\"MathJax-Span-232\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-233\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-234\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">y_i</script> to next assigned tag <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-235\" style=\"width: 1.878em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.57em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-236\"><span class=\"msubsup\" id=\"MathJax-Span-237\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-238\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-239\"><span class=\"mrow\" id=\"MathJax-Span-240\"><span class=\"mi\" id=\"MathJax-Span-241\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-242\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-243\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">y_{i+1}</script> plus the probability given by the LSTM to the tag assigned for the current word <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-244\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-245\"><span class=\"mi\" id=\"MathJax-Span-246\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">i</script>.</p>\n<h5 id=\"training-parameter-estimation\">Training: Parameter Estimation</h5>\n<ul>\n  <li>\n    <p>During training, we assign a probability to each tag but maximizing the probability of the correct tag <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-247\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-248\"><span class=\"mi\" id=\"MathJax-Span-249\" style=\"font-family: STIXGeneral-Italic;\">y</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">y</script> sequence among all the other possible tag sequences.</p>\n  </li>\n  <li>\n    <p>This is modeled by applying a softmax over all the possible taggings <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-250\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-251\"><span class=\"mi\" id=\"MathJax-Span-252\" style=\"font-family: STIXGeneral-Italic;\">y</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">y</script>:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>p(y|X)</mtext></mrow><mo>=</mo><mfrac><msup><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mrow><munder><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msup><mi>y</mi><mo>&amp;#x2032;</mo></msup><mo>&amp;#x2208;</mo><mi>Y</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>x</mi></mrow><mo stretchy=&quot;false&quot;>)</mo></mrow></munder><msup><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo>,</mo><msup><mi>y</mi><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-253\" style=\"width: 12.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.419em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.836em, 1010.42em, 4.586em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-254\"><span class=\"texatom\" id=\"MathJax-Span-255\"><span class=\"mrow\" id=\"MathJax-Span-256\"><span class=\"mtext\" id=\"MathJax-Span-257\" style=\"font-family: STIXGeneral-Regular;\">p(y<span style=\"font-family: STIXVariants; font-style: normal; font-weight: normal;\">|</span>X)</span></span></span><span class=\"mo\" id=\"MathJax-Span-258\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-259\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 6.305em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.023em, 1003.49em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -1.768em;\"><span class=\"msubsup\" id=\"MathJax-Span-260\"><span style=\"display: inline-block; position: relative; width: 3.492em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-261\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-262\"><span class=\"mrow\" id=\"MathJax-Span-263\"><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">s</span><span class=\"mi\" id=\"MathJax-Span-265\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-266\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">o</span><span class=\"mi\" id=\"MathJax-Span-267\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-268\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mo\" id=\"MathJax-Span-269\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-270\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-271\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-272\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span class=\"mo\" id=\"MathJax-Span-273\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.076em, 1006.2em, 5.367em, -999.997em); top: -3.279em; left: 50%; margin-left: -3.07em;\"><span class=\"mrow\" id=\"MathJax-Span-274\"><span class=\"munderover\" id=\"MathJax-Span-275\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0.68em;\"><span class=\"mo\" id=\"MathJax-Span-276\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1002.24em, 4.43em, -999.997em); top: -3.07em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-277\"><span class=\"mrow\" id=\"MathJax-Span-278\"><span class=\"msup\" id=\"MathJax-Span-279\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-280\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.216em; left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-281\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-282\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-283\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-284\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-285\"><span class=\"mrow\" id=\"MathJax-Span-286\"><span class=\"mi\" id=\"MathJax-Span-287\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-288\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-289\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-290\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-291\"><span class=\"mrow\" id=\"MathJax-Span-292\"><span class=\"mi\" id=\"MathJax-Span-293\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">s</span><span class=\"mi\" id=\"MathJax-Span-294\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-295\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">o</span><span class=\"mi\" id=\"MathJax-Span-296\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-297\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mo\" id=\"MathJax-Span-298\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-299\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-300\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"msup\" id=\"MathJax-Span-301\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-302\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.216em; left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-303\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-304\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1006.3em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 6.305em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.372em; border-left: 0px solid; width: 0px; height: 4.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>p(y|X)</mtext></mrow><mo>=</mo><mfrac><msup><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></msup><mrow><munder><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><msup><mi>y</mi><mo>′</mo></msup><mo>∈</mo><mi>Y</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></munder><msup><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><msup><mi>y</mi><mo>′</mo></msup><mo stretchy=\"false\">)</mo></mrow></msup></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-34\">\\textrm{p(y|X)} = \\frac{e^{score(X,y)}}{\\sum\\limits_{y' \\in Y({x})} e^{score(X,y')}}</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-305\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.88em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-306\"><span class=\"mi\" id=\"MathJax-Span-307\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-308\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-309\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-310\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">Y(x)</script> denotes the set of all possible label sequences for <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-311\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-312\"><span class=\"mi\" id=\"MathJax-Span-313\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">x</script>, this denominator is also known as the partition function. So, finding the best sequence is the equivalent of finding the sequence that maximizes <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>score(X,y)</mtext></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-314\" style=\"width: 5.159em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.273em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.22em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-315\"><span class=\"texatom\" id=\"MathJax-Span-316\"><span class=\"mrow\" id=\"MathJax-Span-317\"><span class=\"mtext\" id=\"MathJax-Span-318\" style=\"font-family: STIXGeneral-Regular;\">score(X,y)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>score(X,y)</mtext></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">\\textrm{score(X,y)}</script>.</li>\n    </ul>\n  </li>\n  <li>\n    <p>The loss can be defined as the negative log likelihood of the current tagging <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-319\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-320\"><span class=\"mi\" id=\"MathJax-Span-321\" style=\"font-family: STIXGeneral-Italic;\">y</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">y</script>:</p>\n  </li>\n</ul>\n<p>During training, we assign a probability to each tag but maximizing the probability of the correct tag <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-247\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-248\"><span class=\"mi\" id=\"MathJax-Span-249\" style=\"font-family: STIXGeneral-Italic;\">y</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">y</script> sequence among all the other possible tag sequences.</p>\n<p>This is modeled by applying a softmax over all the possible taggings <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-250\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-251\"><span class=\"mi\" id=\"MathJax-Span-252\" style=\"font-family: STIXGeneral-Italic;\">y</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">y</script>:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-305\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.88em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-306\"><span class=\"mi\" id=\"MathJax-Span-307\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-308\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-309\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-310\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">Y(x)</script> denotes the set of all possible label sequences for <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-311\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-312\"><span class=\"mi\" id=\"MathJax-Span-313\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">x</script>, this denominator is also known as the partition function. So, finding the best sequence is the equivalent of finding the sequence that maximizes <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>score(X,y)</mtext></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-314\" style=\"width: 5.159em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.273em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.22em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-315\"><span class=\"texatom\" id=\"MathJax-Span-316\"><span class=\"mrow\" id=\"MathJax-Span-317\"><span class=\"mtext\" id=\"MathJax-Span-318\" style=\"font-family: STIXGeneral-Regular;\">score(X,y)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>score(X,y)</mtext></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">\\textrm{score(X,y)}</script>.</li>\n    </ul>\n<p>The loss can be defined as the negative log likelihood of the current tagging <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-319\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-320\"><span class=\"mi\" id=\"MathJax-Span-321\" style=\"font-family: STIXGeneral-Italic;\">y</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">y</script>:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>-log p</mtext></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>|X)</mtext></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-322\" style=\"width: 5.523em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.586em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1004.53em, 2.867em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-323\"><span class=\"texatom\" id=\"MathJax-Span-324\"><span class=\"mrow\" id=\"MathJax-Span-325\"><span class=\"mtext\" id=\"MathJax-Span-326\" style=\"font-family: STIXGeneral-Regular;\">-log p</span></span></span><span class=\"mo\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-328\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"texatom\" id=\"MathJax-Span-329\"><span class=\"mrow\" id=\"MathJax-Span-330\"><span class=\"mtext\" id=\"MathJax-Span-331\" style=\"font-family: STIXVariants;\">|<span style=\"font-family: STIXGeneral-Regular; font-style: normal; font-weight: normal;\">X)</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>-log p</mtext></mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>|X)</mtext></mrow></math></span></span></div>\n<ul>\n  <li>So, in simplifying the function above, a first step is to get rid of the fraction using log equivalences, and then get rid of the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>log</mtext></mrow><mtext>&amp;#xA0;</mtext><mi>e</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-332\" style=\"width: 2.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.982em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.93em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-333\"><span class=\"texatom\" id=\"MathJax-Span-334\"><span class=\"mrow\" id=\"MathJax-Span-335\"><span class=\"mtext\" id=\"MathJax-Span-336\" style=\"font-family: STIXGeneral-Regular;\">log</span></span></span><span class=\"mtext\" id=\"MathJax-Span-337\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-338\" style=\"font-family: STIXGeneral-Italic;\">e</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>log</mtext></mrow><mtext>&nbsp;</mtext><mi>e</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">\\textrm{log}\\  e</script> in the first term since they cancel each other out:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>-log p</mtext></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>|X)</mtext></mrow><mo>=</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>score(X,y)</mtext></mrow><mo>+</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>log</mtext></mrow><munder><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msup><mi>y</mi><mo>&amp;#x2032;</mo></msup><mo>&amp;#x2208;</mo><mi>Y</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>x</mi></mrow><mo stretchy=&quot;false&quot;>)</mo></mrow></munder><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>exp</mtext></mrow><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>score(X,y')</mtext></mrow><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-339\" style=\"width: 26.878em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 22.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1022.35em, 4.117em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-340\"><span class=\"texatom\" id=\"MathJax-Span-341\"><span class=\"mrow\" id=\"MathJax-Span-342\"><span class=\"mtext\" id=\"MathJax-Span-343\" style=\"font-family: STIXGeneral-Regular;\">-log p</span></span></span><span class=\"mo\" id=\"MathJax-Span-344\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-345\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"texatom\" id=\"MathJax-Span-346\"><span class=\"mrow\" id=\"MathJax-Span-347\"><span class=\"mtext\" id=\"MathJax-Span-348\" style=\"font-family: STIXVariants;\">|<span style=\"font-family: STIXGeneral-Regular; font-style: normal; font-weight: normal;\">X)</span></span></span></span><span class=\"mo\" id=\"MathJax-Span-349\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-350\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"mtext\" id=\"MathJax-Span-351\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"texatom\" id=\"MathJax-Span-352\"><span class=\"mrow\" id=\"MathJax-Span-353\"><span class=\"mtext\" id=\"MathJax-Span-354\" style=\"font-family: STIXGeneral-Regular;\">score(X,y)</span></span></span><span class=\"mo\" id=\"MathJax-Span-355\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"texatom\" id=\"MathJax-Span-356\" style=\"padding-left: 0.263em;\"><span class=\"mrow\" id=\"MathJax-Span-357\"><span class=\"mtext\" id=\"MathJax-Span-358\" style=\"font-family: STIXGeneral-Regular;\">log</span></span></span><span class=\"munderover\" id=\"MathJax-Span-359\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0.471em;\"><span class=\"mo\" id=\"MathJax-Span-360\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.336em, 1002.24em, 4.43em, -999.997em); top: -2.81em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-361\"><span class=\"mrow\" id=\"MathJax-Span-362\"><span class=\"msup\" id=\"MathJax-Span-363\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-364\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-365\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-366\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-367\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-368\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-369\"><span class=\"mrow\" id=\"MathJax-Span-370\"><span class=\"mi\" id=\"MathJax-Span-371\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-372\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-373\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-374\"><span class=\"mtext\" id=\"MathJax-Span-375\" style=\"font-family: STIXGeneral-Regular;\">exp</span></span></span><span class=\"mo\" id=\"MathJax-Span-376\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-377\"><span class=\"mrow\" id=\"MathJax-Span-378\"><span class=\"mtext\" id=\"MathJax-Span-379\" style=\"font-family: STIXGeneral-Regular;\">score(X,y')</span></span></span><span class=\"mo\" id=\"MathJax-Span-380\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.809em; border-left: 0px solid; width: 0px; height: 3.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>-log p</mtext></mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>|X)</mtext></mrow><mo>=</mo><mo>−</mo><mtext>&nbsp;</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mtext>score(X,y)</mtext></mrow><mo>+</mo><mrow class=\"MJX-TeXAtom-ORD\"><mtext>log</mtext></mrow><munder><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><msup><mi>y</mi><mo>′</mo></msup><mo>∈</mo><mi>Y</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></munder><mrow class=\"MJX-TeXAtom-ORD\"><mtext>exp</mtext></mrow><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mtext>score(X,y')</mtext></mrow><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>Then the second term can be simplified by applying the log-space addition <em>logadd</em>, equivalence, i.e.: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x2295;</mo><mo stretchy=&quot;false&quot;>(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>e</mi><mi>a</mi></msup><mo>+</mo><msup><mi>e</mi><mi>b</mi></msup><mo>+</mo><msup><mi>e</mi><mi>c</mi></msup><mo>+</mo><msup><mi>e</mi><mi>d</mi></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-381\" style=\"width: 17.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.638em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1014.59em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-382\"><span class=\"mo\" id=\"MathJax-Span-383\" style=\"font-family: STIXGeneral-Regular;\">⊕</span><span class=\"mo\" id=\"MathJax-Span-384\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-385\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mo\" id=\"MathJax-Span-386\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-387\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">b</span><span class=\"mo\" id=\"MathJax-Span-388\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-389\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">c</span><span class=\"mo\" id=\"MathJax-Span-390\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-391\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-392\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-393\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-394\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-395\" style=\"font-family: STIXGeneral-Italic;\">o</span><span class=\"mi\" id=\"MathJax-Span-396\" style=\"font-family: STIXGeneral-Italic;\">g</span><span class=\"mo\" id=\"MathJax-Span-397\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-398\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-399\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-400\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-401\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-402\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-403\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-404\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-405\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-406\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-407\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-408\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-409\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-410\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-411\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-412\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-413\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>⊕</mo><mo stretchy=\"false\">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>d</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msup><mi>e</mi><mi>a</mi></msup><mo>+</mo><msup><mi>e</mi><mi>b</mi></msup><mo>+</mo><msup><mi>e</mi><mi>c</mi></msup><mo>+</mo><msup><mi>e</mi><mi>d</mi></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-42\">\\oplus(a, b, c, d) = log(e^a+e^b+e^c+e^d)</script>:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>-log p</mtext></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>|X)</mtext></mrow><mo>=</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>score(X,y)</mtext></mrow><mo>+</mo><munder><mtext>logadd</mtext><mrow><msup><mi>y</mi><mo>&amp;#x2032;</mo></msup><mo>&amp;#x2208;</mo><mi>Y</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>x</mi></mrow><mo stretchy=&quot;false&quot;>)</mo></mrow></munder><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>score(X,y')</mtext></mrow><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-414\" style=\"width: 23.701em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 19.742em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1019.69em, 3.857em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-415\"><span class=\"texatom\" id=\"MathJax-Span-416\"><span class=\"mrow\" id=\"MathJax-Span-417\"><span class=\"mtext\" id=\"MathJax-Span-418\" style=\"font-family: STIXGeneral-Regular;\">-log p</span></span></span><span class=\"mo\" id=\"MathJax-Span-419\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-420\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"texatom\" id=\"MathJax-Span-421\"><span class=\"mrow\" id=\"MathJax-Span-422\"><span class=\"mtext\" id=\"MathJax-Span-423\" style=\"font-family: STIXVariants;\">|<span style=\"font-family: STIXGeneral-Regular; font-style: normal; font-weight: normal;\">X)</span></span></span></span><span class=\"mo\" id=\"MathJax-Span-424\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-425\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"mtext\" id=\"MathJax-Span-426\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"texatom\" id=\"MathJax-Span-427\"><span class=\"mrow\" id=\"MathJax-Span-428\"><span class=\"mtext\" id=\"MathJax-Span-429\" style=\"font-family: STIXGeneral-Regular;\">score(X,y)</span></span></span><span class=\"mo\" id=\"MathJax-Span-430\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"munder\" id=\"MathJax-Span-431\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1002.71em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mtext\" id=\"MathJax-Span-432\" style=\"font-family: STIXGeneral-Regular;\">logadd</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.336em, 1002.24em, 4.43em, -999.997em); top: -3.07em; left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-433\"><span class=\"msup\" id=\"MathJax-Span-434\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-435\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-436\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-437\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-438\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-439\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-440\"><span class=\"mrow\" id=\"MathJax-Span-441\"><span class=\"mi\" id=\"MathJax-Span-442\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-443\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-444\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-445\"><span class=\"mrow\" id=\"MathJax-Span-446\"><span class=\"mtext\" id=\"MathJax-Span-447\" style=\"font-family: STIXGeneral-Regular;\">score(X,y')</span></span></span><span class=\"mo\" id=\"MathJax-Span-448\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.497em; border-left: 0px solid; width: 0px; height: 2.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>-log p</mtext></mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>|X)</mtext></mrow><mo>=</mo><mo>−</mo><mtext>&nbsp;</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mtext>score(X,y)</mtext></mrow><mo>+</mo><munder><mtext>logadd</mtext><mrow><msup><mi>y</mi><mo>′</mo></msup><mo>∈</mo><mi>Y</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></munder><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mtext>score(X,y')</mtext></mrow><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>Then, replacing the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>score</mtext></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-449\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1002.14em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-450\"><span class=\"texatom\" id=\"MathJax-Span-451\"><span class=\"mrow\" id=\"MathJax-Span-452\"><span class=\"mtext\" id=\"MathJax-Span-453\" style=\"font-family: STIXGeneral-Regular;\">score</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>score</mtext></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">\\textrm{score}</script> by it’s definition:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo>=</mo><mo>&amp;#x2212;</mo><mo stretchy=&quot;false&quot;>(</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>T</mtext></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></msub><mo>+</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>P</mtext></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><munder><mtext>logadd</mtext><mrow><msup><mi>y</mi><mo>&amp;#x2032;</mo></msup><mo>&amp;#x2208;</mo><mi>Y</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>x</mi></mrow><mo stretchy=&quot;false&quot;>)</mo></mrow></munder><mo stretchy=&quot;false&quot;>(</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>T</mtext></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msubsup><mi>y</mi><mi>i</mi><mo>&amp;#x2032;</mo></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo>&amp;#x2032;</mo></msubsup></mrow></msub><mo>+</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>P</mtext></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-454\" style=\"width: 27.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 22.815em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.419em, 1022.76em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-455\"><span class=\"mo\" id=\"MathJax-Span-456\" style=\"font-family: STIXGeneral-Regular;\">=</span><span class=\"mo\" id=\"MathJax-Span-457\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"mo\" id=\"MathJax-Span-458\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"munderover\" id=\"MathJax-Span-459\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-460\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.04em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-461\"><span class=\"mrow\" id=\"MathJax-Span-462\"><span class=\"mi\" id=\"MathJax-Span-463\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-464\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-465\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-466\"><span class=\"mrow\" id=\"MathJax-Span-467\"><span class=\"mi\" id=\"MathJax-Span-468\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-469\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.451em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-470\"><span class=\"mrow\" id=\"MathJax-Span-471\"><span class=\"mtext\" id=\"MathJax-Span-472\" style=\"font-family: STIXGeneral-Regular;\">T</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-473\"><span class=\"mrow\" id=\"MathJax-Span-474\"><span class=\"msubsup\" id=\"MathJax-Span-475\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-476\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-477\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-478\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-479\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-480\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-481\"><span class=\"mrow\" id=\"MathJax-Span-482\"><span class=\"mi\" id=\"MathJax-Span-483\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-484\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-485\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-486\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"munderover\" id=\"MathJax-Span-487\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-488\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-489\"><span class=\"mrow\" id=\"MathJax-Span-490\"><span class=\"mi\" id=\"MathJax-Span-491\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-492\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-493\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-494\"><span class=\"mrow\" id=\"MathJax-Span-495\"><span class=\"mi\" id=\"MathJax-Span-496\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-497\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-498\"><span class=\"mrow\" id=\"MathJax-Span-499\"><span class=\"mtext\" id=\"MathJax-Span-500\" style=\"font-family: STIXGeneral-Regular;\">P</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-501\"><span class=\"mrow\" id=\"MathJax-Span-502\"><span class=\"mi\" id=\"MathJax-Span-503\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-504\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-505\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-506\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-507\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-508\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-509\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"munder\" id=\"MathJax-Span-510\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1002.71em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mtext\" id=\"MathJax-Span-511\" style=\"font-family: STIXGeneral-Regular;\">logadd</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.336em, 1002.24em, 4.43em, -999.997em); top: -3.07em; left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-512\"><span class=\"msup\" id=\"MathJax-Span-513\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-514\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-515\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-516\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-517\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-518\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-519\"><span class=\"mrow\" id=\"MathJax-Span-520\"><span class=\"mi\" id=\"MathJax-Span-521\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-522\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-523\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"munderover\" id=\"MathJax-Span-524\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-525\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.04em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-526\"><span class=\"mrow\" id=\"MathJax-Span-527\"><span class=\"mi\" id=\"MathJax-Span-528\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-529\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-530\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-531\"><span class=\"mrow\" id=\"MathJax-Span-532\"><span class=\"mi\" id=\"MathJax-Span-533\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-534\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-535\"><span class=\"mrow\" id=\"MathJax-Span-536\"><span class=\"mtext\" id=\"MathJax-Span-537\" style=\"font-family: STIXGeneral-Regular;\">T</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-538\"><span class=\"mrow\" id=\"MathJax-Span-539\"><span class=\"msubsup\" id=\"MathJax-Span-540\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-541\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.596em, 1000.21em, 4.169em, -999.997em); top: -4.216em; left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-542\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.21em, 4.169em, -999.997em); top: -3.799em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-543\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-544\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-545\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-546\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.596em, 1000.21em, 4.169em, -999.997em); top: -4.216em; left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-547\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.78em, 4.169em, -999.997em); top: -3.799em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-548\"><span class=\"mrow\" id=\"MathJax-Span-549\"><span class=\"mi\" id=\"MathJax-Span-550\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-551\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-552\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-553\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"munderover\" id=\"MathJax-Span-554\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-555\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-556\"><span class=\"mrow\" id=\"MathJax-Span-557\"><span class=\"mi\" id=\"MathJax-Span-558\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-559\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-560\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-561\"><span class=\"mrow\" id=\"MathJax-Span-562\"><span class=\"mi\" id=\"MathJax-Span-563\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-564\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-565\"><span class=\"mrow\" id=\"MathJax-Span-566\"><span class=\"mtext\" id=\"MathJax-Span-567\" style=\"font-family: STIXGeneral-Regular;\">P</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-568\"><span class=\"mrow\" id=\"MathJax-Span-569\"><span class=\"mi\" id=\"MathJax-Span-570\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-571\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-572\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-573\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-574\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-575\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mo>=</mo><mo>−</mo><mo stretchy=\"false\">(</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msub><mrow class=\"MJX-TeXAtom-ORD\"><mtext>T</mtext></mrow><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></msub><mo>+</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msub><mrow class=\"MJX-TeXAtom-ORD\"><mtext>P</mtext></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mo stretchy=\"false\">)</mo><mo>+</mo><munder><mtext>logadd</mtext><mrow><msup><mi>y</mi><mo>′</mo></msup><mo>∈</mo><mi>Y</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>x</mi></mrow><mo stretchy=\"false\">)</mo></mrow></munder><mo stretchy=\"false\">(</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msub><mrow class=\"MJX-TeXAtom-ORD\"><mtext>T</mtext></mrow><mrow class=\"MJX-TeXAtom-ORD\"><msubsup><mi>y</mi><mi>i</mi><mo>′</mo></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo>′</mo></msubsup></mrow></msub><mo>+</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msub><mrow class=\"MJX-TeXAtom-ORD\"><mtext>P</mtext></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>\n    <p>The first term is score for the true data. Computing the second term might be computationally expensive since it requires summing over the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-576\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-577\"><span class=\"msubsup\" id=\"MathJax-Span-578\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-579\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-580\"><span class=\"mrow\" id=\"MathJax-Span-581\"><span class=\"mi\" id=\"MathJax-Span-582\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">k^{n}</script> different sequences in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-583\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.88em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-584\"><span class=\"mi\" id=\"MathJax-Span-585\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-586\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-587\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-588\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">Y(x)</script>, i.e., the set of all possible label sequences for <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-589\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-590\"><span class=\"mi\" id=\"MathJax-Span-591\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">x</script>. This computation can be solved using a variant of the Viterbi algorithm, the forward algorithm.</p>\n  </li>\n  <li>\n    <p>The gradients are then computed using back-propagation, since the CRF is inside the neural-network. Note that the transition scores in the matrix are randomly initialized - or can also bee initialized based on some criteria, to speed up training - and will be updated automatically during your training process.</p>\n  </li>\n</ul>\n<p>The first term is score for the true data. Computing the second term might be computationally expensive since it requires summing over the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-576\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-577\"><span class=\"msubsup\" id=\"MathJax-Span-578\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-579\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-580\"><span class=\"mrow\" id=\"MathJax-Span-581\"><span class=\"mi\" id=\"MathJax-Span-582\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">k^{n}</script> different sequences in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-583\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.88em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-584\"><span class=\"mi\" id=\"MathJax-Span-585\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-586\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-587\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-588\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">Y(x)</script>, i.e., the set of all possible label sequences for <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-589\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-590\"><span class=\"mi\" id=\"MathJax-Span-591\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">x</script>. This computation can be solved using a variant of the Viterbi algorithm, the forward algorithm.</p>\n<p>The gradients are then computed using back-propagation, since the CRF is inside the neural-network. Note that the transition scores in the matrix are randomly initialized - or can also bee initialized based on some criteria, to speed up training - and will be updated automatically during your training process.</p>\n<h5 id=\"inference-determining-the-most-likely-label-sequence-y-given-x\">Inference: Determining the Most Likely Label Sequence <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-592\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-593\"><span class=\"mi\" id=\"MathJax-Span-594\" style=\"font-family: STIXGeneral-Italic;\">y</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-49\">y</script> Given <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-595\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-596\"><span class=\"mi\" id=\"MathJax-Span-597\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">X</script></h5>\n<ul>\n  <li>Decoding is to search for the single label sequence with the largest joint probability conditioned on the input sequence:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><munder><mrow><mi>arg</mi><mo>&amp;#x2061;</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo></mrow><mi>y</mi></munder><mtext>&amp;#xA0;</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>p(y|X;</mtext></mrow><mi>&amp;#x03B8;</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-598\" style=\"width: 8.44em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1006.98em, 3.805em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-599\"><span class=\"munder\" id=\"MathJax-Span-600\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1003.18em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-601\"><span class=\"mi\" id=\"MathJax-Span-602\" style=\"font-family: STIXGeneral-Regular;\">arg</span><span class=\"mo\" id=\"MathJax-Span-603\"></span><span class=\"mo\" id=\"MathJax-Span-604\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">max</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.43em, -999.997em); top: -3.122em; left: 1.461em;\"><span class=\"mi\" id=\"MathJax-Span-605\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-606\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"texatom\" id=\"MathJax-Span-607\"><span class=\"mrow\" id=\"MathJax-Span-608\"><span class=\"mtext\" id=\"MathJax-Span-609\" style=\"font-family: STIXGeneral-Regular;\">p(y<span style=\"font-family: STIXVariants; font-style: normal; font-weight: normal;\">|</span>X;</span></span></span><span class=\"mi\" id=\"MathJax-Span-610\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-611\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.434em; border-left: 0px solid; width: 0px; height: 2.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><munder><mrow><mi>arg</mi><mo>⁡</mo><mo movablelimits=\"true\" form=\"prefix\">max</mo></mrow><mi>y</mi></munder><mtext>&nbsp;</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mtext>p(y|X;</mtext></mrow><mi>θ</mi><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>The parameters <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B8;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-612\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-613\"><span class=\"mi\" id=\"MathJax-Span-614\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>θ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">\\theta</script> correspond to the <em>transition</em> and <em>emission</em> matrices, basically the task is finding the best <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-615\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-616\"><span class=\"texatom\" id=\"MathJax-Span-617\"><span class=\"mrow\" id=\"MathJax-Span-618\"><span class=\"munderover\" id=\"MathJax-Span-619\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-620\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.055em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-621\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">^</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">\\hat{y}</script> given the transition matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>T</mtext></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-622\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-623\"><span class=\"texatom\" id=\"MathJax-Span-624\"><span class=\"mrow\" id=\"MathJax-Span-625\"><span class=\"mtext\" id=\"MathJax-Span-626\" style=\"font-family: STIXGeneral-Regular;\">T</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>T</mtext></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-54\">\\textrm{T}</script> and the matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>P</mtext></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-627\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-628\"><span class=\"texatom\" id=\"MathJax-Span-629\"><span class=\"mrow\" id=\"MathJax-Span-630\"><span class=\"mtext\" id=\"MathJax-Span-631\" style=\"font-family: STIXGeneral-Regular;\">P</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>P</mtext></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-55\">\\textrm{P}</script> with scores for each tag for the individual word:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>score</mtext></mrow><mo>=</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>T</mtext></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></msub><mo>+</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></munderover><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>P</mtext></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-632\" style=\"width: 13.701em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.419em, 1011.41em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-633\"><span class=\"texatom\" id=\"MathJax-Span-634\"><span class=\"mrow\" id=\"MathJax-Span-635\"><span class=\"mtext\" id=\"MathJax-Span-636\" style=\"font-family: STIXGeneral-Regular;\">score</span></span></span><span class=\"mo\" id=\"MathJax-Span-637\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-638\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-639\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.04em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-640\"><span class=\"mrow\" id=\"MathJax-Span-641\"><span class=\"mi\" id=\"MathJax-Span-642\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-643\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-644\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-645\"><span class=\"mrow\" id=\"MathJax-Span-646\"><span class=\"mi\" id=\"MathJax-Span-647\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-648\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.451em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-649\"><span class=\"mrow\" id=\"MathJax-Span-650\"><span class=\"mtext\" id=\"MathJax-Span-651\" style=\"font-family: STIXGeneral-Regular;\">T</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-652\"><span class=\"mrow\" id=\"MathJax-Span-653\"><span class=\"msubsup\" id=\"MathJax-Span-654\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-655\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-656\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-657\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-658\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-659\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-660\"><span class=\"mrow\" id=\"MathJax-Span-661\"><span class=\"mi\" id=\"MathJax-Span-662\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-663\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-664\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-665\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"munderover\" id=\"MathJax-Span-666\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-667\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-668\"><span class=\"mrow\" id=\"MathJax-Span-669\"><span class=\"mi\" id=\"MathJax-Span-670\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-671\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-672\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -5.206em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-673\"><span class=\"mrow\" id=\"MathJax-Span-674\"><span class=\"mi\" id=\"MathJax-Span-675\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-676\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-677\"><span class=\"mrow\" id=\"MathJax-Span-678\"><span class=\"mtext\" id=\"MathJax-Span-679\" style=\"font-family: STIXGeneral-Regular;\">P</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-680\"><span class=\"mrow\" id=\"MathJax-Span-681\"><span class=\"mi\" id=\"MathJax-Span-682\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-683\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-684\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-685\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-686\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mtext>score</mtext></mrow><mo>=</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msub><mrow class=\"MJX-TeXAtom-ORD\"><mtext>T</mtext></mrow><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>y</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></msub><mo>+</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></munderover><msub><mrow class=\"MJX-TeXAtom-ORD\"><mtext>P</mtext></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub></math></span></span></div>\n<ul>\n  <li>A linear-chain sequence CRF model, models only interactions between two successive labels, i.e bi-gram interactions, therefore one can find the sequence <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-687\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-688\"><span class=\"mi\" id=\"MathJax-Span-689\" style=\"font-family: STIXGeneral-Italic;\">y</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">y</script> maximizing the <strong>score</strong> function above by adopting the Viterbi algorithm (Rabiner, 1989).</li>\n</ul>",
    "contentMarkdown": "*   The main lessons learned from these papers are:\n    \n    *   Use two LSTMs (forward and backward)\n    *   CRF on the top/final layer to model tag transitions\n    *   Final embeddings are a combinations of word and character embeddings\n*   The following table summarizes the main characteristics of each of the models:\n    \n\nThe main lessons learned from these papers are:\n\n*   Use two LSTMs (forward and backward)\n*   CRF on the top/final layer to model tag transitions\n*   Final embeddings are a combinations of word and character embeddings\n\nThe following table summarizes the main characteristics of each of the models:\n\n \n\nFeatures\n\nArchitecture Resume\n\nStructured Tagging\n\nEmbeddings\n\nHuang et al. (2015)\n\nYes\n\nbi-LSTM output vectors +  \nfeatures vectors connected to CRF\n\nCRF\n\nCollobert et al. (2011)  \npre-trained  \n50-dimensions\n\nChiu and Nichols (2016)\n\nYes\n\nword embeddings + features vector  \ninput to a bi-LSTM the output  \nat each time step is decoded by a  \nlinear layer and a log-softmax layer  \ninto log-probabilities for each tag category  \n\nSentence-level log-likelihood\n\n\\- Collobert et al. 2011  \n\\- char-level embeddings  \nextracted with a CNN\n\nLample et al. (2016)\n\nNo\n\nchars and word embeddings  \ninput for the bi-LSTM  \noutput vectors are fed to the CRF layer to jointly decode the best label sequence\n\nCRF\n\n\\- char-level embeddings  \nextracted with a bi-LSTM  \n\\- pre-trained word embeddings  \nwith skip-n-gram\n\nMa and Hovy (2016)\n\nNo\n\nchars and word embeddings  \ninput for the bi-LSTM  \noutput vectors are fed to the CRF layer to jointly decode the best label sequence\n\nCRF\n\n\\- char embeddings extracted with a CNN  \n\\- word embeddings: GloVe 100-dimensions\n\n \n\nFeatures\n\nArchitecture Resume\n\nStructured Tagging\n\nEmbeddings\n\nHuang et al. (2015)\n\nYes\n\nbi-LSTM output vectors +  \nfeatures vectors connected to CRF\n\nCRF\n\nCollobert et al. (2011)  \npre-trained  \n50-dimensions\n\nChiu and Nichols (2016)\n\nYes\n\nword embeddings + features vector  \ninput to a bi-LSTM the output  \nat each time step is decoded by a  \nlinear layer and a log-softmax layer  \ninto log-probabilities for each tag category  \n\nSentence-level log-likelihood\n\n\\- Collobert et al. 2011  \n\\- char-level embeddings  \nextracted with a CNN\n\nLample et al. (2016)\n\nNo\n\nchars and word embeddings  \ninput for the bi-LSTM  \noutput vectors are fed to the CRF layer to jointly decode the best label sequence\n\nCRF\n\n\\- char-level embeddings  \nextracted with a bi-LSTM  \n\\- pre-trained word embeddings  \nwith skip-n-gram\n\nMa and Hovy (2016)\n\nNo\n\nchars and word embeddings  \ninput for the bi-LSTM  \noutput vectors are fed to the CRF layer to jointly decode the best label sequence\n\nCRF\n\n\\- char embeddings extracted with a CNN  \n\\- word embeddings: GloVe 100-dimensions\n\n#### Extra: Why a Conditional Random Field at the Top?\n\n*   Having independent classification decisions is limiting when there are strong dependencies across output labels, since you decide the label for a word independently from the previous given tags.\n    \n*   For sequence labeling or general structured prediction tasks, it is beneficial to consider the correlations between labels in neighborhoods and jointly decode the best chain of labels for a given input sentence:\n    \n    *   NER is one such task, since interpretable sequences of tags have constraints, e.g.: I-PER cannot follow B-LOC that would be impossible to model with independence assumptions;\n        \n    *   Another example is in POS tagging, an adjective is more likely to be followed by a noun than a verb;\n        \n*   The idea of using a CRF at the top is to model tagging decisions jointly, that is the probability of a given label to a word depends on the features associated to that word (i.e., final word embedding) and the assigned tag the word before.\n*   This means that the CRF layer could add constrains to the final predicted labels ensuring they are valid. The constrains are learned by the CRF layer automatically based on the annotated samples during the training process.\n\nHaving independent classification decisions is limiting when there are strong dependencies across output labels, since you decide the label for a word independently from the previous given tags.\n\nFor sequence labeling or general structured prediction tasks, it is beneficial to consider the correlations between labels in neighborhoods and jointly decode the best chain of labels for a given input sentence:\n\n*   NER is one such task, since interpretable sequences of tags have constraints, e.g.: I-PER cannot follow B-LOC that would be impossible to model with independence assumptions;\n    \n*   Another example is in POS tagging, an adjective is more likely to be followed by a noun than a verb;\n    \n\nNER is one such task, since interpretable sequences of tags have constraints, e.g.: I-PER cannot follow B-LOC that would be impossible to model with independence assumptions;\n\nAnother example is in POS tagging, an adjective is more likely to be followed by a noun than a verb;\n\n##### Emission Score Matrix\n\n*   The output of the LSTM is given as input to the CRF layer, that is, a matrix PP\\\\textrm{P} with the scores of the LSTM of size n×kn×kn \\\\times k, where nnn is the number of words in the sentence and kkk is the possible number of labels that each word can have, Pi,jPi,j\\\\textrm{P}\\_{i,j} is the score of the jthjthj^{th} tag of the ithithi^{th} word in the sentence. In the image below the matrix would be the concatenation of the yellow blocks coming out of each LSTM. The following diagram from https://createmomo.github.io shows the CRF Input Matrix.\n\n![](/primers/ai/assets/ner/LSTM_CRF_matrix.png)\n\n##### Transition Matrix\n\n*   TT\\\\textrm{T} is a matrix of transition scores such that Pi,jPi,j\\\\textrm{P}\\_{i,j} represents the score of a transition from the tag iii to tag jjj. Two extra tags are added, y0y0y\\_{0} and ynyny\\_{n} are the _start_ and _end_ tags of a sentence, that we add to the set of possible tags, TT\\\\textrm{T} is therefore a square matrix of size k+2k+2\\\\textrm{k}+2. The following diagram from https://eli5.readthedocs.io shows the CRF State Transition Matrix.\n\n![](/primers/ai/assets/ner/transition_matrix.png)\n\n##### Score of a Prediction\n\n*   For a given sequence of predictions for a sequence of words xxx…\n    \n    y\\=(y1,y2,…,yn)y\\=(y1,y2,…,yn)\n    \n    \\\\textrm{y} = (y\\_{1},y\\_{2},\\\\dots,y\\_{n})\n*   …we can compute it’s score based on the _emission_ and _transition_ matrices:\n    \n    score(y)\\=∑i\\=0nTyi,yi+1+∑i\\=1nPi,yiscore(y)\\=∑i\\=0nTyi,yi+1+∑i\\=1nPi,yi\n    \n    \\\\textrm{score}(y) = \\\\sum\\_{i=0}^{n} \\\\textrm{T}\\_{y\\_i,y\\_{i+1}} + \\\\sum\\_{i=1}^{n} \\\\textrm{P}\\_{i,y\\_i}\n*   So the score of a sequence of predictions is, for each word, the sum of the transition from the current assigned tag yiyiy\\_i to next assigned tag yi+1yi+1y\\_{i+1} plus the probability given by the LSTM to the tag assigned for the current word iii.\n    \n\nFor a given sequence of predictions for a sequence of words xxx…\n\n…we can compute it’s score based on the _emission_ and _transition_ matrices:\n\nSo the score of a sequence of predictions is, for each word, the sum of the transition from the current assigned tag yiyiy\\_i to next assigned tag yi+1yi+1y\\_{i+1} plus the probability given by the LSTM to the tag assigned for the current word iii.\n\n##### Training: Parameter Estimation\n\n*   During training, we assign a probability to each tag but maximizing the probability of the correct tag yyy sequence among all the other possible tag sequences.\n    \n*   This is modeled by applying a softmax over all the possible taggings yyy:\n    \n    p(y|X)\\=escore(X,y)∑y′∈Y(x)escore(X,y′)p(y|X)\\=escore(X,y)∑y′∈Y(x)escore(X,y′)\n    \n    \\\\textrm{p(y|X)} = \\\\frac{e^{score(X,y)}}{\\\\sum\\\\limits\\_{y' \\\\in Y({x})} e^{score(X,y')}}\n    *   where Y(x)Y(x)Y(x) denotes the set of all possible label sequences for xxx, this denominator is also known as the partition function. So, finding the best sequence is the equivalent of finding the sequence that maximizes score(X,y)score(X,y)\\\\textrm{score(X,y)}.\n*   The loss can be defined as the negative log likelihood of the current tagging yyy:\n    \n\nDuring training, we assign a probability to each tag but maximizing the probability of the correct tag yyy sequence among all the other possible tag sequences.\n\nThis is modeled by applying a softmax over all the possible taggings yyy:\n\n*   where Y(x)Y(x)Y(x) denotes the set of all possible label sequences for xxx, this denominator is also known as the partition function. So, finding the best sequence is the equivalent of finding the sequence that maximizes score(X,y)score(X,y)\\\\textrm{score(X,y)}.\n\nThe loss can be defined as the negative log likelihood of the current tagging yyy:\n\n\\-log p(y|X)\\-log p(y|X)\n\n*   So, in simplifying the function above, a first step is to get rid of the fraction using log equivalences, and then get rid of the log elog e\\\\textrm{log}\\\\ e in the first term since they cancel each other out:\n\n\\-log p(y|X)\\=− score(X,y)+log∑y′∈Y(x)exp(score(X,y'))\\-log p(y|X)\\=− score(X,y)+log∑y′∈Y(x)exp(score(X,y'))\n\n*   Then the second term can be simplified by applying the log-space addition _logadd_, equivalence, i.e.: ⊕(a,b,c,d)\\=log(ea+eb+ec+ed)⊕(a,b,c,d)\\=log(ea+eb+ec+ed)\\\\oplus(a, b, c, d) = log(e^a+e^b+e^c+e^d):\n\n\\-log p(y|X)\\=− score(X,y)+logaddy′∈Y(x)(score(X,y'))\\-log p(y|X)\\=− score(X,y)+logaddy′∈Y(x)(score(X,y'))\n\n*   Then, replacing the scorescore\\\\textrm{score} by it’s definition:\n\n\\=−(∑i\\=0nTyi,yi+1+∑i\\=1nPi,yi)+logaddy′∈Y(x)(∑i\\=0nTy′i,y′i+1+∑i\\=1nPi,yi)\\=−(∑i\\=0nTyi,yi+1+∑i\\=1nPi,yi)+logaddy′∈Y(x)(∑i\\=0nTyi′,yi+1′+∑i\\=1nPi,yi)\n\n*   The first term is score for the true data. Computing the second term might be computationally expensive since it requires summing over the knknk^{n} different sequences in Y(x)Y(x)Y(x), i.e., the set of all possible label sequences for xxx. This computation can be solved using a variant of the Viterbi algorithm, the forward algorithm.\n    \n*   The gradients are then computed using back-propagation, since the CRF is inside the neural-network. Note that the transition scores in the matrix are randomly initialized - or can also bee initialized based on some criteria, to speed up training - and will be updated automatically during your training process.\n    \n\nThe first term is score for the true data. Computing the second term might be computationally expensive since it requires summing over the knknk^{n} different sequences in Y(x)Y(x)Y(x), i.e., the set of all possible label sequences for xxx. This computation can be solved using a variant of the Viterbi algorithm, the forward algorithm.\n\nThe gradients are then computed using back-propagation, since the CRF is inside the neural-network. Note that the transition scores in the matrix are randomly initialized - or can also bee initialized based on some criteria, to speed up training - and will be updated automatically during your training process.\n\n##### Inference: Determining the Most Likely Label Sequence yyy Given XXX\n\n*   Decoding is to search for the single label sequence with the largest joint probability conditioned on the input sequence:\n\nargmaxy p(y|X;θ)arg⁡maxy p(y|X;θ)\n\n*   The parameters θθ\\\\theta correspond to the _transition_ and _emission_ matrices, basically the task is finding the best ŷ y^\\\\hat{y} given the transition matrix TT\\\\textrm{T} and the matrix PP\\\\textrm{P} with scores for each tag for the individual word:\n\nscore\\=∑i\\=0nTyi,yi+1+∑i\\=1nPi,yiscore\\=∑i\\=0nTyi,yi+1+∑i\\=1nPi,yi\n\n*   A linear-chain sequence CRF model, models only interactions between two successive labels, i.e bi-gram interactions, therefore one can find the sequence yyy maximizing the **score** function above by adopting the Viterbi algorithm (Rabiner, 1989).",
    "order": 10,
    "orderInChapter": 10,
    "difficulty": 3,
    "estimatedMinutes": 9,
    "tags": [
      "nlpllms",
      "embedding",
      "cnn",
      "lstm",
      "bert"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": true,
      "hasImages": true,
      "wordCount": 1701,
      "contentLength": 178731
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ner/#key-takeaways",
    "scrapedAt": "2025-12-28T11:54:15.608Z"
  },
  {
    "id": "ai-ner-references-11",
    "domain": "ai_primers",
    "category": "NLP/LLMs",
    "article": "Named Entity Recognition",
    "articleSlug": "ner",
    "chapter": "Neural Sequence Labelling Models",
    "title": "References",
    "subtitle": "Neural Sequence Labelling Models",
    "contentHtml": "<ul>\n  <li>\n    <p><a href=\"https://arxiv.org/pdf/1508.01991v1.pdf\">Bidirectional LSTM-CRF Models for Sequence Tagging (Huang et al. 2015)</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://www.aclweb.org/anthology/Q16-1026\">Named Entity Recognition with Bidirectional LSTM-CNNs (Chiu and Nichols 2016)</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://www.aclweb.org/anthology/N16-1030\">Neural Architectures for Named Entity Recognition (Lample et al. 2016)</a></p>\n  </li>\n  <li>\n    <p><a href=\"http://www.aclweb.org/anthology/P16-1101\">End-to-end Sequence Labelling via Bi-directional LSTM-CNNs-CRF (Ma and Hovy 2016)</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://www.robots.ox.ac.uk/~vgg/rg/papers/hmm.pdf\">A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://www.youtube.com/watch?v=6dpGB60Q1Ts\">Hugo Larochelle on-line lessons - Neural networks [4.1] : Training CRFs - loss function</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://createmomo.github.io/\">Blog article: CRF Layer on the Top of BiLSTM - 1 to 8</a></p>\n  </li>\n  <li>\n    <p><a href=\"http://www.aclweb.org/anthology/D15-1161\">Not All Contexts Are Created Equal: Better Word Representations with Variable Attention (Ling et al., 2015)</a></p>\n  </li>\n  <li>\n    <p><a href=\"http://www.aclweb.org/anthology/D15-1025\">Non-lexical neural architecture for fine-grained POS Tagging (Labeau et al., 2015)</a></p>\n  </li>\n  <li>\n    <p><a href=\"http://www.anthology.aclweb.org/W/W15/W15-3904.pdf\">Boosting Named Entity Recognition with Neural Character Embeddings (Santos et al., 2015)</a></p>\n  </li>\n  <li>\n    <p><a href=\"http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf\">Natural Language Processing (Almost) from Scratch (2011)</a></p>\n  </li>\n</ul>\n<p><a href=\"https://arxiv.org/pdf/1508.01991v1.pdf\">Bidirectional LSTM-CRF Models for Sequence Tagging (Huang et al. 2015)</a></p>\n<p><a href=\"https://www.aclweb.org/anthology/Q16-1026\">Named Entity Recognition with Bidirectional LSTM-CNNs (Chiu and Nichols 2016)</a></p>\n<p><a href=\"https://www.aclweb.org/anthology/N16-1030\">Neural Architectures for Named Entity Recognition (Lample et al. 2016)</a></p>\n<p><a href=\"http://www.aclweb.org/anthology/P16-1101\">End-to-end Sequence Labelling via Bi-directional LSTM-CNNs-CRF (Ma and Hovy 2016)</a></p>\n<p><a href=\"https://www.robots.ox.ac.uk/~vgg/rg/papers/hmm.pdf\">A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition</a></p>\n<p><a href=\"https://www.youtube.com/watch?v=6dpGB60Q1Ts\">Hugo Larochelle on-line lessons - Neural networks [4.1] : Training CRFs - loss function</a></p>\n<p><a href=\"https://createmomo.github.io/\">Blog article: CRF Layer on the Top of BiLSTM - 1 to 8</a></p>\n<p><a href=\"http://www.aclweb.org/anthology/D15-1161\">Not All Contexts Are Created Equal: Better Word Representations with Variable Attention (Ling et al., 2015)</a></p>\n<p><a href=\"http://www.aclweb.org/anthology/D15-1025\">Non-lexical neural architecture for fine-grained POS Tagging (Labeau et al., 2015)</a></p>\n<p><a href=\"http://www.anthology.aclweb.org/W/W15/W15-3904.pdf\">Boosting Named Entity Recognition with Neural Character Embeddings (Santos et al., 2015)</a></p>\n<p><a href=\"http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf\">Natural Language Processing (Almost) from Scratch (2011)</a></p>",
    "contentMarkdown": "*   [Bidirectional LSTM-CRF Models for Sequence Tagging (Huang et al. 2015)](https://arxiv.org/pdf/1508.01991v1.pdf)\n    \n*   [Named Entity Recognition with Bidirectional LSTM-CNNs (Chiu and Nichols 2016)](https://www.aclweb.org/anthology/Q16-1026)\n    \n*   [Neural Architectures for Named Entity Recognition (Lample et al. 2016)](https://www.aclweb.org/anthology/N16-1030)\n    \n*   [End-to-end Sequence Labelling via Bi-directional LSTM-CNNs-CRF (Ma and Hovy 2016)](http://www.aclweb.org/anthology/P16-1101)\n    \n*   [A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition](https://www.robots.ox.ac.uk/~vgg/rg/papers/hmm.pdf)\n    \n*   [Hugo Larochelle on-line lessons - Neural networks \\[4.1\\] : Training CRFs - loss function](https://www.youtube.com/watch?v=6dpGB60Q1Ts)\n    \n*   [Blog article: CRF Layer on the Top of BiLSTM - 1 to 8](https://createmomo.github.io/)\n    \n*   [Not All Contexts Are Created Equal: Better Word Representations with Variable Attention (Ling et al., 2015)](http://www.aclweb.org/anthology/D15-1161)\n    \n*   [Non-lexical neural architecture for fine-grained POS Tagging (Labeau et al., 2015)](http://www.aclweb.org/anthology/D15-1025)\n    \n*   [Boosting Named Entity Recognition with Neural Character Embeddings (Santos et al., 2015)](http://www.anthology.aclweb.org/W/W15/W15-3904.pdf)\n    \n*   [Natural Language Processing (Almost) from Scratch (2011)](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)\n    \n\n[Bidirectional LSTM-CRF Models for Sequence Tagging (Huang et al. 2015)](https://arxiv.org/pdf/1508.01991v1.pdf)\n\n[Named Entity Recognition with Bidirectional LSTM-CNNs (Chiu and Nichols 2016)](https://www.aclweb.org/anthology/Q16-1026)\n\n[Neural Architectures for Named Entity Recognition (Lample et al. 2016)](https://www.aclweb.org/anthology/N16-1030)\n\n[End-to-end Sequence Labelling via Bi-directional LSTM-CNNs-CRF (Ma and Hovy 2016)](http://www.aclweb.org/anthology/P16-1101)\n\n[A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition](https://www.robots.ox.ac.uk/~vgg/rg/papers/hmm.pdf)\n\n[Hugo Larochelle on-line lessons - Neural networks \\[4.1\\] : Training CRFs - loss function](https://www.youtube.com/watch?v=6dpGB60Q1Ts)\n\n[Blog article: CRF Layer on the Top of BiLSTM - 1 to 8](https://createmomo.github.io/)\n\n[Not All Contexts Are Created Equal: Better Word Representations with Variable Attention (Ling et al., 2015)](http://www.aclweb.org/anthology/D15-1161)\n\n[Non-lexical neural architecture for fine-grained POS Tagging (Labeau et al., 2015)](http://www.aclweb.org/anthology/D15-1025)\n\n[Boosting Named Entity Recognition with Neural Character Embeddings (Santos et al., 2015)](http://www.anthology.aclweb.org/W/W15/W15-3904.pdf)\n\n[Natural Language Processing (Almost) from Scratch (2011)](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)",
    "order": 11,
    "orderInChapter": 11,
    "difficulty": 2,
    "estimatedMinutes": 2,
    "tags": [
      "nlpllms",
      "neural network",
      "attention",
      "embedding",
      "cnn",
      "lstm",
      "bert",
      "loss function"
    ],
    "metadata": {
      "hasCode": false,
      "hasMath": false,
      "hasImages": false,
      "wordCount": 261,
      "contentLength": 3385
    },
    "nextCards": [],
    "relatedCards": [],
    "prerequisites": [],
    "sourceUrl": "https://aman.ai/primers/ai/ner/#references",
    "scrapedAt": "2025-12-28T11:54:15.608Z"
  }
]