{
  "category": "On-Device AI",
  "cards": [
    {
      "id": "ai-model-compression-background-precision-1",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Background: Precision",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>\n    <p>Before diving into quantization, it’s essential to understand precision—specifically, how computers represent decimal numbers like <code class=\"language-plaintext highlighter-rouge\">1.0151</code> or <code class=\"language-plaintext highlighter-rouge\">566132.8</code>. Since we can conceive of infinitely precise values (like π), but only have limited space in memory, there’s a fundamental trade-off between <strong>precision</strong> (how many significant digits can be stored) and <strong>size</strong> (how many bits are used to represent the number).</p>\n  </li>\n  <li>\n    <p>In computer engineering, these values are stored as <strong>floating point numbers</strong>, governed by the <a href=\"#ieee-754-floating-point-standard\">IEEE 754 floating point standard</a>. This specification defines how bits are allocated to represent the <strong>sign</strong>, <strong>exponent</strong>, and <strong>mantissa</strong> (also called the <strong>significand</strong>, which holds the meaningful digits).</p>\n  </li>\n  <li>\n    <p>Floating point formats vary by their bit-width, and each level of precision has a different rounding error margin:</p>\n\n    <ul>\n      <li><strong>Double-precision (<code class=\"language-plaintext highlighter-rouge\">fp64</code>)</strong> – 64 bits, max rounding error of approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>52</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-21\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-22\"><span class=\"msubsup\" id=\"MathJax-Span-23\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-24\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-25\"><span class=\"mrow\" id=\"MathJax-Span-26\"><span class=\"mo\" id=\"MathJax-Span-27\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-28\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">52</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>52</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">2^{-52}</script>.</li>\n      <li><strong>Single-precision (<code class=\"language-plaintext highlighter-rouge\">float32</code>)</strong> – 32 bits, max rounding error of approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>23</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-29\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-30\"><span class=\"msubsup\" id=\"MathJax-Span-31\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-32\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-33\"><span class=\"mrow\" id=\"MathJax-Span-34\"><span class=\"mo\" id=\"MathJax-Span-35\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-36\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">23</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>23</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">2^{-23}</script>.</li>\n      <li><strong>Half-precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>)</strong> – 16 bits, max rounding error of approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>10</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-37\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-38\"><span class=\"msubsup\" id=\"MathJax-Span-39\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-40\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-41\"><span class=\"mrow\" id=\"MathJax-Span-42\"><span class=\"mo\" id=\"MathJax-Span-43\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-44\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">10</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>10</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">2^{-10}</script>.</li>\n    </ul>\n  </li>\n  <li>\n    <p>For a deeper exploration, check out the <a href=\"https://www.youtube.com/watch?v=zguLmgYWhM0\">PyCon 2019 talk: “Floats are Friends: making the most of IEEE754.00000000000000002”</a>.</p>\n  </li>\n  <li>\n    <p>In practice:</p>\n\n    <ul>\n      <li><strong>Python</strong> defaults to using <code class=\"language-plaintext highlighter-rouge\">fp64</code> for its <code class=\"language-plaintext highlighter-rouge\">float</code> type.</li>\n      <li><strong>PyTorch</strong>, which is optimized for performance and memory efficiency, defaults to <code class=\"language-plaintext highlighter-rouge\">float32</code>.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Understanding these formats is crucial when moving on to concepts like <strong>mixed precision training</strong>, where models leverage different floating point types to balance performance and accuracy.</p>\n  </li>\n</ul>\n<p>Before diving into quantization, it’s essential to understand precision—specifically, how computers represent decimal numbers like <code class=\"language-plaintext highlighter-rouge\">1.0151</code> or <code class=\"language-plaintext highlighter-rouge\">566132.8</code>. Since we can conceive of infinitely precise values (like π), but only have limited space in memory, there’s a fundamental trade-off between <strong>precision</strong> (how many significant digits can be stored) and <strong>size</strong> (how many bits are used to represent the number).</p>\n<p>In computer engineering, these values are stored as <strong>floating point numbers</strong>, governed by the <a href=\"#ieee-754-floating-point-standard\">IEEE 754 floating point standard</a>. This specification defines how bits are allocated to represent the <strong>sign</strong>, <strong>exponent</strong>, and <strong>mantissa</strong> (also called the <strong>significand</strong>, which holds the meaningful digits).</p>\n<p>Floating point formats vary by their bit-width, and each level of precision has a different rounding error margin:</p>\n<ul>\n      <li><strong>Double-precision (<code class=\"language-plaintext highlighter-rouge\">fp64</code>)</strong> – 64 bits, max rounding error of approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>52</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-21\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-22\"><span class=\"msubsup\" id=\"MathJax-Span-23\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-24\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-25\"><span class=\"mrow\" id=\"MathJax-Span-26\"><span class=\"mo\" id=\"MathJax-Span-27\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-28\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">52</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>52</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">2^{-52}</script>.</li>\n      <li><strong>Single-precision (<code class=\"language-plaintext highlighter-rouge\">float32</code>)</strong> – 32 bits, max rounding error of approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>23</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-29\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-30\"><span class=\"msubsup\" id=\"MathJax-Span-31\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-32\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-33\"><span class=\"mrow\" id=\"MathJax-Span-34\"><span class=\"mo\" id=\"MathJax-Span-35\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-36\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">23</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>23</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">2^{-23}</script>.</li>\n      <li><strong>Half-precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>)</strong> – 16 bits, max rounding error of approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>10</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-37\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-38\"><span class=\"msubsup\" id=\"MathJax-Span-39\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-40\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-41\"><span class=\"mrow\" id=\"MathJax-Span-42\"><span class=\"mo\" id=\"MathJax-Span-43\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-44\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">10</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>10</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">2^{-10}</script>.</li>\n    </ul>\n<p>For a deeper exploration, check out the <a href=\"https://www.youtube.com/watch?v=zguLmgYWhM0\">PyCon 2019 talk: “Floats are Friends: making the most of IEEE754.00000000000000002”</a>.</p>\n<p>In practice:</p>\n<ul>\n      <li><strong>Python</strong> defaults to using <code class=\"language-plaintext highlighter-rouge\">fp64</code> for its <code class=\"language-plaintext highlighter-rouge\">float</code> type.</li>\n      <li><strong>PyTorch</strong>, which is optimized for performance and memory efficiency, defaults to <code class=\"language-plaintext highlighter-rouge\">float32</code>.</li>\n    </ul>\n<p>Understanding these formats is crucial when moving on to concepts like <strong>mixed precision training</strong>, where models leverage different floating point types to balance performance and accuracy.</p>\n<h4 id=\"ieee-754-floating-point-standard\">IEEE 754 Floating Point Standard</h4>\n<ul>\n  <li>\n    <p>The IEEE 754 standard defines the binary representation of floating point numbers used in nearly all modern hardware and programming environments. A floating-point number is composed of three parts:</p>\n\n    <ul>\n      <li><strong>Sign bit (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>S</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-45\" style=\"width: 0.726em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.571em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.57em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-46\"><span class=\"mi\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>S</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">S</script>)</strong>: 1 bit indicating positive or negative.</li>\n      <li><strong>Exponent (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-48\" style=\"width: 0.777em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.622em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.62em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-49\"><span class=\"mi\" id=\"MathJax-Span-50\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">E</script>)</strong>: Encodes the range (scale) of the number.</li>\n      <li><strong>Mantissa or significand (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-51\" style=\"width: 1.139em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.932em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.93em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-52\"><span class=\"mi\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">M</script>)</strong>: Encodes the precision.</li>\n    </ul>\n  </li>\n  <li>\n    <p>The general representation for a binary floating-point number is:</p>\n  </li>\n</ul>\n<p>The IEEE 754 standard defines the binary representation of floating point numbers used in nearly all modern hardware and programming environments. A floating-point number is composed of three parts:</p>\n<ul>\n      <li><strong>Sign bit (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>S</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-45\" style=\"width: 0.726em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.571em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.57em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-46\"><span class=\"mi\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>S</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">S</script>)</strong>: 1 bit indicating positive or negative.</li>\n      <li><strong>Exponent (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-48\" style=\"width: 0.777em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.622em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.62em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-49\"><span class=\"mi\" id=\"MathJax-Span-50\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">E</script>)</strong>: Encodes the range (scale) of the number.</li>\n      <li><strong>Mantissa or significand (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-51\" style=\"width: 1.139em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.932em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.93em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-52\"><span class=\"mi\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">M</script>)</strong>: Encodes the precision.</li>\n    </ul>\n<p>The general representation for a binary floating-point number is:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>value</mtext><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mo>&amp;#x2212;</mo><mn>1</mn><msup><mo stretchy=&quot;false&quot;>)</mo><mi>S</mi></msup><mo>&amp;#x00D7;</mo><mn>1.</mn><mi>M</mi><mo>&amp;#x00D7;</mo><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>E</mi><mo>&amp;#x2212;</mo><mtext>bias</mtext></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-54\" style=\"width: 14.534em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1012.09em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-55\"><span class=\"mtext\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral-Regular;\">value</span><span class=\"mo\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-58\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mo\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-60\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"msubsup\" id=\"MathJax-Span-61\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-62\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-63\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-65\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1.</span><span class=\"mi\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-68\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-69\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-70\"><span class=\"mrow\" id=\"MathJax-Span-71\"><span class=\"mi\" id=\"MathJax-Span-72\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-73\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mtext\" id=\"MathJax-Span-74\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">bias</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>value</mtext><mo>=</mo><mo stretchy=\"false\">(</mo><mo>−</mo><mn>1</mn><msup><mo stretchy=\"false\">)</mo><mi>S</mi></msup><mo>×</mo><mn>1.</mn><mi>M</mi><mo>×</mo><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>E</mi><mo>−</mo><mtext>bias</mtext></mrow></msup></math></span></span></div>\n<ul>\n  <li>\n    <p>Each format—half, single, and double—allocates a different number of bits to these components, balancing precision and range against memory usage and compute requirements.</p>\n  </li>\n  <li>\n    <p>The following figure <a href=\"https://blogs.nvidia.com/blog/2019/11/15/whats-the-difference-between-single-double-multi-and-mixed-precision-computing/\">(source)</a> shows the IEEE 754 standard formats for floating-point numbers, illustrating the bitwise layout of the signed bit, exponent, and significand across double (64-bit), single (32-bit), and half (16-bit) precision representations.</p>\n  </li>\n</ul>\n<p>Each format—half, single, and double—allocates a different number of bits to these components, balancing precision and range against memory usage and compute requirements.</p>\n<p>The following figure <a href=\"https://blogs.nvidia.com/blog/2019/11/15/whats-the-difference-between-single-double-multi-and-mixed-precision-computing/\">(source)</a> shows the IEEE 754 standard formats for floating-point numbers, illustrating the bitwise layout of the signed bit, exponent, and significand across double (64-bit), single (32-bit), and half (16-bit) precision representations.</p>\n<p><img src=\"/primers/ai/assets/model-compression/ieee_formats.webp\" alt=\"\"></p>\n<h4 id=\"half-precision-float16\">Half-Precision (<code class=\"language-plaintext Highlighter-rouge\">float16</code>)</h4>\n<ul>\n  <li><strong>Bit layout</strong>: 1 sign bit, 5 exponent bits, 10 mantissa/significand bits.</li>\n  <li><strong>Total bits</strong>: 16</li>\n  <li><strong>Exponent bias</strong>: 15</li>\n  <li><strong>Dynamic range</strong>: Approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>6</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>5</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-75\" style=\"width: 4.273em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.544em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1003.54em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-76\"><span class=\"mn\" id=\"MathJax-Span-77\" style=\"font-family: STIXGeneral-Regular;\">6</span><span class=\"mo\" id=\"MathJax-Span-78\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-79\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-80\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-81\"><span class=\"mrow\" id=\"MathJax-Span-82\"><span class=\"mo\" id=\"MathJax-Span-83\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-84\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>6</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">6 \\times 10^{-5}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>6.5</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mn>4</mn></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-85\" style=\"width: 4.534em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1003.75em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-86\"><span class=\"mn\" id=\"MathJax-Span-87\" style=\"font-family: STIXGeneral-Regular;\">6.5</span><span class=\"mo\" id=\"MathJax-Span-88\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-89\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-90\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"mn\" id=\"MathJax-Span-91\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>6.5</mn><mo>×</mo><msup><mn>10</mn><mn>4</mn></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">6.5 \\times 10^4</script></li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mtext>value</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>float16</mtext></mrow></msub><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mo>&amp;#x2212;</mo><mn>1</mn><msup><mo stretchy=&quot;false&quot;>)</mo><mi>S</mi></msup><mo>&amp;#x00D7;</mo><mn>1.</mn><msub><mi>M</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>10</mn></mrow></msub><mo>&amp;#x00D7;</mo><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>E</mi><mo>&amp;#x2212;</mo><mn>15</mn></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-92\" style=\"width: 17.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.378em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1014.38em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"msubsup\" id=\"MathJax-Span-94\"><span style=\"display: inline-block; position: relative; width: 4.221em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1002.14em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mtext\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Regular;\">value</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 2.19em;\"><span class=\"texatom\" id=\"MathJax-Span-96\"><span class=\"mrow\" id=\"MathJax-Span-97\"><span class=\"mtext\" id=\"MathJax-Span-98\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">float16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mo\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"msubsup\" id=\"MathJax-Span-103\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-105\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-106\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1.</span><span class=\"msubsup\" id=\"MathJax-Span-108\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-109\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-110\"><span class=\"mrow\" id=\"MathJax-Span-111\"><span class=\"mn\" id=\"MathJax-Span-112\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">10</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-113\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-114\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-115\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-116\"><span class=\"mrow\" id=\"MathJax-Span-117\"><span class=\"mi\" id=\"MathJax-Span-118\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-119\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-120\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">15</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mtext>value</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mtext>float16</mtext></mrow></msub><mo>=</mo><mo stretchy=\"false\">(</mo><mo>−</mo><mn>1</mn><msup><mo stretchy=\"false\">)</mo><mi>S</mi></msup><mo>×</mo><mn>1.</mn><msub><mi>M</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>10</mn></mrow></msub><mo>×</mo><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>E</mi><mo>−</mo><mn>15</mn></mrow></msup></math></span></span></div>\n<ul>\n  <li>\n    <p>Half-precision is mostly used during inference, especially in low-power or memory-constrained environments such as mobile devices or embedded hardware. It offers limited range and precision, and is generally <em>not</em> suitable for training deep networks unless special care is taken (e.g., using loss scaling or <code class=\"language-plaintext highlighter-rouge\">float32</code> accumulations).</p>\n  </li>\n  <li>\n    <p><strong>GPU Considerations</strong>:</p>\n\n    <ul>\n      <li>Many GPUs, like NVIDIA’s Volta, Turing, Ampere, Hopper, Blackwell, include specialized hardware units called <strong>Tensor Cores</strong> optimized for <code class=\"language-plaintext highlighter-rouge\">float16</code> operations.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">float16</code> can be processed at higher throughput than <code class=\"language-plaintext highlighter-rouge\">float32</code>, enabling significant speedups for matrix multiplications during inference.</li>\n      <li>Often paired with Mixed-Precision Training (MPT), where activations and weights are stored in <code class=\"language-plaintext highlighter-rouge\">float16</code>, but gradients are accumulated in <code class=\"language-plaintext highlighter-rouge\">float32</code>.</li>\n    </ul>\n  </li>\n</ul>\n<p>Half-precision is mostly used during inference, especially in low-power or memory-constrained environments such as mobile devices or embedded hardware. It offers limited range and precision, and is generally <em>not</em> suitable for training deep networks unless special care is taken (e.g., using loss scaling or <code class=\"language-plaintext highlighter-rouge\">float32</code> accumulations).</p>\n<p><strong>GPU Considerations</strong>:</p>\n<ul>\n      <li>Many GPUs, like NVIDIA’s Volta, Turing, Ampere, Hopper, Blackwell, include specialized hardware units called <strong>Tensor Cores</strong> optimized for <code class=\"language-plaintext highlighter-rouge\">float16</code> operations.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">float16</code> can be processed at higher throughput than <code class=\"language-plaintext highlighter-rouge\">float32</code>, enabling significant speedups for matrix multiplications during inference.</li>\n      <li>Often paired with Mixed-Precision Training (MPT), where activations and weights are stored in <code class=\"language-plaintext highlighter-rouge\">float16</code>, but gradients are accumulated in <code class=\"language-plaintext highlighter-rouge\">float32</code>.</li>\n    </ul>\n<h4 id=\"single-precision-float32\">Single-Precision (<code class=\"language-plaintext Highlighter-rouge\">float32</code>)</h4>\n<ul>\n  <li><strong>Bit layout</strong>: 1 sign bit, 8 exponent bits, 23 mantissa bits.</li>\n  <li><strong>Total bits</strong>: 32</li>\n  <li><strong>Exponent bias</strong>: 127</li>\n  <li><strong>Dynamic range</strong>: Approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1.4</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>45</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-121\" style=\"width: 5.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.638em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1004.64em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-122\"><span class=\"mn\" id=\"MathJax-Span-123\" style=\"font-family: STIXGeneral-Regular;\">1.4</span><span class=\"mo\" id=\"MathJax-Span-124\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-125\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-126\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-127\"><span class=\"mrow\" id=\"MathJax-Span-128\"><span class=\"mo\" id=\"MathJax-Span-129\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-130\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">45</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>1.4</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>45</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">1.4 \\times 10^{-45}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>3.4</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>38</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-131\" style=\"width: 4.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1004.12em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-132\"><span class=\"mn\" id=\"MathJax-Span-133\" style=\"font-family: STIXGeneral-Regular;\">3.4</span><span class=\"mo\" id=\"MathJax-Span-134\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-135\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-136\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-137\"><span class=\"mrow\" id=\"MathJax-Span-138\"><span class=\"mn\" id=\"MathJax-Span-139\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">38</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>3.4</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>38</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">3.4 \\times 10^{38}</script></li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mtext>value</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>float32</mtext></mrow></msub><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mo>&amp;#x2212;</mo><mn>1</mn><msup><mo stretchy=&quot;false&quot;>)</mo><mi>S</mi></msup><mo>&amp;#x00D7;</mo><mn>1.</mn><msub><mi>M</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>23</mn></mrow></msub><mo>&amp;#x00D7;</mo><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>E</mi><mo>&amp;#x2212;</mo><mn>127</mn></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-140\" style=\"width: 17.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.69em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1014.69em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-141\"><span class=\"msubsup\" id=\"MathJax-Span-142\"><span style=\"display: inline-block; position: relative; width: 4.221em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1002.14em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mtext\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Regular;\">value</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 2.19em;\"><span class=\"texatom\" id=\"MathJax-Span-144\"><span class=\"mrow\" id=\"MathJax-Span-145\"><span class=\"mtext\" id=\"MathJax-Span-146\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">float32</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mo\" id=\"MathJax-Span-149\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-150\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"msubsup\" id=\"MathJax-Span-151\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-153\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-154\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-155\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1.</span><span class=\"msubsup\" id=\"MathJax-Span-156\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-157\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-158\"><span class=\"mrow\" id=\"MathJax-Span-159\"><span class=\"mn\" id=\"MathJax-Span-160\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">23</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-161\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-162\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-163\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-164\"><span class=\"mrow\" id=\"MathJax-Span-165\"><span class=\"mi\" id=\"MathJax-Span-166\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-167\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-168\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">127</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mtext>value</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mtext>float32</mtext></mrow></msub><mo>=</mo><mo stretchy=\"false\">(</mo><mo>−</mo><mn>1</mn><msup><mo stretchy=\"false\">)</mo><mi>S</mi></msup><mo>×</mo><mn>1.</mn><msub><mi>M</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>23</mn></mrow></msub><mo>×</mo><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>E</mi><mo>−</mo><mn>127</mn></mrow></msup></math></span></span></div>\n<ul>\n  <li>\n    <p><code class=\"language-plaintext highlighter-rouge\">float32</code> is the default numerical format for most deep learning frameworks and hardware. It provides a good balance between precision and range, making it robust for both training and inference.</p>\n  </li>\n  <li>\n    <p><strong>GPU Considerations</strong>:</p>\n\n    <ul>\n      <li>Supported natively on all modern GPUs.</li>\n      <li>Most general-purpose ALUs (Arithmetic Logic Units) on the GPU are designed to process <code class=\"language-plaintext highlighter-rouge\">float32</code> efficiently.</li>\n      <li>Slower than <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> in terms of throughput and power usage but more accurate.</li>\n    </ul>\n  </li>\n</ul>\n<p><code class=\"language-plaintext highlighter-rouge\">float32</code> is the default numerical format for most deep learning frameworks and hardware. It provides a good balance between precision and range, making it robust for both training and inference.</p>\n<p><strong>GPU Considerations</strong>:</p>\n<ul>\n      <li>Supported natively on all modern GPUs.</li>\n      <li>Most general-purpose ALUs (Arithmetic Logic Units) on the GPU are designed to process <code class=\"language-plaintext highlighter-rouge\">float32</code> efficiently.</li>\n      <li>Slower than <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> in terms of throughput and power usage but more accurate.</li>\n    </ul>\n<h4 id=\"double-precision-float64\">Double-Precision (<code class=\"language-plaintext Highlighter-rouge\">float64</code>)</h4>\n<ul>\n  <li><strong>Bit layout</strong>: 1 sign bit, 11 exponent bits, 52 mantissa bits.</li>\n  <li><strong>Total bits</strong>: 64</li>\n  <li><strong>Exponent bias</strong>: 1023</li>\n  <li><strong>Dynamic range</strong>: Approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>5</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>324</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-169\" style=\"width: 5.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1004.22em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-170\"><span class=\"mn\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Regular;\">5</span><span class=\"mo\" id=\"MathJax-Span-172\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-173\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-174\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-175\"><span class=\"mrow\" id=\"MathJax-Span-176\"><span class=\"mo\" id=\"MathJax-Span-177\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-178\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">324</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>5</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>324</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">5 \\times 10^{-324}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1.8</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>308</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-179\" style=\"width: 5.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.482em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1004.48em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-180\"><span class=\"mn\" id=\"MathJax-Span-181\" style=\"font-family: STIXGeneral-Regular;\">1.8</span><span class=\"mo\" id=\"MathJax-Span-182\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-183\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-184\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-185\"><span class=\"mrow\" id=\"MathJax-Span-186\"><span class=\"mn\" id=\"MathJax-Span-187\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">308</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>1.8</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>308</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">1.8 \\times 10^{308}</script></li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mtext>value</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>float64</mtext></mrow></msub><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mo>&amp;#x2212;</mo><mn>1</mn><msup><mo stretchy=&quot;false&quot;>)</mo><mi>S</mi></msup><mo>&amp;#x00D7;</mo><mn>1.</mn><msub><mi>M</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>52</mn></mrow></msub><mo>&amp;#x00D7;</mo><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>E</mi><mo>&amp;#x2212;</mo><mn>1023</mn></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-188\" style=\"width: 18.076em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 15.055em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1015.05em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-189\"><span class=\"msubsup\" id=\"MathJax-Span-190\"><span style=\"display: inline-block; position: relative; width: 4.221em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1002.14em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mtext\" id=\"MathJax-Span-191\" style=\"font-family: STIXGeneral-Regular;\">value</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 2.19em;\"><span class=\"texatom\" id=\"MathJax-Span-192\"><span class=\"mrow\" id=\"MathJax-Span-193\"><span class=\"mtext\" id=\"MathJax-Span-194\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">float64</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-195\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-196\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mo\" id=\"MathJax-Span-197\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-198\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"msubsup\" id=\"MathJax-Span-199\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-200\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-201\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-202\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-203\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1.</span><span class=\"msubsup\" id=\"MathJax-Span-204\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-205\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-206\"><span class=\"mrow\" id=\"MathJax-Span-207\"><span class=\"mn\" id=\"MathJax-Span-208\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">52</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-209\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-210\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.919em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-211\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-212\"><span class=\"mrow\" id=\"MathJax-Span-213\"><span class=\"mi\" id=\"MathJax-Span-214\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-215\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-216\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1023</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mtext>value</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mtext>float64</mtext></mrow></msub><mo>=</mo><mo stretchy=\"false\">(</mo><mo>−</mo><mn>1</mn><msup><mo stretchy=\"false\">)</mo><mi>S</mi></msup><mo>×</mo><mn>1.</mn><msub><mi>M</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>52</mn></mrow></msub><mo>×</mo><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>E</mi><mo>−</mo><mn>1023</mn></mrow></msup></math></span></span></div>\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">float64</code> is typically used in scientific computing, numerical simulations, and applications requiring high precision. It is <strong>rarely used</strong> in deep learning because its benefits are minimal for most ML tasks, while the compute and memory costs are high.</li>\n</ul>\n<h4 id=\"brain-floating-point-bfloat16\">Brain Floating Point (<code class=\"language-plaintext Highlighter-rouge\">bfloat16</code>)</h4>\n<ul>\n  <li><strong>Bit layout</strong>: 1 sign bit, 8 exponent bits, 7 mantissa bits</li>\n  <li><strong>Total bits</strong>: 16</li>\n  <li><strong>Exponent bias</strong>: 127 (same as <code class=\"language-plaintext highlighter-rouge\">float32</code>)</li>\n  <li><strong>Dynamic range</strong>: Approximately <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1.2</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>38</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-217\" style=\"width: 5.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.638em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1004.64em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-218\"><span class=\"mn\" id=\"MathJax-Span-219\" style=\"font-family: STIXGeneral-Regular;\">1.2</span><span class=\"mo\" id=\"MathJax-Span-220\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-221\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-222\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-223\"><span class=\"mrow\" id=\"MathJax-Span-224\"><span class=\"mo\" id=\"MathJax-Span-225\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-226\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">38</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>1.2</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>38</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">1.2 \\times 10^{-38}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>3.4</mn><mo>&amp;#x00D7;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>38</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-227\" style=\"width: 4.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1004.12em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-228\"><span class=\"mn\" id=\"MathJax-Span-229\" style=\"font-family: STIXGeneral-Regular;\">3.4</span><span class=\"mo\" id=\"MathJax-Span-230\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-231\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-232\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-233\"><span class=\"mrow\" id=\"MathJax-Span-234\"><span class=\"mn\" id=\"MathJax-Span-235\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">38</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>3.4</mn><mo>×</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>38</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">3.4 \\times 10^{38}</script></li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mtext>value</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>bfloat16</mtext></mrow></msub><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mo>&amp;#x2212;</mo><mn>1</mn><msup><mo stretchy=&quot;false&quot;>)</mo><mi>S</mi></msup><mo>&amp;#x00D7;</mo><mn>1.</mn><msub><mi>M</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>7</mn></mrow></msub><mo>&amp;#x00D7;</mo><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>E</mi><mo>&amp;#x2212;</mo><mn>127</mn></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-236\" style=\"width: 17.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.69em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1014.69em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-237\"><span class=\"msubsup\" id=\"MathJax-Span-238\"><span style=\"display: inline-block; position: relative; width: 4.586em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1002.14em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mtext\" id=\"MathJax-Span-239\" style=\"font-family: STIXGeneral-Regular;\">value</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 2.19em;\"><span class=\"texatom\" id=\"MathJax-Span-240\"><span class=\"mrow\" id=\"MathJax-Span-241\"><span class=\"mtext\" id=\"MathJax-Span-242\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">bfloat16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-243\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mo\" id=\"MathJax-Span-245\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-246\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"msubsup\" id=\"MathJax-Span-247\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-248\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-249\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-250\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-251\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1.</span><span class=\"msubsup\" id=\"MathJax-Span-252\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-253\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-254\"><span class=\"mrow\" id=\"MathJax-Span-255\"><span class=\"mn\" id=\"MathJax-Span-256\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">7</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-257\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-258\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-259\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-260\"><span class=\"mrow\" id=\"MathJax-Span-261\"><span class=\"mi\" id=\"MathJax-Span-262\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-263\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-264\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">127</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mtext>value</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mtext>bfloat16</mtext></mrow></msub><mo>=</mo><mo stretchy=\"false\">(</mo><mo>−</mo><mn>1</mn><msup><mo stretchy=\"false\">)</mo><mi>S</mi></msup><mo>×</mo><mn>1.</mn><msub><mi>M</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>7</mn></mrow></msub><mo>×</mo><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>E</mi><mo>−</mo><mn>127</mn></mrow></msup></math></span></span></div>\n<ul>\n  <li>\n    <p><code class=\"language-plaintext highlighter-rouge\">bfloat16</code> (Brain Floating Point 16) was introduced by Google for training deep neural networks. Unlike <code class=\"language-plaintext highlighter-rouge\">float16</code>, which reduces both exponent and mantissa bits, <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> keeps the <strong>same exponent width as <code class=\"language-plaintext highlighter-rouge\">float32</code></strong> (8 bits) but reduces the mantissa to 7 bits.</p>\n  </li>\n  <li>\n    <p>This design retains the <strong>dynamic range</strong> of <code class=\"language-plaintext highlighter-rouge\">float32</code>, which makes it far more robust to underflow/overflow issues during training compared to <code class=\"language-plaintext highlighter-rouge\">float16</code>.</p>\n  </li>\n  <li>\n    <p>However, the <strong>precision is reduced</strong>, since fewer mantissa bits mean fewer significant digits are preserved. Despite this, it performs well in practice for training large models.</p>\n  </li>\n  <li>\n    <p><code class=\"language-plaintext highlighter-rouge\">bfloat16</code> is ideal for training tasks where:</p>\n\n    <ul>\n      <li>High dynamic range is important</li>\n      <li>Some loss of precision can be tolerated (e.g., in early layers or gradients)</li>\n      <li>Lower memory and compute overhead is desired compared to <code class=\"language-plaintext highlighter-rouge\">float32</code></li>\n    </ul>\n  </li>\n  <li>\n    <p>It is commonly used in mixed-precision training, often with accumulations in <code class=\"language-plaintext highlighter-rouge\">float32</code> to improve numerical stability.</p>\n  </li>\n  <li>\n    <p><strong>Use cases</strong>:</p>\n\n    <ul>\n      <li>Large-scale model training (e.g., LLMs)</li>\n      <li>TPUs (Google Cloud), and newer GPUs from NVIDIA, AMD, and Intel that support native <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> ops</li>\n    </ul>\n  </li>\n</ul>\n<p><code class=\"language-plaintext highlighter-rouge\">bfloat16</code> (Brain Floating Point 16) was introduced by Google for training deep neural networks. Unlike <code class=\"language-plaintext highlighter-rouge\">float16</code>, which reduces both exponent and mantissa bits, <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> keeps the <strong>same exponent width as <code class=\"language-plaintext highlighter-rouge\">float32</code></strong> (8 bits) but reduces the mantissa to 7 bits.</p>\n<p>This design retains the <strong>dynamic range</strong> of <code class=\"language-plaintext highlighter-rouge\">float32</code>, which makes it far more robust to underflow/overflow issues during training compared to <code class=\"language-plaintext highlighter-rouge\">float16</code>.</p>\n<p>However, the <strong>precision is reduced</strong>, since fewer mantissa bits mean fewer significant digits are preserved. Despite this, it performs well in practice for training large models.</p>\n<p><code class=\"language-plaintext highlighter-rouge\">bfloat16</code> is ideal for training tasks where:</p>\n<ul>\n      <li>High dynamic range is important</li>\n      <li>Some loss of precision can be tolerated (e.g., in early layers or gradients)</li>\n      <li>Lower memory and compute overhead is desired compared to <code class=\"language-plaintext highlighter-rouge\">float32</code></li>\n    </ul>\n<p>It is commonly used in mixed-precision training, often with accumulations in <code class=\"language-plaintext highlighter-rouge\">float32</code> to improve numerical stability.</p>\n<p><strong>Use cases</strong>:</p>\n<ul>\n      <li>Large-scale model training (e.g., LLMs)</li>\n      <li>TPUs (Google Cloud), and newer GPUs from NVIDIA, AMD, and Intel that support native <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> ops</li>\n    </ul>\n<h4 id=\"gputpu-considerations\">GPU/TPU Considerations</h4>\n<ul>\n  <li>\n    <p><strong>float16</strong>:</p>\n\n    <ul>\n      <li>Supported by specialized hardware units (Tensor Cores) in NVIDIA Volta, Turing, Ampere, Hopper, and Blackwell architectures.</li>\n      <li>Offers higher throughput and lower memory usage, especially during inference.</li>\n      <li>Typically used in mixed-precision training with <code class=\"language-plaintext highlighter-rouge\">float32</code> accumulations to maintain stability.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>bfloat16</strong>:</p>\n\n    <ul>\n      <li>Natively supported on Google TPUs, NVIDIA Ampere and newer (e.g., A100, H100), Intel Habana Gaudi accelerators, and select AMD GPUs.</li>\n      <li>Enables <strong>high dynamic range</strong> similar to <code class=\"language-plaintext highlighter-rouge\">float32</code> while halving memory usage.</li>\n      <li>Increasingly adopted in training large models where <code class=\"language-plaintext highlighter-rouge\">float16</code> may encounter stability issues.</li>\n      <li>Like <code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> is also used in mixed-precision training pipelines.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>float32</strong>:</p>\n\n    <ul>\n      <li>Universally supported across all GPU architectures.</li>\n      <li>Offers the best balance between range and precision.</li>\n      <li>Slower and more memory-intensive compared to <code class=\"language-plaintext highlighter-rouge\">float16</code> and <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>float64</strong>:</p>\n\n    <ul>\n      <li>Rare in deep learning; primarily used in scientific computing.</li>\n      <li>Most GPUs support it at much lower throughput.</li>\n      <li>Often omitted entirely from inference workloads due to cost.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>float16</strong>:</p>\n<ul>\n      <li>Supported by specialized hardware units (Tensor Cores) in NVIDIA Volta, Turing, Ampere, Hopper, and Blackwell architectures.</li>\n      <li>Offers higher throughput and lower memory usage, especially during inference.</li>\n      <li>Typically used in mixed-precision training with <code class=\"language-plaintext highlighter-rouge\">float32</code> accumulations to maintain stability.</li>\n    </ul>\n<p><strong>bfloat16</strong>:</p>\n<ul>\n      <li>Natively supported on Google TPUs, NVIDIA Ampere and newer (e.g., A100, H100), Intel Habana Gaudi accelerators, and select AMD GPUs.</li>\n      <li>Enables <strong>high dynamic range</strong> similar to <code class=\"language-plaintext highlighter-rouge\">float32</code> while halving memory usage.</li>\n      <li>Increasingly adopted in training large models where <code class=\"language-plaintext highlighter-rouge\">float16</code> may encounter stability issues.</li>\n      <li>Like <code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> is also used in mixed-precision training pipelines.</li>\n    </ul>\n<p><strong>float32</strong>:</p>\n<ul>\n      <li>Universally supported across all GPU architectures.</li>\n      <li>Offers the best balance between range and precision.</li>\n      <li>Slower and more memory-intensive compared to <code class=\"language-plaintext highlighter-rouge\">float16</code> and <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>.</li>\n    </ul>\n<p><strong>float64</strong>:</p>\n<ul>\n      <li>Rare in deep learning; primarily used in scientific computing.</li>\n      <li>Most GPUs support it at much lower throughput.</li>\n      <li>Often omitted entirely from inference workloads due to cost.</li>\n    </ul>\n<h4 id=\"comparative-summary\">Comparative Summary</h4>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Format</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Bits</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Exponent Bits</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Mantissa Bits</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Bias</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Range (Approx.)</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>GPU Usage</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">float16</td>\n<td class=\"tg-tleft-valign-first\">16</td>\n<td class=\"tg-tleft-valign-first\">5</td>\n<td class=\"tg-tleft-valign-first\">10</td>\n<td class=\"tg-tleft-valign-first\">15</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>5</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-265\" style=\"width: 2.324em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.908em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1001.91em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-266\"><span class=\"msubsup\" id=\"MathJax-Span-267\"><span style=\"display: inline-block; position: relative; width: 1.908em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-268\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-269\"><span class=\"mrow\" id=\"MathJax-Span-270\"><span class=\"mo\" id=\"MathJax-Span-271\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-272\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">10^{-5}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>4</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-273\" style=\"width: 1.729em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.432em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1001.43em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-274\"><span class=\"msubsup\" id=\"MathJax-Span-275\"><span style=\"display: inline-block; position: relative; width: 1.432em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-276\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-277\"><span class=\"mrow\" id=\"MathJax-Span-278\"><span class=\"mn\" id=\"MathJax-Span-279\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>4</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">10^{4}</script></td>\n<td class=\"tg-tleft-valign-second\">Fast inference, mixed precision</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">bfloat16</td>\n<td class=\"tg-tleft-valign-first\">16</td>\n<td class=\"tg-tleft-valign-first\">8</td>\n<td class=\"tg-tleft-valign-first\">7</td>\n<td class=\"tg-tleft-valign-first\">127</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>38</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-280\" style=\"width: 2.741em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.265em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1002.26em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-281\"><span class=\"msubsup\" id=\"MathJax-Span-282\"><span style=\"display: inline-block; position: relative; width: 2.265em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-283\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-284\"><span class=\"mrow\" id=\"MathJax-Span-285\"><span class=\"mo\" id=\"MathJax-Span-286\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-287\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">38</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>38</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">10^{-38}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>38</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-288\" style=\"width: 2.146em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.789em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1001.79em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-289\"><span class=\"msubsup\" id=\"MathJax-Span-290\"><span style=\"display: inline-block; position: relative; width: 1.789em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-291\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-292\"><span class=\"mrow\" id=\"MathJax-Span-293\"><span class=\"mn\" id=\"MathJax-Span-294\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">38</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>38</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">10^{38}</script></td>\n<td class=\"tg-tleft-valign-second\">Training + inference, high range</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">float32</td>\n<td class=\"tg-tleft-valign-first\">32</td>\n<td class=\"tg-tleft-valign-first\">8</td>\n<td class=\"tg-tleft-valign-first\">23</td>\n<td class=\"tg-tleft-valign-first\">127</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>45</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-295\" style=\"width: 2.741em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.265em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1002.26em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-296\"><span class=\"msubsup\" id=\"MathJax-Span-297\"><span style=\"display: inline-block; position: relative; width: 2.265em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-298\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-299\"><span class=\"mrow\" id=\"MathJax-Span-300\"><span class=\"mo\" id=\"MathJax-Span-301\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-302\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">45</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>45</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">10^{-45}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>38</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-303\" style=\"width: 2.146em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.789em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1001.79em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-304\"><span class=\"msubsup\" id=\"MathJax-Span-305\"><span style=\"display: inline-block; position: relative; width: 1.789em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-307\"><span class=\"mrow\" id=\"MathJax-Span-308\"><span class=\"mn\" id=\"MathJax-Span-309\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">38</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>38</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">10^{38}</script></td>\n<td class=\"tg-tleft-valign-second\">Default for training</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">float64</td>\n<td class=\"tg-tleft-valign-first\">64</td>\n<td class=\"tg-tleft-valign-first\">11</td>\n<td class=\"tg-tleft-valign-first\">52</td>\n<td class=\"tg-tleft-valign-first\">1023</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>324</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-310\" style=\"width: 3.158em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.622em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1002.62em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-311\"><span class=\"msubsup\" id=\"MathJax-Span-312\"><span style=\"display: inline-block; position: relative; width: 2.622em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-313\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-314\"><span class=\"mrow\" id=\"MathJax-Span-315\"><span class=\"mo\" id=\"MathJax-Span-316\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-317\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">324</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>324</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">10^{-324}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>308</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-318\" style=\"width: 2.622em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.146em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1002.15em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-319\"><span class=\"msubsup\" id=\"MathJax-Span-320\"><span style=\"display: inline-block; position: relative; width: 2.146em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-321\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-322\"><span class=\"mrow\" id=\"MathJax-Span-323\"><span class=\"mn\" id=\"MathJax-Span-324\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">308</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>308</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">10^{308}</script></td>\n<td class=\"tg-tleft-valign-second\">Rare in ML, slow on GPU</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Format</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Bits</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Exponent Bits</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Mantissa Bits</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Bias</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Range (Approx.)</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>GPU Usage</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">float16</td>\n<td class=\"tg-tleft-valign-first\">16</td>\n<td class=\"tg-tleft-valign-first\">5</td>\n<td class=\"tg-tleft-valign-first\">10</td>\n<td class=\"tg-tleft-valign-first\">15</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>5</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-265\" style=\"width: 2.324em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.908em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1001.91em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-266\"><span class=\"msubsup\" id=\"MathJax-Span-267\"><span style=\"display: inline-block; position: relative; width: 1.908em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-268\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-269\"><span class=\"mrow\" id=\"MathJax-Span-270\"><span class=\"mo\" id=\"MathJax-Span-271\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-272\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">10^{-5}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>4</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-273\" style=\"width: 1.729em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.432em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1001.43em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-274\"><span class=\"msubsup\" id=\"MathJax-Span-275\"><span style=\"display: inline-block; position: relative; width: 1.432em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-276\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-277\"><span class=\"mrow\" id=\"MathJax-Span-278\"><span class=\"mn\" id=\"MathJax-Span-279\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>4</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">10^{4}</script></td>\n<td class=\"tg-tleft-valign-second\">Fast inference, mixed precision</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">bfloat16</td>\n<td class=\"tg-tleft-valign-first\">16</td>\n<td class=\"tg-tleft-valign-first\">8</td>\n<td class=\"tg-tleft-valign-first\">7</td>\n<td class=\"tg-tleft-valign-first\">127</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>38</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-280\" style=\"width: 2.741em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.265em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1002.26em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-281\"><span class=\"msubsup\" id=\"MathJax-Span-282\"><span style=\"display: inline-block; position: relative; width: 2.265em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-283\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-284\"><span class=\"mrow\" id=\"MathJax-Span-285\"><span class=\"mo\" id=\"MathJax-Span-286\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-287\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">38</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>38</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">10^{-38}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>38</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-288\" style=\"width: 2.146em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.789em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1001.79em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-289\"><span class=\"msubsup\" id=\"MathJax-Span-290\"><span style=\"display: inline-block; position: relative; width: 1.789em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-291\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-292\"><span class=\"mrow\" id=\"MathJax-Span-293\"><span class=\"mn\" id=\"MathJax-Span-294\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">38</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>38</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">10^{38}</script></td>\n<td class=\"tg-tleft-valign-second\">Training + inference, high range</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">float32</td>\n<td class=\"tg-tleft-valign-first\">32</td>\n<td class=\"tg-tleft-valign-first\">8</td>\n<td class=\"tg-tleft-valign-first\">23</td>\n<td class=\"tg-tleft-valign-first\">127</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>45</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-295\" style=\"width: 2.741em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.265em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1002.26em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-296\"><span class=\"msubsup\" id=\"MathJax-Span-297\"><span style=\"display: inline-block; position: relative; width: 2.265em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-298\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-299\"><span class=\"mrow\" id=\"MathJax-Span-300\"><span class=\"mo\" id=\"MathJax-Span-301\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-302\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">45</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>45</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">10^{-45}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>38</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-303\" style=\"width: 2.146em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.789em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1001.79em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-304\"><span class=\"msubsup\" id=\"MathJax-Span-305\"><span style=\"display: inline-block; position: relative; width: 1.789em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-307\"><span class=\"mrow\" id=\"MathJax-Span-308\"><span class=\"mn\" id=\"MathJax-Span-309\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">38</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>38</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">10^{38}</script></td>\n<td class=\"tg-tleft-valign-second\">Default for training</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">float64</td>\n<td class=\"tg-tleft-valign-first\">64</td>\n<td class=\"tg-tleft-valign-first\">11</td>\n<td class=\"tg-tleft-valign-first\">52</td>\n<td class=\"tg-tleft-valign-first\">1023</td>\n<td class=\"tg-tleft-valign-first\"><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>324</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-310\" style=\"width: 3.158em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.622em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1002.62em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-311\"><span class=\"msubsup\" id=\"MathJax-Span-312\"><span style=\"display: inline-block; position: relative; width: 2.622em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-313\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-314\"><span class=\"mrow\" id=\"MathJax-Span-315\"><span class=\"mo\" id=\"MathJax-Span-316\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-317\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">324</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>324</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">10^{-324}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>308</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-318\" style=\"width: 2.622em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.146em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.134em, 1002.15em, 2.384em, -999.997em); top: -2.199em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-319\"><span class=\"msubsup\" id=\"MathJax-Span-320\"><span style=\"display: inline-block; position: relative; width: 2.146em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.158em, 1000.96em, 4.17em, -999.997em); top: -3.985em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-321\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span><span style=\"position: absolute; top: -4.402em; left: 1.015em;\"><span class=\"texatom\" id=\"MathJax-Span-322\"><span class=\"mrow\" id=\"MathJax-Span-323\"><span class=\"mn\" id=\"MathJax-Span-324\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">308</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.991em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.205em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.068em; border-left: 0px solid; width: 0px; height: 1.218em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>308</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">10^{308}</script></td>\n<td class=\"tg-tleft-valign-second\">Rare in ML, slow on GPU</td>\n</tr>\n</tbody>\n</table>\n<ul>\n  <li>This foundation in floating point formats prepares us to understand quantization—where bit-widths are reduced even further (e.g., 8-bit, 4-bit, or binary)—to achieve efficient computation with minimal loss in model performance.</li>\n</ul>",
      "contentMarkdown": "*   Before diving into quantization, it’s essential to understand precision—specifically, how computers represent decimal numbers like `1.0151` or `566132.8`. Since we can conceive of infinitely precise values (like π), but only have limited space in memory, there’s a fundamental trade-off between **precision** (how many significant digits can be stored) and **size** (how many bits are used to represent the number).\n    \n*   In computer engineering, these values are stored as **floating point numbers**, governed by the [IEEE 754 floating point standard](#ieee-754-floating-point-standard). This specification defines how bits are allocated to represent the **sign**, **exponent**, and **mantissa** (also called the **significand**, which holds the meaningful digits).\n    \n*   Floating point formats vary by their bit-width, and each level of precision has a different rounding error margin:\n    \n    *   **Double-precision (`fp64`)** – 64 bits, max rounding error of approximately 2−522−522^{-52}.\n    *   **Single-precision (`float32`)** – 32 bits, max rounding error of approximately 2−232−232^{-23}.\n    *   **Half-precision (`float16`)** – 16 bits, max rounding error of approximately 2−102−102^{-10}.\n*   For a deeper exploration, check out the [PyCon 2019 talk: “Floats are Friends: making the most of IEEE754.00000000000000002”](https://www.youtube.com/watch?v=zguLmgYWhM0).\n    \n*   In practice:\n    \n    *   **Python** defaults to using `fp64` for its `float` type.\n    *   **PyTorch**, which is optimized for performance and memory efficiency, defaults to `float32`.\n*   Understanding these formats is crucial when moving on to concepts like **mixed precision training**, where models leverage different floating point types to balance performance and accuracy.\n    \n\nBefore diving into quantization, it’s essential to understand precision—specifically, how computers represent decimal numbers like `1.0151` or `566132.8`. Since we can conceive of infinitely precise values (like π), but only have limited space in memory, there’s a fundamental trade-off between **precision** (how many significant digits can be stored) and **size** (how many bits are used to represent the number).\n\nIn computer engineering, these values are stored as **floating point numbers**, governed by the [IEEE 754 floating point standard](#ieee-754-floating-point-standard). This specification defines how bits are allocated to represent the **sign**, **exponent**, and **mantissa** (also called the **significand**, which holds the meaningful digits).\n\nFloating point formats vary by their bit-width, and each level of precision has a different rounding error margin:\n\n*   **Double-precision (`fp64`)** – 64 bits, max rounding error of approximately 2−522−522^{-52}.\n*   **Single-precision (`float32`)** – 32 bits, max rounding error of approximately 2−232−232^{-23}.\n*   **Half-precision (`float16`)** – 16 bits, max rounding error of approximately 2−102−102^{-10}.\n\nFor a deeper exploration, check out the [PyCon 2019 talk: “Floats are Friends: making the most of IEEE754.00000000000000002”](https://www.youtube.com/watch?v=zguLmgYWhM0).\n\nIn practice:\n\n*   **Python** defaults to using `fp64` for its `float` type.\n*   **PyTorch**, which is optimized for performance and memory efficiency, defaults to `float32`.\n\nUnderstanding these formats is crucial when moving on to concepts like **mixed precision training**, where models leverage different floating point types to balance performance and accuracy.\n\n#### IEEE 754 Floating Point Standard\n\n*   The IEEE 754 standard defines the binary representation of floating point numbers used in nearly all modern hardware and programming environments. A floating-point number is composed of three parts:\n    \n    *   **Sign bit (SSS)**: 1 bit indicating positive or negative.\n    *   **Exponent (EEE)**: Encodes the range (scale) of the number.\n    *   **Mantissa or significand (MMM)**: Encodes the precision.\n*   The general representation for a binary floating-point number is:\n    \n\nThe IEEE 754 standard defines the binary representation of floating point numbers used in nearly all modern hardware and programming environments. A floating-point number is composed of three parts:\n\n*   **Sign bit (SSS)**: 1 bit indicating positive or negative.\n*   **Exponent (EEE)**: Encodes the range (scale) of the number.\n*   **Mantissa or significand (MMM)**: Encodes the precision.\n\nThe general representation for a binary floating-point number is:\n\nvalue\\=(−1)S×1.M×2E−biasvalue\\=(−1)S×1.M×2E−bias\n\n*   Each format—half, single, and double—allocates a different number of bits to these components, balancing precision and range against memory usage and compute requirements.\n    \n*   The following figure [(source)](https://blogs.nvidia.com/blog/2019/11/15/whats-the-difference-between-single-double-multi-and-mixed-precision-computing/) shows the IEEE 754 standard formats for floating-point numbers, illustrating the bitwise layout of the signed bit, exponent, and significand across double (64-bit), single (32-bit), and half (16-bit) precision representations.\n    \n\nEach format—half, single, and double—allocates a different number of bits to these components, balancing precision and range against memory usage and compute requirements.\n\nThe following figure [(source)](https://blogs.nvidia.com/blog/2019/11/15/whats-the-difference-between-single-double-multi-and-mixed-precision-computing/) shows the IEEE 754 standard formats for floating-point numbers, illustrating the bitwise layout of the signed bit, exponent, and significand across double (64-bit), single (32-bit), and half (16-bit) precision representations.\n\n![](/primers/ai/assets/model-compression/ieee_formats.webp)\n\n#### Half-Precision (`float16`)\n\n*   **Bit layout**: 1 sign bit, 5 exponent bits, 10 mantissa/significand bits.\n*   **Total bits**: 16\n*   **Exponent bias**: 15\n*   **Dynamic range**: Approximately 6×10−56×10−56 \\\\times 10^{-5} to 6.5×1046.5×1046.5 \\\\times 10^4\n\nvaluefloat16\\=(−1)S×1.M10×2E−15valuefloat16\\=(−1)S×1.M10×2E−15\n\n*   Half-precision is mostly used during inference, especially in low-power or memory-constrained environments such as mobile devices or embedded hardware. It offers limited range and precision, and is generally _not_ suitable for training deep networks unless special care is taken (e.g., using loss scaling or `float32` accumulations).\n    \n*   **GPU Considerations**:\n    \n    *   Many GPUs, like NVIDIA’s Volta, Turing, Ampere, Hopper, Blackwell, include specialized hardware units called **Tensor Cores** optimized for `float16` operations.\n    *   `float16` can be processed at higher throughput than `float32`, enabling significant speedups for matrix multiplications during inference.\n    *   Often paired with Mixed-Precision Training (MPT), where activations and weights are stored in `float16`, but gradients are accumulated in `float32`.\n\nHalf-precision is mostly used during inference, especially in low-power or memory-constrained environments such as mobile devices or embedded hardware. It offers limited range and precision, and is generally _not_ suitable for training deep networks unless special care is taken (e.g., using loss scaling or `float32` accumulations).\n\n**GPU Considerations**:\n\n*   Many GPUs, like NVIDIA’s Volta, Turing, Ampere, Hopper, Blackwell, include specialized hardware units called **Tensor Cores** optimized for `float16` operations.\n*   `float16` can be processed at higher throughput than `float32`, enabling significant speedups for matrix multiplications during inference.\n*   Often paired with Mixed-Precision Training (MPT), where activations and weights are stored in `float16`, but gradients are accumulated in `float32`.\n\n#### Single-Precision (`float32`)\n\n*   **Bit layout**: 1 sign bit, 8 exponent bits, 23 mantissa bits.\n*   **Total bits**: 32\n*   **Exponent bias**: 127\n*   **Dynamic range**: Approximately 1.4×10−451.4×10−451.4 \\\\times 10^{-45} to 3.4×10383.4×10383.4 \\\\times 10^{38}\n\nvaluefloat32\\=(−1)S×1.M23×2E−127valuefloat32\\=(−1)S×1.M23×2E−127\n\n*   `float32` is the default numerical format for most deep learning frameworks and hardware. It provides a good balance between precision and range, making it robust for both training and inference.\n    \n*   **GPU Considerations**:\n    \n    *   Supported natively on all modern GPUs.\n    *   Most general-purpose ALUs (Arithmetic Logic Units) on the GPU are designed to process `float32` efficiently.\n    *   Slower than `float16` or `bfloat16` in terms of throughput and power usage but more accurate.\n\n`float32` is the default numerical format for most deep learning frameworks and hardware. It provides a good balance between precision and range, making it robust for both training and inference.\n\n**GPU Considerations**:\n\n*   Supported natively on all modern GPUs.\n*   Most general-purpose ALUs (Arithmetic Logic Units) on the GPU are designed to process `float32` efficiently.\n*   Slower than `float16` or `bfloat16` in terms of throughput and power usage but more accurate.\n\n#### Double-Precision (`float64`)\n\n*   **Bit layout**: 1 sign bit, 11 exponent bits, 52 mantissa bits.\n*   **Total bits**: 64\n*   **Exponent bias**: 1023\n*   **Dynamic range**: Approximately 5×10−3245×10−3245 \\\\times 10^{-324} to 1.8×103081.8×103081.8 \\\\times 10^{308}\n\nvaluefloat64\\=(−1)S×1.M52×2E−1023valuefloat64\\=(−1)S×1.M52×2E−1023\n\n*   `float64` is typically used in scientific computing, numerical simulations, and applications requiring high precision. It is **rarely used** in deep learning because its benefits are minimal for most ML tasks, while the compute and memory costs are high.\n\n#### Brain Floating Point (`bfloat16`)\n\n*   **Bit layout**: 1 sign bit, 8 exponent bits, 7 mantissa bits\n*   **Total bits**: 16\n*   **Exponent bias**: 127 (same as `float32`)\n*   **Dynamic range**: Approximately 1.2×10−381.2×10−381.2 \\\\times 10^{-38} to 3.4×10383.4×10383.4 \\\\times 10^{38}\n\nvaluebfloat16\\=(−1)S×1.M7×2E−127valuebfloat16\\=(−1)S×1.M7×2E−127\n\n*   `bfloat16` (Brain Floating Point 16) was introduced by Google for training deep neural networks. Unlike `float16`, which reduces both exponent and mantissa bits, `bfloat16` keeps the **same exponent width as `float32`** (8 bits) but reduces the mantissa to 7 bits.\n    \n*   This design retains the **dynamic range** of `float32`, which makes it far more robust to underflow/overflow issues during training compared to `float16`.\n    \n*   However, the **precision is reduced**, since fewer mantissa bits mean fewer significant digits are preserved. Despite this, it performs well in practice for training large models.\n    \n*   `bfloat16` is ideal for training tasks where:\n    \n    *   High dynamic range is important\n    *   Some loss of precision can be tolerated (e.g., in early layers or gradients)\n    *   Lower memory and compute overhead is desired compared to `float32`\n*   It is commonly used in mixed-precision training, often with accumulations in `float32` to improve numerical stability.\n    \n*   **Use cases**:\n    \n    *   Large-scale model training (e.g., LLMs)\n    *   TPUs (Google Cloud), and newer GPUs from NVIDIA, AMD, and Intel that support native `bfloat16` ops\n\n`bfloat16` (Brain Floating Point 16) was introduced by Google for training deep neural networks. Unlike `float16`, which reduces both exponent and mantissa bits, `bfloat16` keeps the **same exponent width as `float32`** (8 bits) but reduces the mantissa to 7 bits.\n\nThis design retains the **dynamic range** of `float32`, which makes it far more robust to underflow/overflow issues during training compared to `float16`.\n\nHowever, the **precision is reduced**, since fewer mantissa bits mean fewer significant digits are preserved. Despite this, it performs well in practice for training large models.\n\n`bfloat16` is ideal for training tasks where:\n\n*   High dynamic range is important\n*   Some loss of precision can be tolerated (e.g., in early layers or gradients)\n*   Lower memory and compute overhead is desired compared to `float32`\n\nIt is commonly used in mixed-precision training, often with accumulations in `float32` to improve numerical stability.\n\n**Use cases**:\n\n*   Large-scale model training (e.g., LLMs)\n*   TPUs (Google Cloud), and newer GPUs from NVIDIA, AMD, and Intel that support native `bfloat16` ops\n\n#### GPU/TPU Considerations\n\n*   **float16**:\n    \n    *   Supported by specialized hardware units (Tensor Cores) in NVIDIA Volta, Turing, Ampere, Hopper, and Blackwell architectures.\n    *   Offers higher throughput and lower memory usage, especially during inference.\n    *   Typically used in mixed-precision training with `float32` accumulations to maintain stability.\n*   **bfloat16**:\n    \n    *   Natively supported on Google TPUs, NVIDIA Ampere and newer (e.g., A100, H100), Intel Habana Gaudi accelerators, and select AMD GPUs.\n    *   Enables **high dynamic range** similar to `float32` while halving memory usage.\n    *   Increasingly adopted in training large models where `float16` may encounter stability issues.\n    *   Like `float16`, `bfloat16` is also used in mixed-precision training pipelines.\n*   **float32**:\n    \n    *   Universally supported across all GPU architectures.\n    *   Offers the best balance between range and precision.\n    *   Slower and more memory-intensive compared to `float16` and `bfloat16`.\n*   **float64**:\n    \n    *   Rare in deep learning; primarily used in scientific computing.\n    *   Most GPUs support it at much lower throughput.\n    *   Often omitted entirely from inference workloads due to cost.\n\n**float16**:\n\n*   Supported by specialized hardware units (Tensor Cores) in NVIDIA Volta, Turing, Ampere, Hopper, and Blackwell architectures.\n*   Offers higher throughput and lower memory usage, especially during inference.\n*   Typically used in mixed-precision training with `float32` accumulations to maintain stability.\n\n**bfloat16**:\n\n*   Natively supported on Google TPUs, NVIDIA Ampere and newer (e.g., A100, H100), Intel Habana Gaudi accelerators, and select AMD GPUs.\n*   Enables **high dynamic range** similar to `float32` while halving memory usage.\n*   Increasingly adopted in training large models where `float16` may encounter stability issues.\n*   Like `float16`, `bfloat16` is also used in mixed-precision training pipelines.\n\n**float32**:\n\n*   Universally supported across all GPU architectures.\n*   Offers the best balance between range and precision.\n*   Slower and more memory-intensive compared to `float16` and `bfloat16`.\n\n**float64**:\n\n*   Rare in deep learning; primarily used in scientific computing.\n*   Most GPUs support it at much lower throughput.\n*   Often omitted entirely from inference workloads due to cost.\n\n#### Comparative Summary\n\n**Format**\n\n**Bits**\n\n**Exponent Bits**\n\n**Mantissa Bits**\n\n**Bias**\n\n**Range (Approx.)**\n\n**GPU Usage**\n\nfloat16\n\n16\n\n5\n\n10\n\n15\n\n10−510−510^{-5} to 10410410^{4}\n\nFast inference, mixed precision\n\nbfloat16\n\n16\n\n8\n\n7\n\n127\n\n10−3810−3810^{-38} to 1038103810^{38}\n\nTraining + inference, high range\n\nfloat32\n\n32\n\n8\n\n23\n\n127\n\n10−4510−4510^{-45} to 1038103810^{38}\n\nDefault for training\n\nfloat64\n\n64\n\n11\n\n52\n\n1023\n\n10−32410−32410^{-324} to 103081030810^{308}\n\nRare in ML, slow on GPU\n\n**Format**\n\n**Bits**\n\n**Exponent Bits**\n\n**Mantissa Bits**\n\n**Bias**\n\n**Range (Approx.)**\n\n**GPU Usage**\n\nfloat16\n\n16\n\n5\n\n10\n\n15\n\n10−510−510^{-5} to 10410410^{4}\n\nFast inference, mixed precision\n\nbfloat16\n\n16\n\n8\n\n7\n\n127\n\n10−3810−3810^{-38} to 1038103810^{38}\n\nTraining + inference, high range\n\nfloat32\n\n32\n\n8\n\n23\n\n127\n\n10−4510−4510^{-45} to 1038103810^{38}\n\nDefault for training\n\nfloat64\n\n64\n\n11\n\n52\n\n1023\n\n10−32410−32410^{-324} to 103081030810^{308}\n\nRare in ML, slow on GPU\n\n*   This foundation in floating point formats prepares us to understand quantization—where bit-widths are reduced even further (e.g., 8-bit, 4-bit, or binary)—to achieve efficient computation with minimal loss in model performance.",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 11,
      "tags": [
        "ondevice ai",
        "neural network",
        "deep learning",
        "llm",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": true,
        "wordCount": 2073,
        "contentLength": 127396
      },
      "nextCards": [
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3"
      ],
      "relatedCards": [
        "ai-federated-learning-personalization-22",
        "ai-differential-privacy-tightness-3",
        "ai-federated-learning-pros-cons-19",
        "ai-federated-learning-comparison-use-cases-20",
        "ai-federated-learning-privacy-21"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#background:-precision",
      "scrapedAt": "2025-12-28T11:55:50.966Z",
      "siblings": [
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5",
        "ai-model-compression-quantization-workflows-6"
      ]
    },
    {
      "id": "ai-model-compression-background-matrix-multiplication-in-gpus-2",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Background: Matrix Multiplication in GPUs",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>\n    <p>Efficient matrix multiplication is at the heart of modern deep learning acceleration on GPUs. This section provides a high-level view of how matrix-matrix multiplications are implemented and optimized on GPU hardware, with special focus on tiling, Tensor Cores, and the performance implications of quantization.</p>\n  </li>\n  <li>\n    <p>Matrix multiplications, especially General Matrix Multiplications (GEMMs), are a core computational primitive in deep learning workloads. Whether in fully connected layers, convolutions (via <code class=\"language-plaintext highlighter-rouge\">im2col</code>), or attention mechanisms, these operations are executed billions of times during training and inference. As such, optimizing GEMM performance is essential for efficient neural network execution, particularly on GPUs.</p>\n  </li>\n  <li>\n    <p>To execute GEMMs efficiently, GPUs partition the output matrix into <em>tiles</em>. Each tile corresponds to a submatrix of the result and is computed by a thread block. The GPU steps through the input matrices along the shared dimension (K) in tiles, performing multiply-accumulate operations and writing the results into the corresponding tile of the output matrix. The illustration below <a href=\"http://theaiedge.io/\">(source)</a> shows the tiled outer product approach to GEMMs.</p>\n  </li>\n</ul>\n<p>Efficient matrix multiplication is at the heart of modern deep learning acceleration on GPUs. This section provides a high-level view of how matrix-matrix multiplications are implemented and optimized on GPU hardware, with special focus on tiling, Tensor Cores, and the performance implications of quantization.</p>\n<p>Matrix multiplications, especially General Matrix Multiplications (GEMMs), are a core computational primitive in deep learning workloads. Whether in fully connected layers, convolutions (via <code class=\"language-plaintext highlighter-rouge\">im2col</code>), or attention mechanisms, these operations are executed billions of times during training and inference. As such, optimizing GEMM performance is essential for efficient neural network execution, particularly on GPUs.</p>\n<p>To execute GEMMs efficiently, GPUs partition the output matrix into <em>tiles</em>. Each tile corresponds to a submatrix of the result and is computed by a thread block. The GPU steps through the input matrices along the shared dimension (K) in tiles, performing multiply-accumulate operations and writing the results into the corresponding tile of the output matrix. The illustration below <a href=\"http://theaiedge.io/\">(source)</a> shows the tiled outer product approach to GEMMs.</p>\n<p><img src=\"/primers/ai/assets/model-compression/tiled_outer_product_gemm_placeholder.jpg\" alt=\"Tiled outer product approach to GEMMs\"></p>\n<ul>\n  <li>\n    <p>Thread blocks are mapped to the GPU’s <strong>streaming multiprocessors (SMs)</strong>, the fundamental compute units that execute instructions in parallel. Each SM can process one or more thread blocks concurrently, depending on the available resources and occupancy.</p>\n  </li>\n  <li>\n    <p>Performance in GPU matrix multiplication is often bounded by one of two factors:</p>\n\n    <ul>\n      <li><strong>Compute (math) bound</strong>: When the arithmetic intensity (FLOPs per byte) is high enough that math operations dominate runtime.</li>\n      <li><strong>Memory bound</strong>: When the operation requires frequent memory access compared to math operations, limiting throughput.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Whether a given GEMM is compute- or memory-bound depends on the matrix dimensions (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi><mo>,</mo><mi>N</mi><mo>,</mo><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-325\" style=\"width: 4.169em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.44em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.44em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-326\"><span class=\"mi\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-328\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-329\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-330\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-331\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi><mo>,</mo><mi>N</mi><mo>,</mo><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">M, N, K</script>) and the hardware’s characteristics. For example, matrix-vector products (where either <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-332\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-333\"><span class=\"mi\" id=\"MathJax-Span-334\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">M</script> = 1 or <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-335\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-336\"><span class=\"mi\" id=\"MathJax-Span-337\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">N</script> = 1) are typically memory-bound due to their low arithmetic intensity.</p>\n  </li>\n  <li>\n    <p>Modern NVIDIA GPUs include specialized hardware units called <strong>Tensor Cores</strong>, which are designed to accelerate GEMMs involving low-precision data types such as <code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>, and <code class=\"language-plaintext highlighter-rouge\">int8</code>. Tensor Cores perform small matrix multiplications in parallel and require that the matrices’ dimensions align with certain multiples (e.g., 8 for <code class=\"language-plaintext highlighter-rouge\">float16</code>, 16 for <code class=\"language-plaintext highlighter-rouge\">int8</code>) to achieve peak performance. For instance, on Ampere and newer architectures like Hopper or Blackwell, aligning dimensions to larger multiples (e.g., 64 or 128 elements) often yields even better throughput.</p>\n  </li>\n  <li>\n    <p>Matrix dimensions that are not aligned to tile sizes lead to <strong>tile quantization</strong>, where some tiles carry less useful work, reducing efficiency. Similarly, if the total number of tiles is not an even multiple of the number of GPU SMs, <strong>wave quantization</strong> can cause underutilization. Both effects can significantly degrade performance despite identical algorithmic complexity.</p>\n  </li>\n  <li>\n    <p>To address this, libraries like cuBLAS employ heuristics or benchmarking to select optimal tile sizes, balancing between tile reuse (large tiles) and parallelism (many small tiles). Larger tiles tend to be more efficient due to better data reuse, but may reduce parallel occupancy on smaller problems.</p>\n  </li>\n  <li>\n    <p>In summary, matrix multiplication performance on GPUs is a delicate balance between compute, memory bandwidth, and architecture-aware tiling strategies. Quantization not only affects data representation but also interacts intricately with the underlying matrix multiplication engine and GPU efficiency.</p>\n  </li>\n</ul>\n<p>Thread blocks are mapped to the GPU’s <strong>streaming multiprocessors (SMs)</strong>, the fundamental compute units that execute instructions in parallel. Each SM can process one or more thread blocks concurrently, depending on the available resources and occupancy.</p>\n<p>Performance in GPU matrix multiplication is often bounded by one of two factors:</p>\n<ul>\n      <li><strong>Compute (math) bound</strong>: When the arithmetic intensity (FLOPs per byte) is high enough that math operations dominate runtime.</li>\n      <li><strong>Memory bound</strong>: When the operation requires frequent memory access compared to math operations, limiting throughput.</li>\n    </ul>\n<p>Whether a given GEMM is compute- or memory-bound depends on the matrix dimensions (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi><mo>,</mo><mi>N</mi><mo>,</mo><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-325\" style=\"width: 4.169em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.44em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.44em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-326\"><span class=\"mi\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-328\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-329\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-330\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-331\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi><mo>,</mo><mi>N</mi><mo>,</mo><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">M, N, K</script>) and the hardware’s characteristics. For example, matrix-vector products (where either <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-332\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-333\"><span class=\"mi\" id=\"MathJax-Span-334\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">M</script> = 1 or <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-335\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-336\"><span class=\"mi\" id=\"MathJax-Span-337\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">N</script> = 1) are typically memory-bound due to their low arithmetic intensity.</p>\n<p>Modern NVIDIA GPUs include specialized hardware units called <strong>Tensor Cores</strong>, which are designed to accelerate GEMMs involving low-precision data types such as <code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>, and <code class=\"language-plaintext highlighter-rouge\">int8</code>. Tensor Cores perform small matrix multiplications in parallel and require that the matrices’ dimensions align with certain multiples (e.g., 8 for <code class=\"language-plaintext highlighter-rouge\">float16</code>, 16 for <code class=\"language-plaintext highlighter-rouge\">int8</code>) to achieve peak performance. For instance, on Ampere and newer architectures like Hopper or Blackwell, aligning dimensions to larger multiples (e.g., 64 or 128 elements) often yields even better throughput.</p>\n<p>Matrix dimensions that are not aligned to tile sizes lead to <strong>tile quantization</strong>, where some tiles carry less useful work, reducing efficiency. Similarly, if the total number of tiles is not an even multiple of the number of GPU SMs, <strong>wave quantization</strong> can cause underutilization. Both effects can significantly degrade performance despite identical algorithmic complexity.</p>\n<p>To address this, libraries like cuBLAS employ heuristics or benchmarking to select optimal tile sizes, balancing between tile reuse (large tiles) and parallelism (many small tiles). Larger tiles tend to be more efficient due to better data reuse, but may reduce parallel occupancy on smaller problems.</p>\n<p>In summary, matrix multiplication performance on GPUs is a delicate balance between compute, memory bandwidth, and architecture-aware tiling strategies. Quantization not only affects data representation but also interacts intricately with the underlying matrix multiplication engine and GPU efficiency.</p>\n<h4 id=\"under-the-hood\">Under-the-hood</h4>\n<ul>\n  <li>Modern GPUs are capable of performing numerical computations more efficiently using 16-bit or 8-bit formats—such as <code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>, and the emerging <code class=\"language-plaintext highlighter-rouge\">float8</code>, <code class=\"language-plaintext highlighter-rouge\">float6</code>, and <code class=\"language-plaintext highlighter-rouge\">float4</code>—with minimal loss in model performance. Mixed precision training strategically leverages these lower-precision formats to accelerate computation and reduce memory consumption, while preserving high-precision (<code class=\"language-plaintext highlighter-rouge\">float32</code>) for numerically sensitive variables and operations to ensure convergence and model integrity (cf. numerical stability in the section on <a href=\"#overview\">Mixed Precision Overview</a>).</li>\n  <li>NVIDIA’s GPUs, from Volta onward, offer specialized hardware units known as <strong>Tensor Cores</strong>. These units are optimized for dense matrix operations and drastically improve throughput when leveraging reduced-precision data types. For developers using PyTorch, the <a href=\"https://pytorch.org/docs/stable/amp.html\"><code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp</code></a> module offers automatic mixed-precision training functionality, simplifying adoption with minimal code edits. This automates casting, loss scaling, and fallback to high precision where necessary, ensuring both performance and stability during training.</li>\n</ul>\n<h4 id=\"how-tensor-cores-work\">How Tensor Cores Work</h4>\n<ul>\n  <li>\n    <p>Tensor Cores serve as specialized hardware designed to accelerate matrix multiplications—critical operations in forward and backward neural network passes. A standard Tensor Core can perform operations such as multiply-and-accumulate on small tiles of data (e.g., 4×4) in reduced-precision formats (e.g., <code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>), or more recent mixed-precision variants using integer types.</p>\n  </li>\n  <li>\n    <p>Crucially, if model tensors remain in <code class=\"language-plaintext highlighter-rouge\">float32</code> in the absence of mixed-precision data handling, the Tensor Cores remain unused and the GPU fails to attain its full performance potential. Enabling automatic mixed precision is therefore essential to utilize these units effectively.</p>\n  </li>\n  <li>\n    <p>In summary, as NVIDIA’s GPU microarchitectures have progressed from Volta through Blackwell, Tensor Cores have become increasingly versatile—offering a widening array of lower-precision formats and hardware optimizations. To fully exploit their capabilities, developers must adopt mixed precision training frameworks (such as AMP), ensuring that compute and memory resources are used optimally while preserving model fidelity.</p>\n  </li>\n  <li>\n    <p>Tensor Core architectures have evolved across NVIDIA’s GPU microarchitectures:</p>\n\n    <ul>\n      <li><strong>Volta</strong> introduced first-generation Tensor Cores, supporting <code class=\"language-plaintext highlighter-rouge\">float16</code> matrix-multiply-accumulate (MMA) fused operations.</li>\n      <li><strong>Turing</strong> brought second-generation Tensor Cores, adding support for <code class=\"language-plaintext highlighter-rouge\">int8</code> and <code class=\"language-plaintext highlighter-rouge\">int4</code> operations, as well as warp-level synchronous MMA primitives and early AI applications such as DLSS.</li>\n      <li><strong>Hopper</strong> (e.g., H100) features fourth-generation Tensor Cores with native <code class=\"language-plaintext highlighter-rouge\">float8</code> precision in the Transformer Engine, yielding up to 4× faster training on models such as GPT‑3 (175B) compared to previous Tensor Core models.</li>\n      <li><strong>Blackwell</strong> advances to fifth‑generation Tensor Cores, introducing support for sub‑byte floating-point formats including <code class=\"language-plaintext highlighter-rouge\">float4</code> and FP6, alongside <code class=\"language-plaintext highlighter-rouge\">float8</code>/<code class=\"language-plaintext highlighter-rouge\">bfloat16</code>/<code class=\"language-plaintext highlighter-rouge\">float16</code> and <code class=\"language-plaintext highlighter-rouge\">int8</code> support. These “Ultra Tensor Cores” incorporate micro‑tensor scaling techniques to fine-tune performance and accuracy—doubling attention-layer throughput and increasing AI FLOPs by 1.5× compared to earlier Blackwell versions.</li>\n    </ul>\n  </li>\n  <li>\n    <p>The following figure (<a href=\"https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/\">source</a>) depicts the fundamental computational pattern executed by a Tensor Core—a fused MMA on small matrix tiles, typically of size 4×4 in early architectures. In this operation, two input matrices (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-338\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-339\"><span class=\"mi\" id=\"MathJax-Span-340\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-341\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-342\"><span class=\"mi\" id=\"MathJax-Span-343\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">B</script>), stored in a reduced-precision format such as <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>, are multiplied together. The results of these element-wise multiplications are then summed and accumulated directly into a third matrix (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>C</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-344\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-345\"><span class=\"mi\" id=\"MathJax-Span-346\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>C</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">C</script>), which may be stored in either <code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>, <code class=\"language-plaintext highlighter-rouge\">float32</code>, or, in newer architectures, <code class=\"language-plaintext highlighter-rouge\">float8</code> or <code class=\"language-plaintext highlighter-rouge\">float4</code>. This fusion of multiplication and accumulation into a single hardware instruction eliminates the need to store intermediate results in memory, drastically reducing memory bandwidth requirements and increasing throughput. Larger GEMM (General Matrix Multiply) operations are implemented by tiling them into many such MMA operations executed in parallel across the GPU’s Tensor Cores.</p>\n  </li>\n</ul>\n<p>Tensor Cores serve as specialized hardware designed to accelerate matrix multiplications—critical operations in forward and backward neural network passes. A standard Tensor Core can perform operations such as multiply-and-accumulate on small tiles of data (e.g., 4×4) in reduced-precision formats (e.g., <code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>), or more recent mixed-precision variants using integer types.</p>\n<p>Crucially, if model tensors remain in <code class=\"language-plaintext highlighter-rouge\">float32</code> in the absence of mixed-precision data handling, the Tensor Cores remain unused and the GPU fails to attain its full performance potential. Enabling automatic mixed precision is therefore essential to utilize these units effectively.</p>\n<p>In summary, as NVIDIA’s GPU microarchitectures have progressed from Volta through Blackwell, Tensor Cores have become increasingly versatile—offering a widening array of lower-precision formats and hardware optimizations. To fully exploit their capabilities, developers must adopt mixed precision training frameworks (such as AMP), ensuring that compute and memory resources are used optimally while preserving model fidelity.</p>\n<p>Tensor Core architectures have evolved across NVIDIA’s GPU microarchitectures:</p>\n<ul>\n      <li><strong>Volta</strong> introduced first-generation Tensor Cores, supporting <code class=\"language-plaintext highlighter-rouge\">float16</code> matrix-multiply-accumulate (MMA) fused operations.</li>\n      <li><strong>Turing</strong> brought second-generation Tensor Cores, adding support for <code class=\"language-plaintext highlighter-rouge\">int8</code> and <code class=\"language-plaintext highlighter-rouge\">int4</code> operations, as well as warp-level synchronous MMA primitives and early AI applications such as DLSS.</li>\n      <li><strong>Hopper</strong> (e.g., H100) features fourth-generation Tensor Cores with native <code class=\"language-plaintext highlighter-rouge\">float8</code> precision in the Transformer Engine, yielding up to 4× faster training on models such as GPT‑3 (175B) compared to previous Tensor Core models.</li>\n      <li><strong>Blackwell</strong> advances to fifth‑generation Tensor Cores, introducing support for sub‑byte floating-point formats including <code class=\"language-plaintext highlighter-rouge\">float4</code> and FP6, alongside <code class=\"language-plaintext highlighter-rouge\">float8</code>/<code class=\"language-plaintext highlighter-rouge\">bfloat16</code>/<code class=\"language-plaintext highlighter-rouge\">float16</code> and <code class=\"language-plaintext highlighter-rouge\">int8</code> support. These “Ultra Tensor Cores” incorporate micro‑tensor scaling techniques to fine-tune performance and accuracy—doubling attention-layer throughput and increasing AI FLOPs by 1.5× compared to earlier Blackwell versions.</li>\n    </ul>\n<p>The following figure (<a href=\"https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/\">source</a>) depicts the fundamental computational pattern executed by a Tensor Core—a fused MMA on small matrix tiles, typically of size 4×4 in early architectures. In this operation, two input matrices (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-338\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-339\"><span class=\"mi\" id=\"MathJax-Span-340\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-341\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-342\"><span class=\"mi\" id=\"MathJax-Span-343\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">B</script>), stored in a reduced-precision format such as <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>, are multiplied together. The results of these element-wise multiplications are then summed and accumulated directly into a third matrix (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>C</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-344\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-345\"><span class=\"mi\" id=\"MathJax-Span-346\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>C</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">C</script>), which may be stored in either <code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>, <code class=\"language-plaintext highlighter-rouge\">float32</code>, or, in newer architectures, <code class=\"language-plaintext highlighter-rouge\">float8</code> or <code class=\"language-plaintext highlighter-rouge\">float4</code>. This fusion of multiplication and accumulation into a single hardware instruction eliminates the need to store intermediate results in memory, drastically reducing memory bandwidth requirements and increasing throughput. Larger GEMM (General Matrix Multiply) operations are implemented by tiling them into many such MMA operations executed in parallel across the GPU’s Tensor Cores.</p>\n<p><img src=\"/primers/ai/assets/model-compression/tensor.avif\" alt=\"\"></p>",
      "contentMarkdown": "*   Efficient matrix multiplication is at the heart of modern deep learning acceleration on GPUs. This section provides a high-level view of how matrix-matrix multiplications are implemented and optimized on GPU hardware, with special focus on tiling, Tensor Cores, and the performance implications of quantization.\n    \n*   Matrix multiplications, especially General Matrix Multiplications (GEMMs), are a core computational primitive in deep learning workloads. Whether in fully connected layers, convolutions (via `im2col`), or attention mechanisms, these operations are executed billions of times during training and inference. As such, optimizing GEMM performance is essential for efficient neural network execution, particularly on GPUs.\n    \n*   To execute GEMMs efficiently, GPUs partition the output matrix into _tiles_. Each tile corresponds to a submatrix of the result and is computed by a thread block. The GPU steps through the input matrices along the shared dimension (K) in tiles, performing multiply-accumulate operations and writing the results into the corresponding tile of the output matrix. The illustration below [(source)](http://theaiedge.io/) shows the tiled outer product approach to GEMMs.\n    \n\nEfficient matrix multiplication is at the heart of modern deep learning acceleration on GPUs. This section provides a high-level view of how matrix-matrix multiplications are implemented and optimized on GPU hardware, with special focus on tiling, Tensor Cores, and the performance implications of quantization.\n\nMatrix multiplications, especially General Matrix Multiplications (GEMMs), are a core computational primitive in deep learning workloads. Whether in fully connected layers, convolutions (via `im2col`), or attention mechanisms, these operations are executed billions of times during training and inference. As such, optimizing GEMM performance is essential for efficient neural network execution, particularly on GPUs.\n\nTo execute GEMMs efficiently, GPUs partition the output matrix into _tiles_. Each tile corresponds to a submatrix of the result and is computed by a thread block. The GPU steps through the input matrices along the shared dimension (K) in tiles, performing multiply-accumulate operations and writing the results into the corresponding tile of the output matrix. The illustration below [(source)](http://theaiedge.io/) shows the tiled outer product approach to GEMMs.\n\n![Tiled outer product approach to GEMMs](/primers/ai/assets/model-compression/tiled_outer_product_gemm_placeholder.jpg)\n\n*   Thread blocks are mapped to the GPU’s **streaming multiprocessors (SMs)**, the fundamental compute units that execute instructions in parallel. Each SM can process one or more thread blocks concurrently, depending on the available resources and occupancy.\n    \n*   Performance in GPU matrix multiplication is often bounded by one of two factors:\n    \n    *   **Compute (math) bound**: When the arithmetic intensity (FLOPs per byte) is high enough that math operations dominate runtime.\n    *   **Memory bound**: When the operation requires frequent memory access compared to math operations, limiting throughput.\n*   Whether a given GEMM is compute- or memory-bound depends on the matrix dimensions (M,N,KM,N,KM, N, K) and the hardware’s characteristics. For example, matrix-vector products (where either MMM = 1 or NNN = 1) are typically memory-bound due to their low arithmetic intensity.\n    \n*   Modern NVIDIA GPUs include specialized hardware units called **Tensor Cores**, which are designed to accelerate GEMMs involving low-precision data types such as `float16`, `bfloat16`, and `int8`. Tensor Cores perform small matrix multiplications in parallel and require that the matrices’ dimensions align with certain multiples (e.g., 8 for `float16`, 16 for `int8`) to achieve peak performance. For instance, on Ampere and newer architectures like Hopper or Blackwell, aligning dimensions to larger multiples (e.g., 64 or 128 elements) often yields even better throughput.\n    \n*   Matrix dimensions that are not aligned to tile sizes lead to **tile quantization**, where some tiles carry less useful work, reducing efficiency. Similarly, if the total number of tiles is not an even multiple of the number of GPU SMs, **wave quantization** can cause underutilization. Both effects can significantly degrade performance despite identical algorithmic complexity.\n    \n*   To address this, libraries like cuBLAS employ heuristics or benchmarking to select optimal tile sizes, balancing between tile reuse (large tiles) and parallelism (many small tiles). Larger tiles tend to be more efficient due to better data reuse, but may reduce parallel occupancy on smaller problems.\n    \n*   In summary, matrix multiplication performance on GPUs is a delicate balance between compute, memory bandwidth, and architecture-aware tiling strategies. Quantization not only affects data representation but also interacts intricately with the underlying matrix multiplication engine and GPU efficiency.\n    \n\nThread blocks are mapped to the GPU’s **streaming multiprocessors (SMs)**, the fundamental compute units that execute instructions in parallel. Each SM can process one or more thread blocks concurrently, depending on the available resources and occupancy.\n\nPerformance in GPU matrix multiplication is often bounded by one of two factors:\n\n*   **Compute (math) bound**: When the arithmetic intensity (FLOPs per byte) is high enough that math operations dominate runtime.\n*   **Memory bound**: When the operation requires frequent memory access compared to math operations, limiting throughput.\n\nWhether a given GEMM is compute- or memory-bound depends on the matrix dimensions (M,N,KM,N,KM, N, K) and the hardware’s characteristics. For example, matrix-vector products (where either MMM = 1 or NNN = 1) are typically memory-bound due to their low arithmetic intensity.\n\nModern NVIDIA GPUs include specialized hardware units called **Tensor Cores**, which are designed to accelerate GEMMs involving low-precision data types such as `float16`, `bfloat16`, and `int8`. Tensor Cores perform small matrix multiplications in parallel and require that the matrices’ dimensions align with certain multiples (e.g., 8 for `float16`, 16 for `int8`) to achieve peak performance. For instance, on Ampere and newer architectures like Hopper or Blackwell, aligning dimensions to larger multiples (e.g., 64 or 128 elements) often yields even better throughput.\n\nMatrix dimensions that are not aligned to tile sizes lead to **tile quantization**, where some tiles carry less useful work, reducing efficiency. Similarly, if the total number of tiles is not an even multiple of the number of GPU SMs, **wave quantization** can cause underutilization. Both effects can significantly degrade performance despite identical algorithmic complexity.\n\nTo address this, libraries like cuBLAS employ heuristics or benchmarking to select optimal tile sizes, balancing between tile reuse (large tiles) and parallelism (many small tiles). Larger tiles tend to be more efficient due to better data reuse, but may reduce parallel occupancy on smaller problems.\n\nIn summary, matrix multiplication performance on GPUs is a delicate balance between compute, memory bandwidth, and architecture-aware tiling strategies. Quantization not only affects data representation but also interacts intricately with the underlying matrix multiplication engine and GPU efficiency.\n\n#### Under-the-hood\n\n*   Modern GPUs are capable of performing numerical computations more efficiently using 16-bit or 8-bit formats—such as `float16`, `bfloat16`, and the emerging `float8`, `float6`, and `float4`—with minimal loss in model performance. Mixed precision training strategically leverages these lower-precision formats to accelerate computation and reduce memory consumption, while preserving high-precision (`float32`) for numerically sensitive variables and operations to ensure convergence and model integrity (cf. numerical stability in the section on [Mixed Precision Overview](#overview)).\n*   NVIDIA’s GPUs, from Volta onward, offer specialized hardware units known as **Tensor Cores**. These units are optimized for dense matrix operations and drastically improve throughput when leveraging reduced-precision data types. For developers using PyTorch, the [`torch.cuda.amp`](https://pytorch.org/docs/stable/amp.html) module offers automatic mixed-precision training functionality, simplifying adoption with minimal code edits. This automates casting, loss scaling, and fallback to high precision where necessary, ensuring both performance and stability during training.\n\n#### How Tensor Cores Work\n\n*   Tensor Cores serve as specialized hardware designed to accelerate matrix multiplications—critical operations in forward and backward neural network passes. A standard Tensor Core can perform operations such as multiply-and-accumulate on small tiles of data (e.g., 4×4) in reduced-precision formats (e.g., `float16`, `bfloat16`), or more recent mixed-precision variants using integer types.\n    \n*   Crucially, if model tensors remain in `float32` in the absence of mixed-precision data handling, the Tensor Cores remain unused and the GPU fails to attain its full performance potential. Enabling automatic mixed precision is therefore essential to utilize these units effectively.\n    \n*   In summary, as NVIDIA’s GPU microarchitectures have progressed from Volta through Blackwell, Tensor Cores have become increasingly versatile—offering a widening array of lower-precision formats and hardware optimizations. To fully exploit their capabilities, developers must adopt mixed precision training frameworks (such as AMP), ensuring that compute and memory resources are used optimally while preserving model fidelity.\n    \n*   Tensor Core architectures have evolved across NVIDIA’s GPU microarchitectures:\n    \n    *   **Volta** introduced first-generation Tensor Cores, supporting `float16` matrix-multiply-accumulate (MMA) fused operations.\n    *   **Turing** brought second-generation Tensor Cores, adding support for `int8` and `int4` operations, as well as warp-level synchronous MMA primitives and early AI applications such as DLSS.\n    *   **Hopper** (e.g., H100) features fourth-generation Tensor Cores with native `float8` precision in the Transformer Engine, yielding up to 4× faster training on models such as GPT‑3 (175B) compared to previous Tensor Core models.\n    *   **Blackwell** advances to fifth‑generation Tensor Cores, introducing support for sub‑byte floating-point formats including `float4` and FP6, alongside `float8`/`bfloat16`/`float16` and `int8` support. These “Ultra Tensor Cores” incorporate micro‑tensor scaling techniques to fine-tune performance and accuracy—doubling attention-layer throughput and increasing AI FLOPs by 1.5× compared to earlier Blackwell versions.\n*   The following figure ([source](https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/)) depicts the fundamental computational pattern executed by a Tensor Core—a fused MMA on small matrix tiles, typically of size 4×4 in early architectures. In this operation, two input matrices (AAA and BBB), stored in a reduced-precision format such as `float16` or `bfloat16`, are multiplied together. The results of these element-wise multiplications are then summed and accumulated directly into a third matrix (CCC), which may be stored in either `float16`, `bfloat16`, `float32`, or, in newer architectures, `float8` or `float4`. This fusion of multiplication and accumulation into a single hardware instruction eliminates the need to store intermediate results in memory, drastically reducing memory bandwidth requirements and increasing throughput. Larger GEMM (General Matrix Multiply) operations are implemented by tiling them into many such MMA operations executed in parallel across the GPU’s Tensor Cores.\n    \n\nTensor Cores serve as specialized hardware designed to accelerate matrix multiplications—critical operations in forward and backward neural network passes. A standard Tensor Core can perform operations such as multiply-and-accumulate on small tiles of data (e.g., 4×4) in reduced-precision formats (e.g., `float16`, `bfloat16`), or more recent mixed-precision variants using integer types.\n\nCrucially, if model tensors remain in `float32` in the absence of mixed-precision data handling, the Tensor Cores remain unused and the GPU fails to attain its full performance potential. Enabling automatic mixed precision is therefore essential to utilize these units effectively.\n\nIn summary, as NVIDIA’s GPU microarchitectures have progressed from Volta through Blackwell, Tensor Cores have become increasingly versatile—offering a widening array of lower-precision formats and hardware optimizations. To fully exploit their capabilities, developers must adopt mixed precision training frameworks (such as AMP), ensuring that compute and memory resources are used optimally while preserving model fidelity.\n\nTensor Core architectures have evolved across NVIDIA’s GPU microarchitectures:\n\n*   **Volta** introduced first-generation Tensor Cores, supporting `float16` matrix-multiply-accumulate (MMA) fused operations.\n*   **Turing** brought second-generation Tensor Cores, adding support for `int8` and `int4` operations, as well as warp-level synchronous MMA primitives and early AI applications such as DLSS.\n*   **Hopper** (e.g., H100) features fourth-generation Tensor Cores with native `float8` precision in the Transformer Engine, yielding up to 4× faster training on models such as GPT‑3 (175B) compared to previous Tensor Core models.\n*   **Blackwell** advances to fifth‑generation Tensor Cores, introducing support for sub‑byte floating-point formats including `float4` and FP6, alongside `float8`/`bfloat16`/`float16` and `int8` support. These “Ultra Tensor Cores” incorporate micro‑tensor scaling techniques to fine-tune performance and accuracy—doubling attention-layer throughput and increasing AI FLOPs by 1.5× compared to earlier Blackwell versions.\n\nThe following figure ([source](https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/)) depicts the fundamental computational pattern executed by a Tensor Core—a fused MMA on small matrix tiles, typically of size 4×4 in early architectures. In this operation, two input matrices (AAA and BBB), stored in a reduced-precision format such as `float16` or `bfloat16`, are multiplied together. The results of these element-wise multiplications are then summed and accumulated directly into a third matrix (CCC), which may be stored in either `float16`, `bfloat16`, `float32`, or, in newer architectures, `float8` or `float4`. This fusion of multiplication and accumulation into a single hardware instruction eliminates the need to store intermediate results in memory, drastically reducing memory bandwidth requirements and increasing throughput. Larger GEMM (General Matrix Multiply) operations are implemented by tiling them into many such MMA operations executed in parallel across the GPU’s Tensor Cores.\n\n![](/primers/ai/assets/model-compression/tensor.avif)",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 4,
      "estimatedMinutes": 11,
      "tags": [
        "ondevice ai",
        "neural network",
        "deep learning",
        "transformer",
        "attention",
        "convolution",
        "gpt",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": true,
        "wordCount": 2011,
        "contentLength": 35503
      },
      "nextCards": [
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4"
      ],
      "relatedCards": [
        "ai-on-device-transformers-balancing-compute-and-memory-prefill-vs-decode-opt-8",
        "ai-federated-learning-privacy-21",
        "ai-on-device-transformers-key-value-kv-cache-optimization-9",
        "ai-on-device-transformers-embedding-size-times-vocabulary-size-times-depth-26",
        "ai-on-device-transformers-parameter-tuning-recipes-for-ml-runtimes-27"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#background:-matrix-multiplication-in-gpus",
      "scrapedAt": "2025-12-28T11:55:50.966Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5",
        "ai-model-compression-quantization-workflows-6"
      ]
    },
    {
      "id": "ai-model-compression-definition-3",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Definition",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>Quantization in the context of deep learning refers to the process of reducing the numerical precision of a model’s parameters (weights) and/or intermediate computations (activations). Typically, models are trained and stored using 32-bit floating-point (<code class=\"language-plaintext highlighter-rouge\">float32</code>) precision. Quantization replaces these high-precision values with lower-precision representations—such as 16-bit floating point (<code class=\"language-plaintext highlighter-rouge\">float16</code>), 8-bit integer (<code class=\"language-plaintext highlighter-rouge\">int8</code>), 4-bit integer, or binary formats in more extreme scenarios. The primary goals are to reduce model size, improve memory and compute efficiency, and accelerate inference—particularly on hardware that supports low-precision arithmetic.</li>\n  <li>The primary goal of quantization is to enhance inference speed. In contrast, as will be discussed in the section on <a href=\"#mixed-precision-training\">Mixed Precision Training</a>, the goal of Automatic Mixed Precision (AMP) is to reduce training time. Quantization is effective, in part, because modern neural networks are typically highly over-parameterized and exhibit robustness to minor numerical perturbations. With appropriate calibration and suitable tools, lower-precision representations can approximate the full-precision model closely enough for practical deployment.</li>\n</ul>",
      "contentMarkdown": "*   Quantization in the context of deep learning refers to the process of reducing the numerical precision of a model’s parameters (weights) and/or intermediate computations (activations). Typically, models are trained and stored using 32-bit floating-point (`float32`) precision. Quantization replaces these high-precision values with lower-precision representations—such as 16-bit floating point (`float16`), 8-bit integer (`int8`), 4-bit integer, or binary formats in more extreme scenarios. The primary goals are to reduce model size, improve memory and compute efficiency, and accelerate inference—particularly on hardware that supports low-precision arithmetic.\n*   The primary goal of quantization is to enhance inference speed. In contrast, as will be discussed in the section on [Mixed Precision Training](#mixed-precision-training), the goal of Automatic Mixed Precision (AMP) is to reduce training time. Quantization is effective, in part, because modern neural networks are typically highly over-parameterized and exhibit robustness to minor numerical perturbations. With appropriate calibration and suitable tools, lower-precision representations can approximate the full-precision model closely enough for practical deployment.",
      "order": 3,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "neural network",
        "deep learning",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 158,
        "contentLength": 1445
      },
      "nextCards": [
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ],
      "relatedCards": [
        "ai-federated-learning-personalization-22",
        "ai-differential-privacy-tightness-3",
        "ai-federated-learning-pros-cons-19",
        "ai-federated-learning-comparison-use-cases-20",
        "ai-differential-privacy-pros-15"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#definition",
      "scrapedAt": "2025-12-28T11:55:50.966Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5",
        "ai-model-compression-quantization-workflows-6"
      ]
    },
    {
      "id": "ai-model-compression-types-of-quantization-4",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Types of Quantization",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>\n    <p>There are two general categories of quantization:</p>\n\n    <ul>\n      <li>\n        <p><strong>Floating-point quantization</strong>: This reduces the bit-width of floating-point values—for example, converting from <code class=\"language-plaintext highlighter-rouge\">float32</code> to <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>. These formats retain the same general IEEE 754 structure (sign, exponent, mantissa) but use fewer bits, reducing precision and dynamic range. This kind of quantization is primarily used for inference on GPUs and accelerators optimized for low-precision floating-point math (e.g., NVIDIA Tensor Cores).</p>\n      </li>\n      <li>\n        <p><strong>Integer quantization</strong>: This maps floating-point values to fixed-point integer representations (e.g., <code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">uint8</code>). This type requires an additional transformation using <strong>scale</strong> and <strong>zero-point</strong> to linearly approximate real values using integers, enabling integer-only arithmetic during inference on CPUs and certain edge devices.</p>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p>There are two general categories of quantization:</p>\n<ul>\n      <li>\n        <p><strong>Floating-point quantization</strong>: This reduces the bit-width of floating-point values—for example, converting from <code class=\"language-plaintext highlighter-rouge\">float32</code> to <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>. These formats retain the same general IEEE 754 structure (sign, exponent, mantissa) but use fewer bits, reducing precision and dynamic range. This kind of quantization is primarily used for inference on GPUs and accelerators optimized for low-precision floating-point math (e.g., NVIDIA Tensor Cores).</p>\n      </li>\n      <li>\n        <p><strong>Integer quantization</strong>: This maps floating-point values to fixed-point integer representations (e.g., <code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">uint8</code>). This type requires an additional transformation using <strong>scale</strong> and <strong>zero-point</strong> to linearly approximate real values using integers, enabling integer-only arithmetic during inference on CPUs and certain edge devices.</p>\n      </li>\n    </ul>\n<p><strong>Floating-point quantization</strong>: This reduces the bit-width of floating-point values—for example, converting from <code class=\"language-plaintext highlighter-rouge\">float32</code> to <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>. These formats retain the same general IEEE 754 structure (sign, exponent, mantissa) but use fewer bits, reducing precision and dynamic range. This kind of quantization is primarily used for inference on GPUs and accelerators optimized for low-precision floating-point math (e.g., NVIDIA Tensor Cores).</p>\n<p><strong>Integer quantization</strong>: This maps floating-point values to fixed-point integer representations (e.g., <code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">uint8</code>). This type requires an additional transformation using <strong>scale</strong> and <strong>zero-point</strong> to linearly approximate real values using integers, enabling integer-only arithmetic during inference on CPUs and certain edge devices.</p>\n<h4 id=\"integer-quantization\">Integer Quantization</h4>\n<ul>\n  <li>\n    <p>Integer quantization is typically implemented as a learned linear transformation (i.e., linear mapping) defined by two parameters: <strong>scale</strong> and <strong>zero-point</strong>.</p>\n\n    <ul>\n      <li>\n        <p>The <strong>scale</strong> is a floating-point multiplier that determines the resolution or step size between adjacent quantized integer values.</p>\n      </li>\n      <li>\n        <p>The <strong>zero-point</strong> is an integer offset that aligns a real-valued zero to the corresponding integer value in the quantized (i.e., target) range. This allows for asymmetric distributions, where zero is not necessarily centered.</p>\n\n        <ul>\n          <li>The forward quantization formula (<code class=\"language-plaintext highlighter-rouge\">float</code> to <code class=\"language-plaintext highlighter-rouge\">int</code>) is:</li>\n        </ul>\n\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\">q = round(x / scale) + zero_point\n</code></pre></div>        </div>\n\n        <ul>\n          <li>The reverse dequantization formula (<code class=\"language-plaintext highlighter-rouge\">int</code> to <code class=\"language-plaintext highlighter-rouge\">float</code>) is:</li>\n        </ul>\n\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\">x = scale * (q - zero_point)\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p>As an example:</p>\n\n        <ul>\n          <li>Suppose we want to quantize floating-point values in the range <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>[</mo><mo>&amp;#x2212;</mo><mn>1.0</mn><mo>,</mo><mn>1.0</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-347\" style=\"width: 5.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.12em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-348\"><span class=\"mo\" id=\"MathJax-Span-349\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mo\" id=\"MathJax-Span-350\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-351\" style=\"font-family: STIXGeneral-Regular;\">1.0</span><span class=\"mo\" id=\"MathJax-Span-352\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-353\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">1.0</span><span class=\"mo\" id=\"MathJax-Span-354\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">[</mo><mo>−</mo><mn>1.0</mn><mo>,</mo><mn>1.0</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">[-1.0, 1.0]</script> to 8-bit unsigned integers (<code class=\"language-plaintext highlighter-rouge\">uint8</code>, range 0–255). The mapping would look like:</li>\n        </ul>\n\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\">scale = (max - min) / (quant_max - quant_min) = (1.0 - (-1.0)) / (255 - 0) ≈ 0.00784\nzero_point = round(0 - min / scale) = round(0 - (-1.0 / 0.00784)) = 128\n</code></pre></div>        </div>\n\n        <ul>\n          <li>This means that the floating-point value 0.0 maps to 128, 1.0 maps to 255, and -1.0 maps to 0. Intermediate values are linearly interpolated. This transformation enables low-bit integer operations that approximate floating-point behavior.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p>Integer quantization is typically implemented as a learned linear transformation (i.e., linear mapping) defined by two parameters: <strong>scale</strong> and <strong>zero-point</strong>.</p>\n<ul>\n      <li>\n        <p>The <strong>scale</strong> is a floating-point multiplier that determines the resolution or step size between adjacent quantized integer values.</p>\n      </li>\n      <li>\n        <p>The <strong>zero-point</strong> is an integer offset that aligns a real-valued zero to the corresponding integer value in the quantized (i.e., target) range. This allows for asymmetric distributions, where zero is not necessarily centered.</p>\n\n        <ul>\n          <li>The forward quantization formula (<code class=\"language-plaintext highlighter-rouge\">float</code> to <code class=\"language-plaintext highlighter-rouge\">int</code>) is:</li>\n        </ul>\n\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\">q = round(x / scale) + zero_point\n</code></pre></div>        </div>\n\n        <ul>\n          <li>The reverse dequantization formula (<code class=\"language-plaintext highlighter-rouge\">int</code> to <code class=\"language-plaintext highlighter-rouge\">float</code>) is:</li>\n        </ul>\n\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\">x = scale * (q - zero_point)\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p>As an example:</p>\n\n        <ul>\n          <li>Suppose we want to quantize floating-point values in the range <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>[</mo><mo>&amp;#x2212;</mo><mn>1.0</mn><mo>,</mo><mn>1.0</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-347\" style=\"width: 5.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.12em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-348\"><span class=\"mo\" id=\"MathJax-Span-349\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mo\" id=\"MathJax-Span-350\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-351\" style=\"font-family: STIXGeneral-Regular;\">1.0</span><span class=\"mo\" id=\"MathJax-Span-352\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-353\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">1.0</span><span class=\"mo\" id=\"MathJax-Span-354\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">[</mo><mo>−</mo><mn>1.0</mn><mo>,</mo><mn>1.0</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">[-1.0, 1.0]</script> to 8-bit unsigned integers (<code class=\"language-plaintext highlighter-rouge\">uint8</code>, range 0–255). The mapping would look like:</li>\n        </ul>\n\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\">scale = (max - min) / (quant_max - quant_min) = (1.0 - (-1.0)) / (255 - 0) ≈ 0.00784\nzero_point = round(0 - min / scale) = round(0 - (-1.0 / 0.00784)) = 128\n</code></pre></div>        </div>\n\n        <ul>\n          <li>This means that the floating-point value 0.0 maps to 128, 1.0 maps to 255, and -1.0 maps to 0. Intermediate values are linearly interpolated. This transformation enables low-bit integer operations that approximate floating-point behavior.</li>\n        </ul>\n      </li>\n    </ul>\n<p>The <strong>scale</strong> is a floating-point multiplier that determines the resolution or step size between adjacent quantized integer values.</p>\n<p>The <strong>zero-point</strong> is an integer offset that aligns a real-valued zero to the corresponding integer value in the quantized (i.e., target) range. This allows for asymmetric distributions, where zero is not necessarily centered.</p>\n<ul>\n          <li>The forward quantization formula (<code class=\"language-plaintext highlighter-rouge\">float</code> to <code class=\"language-plaintext highlighter-rouge\">int</code>) is:</li>\n        </ul>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\">q = round(x / scale) + zero_point\n</code></pre>\n<ul>\n          <li>The reverse dequantization formula (<code class=\"language-plaintext highlighter-rouge\">int</code> to <code class=\"language-plaintext highlighter-rouge\">float</code>) is:</li>\n        </ul>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\">x = scale * (q - zero_point)\n</code></pre>\n<p>As an example:</p>\n<ul>\n          <li>Suppose we want to quantize floating-point values in the range <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>[</mo><mo>&amp;#x2212;</mo><mn>1.0</mn><mo>,</mo><mn>1.0</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-347\" style=\"width: 5.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.12em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-348\"><span class=\"mo\" id=\"MathJax-Span-349\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mo\" id=\"MathJax-Span-350\" style=\"font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-351\" style=\"font-family: STIXGeneral-Regular;\">1.0</span><span class=\"mo\" id=\"MathJax-Span-352\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-353\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">1.0</span><span class=\"mo\" id=\"MathJax-Span-354\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">[</mo><mo>−</mo><mn>1.0</mn><mo>,</mo><mn>1.0</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">[-1.0, 1.0]</script> to 8-bit unsigned integers (<code class=\"language-plaintext highlighter-rouge\">uint8</code>, range 0–255). The mapping would look like:</li>\n        </ul>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\">scale = (max - min) / (quant_max - quant_min) = (1.0 - (-1.0)) / (255 - 0) ≈ 0.00784\nzero_point = round(0 - min / scale) = round(0 - (-1.0 / 0.00784)) = 128\n</code></pre>\n<ul>\n          <li>This means that the floating-point value 0.0 maps to 128, 1.0 maps to 255, and -1.0 maps to 0. Intermediate values are linearly interpolated. This transformation enables low-bit integer operations that approximate floating-point behavior.</li>\n        </ul>\n<h4 id=\"non-linear-integer-quantization-methods\">Non-Linear Integer Quantization Methods</h4>\n<ul>\n  <li>\n    <p>While linear quantization using scale and zero‑point is the most common, <strong>non‑linear quantization methods</strong> are also employed in integer quantization to better match real-world data distributions. These methods typically apply to <strong>integer quantization</strong>, as they redefine how integer values map to real numbers:</p>\n\n    <ul>\n      <li>\n        <p><strong>Logarithmic quantization</strong> uses exponentially spaced quantization levels—e.g. powers of two—providing better representation across a wide dynamic range. This method is non‑linear and particularly used for integer-only inference pipelines. It is <strong>not</strong> relevant for floating‑point quantization, which already uses a non‑uniform exponent-based scale inherently built into its representation.</p>\n      </li>\n      <li>\n        <p><strong>K-means or cluster-based quantization</strong> groups floating-point values into clusters, mapping each to its centroid—another non-linear approach for integer quantization or weight sharing schemes.</p>\n      </li>\n      <li>\n        <p><strong>Learned transformations</strong>, such as LSQ (Learned Step Size Quantization) and its non-uniform variant nuLSQ, optimize quantization step sizes or level spacing via backpropagation. These methods are applied to <strong>integer quantization</strong> of weights and activations (e.g., 2‑, 3‑, or 4‑bit integer quantization) and involve non-linear quantizer parameterization.</p>\n      </li>\n      <li>\n        <p>In summary, <strong>non-linear quantization techniques are relevant for integer quantization workflows</strong>, where they redefine integer mapping to better match value distributions. Floating‑point quantization (e.g., float32 <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-355\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-356\"><span class=\"mo\" id=\"MathJax-Span-357\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">\\rightarrow</script> float16/bfloat16), while structurally non-linear due to its exponent/mantissa hierarchy, does not employ these learned or clustering-based non-linear integer mapping schemes.</p>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p>While linear quantization using scale and zero‑point is the most common, <strong>non‑linear quantization methods</strong> are also employed in integer quantization to better match real-world data distributions. These methods typically apply to <strong>integer quantization</strong>, as they redefine how integer values map to real numbers:</p>\n<ul>\n      <li>\n        <p><strong>Logarithmic quantization</strong> uses exponentially spaced quantization levels—e.g. powers of two—providing better representation across a wide dynamic range. This method is non‑linear and particularly used for integer-only inference pipelines. It is <strong>not</strong> relevant for floating‑point quantization, which already uses a non‑uniform exponent-based scale inherently built into its representation.</p>\n      </li>\n      <li>\n        <p><strong>K-means or cluster-based quantization</strong> groups floating-point values into clusters, mapping each to its centroid—another non-linear approach for integer quantization or weight sharing schemes.</p>\n      </li>\n      <li>\n        <p><strong>Learned transformations</strong>, such as LSQ (Learned Step Size Quantization) and its non-uniform variant nuLSQ, optimize quantization step sizes or level spacing via backpropagation. These methods are applied to <strong>integer quantization</strong> of weights and activations (e.g., 2‑, 3‑, or 4‑bit integer quantization) and involve non-linear quantizer parameterization.</p>\n      </li>\n      <li>\n        <p>In summary, <strong>non-linear quantization techniques are relevant for integer quantization workflows</strong>, where they redefine integer mapping to better match value distributions. Floating‑point quantization (e.g., float32 <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-355\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-356\"><span class=\"mo\" id=\"MathJax-Span-357\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">\\rightarrow</script> float16/bfloat16), while structurally non-linear due to its exponent/mantissa hierarchy, does not employ these learned or clustering-based non-linear integer mapping schemes.</p>\n      </li>\n    </ul>\n<p><strong>Logarithmic quantization</strong> uses exponentially spaced quantization levels—e.g. powers of two—providing better representation across a wide dynamic range. This method is non‑linear and particularly used for integer-only inference pipelines. It is <strong>not</strong> relevant for floating‑point quantization, which already uses a non‑uniform exponent-based scale inherently built into its representation.</p>\n<p><strong>K-means or cluster-based quantization</strong> groups floating-point values into clusters, mapping each to its centroid—another non-linear approach for integer quantization or weight sharing schemes.</p>\n<p><strong>Learned transformations</strong>, such as LSQ (Learned Step Size Quantization) and its non-uniform variant nuLSQ, optimize quantization step sizes or level spacing via backpropagation. These methods are applied to <strong>integer quantization</strong> of weights and activations (e.g., 2‑, 3‑, or 4‑bit integer quantization) and involve non-linear quantizer parameterization.</p>\n<p>In summary, <strong>non-linear quantization techniques are relevant for integer quantization workflows</strong>, where they redefine integer mapping to better match value distributions. Floating‑point quantization (e.g., float32 <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-355\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-356\"><span class=\"mo\" id=\"MathJax-Span-357\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">\\rightarrow</script> float16/bfloat16), while structurally non-linear due to its exponent/mantissa hierarchy, does not employ these learned or clustering-based non-linear integer mapping schemes.</p>\n<h4 id=\"floating-point-quantization\">Floating-Point Quantization</h4>\n<ul>\n  <li>Floating-point quantization is implemented by truncating or rounding the mantissa and exponent fields in IEEE754 representation—e.g. conversion from <code class=\"language-plaintext highlighter-rouge\">float32</code> to <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>—preserving the format structure but reducing bit-width. This form of quantization (i.e., bit‑width reduction) is <strong>non-linear</strong> in effect because the quantization steps vary by exponent range: numbers near zero have finer granularity than large values due to the floating-point exponent scaling.</li>\n  <li>This approach aligns with a well-known model in signal quantization theory often referred to as the compressor–quantizer–expander model. In this framework, the exponential scaling of floating-point numbers acts as a compressor that non-linearly maps real values into a domain where uniform quantization (truncation of mantissa bits) is applied. The quantizer then discretizes the mantissa (hidden quantizer), and the expander step reconstructs the approximate value from the compressed representation. This structure enables efficient representation of a wide dynamic range with relatively coarse quantization, especially benefiting smaller values close to zero.</li>\n  <li>Common APIs include casting methods like <code class=\"language-plaintext highlighter-rouge\">model.half()</code> in PyTorch and PyTorch’s support for <code class=\"language-plaintext highlighter-rouge\">float16</code> static quantization configs (e.g. <code class=\"language-plaintext highlighter-rouge\">float16_static_qconfig</code>). Floating-point quantization halves memory footprint with minimal accuracy loss on GPU inference platforms.</li>\n</ul>",
      "contentMarkdown": "*   There are two general categories of quantization:\n    \n    *   **Floating-point quantization**: This reduces the bit-width of floating-point values—for example, converting from `float32` to `float16` or `bfloat16`. These formats retain the same general IEEE 754 structure (sign, exponent, mantissa) but use fewer bits, reducing precision and dynamic range. This kind of quantization is primarily used for inference on GPUs and accelerators optimized for low-precision floating-point math (e.g., NVIDIA Tensor Cores).\n        \n    *   **Integer quantization**: This maps floating-point values to fixed-point integer representations (e.g., `int8` or `uint8`). This type requires an additional transformation using **scale** and **zero-point** to linearly approximate real values using integers, enabling integer-only arithmetic during inference on CPUs and certain edge devices.\n        \n\nThere are two general categories of quantization:\n\n*   **Floating-point quantization**: This reduces the bit-width of floating-point values—for example, converting from `float32` to `float16` or `bfloat16`. These formats retain the same general IEEE 754 structure (sign, exponent, mantissa) but use fewer bits, reducing precision and dynamic range. This kind of quantization is primarily used for inference on GPUs and accelerators optimized for low-precision floating-point math (e.g., NVIDIA Tensor Cores).\n    \n*   **Integer quantization**: This maps floating-point values to fixed-point integer representations (e.g., `int8` or `uint8`). This type requires an additional transformation using **scale** and **zero-point** to linearly approximate real values using integers, enabling integer-only arithmetic during inference on CPUs and certain edge devices.\n    \n\n**Floating-point quantization**: This reduces the bit-width of floating-point values—for example, converting from `float32` to `float16` or `bfloat16`. These formats retain the same general IEEE 754 structure (sign, exponent, mantissa) but use fewer bits, reducing precision and dynamic range. This kind of quantization is primarily used for inference on GPUs and accelerators optimized for low-precision floating-point math (e.g., NVIDIA Tensor Cores).\n\n**Integer quantization**: This maps floating-point values to fixed-point integer representations (e.g., `int8` or `uint8`). This type requires an additional transformation using **scale** and **zero-point** to linearly approximate real values using integers, enabling integer-only arithmetic during inference on CPUs and certain edge devices.\n\n#### Integer Quantization\n\n*   Integer quantization is typically implemented as a learned linear transformation (i.e., linear mapping) defined by two parameters: **scale** and **zero-point**.\n    \n    *   The **scale** is a floating-point multiplier that determines the resolution or step size between adjacent quantized integer values.\n        \n    *   The **zero-point** is an integer offset that aligns a real-valued zero to the corresponding integer value in the quantized (i.e., target) range. This allows for asymmetric distributions, where zero is not necessarily centered.\n        \n        *   The forward quantization formula (`float` to `int`) is:\n        \n        ![](https://aman.ai/images/copy.png)\n        \n        `q = round(x / scale) + zero_point`\n        \n        *   The reverse dequantization formula (`int` to `float`) is:\n        \n        ![](https://aman.ai/images/copy.png)\n        \n        `x = scale * (q - zero_point)`\n        \n    *   As an example:\n        \n        *   Suppose we want to quantize floating-point values in the range \\[−1.0,1.0\\]\\[−1.0,1.0\\]\\[-1.0, 1.0\\] to 8-bit unsigned integers (`uint8`, range 0–255). The mapping would look like:\n        \n        ![](https://aman.ai/images/copy.png)\n        \n        `scale = (max - min) / (quant_max - quant_min) = (1.0 - (-1.0)) / (255 - 0) ≈ 0.00784 zero_point = round(0 - min / scale) = round(0 - (-1.0 / 0.00784)) = 128`\n        \n        *   This means that the floating-point value 0.0 maps to 128, 1.0 maps to 255, and -1.0 maps to 0. Intermediate values are linearly interpolated. This transformation enables low-bit integer operations that approximate floating-point behavior.\n\nInteger quantization is typically implemented as a learned linear transformation (i.e., linear mapping) defined by two parameters: **scale** and **zero-point**.\n\n*   The **scale** is a floating-point multiplier that determines the resolution or step size between adjacent quantized integer values.\n    \n*   The **zero-point** is an integer offset that aligns a real-valued zero to the corresponding integer value in the quantized (i.e., target) range. This allows for asymmetric distributions, where zero is not necessarily centered.\n    \n    *   The forward quantization formula (`float` to `int`) is:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n    `q = round(x / scale) + zero_point`\n    \n    *   The reverse dequantization formula (`int` to `float`) is:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n    `x = scale * (q - zero_point)`\n    \n*   As an example:\n    \n    *   Suppose we want to quantize floating-point values in the range \\[−1.0,1.0\\]\\[−1.0,1.0\\]\\[-1.0, 1.0\\] to 8-bit unsigned integers (`uint8`, range 0–255). The mapping would look like:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n    `scale = (max - min) / (quant_max - quant_min) = (1.0 - (-1.0)) / (255 - 0) ≈ 0.00784 zero_point = round(0 - min / scale) = round(0 - (-1.0 / 0.00784)) = 128`\n    \n    *   This means that the floating-point value 0.0 maps to 128, 1.0 maps to 255, and -1.0 maps to 0. Intermediate values are linearly interpolated. This transformation enables low-bit integer operations that approximate floating-point behavior.\n\nThe **scale** is a floating-point multiplier that determines the resolution or step size between adjacent quantized integer values.\n\nThe **zero-point** is an integer offset that aligns a real-valued zero to the corresponding integer value in the quantized (i.e., target) range. This allows for asymmetric distributions, where zero is not necessarily centered.\n\n*   The forward quantization formula (`float` to `int`) is:\n\n![](https://aman.ai/images/copy.png)\n\n`q = round(x / scale) + zero_point`\n\n*   The reverse dequantization formula (`int` to `float`) is:\n\n![](https://aman.ai/images/copy.png)\n\n`x = scale * (q - zero_point)`\n\nAs an example:\n\n*   Suppose we want to quantize floating-point values in the range \\[−1.0,1.0\\]\\[−1.0,1.0\\]\\[-1.0, 1.0\\] to 8-bit unsigned integers (`uint8`, range 0–255). The mapping would look like:\n\n![](https://aman.ai/images/copy.png)\n\n`scale = (max - min) / (quant_max - quant_min) = (1.0 - (-1.0)) / (255 - 0) ≈ 0.00784 zero_point = round(0 - min / scale) = round(0 - (-1.0 / 0.00784)) = 128`\n\n*   This means that the floating-point value 0.0 maps to 128, 1.0 maps to 255, and -1.0 maps to 0. Intermediate values are linearly interpolated. This transformation enables low-bit integer operations that approximate floating-point behavior.\n\n#### Non-Linear Integer Quantization Methods\n\n*   While linear quantization using scale and zero‑point is the most common, **non‑linear quantization methods** are also employed in integer quantization to better match real-world data distributions. These methods typically apply to **integer quantization**, as they redefine how integer values map to real numbers:\n    \n    *   **Logarithmic quantization** uses exponentially spaced quantization levels—e.g. powers of two—providing better representation across a wide dynamic range. This method is non‑linear and particularly used for integer-only inference pipelines. It is **not** relevant for floating‑point quantization, which already uses a non‑uniform exponent-based scale inherently built into its representation.\n        \n    *   **K-means or cluster-based quantization** groups floating-point values into clusters, mapping each to its centroid—another non-linear approach for integer quantization or weight sharing schemes.\n        \n    *   **Learned transformations**, such as LSQ (Learned Step Size Quantization) and its non-uniform variant nuLSQ, optimize quantization step sizes or level spacing via backpropagation. These methods are applied to **integer quantization** of weights and activations (e.g., 2‑, 3‑, or 4‑bit integer quantization) and involve non-linear quantizer parameterization.\n        \n    *   In summary, **non-linear quantization techniques are relevant for integer quantization workflows**, where they redefine integer mapping to better match value distributions. Floating‑point quantization (e.g., float32 →→\\\\rightarrow float16/bfloat16), while structurally non-linear due to its exponent/mantissa hierarchy, does not employ these learned or clustering-based non-linear integer mapping schemes.\n        \n\nWhile linear quantization using scale and zero‑point is the most common, **non‑linear quantization methods** are also employed in integer quantization to better match real-world data distributions. These methods typically apply to **integer quantization**, as they redefine how integer values map to real numbers:\n\n*   **Logarithmic quantization** uses exponentially spaced quantization levels—e.g. powers of two—providing better representation across a wide dynamic range. This method is non‑linear and particularly used for integer-only inference pipelines. It is **not** relevant for floating‑point quantization, which already uses a non‑uniform exponent-based scale inherently built into its representation.\n    \n*   **K-means or cluster-based quantization** groups floating-point values into clusters, mapping each to its centroid—another non-linear approach for integer quantization or weight sharing schemes.\n    \n*   **Learned transformations**, such as LSQ (Learned Step Size Quantization) and its non-uniform variant nuLSQ, optimize quantization step sizes or level spacing via backpropagation. These methods are applied to **integer quantization** of weights and activations (e.g., 2‑, 3‑, or 4‑bit integer quantization) and involve non-linear quantizer parameterization.\n    \n*   In summary, **non-linear quantization techniques are relevant for integer quantization workflows**, where they redefine integer mapping to better match value distributions. Floating‑point quantization (e.g., float32 →→\\\\rightarrow float16/bfloat16), while structurally non-linear due to its exponent/mantissa hierarchy, does not employ these learned or clustering-based non-linear integer mapping schemes.\n    \n\n**Logarithmic quantization** uses exponentially spaced quantization levels—e.g. powers of two—providing better representation across a wide dynamic range. This method is non‑linear and particularly used for integer-only inference pipelines. It is **not** relevant for floating‑point quantization, which already uses a non‑uniform exponent-based scale inherently built into its representation.\n\n**K-means or cluster-based quantization** groups floating-point values into clusters, mapping each to its centroid—another non-linear approach for integer quantization or weight sharing schemes.\n\n**Learned transformations**, such as LSQ (Learned Step Size Quantization) and its non-uniform variant nuLSQ, optimize quantization step sizes or level spacing via backpropagation. These methods are applied to **integer quantization** of weights and activations (e.g., 2‑, 3‑, or 4‑bit integer quantization) and involve non-linear quantizer parameterization.\n\nIn summary, **non-linear quantization techniques are relevant for integer quantization workflows**, where they redefine integer mapping to better match value distributions. Floating‑point quantization (e.g., float32 →→\\\\rightarrow float16/bfloat16), while structurally non-linear due to its exponent/mantissa hierarchy, does not employ these learned or clustering-based non-linear integer mapping schemes.\n\n#### Floating-Point Quantization\n\n*   Floating-point quantization is implemented by truncating or rounding the mantissa and exponent fields in IEEE754 representation—e.g. conversion from `float32` to `float16` or `bfloat16`—preserving the format structure but reducing bit-width. This form of quantization (i.e., bit‑width reduction) is **non-linear** in effect because the quantization steps vary by exponent range: numbers near zero have finer granularity than large values due to the floating-point exponent scaling.\n*   This approach aligns with a well-known model in signal quantization theory often referred to as the compressor–quantizer–expander model. In this framework, the exponential scaling of floating-point numbers acts as a compressor that non-linearly maps real values into a domain where uniform quantization (truncation of mantissa bits) is applied. The quantizer then discretizes the mantissa (hidden quantizer), and the expander step reconstructs the approximate value from the compressed representation. This structure enables efficient representation of a wide dynamic range with relatively coarse quantization, especially benefiting smaller values close to zero.\n*   Common APIs include casting methods like `model.half()` in PyTorch and PyTorch’s support for `float16` static quantization configs (e.g. `float16_static_qconfig`). Floating-point quantization halves memory footprint with minimal accuracy loss on GPU inference platforms.",
      "order": 4,
      "orderInChapter": 4,
      "difficulty": 3,
      "estimatedMinutes": 9,
      "tags": [
        "ondevice ai",
        "backpropagation",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": true,
        "wordCount": 1700,
        "contentLength": 29054
      },
      "nextCards": [
        "ai-model-compression-dequantization-considerations-5",
        "ai-model-compression-quantization-workflows-6"
      ],
      "relatedCards": [
        "ai-federated-learning-pros-cons-19",
        "ai-federated-learning-comparison-use-cases-20",
        "ai-on-device-transformers-quantization-48-bit-11",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#types-of-quantization",
      "scrapedAt": "2025-12-28T11:55:50.967Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-dequantization-considerations-5",
        "ai-model-compression-quantization-workflows-6"
      ]
    },
    {
      "id": "ai-model-compression-dequantization-considerations-5",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Dequantization Considerations",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li><strong>Dequantization is not always needed during inference</strong>, and its necessity depends on the type of quantization and the underlying hardware. In <strong>integer-only quantization pipelines</strong>—commonly used for inference on mobile CPUs or edge devices—computations are performed entirely in the integer domain (e.g., <code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">uint8</code>), and <strong>dequantization is typically only applied at the final stage</strong>, such as for logits or output activations. This avoids floating-point operations altogether during inference.\n    <ul>\n      <li>However, in <strong>hybrid quantization workflows</strong>, where some layers are quantized (e.g., to <code class=\"language-plaintext highlighter-rouge\">int8</code>) and others remain in higher precision (<code class=\"language-plaintext highlighter-rouge\">float32</code> or <code class=\"language-plaintext highlighter-rouge\">float16</code>), <strong>intermediate dequantization is required</strong> at the layer boundaries to enable compatibility between quantized and non-quantized components. This is common in models that cannot fully tolerate quantization across all layers due to accuracy degradation or unsupported ops.</li>\n      <li>In contrast, when quantizing to <strong>lower-precision floating-point formats</strong> like <code class=\"language-plaintext highlighter-rouge\">float16</code>, <strong>dequantization is not needed at all</strong>, because these formats are still natively supported by GPU hardware. For example, <strong>NVIDIA Tensor Cores</strong> are optimized for <code class=\"language-plaintext highlighter-rouge\">float16</code> (and <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>) matrix operations, so models using <code class=\"language-plaintext highlighter-rouge\">float16</code> quantization can be executed directly end-to-end without converting back to <code class=\"language-plaintext highlighter-rouge\">float32</code>. All computations remain in low-precision floating-point format, maintaining performance while avoiding the complexity of dequantization logic entirely.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>However, in <strong>hybrid quantization workflows</strong>, where some layers are quantized (e.g., to <code class=\"language-plaintext highlighter-rouge\">int8</code>) and others remain in higher precision (<code class=\"language-plaintext highlighter-rouge\">float32</code> or <code class=\"language-plaintext highlighter-rouge\">float16</code>), <strong>intermediate dequantization is required</strong> at the layer boundaries to enable compatibility between quantized and non-quantized components. This is common in models that cannot fully tolerate quantization across all layers due to accuracy degradation or unsupported ops.</li>\n      <li>In contrast, when quantizing to <strong>lower-precision floating-point formats</strong> like <code class=\"language-plaintext highlighter-rouge\">float16</code>, <strong>dequantization is not needed at all</strong>, because these formats are still natively supported by GPU hardware. For example, <strong>NVIDIA Tensor Cores</strong> are optimized for <code class=\"language-plaintext highlighter-rouge\">float16</code> (and <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>) matrix operations, so models using <code class=\"language-plaintext highlighter-rouge\">float16</code> quantization can be executed directly end-to-end without converting back to <code class=\"language-plaintext highlighter-rouge\">float32</code>. All computations remain in low-precision floating-point format, maintaining performance while avoiding the complexity of dequantization logic entirely.</li>\n    </ul>",
      "contentMarkdown": "*   **Dequantization is not always needed during inference**, and its necessity depends on the type of quantization and the underlying hardware. In **integer-only quantization pipelines**—commonly used for inference on mobile CPUs or edge devices—computations are performed entirely in the integer domain (e.g., `int8` or `uint8`), and **dequantization is typically only applied at the final stage**, such as for logits or output activations. This avoids floating-point operations altogether during inference.\n    *   However, in **hybrid quantization workflows**, where some layers are quantized (e.g., to `int8`) and others remain in higher precision (`float32` or `float16`), **intermediate dequantization is required** at the layer boundaries to enable compatibility between quantized and non-quantized components. This is common in models that cannot fully tolerate quantization across all layers due to accuracy degradation or unsupported ops.\n    *   In contrast, when quantizing to **lower-precision floating-point formats** like `float16`, **dequantization is not needed at all**, because these formats are still natively supported by GPU hardware. For example, **NVIDIA Tensor Cores** are optimized for `float16` (and `bfloat16`) matrix operations, so models using `float16` quantization can be executed directly end-to-end without converting back to `float32`. All computations remain in low-precision floating-point format, maintaining performance while avoiding the complexity of dequantization logic entirely.\n\n*   However, in **hybrid quantization workflows**, where some layers are quantized (e.g., to `int8`) and others remain in higher precision (`float32` or `float16`), **intermediate dequantization is required** at the layer boundaries to enable compatibility between quantized and non-quantized components. This is common in models that cannot fully tolerate quantization across all layers due to accuracy degradation or unsupported ops.\n*   In contrast, when quantizing to **lower-precision floating-point formats** like `float16`, **dequantization is not needed at all**, because these formats are still natively supported by GPU hardware. For example, **NVIDIA Tensor Cores** are optimized for `float16` (and `bfloat16`) matrix operations, so models using `float16` quantization can be executed directly end-to-end without converting back to `float32`. All computations remain in low-precision floating-point format, maintaining performance while avoiding the complexity of dequantization logic entirely.",
      "order": 5,
      "orderInChapter": 5,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 331,
        "contentLength": 3797
      },
      "nextCards": [
        "ai-model-compression-quantization-workflows-6",
        "ai-model-compression-benefits-and-limitations-7"
      ],
      "relatedCards": [
        "ai-federated-learning-pros-cons-19",
        "ai-federated-learning-comparison-use-cases-20",
        "ai-on-device-transformers-quantization-48-bit-11",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#dequantization-considerations",
      "scrapedAt": "2025-12-28T11:55:50.967Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-quantization-workflows-6"
      ]
    },
    {
      "id": "ai-model-compression-quantization-workflows-6",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Quantization Workflows",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>\n    <p>There are three main workflows/approaches to apply quantization:</p>\n\n    <ul>\n      <li>\n        <p><strong>Dynamic / Runtime Quantization</strong>: This method quantizes <strong>model weights statically</strong> (e.g. to <code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">float16</code>), while <strong>activations remain in full precision until runtime</strong>, where they are quantized dynamically at each inference step right before computation. It requires no calibration dataset and no fine‑tuning, making it the easiest quantization method provided by PyTorch. It is particularly effective for models dominated by weight‑heavy layers such as <code class=\"language-plaintext highlighter-rouge\">torch.nn.Linear</code>, recurrent layers (<code class=\"language-plaintext highlighter-rouge\">nn.LSTM</code>, <code class=\"language-plaintext highlighter-rouge\">nn.GRU</code>), and transformers. In PyTorch, this is implemented via the function <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.ao.quantization.quantize_dynamic\"><code class=\"language-plaintext highlighter-rouge\">quantize_dynamic</code></a>, for example:</p>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"n\">quantized_model</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">ao</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">quantize_dynamic</span><span class=\"p\">(</span>\n    <span class=\"n\">model_float32</span><span class=\"p\">,</span>\n    <span class=\"p\">{</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">LSTM</span><span class=\"p\">},</span>\n    <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">qint8</span>\n<span class=\"p\">)</span>\n</code></pre></div>        </div>\n\n        <ul>\n          <li>With this approach, the quantized model is memory‑efficient and can accelerate inference for NLP architectures, often with negligible accuracy loss compared to PTQ—though with lower benefit on convolution‑heavy vision models.</li>\n          <li>Look up the <a href=\"#dynamic--runtime-quantization\">Dynamic / Runtime Quantization</a> section for a detailed discourse on this topic.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Post-Training Quantization (PTQ)</strong>: Converts a fully trained high-precision model to a lower-precision format without retraining. PTQ typically uses a calibration dataset to compute appropriate <code class=\"language-plaintext highlighter-rouge\">scale</code> and <code class=\"language-plaintext highlighter-rouge\">zero-point</code> values using strategies like min-max range or percentile clipping. It is simple to use and well-supported in frameworks such as TensorFlow Lite and PyTorch, but may incur accuracy loss—particularly for sensitive or activation-heavy layers.</p>\n\n        <ul>\n          <li>From the TensorFlow <a href=\"https://www.tensorflow.org/model_optimization/guide/quantization/post_training\">post-training quantization</a> documentation:</li>\n        </ul>\n\n        <blockquote>\n          <p>We generally recommend 16-bit floats for GPU acceleration and 8-bit integer for CPU execution.</p>\n        </blockquote>\n\n        <ul>\n          <li>This reflects hardware preferences: <code class=\"language-plaintext highlighter-rouge\">float16</code> enables faster matrix multiplications on GPU accelerators like Tensor Cores, while <code class=\"language-plaintext highlighter-rouge\">int8</code> is more efficient on CPU architectures with dedicated integer units such as integer SIMD extensions (e.g., AVX, NEON, VNNI).</li>\n          <li>Look up the <a href=\"#post-training-quantization\">Post-Training Quantization</a> section for a detailed discourse on this topic.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Quantization-Aware Training (QAT)</strong>: In QAT, quantization effects—specifically the non-linearity introduced by rounding and clipping—are simulated during training, allowing the model to adapt. The model behaves as though it operates in lower precision during the forward pass, using the quantization formula above with fake quantization modules (e.g., <code class=\"language-plaintext highlighter-rouge\">FakeQuantize</code> in PyTorch or <code class=\"language-plaintext highlighter-rouge\">tf.quantization.fake_quant_with_min_max_vars</code> in TensorFlow). These modules apply quantization and dequantization logic using scale and zero-point. However, backpropagation remains in full precision. In other words, gradients and parameter updates are still computed using full <code class=\"language-plaintext highlighter-rouge\">float32</code> precision. This allows the model to adapt to quantization-induced noise, often resulting in better accuracy retention compared to PTQ. QAT is especially useful when targeting very low-bit formats (such as 4-bit or lower) or when quantizing sensitive components like attention layers in transformers or deploying models in high-accuracy applications.</p>\n        <ul>\n          <li>Look up the <a href=\"#quantizationaware-training-qat\">Quantization‑aware Training (QAT)</a> section for a detailed discourse on this topic.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p>There are three main workflows/approaches to apply quantization:</p>\n<ul>\n      <li>\n        <p><strong>Dynamic / Runtime Quantization</strong>: This method quantizes <strong>model weights statically</strong> (e.g. to <code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">float16</code>), while <strong>activations remain in full precision until runtime</strong>, where they are quantized dynamically at each inference step right before computation. It requires no calibration dataset and no fine‑tuning, making it the easiest quantization method provided by PyTorch. It is particularly effective for models dominated by weight‑heavy layers such as <code class=\"language-plaintext highlighter-rouge\">torch.nn.Linear</code>, recurrent layers (<code class=\"language-plaintext highlighter-rouge\">nn.LSTM</code>, <code class=\"language-plaintext highlighter-rouge\">nn.GRU</code>), and transformers. In PyTorch, this is implemented via the function <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.ao.quantization.quantize_dynamic\"><code class=\"language-plaintext highlighter-rouge\">quantize_dynamic</code></a>, for example:</p>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"n\">quantized_model</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">ao</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">quantize_dynamic</span><span class=\"p\">(</span>\n    <span class=\"n\">model_float32</span><span class=\"p\">,</span>\n    <span class=\"p\">{</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">LSTM</span><span class=\"p\">},</span>\n    <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">qint8</span>\n<span class=\"p\">)</span>\n</code></pre></div>        </div>\n\n        <ul>\n          <li>With this approach, the quantized model is memory‑efficient and can accelerate inference for NLP architectures, often with negligible accuracy loss compared to PTQ—though with lower benefit on convolution‑heavy vision models.</li>\n          <li>Look up the <a href=\"#dynamic--runtime-quantization\">Dynamic / Runtime Quantization</a> section for a detailed discourse on this topic.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Post-Training Quantization (PTQ)</strong>: Converts a fully trained high-precision model to a lower-precision format without retraining. PTQ typically uses a calibration dataset to compute appropriate <code class=\"language-plaintext highlighter-rouge\">scale</code> and <code class=\"language-plaintext highlighter-rouge\">zero-point</code> values using strategies like min-max range or percentile clipping. It is simple to use and well-supported in frameworks such as TensorFlow Lite and PyTorch, but may incur accuracy loss—particularly for sensitive or activation-heavy layers.</p>\n\n        <ul>\n          <li>From the TensorFlow <a href=\"https://www.tensorflow.org/model_optimization/guide/quantization/post_training\">post-training quantization</a> documentation:</li>\n        </ul>\n\n        <blockquote>\n          <p>We generally recommend 16-bit floats for GPU acceleration and 8-bit integer for CPU execution.</p>\n        </blockquote>\n\n        <ul>\n          <li>This reflects hardware preferences: <code class=\"language-plaintext highlighter-rouge\">float16</code> enables faster matrix multiplications on GPU accelerators like Tensor Cores, while <code class=\"language-plaintext highlighter-rouge\">int8</code> is more efficient on CPU architectures with dedicated integer units such as integer SIMD extensions (e.g., AVX, NEON, VNNI).</li>\n          <li>Look up the <a href=\"#post-training-quantization\">Post-Training Quantization</a> section for a detailed discourse on this topic.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Quantization-Aware Training (QAT)</strong>: In QAT, quantization effects—specifically the non-linearity introduced by rounding and clipping—are simulated during training, allowing the model to adapt. The model behaves as though it operates in lower precision during the forward pass, using the quantization formula above with fake quantization modules (e.g., <code class=\"language-plaintext highlighter-rouge\">FakeQuantize</code> in PyTorch or <code class=\"language-plaintext highlighter-rouge\">tf.quantization.fake_quant_with_min_max_vars</code> in TensorFlow). These modules apply quantization and dequantization logic using scale and zero-point. However, backpropagation remains in full precision. In other words, gradients and parameter updates are still computed using full <code class=\"language-plaintext highlighter-rouge\">float32</code> precision. This allows the model to adapt to quantization-induced noise, often resulting in better accuracy retention compared to PTQ. QAT is especially useful when targeting very low-bit formats (such as 4-bit or lower) or when quantizing sensitive components like attention layers in transformers or deploying models in high-accuracy applications.</p>\n        <ul>\n          <li>Look up the <a href=\"#quantizationaware-training-qat\">Quantization‑aware Training (QAT)</a> section for a detailed discourse on this topic.</li>\n        </ul>\n      </li>\n    </ul>\n<p><strong>Dynamic / Runtime Quantization</strong>: This method quantizes <strong>model weights statically</strong> (e.g. to <code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">float16</code>), while <strong>activations remain in full precision until runtime</strong>, where they are quantized dynamically at each inference step right before computation. It requires no calibration dataset and no fine‑tuning, making it the easiest quantization method provided by PyTorch. It is particularly effective for models dominated by weight‑heavy layers such as <code class=\"language-plaintext highlighter-rouge\">torch.nn.Linear</code>, recurrent layers (<code class=\"language-plaintext highlighter-rouge\">nn.LSTM</code>, <code class=\"language-plaintext highlighter-rouge\">nn.GRU</code>), and transformers. In PyTorch, this is implemented via the function <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.ao.quantization.quantize_dynamic\"><code class=\"language-plaintext highlighter-rouge\">quantize_dynamic</code></a>, for example:</p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"n\">quantized_model</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">ao</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">quantize_dynamic</span><span class=\"p\">(</span>\n    <span class=\"n\">model_float32</span><span class=\"p\">,</span>\n    <span class=\"p\">{</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">,</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">LSTM</span><span class=\"p\">},</span>\n    <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">qint8</span>\n<span class=\"p\">)</span>\n</code></pre>\n<ul>\n          <li>With this approach, the quantized model is memory‑efficient and can accelerate inference for NLP architectures, often with negligible accuracy loss compared to PTQ—though with lower benefit on convolution‑heavy vision models.</li>\n          <li>Look up the <a href=\"#dynamic--runtime-quantization\">Dynamic / Runtime Quantization</a> section for a detailed discourse on this topic.</li>\n        </ul>\n<p><strong>Post-Training Quantization (PTQ)</strong>: Converts a fully trained high-precision model to a lower-precision format without retraining. PTQ typically uses a calibration dataset to compute appropriate <code class=\"language-plaintext highlighter-rouge\">scale</code> and <code class=\"language-plaintext highlighter-rouge\">zero-point</code> values using strategies like min-max range or percentile clipping. It is simple to use and well-supported in frameworks such as TensorFlow Lite and PyTorch, but may incur accuracy loss—particularly for sensitive or activation-heavy layers.</p>\n<ul>\n          <li>From the TensorFlow <a href=\"https://www.tensorflow.org/model_optimization/guide/quantization/post_training\">post-training quantization</a> documentation:</li>\n        </ul>\n<blockquote>\n          <p>We generally recommend 16-bit floats for GPU acceleration and 8-bit integer for CPU execution.</p>\n        </blockquote>\n<p>We generally recommend 16-bit floats for GPU acceleration and 8-bit integer for CPU execution.</p>\n<ul>\n          <li>This reflects hardware preferences: <code class=\"language-plaintext highlighter-rouge\">float16</code> enables faster matrix multiplications on GPU accelerators like Tensor Cores, while <code class=\"language-plaintext highlighter-rouge\">int8</code> is more efficient on CPU architectures with dedicated integer units such as integer SIMD extensions (e.g., AVX, NEON, VNNI).</li>\n          <li>Look up the <a href=\"#post-training-quantization\">Post-Training Quantization</a> section for a detailed discourse on this topic.</li>\n        </ul>\n<p><strong>Quantization-Aware Training (QAT)</strong>: In QAT, quantization effects—specifically the non-linearity introduced by rounding and clipping—are simulated during training, allowing the model to adapt. The model behaves as though it operates in lower precision during the forward pass, using the quantization formula above with fake quantization modules (e.g., <code class=\"language-plaintext highlighter-rouge\">FakeQuantize</code> in PyTorch or <code class=\"language-plaintext highlighter-rouge\">tf.quantization.fake_quant_with_min_max_vars</code> in TensorFlow). These modules apply quantization and dequantization logic using scale and zero-point. However, backpropagation remains in full precision. In other words, gradients and parameter updates are still computed using full <code class=\"language-plaintext highlighter-rouge\">float32</code> precision. This allows the model to adapt to quantization-induced noise, often resulting in better accuracy retention compared to PTQ. QAT is especially useful when targeting very low-bit formats (such as 4-bit or lower) or when quantizing sensitive components like attention layers in transformers or deploying models in high-accuracy applications.</p>\n<ul>\n          <li>Look up the <a href=\"#quantizationaware-training-qat\">Quantization‑aware Training (QAT)</a> section for a detailed discourse on this topic.</li>\n        </ul>",
      "contentMarkdown": "*   There are three main workflows/approaches to apply quantization:\n    \n    *   **Dynamic / Runtime Quantization**: This method quantizes **model weights statically** (e.g. to `int8` or `float16`), while **activations remain in full precision until runtime**, where they are quantized dynamically at each inference step right before computation. It requires no calibration dataset and no fine‑tuning, making it the easiest quantization method provided by PyTorch. It is particularly effective for models dominated by weight‑heavy layers such as `torch.nn.Linear`, recurrent layers (`nn.LSTM`, `nn.GRU`), and transformers. In PyTorch, this is implemented via the function [`quantize_dynamic`](https://pytorch.org/docs/stable/quantization.html#torch.ao.quantization.quantize_dynamic), for example:\n        \n        ![](https://aman.ai/images/copy.png)\n        \n        `import torch quantized_model = torch.ao.quantization.quantize_dynamic(     model_float32,     {torch.nn.Linear, torch.nn.LSTM},     dtype=torch.qint8 )`\n        \n        *   With this approach, the quantized model is memory‑efficient and can accelerate inference for NLP architectures, often with negligible accuracy loss compared to PTQ—though with lower benefit on convolution‑heavy vision models.\n        *   Look up the [Dynamic / Runtime Quantization](#dynamic--runtime-quantization) section for a detailed discourse on this topic.\n    *   **Post-Training Quantization (PTQ)**: Converts a fully trained high-precision model to a lower-precision format without retraining. PTQ typically uses a calibration dataset to compute appropriate `scale` and `zero-point` values using strategies like min-max range or percentile clipping. It is simple to use and well-supported in frameworks such as TensorFlow Lite and PyTorch, but may incur accuracy loss—particularly for sensitive or activation-heavy layers.\n        \n        *   From the TensorFlow [post-training quantization](https://www.tensorflow.org/model_optimization/guide/quantization/post_training) documentation:\n        \n        > We generally recommend 16-bit floats for GPU acceleration and 8-bit integer for CPU execution.\n        \n        *   This reflects hardware preferences: `float16` enables faster matrix multiplications on GPU accelerators like Tensor Cores, while `int8` is more efficient on CPU architectures with dedicated integer units such as integer SIMD extensions (e.g., AVX, NEON, VNNI).\n        *   Look up the [Post-Training Quantization](#post-training-quantization) section for a detailed discourse on this topic.\n    *   **Quantization-Aware Training (QAT)**: In QAT, quantization effects—specifically the non-linearity introduced by rounding and clipping—are simulated during training, allowing the model to adapt. The model behaves as though it operates in lower precision during the forward pass, using the quantization formula above with fake quantization modules (e.g., `FakeQuantize` in PyTorch or `tf.quantization.fake_quant_with_min_max_vars` in TensorFlow). These modules apply quantization and dequantization logic using scale and zero-point. However, backpropagation remains in full precision. In other words, gradients and parameter updates are still computed using full `float32` precision. This allows the model to adapt to quantization-induced noise, often resulting in better accuracy retention compared to PTQ. QAT is especially useful when targeting very low-bit formats (such as 4-bit or lower) or when quantizing sensitive components like attention layers in transformers or deploying models in high-accuracy applications.\n        \n        *   Look up the [Quantization‑aware Training (QAT)](#quantizationaware-training-qat) section for a detailed discourse on this topic.\n\nThere are three main workflows/approaches to apply quantization:\n\n*   **Dynamic / Runtime Quantization**: This method quantizes **model weights statically** (e.g. to `int8` or `float16`), while **activations remain in full precision until runtime**, where they are quantized dynamically at each inference step right before computation. It requires no calibration dataset and no fine‑tuning, making it the easiest quantization method provided by PyTorch. It is particularly effective for models dominated by weight‑heavy layers such as `torch.nn.Linear`, recurrent layers (`nn.LSTM`, `nn.GRU`), and transformers. In PyTorch, this is implemented via the function [`quantize_dynamic`](https://pytorch.org/docs/stable/quantization.html#torch.ao.quantization.quantize_dynamic), for example:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n    `import torch quantized_model = torch.ao.quantization.quantize_dynamic(     model_float32,     {torch.nn.Linear, torch.nn.LSTM},     dtype=torch.qint8 )`\n    \n    *   With this approach, the quantized model is memory‑efficient and can accelerate inference for NLP architectures, often with negligible accuracy loss compared to PTQ—though with lower benefit on convolution‑heavy vision models.\n    *   Look up the [Dynamic / Runtime Quantization](#dynamic--runtime-quantization) section for a detailed discourse on this topic.\n*   **Post-Training Quantization (PTQ)**: Converts a fully trained high-precision model to a lower-precision format without retraining. PTQ typically uses a calibration dataset to compute appropriate `scale` and `zero-point` values using strategies like min-max range or percentile clipping. It is simple to use and well-supported in frameworks such as TensorFlow Lite and PyTorch, but may incur accuracy loss—particularly for sensitive or activation-heavy layers.\n    \n    *   From the TensorFlow [post-training quantization](https://www.tensorflow.org/model_optimization/guide/quantization/post_training) documentation:\n    \n    > We generally recommend 16-bit floats for GPU acceleration and 8-bit integer for CPU execution.\n    \n    *   This reflects hardware preferences: `float16` enables faster matrix multiplications on GPU accelerators like Tensor Cores, while `int8` is more efficient on CPU architectures with dedicated integer units such as integer SIMD extensions (e.g., AVX, NEON, VNNI).\n    *   Look up the [Post-Training Quantization](#post-training-quantization) section for a detailed discourse on this topic.\n*   **Quantization-Aware Training (QAT)**: In QAT, quantization effects—specifically the non-linearity introduced by rounding and clipping—are simulated during training, allowing the model to adapt. The model behaves as though it operates in lower precision during the forward pass, using the quantization formula above with fake quantization modules (e.g., `FakeQuantize` in PyTorch or `tf.quantization.fake_quant_with_min_max_vars` in TensorFlow). These modules apply quantization and dequantization logic using scale and zero-point. However, backpropagation remains in full precision. In other words, gradients and parameter updates are still computed using full `float32` precision. This allows the model to adapt to quantization-induced noise, often resulting in better accuracy retention compared to PTQ. QAT is especially useful when targeting very low-bit formats (such as 4-bit or lower) or when quantizing sensitive components like attention layers in transformers or deploying models in high-accuracy applications.\n    \n    *   Look up the [Quantization‑aware Training (QAT)](#quantizationaware-training-qat) section for a detailed discourse on this topic.\n\n**Dynamic / Runtime Quantization**: This method quantizes **model weights statically** (e.g. to `int8` or `float16`), while **activations remain in full precision until runtime**, where they are quantized dynamically at each inference step right before computation. It requires no calibration dataset and no fine‑tuning, making it the easiest quantization method provided by PyTorch. It is particularly effective for models dominated by weight‑heavy layers such as `torch.nn.Linear`, recurrent layers (`nn.LSTM`, `nn.GRU`), and transformers. In PyTorch, this is implemented via the function [`quantize_dynamic`](https://pytorch.org/docs/stable/quantization.html#torch.ao.quantization.quantize_dynamic), for example:\n\n![](https://aman.ai/images/copy.png)\n\n`import torch quantized_model = torch.ao.quantization.quantize_dynamic(     model_float32,     {torch.nn.Linear, torch.nn.LSTM},     dtype=torch.qint8 )`\n\n*   With this approach, the quantized model is memory‑efficient and can accelerate inference for NLP architectures, often with negligible accuracy loss compared to PTQ—though with lower benefit on convolution‑heavy vision models.\n*   Look up the [Dynamic / Runtime Quantization](#dynamic--runtime-quantization) section for a detailed discourse on this topic.\n\n**Post-Training Quantization (PTQ)**: Converts a fully trained high-precision model to a lower-precision format without retraining. PTQ typically uses a calibration dataset to compute appropriate `scale` and `zero-point` values using strategies like min-max range or percentile clipping. It is simple to use and well-supported in frameworks such as TensorFlow Lite and PyTorch, but may incur accuracy loss—particularly for sensitive or activation-heavy layers.\n\n*   From the TensorFlow [post-training quantization](https://www.tensorflow.org/model_optimization/guide/quantization/post_training) documentation:\n\n> We generally recommend 16-bit floats for GPU acceleration and 8-bit integer for CPU execution.\n\nWe generally recommend 16-bit floats for GPU acceleration and 8-bit integer for CPU execution.\n\n*   This reflects hardware preferences: `float16` enables faster matrix multiplications on GPU accelerators like Tensor Cores, while `int8` is more efficient on CPU architectures with dedicated integer units such as integer SIMD extensions (e.g., AVX, NEON, VNNI).\n*   Look up the [Post-Training Quantization](#post-training-quantization) section for a detailed discourse on this topic.\n\n**Quantization-Aware Training (QAT)**: In QAT, quantization effects—specifically the non-linearity introduced by rounding and clipping—are simulated during training, allowing the model to adapt. The model behaves as though it operates in lower precision during the forward pass, using the quantization formula above with fake quantization modules (e.g., `FakeQuantize` in PyTorch or `tf.quantization.fake_quant_with_min_max_vars` in TensorFlow). These modules apply quantization and dequantization logic using scale and zero-point. However, backpropagation remains in full precision. In other words, gradients and parameter updates are still computed using full `float32` precision. This allows the model to adapt to quantization-induced noise, often resulting in better accuracy retention compared to PTQ. QAT is especially useful when targeting very low-bit formats (such as 4-bit or lower) or when quantizing sensitive components like attention layers in transformers or deploying models in high-accuracy applications.\n\n*   Look up the [Quantization‑aware Training (QAT)](#quantizationaware-training-qat) section for a detailed discourse on this topic.",
      "order": 6,
      "orderInChapter": 6,
      "difficulty": 4,
      "estimatedMinutes": 7,
      "tags": [
        "ondevice ai",
        "transformer",
        "attention",
        "convolution",
        "lstm",
        "gru",
        "nlp",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 1300,
        "contentLength": 17606
      },
      "nextCards": [
        "ai-model-compression-benefits-and-limitations-7",
        "ai-model-compression-mitigation-strategies-8"
      ],
      "relatedCards": [
        "ai-on-device-transformers-balancing-compute-and-memory-prefill-vs-decode-opt-8",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-cpu-deployment-considerations-17",
        "ai-on-device-transformers-embedding-size-times-vocabulary-size-times-depth-26",
        "ai-on-device-transformers-parameter-tuning-recipes-for-ml-runtimes-27"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#quantization-workflows",
      "scrapedAt": "2025-12-28T11:55:50.967Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-benefits-and-limitations-7",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Benefits and Limitations",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>Quantization can lead to significant reductions in efficiency, both in terms of memory footprint and computational load. For example, converting <code class=\"language-plaintext highlighter-rouge\">float32</code> weights to <code class=\"language-plaintext highlighter-rouge\">int8</code> reduces storage requirements by 75%, and on supported hardware, can improve inference speed by 2× to 4×. These benefits are amplified on edge devices with limited memory, power, and compute such as mobile phones, IoT sensors, embedded processors, and accelerators that support low-precision execution.</li>\n</ul>\n<blockquote>\n  <p>While quantization offers compelling advantages, it can also suffer from practical limitations. Quantization might not work uniformly well across all architectures or layers since some operators (powering these layers and architectures) might not be supported in quantized form on all hardware targets. Put simply, some hardware can lack native support for certain quantized operators, since they typically have to be implemented individually. Operators like group convolutions, custom layers, normalization approaches (say LayerNorm), etc. may fall back to <code class=\"language-plaintext highlighter-rouge\">float32</code> or require custom low-level kernels, potentially limiting compatibility or efficiency on target platforms. Lastly, layers with small value ranges, heavy outliers, or complex nonlinear interactions may require higher precision (e.g., <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">float32</code>) to avoid accuracy degradation.</p>\n</blockquote>\n<p>While quantization offers compelling advantages, it can also suffer from practical limitations. Quantization might not work uniformly well across all architectures or layers since some operators (powering these layers and architectures) might not be supported in quantized form on all hardware targets. Put simply, some hardware can lack native support for certain quantized operators, since they typically have to be implemented individually. Operators like group convolutions, custom layers, normalization approaches (say LayerNorm), etc. may fall back to <code class=\"language-plaintext highlighter-rouge\">float32</code> or require custom low-level kernels, potentially limiting compatibility or efficiency on target platforms. Lastly, layers with small value ranges, heavy outliers, or complex nonlinear interactions may require higher precision (e.g., <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">float32</code>) to avoid accuracy degradation.</p>",
      "contentMarkdown": "*   Quantization can lead to significant reductions in efficiency, both in terms of memory footprint and computational load. For example, converting `float32` weights to `int8` reduces storage requirements by 75%, and on supported hardware, can improve inference speed by 2× to 4×. These benefits are amplified on edge devices with limited memory, power, and compute such as mobile phones, IoT sensors, embedded processors, and accelerators that support low-precision execution.\n\n> While quantization offers compelling advantages, it can also suffer from practical limitations. Quantization might not work uniformly well across all architectures or layers since some operators (powering these layers and architectures) might not be supported in quantized form on all hardware targets. Put simply, some hardware can lack native support for certain quantized operators, since they typically have to be implemented individually. Operators like group convolutions, custom layers, normalization approaches (say LayerNorm), etc. may fall back to `float32` or require custom low-level kernels, potentially limiting compatibility or efficiency on target platforms. Lastly, layers with small value ranges, heavy outliers, or complex nonlinear interactions may require higher precision (e.g., `float16` or `float32`) to avoid accuracy degradation.\n\nWhile quantization offers compelling advantages, it can also suffer from practical limitations. Quantization might not work uniformly well across all architectures or layers since some operators (powering these layers and architectures) might not be supported in quantized form on all hardware targets. Put simply, some hardware can lack native support for certain quantized operators, since they typically have to be implemented individually. Operators like group convolutions, custom layers, normalization approaches (say LayerNorm), etc. may fall back to `float32` or require custom low-level kernels, potentially limiting compatibility or efficiency on target platforms. Lastly, layers with small value ranges, heavy outliers, or complex nonlinear interactions may require higher precision (e.g., `float16` or `float32`) to avoid accuracy degradation.",
      "order": 7,
      "orderInChapter": 7,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "convolution"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 300,
        "contentLength": 2699
      },
      "nextCards": [
        "ai-model-compression-mitigation-strategies-8",
        "ai-model-compression-weights-vs-activation-quantization-9"
      ],
      "relatedCards": [
        "ai-on-device-transformers-tensor-processing-unit-tpu-5",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-mime-9",
        "ai-federated-learning-tooling-frameworks-14"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#benefits-and-limitations",
      "scrapedAt": "2025-12-28T11:55:50.967Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-mitigation-strategies-8",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Mitigation Strategies",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>\n    <p>Selective strategies like <strong>per-channel</strong>, <strong>per-group</strong>, <strong>per-layer</strong>, <strong>per-tensor</strong>, and <strong>mixed-precision quantization</strong> are commonly used to mitigate limitations such as accuracy loss, hardware incompatibilities, and uneven value distributions across layers.</p>\n  </li>\n  <li>\n    <p><strong>Per-channel quantization</strong> (also referred to as <em>channel-wise quantization</em>): This approach assigns separate scale and zero-point values to each output channel of a weight tensor. It is particularly effective in convolutional and linear layers where each output channel (or filter) may have significantly different weight distributions. By capturing channel-wise variations in magnitude, it provides better quantization accuracy, especially in vision models like <strong>ResNet</strong>, <strong>MobileNet</strong>, and <strong>EfficientNet</strong>, as well as transformer-based architectures such as <strong>BERT</strong> and <strong>GPT</strong>. PyTorch implements this using the <code class=\"language-plaintext highlighter-rouge\">torch.per_channel_affine</code> quantization scheme.</p>\n  </li>\n  <li>\n    <p><strong>Per-group quantization</strong> (also referred to as <em>group-wise quantization</em>): A compromise between per-tensor and per-channel quantization. Channels are divided into groups, with each group sharing quantization parameters. This reduces the overhead of storing separate scale/zero-point values for every channel, while still preserving more distributional information than per-tensor quantization. It is particularly useful in resource-constrained deployment scenarios where memory and compute costs must be balanced against accuracy. Though not always exposed via high-level APIs in frameworks like PyTorch, this strategy is supported in certain hardware accelerators and vendor-specific toolchains (e.g., Qualcomm, MediaTek, Xilinx).</p>\n  </li>\n  <li>\n    <p><strong>Per-layer quantization</strong> (also referred to as <em>layer-wise quantization</em>): This applies the same quantization parameters (scale and zero-point) across an entire layer’s output or weight tensor. For example, all outputs of a linear layer or all weights of a convolution kernel are quantized using a single shared set of parameters. This method is computationally efficient and requires minimal additional metadata, making it widely used in low-resource settings or for fast prototyping. However, it often leads to higher quantization error in layers with highly varied internal distributions.</p>\n  </li>\n  <li>\n    <p><strong>Per-tensor quantization</strong> (also referred to as <em>tensor-wise quantization</em>): A special case of per-layer quantization where quantization is applied uniformly across an entire tensor (typically weight or activation). A single scale and zero-point are calculated for the full tensor, regardless of dimensionality or channel boundaries. This is the simplest and most lightweight quantization method, requiring minimal bookkeeping and fast execution. While effective for layers with narrow and uniform value ranges, it can result in significant information loss when used on tensors with wide dynamic range or uneven channel statistics. This method is often the default in early-stage quantization workflows or on hardware that does not support fine-grained schemes.</p>\n  </li>\n  <li>\n    <p><strong>Mixed-precision quantization</strong>: Instead of applying uniform quantization across all model layers, mixed-precision quantization selectively retains higher-precision (e.g., <code class=\"language-plaintext highlighter-rouge\">float32</code> or <code class=\"language-plaintext highlighter-rouge\">float16</code>) computation in layers that are sensitive to quantization noise—such as attention heads, layer normalization, or output classifiers. Other layers, particularly early convolution blocks or MLPs, can be safely quantized to <code class=\"language-plaintext highlighter-rouge\">int8</code> or lower. This approach enables developers to achieve a favorable trade-off between accuracy and efficiency. Mixed-precision is supported by most modern inference engines including <strong>TensorRT</strong>, <strong>TVM</strong>, <strong>XNNPACK</strong>, and <strong>PyTorch FX Graph Mode Quantization</strong>.</p>\n  </li>\n</ul>\n<p>Selective strategies like <strong>per-channel</strong>, <strong>per-group</strong>, <strong>per-layer</strong>, <strong>per-tensor</strong>, and <strong>mixed-precision quantization</strong> are commonly used to mitigate limitations such as accuracy loss, hardware incompatibilities, and uneven value distributions across layers.</p>\n<p><strong>Per-channel quantization</strong> (also referred to as <em>channel-wise quantization</em>): This approach assigns separate scale and zero-point values to each output channel of a weight tensor. It is particularly effective in convolutional and linear layers where each output channel (or filter) may have significantly different weight distributions. By capturing channel-wise variations in magnitude, it provides better quantization accuracy, especially in vision models like <strong>ResNet</strong>, <strong>MobileNet</strong>, and <strong>EfficientNet</strong>, as well as transformer-based architectures such as <strong>BERT</strong> and <strong>GPT</strong>. PyTorch implements this using the <code class=\"language-plaintext highlighter-rouge\">torch.per_channel_affine</code> quantization scheme.</p>\n<p><strong>Per-group quantization</strong> (also referred to as <em>group-wise quantization</em>): A compromise between per-tensor and per-channel quantization. Channels are divided into groups, with each group sharing quantization parameters. This reduces the overhead of storing separate scale/zero-point values for every channel, while still preserving more distributional information than per-tensor quantization. It is particularly useful in resource-constrained deployment scenarios where memory and compute costs must be balanced against accuracy. Though not always exposed via high-level APIs in frameworks like PyTorch, this strategy is supported in certain hardware accelerators and vendor-specific toolchains (e.g., Qualcomm, MediaTek, Xilinx).</p>\n<p><strong>Per-layer quantization</strong> (also referred to as <em>layer-wise quantization</em>): This applies the same quantization parameters (scale and zero-point) across an entire layer’s output or weight tensor. For example, all outputs of a linear layer or all weights of a convolution kernel are quantized using a single shared set of parameters. This method is computationally efficient and requires minimal additional metadata, making it widely used in low-resource settings or for fast prototyping. However, it often leads to higher quantization error in layers with highly varied internal distributions.</p>\n<p><strong>Per-tensor quantization</strong> (also referred to as <em>tensor-wise quantization</em>): A special case of per-layer quantization where quantization is applied uniformly across an entire tensor (typically weight or activation). A single scale and zero-point are calculated for the full tensor, regardless of dimensionality or channel boundaries. This is the simplest and most lightweight quantization method, requiring minimal bookkeeping and fast execution. While effective for layers with narrow and uniform value ranges, it can result in significant information loss when used on tensors with wide dynamic range or uneven channel statistics. This method is often the default in early-stage quantization workflows or on hardware that does not support fine-grained schemes.</p>\n<p><strong>Mixed-precision quantization</strong>: Instead of applying uniform quantization across all model layers, mixed-precision quantization selectively retains higher-precision (e.g., <code class=\"language-plaintext highlighter-rouge\">float32</code> or <code class=\"language-plaintext highlighter-rouge\">float16</code>) computation in layers that are sensitive to quantization noise—such as attention heads, layer normalization, or output classifiers. Other layers, particularly early convolution blocks or MLPs, can be safely quantized to <code class=\"language-plaintext highlighter-rouge\">int8</code> or lower. This approach enables developers to achieve a favorable trade-off between accuracy and efficiency. Mixed-precision is supported by most modern inference engines including <strong>TensorRT</strong>, <strong>TVM</strong>, <strong>XNNPACK</strong>, and <strong>PyTorch FX Graph Mode Quantization</strong>.</p>",
      "contentMarkdown": "*   Selective strategies like **per-channel**, **per-group**, **per-layer**, **per-tensor**, and **mixed-precision quantization** are commonly used to mitigate limitations such as accuracy loss, hardware incompatibilities, and uneven value distributions across layers.\n    \n*   **Per-channel quantization** (also referred to as _channel-wise quantization_): This approach assigns separate scale and zero-point values to each output channel of a weight tensor. It is particularly effective in convolutional and linear layers where each output channel (or filter) may have significantly different weight distributions. By capturing channel-wise variations in magnitude, it provides better quantization accuracy, especially in vision models like **ResNet**, **MobileNet**, and **EfficientNet**, as well as transformer-based architectures such as **BERT** and **GPT**. PyTorch implements this using the `torch.per_channel_affine` quantization scheme.\n    \n*   **Per-group quantization** (also referred to as _group-wise quantization_): A compromise between per-tensor and per-channel quantization. Channels are divided into groups, with each group sharing quantization parameters. This reduces the overhead of storing separate scale/zero-point values for every channel, while still preserving more distributional information than per-tensor quantization. It is particularly useful in resource-constrained deployment scenarios where memory and compute costs must be balanced against accuracy. Though not always exposed via high-level APIs in frameworks like PyTorch, this strategy is supported in certain hardware accelerators and vendor-specific toolchains (e.g., Qualcomm, MediaTek, Xilinx).\n    \n*   **Per-layer quantization** (also referred to as _layer-wise quantization_): This applies the same quantization parameters (scale and zero-point) across an entire layer’s output or weight tensor. For example, all outputs of a linear layer or all weights of a convolution kernel are quantized using a single shared set of parameters. This method is computationally efficient and requires minimal additional metadata, making it widely used in low-resource settings or for fast prototyping. However, it often leads to higher quantization error in layers with highly varied internal distributions.\n    \n*   **Per-tensor quantization** (also referred to as _tensor-wise quantization_): A special case of per-layer quantization where quantization is applied uniformly across an entire tensor (typically weight or activation). A single scale and zero-point are calculated for the full tensor, regardless of dimensionality or channel boundaries. This is the simplest and most lightweight quantization method, requiring minimal bookkeeping and fast execution. While effective for layers with narrow and uniform value ranges, it can result in significant information loss when used on tensors with wide dynamic range or uneven channel statistics. This method is often the default in early-stage quantization workflows or on hardware that does not support fine-grained schemes.\n    \n*   **Mixed-precision quantization**: Instead of applying uniform quantization across all model layers, mixed-precision quantization selectively retains higher-precision (e.g., `float32` or `float16`) computation in layers that are sensitive to quantization noise—such as attention heads, layer normalization, or output classifiers. Other layers, particularly early convolution blocks or MLPs, can be safely quantized to `int8` or lower. This approach enables developers to achieve a favorable trade-off between accuracy and efficiency. Mixed-precision is supported by most modern inference engines including **TensorRT**, **TVM**, **XNNPACK**, and **PyTorch FX Graph Mode Quantization**.\n    \n\nSelective strategies like **per-channel**, **per-group**, **per-layer**, **per-tensor**, and **mixed-precision quantization** are commonly used to mitigate limitations such as accuracy loss, hardware incompatibilities, and uneven value distributions across layers.\n\n**Per-channel quantization** (also referred to as _channel-wise quantization_): This approach assigns separate scale and zero-point values to each output channel of a weight tensor. It is particularly effective in convolutional and linear layers where each output channel (or filter) may have significantly different weight distributions. By capturing channel-wise variations in magnitude, it provides better quantization accuracy, especially in vision models like **ResNet**, **MobileNet**, and **EfficientNet**, as well as transformer-based architectures such as **BERT** and **GPT**. PyTorch implements this using the `torch.per_channel_affine` quantization scheme.\n\n**Per-group quantization** (also referred to as _group-wise quantization_): A compromise between per-tensor and per-channel quantization. Channels are divided into groups, with each group sharing quantization parameters. This reduces the overhead of storing separate scale/zero-point values for every channel, while still preserving more distributional information than per-tensor quantization. It is particularly useful in resource-constrained deployment scenarios where memory and compute costs must be balanced against accuracy. Though not always exposed via high-level APIs in frameworks like PyTorch, this strategy is supported in certain hardware accelerators and vendor-specific toolchains (e.g., Qualcomm, MediaTek, Xilinx).\n\n**Per-layer quantization** (also referred to as _layer-wise quantization_): This applies the same quantization parameters (scale and zero-point) across an entire layer’s output or weight tensor. For example, all outputs of a linear layer or all weights of a convolution kernel are quantized using a single shared set of parameters. This method is computationally efficient and requires minimal additional metadata, making it widely used in low-resource settings or for fast prototyping. However, it often leads to higher quantization error in layers with highly varied internal distributions.\n\n**Per-tensor quantization** (also referred to as _tensor-wise quantization_): A special case of per-layer quantization where quantization is applied uniformly across an entire tensor (typically weight or activation). A single scale and zero-point are calculated for the full tensor, regardless of dimensionality or channel boundaries. This is the simplest and most lightweight quantization method, requiring minimal bookkeeping and fast execution. While effective for layers with narrow and uniform value ranges, it can result in significant information loss when used on tensors with wide dynamic range or uneven channel statistics. This method is often the default in early-stage quantization workflows or on hardware that does not support fine-grained schemes.\n\n**Mixed-precision quantization**: Instead of applying uniform quantization across all model layers, mixed-precision quantization selectively retains higher-precision (e.g., `float32` or `float16`) computation in layers that are sensitive to quantization noise—such as attention heads, layer normalization, or output classifiers. Other layers, particularly early convolution blocks or MLPs, can be safely quantized to `int8` or lower. This approach enables developers to achieve a favorable trade-off between accuracy and efficiency. Mixed-precision is supported by most modern inference engines including **TensorRT**, **TVM**, **XNNPACK**, and **PyTorch FX Graph Mode Quantization**.",
      "order": 8,
      "orderInChapter": 8,
      "difficulty": 3,
      "estimatedMinutes": 5,
      "tags": [
        "ondevice ai",
        "transformer",
        "attention",
        "convolution",
        "bert",
        "gpt",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 962,
        "contentLength": 8624
      },
      "nextCards": [
        "ai-model-compression-weights-vs-activation-quantization-9",
        "ai-model-compression-quantization-with-pytorch-10"
      ],
      "relatedCards": [
        "ai-on-device-transformers-modelembedding-dimension-23",
        "ai-on-device-transformers-sequence-length-and-kv-cache-size-24",
        "ai-on-device-transformers-balancing-compute-and-memory-prefill-vs-decode-opt-8",
        "ai-on-device-transformers-tokenizer-and-vocabulary-size-22",
        "ai-on-device-transformers-parameter-count-and-model-depth-25"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#mitigation-strategies",
      "scrapedAt": "2025-12-28T11:55:50.967Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-weights-vs-activation-quantization-9",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Weights vs. Activation Quantization",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>\n    <p>Why they’re not the same: Weights and activations have fundamentally different roles and constraints during quantization.</p>\n\n    <ul>\n      <li>\n        <p>Weights are static once training completes. Since they do not vary across inputs, they may be quantized offline using fixed calibration data or analytically via heuristics such as min‑max scaling or percentile statistics. This allows more aggressive optimization techniques such as <em>per‑channel quantization</em> or <em>non‑uniform quantization</em>, and values may be precomputed and stored in compact low‑precision formats like <code class=\"language-plaintext highlighter-rouge\">int8</code>, <code class=\"language-plaintext highlighter-rouge\">int4</code>, or binary.</p>\n      </li>\n      <li>\n        <p>Activations, by contrast, are dynamic and input‑dependent. Their value ranges vary based on data processed during inference. Therefore, activation quantization must accommodate runtime variability. Two common modes exist:</p>\n\n        <ul>\n          <li>\n            <p><strong>Static Activation Quantization</strong>: Calibration datasets estimate typical activation ranges (min/max or histogram‑based) per layer. These statistics are then used to assign fixed scale/zero‑point pairs for quantized representation, using observers such as <a href=\"https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.MinMaxObserver.html\"><code class=\"language-plaintext highlighter-rouge\">MinMaxObserver</code></a> or histogram‑based observers.</p>\n          </li>\n          <li>\n            <p><strong>Dynamic Activation Quantization</strong>: During inference, scale and zero‑point values are computed on‑the‑fly from input‑dependent statistics (e.g. dynamic min/max per batch). This avoids calibration datasets but may add latency due to runtime computation.</p>\n          </li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>Handling outliers: Outliers in activation or weight distributions can drastically degrade quantization quality by skewing range and reducing effective precision. Several mitigation strategies include:</p>\n\n    <ul>\n      <li>\n        <p><strong>Percentile‑based Clipping</strong>: Rather than using absolute min/max, activations may be clipped to a percentile‑based range (e.g. 99.9%) to discard extreme outliers. Techniques include KL divergence minimization or MSE‑based clipping, used in frameworks such as PyTorch or TensorFlow Lite.</p>\n      </li>\n      <li>\n        <p><strong>Per‑Channel Quantization</strong>: Instead of applying a single scale across a tensor, per‑channel quantization assigns unique scale and zero‑point values per output channel. This adapts to local distribution variations, particularly in convolutional or linear layers. In PyTorch, this is implemented via <a href=\"https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html\">PerChannelMinMaxObserver</a> or similar observers using <a href=\"https://pytorch.org/docs/stable/quantization.html\"><code class=\"language-plaintext highlighter-rouge\">torch.per_channel_affine</code></a> schemes. Core functions include <a href=\"https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html\"><code class=\"language-plaintext highlighter-rouge\">torch.quantize_per_channel</code></a> and <a href=\"https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html\"><code class=\"language-plaintext highlighter-rouge\">torch.fake_quantize_per_channel_affine</code></a> to simulate quantization.</p>\n\n        <ul>\n          <li>\n            <p>Per‑channel quantization is especially effective in:</p>\n\n            <ul>\n              <li>Convolutional neural networks (e.g. ResNet, MobileNet, EfficientNet, YOLO), where filter/channels have distinct distribution ranges.</li>\n              <li>Transformer architectures (e.g. multi‑head attention or feed‑forward layers), where weight and activation distributions vary widely across heads or projection layers.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Learned Scale Factors</strong>: Methods like Learned Step Size Quantization (LSQ), introduced in the paper <a href=\"https://arxiv.org/abs/1902.08153\">“Learned Step Size Quantization”</a> by Esser et al. (2020), enable scale parameters to be optimized via backpropagation during fine‑tuning. This adaptive scaling is especially beneficial in models with skewed distributions—such as transformer-based language models, where operations like softmax produce skewed outputs.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>Advanced Weight Handling:</p>\n\n    <ul>\n      <li>\n        <p><strong>Per‑Group Quantization</strong>: Instead of full per‑channel (i.e. one scale per channel), per‑group quantization assigns one scale per group of channels (e.g. N channels or per row). This balances granularity and memory overhead and is prevalent in formats like ONNX or TensorRT.</p>\n      </li>\n      <li>\n        <p><strong>Activation‑Aware Weight Quantization (AWQ)</strong>: AWQ techniques tailor weight quantization ranges and groupings using activation patterns observed during calibration. Rather than uniformly quantizing weights, these methods use sensitivity analysis to allocate bit budgets or adjust grouping for performance‑critical weights.</p>\n      </li>\n      <li>\n        <p><strong>Zero‑Point Optimization</strong>: For symmetric quantization (typically weight tensors), zero‑point is fixed at zero. For asymmetric quantization—commonly used for activations—the zero‑point shifts the quantized range. Some frameworks (e.g. ONNX, TensorFlow Lite) allow fine‑grained control over zero‑point alignment, which influences both accuracy and hardware compatibility.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>In practice, weight and activation quantization are applied jointly but with distinct parameter sets and calibration workflows. Modern toolkits support fine-grained configuration, including:</p>\n\n    <ul>\n      <li>PyTorch’s <a href=\"https://pytorch.org/docs/stable/quantization.html\">torch.quantization</a> and <a href=\"https://pytorch.org/docs/stable/quantization-support.html\">torch.ao.quantization</a> modules;</li>\n      <li>TensorFlow Model Optimization Toolkit (<a href=\"https://www.tensorflow.org/model_optimization/guide/quantization\">quantization guide</a>) supporting calibration APIs and quantization-aware training;</li>\n      <li>NVIDIA TensorRT, which enables layer-wise quantization, quantization-aware training, and PTQ via its <a href=\"https://docs.nvidia.com/deeplearning/tensorrt/latest/index.html\">TensorRT SDK documentation</a> and the Torch‑TensorRT Model Optimizer;</li>\n      <li>Intel Neural Compressor, an open‑source framework offering post‑training static, dynamic, and quantization‑aware training workflows for PyTorch and TensorFlow (<a href=\"https://intel.github.io/neural-compressor/latest/docs/source/quantization.html\">Intel Neural Compressor documentation</a>).</li>\n    </ul>\n  </li>\n</ul>\n<p>Why they’re not the same: Weights and activations have fundamentally different roles and constraints during quantization.</p>\n<ul>\n      <li>\n        <p>Weights are static once training completes. Since they do not vary across inputs, they may be quantized offline using fixed calibration data or analytically via heuristics such as min‑max scaling or percentile statistics. This allows more aggressive optimization techniques such as <em>per‑channel quantization</em> or <em>non‑uniform quantization</em>, and values may be precomputed and stored in compact low‑precision formats like <code class=\"language-plaintext highlighter-rouge\">int8</code>, <code class=\"language-plaintext highlighter-rouge\">int4</code>, or binary.</p>\n      </li>\n      <li>\n        <p>Activations, by contrast, are dynamic and input‑dependent. Their value ranges vary based on data processed during inference. Therefore, activation quantization must accommodate runtime variability. Two common modes exist:</p>\n\n        <ul>\n          <li>\n            <p><strong>Static Activation Quantization</strong>: Calibration datasets estimate typical activation ranges (min/max or histogram‑based) per layer. These statistics are then used to assign fixed scale/zero‑point pairs for quantized representation, using observers such as <a href=\"https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.MinMaxObserver.html\"><code class=\"language-plaintext highlighter-rouge\">MinMaxObserver</code></a> or histogram‑based observers.</p>\n          </li>\n          <li>\n            <p><strong>Dynamic Activation Quantization</strong>: During inference, scale and zero‑point values are computed on‑the‑fly from input‑dependent statistics (e.g. dynamic min/max per batch). This avoids calibration datasets but may add latency due to runtime computation.</p>\n          </li>\n        </ul>\n      </li>\n    </ul>\n<p>Weights are static once training completes. Since they do not vary across inputs, they may be quantized offline using fixed calibration data or analytically via heuristics such as min‑max scaling or percentile statistics. This allows more aggressive optimization techniques such as <em>per‑channel quantization</em> or <em>non‑uniform quantization</em>, and values may be precomputed and stored in compact low‑precision formats like <code class=\"language-plaintext highlighter-rouge\">int8</code>, <code class=\"language-plaintext highlighter-rouge\">int4</code>, or binary.</p>\n<p>Activations, by contrast, are dynamic and input‑dependent. Their value ranges vary based on data processed during inference. Therefore, activation quantization must accommodate runtime variability. Two common modes exist:</p>\n<ul>\n          <li>\n            <p><strong>Static Activation Quantization</strong>: Calibration datasets estimate typical activation ranges (min/max or histogram‑based) per layer. These statistics are then used to assign fixed scale/zero‑point pairs for quantized representation, using observers such as <a href=\"https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.MinMaxObserver.html\"><code class=\"language-plaintext highlighter-rouge\">MinMaxObserver</code></a> or histogram‑based observers.</p>\n          </li>\n          <li>\n            <p><strong>Dynamic Activation Quantization</strong>: During inference, scale and zero‑point values are computed on‑the‑fly from input‑dependent statistics (e.g. dynamic min/max per batch). This avoids calibration datasets but may add latency due to runtime computation.</p>\n          </li>\n        </ul>\n<p><strong>Static Activation Quantization</strong>: Calibration datasets estimate typical activation ranges (min/max or histogram‑based) per layer. These statistics are then used to assign fixed scale/zero‑point pairs for quantized representation, using observers such as <a href=\"https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.MinMaxObserver.html\"><code class=\"language-plaintext highlighter-rouge\">MinMaxObserver</code></a> or histogram‑based observers.</p>\n<p><strong>Dynamic Activation Quantization</strong>: During inference, scale and zero‑point values are computed on‑the‑fly from input‑dependent statistics (e.g. dynamic min/max per batch). This avoids calibration datasets but may add latency due to runtime computation.</p>\n<p>Handling outliers: Outliers in activation or weight distributions can drastically degrade quantization quality by skewing range and reducing effective precision. Several mitigation strategies include:</p>\n<ul>\n      <li>\n        <p><strong>Percentile‑based Clipping</strong>: Rather than using absolute min/max, activations may be clipped to a percentile‑based range (e.g. 99.9%) to discard extreme outliers. Techniques include KL divergence minimization or MSE‑based clipping, used in frameworks such as PyTorch or TensorFlow Lite.</p>\n      </li>\n      <li>\n        <p><strong>Per‑Channel Quantization</strong>: Instead of applying a single scale across a tensor, per‑channel quantization assigns unique scale and zero‑point values per output channel. This adapts to local distribution variations, particularly in convolutional or linear layers. In PyTorch, this is implemented via <a href=\"https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html\">PerChannelMinMaxObserver</a> or similar observers using <a href=\"https://pytorch.org/docs/stable/quantization.html\"><code class=\"language-plaintext highlighter-rouge\">torch.per_channel_affine</code></a> schemes. Core functions include <a href=\"https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html\"><code class=\"language-plaintext highlighter-rouge\">torch.quantize_per_channel</code></a> and <a href=\"https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html\"><code class=\"language-plaintext highlighter-rouge\">torch.fake_quantize_per_channel_affine</code></a> to simulate quantization.</p>\n\n        <ul>\n          <li>\n            <p>Per‑channel quantization is especially effective in:</p>\n\n            <ul>\n              <li>Convolutional neural networks (e.g. ResNet, MobileNet, EfficientNet, YOLO), where filter/channels have distinct distribution ranges.</li>\n              <li>Transformer architectures (e.g. multi‑head attention or feed‑forward layers), where weight and activation distributions vary widely across heads or projection layers.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Learned Scale Factors</strong>: Methods like Learned Step Size Quantization (LSQ), introduced in the paper <a href=\"https://arxiv.org/abs/1902.08153\">“Learned Step Size Quantization”</a> by Esser et al. (2020), enable scale parameters to be optimized via backpropagation during fine‑tuning. This adaptive scaling is especially beneficial in models with skewed distributions—such as transformer-based language models, where operations like softmax produce skewed outputs.</p>\n      </li>\n    </ul>\n<p><strong>Percentile‑based Clipping</strong>: Rather than using absolute min/max, activations may be clipped to a percentile‑based range (e.g. 99.9%) to discard extreme outliers. Techniques include KL divergence minimization or MSE‑based clipping, used in frameworks such as PyTorch or TensorFlow Lite.</p>\n<p><strong>Per‑Channel Quantization</strong>: Instead of applying a single scale across a tensor, per‑channel quantization assigns unique scale and zero‑point values per output channel. This adapts to local distribution variations, particularly in convolutional or linear layers. In PyTorch, this is implemented via <a href=\"https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html\">PerChannelMinMaxObserver</a> or similar observers using <a href=\"https://pytorch.org/docs/stable/quantization.html\"><code class=\"language-plaintext highlighter-rouge\">torch.per_channel_affine</code></a> schemes. Core functions include <a href=\"https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html\"><code class=\"language-plaintext highlighter-rouge\">torch.quantize_per_channel</code></a> and <a href=\"https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html\"><code class=\"language-plaintext highlighter-rouge\">torch.fake_quantize_per_channel_affine</code></a> to simulate quantization.</p>\n<ul>\n          <li>\n            <p>Per‑channel quantization is especially effective in:</p>\n\n            <ul>\n              <li>Convolutional neural networks (e.g. ResNet, MobileNet, EfficientNet, YOLO), where filter/channels have distinct distribution ranges.</li>\n              <li>Transformer architectures (e.g. multi‑head attention or feed‑forward layers), where weight and activation distributions vary widely across heads or projection layers.</li>\n            </ul>\n          </li>\n        </ul>\n<p>Per‑channel quantization is especially effective in:</p>\n<ul>\n              <li>Convolutional neural networks (e.g. ResNet, MobileNet, EfficientNet, YOLO), where filter/channels have distinct distribution ranges.</li>\n              <li>Transformer architectures (e.g. multi‑head attention or feed‑forward layers), where weight and activation distributions vary widely across heads or projection layers.</li>\n            </ul>\n<p><strong>Learned Scale Factors</strong>: Methods like Learned Step Size Quantization (LSQ), introduced in the paper <a href=\"https://arxiv.org/abs/1902.08153\">“Learned Step Size Quantization”</a> by Esser et al. (2020), enable scale parameters to be optimized via backpropagation during fine‑tuning. This adaptive scaling is especially beneficial in models with skewed distributions—such as transformer-based language models, where operations like softmax produce skewed outputs.</p>\n<p>Advanced Weight Handling:</p>\n<ul>\n      <li>\n        <p><strong>Per‑Group Quantization</strong>: Instead of full per‑channel (i.e. one scale per channel), per‑group quantization assigns one scale per group of channels (e.g. N channels or per row). This balances granularity and memory overhead and is prevalent in formats like ONNX or TensorRT.</p>\n      </li>\n      <li>\n        <p><strong>Activation‑Aware Weight Quantization (AWQ)</strong>: AWQ techniques tailor weight quantization ranges and groupings using activation patterns observed during calibration. Rather than uniformly quantizing weights, these methods use sensitivity analysis to allocate bit budgets or adjust grouping for performance‑critical weights.</p>\n      </li>\n      <li>\n        <p><strong>Zero‑Point Optimization</strong>: For symmetric quantization (typically weight tensors), zero‑point is fixed at zero. For asymmetric quantization—commonly used for activations—the zero‑point shifts the quantized range. Some frameworks (e.g. ONNX, TensorFlow Lite) allow fine‑grained control over zero‑point alignment, which influences both accuracy and hardware compatibility.</p>\n      </li>\n    </ul>\n<p><strong>Per‑Group Quantization</strong>: Instead of full per‑channel (i.e. one scale per channel), per‑group quantization assigns one scale per group of channels (e.g. N channels or per row). This balances granularity and memory overhead and is prevalent in formats like ONNX or TensorRT.</p>\n<p><strong>Activation‑Aware Weight Quantization (AWQ)</strong>: AWQ techniques tailor weight quantization ranges and groupings using activation patterns observed during calibration. Rather than uniformly quantizing weights, these methods use sensitivity analysis to allocate bit budgets or adjust grouping for performance‑critical weights.</p>\n<p><strong>Zero‑Point Optimization</strong>: For symmetric quantization (typically weight tensors), zero‑point is fixed at zero. For asymmetric quantization—commonly used for activations—the zero‑point shifts the quantized range. Some frameworks (e.g. ONNX, TensorFlow Lite) allow fine‑grained control over zero‑point alignment, which influences both accuracy and hardware compatibility.</p>\n<p>In practice, weight and activation quantization are applied jointly but with distinct parameter sets and calibration workflows. Modern toolkits support fine-grained configuration, including:</p>\n<ul>\n      <li>PyTorch’s <a href=\"https://pytorch.org/docs/stable/quantization.html\">torch.quantization</a> and <a href=\"https://pytorch.org/docs/stable/quantization-support.html\">torch.ao.quantization</a> modules;</li>\n      <li>TensorFlow Model Optimization Toolkit (<a href=\"https://www.tensorflow.org/model_optimization/guide/quantization\">quantization guide</a>) supporting calibration APIs and quantization-aware training;</li>\n      <li>NVIDIA TensorRT, which enables layer-wise quantization, quantization-aware training, and PTQ via its <a href=\"https://docs.nvidia.com/deeplearning/tensorrt/latest/index.html\">TensorRT SDK documentation</a> and the Torch‑TensorRT Model Optimizer;</li>\n      <li>Intel Neural Compressor, an open‑source framework offering post‑training static, dynamic, and quantization‑aware training workflows for PyTorch and TensorFlow (<a href=\"https://intel.github.io/neural-compressor/latest/docs/source/quantization.html\">Intel Neural Compressor documentation</a>).</li>\n    </ul>\n<blockquote>\n  <p>Effective quantization requires balancing statistical rigor, hardware compatibility, and architecture sensitivity. Activations require runtime awareness, while weights benefit from static optimization—and both may leverage learned or adaptive scaling to maintain fidelity in low‑bit regimes.</p>\n</blockquote>\n<p>Effective quantization requires balancing statistical rigor, hardware compatibility, and architecture sensitivity. Activations require runtime awareness, while weights benefit from static optimization—and both may leverage learned or adaptive scaling to maintain fidelity in low‑bit regimes.</p>",
      "contentMarkdown": "*   Why they’re not the same: Weights and activations have fundamentally different roles and constraints during quantization.\n    \n    *   Weights are static once training completes. Since they do not vary across inputs, they may be quantized offline using fixed calibration data or analytically via heuristics such as min‑max scaling or percentile statistics. This allows more aggressive optimization techniques such as _per‑channel quantization_ or _non‑uniform quantization_, and values may be precomputed and stored in compact low‑precision formats like `int8`, `int4`, or binary.\n        \n    *   Activations, by contrast, are dynamic and input‑dependent. Their value ranges vary based on data processed during inference. Therefore, activation quantization must accommodate runtime variability. Two common modes exist:\n        \n        *   **Static Activation Quantization**: Calibration datasets estimate typical activation ranges (min/max or histogram‑based) per layer. These statistics are then used to assign fixed scale/zero‑point pairs for quantized representation, using observers such as [`MinMaxObserver`](https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.MinMaxObserver.html) or histogram‑based observers.\n            \n        *   **Dynamic Activation Quantization**: During inference, scale and zero‑point values are computed on‑the‑fly from input‑dependent statistics (e.g. dynamic min/max per batch). This avoids calibration datasets but may add latency due to runtime computation.\n            \n*   Handling outliers: Outliers in activation or weight distributions can drastically degrade quantization quality by skewing range and reducing effective precision. Several mitigation strategies include:\n    \n    *   **Percentile‑based Clipping**: Rather than using absolute min/max, activations may be clipped to a percentile‑based range (e.g. 99.9%) to discard extreme outliers. Techniques include KL divergence minimization or MSE‑based clipping, used in frameworks such as PyTorch or TensorFlow Lite.\n        \n    *   **Per‑Channel Quantization**: Instead of applying a single scale across a tensor, per‑channel quantization assigns unique scale and zero‑point values per output channel. This adapts to local distribution variations, particularly in convolutional or linear layers. In PyTorch, this is implemented via [PerChannelMinMaxObserver](https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html) or similar observers using [`torch.per_channel_affine`](https://pytorch.org/docs/stable/quantization.html) schemes. Core functions include [`torch.quantize_per_channel`](https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html) and [`torch.fake_quantize_per_channel_affine`](https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html) to simulate quantization.\n        \n        *   Per‑channel quantization is especially effective in:\n            \n            *   Convolutional neural networks (e.g. ResNet, MobileNet, EfficientNet, YOLO), where filter/channels have distinct distribution ranges.\n            *   Transformer architectures (e.g. multi‑head attention or feed‑forward layers), where weight and activation distributions vary widely across heads or projection layers.\n    *   **Learned Scale Factors**: Methods like Learned Step Size Quantization (LSQ), introduced in the paper [“Learned Step Size Quantization”](https://arxiv.org/abs/1902.08153) by Esser et al. (2020), enable scale parameters to be optimized via backpropagation during fine‑tuning. This adaptive scaling is especially beneficial in models with skewed distributions—such as transformer-based language models, where operations like softmax produce skewed outputs.\n        \n*   Advanced Weight Handling:\n    \n    *   **Per‑Group Quantization**: Instead of full per‑channel (i.e. one scale per channel), per‑group quantization assigns one scale per group of channels (e.g. N channels or per row). This balances granularity and memory overhead and is prevalent in formats like ONNX or TensorRT.\n        \n    *   **Activation‑Aware Weight Quantization (AWQ)**: AWQ techniques tailor weight quantization ranges and groupings using activation patterns observed during calibration. Rather than uniformly quantizing weights, these methods use sensitivity analysis to allocate bit budgets or adjust grouping for performance‑critical weights.\n        \n    *   **Zero‑Point Optimization**: For symmetric quantization (typically weight tensors), zero‑point is fixed at zero. For asymmetric quantization—commonly used for activations—the zero‑point shifts the quantized range. Some frameworks (e.g. ONNX, TensorFlow Lite) allow fine‑grained control over zero‑point alignment, which influences both accuracy and hardware compatibility.\n        \n*   In practice, weight and activation quantization are applied jointly but with distinct parameter sets and calibration workflows. Modern toolkits support fine-grained configuration, including:\n    \n    *   PyTorch’s [torch.quantization](https://pytorch.org/docs/stable/quantization.html) and [torch.ao.quantization](https://pytorch.org/docs/stable/quantization-support.html) modules;\n    *   TensorFlow Model Optimization Toolkit ([quantization guide](https://www.tensorflow.org/model_optimization/guide/quantization)) supporting calibration APIs and quantization-aware training;\n    *   NVIDIA TensorRT, which enables layer-wise quantization, quantization-aware training, and PTQ via its [TensorRT SDK documentation](https://docs.nvidia.com/deeplearning/tensorrt/latest/index.html) and the Torch‑TensorRT Model Optimizer;\n    *   Intel Neural Compressor, an open‑source framework offering post‑training static, dynamic, and quantization‑aware training workflows for PyTorch and TensorFlow ([Intel Neural Compressor documentation](https://intel.github.io/neural-compressor/latest/docs/source/quantization.html)).\n\nWhy they’re not the same: Weights and activations have fundamentally different roles and constraints during quantization.\n\n*   Weights are static once training completes. Since they do not vary across inputs, they may be quantized offline using fixed calibration data or analytically via heuristics such as min‑max scaling or percentile statistics. This allows more aggressive optimization techniques such as _per‑channel quantization_ or _non‑uniform quantization_, and values may be precomputed and stored in compact low‑precision formats like `int8`, `int4`, or binary.\n    \n*   Activations, by contrast, are dynamic and input‑dependent. Their value ranges vary based on data processed during inference. Therefore, activation quantization must accommodate runtime variability. Two common modes exist:\n    \n    *   **Static Activation Quantization**: Calibration datasets estimate typical activation ranges (min/max or histogram‑based) per layer. These statistics are then used to assign fixed scale/zero‑point pairs for quantized representation, using observers such as [`MinMaxObserver`](https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.MinMaxObserver.html) or histogram‑based observers.\n        \n    *   **Dynamic Activation Quantization**: During inference, scale and zero‑point values are computed on‑the‑fly from input‑dependent statistics (e.g. dynamic min/max per batch). This avoids calibration datasets but may add latency due to runtime computation.\n        \n\nWeights are static once training completes. Since they do not vary across inputs, they may be quantized offline using fixed calibration data or analytically via heuristics such as min‑max scaling or percentile statistics. This allows more aggressive optimization techniques such as _per‑channel quantization_ or _non‑uniform quantization_, and values may be precomputed and stored in compact low‑precision formats like `int8`, `int4`, or binary.\n\nActivations, by contrast, are dynamic and input‑dependent. Their value ranges vary based on data processed during inference. Therefore, activation quantization must accommodate runtime variability. Two common modes exist:\n\n*   **Static Activation Quantization**: Calibration datasets estimate typical activation ranges (min/max or histogram‑based) per layer. These statistics are then used to assign fixed scale/zero‑point pairs for quantized representation, using observers such as [`MinMaxObserver`](https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.MinMaxObserver.html) or histogram‑based observers.\n    \n*   **Dynamic Activation Quantization**: During inference, scale and zero‑point values are computed on‑the‑fly from input‑dependent statistics (e.g. dynamic min/max per batch). This avoids calibration datasets but may add latency due to runtime computation.\n    \n\n**Static Activation Quantization**: Calibration datasets estimate typical activation ranges (min/max or histogram‑based) per layer. These statistics are then used to assign fixed scale/zero‑point pairs for quantized representation, using observers such as [`MinMaxObserver`](https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.MinMaxObserver.html) or histogram‑based observers.\n\n**Dynamic Activation Quantization**: During inference, scale and zero‑point values are computed on‑the‑fly from input‑dependent statistics (e.g. dynamic min/max per batch). This avoids calibration datasets but may add latency due to runtime computation.\n\nHandling outliers: Outliers in activation or weight distributions can drastically degrade quantization quality by skewing range and reducing effective precision. Several mitigation strategies include:\n\n*   **Percentile‑based Clipping**: Rather than using absolute min/max, activations may be clipped to a percentile‑based range (e.g. 99.9%) to discard extreme outliers. Techniques include KL divergence minimization or MSE‑based clipping, used in frameworks such as PyTorch or TensorFlow Lite.\n    \n*   **Per‑Channel Quantization**: Instead of applying a single scale across a tensor, per‑channel quantization assigns unique scale and zero‑point values per output channel. This adapts to local distribution variations, particularly in convolutional or linear layers. In PyTorch, this is implemented via [PerChannelMinMaxObserver](https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html) or similar observers using [`torch.per_channel_affine`](https://pytorch.org/docs/stable/quantization.html) schemes. Core functions include [`torch.quantize_per_channel`](https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html) and [`torch.fake_quantize_per_channel_affine`](https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html) to simulate quantization.\n    \n    *   Per‑channel quantization is especially effective in:\n        \n        *   Convolutional neural networks (e.g. ResNet, MobileNet, EfficientNet, YOLO), where filter/channels have distinct distribution ranges.\n        *   Transformer architectures (e.g. multi‑head attention or feed‑forward layers), where weight and activation distributions vary widely across heads or projection layers.\n*   **Learned Scale Factors**: Methods like Learned Step Size Quantization (LSQ), introduced in the paper [“Learned Step Size Quantization”](https://arxiv.org/abs/1902.08153) by Esser et al. (2020), enable scale parameters to be optimized via backpropagation during fine‑tuning. This adaptive scaling is especially beneficial in models with skewed distributions—such as transformer-based language models, where operations like softmax produce skewed outputs.\n    \n\n**Percentile‑based Clipping**: Rather than using absolute min/max, activations may be clipped to a percentile‑based range (e.g. 99.9%) to discard extreme outliers. Techniques include KL divergence minimization or MSE‑based clipping, used in frameworks such as PyTorch or TensorFlow Lite.\n\n**Per‑Channel Quantization**: Instead of applying a single scale across a tensor, per‑channel quantization assigns unique scale and zero‑point values per output channel. This adapts to local distribution variations, particularly in convolutional or linear layers. In PyTorch, this is implemented via [PerChannelMinMaxObserver](https://pytorch.org/docs/stable/generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html) or similar observers using [`torch.per_channel_affine`](https://pytorch.org/docs/stable/quantization.html) schemes. Core functions include [`torch.quantize_per_channel`](https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html) and [`torch.fake_quantize_per_channel_affine`](https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html) to simulate quantization.\n\n*   Per‑channel quantization is especially effective in:\n    \n    *   Convolutional neural networks (e.g. ResNet, MobileNet, EfficientNet, YOLO), where filter/channels have distinct distribution ranges.\n    *   Transformer architectures (e.g. multi‑head attention or feed‑forward layers), where weight and activation distributions vary widely across heads or projection layers.\n\nPer‑channel quantization is especially effective in:\n\n*   Convolutional neural networks (e.g. ResNet, MobileNet, EfficientNet, YOLO), where filter/channels have distinct distribution ranges.\n*   Transformer architectures (e.g. multi‑head attention or feed‑forward layers), where weight and activation distributions vary widely across heads or projection layers.\n\n**Learned Scale Factors**: Methods like Learned Step Size Quantization (LSQ), introduced in the paper [“Learned Step Size Quantization”](https://arxiv.org/abs/1902.08153) by Esser et al. (2020), enable scale parameters to be optimized via backpropagation during fine‑tuning. This adaptive scaling is especially beneficial in models with skewed distributions—such as transformer-based language models, where operations like softmax produce skewed outputs.\n\nAdvanced Weight Handling:\n\n*   **Per‑Group Quantization**: Instead of full per‑channel (i.e. one scale per channel), per‑group quantization assigns one scale per group of channels (e.g. N channels or per row). This balances granularity and memory overhead and is prevalent in formats like ONNX or TensorRT.\n    \n*   **Activation‑Aware Weight Quantization (AWQ)**: AWQ techniques tailor weight quantization ranges and groupings using activation patterns observed during calibration. Rather than uniformly quantizing weights, these methods use sensitivity analysis to allocate bit budgets or adjust grouping for performance‑critical weights.\n    \n*   **Zero‑Point Optimization**: For symmetric quantization (typically weight tensors), zero‑point is fixed at zero. For asymmetric quantization—commonly used for activations—the zero‑point shifts the quantized range. Some frameworks (e.g. ONNX, TensorFlow Lite) allow fine‑grained control over zero‑point alignment, which influences both accuracy and hardware compatibility.\n    \n\n**Per‑Group Quantization**: Instead of full per‑channel (i.e. one scale per channel), per‑group quantization assigns one scale per group of channels (e.g. N channels or per row). This balances granularity and memory overhead and is prevalent in formats like ONNX or TensorRT.\n\n**Activation‑Aware Weight Quantization (AWQ)**: AWQ techniques tailor weight quantization ranges and groupings using activation patterns observed during calibration. Rather than uniformly quantizing weights, these methods use sensitivity analysis to allocate bit budgets or adjust grouping for performance‑critical weights.\n\n**Zero‑Point Optimization**: For symmetric quantization (typically weight tensors), zero‑point is fixed at zero. For asymmetric quantization—commonly used for activations—the zero‑point shifts the quantized range. Some frameworks (e.g. ONNX, TensorFlow Lite) allow fine‑grained control over zero‑point alignment, which influences both accuracy and hardware compatibility.\n\nIn practice, weight and activation quantization are applied jointly but with distinct parameter sets and calibration workflows. Modern toolkits support fine-grained configuration, including:\n\n*   PyTorch’s [torch.quantization](https://pytorch.org/docs/stable/quantization.html) and [torch.ao.quantization](https://pytorch.org/docs/stable/quantization-support.html) modules;\n*   TensorFlow Model Optimization Toolkit ([quantization guide](https://www.tensorflow.org/model_optimization/guide/quantization)) supporting calibration APIs and quantization-aware training;\n*   NVIDIA TensorRT, which enables layer-wise quantization, quantization-aware training, and PTQ via its [TensorRT SDK documentation](https://docs.nvidia.com/deeplearning/tensorrt/latest/index.html) and the Torch‑TensorRT Model Optimizer;\n*   Intel Neural Compressor, an open‑source framework offering post‑training static, dynamic, and quantization‑aware training workflows for PyTorch and TensorFlow ([Intel Neural Compressor documentation](https://intel.github.io/neural-compressor/latest/docs/source/quantization.html)).\n\n> Effective quantization requires balancing statistical rigor, hardware compatibility, and architecture sensitivity. Activations require runtime awareness, while weights benefit from static optimization—and both may leverage learned or adaptive scaling to maintain fidelity in low‑bit regimes.\n\nEffective quantization requires balancing statistical rigor, hardware compatibility, and architecture sensitivity. Activations require runtime awareness, while weights benefit from static optimization—and both may leverage learned or adaptive scaling to maintain fidelity in low‑bit regimes.",
      "order": 9,
      "orderInChapter": 9,
      "difficulty": 4,
      "estimatedMinutes": 10,
      "tags": [
        "ondevice ai",
        "neural network",
        "transformer",
        "attention",
        "convolution",
        "optimization",
        "backpropagation",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 1875,
        "contentLength": 20919
      },
      "nextCards": [
        "ai-model-compression-quantization-with-pytorch-10",
        "ai-model-compression-comparative-analysis-11"
      ],
      "relatedCards": [
        "ai-on-device-transformers-balancing-compute-and-memory-prefill-vs-decode-opt-8",
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-quantization-48-bit-11",
        "ai-on-device-transformers-cpu-deployment-considerations-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#weights-vs.-activation-quantization",
      "scrapedAt": "2025-12-28T11:55:50.967Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-quantization-with-pytorch-10",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Quantization with PyTorch",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>\n    <p>Quantization in PyTorch enables the execution of computations and memory accesses with reduced-precision data types, typically <code class=\"language-plaintext highlighter-rouge\">int8</code>, leading to improvements in model efficiency, inference speed, and memory footprint. PyTorch provides comprehensive support for quantization, starting from version 1.3, through an API that integrates seamlessly with the existing eager execution model.</p>\n  </li>\n  <li>\n    <p><strong>Quantized Tensor Representation</strong></p>\n\n    <ul>\n      <li>\n        <p>PyTorch introduces special data types for quantized tensors, enabling the representation of weights and activations in reduced precision (typically <code class=\"language-plaintext highlighter-rouge\">int8</code>, and sometimes <code class=\"language-plaintext highlighter-rouge\">float16</code>). These tensors can be operated on via quantized kernels available under <code class=\"language-plaintext highlighter-rouge\">torch.nn.quantized</code> and <code class=\"language-plaintext highlighter-rouge\">torch.nn.quantized.dynamic</code>. These quantized operations allow for a 4× reduction in model size and 2-4× improvements in memory bandwidth and inference latency, depending on the hardware and model structure.</p>\n      </li>\n      <li>\n        <p>Quantization in PyTorch relies on calibration, which is the process of gathering statistics on representative inputs to determine optimal quantization parameters (such as scale and zero-point). These parameters are used in quantization functions of the form <code class=\"language-plaintext highlighter-rouge\">round(x / scale) + zero_point</code>, enabling a linear mapping between floating point and integer domains.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Quantization Backends</strong></p>\n\n    <ul>\n      <li>PyTorch leverages optimized backend libraries to execute quantized operations efficiently. FBGEMM (Facebook’s GEMM library) is optimized for server environments (x86 CPUs), while QNNPACK is designed for mobile and embedded environments. These are analogous to BLAS/MKL libraries in floating-point computation and are integrated automatically based on the target deployment platform.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Numerical Stability and Mixed Precision</strong></p>\n\n    <ul>\n      <li>One challenge in quantization is maintaining numerical stability, particularly for operations involving accumulation or exponentiation. To address this, PyTorch supports mixed-precision training and inference using the <code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp</code> module. AMP (Automatic Mixed Precision) allows portions of the model to be cast to <code class=\"language-plaintext highlighter-rouge\">torch.float16</code> while retaining <code class=\"language-plaintext highlighter-rouge\">torch.float32</code> for operations requiring higher precision, improving performance with minimal loss of accuracy. Although initially introduced for CUDA GPUs, mixed-precision techniques are distinct from quantization but can be complementary in certain scenarios.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Quantization Techniques in PyTorch</strong></p>\n\n    <ul>\n      <li>\n        <p>PyTorch provides three primary quantization workflows under the <code class=\"language-plaintext highlighter-rouge\">torch.quantization</code> namespace, often referred to collectively as “eager mode quantization”:</p>\n      </li>\n      <li><strong>Dynamic Quantization</strong>:\n        <ul>\n          <li>\n            <p>Weights are statically quantized and stored in int8 format, while activations are dynamically quantized at runtime before computation. This method requires minimal code changes and no calibration data. It is most effective for models dominated by linear layers (e.g., LSTM, GRU, Transformer-based models).</p>\n          </li>\n          <li>\n            <p>Example:</p>\n          </li>\n        </ul>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\"><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">quantize_dynamic</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"p\">{</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">},</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">qint8</span><span class=\"p\">)</span>\n</code></pre></div>        </div>\n      </li>\n      <li><strong>Post-Training Quantization</strong>:\n        <ul>\n          <li>\n            <p>Both weights and activations are quantized. This approach requires calibration, where representative input data is passed through the model to collect statistics via observer modules. Operator fusion (e.g., Conv + ReLU) and per-channel quantization are supported to improve performance and accuracy.</p>\n          </li>\n          <li>\n            <p>Example sequence:</p>\n          </li>\n        </ul>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\"><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">qconfig</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">get_default_qconfig</span><span class=\"p\">(</span><span class=\"s\">'fbgemm'</span><span class=\"p\">)</span>\n<span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">prepare</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"c1\"># Run calibration with representative data\n</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n</code></pre></div>        </div>\n      </li>\n      <li><strong>Quantization-Aware Training (QAT)</strong>:\n        <ul>\n          <li>\n            <p>This technique inserts fake-quantization modules during training, simulating quantization effects in both forward and backward passes. It typically yields the highest post-quantization accuracy, especially in cases where model accuracy is sensitive to quantization noise.</p>\n          </li>\n          <li>\n            <p>Example sequence:</p>\n          </li>\n        </ul>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code6\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code6\"><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">qconfig</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">get_default_qat_qconfig</span><span class=\"p\">(</span><span class=\"s\">'fbgemm'</span><span class=\"p\">)</span>\n<span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">prepare_qat</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"c1\"># Train model\n</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">eval</span><span class=\"p\">(),</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n</code></pre></div>        </div>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Operator and Layer Coverage</strong></p>\n\n    <ul>\n      <li>Quantization support varies by method. Dynamic quantization supports layers like Linear and RNNs, while static and QAT methods support a broader set including Conv, ReLU, BatchNorm (via fusion), and more. FX Graph Mode Quantization (a newer, graph-level approach not covered here) further expands operator support and streamlines workflows.</li>\n    </ul>\n  </li>\n  <li>\n    <p>For additional guidance and end-to-end examples, refer to the official PyTorch blog: <a href=\"https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\">Introduction to Quantization on PyTorch</a>.</p>\n  </li>\n</ul>\n<p>Quantization in PyTorch enables the execution of computations and memory accesses with reduced-precision data types, typically <code class=\"language-plaintext highlighter-rouge\">int8</code>, leading to improvements in model efficiency, inference speed, and memory footprint. PyTorch provides comprehensive support for quantization, starting from version 1.3, through an API that integrates seamlessly with the existing eager execution model.</p>\n<p><strong>Quantized Tensor Representation</strong></p>\n<ul>\n      <li>\n        <p>PyTorch introduces special data types for quantized tensors, enabling the representation of weights and activations in reduced precision (typically <code class=\"language-plaintext highlighter-rouge\">int8</code>, and sometimes <code class=\"language-plaintext highlighter-rouge\">float16</code>). These tensors can be operated on via quantized kernels available under <code class=\"language-plaintext highlighter-rouge\">torch.nn.quantized</code> and <code class=\"language-plaintext highlighter-rouge\">torch.nn.quantized.dynamic</code>. These quantized operations allow for a 4× reduction in model size and 2-4× improvements in memory bandwidth and inference latency, depending on the hardware and model structure.</p>\n      </li>\n      <li>\n        <p>Quantization in PyTorch relies on calibration, which is the process of gathering statistics on representative inputs to determine optimal quantization parameters (such as scale and zero-point). These parameters are used in quantization functions of the form <code class=\"language-plaintext highlighter-rouge\">round(x / scale) + zero_point</code>, enabling a linear mapping between floating point and integer domains.</p>\n      </li>\n    </ul>\n<p>PyTorch introduces special data types for quantized tensors, enabling the representation of weights and activations in reduced precision (typically <code class=\"language-plaintext highlighter-rouge\">int8</code>, and sometimes <code class=\"language-plaintext highlighter-rouge\">float16</code>). These tensors can be operated on via quantized kernels available under <code class=\"language-plaintext highlighter-rouge\">torch.nn.quantized</code> and <code class=\"language-plaintext highlighter-rouge\">torch.nn.quantized.dynamic</code>. These quantized operations allow for a 4× reduction in model size and 2-4× improvements in memory bandwidth and inference latency, depending on the hardware and model structure.</p>\n<p>Quantization in PyTorch relies on calibration, which is the process of gathering statistics on representative inputs to determine optimal quantization parameters (such as scale and zero-point). These parameters are used in quantization functions of the form <code class=\"language-plaintext highlighter-rouge\">round(x / scale) + zero_point</code>, enabling a linear mapping between floating point and integer domains.</p>\n<p><strong>Quantization Backends</strong></p>\n<ul>\n      <li>PyTorch leverages optimized backend libraries to execute quantized operations efficiently. FBGEMM (Facebook’s GEMM library) is optimized for server environments (x86 CPUs), while QNNPACK is designed for mobile and embedded environments. These are analogous to BLAS/MKL libraries in floating-point computation and are integrated automatically based on the target deployment platform.</li>\n    </ul>\n<p><strong>Numerical Stability and Mixed Precision</strong></p>\n<ul>\n      <li>One challenge in quantization is maintaining numerical stability, particularly for operations involving accumulation or exponentiation. To address this, PyTorch supports mixed-precision training and inference using the <code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp</code> module. AMP (Automatic Mixed Precision) allows portions of the model to be cast to <code class=\"language-plaintext highlighter-rouge\">torch.float16</code> while retaining <code class=\"language-plaintext highlighter-rouge\">torch.float32</code> for operations requiring higher precision, improving performance with minimal loss of accuracy. Although initially introduced for CUDA GPUs, mixed-precision techniques are distinct from quantization but can be complementary in certain scenarios.</li>\n    </ul>\n<p><strong>Quantization Techniques in PyTorch</strong></p>\n<ul>\n      <li>\n        <p>PyTorch provides three primary quantization workflows under the <code class=\"language-plaintext highlighter-rouge\">torch.quantization</code> namespace, often referred to collectively as “eager mode quantization”:</p>\n      </li>\n      <li><strong>Dynamic Quantization</strong>:\n        <ul>\n          <li>\n            <p>Weights are statically quantized and stored in int8 format, while activations are dynamically quantized at runtime before computation. This method requires minimal code changes and no calibration data. It is most effective for models dominated by linear layers (e.g., LSTM, GRU, Transformer-based models).</p>\n          </li>\n          <li>\n            <p>Example:</p>\n          </li>\n        </ul>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\"><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">quantize_dynamic</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"p\">{</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">},</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">qint8</span><span class=\"p\">)</span>\n</code></pre></div>        </div>\n      </li>\n      <li><strong>Post-Training Quantization</strong>:\n        <ul>\n          <li>\n            <p>Both weights and activations are quantized. This approach requires calibration, where representative input data is passed through the model to collect statistics via observer modules. Operator fusion (e.g., Conv + ReLU) and per-channel quantization are supported to improve performance and accuracy.</p>\n          </li>\n          <li>\n            <p>Example sequence:</p>\n          </li>\n        </ul>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\"><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">qconfig</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">get_default_qconfig</span><span class=\"p\">(</span><span class=\"s\">'fbgemm'</span><span class=\"p\">)</span>\n<span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">prepare</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"c1\"># Run calibration with representative data\n</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n</code></pre></div>        </div>\n      </li>\n      <li><strong>Quantization-Aware Training (QAT)</strong>:\n        <ul>\n          <li>\n            <p>This technique inserts fake-quantization modules during training, simulating quantization effects in both forward and backward passes. It typically yields the highest post-quantization accuracy, especially in cases where model accuracy is sensitive to quantization noise.</p>\n          </li>\n          <li>\n            <p>Example sequence:</p>\n          </li>\n        </ul>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code6\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code6\"><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">qconfig</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">get_default_qat_qconfig</span><span class=\"p\">(</span><span class=\"s\">'fbgemm'</span><span class=\"p\">)</span>\n<span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">prepare_qat</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"c1\"># Train model\n</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">eval</span><span class=\"p\">(),</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n</code></pre></div>        </div>\n      </li>\n    </ul>\n<p>PyTorch provides three primary quantization workflows under the <code class=\"language-plaintext highlighter-rouge\">torch.quantization</code> namespace, often referred to collectively as “eager mode quantization”:</p>\n<ul>\n          <li>\n            <p>Weights are statically quantized and stored in int8 format, while activations are dynamically quantized at runtime before computation. This method requires minimal code changes and no calibration data. It is most effective for models dominated by linear layers (e.g., LSTM, GRU, Transformer-based models).</p>\n          </li>\n          <li>\n            <p>Example:</p>\n          </li>\n        </ul>\n<p>Weights are statically quantized and stored in int8 format, while activations are dynamically quantized at runtime before computation. This method requires minimal code changes and no calibration data. It is most effective for models dominated by linear layers (e.g., LSTM, GRU, Transformer-based models).</p>\n<p>Example:</p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\"><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">quantize_dynamic</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"p\">{</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">},</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">qint8</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n          <li>\n            <p>Both weights and activations are quantized. This approach requires calibration, where representative input data is passed through the model to collect statistics via observer modules. Operator fusion (e.g., Conv + ReLU) and per-channel quantization are supported to improve performance and accuracy.</p>\n          </li>\n          <li>\n            <p>Example sequence:</p>\n          </li>\n        </ul>\n<p>Both weights and activations are quantized. This approach requires calibration, where representative input data is passed through the model to collect statistics via observer modules. Operator fusion (e.g., Conv + ReLU) and per-channel quantization are supported to improve performance and accuracy.</p>\n<p>Example sequence:</p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\"><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">qconfig</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">get_default_qconfig</span><span class=\"p\">(</span><span class=\"s\">'fbgemm'</span><span class=\"p\">)</span>\n<span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">prepare</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"c1\"># Run calibration with representative data\n</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n          <li>\n            <p>This technique inserts fake-quantization modules during training, simulating quantization effects in both forward and backward passes. It typically yields the highest post-quantization accuracy, especially in cases where model accuracy is sensitive to quantization noise.</p>\n          </li>\n          <li>\n            <p>Example sequence:</p>\n          </li>\n        </ul>\n<p>This technique inserts fake-quantization modules during training, simulating quantization effects in both forward and backward passes. It typically yields the highest post-quantization accuracy, especially in cases where model accuracy is sensitive to quantization noise.</p>\n<p>Example sequence:</p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code6\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code6\"><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">qconfig</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">get_default_qat_qconfig</span><span class=\"p\">(</span><span class=\"s\">'fbgemm'</span><span class=\"p\">)</span>\n<span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">prepare_qat</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"c1\"># Train model\n</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">eval</span><span class=\"p\">(),</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n</code></pre>\n<p><strong>Operator and Layer Coverage</strong></p>\n<ul>\n      <li>Quantization support varies by method. Dynamic quantization supports layers like Linear and RNNs, while static and QAT methods support a broader set including Conv, ReLU, BatchNorm (via fusion), and more. FX Graph Mode Quantization (a newer, graph-level approach not covered here) further expands operator support and streamlines workflows.</li>\n    </ul>\n<p>For additional guidance and end-to-end examples, refer to the official PyTorch blog: <a href=\"https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\">Introduction to Quantization on PyTorch</a>.</p>\n<h4 id=\"dynamic--runtime-quantization\">Dynamic / Runtime Quantization</h4>\n<ul>\n  <li>\n    <p><a href=\"https://pytorch.org/docs/stable/quantization.html#dynamic-quantization\">Dynamic quantization</a> is one of the most simple quantization techniques in PyTorch, particularly suitable for models where most computation occurs in linear layers—such as transformer models (e.g. BERT) or recurrent networks (e.g. LSTM)—because these operations are dominated by matrix multiplications, which benefit significantly from <code class=\"language-plaintext highlighter-rouge\">int8</code> acceleration without requiring quantized convolutions.</p>\n  </li>\n  <li>\n    <p>In dynamic quantization, model weights are converted from 32-bit floating point (<code class=\"language-plaintext highlighter-rouge\">float32</code>) to a lower precision format such as <code class=\"language-plaintext highlighter-rouge\">int8</code> and are permanently stored in this quantized form. Activations, however, remain in <code class=\"language-plaintext highlighter-rouge\">float32</code> format until runtime. At inference time, these activations are dynamically quantized to <code class=\"language-plaintext highlighter-rouge\">int8</code> immediately before the corresponding computation (i.e., matrix multiplication or linear operation) is executed. After the operation, the result is stored back in <code class=\"language-plaintext highlighter-rouge\">float32</code>. This hybrid approach enables significant performance gains—such as reduced latency and memory usage—while maintaining reasonable model accuracy.</p>\n  </li>\n</ul>\n<p><a href=\"https://pytorch.org/docs/stable/quantization.html#dynamic-quantization\">Dynamic quantization</a> is one of the most simple quantization techniques in PyTorch, particularly suitable for models where most computation occurs in linear layers—such as transformer models (e.g. BERT) or recurrent networks (e.g. LSTM)—because these operations are dominated by matrix multiplications, which benefit significantly from <code class=\"language-plaintext highlighter-rouge\">int8</code> acceleration without requiring quantized convolutions.</p>\n<p>In dynamic quantization, model weights are converted from 32-bit floating point (<code class=\"language-plaintext highlighter-rouge\">float32</code>) to a lower precision format such as <code class=\"language-plaintext highlighter-rouge\">int8</code> and are permanently stored in this quantized form. Activations, however, remain in <code class=\"language-plaintext highlighter-rouge\">float32</code> format until runtime. At inference time, these activations are dynamically quantized to <code class=\"language-plaintext highlighter-rouge\">int8</code> immediately before the corresponding computation (i.e., matrix multiplication or linear operation) is executed. After the operation, the result is stored back in <code class=\"language-plaintext highlighter-rouge\">float32</code>. This hybrid approach enables significant performance gains—such as reduced latency and memory usage—while maintaining reasonable model accuracy.</p>\n<blockquote>\n  <p>The aim of dynamic quantization is thus to save compute through faster arithmetic, rather than primarily to reduce storage needs.</p>\n</blockquote>\n<p>The aim of dynamic quantization is thus to save compute through faster arithmetic, rather than primarily to reduce storage needs.</p>\n<ul>\n  <li>\n    <p>Unlike static quantization or quantization-aware training (QAT), dynamic quantization requires no calibration dataset or retraining. This makes it ideal when representative data is unavailable or ease of deployment is paramount.</p>\n  </li>\n  <li>\n    <p>Quantization parameters (scale and zero-point) for activations are determined dynamically at each invocation based on the input data range, while weights use fixed scale and zero-point values computed ahead of time. As such, since only the model weights are quantized ahead of time, while activations remain in <code class=\"language-plaintext highlighter-rouge\">float32</code> and are quantized dynamically at runtime based on input data, dynamic quantization is often referred to as a data-free or weight-only quantization method during preparation.</p>\n  </li>\n  <li>\n    <p>PyTorch provides the API <a href=\"https://pytorch.org/docs/stable/generated/torch.ao.quantization.quantize_dynamic.html\"><code class=\"language-plaintext highlighter-rouge\">torch.ao.quantization.quantize_dynamic(...)</code></a> for applying dynamic quantization:</p>\n\n    <ul>\n      <li>A model (<code class=\"language-plaintext highlighter-rouge\">torch.nn.Module</code>)</li>\n      <li>A specification of target layer types or names (commonly <code class=\"language-plaintext highlighter-rouge\">{nn.Linear, nn.LSTM}</code>)</li>\n      <li>A target dtype (e.g., <code class=\"language-plaintext highlighter-rouge\">torch.qint8</code>).</li>\n    </ul>\n  </li>\n  <li>\n    <p>Only supported layer types are quantized—primarily <code class=\"language-plaintext highlighter-rouge\">nn.Linear</code> and RNN variants; convolutional layers (e.g., <code class=\"language-plaintext highlighter-rouge\">nn.Conv2d</code>) are not supported by dynamic quantization.</p>\n  </li>\n  <li>\n    <p>This approach is particularly effective for transformer and RNN models, where inference throughput is limited by memory-bound weight matrices. For example, quantizing BERT with dynamic quantization often yields up to 4× reduction in model size and measurable speedups in CPU inference latency.</p>\n  </li>\n</ul>\n<p>Unlike static quantization or quantization-aware training (QAT), dynamic quantization requires no calibration dataset or retraining. This makes it ideal when representative data is unavailable or ease of deployment is paramount.</p>\n<p>Quantization parameters (scale and zero-point) for activations are determined dynamically at each invocation based on the input data range, while weights use fixed scale and zero-point values computed ahead of time. As such, since only the model weights are quantized ahead of time, while activations remain in <code class=\"language-plaintext highlighter-rouge\">float32</code> and are quantized dynamically at runtime based on input data, dynamic quantization is often referred to as a data-free or weight-only quantization method during preparation.</p>\n<p>PyTorch provides the API <a href=\"https://pytorch.org/docs/stable/generated/torch.ao.quantization.quantize_dynamic.html\"><code class=\"language-plaintext highlighter-rouge\">torch.ao.quantization.quantize_dynamic(...)</code></a> for applying dynamic quantization:</p>\n<ul>\n      <li>A model (<code class=\"language-plaintext highlighter-rouge\">torch.nn.Module</code>)</li>\n      <li>A specification of target layer types or names (commonly <code class=\"language-plaintext highlighter-rouge\">{nn.Linear, nn.LSTM}</code>)</li>\n      <li>A target dtype (e.g., <code class=\"language-plaintext highlighter-rouge\">torch.qint8</code>).</li>\n    </ul>\n<p>Only supported layer types are quantized—primarily <code class=\"language-plaintext highlighter-rouge\">nn.Linear</code> and RNN variants; convolutional layers (e.g., <code class=\"language-plaintext highlighter-rouge\">nn.Conv2d</code>) are not supported by dynamic quantization.</p>\n<p>This approach is particularly effective for transformer and RNN models, where inference throughput is limited by memory-bound weight matrices. For example, quantizing BERT with dynamic quantization often yields up to 4× reduction in model size and measurable speedups in CPU inference latency.</p>\n<h5 id=\"dynamic-quantization-vs-post-training-quantization\">Dynamic Quantization vs. Post-Training Quantization</h5>\n<ul>\n  <li>Unlike <a href=\"#post-training-static-quantization\">Post-Training Quantization</a>, dynamic quantization does not use minmax observers or any calibration mechanism. Activation ranges are computed dynamically at runtime based on actual input data, so no observers are required during model preparation.</li>\n</ul>\n<h5 id=\"example-workflow\">Example Workflow</h5>\n<ul>\n  <li>Below is an example illustrating a typical dynamic quantization workflow:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code7\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code7\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch</span> <span class=\"kn\">import</span> <span class=\"n\">nn</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.ao.quantization</span> <span class=\"kn\">import</span> <span class=\"n\">quantize_dynamic</span>\n\n<span class=\"c1\"># Assume `model` is a pretrained floating‑point nn.Module, in eval mode\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">eval</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Apply dynamic quantization to Linear and LSTM layers\n</span><span class=\"n\">quantized_model</span> <span class=\"o\">=</span> <span class=\"n\">quantize_dynamic</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"p\">,</span>\n    <span class=\"p\">{</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">,</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">LSTM</span><span class=\"p\">},</span>\n    <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">qint8</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Run inference: activations will be quantized at runtime\n</span><span class=\"n\">input_data</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">seq_length</span><span class=\"p\">,</span> <span class=\"n\">feature_dim</span><span class=\"p\">)</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">quantized_model</span><span class=\"p\">(</span><span class=\"n\">input_data</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code7\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code7\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch</span> <span class=\"kn\">import</span> <span class=\"n\">nn</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.ao.quantization</span> <span class=\"kn\">import</span> <span class=\"n\">quantize_dynamic</span>\n\n<span class=\"c1\"># Assume `model` is a pretrained floating‑point nn.Module, in eval mode\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">eval</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Apply dynamic quantization to Linear and LSTM layers\n</span><span class=\"n\">quantized_model</span> <span class=\"o\">=</span> <span class=\"n\">quantize_dynamic</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"p\">,</span>\n    <span class=\"p\">{</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">,</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">LSTM</span><span class=\"p\">},</span>\n    <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">qint8</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Run inference: activations will be quantized at runtime\n</span><span class=\"n\">input_data</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">seq_length</span><span class=\"p\">,</span> <span class=\"n\">feature_dim</span><span class=\"p\">)</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">quantized_model</span><span class=\"p\">(</span><span class=\"n\">input_data</span><span class=\"p\">)</span>\n</code></pre>\n<h6 id=\"workflow-explanation\">Workflow Explanation</h6>\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">model.eval()</code> ensures deterministic behavior during quantization.</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">quantize_dynamic(...)</code> replaces supported layers with their dynamic quantized implementations.</li>\n  <li>Activations remain in <code class=\"language-plaintext highlighter-rouge\">float32</code> until needed.</li>\n  <li>At runtime, activations are quantized to <code class=\"language-plaintext highlighter-rouge\">int8</code> on-the-fly, and computations are performed with <code class=\"language-plaintext highlighter-rouge\">int8</code> weights and mixed precision accumulators.</li>\n  <li>After the operation, results return to <code class=\"language-plaintext highlighter-rouge\">float32</code>.</li>\n</ul>\n<h6 id=\"typical-benefits\">Typical Benefits</h6>\n<ul>\n  <li>Model size reduced by ~75%, thanks to <code class=\"language-plaintext highlighter-rouge\">int8</code> weights.</li>\n  <li>Latency improvements, especially on CPU-bound operations.</li>\n  <li>No calibration or fine-tuning required.</li>\n</ul>\n<h6 id=\"notes--tradeoffs\">Notes &amp; Trade‑offs</h6>\n<ul>\n  <li>Dynamic quantization does not support convolution or custom layers unless manually wrapped.</li>\n  <li>Dynamic quantization handles input distributions that vary widely more gracefully than static quantization, which uses fixed calibration ranges.</li>\n  <li>For CNN models or workloads where activations must also be quantized ahead of time, static quantization or QAT may yield better performance and accuracy.</li>\n</ul>\n<h6 id=\"further-reading\">Further Reading</h6>\n<ul>\n  <li>A comprehensive end-to-end tutorial for dynamic quantization on BERT is available <a href=\"https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html\">here</a>.</li>\n  <li>For a more general example and advanced usage guide, see the <a href=\"https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html\">dynamic quantization tutorial</a>.</li>\n  <li>The full API documentation for <code class=\"language-plaintext highlighter-rouge\">torch.quantization.quantize_dynamic</code> is available <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic\">here</a>.</li>\n</ul>\n<h4 id=\"post-training-quantization\">Post-Training Quantization</h4>\n<ul>\n  <li>\n    <p>Post-Training Quantization (PTQ) is a technique in PyTorch that enables the conversion of a model’s weights and activations from floating-point (typically <code class=\"language-plaintext highlighter-rouge\">float32</code>) to 8-bit integers (<code class=\"language-plaintext highlighter-rouge\">int8</code>), significantly improving inference efficiency in terms of speed and memory usage. This method is particularly well-suited for deployment scenarios on both server and edge devices, where latency and resource constraints are critical.</p>\n  </li>\n  <li>\n    <p>To facilitate this process, PyTorch inserts special modules known as <em>observers</em> into the model. These modules capture the activation ranges at various points in the network. Once sufficient data has been passed through the model during calibration, the observers record min-max values or histograms (depending on the observer type), which are then used during quantization.</p>\n  </li>\n  <li>\n    <p>A key benefit of static quantization is that it allows quantized values to be passed between operations directly, eliminating the need for costly float-to-int and int-to-float conversions at each layer. This optimization significantly reduces runtime overhead and enables end-to-end execution in <code class=\"language-plaintext highlighter-rouge\">int8</code>.</p>\n  </li>\n  <li>\n    <p>PyTorch also supports several advanced features to further improve the effectiveness of static quantization:</p>\n\n    <ul>\n      <li>\n        <p><strong>Observers</strong>:</p>\n\n        <ul>\n          <li>Observer modules are used to collect statistics on activations and weights during calibration. These can be customized to suit different data distributions or quantization strategies. PyTorch provides default observers like <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.MinMaxObserver\"><code class=\"language-plaintext highlighter-rouge\">MinMaxObserver</code></a> and <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.HistogramObserver\"><code class=\"language-plaintext highlighter-rouge\">HistogramObserver</code></a>, and users can register them via the model’s <code class=\"language-plaintext highlighter-rouge\">qconfig</code>.</li>\n          <li>Observers are inserted using <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.prepare\"><code class=\"language-plaintext highlighter-rouge\">torch.quantization.prepare</code></a>.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Operator Fusion</strong>:</p>\n\n        <ul>\n          <li>PyTorch supports the fusion of multiple operations (e.g., convolution + batch normalization + ReLU) into a single fused operator. This reduces memory access overhead and improves both runtime performance and numerical stability.</li>\n          <li>Modules can be fused using <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.fuse_modules\"><code class=\"language-plaintext highlighter-rouge\">torch.quantization.fuse_modules</code></a>.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Per-Channel Weight Quantization</strong>:</p>\n\n        <ul>\n          <li>Instead of applying the same quantization parameters across all weights in a layer, per-channel quantization independently quantizes each output channel (particularly in convolution or linear layers). This approach improves accuracy while maintaining the performance benefits of quantization.</li>\n          <li>Final conversion to the quantized model is done using <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.convert\"><code class=\"language-plaintext highlighter-rouge\">torch.quantization.convert</code></a>.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p>Post-Training Quantization (PTQ) is a technique in PyTorch that enables the conversion of a model’s weights and activations from floating-point (typically <code class=\"language-plaintext highlighter-rouge\">float32</code>) to 8-bit integers (<code class=\"language-plaintext highlighter-rouge\">int8</code>), significantly improving inference efficiency in terms of speed and memory usage. This method is particularly well-suited for deployment scenarios on both server and edge devices, where latency and resource constraints are critical.</p>\n<p>To facilitate this process, PyTorch inserts special modules known as <em>observers</em> into the model. These modules capture the activation ranges at various points in the network. Once sufficient data has been passed through the model during calibration, the observers record min-max values or histograms (depending on the observer type), which are then used during quantization.</p>\n<p>A key benefit of static quantization is that it allows quantized values to be passed between operations directly, eliminating the need for costly float-to-int and int-to-float conversions at each layer. This optimization significantly reduces runtime overhead and enables end-to-end execution in <code class=\"language-plaintext highlighter-rouge\">int8</code>.</p>\n<p>PyTorch also supports several advanced features to further improve the effectiveness of static quantization:</p>\n<ul>\n      <li>\n        <p><strong>Observers</strong>:</p>\n\n        <ul>\n          <li>Observer modules are used to collect statistics on activations and weights during calibration. These can be customized to suit different data distributions or quantization strategies. PyTorch provides default observers like <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.MinMaxObserver\"><code class=\"language-plaintext highlighter-rouge\">MinMaxObserver</code></a> and <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.HistogramObserver\"><code class=\"language-plaintext highlighter-rouge\">HistogramObserver</code></a>, and users can register them via the model’s <code class=\"language-plaintext highlighter-rouge\">qconfig</code>.</li>\n          <li>Observers are inserted using <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.prepare\"><code class=\"language-plaintext highlighter-rouge\">torch.quantization.prepare</code></a>.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Operator Fusion</strong>:</p>\n\n        <ul>\n          <li>PyTorch supports the fusion of multiple operations (e.g., convolution + batch normalization + ReLU) into a single fused operator. This reduces memory access overhead and improves both runtime performance and numerical stability.</li>\n          <li>Modules can be fused using <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.fuse_modules\"><code class=\"language-plaintext highlighter-rouge\">torch.quantization.fuse_modules</code></a>.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Per-Channel Weight Quantization</strong>:</p>\n\n        <ul>\n          <li>Instead of applying the same quantization parameters across all weights in a layer, per-channel quantization independently quantizes each output channel (particularly in convolution or linear layers). This approach improves accuracy while maintaining the performance benefits of quantization.</li>\n          <li>Final conversion to the quantized model is done using <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.convert\"><code class=\"language-plaintext highlighter-rouge\">torch.quantization.convert</code></a>.</li>\n        </ul>\n      </li>\n    </ul>\n<p><strong>Observers</strong>:</p>\n<ul>\n          <li>Observer modules are used to collect statistics on activations and weights during calibration. These can be customized to suit different data distributions or quantization strategies. PyTorch provides default observers like <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.MinMaxObserver\"><code class=\"language-plaintext highlighter-rouge\">MinMaxObserver</code></a> and <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.HistogramObserver\"><code class=\"language-plaintext highlighter-rouge\">HistogramObserver</code></a>, and users can register them via the model’s <code class=\"language-plaintext highlighter-rouge\">qconfig</code>.</li>\n          <li>Observers are inserted using <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.prepare\"><code class=\"language-plaintext highlighter-rouge\">torch.quantization.prepare</code></a>.</li>\n        </ul>\n<p><strong>Operator Fusion</strong>:</p>\n<ul>\n          <li>PyTorch supports the fusion of multiple operations (e.g., convolution + batch normalization + ReLU) into a single fused operator. This reduces memory access overhead and improves both runtime performance and numerical stability.</li>\n          <li>Modules can be fused using <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.fuse_modules\"><code class=\"language-plaintext highlighter-rouge\">torch.quantization.fuse_modules</code></a>.</li>\n        </ul>\n<p><strong>Per-Channel Weight Quantization</strong>:</p>\n<ul>\n          <li>Instead of applying the same quantization parameters across all weights in a layer, per-channel quantization independently quantizes each output channel (particularly in convolution or linear layers). This approach improves accuracy while maintaining the performance benefits of quantization.</li>\n          <li>Final conversion to the quantized model is done using <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.convert\"><code class=\"language-plaintext highlighter-rouge\">torch.quantization.convert</code></a>.</li>\n        </ul>\n<h5 id=\"post-training-quantization-vs-dynamic-quantization\">Post-Training Quantization vs. Dynamic Quantization</h5>\n<ul>\n  <li>Unlike <a href=\"#dynamic--runtime-quantization\">dynamic quantization</a>, which quantizes activations on-the-fly during inference, static quantization requires an additional calibration step. This calibration involves running representative data through the model to collect statistics on the distribution of activations. These statistics guide the quantization process by determining appropriate scaling factors and zero points for each tensor.</li>\n  <li>Put simply, in PTQ, while weights are quantized ahead of time, activations are quantized using calibration data collected via observers, enabling fully quantized inference across all layers.</li>\n</ul>\n<h5 id=\"example-workflow-1\">Example Workflow</h5>\n<ul>\n  <li>Below is an example illustrating a typical PTQ workflow:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code8\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code8\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.quantization</span>\n\n<span class=\"c1\"># Step 1: Define or load the model\n</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"p\">...</span>  <span class=\"c1\"># assume a pre-trained model is loaded\n</span>\n<span class=\"c1\"># Step 2: Set the quantization configuration\n# Choose backend depending on target device\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">qconfig</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">get_default_qconfig</span><span class=\"p\">(</span><span class=\"s\">'qnnpack'</span><span class=\"p\">)</span>  <span class=\"c1\"># for ARM/mobile\n# model.qconfig = torch.quantization.get_default_qconfig('fbgemm')  # for x86/server\n</span>\n<span class=\"c1\"># Step 3: Fuse modules (e.g., Conv + BN + ReLU)\n</span><span class=\"n\">model_fused</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">fuse_modules</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"p\">[[</span><span class=\"s\">'conv'</span><span class=\"p\">,</span> <span class=\"s\">'bn'</span><span class=\"p\">,</span> <span class=\"s\">'relu'</span><span class=\"p\">]])</span>\n\n<span class=\"c1\"># Step 4: Insert observer modules\n</span><span class=\"n\">model_prepared</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">prepare</span><span class=\"p\">(</span><span class=\"n\">model_fused</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 5: Calibrate the model with representative data\n# For example, run a few batches of real or synthetic inputs\n</span><span class=\"n\">model_prepared</span><span class=\"p\">(</span><span class=\"n\">example_batch</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 6: Convert to a quantized model\n</span><span class=\"n\">model_quantized</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"n\">model_prepared</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code8\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code8\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.quantization</span>\n\n<span class=\"c1\"># Step 1: Define or load the model\n</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"p\">...</span>  <span class=\"c1\"># assume a pre-trained model is loaded\n</span>\n<span class=\"c1\"># Step 2: Set the quantization configuration\n# Choose backend depending on target device\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">qconfig</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">get_default_qconfig</span><span class=\"p\">(</span><span class=\"s\">'qnnpack'</span><span class=\"p\">)</span>  <span class=\"c1\"># for ARM/mobile\n# model.qconfig = torch.quantization.get_default_qconfig('fbgemm')  # for x86/server\n</span>\n<span class=\"c1\"># Step 3: Fuse modules (e.g., Conv + BN + ReLU)\n</span><span class=\"n\">model_fused</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">fuse_modules</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"p\">[[</span><span class=\"s\">'conv'</span><span class=\"p\">,</span> <span class=\"s\">'bn'</span><span class=\"p\">,</span> <span class=\"s\">'relu'</span><span class=\"p\">]])</span>\n\n<span class=\"c1\"># Step 4: Insert observer modules\n</span><span class=\"n\">model_prepared</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">prepare</span><span class=\"p\">(</span><span class=\"n\">model_fused</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 5: Calibrate the model with representative data\n# For example, run a few batches of real or synthetic inputs\n</span><span class=\"n\">model_prepared</span><span class=\"p\">(</span><span class=\"n\">example_batch</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 6: Convert to a quantized model\n</span><span class=\"n\">model_quantized</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"n\">model_prepared</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>This static quantization pipeline can yield 2× to 4× speedups in inference latency and a 4× reduction in model size, with minimal degradation in accuracy when calibrated effectively.</li>\n</ul>\n<h6 id=\"workflow-explanation-1\">Workflow Explanation</h6>\n<ul>\n  <li>\n    <p>After the example workflow, here is a breakdown of each step and its purpose:</p>\n\n    <ul>\n      <li><strong>Model Preparation</strong>: The model must be in eval mode (<code class=\"language-plaintext highlighter-rouge\">model.eval()</code>) so that observers and quantization stubs function deterministically. Depending on the backend, <code class=\"language-plaintext highlighter-rouge\">model.qconfig = torch.ao.quantization.get_default_qconfig('x86')</code> or <code class=\"language-plaintext highlighter-rouge\">'qnnpack'</code> sets the appropriate quantization configuration.</li>\n      <li><strong>Operator Fusion</strong>: Use <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.fuse_modules\"><code class=\"language-plaintext highlighter-rouge\">torch.quantization.fuse_modules</code></a> to merge modules like Conv‑BatchNorm‑ReLU into a single fused operator. This improves numerical stability and reduces redundant quant‑dequant steps.</li>\n      <li><strong>Observer Insertion</strong>: Invoke <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.prepare\"><code class=\"language-plaintext highlighter-rouge\">torch.quantization.prepare</code></a> to automatically insert observer modules (e.g., <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.MinMaxObserver\"><code class=\"language-plaintext highlighter-rouge\">MinMaxObserver</code></a>). These record activation statistics during the calibration phase.</li>\n      <li><strong>Calibration</strong>: Run representative real-world input data through the prepared model to collect min/max or histogram statistics via observers. Approximately 100‑200 mini‑batches often suffice for good calibration.</li>\n      <li><strong>Conversion to Quantized Model</strong>: Use <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.convert\"><code class=\"language-plaintext highlighter-rouge\">torch.quantization.convert</code></a> to replace observed layers with quantized counterparts, applying pre-determined scales and zero points. The resulting model executes end‑to‑end in <code class=\"language-plaintext highlighter-rouge\">int8</code> arithmetic.</li>\n    </ul>\n  </li>\n</ul>\n<p>After the example workflow, here is a breakdown of each step and its purpose:</p>\n<ul>\n      <li><strong>Model Preparation</strong>: The model must be in eval mode (<code class=\"language-plaintext highlighter-rouge\">model.eval()</code>) so that observers and quantization stubs function deterministically. Depending on the backend, <code class=\"language-plaintext highlighter-rouge\">model.qconfig = torch.ao.quantization.get_default_qconfig('x86')</code> or <code class=\"language-plaintext highlighter-rouge\">'qnnpack'</code> sets the appropriate quantization configuration.</li>\n      <li><strong>Operator Fusion</strong>: Use <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.fuse_modules\"><code class=\"language-plaintext highlighter-rouge\">torch.quantization.fuse_modules</code></a> to merge modules like Conv‑BatchNorm‑ReLU into a single fused operator. This improves numerical stability and reduces redundant quant‑dequant steps.</li>\n      <li><strong>Observer Insertion</strong>: Invoke <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.prepare\"><code class=\"language-plaintext highlighter-rouge\">torch.quantization.prepare</code></a> to automatically insert observer modules (e.g., <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.MinMaxObserver\"><code class=\"language-plaintext highlighter-rouge\">MinMaxObserver</code></a>). These record activation statistics during the calibration phase.</li>\n      <li><strong>Calibration</strong>: Run representative real-world input data through the prepared model to collect min/max or histogram statistics via observers. Approximately 100‑200 mini‑batches often suffice for good calibration.</li>\n      <li><strong>Conversion to Quantized Model</strong>: Use <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.convert\"><code class=\"language-plaintext highlighter-rouge\">torch.quantization.convert</code></a> to replace observed layers with quantized counterparts, applying pre-determined scales and zero points. The resulting model executes end‑to‑end in <code class=\"language-plaintext highlighter-rouge\">int8</code> arithmetic.</li>\n    </ul>\n<h6 id=\"typical-benefits-1\">Typical Benefits</h6>\n<ul>\n  <li>Model size is typically reduced by ≈4× (since <code class=\"language-plaintext highlighter-rouge\">int8</code> requires only 1 byte per parameter instead of 4) and memory bandwidth requirements drop significantly.</li>\n  <li>Inference latency improves—often 2× to 4× faster than float32—by eliminating repeated float‑int conversions and enabling optimized integer kernels on CPU and mobile.</li>\n  <li>Enables uniform quantized execution across the network, which improves cache locality and enables hardware acceleration on supported platforms.</li>\n</ul>\n<h6 id=\"notes--tradeoffs-1\">Notes &amp; Trade‑offs</h6>\n<ul>\n  <li>Requires a representative calibration dataset. If the input distribution drifts significantly, fixed quantization ranges may degrade accuracy over time.</li>\n  <li>Slight accuracy loss compared to floating‑point baseline—although typically small (~1‑2%)—especially on highly non-linear or sensitive models. For critical accuracy use‑cases, Quantization Aware Training may be more suitable.</li>\n  <li>Not all operators are supported for eager/static quantization. While convolution, linear, and RNN layers are supported, custom or unsupported layers may need manual handling or fallbacks. Per-channel quantization support is available, but requires proper qconfig settings.</li>\n  <li>The quantization workflow in PyTorch uses either Eager Mode or FX Graph Mode. FX mode can automate fusion and support functional operators, but may require model refactoring. Eager Mode offers more manual control but with limited operator coverage.</li>\n</ul>\n<h4 id=\"quantizationaware-training-qat\">Quantization‑aware Training (QAT)</h4>\n<ul>\n  <li>\n    <p>QAT is the most accurate among PyTorch’s three quantization techniques for static quantization. With QAT, all weights and activations are subject to “fake quantization” during both forward and backward passes: values are rounded to simulate <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization, while computations remain in floating‑point. Consequently, weight updates occur with full awareness that the model will eventually operate in <code class=\"language-plaintext highlighter-rouge\">int8</code>. As a result, models trained with QAT generally achieve higher post‑quantization accuracy than those produced by post‑training quantization or dynamic quantization.</p>\n  </li>\n  <li>\n    <p>The principle is straightforward: the training process is informed about the ultimate quantized inference format. During training, activations and weights are rounded appropriately, so gradient flow reflects the quantization effects. However, the backpropagation itself—the gradient descent—is executed using full‑precision arithmetic.</p>\n  </li>\n  <li>\n    <p>After QAT training and conversion, the final model stores both weights and activations in the <code class=\"language-plaintext highlighter-rouge\">int8</code> quantized format, making it suitable for efficient inference on quantization-compatible hardware.</p>\n  </li>\n  <li>\n    <p>To implement QAT in PyTorch’s eager‑mode workflow, one typically follows these steps:</p>\n\n    <ol>\n      <li>Fuse suitable modules (e.g. Conv+ReLU, Conv+BatchNorm) via <code class=\"language-plaintext highlighter-rouge\">torch.quantization.fuse_modules</code>.</li>\n      <li>Insert <code class=\"language-plaintext highlighter-rouge\">QuantStub</code> and <code class=\"language-plaintext highlighter-rouge\">DeQuantStub</code> modules to manage quantization boundaries.</li>\n      <li>Assign <code class=\"language-plaintext highlighter-rouge\">.qconfig</code> to modules—e.g. via <code class=\"language-plaintext highlighter-rouge\">torch.quantization.get_default_qat_qconfig('fbgemm')</code> or <code class=\"language-plaintext highlighter-rouge\">'qnnpack'</code>.</li>\n      <li>Prepare the model using <code class=\"language-plaintext highlighter-rouge\">torch.ao.quantization.prepare_qat()</code> or <code class=\"language-plaintext highlighter-rouge\">torch.quantization.prepare_qat()</code>.</li>\n      <li>Train or fine‑tune the model in training mode.</li>\n      <li>After training, apply <code class=\"language-plaintext highlighter-rouge\">torch.ao.quantization.convert()</code> or <code class=\"language-plaintext highlighter-rouge\">torch.quantization.convert()</code> to produce the fully quantized <code class=\"language-plaintext highlighter-rouge\">int8</code> model.</li>\n    </ol>\n  </li>\n  <li>\n    <p>A code snippet in PyTorch invoking QAT:</p>\n  </li>\n</ul>\n<p>QAT is the most accurate among PyTorch’s three quantization techniques for static quantization. With QAT, all weights and activations are subject to “fake quantization” during both forward and backward passes: values are rounded to simulate <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization, while computations remain in floating‑point. Consequently, weight updates occur with full awareness that the model will eventually operate in <code class=\"language-plaintext highlighter-rouge\">int8</code>. As a result, models trained with QAT generally achieve higher post‑quantization accuracy than those produced by post‑training quantization or dynamic quantization.</p>\n<p>The principle is straightforward: the training process is informed about the ultimate quantized inference format. During training, activations and weights are rounded appropriately, so gradient flow reflects the quantization effects. However, the backpropagation itself—the gradient descent—is executed using full‑precision arithmetic.</p>\n<p>After QAT training and conversion, the final model stores both weights and activations in the <code class=\"language-plaintext highlighter-rouge\">int8</code> quantized format, making it suitable for efficient inference on quantization-compatible hardware.</p>\n<p>To implement QAT in PyTorch’s eager‑mode workflow, one typically follows these steps:</p>\n<ol>\n      <li>Fuse suitable modules (e.g. Conv+ReLU, Conv+BatchNorm) via <code class=\"language-plaintext highlighter-rouge\">torch.quantization.fuse_modules</code>.</li>\n      <li>Insert <code class=\"language-plaintext highlighter-rouge\">QuantStub</code> and <code class=\"language-plaintext highlighter-rouge\">DeQuantStub</code> modules to manage quantization boundaries.</li>\n      <li>Assign <code class=\"language-plaintext highlighter-rouge\">.qconfig</code> to modules—e.g. via <code class=\"language-plaintext highlighter-rouge\">torch.quantization.get_default_qat_qconfig('fbgemm')</code> or <code class=\"language-plaintext highlighter-rouge\">'qnnpack'</code>.</li>\n      <li>Prepare the model using <code class=\"language-plaintext highlighter-rouge\">torch.ao.quantization.prepare_qat()</code> or <code class=\"language-plaintext highlighter-rouge\">torch.quantization.prepare_qat()</code>.</li>\n      <li>Train or fine‑tune the model in training mode.</li>\n      <li>After training, apply <code class=\"language-plaintext highlighter-rouge\">torch.ao.quantization.convert()</code> or <code class=\"language-plaintext highlighter-rouge\">torch.quantization.convert()</code> to produce the fully quantized <code class=\"language-plaintext highlighter-rouge\">int8</code> model.</li>\n    </ol>\n<p>A code snippet in PyTorch invoking QAT:</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code9\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code9\"><span class=\"n\">qat_model</span><span class=\"p\">.</span><span class=\"n\">qconfig</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">get_default_qat_qconfig</span><span class=\"p\">(</span><span class=\"s\">'fbgemm'</span><span class=\"p\">)</span>\n<span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">prepare_qat</span><span class=\"p\">(</span><span class=\"n\">qat_model</span><span class=\"p\">,</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"c1\"># train or fine‑tune qat_model ...\n</span><span class=\"n\">quantized_model</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"n\">qat_model</span><span class=\"p\">.</span><span class=\"nb\">eval</span><span class=\"p\">(),</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code9\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code9\"><span class=\"n\">qat_model</span><span class=\"p\">.</span><span class=\"n\">qconfig</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">get_default_qat_qconfig</span><span class=\"p\">(</span><span class=\"s\">'fbgemm'</span><span class=\"p\">)</span>\n<span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">prepare_qat</span><span class=\"p\">(</span><span class=\"n\">qat_model</span><span class=\"p\">,</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"c1\"># train or fine‑tune qat_model ...\n</span><span class=\"n\">quantized_model</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"n\">qat_model</span><span class=\"p\">.</span><span class=\"nb\">eval</span><span class=\"p\">(),</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>The fake quantization modules, which simulate the effects of quantization during both forward and backward passes, internally use observers (e.g., <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.MinMaxObserver\"><code class=\"language-plaintext highlighter-rouge\">MinMaxObserver</code></a> or <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.HistogramObserver\"><code class=\"language-plaintext highlighter-rouge\">HistogramObserver</code></a>) to track activation and weight ranges during training. These fake quantization modules, which are inserted during training, are typically replaced with real quantized operators in the converted model.</li>\n</ul>\n<h5 id=\"quantizationaware-training-vs-post-training-quantization-vs-dynamic-quantization\">Quantization‑aware Training vs. Post-Training Quantization vs. Dynamic Quantization</h5>\n<ul>\n  <li>Note that unlike <a href=\"#dynamic--runtime-quantization\">dynamic quantization</a> (which only quantizes weights statically and activations on-the-fly during inference), QAT simulates quantization for both weights and activations during training. This allows the model to learn parameters that are robust to quantization-induced errors introduced at inference time. Put simply, it allows the parameters to adapt to quantization noise during inference and typically results in significantly better accuracy, especially for models with activation-sensitive layers such as convolutional networks.</li>\n  <li>Furthermore, unlike <a href=\"#post-training-quantization\">post-training quantization</a>, QAT does not require a separate calibration phase after training. Instead, it uses the observer modules during training itself to learn and track the necessary quantization parameters, effectively integrating calibration into the training loop.</li>\n</ul>\n<h5 id=\"example-workflow-2\">Example Workflow</h5>\n<ul>\n  <li>This sub‑section illustrates a complete workflow for applying static QAT to a convolutional neural network (e.g. ResNet18):</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code10\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code10\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"n\">nn</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.quantization</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.quantization</span> <span class=\"kn\">import</span> <span class=\"n\">QuantStub</span><span class=\"p\">,</span> <span class=\"n\">DeQuantStub</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">MyModel</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">().</span><span class=\"n\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">quant</span> <span class=\"o\">=</span> <span class=\"n\">QuantStub</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">conv</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">bn</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">BatchNorm2d</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">relu</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">ReLU</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">fc</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"o\">*</span><span class=\"mi\">32</span><span class=\"o\">*</span><span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">dequant</span> <span class=\"o\">=</span> <span class=\"n\">DeQuantStub</span><span class=\"p\">()</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">quant</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">bn</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">conv</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)))</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">flatten</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">fc</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">dequant</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">x</span>\n\n<span class=\"c1\"># 1. Load pre‑trained or float‑trained model\n</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">MyModel</span><span class=\"p\">()</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">eval</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># 2. Fuse conv, bn, relu\n</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">fuse_modules</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"p\">[[</span><span class=\"s\">'conv'</span><span class=\"p\">,</span><span class=\"s\">'bn'</span><span class=\"p\">,</span><span class=\"s\">'relu'</span><span class=\"p\">]],</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 3. Attach QAT config\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">qconfig</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">get_default_qat_qconfig</span><span class=\"p\">(</span><span class=\"s\">'fbgemm'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 4. Prepare QAT\n</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">prepare_qat</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 5. Fine‑tune QAT model\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n<span class=\"c1\"># run training loop for several epochs ...\n</span>\n<span class=\"c1\"># 6. Convert to quantized model\n</span><span class=\"n\">quantized_model</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">eval</span><span class=\"p\">(),</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 7. Evaluate quantized_model for accuracy and inference performance\n</span></code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code10\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code10\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"n\">nn</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.quantization</span>\n<span class=\"kn\">from</span> <span class=\"nn\">torch.quantization</span> <span class=\"kn\">import</span> <span class=\"n\">QuantStub</span><span class=\"p\">,</span> <span class=\"n\">DeQuantStub</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">MyModel</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">().</span><span class=\"n\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">quant</span> <span class=\"o\">=</span> <span class=\"n\">QuantStub</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">conv</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Conv2d</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">stride</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">bn</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">BatchNorm2d</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">relu</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">ReLU</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">fc</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"o\">*</span><span class=\"mi\">32</span><span class=\"o\">*</span><span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">dequant</span> <span class=\"o\">=</span> <span class=\"n\">DeQuantStub</span><span class=\"p\">()</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">quant</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">bn</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">conv</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)))</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"p\">.</span><span class=\"n\">flatten</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">fc</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">dequant</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">x</span>\n\n<span class=\"c1\"># 1. Load pre‑trained or float‑trained model\n</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">MyModel</span><span class=\"p\">()</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">eval</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># 2. Fuse conv, bn, relu\n</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">fuse_modules</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"p\">[[</span><span class=\"s\">'conv'</span><span class=\"p\">,</span><span class=\"s\">'bn'</span><span class=\"p\">,</span><span class=\"s\">'relu'</span><span class=\"p\">]],</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 3. Attach QAT config\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">qconfig</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">get_default_qat_qconfig</span><span class=\"p\">(</span><span class=\"s\">'fbgemm'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 4. Prepare QAT\n</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">prepare_qat</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 5. Fine‑tune QAT model\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n<span class=\"c1\"># run training loop for several epochs ...\n</span>\n<span class=\"c1\"># 6. Convert to quantized model\n</span><span class=\"n\">quantized_model</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">eval</span><span class=\"p\">(),</span> <span class=\"n\">inplace</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 7. Evaluate quantized_model for accuracy and inference performance\n</span></code></pre>\n<h6 id=\"workflow-explanation-2\">Workflow Explanation</h6>\n<ul>\n  <li>This workflow enforces quantization effects during training by simulating rounding and clamping via fake quantization modules. <code class=\"language-plaintext highlighter-rouge\">QuantStub</code> and <code class=\"language-plaintext highlighter-rouge\">DeQuantStub</code> demarcate where data transitions between float and quantized domains. <code class=\"language-plaintext highlighter-rouge\">qconfig</code> controls observer placement and quantization schemes (e.g. symmetric vs affine, per‑tensor vs per‑channel).</li>\n  <li>Fake quantization is active during training, guiding the network to adapt to the constraints of int8 inference arithmetic. Only after fine‑tuning does <code class=\"language-plaintext highlighter-rouge\">convert()</code> replace fake quant modules with actual quantized operators for efficient <code class=\"language-plaintext highlighter-rouge\">int8</code> inference execution.</li>\n</ul>\n<h6 id=\"typical-benefits-2\">Typical Benefits</h6>\n<ul>\n  <li>Higher quantized accuracy than post‑training static or dynamic quantization, often reducing performance degradation to minimal levels.</li>\n  <li>Improved robustness to quantization noise, particularly important for convolutional networks and vision models.</li>\n  <li>Retains compression benefits: reduced model size (≈25% of float model) and faster inference on hardware optimized for <code class=\"language-plaintext highlighter-rouge\">int8</code>.</li>\n</ul>\n<h6 id=\"notes--tradeoffs-2\">Notes &amp; Trade‑offs</h6>\n<ul>\n  <li>QAT requires additional training or fine‑tuning, increasing overall development time.</li>\n  <li>Careful scheduling is needed: a small learning rate is recommended to avoid instability introduced by straight‑through estimator (STE) approximations.</li>\n  <li>Model preparation steps such as layer fusion and correct placement of quant stubs are critical. Missing fusions can degrade accuracy.</li>\n  <li>Not all operators or model architectures are fully quantization‑aware; some require manual adaptation.</li>\n  <li>The quantized model behavior may differ subtly from the fake‑quant version: as reported, the output of a real quantized model may diverge slightly from fake‑quant during testing on toy models.</li>\n</ul>",
      "contentMarkdown": "*   Quantization in PyTorch enables the execution of computations and memory accesses with reduced-precision data types, typically `int8`, leading to improvements in model efficiency, inference speed, and memory footprint. PyTorch provides comprehensive support for quantization, starting from version 1.3, through an API that integrates seamlessly with the existing eager execution model.\n    \n*   **Quantized Tensor Representation**\n    \n    *   PyTorch introduces special data types for quantized tensors, enabling the representation of weights and activations in reduced precision (typically `int8`, and sometimes `float16`). These tensors can be operated on via quantized kernels available under `torch.nn.quantized` and `torch.nn.quantized.dynamic`. These quantized operations allow for a 4× reduction in model size and 2-4× improvements in memory bandwidth and inference latency, depending on the hardware and model structure.\n        \n    *   Quantization in PyTorch relies on calibration, which is the process of gathering statistics on representative inputs to determine optimal quantization parameters (such as scale and zero-point). These parameters are used in quantization functions of the form `round(x / scale) + zero_point`, enabling a linear mapping between floating point and integer domains.\n        \n*   **Quantization Backends**\n    \n    *   PyTorch leverages optimized backend libraries to execute quantized operations efficiently. FBGEMM (Facebook’s GEMM library) is optimized for server environments (x86 CPUs), while QNNPACK is designed for mobile and embedded environments. These are analogous to BLAS/MKL libraries in floating-point computation and are integrated automatically based on the target deployment platform.\n*   **Numerical Stability and Mixed Precision**\n    \n    *   One challenge in quantization is maintaining numerical stability, particularly for operations involving accumulation or exponentiation. To address this, PyTorch supports mixed-precision training and inference using the `torch.cuda.amp` module. AMP (Automatic Mixed Precision) allows portions of the model to be cast to `torch.float16` while retaining `torch.float32` for operations requiring higher precision, improving performance with minimal loss of accuracy. Although initially introduced for CUDA GPUs, mixed-precision techniques are distinct from quantization but can be complementary in certain scenarios.\n*   **Quantization Techniques in PyTorch**\n    \n    *   PyTorch provides three primary quantization workflows under the `torch.quantization` namespace, often referred to collectively as “eager mode quantization”:\n        \n    *   **Dynamic Quantization**:\n        \n        *   Weights are statically quantized and stored in int8 format, while activations are dynamically quantized at runtime before computation. This method requires minimal code changes and no calibration data. It is most effective for models dominated by linear layers (e.g., LSTM, GRU, Transformer-based models).\n            \n        *   Example:\n            \n        \n        ![](https://aman.ai/images/copy.png)\n        \n        `torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)`\n        \n    *   **Post-Training Quantization**:\n        \n        *   Both weights and activations are quantized. This approach requires calibration, where representative input data is passed through the model to collect statistics via observer modules. Operator fusion (e.g., Conv + ReLU) and per-channel quantization are supported to improve performance and accuracy.\n            \n        *   Example sequence:\n            \n        \n        ![](https://aman.ai/images/copy.png)\n        \n        `model.qconfig = torch.quantization.get_default_qconfig('fbgemm') torch.quantization.prepare(model, inplace=True) # Run calibration with representative data torch.quantization.convert(model, inplace=True)`\n        \n    *   **Quantization-Aware Training (QAT)**:\n        \n        *   This technique inserts fake-quantization modules during training, simulating quantization effects in both forward and backward passes. It typically yields the highest post-quantization accuracy, especially in cases where model accuracy is sensitive to quantization noise.\n            \n        *   Example sequence:\n            \n        \n        ![](https://aman.ai/images/copy.png)\n        \n        `model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm') torch.quantization.prepare_qat(model, inplace=True) # Train model torch.quantization.convert(model.eval(), inplace=True)`\n        \n*   **Operator and Layer Coverage**\n    \n    *   Quantization support varies by method. Dynamic quantization supports layers like Linear and RNNs, while static and QAT methods support a broader set including Conv, ReLU, BatchNorm (via fusion), and more. FX Graph Mode Quantization (a newer, graph-level approach not covered here) further expands operator support and streamlines workflows.\n*   For additional guidance and end-to-end examples, refer to the official PyTorch blog: [Introduction to Quantization on PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/).\n    \n\nQuantization in PyTorch enables the execution of computations and memory accesses with reduced-precision data types, typically `int8`, leading to improvements in model efficiency, inference speed, and memory footprint. PyTorch provides comprehensive support for quantization, starting from version 1.3, through an API that integrates seamlessly with the existing eager execution model.\n\n**Quantized Tensor Representation**\n\n*   PyTorch introduces special data types for quantized tensors, enabling the representation of weights and activations in reduced precision (typically `int8`, and sometimes `float16`). These tensors can be operated on via quantized kernels available under `torch.nn.quantized` and `torch.nn.quantized.dynamic`. These quantized operations allow for a 4× reduction in model size and 2-4× improvements in memory bandwidth and inference latency, depending on the hardware and model structure.\n    \n*   Quantization in PyTorch relies on calibration, which is the process of gathering statistics on representative inputs to determine optimal quantization parameters (such as scale and zero-point). These parameters are used in quantization functions of the form `round(x / scale) + zero_point`, enabling a linear mapping between floating point and integer domains.\n    \n\nPyTorch introduces special data types for quantized tensors, enabling the representation of weights and activations in reduced precision (typically `int8`, and sometimes `float16`). These tensors can be operated on via quantized kernels available under `torch.nn.quantized` and `torch.nn.quantized.dynamic`. These quantized operations allow for a 4× reduction in model size and 2-4× improvements in memory bandwidth and inference latency, depending on the hardware and model structure.\n\nQuantization in PyTorch relies on calibration, which is the process of gathering statistics on representative inputs to determine optimal quantization parameters (such as scale and zero-point). These parameters are used in quantization functions of the form `round(x / scale) + zero_point`, enabling a linear mapping between floating point and integer domains.\n\n**Quantization Backends**\n\n*   PyTorch leverages optimized backend libraries to execute quantized operations efficiently. FBGEMM (Facebook’s GEMM library) is optimized for server environments (x86 CPUs), while QNNPACK is designed for mobile and embedded environments. These are analogous to BLAS/MKL libraries in floating-point computation and are integrated automatically based on the target deployment platform.\n\n**Numerical Stability and Mixed Precision**\n\n*   One challenge in quantization is maintaining numerical stability, particularly for operations involving accumulation or exponentiation. To address this, PyTorch supports mixed-precision training and inference using the `torch.cuda.amp` module. AMP (Automatic Mixed Precision) allows portions of the model to be cast to `torch.float16` while retaining `torch.float32` for operations requiring higher precision, improving performance with minimal loss of accuracy. Although initially introduced for CUDA GPUs, mixed-precision techniques are distinct from quantization but can be complementary in certain scenarios.\n\n**Quantization Techniques in PyTorch**\n\n*   PyTorch provides three primary quantization workflows under the `torch.quantization` namespace, often referred to collectively as “eager mode quantization”:\n    \n*   **Dynamic Quantization**:\n    \n    *   Weights are statically quantized and stored in int8 format, while activations are dynamically quantized at runtime before computation. This method requires minimal code changes and no calibration data. It is most effective for models dominated by linear layers (e.g., LSTM, GRU, Transformer-based models).\n        \n    *   Example:\n        \n    \n    ![](https://aman.ai/images/copy.png)\n    \n    `torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)`\n    \n*   **Post-Training Quantization**:\n    \n    *   Both weights and activations are quantized. This approach requires calibration, where representative input data is passed through the model to collect statistics via observer modules. Operator fusion (e.g., Conv + ReLU) and per-channel quantization are supported to improve performance and accuracy.\n        \n    *   Example sequence:\n        \n    \n    ![](https://aman.ai/images/copy.png)\n    \n    `model.qconfig = torch.quantization.get_default_qconfig('fbgemm') torch.quantization.prepare(model, inplace=True) # Run calibration with representative data torch.quantization.convert(model, inplace=True)`\n    \n*   **Quantization-Aware Training (QAT)**:\n    \n    *   This technique inserts fake-quantization modules during training, simulating quantization effects in both forward and backward passes. It typically yields the highest post-quantization accuracy, especially in cases where model accuracy is sensitive to quantization noise.\n        \n    *   Example sequence:\n        \n    \n    ![](https://aman.ai/images/copy.png)\n    \n    `model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm') torch.quantization.prepare_qat(model, inplace=True) # Train model torch.quantization.convert(model.eval(), inplace=True)`\n    \n\nPyTorch provides three primary quantization workflows under the `torch.quantization` namespace, often referred to collectively as “eager mode quantization”:\n\n*   Weights are statically quantized and stored in int8 format, while activations are dynamically quantized at runtime before computation. This method requires minimal code changes and no calibration data. It is most effective for models dominated by linear layers (e.g., LSTM, GRU, Transformer-based models).\n    \n*   Example:\n    \n\nWeights are statically quantized and stored in int8 format, while activations are dynamically quantized at runtime before computation. This method requires minimal code changes and no calibration data. It is most effective for models dominated by linear layers (e.g., LSTM, GRU, Transformer-based models).\n\nExample:\n\n![](https://aman.ai/images/copy.png)\n\n`torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)`\n\n*   Both weights and activations are quantized. This approach requires calibration, where representative input data is passed through the model to collect statistics via observer modules. Operator fusion (e.g., Conv + ReLU) and per-channel quantization are supported to improve performance and accuracy.\n    \n*   Example sequence:\n    \n\nBoth weights and activations are quantized. This approach requires calibration, where representative input data is passed through the model to collect statistics via observer modules. Operator fusion (e.g., Conv + ReLU) and per-channel quantization are supported to improve performance and accuracy.\n\nExample sequence:\n\n![](https://aman.ai/images/copy.png)\n\n`model.qconfig = torch.quantization.get_default_qconfig('fbgemm') torch.quantization.prepare(model, inplace=True) # Run calibration with representative data torch.quantization.convert(model, inplace=True)`\n\n*   This technique inserts fake-quantization modules during training, simulating quantization effects in both forward and backward passes. It typically yields the highest post-quantization accuracy, especially in cases where model accuracy is sensitive to quantization noise.\n    \n*   Example sequence:\n    \n\nThis technique inserts fake-quantization modules during training, simulating quantization effects in both forward and backward passes. It typically yields the highest post-quantization accuracy, especially in cases where model accuracy is sensitive to quantization noise.\n\nExample sequence:\n\n![](https://aman.ai/images/copy.png)\n\n`model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm') torch.quantization.prepare_qat(model, inplace=True) # Train model torch.quantization.convert(model.eval(), inplace=True)`\n\n**Operator and Layer Coverage**\n\n*   Quantization support varies by method. Dynamic quantization supports layers like Linear and RNNs, while static and QAT methods support a broader set including Conv, ReLU, BatchNorm (via fusion), and more. FX Graph Mode Quantization (a newer, graph-level approach not covered here) further expands operator support and streamlines workflows.\n\nFor additional guidance and end-to-end examples, refer to the official PyTorch blog: [Introduction to Quantization on PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/).\n\n#### Dynamic / Runtime Quantization\n\n*   [Dynamic quantization](https://pytorch.org/docs/stable/quantization.html#dynamic-quantization) is one of the most simple quantization techniques in PyTorch, particularly suitable for models where most computation occurs in linear layers—such as transformer models (e.g. BERT) or recurrent networks (e.g. LSTM)—because these operations are dominated by matrix multiplications, which benefit significantly from `int8` acceleration without requiring quantized convolutions.\n    \n*   In dynamic quantization, model weights are converted from 32-bit floating point (`float32`) to a lower precision format such as `int8` and are permanently stored in this quantized form. Activations, however, remain in `float32` format until runtime. At inference time, these activations are dynamically quantized to `int8` immediately before the corresponding computation (i.e., matrix multiplication or linear operation) is executed. After the operation, the result is stored back in `float32`. This hybrid approach enables significant performance gains—such as reduced latency and memory usage—while maintaining reasonable model accuracy.\n    \n\n[Dynamic quantization](https://pytorch.org/docs/stable/quantization.html#dynamic-quantization) is one of the most simple quantization techniques in PyTorch, particularly suitable for models where most computation occurs in linear layers—such as transformer models (e.g. BERT) or recurrent networks (e.g. LSTM)—because these operations are dominated by matrix multiplications, which benefit significantly from `int8` acceleration without requiring quantized convolutions.\n\nIn dynamic quantization, model weights are converted from 32-bit floating point (`float32`) to a lower precision format such as `int8` and are permanently stored in this quantized form. Activations, however, remain in `float32` format until runtime. At inference time, these activations are dynamically quantized to `int8` immediately before the corresponding computation (i.e., matrix multiplication or linear operation) is executed. After the operation, the result is stored back in `float32`. This hybrid approach enables significant performance gains—such as reduced latency and memory usage—while maintaining reasonable model accuracy.\n\n> The aim of dynamic quantization is thus to save compute through faster arithmetic, rather than primarily to reduce storage needs.\n\nThe aim of dynamic quantization is thus to save compute through faster arithmetic, rather than primarily to reduce storage needs.\n\n*   Unlike static quantization or quantization-aware training (QAT), dynamic quantization requires no calibration dataset or retraining. This makes it ideal when representative data is unavailable or ease of deployment is paramount.\n    \n*   Quantization parameters (scale and zero-point) for activations are determined dynamically at each invocation based on the input data range, while weights use fixed scale and zero-point values computed ahead of time. As such, since only the model weights are quantized ahead of time, while activations remain in `float32` and are quantized dynamically at runtime based on input data, dynamic quantization is often referred to as a data-free or weight-only quantization method during preparation.\n    \n*   PyTorch provides the API [`torch.ao.quantization.quantize_dynamic(...)`](https://pytorch.org/docs/stable/generated/torch.ao.quantization.quantize_dynamic.html) for applying dynamic quantization:\n    \n    *   A model (`torch.nn.Module`)\n    *   A specification of target layer types or names (commonly `{nn.Linear, nn.LSTM}`)\n    *   A target dtype (e.g., `torch.qint8`).\n*   Only supported layer types are quantized—primarily `nn.Linear` and RNN variants; convolutional layers (e.g., `nn.Conv2d`) are not supported by dynamic quantization.\n    \n*   This approach is particularly effective for transformer and RNN models, where inference throughput is limited by memory-bound weight matrices. For example, quantizing BERT with dynamic quantization often yields up to 4× reduction in model size and measurable speedups in CPU inference latency.\n    \n\nUnlike static quantization or quantization-aware training (QAT), dynamic quantization requires no calibration dataset or retraining. This makes it ideal when representative data is unavailable or ease of deployment is paramount.\n\nQuantization parameters (scale and zero-point) for activations are determined dynamically at each invocation based on the input data range, while weights use fixed scale and zero-point values computed ahead of time. As such, since only the model weights are quantized ahead of time, while activations remain in `float32` and are quantized dynamically at runtime based on input data, dynamic quantization is often referred to as a data-free or weight-only quantization method during preparation.\n\nPyTorch provides the API [`torch.ao.quantization.quantize_dynamic(...)`](https://pytorch.org/docs/stable/generated/torch.ao.quantization.quantize_dynamic.html) for applying dynamic quantization:\n\n*   A model (`torch.nn.Module`)\n*   A specification of target layer types or names (commonly `{nn.Linear, nn.LSTM}`)\n*   A target dtype (e.g., `torch.qint8`).\n\nOnly supported layer types are quantized—primarily `nn.Linear` and RNN variants; convolutional layers (e.g., `nn.Conv2d`) are not supported by dynamic quantization.\n\nThis approach is particularly effective for transformer and RNN models, where inference throughput is limited by memory-bound weight matrices. For example, quantizing BERT with dynamic quantization often yields up to 4× reduction in model size and measurable speedups in CPU inference latency.\n\n##### Dynamic Quantization vs. Post-Training Quantization\n\n*   Unlike [Post-Training Quantization](#post-training-static-quantization), dynamic quantization does not use minmax observers or any calibration mechanism. Activation ranges are computed dynamically at runtime based on actual input data, so no observers are required during model preparation.\n\n##### Example Workflow\n\n*   Below is an example illustrating a typical dynamic quantization workflow:\n\n![](https://aman.ai/images/copy.png)\n\n``import torch from torch import nn from torch.ao.quantization import quantize_dynamic  # Assume `model` is a pretrained floating‑point nn.Module, in eval mode model.eval()  # Apply dynamic quantization to Linear and LSTM layers quantized_model = quantize_dynamic(     model,     {nn.Linear, nn.LSTM},     dtype=torch.qint8 )  # Run inference: activations will be quantized at runtime input_data = torch.randn(batch_size, seq_length, feature_dim) output = quantized_model(input_data)``\n\n![](https://aman.ai/images/copy.png)\n\n``import torch from torch import nn from torch.ao.quantization import quantize_dynamic  # Assume `model` is a pretrained floating‑point nn.Module, in eval mode model.eval()  # Apply dynamic quantization to Linear and LSTM layers quantized_model = quantize_dynamic(     model,     {nn.Linear, nn.LSTM},     dtype=torch.qint8 )  # Run inference: activations will be quantized at runtime input_data = torch.randn(batch_size, seq_length, feature_dim) output = quantized_model(input_data)``\n\n###### Workflow Explanation\n\n*   `model.eval()` ensures deterministic behavior during quantization.\n*   `quantize_dynamic(...)` replaces supported layers with their dynamic quantized implementations.\n*   Activations remain in `float32` until needed.\n*   At runtime, activations are quantized to `int8` on-the-fly, and computations are performed with `int8` weights and mixed precision accumulators.\n*   After the operation, results return to `float32`.\n\n###### Typical Benefits\n\n*   Model size reduced by ~75%, thanks to `int8` weights.\n*   Latency improvements, especially on CPU-bound operations.\n*   No calibration or fine-tuning required.\n\n###### Notes & Trade‑offs\n\n*   Dynamic quantization does not support convolution or custom layers unless manually wrapped.\n*   Dynamic quantization handles input distributions that vary widely more gracefully than static quantization, which uses fixed calibration ranges.\n*   For CNN models or workloads where activations must also be quantized ahead of time, static quantization or QAT may yield better performance and accuracy.\n\n###### Further Reading\n\n*   A comprehensive end-to-end tutorial for dynamic quantization on BERT is available [here](https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html).\n*   For a more general example and advanced usage guide, see the [dynamic quantization tutorial](https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html).\n*   The full API documentation for `torch.quantization.quantize_dynamic` is available [here](https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic).\n\n#### Post-Training Quantization\n\n*   Post-Training Quantization (PTQ) is a technique in PyTorch that enables the conversion of a model’s weights and activations from floating-point (typically `float32`) to 8-bit integers (`int8`), significantly improving inference efficiency in terms of speed and memory usage. This method is particularly well-suited for deployment scenarios on both server and edge devices, where latency and resource constraints are critical.\n    \n*   To facilitate this process, PyTorch inserts special modules known as _observers_ into the model. These modules capture the activation ranges at various points in the network. Once sufficient data has been passed through the model during calibration, the observers record min-max values or histograms (depending on the observer type), which are then used during quantization.\n    \n*   A key benefit of static quantization is that it allows quantized values to be passed between operations directly, eliminating the need for costly float-to-int and int-to-float conversions at each layer. This optimization significantly reduces runtime overhead and enables end-to-end execution in `int8`.\n    \n*   PyTorch also supports several advanced features to further improve the effectiveness of static quantization:\n    \n    *   **Observers**:\n        \n        *   Observer modules are used to collect statistics on activations and weights during calibration. These can be customized to suit different data distributions or quantization strategies. PyTorch provides default observers like [`MinMaxObserver`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.MinMaxObserver) and [`HistogramObserver`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.HistogramObserver), and users can register them via the model’s `qconfig`.\n        *   Observers are inserted using [`torch.quantization.prepare`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.prepare).\n    *   **Operator Fusion**:\n        \n        *   PyTorch supports the fusion of multiple operations (e.g., convolution + batch normalization + ReLU) into a single fused operator. This reduces memory access overhead and improves both runtime performance and numerical stability.\n        *   Modules can be fused using [`torch.quantization.fuse_modules`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.fuse_modules).\n    *   **Per-Channel Weight Quantization**:\n        \n        *   Instead of applying the same quantization parameters across all weights in a layer, per-channel quantization independently quantizes each output channel (particularly in convolution or linear layers). This approach improves accuracy while maintaining the performance benefits of quantization.\n        *   Final conversion to the quantized model is done using [`torch.quantization.convert`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.convert).\n\nPost-Training Quantization (PTQ) is a technique in PyTorch that enables the conversion of a model’s weights and activations from floating-point (typically `float32`) to 8-bit integers (`int8`), significantly improving inference efficiency in terms of speed and memory usage. This method is particularly well-suited for deployment scenarios on both server and edge devices, where latency and resource constraints are critical.\n\nTo facilitate this process, PyTorch inserts special modules known as _observers_ into the model. These modules capture the activation ranges at various points in the network. Once sufficient data has been passed through the model during calibration, the observers record min-max values or histograms (depending on the observer type), which are then used during quantization.\n\nA key benefit of static quantization is that it allows quantized values to be passed between operations directly, eliminating the need for costly float-to-int and int-to-float conversions at each layer. This optimization significantly reduces runtime overhead and enables end-to-end execution in `int8`.\n\nPyTorch also supports several advanced features to further improve the effectiveness of static quantization:\n\n*   **Observers**:\n    \n    *   Observer modules are used to collect statistics on activations and weights during calibration. These can be customized to suit different data distributions or quantization strategies. PyTorch provides default observers like [`MinMaxObserver`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.MinMaxObserver) and [`HistogramObserver`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.HistogramObserver), and users can register them via the model’s `qconfig`.\n    *   Observers are inserted using [`torch.quantization.prepare`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.prepare).\n*   **Operator Fusion**:\n    \n    *   PyTorch supports the fusion of multiple operations (e.g., convolution + batch normalization + ReLU) into a single fused operator. This reduces memory access overhead and improves both runtime performance and numerical stability.\n    *   Modules can be fused using [`torch.quantization.fuse_modules`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.fuse_modules).\n*   **Per-Channel Weight Quantization**:\n    \n    *   Instead of applying the same quantization parameters across all weights in a layer, per-channel quantization independently quantizes each output channel (particularly in convolution or linear layers). This approach improves accuracy while maintaining the performance benefits of quantization.\n    *   Final conversion to the quantized model is done using [`torch.quantization.convert`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.convert).\n\n**Observers**:\n\n*   Observer modules are used to collect statistics on activations and weights during calibration. These can be customized to suit different data distributions or quantization strategies. PyTorch provides default observers like [`MinMaxObserver`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.MinMaxObserver) and [`HistogramObserver`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.HistogramObserver), and users can register them via the model’s `qconfig`.\n*   Observers are inserted using [`torch.quantization.prepare`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.prepare).\n\n**Operator Fusion**:\n\n*   PyTorch supports the fusion of multiple operations (e.g., convolution + batch normalization + ReLU) into a single fused operator. This reduces memory access overhead and improves both runtime performance and numerical stability.\n*   Modules can be fused using [`torch.quantization.fuse_modules`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.fuse_modules).\n\n**Per-Channel Weight Quantization**:\n\n*   Instead of applying the same quantization parameters across all weights in a layer, per-channel quantization independently quantizes each output channel (particularly in convolution or linear layers). This approach improves accuracy while maintaining the performance benefits of quantization.\n*   Final conversion to the quantized model is done using [`torch.quantization.convert`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.convert).\n\n##### Post-Training Quantization vs. Dynamic Quantization\n\n*   Unlike [dynamic quantization](#dynamic--runtime-quantization), which quantizes activations on-the-fly during inference, static quantization requires an additional calibration step. This calibration involves running representative data through the model to collect statistics on the distribution of activations. These statistics guide the quantization process by determining appropriate scaling factors and zero points for each tensor.\n*   Put simply, in PTQ, while weights are quantized ahead of time, activations are quantized using calibration data collected via observers, enabling fully quantized inference across all layers.\n\n##### Example Workflow\n\n*   Below is an example illustrating a typical PTQ workflow:\n\n![](https://aman.ai/images/copy.png)\n\n`import torch import torch.quantization  # Step 1: Define or load the model model = ...  # assume a pre-trained model is loaded # Step 2: Set the quantization configuration # Choose backend depending on target device model.qconfig = torch.quantization.get_default_qconfig('qnnpack')  # for ARM/mobile # model.qconfig = torch.quantization.get_default_qconfig('fbgemm')  # for x86/server # Step 3: Fuse modules (e.g., Conv + BN + ReLU) model_fused = torch.quantization.fuse_modules(model, [['conv', 'bn', 'relu']])  # Step 4: Insert observer modules model_prepared = torch.quantization.prepare(model_fused)  # Step 5: Calibrate the model with representative data # For example, run a few batches of real or synthetic inputs model_prepared(example_batch)  # Step 6: Convert to a quantized model model_quantized = torch.quantization.convert(model_prepared)`\n\n![](https://aman.ai/images/copy.png)\n\n`import torch import torch.quantization  # Step 1: Define or load the model model = ...  # assume a pre-trained model is loaded # Step 2: Set the quantization configuration # Choose backend depending on target device model.qconfig = torch.quantization.get_default_qconfig('qnnpack')  # for ARM/mobile # model.qconfig = torch.quantization.get_default_qconfig('fbgemm')  # for x86/server # Step 3: Fuse modules (e.g., Conv + BN + ReLU) model_fused = torch.quantization.fuse_modules(model, [['conv', 'bn', 'relu']])  # Step 4: Insert observer modules model_prepared = torch.quantization.prepare(model_fused)  # Step 5: Calibrate the model with representative data # For example, run a few batches of real or synthetic inputs model_prepared(example_batch)  # Step 6: Convert to a quantized model model_quantized = torch.quantization.convert(model_prepared)`\n\n*   This static quantization pipeline can yield 2× to 4× speedups in inference latency and a 4× reduction in model size, with minimal degradation in accuracy when calibrated effectively.\n\n###### Workflow Explanation\n\n*   After the example workflow, here is a breakdown of each step and its purpose:\n    \n    *   **Model Preparation**: The model must be in eval mode (`model.eval()`) so that observers and quantization stubs function deterministically. Depending on the backend, `model.qconfig = torch.ao.quantization.get_default_qconfig('x86')` or `'qnnpack'` sets the appropriate quantization configuration.\n    *   **Operator Fusion**: Use [`torch.quantization.fuse_modules`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.fuse_modules) to merge modules like Conv‑BatchNorm‑ReLU into a single fused operator. This improves numerical stability and reduces redundant quant‑dequant steps.\n    *   **Observer Insertion**: Invoke [`torch.quantization.prepare`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.prepare) to automatically insert observer modules (e.g., [`MinMaxObserver`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.MinMaxObserver)). These record activation statistics during the calibration phase.\n    *   **Calibration**: Run representative real-world input data through the prepared model to collect min/max or histogram statistics via observers. Approximately 100‑200 mini‑batches often suffice for good calibration.\n    *   **Conversion to Quantized Model**: Use [`torch.quantization.convert`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.convert) to replace observed layers with quantized counterparts, applying pre-determined scales and zero points. The resulting model executes end‑to‑end in `int8` arithmetic.\n\nAfter the example workflow, here is a breakdown of each step and its purpose:\n\n*   **Model Preparation**: The model must be in eval mode (`model.eval()`) so that observers and quantization stubs function deterministically. Depending on the backend, `model.qconfig = torch.ao.quantization.get_default_qconfig('x86')` or `'qnnpack'` sets the appropriate quantization configuration.\n*   **Operator Fusion**: Use [`torch.quantization.fuse_modules`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.fuse_modules) to merge modules like Conv‑BatchNorm‑ReLU into a single fused operator. This improves numerical stability and reduces redundant quant‑dequant steps.\n*   **Observer Insertion**: Invoke [`torch.quantization.prepare`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.prepare) to automatically insert observer modules (e.g., [`MinMaxObserver`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.MinMaxObserver)). These record activation statistics during the calibration phase.\n*   **Calibration**: Run representative real-world input data through the prepared model to collect min/max or histogram statistics via observers. Approximately 100‑200 mini‑batches often suffice for good calibration.\n*   **Conversion to Quantized Model**: Use [`torch.quantization.convert`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.convert) to replace observed layers with quantized counterparts, applying pre-determined scales and zero points. The resulting model executes end‑to‑end in `int8` arithmetic.\n\n###### Typical Benefits\n\n*   Model size is typically reduced by ≈4× (since `int8` requires only 1 byte per parameter instead of 4) and memory bandwidth requirements drop significantly.\n*   Inference latency improves—often 2× to 4× faster than float32—by eliminating repeated float‑int conversions and enabling optimized integer kernels on CPU and mobile.\n*   Enables uniform quantized execution across the network, which improves cache locality and enables hardware acceleration on supported platforms.\n\n###### Notes & Trade‑offs\n\n*   Requires a representative calibration dataset. If the input distribution drifts significantly, fixed quantization ranges may degrade accuracy over time.\n*   Slight accuracy loss compared to floating‑point baseline—although typically small (~1‑2%)—especially on highly non-linear or sensitive models. For critical accuracy use‑cases, Quantization Aware Training may be more suitable.\n*   Not all operators are supported for eager/static quantization. While convolution, linear, and RNN layers are supported, custom or unsupported layers may need manual handling or fallbacks. Per-channel quantization support is available, but requires proper qconfig settings.\n*   The quantization workflow in PyTorch uses either Eager Mode or FX Graph Mode. FX mode can automate fusion and support functional operators, but may require model refactoring. Eager Mode offers more manual control but with limited operator coverage.\n\n#### Quantization‑aware Training (QAT)\n\n*   QAT is the most accurate among PyTorch’s three quantization techniques for static quantization. With QAT, all weights and activations are subject to “fake quantization” during both forward and backward passes: values are rounded to simulate `int8` quantization, while computations remain in floating‑point. Consequently, weight updates occur with full awareness that the model will eventually operate in `int8`. As a result, models trained with QAT generally achieve higher post‑quantization accuracy than those produced by post‑training quantization or dynamic quantization.\n    \n*   The principle is straightforward: the training process is informed about the ultimate quantized inference format. During training, activations and weights are rounded appropriately, so gradient flow reflects the quantization effects. However, the backpropagation itself—the gradient descent—is executed using full‑precision arithmetic.\n    \n*   After QAT training and conversion, the final model stores both weights and activations in the `int8` quantized format, making it suitable for efficient inference on quantization-compatible hardware.\n    \n*   To implement QAT in PyTorch’s eager‑mode workflow, one typically follows these steps:\n    \n    1.  Fuse suitable modules (e.g. Conv+ReLU, Conv+BatchNorm) via `torch.quantization.fuse_modules`.\n    2.  Insert `QuantStub` and `DeQuantStub` modules to manage quantization boundaries.\n    3.  Assign `.qconfig` to modules—e.g. via `torch.quantization.get_default_qat_qconfig('fbgemm')` or `'qnnpack'`.\n    4.  Prepare the model using `torch.ao.quantization.prepare_qat()` or `torch.quantization.prepare_qat()`.\n    5.  Train or fine‑tune the model in training mode.\n    6.  After training, apply `torch.ao.quantization.convert()` or `torch.quantization.convert()` to produce the fully quantized `int8` model.\n*   A code snippet in PyTorch invoking QAT:\n    \n\nQAT is the most accurate among PyTorch’s three quantization techniques for static quantization. With QAT, all weights and activations are subject to “fake quantization” during both forward and backward passes: values are rounded to simulate `int8` quantization, while computations remain in floating‑point. Consequently, weight updates occur with full awareness that the model will eventually operate in `int8`. As a result, models trained with QAT generally achieve higher post‑quantization accuracy than those produced by post‑training quantization or dynamic quantization.\n\nThe principle is straightforward: the training process is informed about the ultimate quantized inference format. During training, activations and weights are rounded appropriately, so gradient flow reflects the quantization effects. However, the backpropagation itself—the gradient descent—is executed using full‑precision arithmetic.\n\nAfter QAT training and conversion, the final model stores both weights and activations in the `int8` quantized format, making it suitable for efficient inference on quantization-compatible hardware.\n\nTo implement QAT in PyTorch’s eager‑mode workflow, one typically follows these steps:\n\n1.  Fuse suitable modules (e.g. Conv+ReLU, Conv+BatchNorm) via `torch.quantization.fuse_modules`.\n2.  Insert `QuantStub` and `DeQuantStub` modules to manage quantization boundaries.\n3.  Assign `.qconfig` to modules—e.g. via `torch.quantization.get_default_qat_qconfig('fbgemm')` or `'qnnpack'`.\n4.  Prepare the model using `torch.ao.quantization.prepare_qat()` or `torch.quantization.prepare_qat()`.\n5.  Train or fine‑tune the model in training mode.\n6.  After training, apply `torch.ao.quantization.convert()` or `torch.quantization.convert()` to produce the fully quantized `int8` model.\n\nA code snippet in PyTorch invoking QAT:\n\n![](https://aman.ai/images/copy.png)\n\n`qat_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm') torch.quantization.prepare_qat(qat_model, inplace=True) # train or fine‑tune qat_model ... quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False)`\n\n![](https://aman.ai/images/copy.png)\n\n`qat_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm') torch.quantization.prepare_qat(qat_model, inplace=True) # train or fine‑tune qat_model ... quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False)`\n\n*   The fake quantization modules, which simulate the effects of quantization during both forward and backward passes, internally use observers (e.g., [`MinMaxObserver`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.MinMaxObserver) or [`HistogramObserver`](https://pytorch.org/docs/stable/quantization.html#torch.quantization.HistogramObserver)) to track activation and weight ranges during training. These fake quantization modules, which are inserted during training, are typically replaced with real quantized operators in the converted model.\n\n##### Quantization‑aware Training vs. Post-Training Quantization vs. Dynamic Quantization\n\n*   Note that unlike [dynamic quantization](#dynamic--runtime-quantization) (which only quantizes weights statically and activations on-the-fly during inference), QAT simulates quantization for both weights and activations during training. This allows the model to learn parameters that are robust to quantization-induced errors introduced at inference time. Put simply, it allows the parameters to adapt to quantization noise during inference and typically results in significantly better accuracy, especially for models with activation-sensitive layers such as convolutional networks.\n*   Furthermore, unlike [post-training quantization](#post-training-quantization), QAT does not require a separate calibration phase after training. Instead, it uses the observer modules during training itself to learn and track the necessary quantization parameters, effectively integrating calibration into the training loop.\n\n##### Example Workflow\n\n*   This sub‑section illustrates a complete workflow for applying static QAT to a convolutional neural network (e.g. ResNet18):\n\n![](https://aman.ai/images/copy.png)\n\n`import torch import torch.nn as nn import torch.quantization from torch.quantization import QuantStub, DeQuantStub  class MyModel(nn.Module):     def __init__(self):         super().__init__()         self.quant = QuantStub()         self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)         self.bn = nn.BatchNorm2d(16)         self.relu = nn.ReLU()         self.fc = nn.Linear(16*32*32, 10)         self.dequant = DeQuantStub()      def forward(self, x):         x = self.quant(x)         x = self.relu(self.bn(self.conv(x)))         x = x.flatten(1)         x = self.fc(x)         x = self.dequant(x)         return x  # 1. Load pre‑trained or float‑trained model model = MyModel() model.eval()  # 2. Fuse conv, bn, relu torch.quantization.fuse_modules(model, [['conv','bn','relu']], inplace=True)  # 3. Attach QAT config model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')  # 4. Prepare QAT torch.quantization.prepare_qat(model, inplace=True)  # 5. Fine‑tune QAT model model.train() # run training loop for several epochs ... # 6. Convert to quantized model quantized_model = torch.quantization.convert(model.eval(), inplace=False)  # 7. Evaluate quantized_model for accuracy and inference performance`\n\n![](https://aman.ai/images/copy.png)\n\n`import torch import torch.nn as nn import torch.quantization from torch.quantization import QuantStub, DeQuantStub  class MyModel(nn.Module):     def __init__(self):         super().__init__()         self.quant = QuantStub()         self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)         self.bn = nn.BatchNorm2d(16)         self.relu = nn.ReLU()         self.fc = nn.Linear(16*32*32, 10)         self.dequant = DeQuantStub()      def forward(self, x):         x = self.quant(x)         x = self.relu(self.bn(self.conv(x)))         x = x.flatten(1)         x = self.fc(x)         x = self.dequant(x)         return x  # 1. Load pre‑trained or float‑trained model model = MyModel() model.eval()  # 2. Fuse conv, bn, relu torch.quantization.fuse_modules(model, [['conv','bn','relu']], inplace=True)  # 3. Attach QAT config model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')  # 4. Prepare QAT torch.quantization.prepare_qat(model, inplace=True)  # 5. Fine‑tune QAT model model.train() # run training loop for several epochs ... # 6. Convert to quantized model quantized_model = torch.quantization.convert(model.eval(), inplace=False)  # 7. Evaluate quantized_model for accuracy and inference performance`\n\n###### Workflow Explanation\n\n*   This workflow enforces quantization effects during training by simulating rounding and clamping via fake quantization modules. `QuantStub` and `DeQuantStub` demarcate where data transitions between float and quantized domains. `qconfig` controls observer placement and quantization schemes (e.g. symmetric vs affine, per‑tensor vs per‑channel).\n*   Fake quantization is active during training, guiding the network to adapt to the constraints of int8 inference arithmetic. Only after fine‑tuning does `convert()` replace fake quant modules with actual quantized operators for efficient `int8` inference execution.\n\n###### Typical Benefits\n\n*   Higher quantized accuracy than post‑training static or dynamic quantization, often reducing performance degradation to minimal levels.\n*   Improved robustness to quantization noise, particularly important for convolutional networks and vision models.\n*   Retains compression benefits: reduced model size (≈25% of float model) and faster inference on hardware optimized for `int8`.\n\n###### Notes & Trade‑offs\n\n*   QAT requires additional training or fine‑tuning, increasing overall development time.\n*   Careful scheduling is needed: a small learning rate is recommended to avoid instability introduced by straight‑through estimator (STE) approximations.\n*   Model preparation steps such as layer fusion and correct placement of quant stubs are critical. Missing fusions can degrade accuracy.\n*   Not all operators or model architectures are fully quantization‑aware; some require manual adaptation.\n*   The quantized model behavior may differ subtly from the fake‑quant version: as reported, the output of a real quantized model may diverge slightly from fake‑quant during testing on toy models.",
      "order": 10,
      "orderInChapter": 10,
      "difficulty": 4,
      "estimatedMinutes": 28,
      "tags": [
        "ondevice ai",
        "neural network",
        "transformer",
        "convolution",
        "cnn",
        "rnn",
        "lstm",
        "gru"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 5418,
        "contentLength": 90483
      },
      "nextCards": [
        "ai-model-compression-comparative-analysis-11",
        "ai-model-compression-compute-vs-memory-bottlenecks-12"
      ],
      "relatedCards": [
        "ai-padding-and-packing-motivation-the-problem-with-padding-6",
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-balancing-compute-and-memory-prefill-vs-decode-opt-8",
        "ai-federated-learning-personalization-22",
        "ai-differential-privacy-tightness-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#quantization-with-pytorch",
      "scrapedAt": "2025-12-28T11:55:50.968Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-comparative-analysis-11",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Comparative Analysis",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>Here is a detailed comparative analysis of Dynamic Quantization, PTQ, and QAT:</li>\n</ul>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Aspect</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Dynamic / Runtime Quantization</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Post-Training Quantization (PTQ)</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Quantization-Aware Training (QAT)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Primary Use Case</td>\n<td class=\"tg-tleft-valign-first\">Fast, easy quantization of models with primarily linear operations (e.g. LSTM, BERT)</td>\n<td class=\"tg-tleft-valign-first\">Quantization of convolutional or more complex models with moderate accuracy tradeoff</td>\n<td class=\"tg-tleft-valign-second\">High-accuracy quantization for models sensitive to quantization (e.g. CNNs, object detectors)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Requires Retraining?</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Requires Calibration Data?</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">Yes (to collect activation statistics)</td>\n<td class=\"tg-tleft-valign-second\">No separate calibration; statistics are collected during training</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">When Activations Are Quantized</td>\n<td class=\"tg-tleft-valign-first\">At runtime (dynamically before each operation)</td>\n<td class=\"tg-tleft-valign-first\">Statically using observer statistics from calibration</td>\n<td class=\"tg-tleft-valign-second\">During training using fake quantization modules</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantization of Weights</td>\n<td class=\"tg-tleft-valign-first\">Done ahead of time (static)</td>\n<td class=\"tg-tleft-valign-first\">Done ahead of time (static)</td>\n<td class=\"tg-tleft-valign-second\">Simulated during training, finalized during conversion</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantization of Activations</td>\n<td class=\"tg-tleft-valign-first\">Dynamically quantized at inference time</td>\n<td class=\"tg-tleft-valign-first\">Statically quantized using calibration ranges</td>\n<td class=\"tg-tleft-valign-second\">Simulated via fake quantization during training</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Typical Accuracy</td>\n<td class=\"tg-tleft-valign-first\">Moderate loss (acceptable for linear-dominant models)</td>\n<td class=\"tg-tleft-valign-first\">Slight to moderate loss</td>\n<td class=\"tg-tleft-valign-second\">Minimal loss; best accuracy among all methods</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Complexity of Setup</td>\n<td class=\"tg-tleft-valign-first\">Very low</td>\n<td class=\"tg-tleft-valign-first\">Moderate</td>\n<td class=\"tg-tleft-valign-second\">High</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Computation Format During Training</td>\n<td class=\"tg-tleft-valign-first\">Full float32</td>\n<td class=\"tg-tleft-valign-first\">Full float32</td>\n<td class=\"tg-tleft-valign-second\">Simulated int8 via fake quantization in float32</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Final Inference Format</td>\n<td class=\"tg-tleft-valign-first\">int8 weights, float32 activations outside ops</td>\n<td class=\"tg-tleft-valign-first\">int8 weights and activations</td>\n<td class=\"tg-tleft-valign-second\">int8 weights and activations</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Deployment Readiness</td>\n<td class=\"tg-tleft-valign-first\">Easy and quick to apply, suitable for rapid deployment</td>\n<td class=\"tg-tleft-valign-first\">Requires calibration workflow</td>\n<td class=\"tg-tleft-valign-second\">Requires full training pipeline</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Main Benefit</td>\n<td class=\"tg-tleft-valign-first\">Faster inference via int8 compute; no data or retraining needed</td>\n<td class=\"tg-tleft-valign-first\">Reduced latency and memory with moderate setup</td>\n<td class=\"tg-tleft-valign-second\">Maximum accuracy with full int8 inference</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Target Operators</td>\n<td class=\"tg-tleft-valign-first\">Mostly linear layers (e.g. <code>nn.Linear</code>, <code>nn.LSTM</code>)</td>\n<td class=\"tg-tleft-valign-first\">Broad operator support with fused modules (e.g. <code>Conv+ReLU</code>)</td>\n<td class=\"tg-tleft-valign-second\">Full model coverage with operator fusion and training</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Memory Footprint Reduction</td>\n<td class=\"tg-tleft-valign-first\">Partial (activations still float32)</td>\n<td class=\"tg-tleft-valign-first\">Full (weights and activations in int8)</td>\n<td class=\"tg-tleft-valign-second\">Full (weights and activations in int8)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Primary Optimization Goal</td>\n<td class=\"tg-tleft-valign-first\">Compute efficiency (faster matmuls), leading to latency savings</td>\n<td class=\"tg-tleft-valign-first\">Latency and memory savings</td>\n<td class=\"tg-tleft-valign-second\">Accuracy preservation under quantization</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Example Usage</td>\n<td class=\"tg-tleft-valign-first\"><code>torch.quantization.quantize_dynamic</code></td>\n<td class=\"tg-tleft-valign-first\"><code>prepare</code>, <code>calibrate</code>, <code>convert</code></td>\n<td class=\"tg-tleft-valign-second\"><code>prepare_qat</code>, <code>train</code>, <code>convert</code></td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Aspect</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Dynamic / Runtime Quantization</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Post-Training Quantization (PTQ)</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Quantization-Aware Training (QAT)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Primary Use Case</td>\n<td class=\"tg-tleft-valign-first\">Fast, easy quantization of models with primarily linear operations (e.g. LSTM, BERT)</td>\n<td class=\"tg-tleft-valign-first\">Quantization of convolutional or more complex models with moderate accuracy tradeoff</td>\n<td class=\"tg-tleft-valign-second\">High-accuracy quantization for models sensitive to quantization (e.g. CNNs, object detectors)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Requires Retraining?</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Requires Calibration Data?</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">Yes (to collect activation statistics)</td>\n<td class=\"tg-tleft-valign-second\">No separate calibration; statistics are collected during training</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">When Activations Are Quantized</td>\n<td class=\"tg-tleft-valign-first\">At runtime (dynamically before each operation)</td>\n<td class=\"tg-tleft-valign-first\">Statically using observer statistics from calibration</td>\n<td class=\"tg-tleft-valign-second\">During training using fake quantization modules</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantization of Weights</td>\n<td class=\"tg-tleft-valign-first\">Done ahead of time (static)</td>\n<td class=\"tg-tleft-valign-first\">Done ahead of time (static)</td>\n<td class=\"tg-tleft-valign-second\">Simulated during training, finalized during conversion</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantization of Activations</td>\n<td class=\"tg-tleft-valign-first\">Dynamically quantized at inference time</td>\n<td class=\"tg-tleft-valign-first\">Statically quantized using calibration ranges</td>\n<td class=\"tg-tleft-valign-second\">Simulated via fake quantization during training</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Typical Accuracy</td>\n<td class=\"tg-tleft-valign-first\">Moderate loss (acceptable for linear-dominant models)</td>\n<td class=\"tg-tleft-valign-first\">Slight to moderate loss</td>\n<td class=\"tg-tleft-valign-second\">Minimal loss; best accuracy among all methods</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Complexity of Setup</td>\n<td class=\"tg-tleft-valign-first\">Very low</td>\n<td class=\"tg-tleft-valign-first\">Moderate</td>\n<td class=\"tg-tleft-valign-second\">High</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Computation Format During Training</td>\n<td class=\"tg-tleft-valign-first\">Full float32</td>\n<td class=\"tg-tleft-valign-first\">Full float32</td>\n<td class=\"tg-tleft-valign-second\">Simulated int8 via fake quantization in float32</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Final Inference Format</td>\n<td class=\"tg-tleft-valign-first\">int8 weights, float32 activations outside ops</td>\n<td class=\"tg-tleft-valign-first\">int8 weights and activations</td>\n<td class=\"tg-tleft-valign-second\">int8 weights and activations</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Deployment Readiness</td>\n<td class=\"tg-tleft-valign-first\">Easy and quick to apply, suitable for rapid deployment</td>\n<td class=\"tg-tleft-valign-first\">Requires calibration workflow</td>\n<td class=\"tg-tleft-valign-second\">Requires full training pipeline</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Main Benefit</td>\n<td class=\"tg-tleft-valign-first\">Faster inference via int8 compute; no data or retraining needed</td>\n<td class=\"tg-tleft-valign-first\">Reduced latency and memory with moderate setup</td>\n<td class=\"tg-tleft-valign-second\">Maximum accuracy with full int8 inference</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Target Operators</td>\n<td class=\"tg-tleft-valign-first\">Mostly linear layers (e.g. <code>nn.Linear</code>, <code>nn.LSTM</code>)</td>\n<td class=\"tg-tleft-valign-first\">Broad operator support with fused modules (e.g. <code>Conv+ReLU</code>)</td>\n<td class=\"tg-tleft-valign-second\">Full model coverage with operator fusion and training</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Memory Footprint Reduction</td>\n<td class=\"tg-tleft-valign-first\">Partial (activations still float32)</td>\n<td class=\"tg-tleft-valign-first\">Full (weights and activations in int8)</td>\n<td class=\"tg-tleft-valign-second\">Full (weights and activations in int8)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Primary Optimization Goal</td>\n<td class=\"tg-tleft-valign-first\">Compute efficiency (faster matmuls), leading to latency savings</td>\n<td class=\"tg-tleft-valign-first\">Latency and memory savings</td>\n<td class=\"tg-tleft-valign-second\">Accuracy preservation under quantization</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Example Usage</td>\n<td class=\"tg-tleft-valign-first\"><code>torch.quantization.quantize_dynamic</code></td>\n<td class=\"tg-tleft-valign-first\"><code>prepare</code>, <code>calibrate</code>, <code>convert</code></td>\n<td class=\"tg-tleft-valign-second\"><code>prepare_qat</code>, <code>train</code>, <code>convert</code></td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "*   Here is a detailed comparative analysis of Dynamic Quantization, PTQ, and QAT:\n\n**Aspect**\n\n**Dynamic / Runtime Quantization**\n\n**Post-Training Quantization (PTQ)**\n\n**Quantization-Aware Training (QAT)**\n\nPrimary Use Case\n\nFast, easy quantization of models with primarily linear operations (e.g. LSTM, BERT)\n\nQuantization of convolutional or more complex models with moderate accuracy tradeoff\n\nHigh-accuracy quantization for models sensitive to quantization (e.g. CNNs, object detectors)\n\nRequires Retraining?\n\nNo\n\nNo\n\nYes\n\nRequires Calibration Data?\n\nNo\n\nYes (to collect activation statistics)\n\nNo separate calibration; statistics are collected during training\n\nWhen Activations Are Quantized\n\nAt runtime (dynamically before each operation)\n\nStatically using observer statistics from calibration\n\nDuring training using fake quantization modules\n\nQuantization of Weights\n\nDone ahead of time (static)\n\nDone ahead of time (static)\n\nSimulated during training, finalized during conversion\n\nQuantization of Activations\n\nDynamically quantized at inference time\n\nStatically quantized using calibration ranges\n\nSimulated via fake quantization during training\n\nTypical Accuracy\n\nModerate loss (acceptable for linear-dominant models)\n\nSlight to moderate loss\n\nMinimal loss; best accuracy among all methods\n\nComplexity of Setup\n\nVery low\n\nModerate\n\nHigh\n\nComputation Format During Training\n\nFull float32\n\nFull float32\n\nSimulated int8 via fake quantization in float32\n\nFinal Inference Format\n\nint8 weights, float32 activations outside ops\n\nint8 weights and activations\n\nint8 weights and activations\n\nDeployment Readiness\n\nEasy and quick to apply, suitable for rapid deployment\n\nRequires calibration workflow\n\nRequires full training pipeline\n\nMain Benefit\n\nFaster inference via int8 compute; no data or retraining needed\n\nReduced latency and memory with moderate setup\n\nMaximum accuracy with full int8 inference\n\nTarget Operators\n\nMostly linear layers (e.g. `nn.Linear`, `nn.LSTM`)\n\nBroad operator support with fused modules (e.g. `Conv+ReLU`)\n\nFull model coverage with operator fusion and training\n\nMemory Footprint Reduction\n\nPartial (activations still float32)\n\nFull (weights and activations in int8)\n\nFull (weights and activations in int8)\n\nPrimary Optimization Goal\n\nCompute efficiency (faster matmuls), leading to latency savings\n\nLatency and memory savings\n\nAccuracy preservation under quantization\n\nExample Usage\n\n`torch.quantization.quantize_dynamic`\n\n`prepare`, `calibrate`, `convert`\n\n`prepare_qat`, `train`, `convert`\n\n**Aspect**\n\n**Dynamic / Runtime Quantization**\n\n**Post-Training Quantization (PTQ)**\n\n**Quantization-Aware Training (QAT)**\n\nPrimary Use Case\n\nFast, easy quantization of models with primarily linear operations (e.g. LSTM, BERT)\n\nQuantization of convolutional or more complex models with moderate accuracy tradeoff\n\nHigh-accuracy quantization for models sensitive to quantization (e.g. CNNs, object detectors)\n\nRequires Retraining?\n\nNo\n\nNo\n\nYes\n\nRequires Calibration Data?\n\nNo\n\nYes (to collect activation statistics)\n\nNo separate calibration; statistics are collected during training\n\nWhen Activations Are Quantized\n\nAt runtime (dynamically before each operation)\n\nStatically using observer statistics from calibration\n\nDuring training using fake quantization modules\n\nQuantization of Weights\n\nDone ahead of time (static)\n\nDone ahead of time (static)\n\nSimulated during training, finalized during conversion\n\nQuantization of Activations\n\nDynamically quantized at inference time\n\nStatically quantized using calibration ranges\n\nSimulated via fake quantization during training\n\nTypical Accuracy\n\nModerate loss (acceptable for linear-dominant models)\n\nSlight to moderate loss\n\nMinimal loss; best accuracy among all methods\n\nComplexity of Setup\n\nVery low\n\nModerate\n\nHigh\n\nComputation Format During Training\n\nFull float32\n\nFull float32\n\nSimulated int8 via fake quantization in float32\n\nFinal Inference Format\n\nint8 weights, float32 activations outside ops\n\nint8 weights and activations\n\nint8 weights and activations\n\nDeployment Readiness\n\nEasy and quick to apply, suitable for rapid deployment\n\nRequires calibration workflow\n\nRequires full training pipeline\n\nMain Benefit\n\nFaster inference via int8 compute; no data or retraining needed\n\nReduced latency and memory with moderate setup\n\nMaximum accuracy with full int8 inference\n\nTarget Operators\n\nMostly linear layers (e.g. `nn.Linear`, `nn.LSTM`)\n\nBroad operator support with fused modules (e.g. `Conv+ReLU`)\n\nFull model coverage with operator fusion and training\n\nMemory Footprint Reduction\n\nPartial (activations still float32)\n\nFull (weights and activations in int8)\n\nFull (weights and activations in int8)\n\nPrimary Optimization Goal\n\nCompute efficiency (faster matmuls), leading to latency savings\n\nLatency and memory savings\n\nAccuracy preservation under quantization\n\nExample Usage\n\n`torch.quantization.quantize_dynamic`\n\n`prepare`, `calibrate`, `convert`\n\n`prepare_qat`, `train`, `convert`",
      "order": 11,
      "orderInChapter": 11,
      "difficulty": 3,
      "estimatedMinutes": 4,
      "tags": [
        "ondevice ai",
        "convolution",
        "cnn",
        "lstm",
        "bert",
        "optimization",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 617,
        "contentLength": 11046
      },
      "nextCards": [
        "ai-model-compression-compute-vs-memory-bottlenecks-12",
        "ai-model-compression-modern-quantization-techniques-13"
      ],
      "relatedCards": [
        "ai-on-device-transformers-tensor-processing-unit-tpu-5",
        "ai-on-device-transformers-balancing-compute-and-memory-prefill-vs-decode-opt-8",
        "ai-federated-learning-fedprox-7",
        "ai-federated-learning-scaffold-8",
        "ai-on-device-transformers-hardware-specific-optimization-notes-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#comparative-analysis",
      "scrapedAt": "2025-12-28T11:55:50.968Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-compute-vs-memory-bottlenecks-12",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Compute vs. Memory Bottlenecks",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>\n    <p>Deep learning performance is typically constrained by one of two primary bottlenecks: compute (arithmetic throughput) or memory (bandwidth and capacity). The balance between them depends on the hardware architecture, the model’s structure, and the numerical precision used.</p>\n  </li>\n  <li>\n    <p><strong>Compute-Bound Workloads</strong>:</p>\n\n    <ul>\n      <li>\n        <p>A workload is compute-bound when the GPU/CPU spends most of its time performing arithmetic operations rather than waiting for data from memory. This is common in:</p>\n\n        <ul>\n          <li>Large matrix multiplications with high arithmetic intensity (high FLOPs-to-bytes ratio).</li>\n          <li>Dense layers and convolution layers with large channel counts and large kernel sizes.</li>\n          <li>Transformer attention mechanisms with large batch sizes or long sequence lengths.</li>\n        </ul>\n      </li>\n      <li>\n        <p>In compute-bound scenarios, lowering the precision of operands (e.g., <code class=\"language-plaintext highlighter-rouge\">float32</code> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-358\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-359\"><span class=\"mo\" id=\"MathJax-Span-360\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">int8</code>) allows hardware to execute more operations per clock cycle. For example:</p>\n\n        <ul>\n          <li>NVIDIA Tensor Cores can deliver up to 2×–4× the throughput for <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> GEMMs compared to <code class=\"language-plaintext highlighter-rouge\">float32</code>.</li>\n          <li>Integer accelerators (e.g., <code class=\"language-plaintext highlighter-rouge\">int8</code> SIMD or systolic arrays) can achieve even higher gains, especially on CPUs or edge NPUs.</li>\n        </ul>\n      </li>\n      <li>\n        <p>By reducing the number of bits per operand, quantization directly increases the number of multiply-accumulate operations that can be executed in parallel within the same silicon area and clock period.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Memory-Bound Workloads</strong>:</p>\n\n    <ul>\n      <li>\n        <p>A workload is memory-bound when the processor spends more time fetching/storing data than performing arithmetic. This is common when:</p>\n\n        <ul>\n          <li>The layer has small arithmetic intensity, such as pointwise operations or small matrix-vector products.</li>\n          <li>Batch sizes are small, reducing the amount of computation per data load.</li>\n          <li>Model parameters or activations exceed on-chip cache capacity, forcing frequent DRAM access.</li>\n        </ul>\n      </li>\n      <li>\n        <p>Memory-bound operations are limited by memory bandwidth and latency rather than raw compute throughput. Here, quantization helps by:</p>\n\n        <ul>\n          <li><strong>Reducing memory footprint:</strong> Lower precision reduces the size of weights and activations (e.g., <code class=\"language-plaintext highlighter-rouge\">float32</code> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-361\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-362\"><span class=\"mo\" id=\"MathJax-Span-363\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">int8</code> cuts memory use by 75%).</li>\n          <li><strong>Improving cache locality:</strong> More parameters fit in L1/L2 cache or shared memory, reducing expensive DRAM fetches.</li>\n          <li><strong>Increasing effective bandwidth:</strong> Smaller data transfers mean more elements can be moved per memory transaction.</li>\n        </ul>\n      </li>\n      <li>\n        <p>On many edge devices, memory-bound layers see the largest relative speedups from quantization because external DRAM bandwidth is a critical bottleneck.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Mixed Bottleneck Scenarios</strong>:</p>\n\n    <ul>\n      <li>\n        <p>Many real-world models contain both compute-bound and memory-bound regions:</p>\n\n        <ul>\n          <li>Early convolution layers in vision models often run close to peak compute throughput, benefiting most from Tensor Core–accelerated low-precision compute.</li>\n          <li>Later layers with smaller spatial dimensions but large channel counts may become memory-bound, benefiting more from reduced memory bandwidth pressure than raw FLOP gains.</li>\n          <li>Transformer feed-forward layers can be compute-bound, while embedding lookups or normalization layers can be memory-bound.</li>\n        </ul>\n      </li>\n      <li>\n        <p>In such cases, <strong>mixed-precision quantization</strong> can optimize each region separately—keeping sensitive, low-intensity operations in higher precision while aggressively quantizing compute-heavy layers.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Where Quantization Delivers the Most Impact</strong>:</p>\n\n    <ul>\n      <li>\n        <p>Quantization is most impactful when:</p>\n\n        <ul>\n          <li>The model runs on hardware with specialized low-precision units (Tensor Cores, <code class=\"language-plaintext highlighter-rouge\">int8</code> MAC units, <code class=\"language-plaintext highlighter-rouge\">float8</code> engines).</li>\n          <li>Memory bandwidth is a limiting factor (common in mobile SoCs, edge AI chips, or when serving many inference requests in parallel).</li>\n          <li>Model size exceeds cache capacity, leading to frequent DRAM access.</li>\n          <li>Deployment constraints demand both latency and memory footprint reductions (e.g., real-time inference on embedded systems).</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>In summary, quantization addresses <strong>compute bottlenecks</strong> by enabling more operations per cycle and <strong>memory bottlenecks</strong> by reducing data transfer volume and improving cache utilization. Understanding which bottleneck dominates for a given layer or model is key to selecting the right quantization strategy.</p>\n  </li>\n</ul>\n<p>Deep learning performance is typically constrained by one of two primary bottlenecks: compute (arithmetic throughput) or memory (bandwidth and capacity). The balance between them depends on the hardware architecture, the model’s structure, and the numerical precision used.</p>\n<p><strong>Compute-Bound Workloads</strong>:</p>\n<ul>\n      <li>\n        <p>A workload is compute-bound when the GPU/CPU spends most of its time performing arithmetic operations rather than waiting for data from memory. This is common in:</p>\n\n        <ul>\n          <li>Large matrix multiplications with high arithmetic intensity (high FLOPs-to-bytes ratio).</li>\n          <li>Dense layers and convolution layers with large channel counts and large kernel sizes.</li>\n          <li>Transformer attention mechanisms with large batch sizes or long sequence lengths.</li>\n        </ul>\n      </li>\n      <li>\n        <p>In compute-bound scenarios, lowering the precision of operands (e.g., <code class=\"language-plaintext highlighter-rouge\">float32</code> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-358\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-359\"><span class=\"mo\" id=\"MathJax-Span-360\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">int8</code>) allows hardware to execute more operations per clock cycle. For example:</p>\n\n        <ul>\n          <li>NVIDIA Tensor Cores can deliver up to 2×–4× the throughput for <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> GEMMs compared to <code class=\"language-plaintext highlighter-rouge\">float32</code>.</li>\n          <li>Integer accelerators (e.g., <code class=\"language-plaintext highlighter-rouge\">int8</code> SIMD or systolic arrays) can achieve even higher gains, especially on CPUs or edge NPUs.</li>\n        </ul>\n      </li>\n      <li>\n        <p>By reducing the number of bits per operand, quantization directly increases the number of multiply-accumulate operations that can be executed in parallel within the same silicon area and clock period.</p>\n      </li>\n    </ul>\n<p>A workload is compute-bound when the GPU/CPU spends most of its time performing arithmetic operations rather than waiting for data from memory. This is common in:</p>\n<ul>\n          <li>Large matrix multiplications with high arithmetic intensity (high FLOPs-to-bytes ratio).</li>\n          <li>Dense layers and convolution layers with large channel counts and large kernel sizes.</li>\n          <li>Transformer attention mechanisms with large batch sizes or long sequence lengths.</li>\n        </ul>\n<p>In compute-bound scenarios, lowering the precision of operands (e.g., <code class=\"language-plaintext highlighter-rouge\">float32</code> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-358\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-359\"><span class=\"mo\" id=\"MathJax-Span-360\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">int8</code>) allows hardware to execute more operations per clock cycle. For example:</p>\n<ul>\n          <li>NVIDIA Tensor Cores can deliver up to 2×–4× the throughput for <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> GEMMs compared to <code class=\"language-plaintext highlighter-rouge\">float32</code>.</li>\n          <li>Integer accelerators (e.g., <code class=\"language-plaintext highlighter-rouge\">int8</code> SIMD or systolic arrays) can achieve even higher gains, especially on CPUs or edge NPUs.</li>\n        </ul>\n<p>By reducing the number of bits per operand, quantization directly increases the number of multiply-accumulate operations that can be executed in parallel within the same silicon area and clock period.</p>\n<p><strong>Memory-Bound Workloads</strong>:</p>\n<ul>\n      <li>\n        <p>A workload is memory-bound when the processor spends more time fetching/storing data than performing arithmetic. This is common when:</p>\n\n        <ul>\n          <li>The layer has small arithmetic intensity, such as pointwise operations or small matrix-vector products.</li>\n          <li>Batch sizes are small, reducing the amount of computation per data load.</li>\n          <li>Model parameters or activations exceed on-chip cache capacity, forcing frequent DRAM access.</li>\n        </ul>\n      </li>\n      <li>\n        <p>Memory-bound operations are limited by memory bandwidth and latency rather than raw compute throughput. Here, quantization helps by:</p>\n\n        <ul>\n          <li><strong>Reducing memory footprint:</strong> Lower precision reduces the size of weights and activations (e.g., <code class=\"language-plaintext highlighter-rouge\">float32</code> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-361\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-362\"><span class=\"mo\" id=\"MathJax-Span-363\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">int8</code> cuts memory use by 75%).</li>\n          <li><strong>Improving cache locality:</strong> More parameters fit in L1/L2 cache or shared memory, reducing expensive DRAM fetches.</li>\n          <li><strong>Increasing effective bandwidth:</strong> Smaller data transfers mean more elements can be moved per memory transaction.</li>\n        </ul>\n      </li>\n      <li>\n        <p>On many edge devices, memory-bound layers see the largest relative speedups from quantization because external DRAM bandwidth is a critical bottleneck.</p>\n      </li>\n    </ul>\n<p>A workload is memory-bound when the processor spends more time fetching/storing data than performing arithmetic. This is common when:</p>\n<ul>\n          <li>The layer has small arithmetic intensity, such as pointwise operations or small matrix-vector products.</li>\n          <li>Batch sizes are small, reducing the amount of computation per data load.</li>\n          <li>Model parameters or activations exceed on-chip cache capacity, forcing frequent DRAM access.</li>\n        </ul>\n<p>Memory-bound operations are limited by memory bandwidth and latency rather than raw compute throughput. Here, quantization helps by:</p>\n<ul>\n          <li><strong>Reducing memory footprint:</strong> Lower precision reduces the size of weights and activations (e.g., <code class=\"language-plaintext highlighter-rouge\">float32</code> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-361\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-362\"><span class=\"mo\" id=\"MathJax-Span-363\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">int8</code> cuts memory use by 75%).</li>\n          <li><strong>Improving cache locality:</strong> More parameters fit in L1/L2 cache or shared memory, reducing expensive DRAM fetches.</li>\n          <li><strong>Increasing effective bandwidth:</strong> Smaller data transfers mean more elements can be moved per memory transaction.</li>\n        </ul>\n<p>On many edge devices, memory-bound layers see the largest relative speedups from quantization because external DRAM bandwidth is a critical bottleneck.</p>\n<p><strong>Mixed Bottleneck Scenarios</strong>:</p>\n<ul>\n      <li>\n        <p>Many real-world models contain both compute-bound and memory-bound regions:</p>\n\n        <ul>\n          <li>Early convolution layers in vision models often run close to peak compute throughput, benefiting most from Tensor Core–accelerated low-precision compute.</li>\n          <li>Later layers with smaller spatial dimensions but large channel counts may become memory-bound, benefiting more from reduced memory bandwidth pressure than raw FLOP gains.</li>\n          <li>Transformer feed-forward layers can be compute-bound, while embedding lookups or normalization layers can be memory-bound.</li>\n        </ul>\n      </li>\n      <li>\n        <p>In such cases, <strong>mixed-precision quantization</strong> can optimize each region separately—keeping sensitive, low-intensity operations in higher precision while aggressively quantizing compute-heavy layers.</p>\n      </li>\n    </ul>\n<p>Many real-world models contain both compute-bound and memory-bound regions:</p>\n<ul>\n          <li>Early convolution layers in vision models often run close to peak compute throughput, benefiting most from Tensor Core–accelerated low-precision compute.</li>\n          <li>Later layers with smaller spatial dimensions but large channel counts may become memory-bound, benefiting more from reduced memory bandwidth pressure than raw FLOP gains.</li>\n          <li>Transformer feed-forward layers can be compute-bound, while embedding lookups or normalization layers can be memory-bound.</li>\n        </ul>\n<p>In such cases, <strong>mixed-precision quantization</strong> can optimize each region separately—keeping sensitive, low-intensity operations in higher precision while aggressively quantizing compute-heavy layers.</p>\n<p><strong>Where Quantization Delivers the Most Impact</strong>:</p>\n<ul>\n      <li>\n        <p>Quantization is most impactful when:</p>\n\n        <ul>\n          <li>The model runs on hardware with specialized low-precision units (Tensor Cores, <code class=\"language-plaintext highlighter-rouge\">int8</code> MAC units, <code class=\"language-plaintext highlighter-rouge\">float8</code> engines).</li>\n          <li>Memory bandwidth is a limiting factor (common in mobile SoCs, edge AI chips, or when serving many inference requests in parallel).</li>\n          <li>Model size exceeds cache capacity, leading to frequent DRAM access.</li>\n          <li>Deployment constraints demand both latency and memory footprint reductions (e.g., real-time inference on embedded systems).</li>\n        </ul>\n      </li>\n    </ul>\n<p>Quantization is most impactful when:</p>\n<ul>\n          <li>The model runs on hardware with specialized low-precision units (Tensor Cores, <code class=\"language-plaintext highlighter-rouge\">int8</code> MAC units, <code class=\"language-plaintext highlighter-rouge\">float8</code> engines).</li>\n          <li>Memory bandwidth is a limiting factor (common in mobile SoCs, edge AI chips, or when serving many inference requests in parallel).</li>\n          <li>Model size exceeds cache capacity, leading to frequent DRAM access.</li>\n          <li>Deployment constraints demand both latency and memory footprint reductions (e.g., real-time inference on embedded systems).</li>\n        </ul>\n<p>In summary, quantization addresses <strong>compute bottlenecks</strong> by enabling more operations per cycle and <strong>memory bottlenecks</strong> by reducing data transfer volume and improving cache utilization. Understanding which bottleneck dominates for a given layer or model is key to selecting the right quantization strategy.</p>",
      "contentMarkdown": "*   Deep learning performance is typically constrained by one of two primary bottlenecks: compute (arithmetic throughput) or memory (bandwidth and capacity). The balance between them depends on the hardware architecture, the model’s structure, and the numerical precision used.\n    \n*   **Compute-Bound Workloads**:\n    \n    *   A workload is compute-bound when the GPU/CPU spends most of its time performing arithmetic operations rather than waiting for data from memory. This is common in:\n        \n        *   Large matrix multiplications with high arithmetic intensity (high FLOPs-to-bytes ratio).\n        *   Dense layers and convolution layers with large channel counts and large kernel sizes.\n        *   Transformer attention mechanisms with large batch sizes or long sequence lengths.\n    *   In compute-bound scenarios, lowering the precision of operands (e.g., `float32` →→\\\\rightarrow `float16` or `int8`) allows hardware to execute more operations per clock cycle. For example:\n        \n        *   NVIDIA Tensor Cores can deliver up to 2×–4× the throughput for `float16` or `bfloat16` GEMMs compared to `float32`.\n        *   Integer accelerators (e.g., `int8` SIMD or systolic arrays) can achieve even higher gains, especially on CPUs or edge NPUs.\n    *   By reducing the number of bits per operand, quantization directly increases the number of multiply-accumulate operations that can be executed in parallel within the same silicon area and clock period.\n        \n*   **Memory-Bound Workloads**:\n    \n    *   A workload is memory-bound when the processor spends more time fetching/storing data than performing arithmetic. This is common when:\n        \n        *   The layer has small arithmetic intensity, such as pointwise operations or small matrix-vector products.\n        *   Batch sizes are small, reducing the amount of computation per data load.\n        *   Model parameters or activations exceed on-chip cache capacity, forcing frequent DRAM access.\n    *   Memory-bound operations are limited by memory bandwidth and latency rather than raw compute throughput. Here, quantization helps by:\n        \n        *   **Reducing memory footprint:** Lower precision reduces the size of weights and activations (e.g., `float32` →→\\\\rightarrow `int8` cuts memory use by 75%).\n        *   **Improving cache locality:** More parameters fit in L1/L2 cache or shared memory, reducing expensive DRAM fetches.\n        *   **Increasing effective bandwidth:** Smaller data transfers mean more elements can be moved per memory transaction.\n    *   On many edge devices, memory-bound layers see the largest relative speedups from quantization because external DRAM bandwidth is a critical bottleneck.\n        \n*   **Mixed Bottleneck Scenarios**:\n    \n    *   Many real-world models contain both compute-bound and memory-bound regions:\n        \n        *   Early convolution layers in vision models often run close to peak compute throughput, benefiting most from Tensor Core–accelerated low-precision compute.\n        *   Later layers with smaller spatial dimensions but large channel counts may become memory-bound, benefiting more from reduced memory bandwidth pressure than raw FLOP gains.\n        *   Transformer feed-forward layers can be compute-bound, while embedding lookups or normalization layers can be memory-bound.\n    *   In such cases, **mixed-precision quantization** can optimize each region separately—keeping sensitive, low-intensity operations in higher precision while aggressively quantizing compute-heavy layers.\n        \n*   **Where Quantization Delivers the Most Impact**:\n    \n    *   Quantization is most impactful when:\n        \n        *   The model runs on hardware with specialized low-precision units (Tensor Cores, `int8` MAC units, `float8` engines).\n        *   Memory bandwidth is a limiting factor (common in mobile SoCs, edge AI chips, or when serving many inference requests in parallel).\n        *   Model size exceeds cache capacity, leading to frequent DRAM access.\n        *   Deployment constraints demand both latency and memory footprint reductions (e.g., real-time inference on embedded systems).\n*   In summary, quantization addresses **compute bottlenecks** by enabling more operations per cycle and **memory bottlenecks** by reducing data transfer volume and improving cache utilization. Understanding which bottleneck dominates for a given layer or model is key to selecting the right quantization strategy.\n    \n\nDeep learning performance is typically constrained by one of two primary bottlenecks: compute (arithmetic throughput) or memory (bandwidth and capacity). The balance between them depends on the hardware architecture, the model’s structure, and the numerical precision used.\n\n**Compute-Bound Workloads**:\n\n*   A workload is compute-bound when the GPU/CPU spends most of its time performing arithmetic operations rather than waiting for data from memory. This is common in:\n    \n    *   Large matrix multiplications with high arithmetic intensity (high FLOPs-to-bytes ratio).\n    *   Dense layers and convolution layers with large channel counts and large kernel sizes.\n    *   Transformer attention mechanisms with large batch sizes or long sequence lengths.\n*   In compute-bound scenarios, lowering the precision of operands (e.g., `float32` →→\\\\rightarrow `float16` or `int8`) allows hardware to execute more operations per clock cycle. For example:\n    \n    *   NVIDIA Tensor Cores can deliver up to 2×–4× the throughput for `float16` or `bfloat16` GEMMs compared to `float32`.\n    *   Integer accelerators (e.g., `int8` SIMD or systolic arrays) can achieve even higher gains, especially on CPUs or edge NPUs.\n*   By reducing the number of bits per operand, quantization directly increases the number of multiply-accumulate operations that can be executed in parallel within the same silicon area and clock period.\n    \n\nA workload is compute-bound when the GPU/CPU spends most of its time performing arithmetic operations rather than waiting for data from memory. This is common in:\n\n*   Large matrix multiplications with high arithmetic intensity (high FLOPs-to-bytes ratio).\n*   Dense layers and convolution layers with large channel counts and large kernel sizes.\n*   Transformer attention mechanisms with large batch sizes or long sequence lengths.\n\nIn compute-bound scenarios, lowering the precision of operands (e.g., `float32` →→\\\\rightarrow `float16` or `int8`) allows hardware to execute more operations per clock cycle. For example:\n\n*   NVIDIA Tensor Cores can deliver up to 2×–4× the throughput for `float16` or `bfloat16` GEMMs compared to `float32`.\n*   Integer accelerators (e.g., `int8` SIMD or systolic arrays) can achieve even higher gains, especially on CPUs or edge NPUs.\n\nBy reducing the number of bits per operand, quantization directly increases the number of multiply-accumulate operations that can be executed in parallel within the same silicon area and clock period.\n\n**Memory-Bound Workloads**:\n\n*   A workload is memory-bound when the processor spends more time fetching/storing data than performing arithmetic. This is common when:\n    \n    *   The layer has small arithmetic intensity, such as pointwise operations or small matrix-vector products.\n    *   Batch sizes are small, reducing the amount of computation per data load.\n    *   Model parameters or activations exceed on-chip cache capacity, forcing frequent DRAM access.\n*   Memory-bound operations are limited by memory bandwidth and latency rather than raw compute throughput. Here, quantization helps by:\n    \n    *   **Reducing memory footprint:** Lower precision reduces the size of weights and activations (e.g., `float32` →→\\\\rightarrow `int8` cuts memory use by 75%).\n    *   **Improving cache locality:** More parameters fit in L1/L2 cache or shared memory, reducing expensive DRAM fetches.\n    *   **Increasing effective bandwidth:** Smaller data transfers mean more elements can be moved per memory transaction.\n*   On many edge devices, memory-bound layers see the largest relative speedups from quantization because external DRAM bandwidth is a critical bottleneck.\n    \n\nA workload is memory-bound when the processor spends more time fetching/storing data than performing arithmetic. This is common when:\n\n*   The layer has small arithmetic intensity, such as pointwise operations or small matrix-vector products.\n*   Batch sizes are small, reducing the amount of computation per data load.\n*   Model parameters or activations exceed on-chip cache capacity, forcing frequent DRAM access.\n\nMemory-bound operations are limited by memory bandwidth and latency rather than raw compute throughput. Here, quantization helps by:\n\n*   **Reducing memory footprint:** Lower precision reduces the size of weights and activations (e.g., `float32` →→\\\\rightarrow `int8` cuts memory use by 75%).\n*   **Improving cache locality:** More parameters fit in L1/L2 cache or shared memory, reducing expensive DRAM fetches.\n*   **Increasing effective bandwidth:** Smaller data transfers mean more elements can be moved per memory transaction.\n\nOn many edge devices, memory-bound layers see the largest relative speedups from quantization because external DRAM bandwidth is a critical bottleneck.\n\n**Mixed Bottleneck Scenarios**:\n\n*   Many real-world models contain both compute-bound and memory-bound regions:\n    \n    *   Early convolution layers in vision models often run close to peak compute throughput, benefiting most from Tensor Core–accelerated low-precision compute.\n    *   Later layers with smaller spatial dimensions but large channel counts may become memory-bound, benefiting more from reduced memory bandwidth pressure than raw FLOP gains.\n    *   Transformer feed-forward layers can be compute-bound, while embedding lookups or normalization layers can be memory-bound.\n*   In such cases, **mixed-precision quantization** can optimize each region separately—keeping sensitive, low-intensity operations in higher precision while aggressively quantizing compute-heavy layers.\n    \n\nMany real-world models contain both compute-bound and memory-bound regions:\n\n*   Early convolution layers in vision models often run close to peak compute throughput, benefiting most from Tensor Core–accelerated low-precision compute.\n*   Later layers with smaller spatial dimensions but large channel counts may become memory-bound, benefiting more from reduced memory bandwidth pressure than raw FLOP gains.\n*   Transformer feed-forward layers can be compute-bound, while embedding lookups or normalization layers can be memory-bound.\n\nIn such cases, **mixed-precision quantization** can optimize each region separately—keeping sensitive, low-intensity operations in higher precision while aggressively quantizing compute-heavy layers.\n\n**Where Quantization Delivers the Most Impact**:\n\n*   Quantization is most impactful when:\n    \n    *   The model runs on hardware with specialized low-precision units (Tensor Cores, `int8` MAC units, `float8` engines).\n    *   Memory bandwidth is a limiting factor (common in mobile SoCs, edge AI chips, or when serving many inference requests in parallel).\n    *   Model size exceeds cache capacity, leading to frequent DRAM access.\n    *   Deployment constraints demand both latency and memory footprint reductions (e.g., real-time inference on embedded systems).\n\nQuantization is most impactful when:\n\n*   The model runs on hardware with specialized low-precision units (Tensor Cores, `int8` MAC units, `float8` engines).\n*   Memory bandwidth is a limiting factor (common in mobile SoCs, edge AI chips, or when serving many inference requests in parallel).\n*   Model size exceeds cache capacity, leading to frequent DRAM access.\n*   Deployment constraints demand both latency and memory footprint reductions (e.g., real-time inference on embedded systems).\n\nIn summary, quantization addresses **compute bottlenecks** by enabling more operations per cycle and **memory bottlenecks** by reducing data transfer volume and improving cache utilization. Understanding which bottleneck dominates for a given layer or model is key to selecting the right quantization strategy.",
      "order": 12,
      "orderInChapter": 12,
      "difficulty": 4,
      "estimatedMinutes": 9,
      "tags": [
        "ondevice ai",
        "deep learning",
        "transformer",
        "attention",
        "embedding",
        "convolution",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 1630,
        "contentLength": 23532
      },
      "nextCards": [
        "ai-model-compression-modern-quantization-techniques-13",
        "ai-model-compression-multimodal-quantization-14"
      ],
      "relatedCards": [
        "ai-on-device-transformers-modelembedding-dimension-23",
        "ai-on-device-transformers-balancing-compute-and-memory-prefill-vs-decode-opt-8",
        "ai-on-device-transformers-tokenizer-and-vocabulary-size-22",
        "ai-on-device-transformers-parameter-count-and-model-depth-25",
        "ai-on-device-transformers-decoder-memory-bound-nature-2"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#compute-vs.-memory-bottlenecks",
      "scrapedAt": "2025-12-28T11:55:50.968Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-modern-quantization-techniques-13",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Modern Quantization Techniques",
      "subtitle": "Quantization",
      "contentHtml": "<h4 id=\"gptq-quantization-with-second-order-error-compensation\">GPTQ: Quantization with Second-Order Error Compensation</h4>\n<ul>\n  <li>\n    <p>Introduced in <a href=\"https://arxiv.org/abs/2210.17323\">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a> by Frantar et al. (2023), GPTQ is a high-accuracy post-training quantization (PTQ) method tailored for large-scale transformers. Unlike round-to-nearest schemes, GPTQ minimizes the quantization error using approximate second-order information derived from the Hessian of the loss. It quantizes weights in a layer-wise fashion while updating unquantized weights to compensate for introduced error, achieving efficient <code class=\"language-plaintext highlighter-rouge\">int3/4</code> quantization of models as large as OPT-175B or BLOOM-176B without finetuning. Practical implementations are available through <a href=\"https://github.com/PanQiWei/AutoGPTQ\">AutoGPTQ</a> and <a href=\"https://github.com/IST-DASLab/gptq\">LLM.int4</a>.</p>\n  </li>\n  <li>\n    <p>GPTQ significantly outperforms simple rounding methods by preserving perplexity under low-bit quantization. Notably, it is one of the few techniques that scales to 100B+ parameter models using modest compute (e.g., a single A100 GPU). While it focuses on weight-only quantization, activation quantization can be layered on top via orthogonal techniques such as SmoothQuant.</p>\n  </li>\n</ul>\n<p>Introduced in <a href=\"https://arxiv.org/abs/2210.17323\">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a> by Frantar et al. (2023), GPTQ is a high-accuracy post-training quantization (PTQ) method tailored for large-scale transformers. Unlike round-to-nearest schemes, GPTQ minimizes the quantization error using approximate second-order information derived from the Hessian of the loss. It quantizes weights in a layer-wise fashion while updating unquantized weights to compensate for introduced error, achieving efficient <code class=\"language-plaintext highlighter-rouge\">int3/4</code> quantization of models as large as OPT-175B or BLOOM-176B without finetuning. Practical implementations are available through <a href=\"https://github.com/PanQiWei/AutoGPTQ\">AutoGPTQ</a> and <a href=\"https://github.com/IST-DASLab/gptq\">LLM.int4</a>.</p>\n<p>GPTQ significantly outperforms simple rounding methods by preserving perplexity under low-bit quantization. Notably, it is one of the few techniques that scales to 100B+ parameter models using modest compute (e.g., a single A100 GPU). While it focuses on weight-only quantization, activation quantization can be layered on top via orthogonal techniques such as SmoothQuant.</p>\n<h5 id=\"process\">Process</h5>\n<ol>\n  <li>\n    <p><strong>Layer-Wise Quantization Objective</strong></p>\n\n    <ul>\n      <li>\n        <p>For a linear layer with weight matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-364\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-365\"><span class=\"mi\" id=\"MathJax-Span-366\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-41\">W</script> and input activations <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-367\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-368\"><span class=\"mi\" id=\"MathJax-Span-369\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-42\">X</script>, GPTQ minimizes the reconstruction error after quantization:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><munder><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>W</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow></mrow></munder><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>W</mi><mi>X</mi><mo>&amp;#x2212;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>W</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mi>X</mi><msubsup><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mn>2</mn><mn>2</mn></msubsup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-370\" style=\"width: 9.065em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.555em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1007.55em, 3.336em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-371\"><span class=\"munderover\" id=\"MathJax-Span-372\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.57em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-373\" style=\"font-family: STIXGeneral-Regular;\">min</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.128em, 1000.63em, 4.273em, -999.997em); top: -3.122em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-374\"><span class=\"mrow\" id=\"MathJax-Span-375\"><span class=\"texatom\" id=\"MathJax-Span-376\"><span class=\"mrow\" id=\"MathJax-Span-377\"><span class=\"munderover\" id=\"MathJax-Span-378\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-379\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.216em; left: 0.263em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.263em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-380\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.154em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-381\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">‖</span><span class=\"mi\" id=\"MathJax-Span-382\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-383\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-384\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"texatom\" id=\"MathJax-Span-385\" style=\"padding-left: 0.263em;\"><span class=\"mrow\" id=\"MathJax-Span-386\"><span class=\"munderover\" id=\"MathJax-Span-387\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-388\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.367em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-389\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-390\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-391\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-392\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-393\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-394\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.247em; border-left: 0px solid; width: 0px; height: 2.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><munder><mo movablelimits=\"true\" form=\"prefix\">min</mo><mrow class=\"MJX-TeXAtom-ORD\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>W</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow></munder><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>W</mi><mi>X</mi><mo>−</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>W</mi><mo stretchy=\"false\">^</mo></mover></mrow><mi>X</mi><msubsup><mo fence=\"false\" stretchy=\"false\">‖</mo><mn>2</mn><mn>2</mn></msubsup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-43\">\\min_{\\hat{W}} \\| WX - \\hat{W}X \\|_2^2</script>\n      </li>\n      <li>\n        <p>Quantization is performed column-by-column (i.e., per weight vector), and compensation is applied to unquantized weights to preserve the overall output fidelity.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Approximate Second-Order Weight Selection (OBQ Foundation)</strong></p>\n\n    <ul>\n      <li>\n        <p>GPTQ builds on the Optimal Brain Quantization (OBQ) framework, which selects the next weight <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-395\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.1em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-396\"><span class=\"msubsup\" id=\"MathJax-Span-397\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-398\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-399\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">w_q</script> to quantize by minimizing its induced error, scaled by its Hessian diagonal element:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>w</mi><mi>q</mi></msub><mo>=</mo><mi>arg</mi><mo>&amp;#x2061;</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mfrac><mrow><mo stretchy=&quot;false&quot;>(</mo><mtext>quant</mtext><mo stretchy=&quot;false&quot;>(</mo><msub><mi>w</mi><mi>q</mi></msub><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><msub><mi>w</mi><mi>q</mi></msub><msup><mo stretchy=&quot;false&quot;>)</mo><mn>2</mn></msup></mrow><mrow><mo stretchy=&quot;false&quot;>[</mo><msup><mi>H</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><msub><mo stretchy=&quot;false&quot;>]</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>q</mi><mi>q</mi></mrow></msub></mrow></mfrac><mo>,</mo><mspace width=&quot;1em&quot; /><mi>&amp;#x03B4;</mi><mo>=</mo><mo>&amp;#x2212;</mo><mfrac><mrow><msub><mi>w</mi><mi>q</mi></msub><mo>&amp;#x2212;</mo><mtext>quant</mtext><mo stretchy=&quot;false&quot;>(</mo><msub><mi>w</mi><mi>q</mi></msub><mo stretchy=&quot;false&quot;>)</mo></mrow><mrow><mo stretchy=&quot;false&quot;>[</mo><msup><mi>H</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><msub><mo stretchy=&quot;false&quot;>]</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>q</mi><mi>q</mi></mrow></msub></mrow></mfrac><mo stretchy=&quot;false&quot;>(</mo><msup><mi>H</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><msub><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>:</mo><mo>,</mo><mi>q</mi></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-400\" style=\"width: 32.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 27.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.419em, 1027.03em, 3.388em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-401\"><span class=\"msubsup\" id=\"MathJax-Span-402\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-403\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-404\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-405\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-406\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">arg</span><span class=\"mo\" id=\"MathJax-Span-407\"></span><span class=\"mo\" id=\"MathJax-Span-408\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">min</span><span class=\"mfrac\" id=\"MathJax-Span-409\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 7.503em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.023em, 1007.4em, 4.482em, -999.997em); top: -4.789em; left: 50%; margin-left: -3.695em;\"><span class=\"mrow\" id=\"MathJax-Span-410\"><span class=\"mo\" id=\"MathJax-Span-411\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mtext\" id=\"MathJax-Span-412\" style=\"font-family: STIXGeneral-Regular;\">quant</span><span class=\"mo\" id=\"MathJax-Span-413\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-414\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-415\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-416\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-417\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-418\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-419\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-420\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-421\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-422\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-423\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.315em;\"><span class=\"mn\" id=\"MathJax-Span-424\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.076em, 1003.18em, 4.482em, -999.997em); top: -3.279em; left: 50%; margin-left: -1.612em;\"><span class=\"mrow\" id=\"MathJax-Span-425\"><span class=\"mo\" id=\"MathJax-Span-426\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"msubsup\" id=\"MathJax-Span-427\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-428\" style=\"font-family: STIXGeneral-Italic;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-429\"><span class=\"mrow\" id=\"MathJax-Span-430\"><span class=\"mo\" id=\"MathJax-Span-431\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-432\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-433\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-434\" style=\"font-family: STIXGeneral-Regular;\">]</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-435\"><span class=\"mrow\" id=\"MathJax-Span-436\"><span class=\"mi\" id=\"MathJax-Span-437\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span class=\"mi\" id=\"MathJax-Span-438\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1007.5em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 7.503em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-439\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-440\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-441\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">δ</span><span class=\"mo\" id=\"MathJax-Span-442\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-443\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"mfrac\" id=\"MathJax-Span-444\"><span style=\"display: inline-block; position: relative; width: 6.409em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1006.25em, 4.482em, -999.997em); top: -4.789em; left: 50%; margin-left: -3.122em;\"><span class=\"mrow\" id=\"MathJax-Span-445\"><span class=\"msubsup\" id=\"MathJax-Span-446\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-447\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-448\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-449\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mtext\" id=\"MathJax-Span-450\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">quant</span><span class=\"mo\" id=\"MathJax-Span-451\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-452\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-453\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-454\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-455\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.076em, 1003.18em, 4.482em, -999.997em); top: -3.279em; left: 50%; margin-left: -1.612em;\"><span class=\"mrow\" id=\"MathJax-Span-456\"><span class=\"mo\" id=\"MathJax-Span-457\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"msubsup\" id=\"MathJax-Span-458\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-459\" style=\"font-family: STIXGeneral-Italic;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-460\"><span class=\"mrow\" id=\"MathJax-Span-461\"><span class=\"mo\" id=\"MathJax-Span-462\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-463\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-464\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-465\" style=\"font-family: STIXGeneral-Regular;\">]</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-466\"><span class=\"mrow\" id=\"MathJax-Span-467\"><span class=\"mi\" id=\"MathJax-Span-468\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span class=\"mi\" id=\"MathJax-Span-469\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1006.41em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 6.409em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-470\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-471\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-472\" style=\"font-family: STIXGeneral-Italic;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-473\"><span class=\"mrow\" id=\"MathJax-Span-474\"><span class=\"mo\" id=\"MathJax-Span-475\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-476\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-477\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-478\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-479\"><span class=\"mrow\" id=\"MathJax-Span-480\"><span class=\"mo\" id=\"MathJax-Span-481\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mo\" id=\"MathJax-Span-482\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-483\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.309em; border-left: 0px solid; width: 0px; height: 3.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>w</mi><mi>q</mi></msub><mo>=</mo><mi>arg</mi><mo>⁡</mo><mo movablelimits=\"true\" form=\"prefix\">min</mo><mfrac><mrow><mo stretchy=\"false\">(</mo><mtext>quant</mtext><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>q</mi></msub><mo stretchy=\"false\">)</mo><mo>−</mo><msub><mi>w</mi><mi>q</mi></msub><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup></mrow><mrow><mo stretchy=\"false\">[</mo><msup><mi>H</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup><msub><mo stretchy=\"false\">]</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>q</mi><mi>q</mi></mrow></msub></mrow></mfrac><mo>,</mo><mspace width=\"1em\"></mspace><mi>δ</mi><mo>=</mo><mo>−</mo><mfrac><mrow><msub><mi>w</mi><mi>q</mi></msub><mo>−</mo><mtext>quant</mtext><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>q</mi></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mo stretchy=\"false\">[</mo><msup><mi>H</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup><msub><mo stretchy=\"false\">]</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>q</mi><mi>q</mi></mrow></msub></mrow></mfrac><mo stretchy=\"false\">(</mo><msup><mi>H</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup><msub><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo>:</mo><mo>,</mo><mi>q</mi></mrow></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-45\">w_q = \\arg\\min \\frac{(\\text{quant}(w_q) - w_q)^2}{[H^{-1}]_{qq}}, \\quad \\delta = -\\frac{w_q - \\text{quant}(w_q)}{[H^{-1}]_{qq}} (H^{-1})_{:,q}</script>\n      </li>\n      <li>\n        <p>The inverse Hessian <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>H</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mn>2</mn><mi>X</mi><msup><mi>X</mi><mi mathvariant=&quot;normal&quot;>&amp;#x22A4;</mi></msup><mo>+</mo><mi>&amp;#x03BB;</mi><mi>I</mi><msup><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-484\" style=\"width: 10.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1009.12em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-485\"><span class=\"msubsup\" id=\"MathJax-Span-486\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-487\" style=\"font-family: STIXGeneral-Italic;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-488\"><span class=\"mrow\" id=\"MathJax-Span-489\"><span class=\"mo\" id=\"MathJax-Span-490\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-491\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-492\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-493\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mn\" id=\"MathJax-Span-494\" style=\"font-family: STIXGeneral-Regular;\">2</span><span class=\"mi\" id=\"MathJax-Span-495\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-496\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-497\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-498\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⊤</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-499\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-500\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">λ</span><span class=\"mi\" id=\"MathJax-Span-501\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-502\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-503\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-504\"><span class=\"mrow\" id=\"MathJax-Span-505\"><span class=\"mo\" id=\"MathJax-Span-506\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-507\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>H</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mo stretchy=\"false\">(</mo><mn>2</mn><mi>X</mi><msup><mi>X</mi><mi mathvariant=\"normal\">⊤</mi></msup><mo>+</mo><mi>λ</mi><mi>I</mi><msup><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">H^{-1} = (2X X^\\top + \\lambda I)^{-1}</script> captures sensitivity of the layer outputs to changes in weights. This allows for compensation of quantization-induced error by adjusting the remaining unquantized weights.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Blockwise Column Quantization with Shared Hessian</strong></p>\n\n    <ul>\n      <li>\n        <p>GPTQ introduces the insight that, in large layers, quantizing all rows in a fixed column order yields nearly the same accuracy as a greedy per-weight order.</p>\n      </li>\n      <li>\n        <p>This allows sharing the Hessian across rows and amortizing its computation—resulting in a complexity reduction from <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>row</mtext></mrow></msub><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>col</mtext></mrow><mn>3</mn></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-508\" style=\"width: 6.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1005.42em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-509\"><span class=\"mi\" id=\"MathJax-Span-510\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-511\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-512\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-513\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-514\"><span class=\"mrow\" id=\"MathJax-Span-515\"><span class=\"mtext\" id=\"MathJax-Span-516\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">row</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-517\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-518\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-519\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-520\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-521\"><span class=\"mrow\" id=\"MathJax-Span-522\"><span class=\"mtext\" id=\"MathJax-Span-523\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">col</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-524\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>row</mtext></mrow></msub><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>col</mtext></mrow><mn>3</mn></msubsup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">O(d_{\\text{row}} \\cdot d_{\\text{col}}^3)</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>row</mtext></mrow></msub><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>col</mtext></mrow><mn>2</mn></msubsup><mo>,</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>col</mtext></mrow><mn>3</mn></msubsup><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-525\" style=\"width: 11.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.794em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1009.74em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-526\"><span class=\"mi\" id=\"MathJax-Span-527\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-528\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mo\" id=\"MathJax-Span-529\" style=\"font-family: STIXGeneral-Regular;\">max</span><span class=\"mo\" id=\"MathJax-Span-530\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-531\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-532\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-533\"><span class=\"mrow\" id=\"MathJax-Span-534\"><span class=\"mtext\" id=\"MathJax-Span-535\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">row</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-536\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-537\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-538\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-539\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-540\"><span class=\"mrow\" id=\"MathJax-Span-541\"><span class=\"mtext\" id=\"MathJax-Span-542\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">col</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-543\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-544\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-545\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-546\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-547\"><span class=\"mrow\" id=\"MathJax-Span-548\"><span class=\"mtext\" id=\"MathJax-Span-549\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">col</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-550\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-551\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mo movablelimits=\"true\" form=\"prefix\">max</mo><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>row</mtext></mrow></msub><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>col</mtext></mrow><mn>2</mn></msubsup><mo>,</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>col</mtext></mrow><mn>3</mn></msubsup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">O(\\max(d_{\\text{row}} \\cdot d_{\\text{col}}^2, d_{\\text{col}}^3))</script>.</p>\n      </li>\n      <li>\n        <p>The following figure (<a href=\"https://arxiv.org/abs/2210.17323\">source</a>) illustrates the GPTQ quantization procedure. Blocks of consecutive columns (bolded) are quantized at a given step, using the inverse Hessian information stored in the Cholesky decomposition, and the remaining weights (blue) are updated at the end of the step. The quantization procedure is applied recursively inside each block: the white middle column is currently being quantized.</p>\n      </li>\n    </ul>\n\n    <p><img src=\"../assets/model-compression/GPTQ.jpg\" alt=\"GPTQ Figure 2 - Quantization Procedure\"></p>\n  </li>\n  <li>\n    <p><strong>Lazy Batch Updates for GPU Efficiency</strong></p>\n\n    <ul>\n      <li>To alleviate the memory-bandwidth bottleneck of GPU kernels, GPTQ processes batches of columns (e.g., 128) before updating the full weight matrix.</li>\n      <li>This “lazy update” scheme postpones weight and Hessian modifications until a full block has been processed, improving throughput and parallelism.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cholesky-Based Inversion for Numerical Stability</strong></p>\n\n    <ul>\n      <li>\n        <p>To avoid numerical instability from repeatedly inverting Hessians during block updates, GPTQ reformulates the update rule using a Cholesky decomposition:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>H</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><mo>=</mo><mi>L</mi><msup><mi>L</mi><mi mathvariant=&quot;normal&quot;>&amp;#x22A4;</mi></msup><mo>,</mo><mspace width=&quot;1em&quot; /><mtext>computed once per block</mtext></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-552\" style=\"width: 19.69em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 16.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1016.41em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-553\"><span class=\"msubsup\" id=\"MathJax-Span-554\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-555\" style=\"font-family: STIXGeneral-Italic;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-556\"><span class=\"mrow\" id=\"MathJax-Span-557\"><span class=\"mo\" id=\"MathJax-Span-558\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-559\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-560\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-561\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-562\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-563\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-564\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⊤</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-565\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-566\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mtext\" id=\"MathJax-Span-567\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">computed once per block</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>H</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mi>L</mi><msup><mi>L</mi><mi mathvariant=\"normal\">⊤</mi></msup><mo>,</mo><mspace width=\"1em\"></mspace><mtext>computed once per block</mtext></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-49\">H^{-1} = L L^\\top, \\quad \\text{computed once per block}</script>\n      </li>\n      <li>\n        <p>Combined with dampening (adding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03BB;</mi><mi>I</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-568\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.89em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-569\"><span class=\"mi\" id=\"MathJax-Span-570\" style=\"font-family: STIXGeneral-Italic;\">λ</span><span class=\"mi\" id=\"MathJax-Span-571\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>λ</mi><mi>I</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">\\lambda I</script> to the Hessian), this ensures stability across very large models (e.g., &gt;100B parameters).</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Quantization Scheme</strong></p>\n\n    <ul>\n      <li>GPTQ supports asymmetric per-row quantization with <code class=\"language-plaintext highlighter-rouge\">int4</code> or <code class=\"language-plaintext highlighter-rouge\">int3</code> bitwidths.</li>\n      <li>\n        <p>The quantization grid is fixed via min/max values per row, and weights are quantized using:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>i</mi></msub><mo>&amp;#x22C5;</mo><mtext>Round</mtext><mrow><mo>(</mo><mfrac><mrow><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>j</mi></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>z</mi><mi>i</mi></msub></mrow><msub><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>i</mi></msub></mfrac><mo>)</mo></mrow><mo>+</mo><msub><mi>z</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-572\" style=\"width: 16.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.596em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.346em, 1013.6em, 4.794em, -999.997em); top: -3.799em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-573\"><span class=\"msubsup\" id=\"MathJax-Span-574\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.128em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-575\"><span class=\"mrow\" id=\"MathJax-Span-576\"><span class=\"munderover\" id=\"MathJax-Span-577\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-578\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.159em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-579\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-580\"><span class=\"mrow\" id=\"MathJax-Span-581\"><span class=\"mi\" id=\"MathJax-Span-582\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-583\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-584\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-585\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-586\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-587\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-588\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mtext\" id=\"MathJax-Span-589\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">Round</span><span class=\"mrow\" id=\"MathJax-Span-590\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-591\" style=\"vertical-align: -0.466em;\"><span><span style=\"font-size: 111%; font-family: STIXSizeTwoSym;\">(</span></span></span><span class=\"mfrac\" id=\"MathJax-Span-592\"><span style=\"display: inline-block; position: relative; width: 3.128em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.284em, 1003.02em, 4.482em, -999.997em); top: -4.789em; left: 50%; margin-left: -1.508em;\"><span class=\"mrow\" id=\"MathJax-Span-593\"><span class=\"msubsup\" id=\"MathJax-Span-594\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-595\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-596\"><span class=\"mrow\" id=\"MathJax-Span-597\"><span class=\"mi\" id=\"MathJax-Span-598\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-599\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-600\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-601\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-602\" style=\"font-family: STIXGeneral-Italic;\">z</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-603\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.326em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.518em;\"><span class=\"msubsup\" id=\"MathJax-Span-604\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-605\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-606\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.13em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.128em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-607\" style=\"vertical-align: -0.466em;\"><span><span style=\"font-size: 111%; font-family: STIXSizeTwoSym;\">)</span></span></span></span><span class=\"mo\" id=\"MathJax-Span-608\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-609\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-610\" style=\"font-family: STIXGeneral-Italic;\">z</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-611\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.805em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.059em; border-left: 0px solid; width: 0px; height: 2.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">^</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi mathvariant=\"normal\">Δ</mi><mi>i</mi></msub><mo>⋅</mo><mtext>Round</mtext><mrow><mo>(</mo><mfrac><mrow><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>j</mi></mrow></msub><mo>−</mo><msub><mi>z</mi><mi>i</mi></msub></mrow><msub><mi mathvariant=\"normal\">Δ</mi><mi>i</mi></msub></mfrac><mo>)</mo></mrow><mo>+</mo><msub><mi>z</mi><mi>i</mi></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-51\">\\hat{w}_{ij} = \\Delta_i \\cdot \\text{Round} \\left( \\frac{w_{ij} - z_i}{\\Delta_i} \\right) + z_i</script>\n\n        <ul>\n          <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-612\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-613\"><span class=\"msubsup\" id=\"MathJax-Span-614\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-615\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-616\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi mathvariant=\"normal\">Δ</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">\\Delta_i</script> is the scale and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>z</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-617\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-618\"><span class=\"msubsup\" id=\"MathJax-Span-619\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-620\" style=\"font-family: STIXGeneral-Italic;\">z</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-621\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>z</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">z_i</script> the zero point for row <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-622\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-623\"><span class=\"mi\" id=\"MathJax-Span-624\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-54\">i</script>.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Implementation &amp; Runtime</strong></p>\n\n    <ul>\n      <li>Full quantization of a 175B parameter model (OPT-175B or BLOOM-176B) takes ~4 GPU hours on an NVIDIA A100 using 128 calibration samples from C4.</li>\n      <li>Quantization is applied layer-wise with minimal memory overhead by reloading and processing one transformer block at a time.</li>\n      <li>Models quantized with GPTQ can run on a single GPU, achieving up to 4.5× speedups over <code class=\"language-plaintext highlighter-rouge\">float16</code> baselines.</li>\n    </ul>\n  </li>\n</ol>\n<p><strong>Layer-Wise Quantization Objective</strong></p>\n<ul>\n      <li>\n        <p>For a linear layer with weight matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-364\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-365\"><span class=\"mi\" id=\"MathJax-Span-366\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-41\">W</script> and input activations <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-367\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-368\"><span class=\"mi\" id=\"MathJax-Span-369\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-42\">X</script>, GPTQ minimizes the reconstruction error after quantization:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><munder><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>W</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow></mrow></munder><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>W</mi><mi>X</mi><mo>&amp;#x2212;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>W</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mi>X</mi><msubsup><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mn>2</mn><mn>2</mn></msubsup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-370\" style=\"width: 9.065em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.555em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1007.55em, 3.336em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-371\"><span class=\"munderover\" id=\"MathJax-Span-372\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.57em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-373\" style=\"font-family: STIXGeneral-Regular;\">min</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.128em, 1000.63em, 4.273em, -999.997em); top: -3.122em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-374\"><span class=\"mrow\" id=\"MathJax-Span-375\"><span class=\"texatom\" id=\"MathJax-Span-376\"><span class=\"mrow\" id=\"MathJax-Span-377\"><span class=\"munderover\" id=\"MathJax-Span-378\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-379\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.216em; left: 0.263em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.263em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-380\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.154em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-381\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">‖</span><span class=\"mi\" id=\"MathJax-Span-382\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-383\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-384\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"texatom\" id=\"MathJax-Span-385\" style=\"padding-left: 0.263em;\"><span class=\"mrow\" id=\"MathJax-Span-386\"><span class=\"munderover\" id=\"MathJax-Span-387\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-388\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.367em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-389\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-390\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-391\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-392\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-393\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-394\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.247em; border-left: 0px solid; width: 0px; height: 2.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><munder><mo movablelimits=\"true\" form=\"prefix\">min</mo><mrow class=\"MJX-TeXAtom-ORD\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>W</mi><mo stretchy=\"false\">^</mo></mover></mrow></mrow></munder><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>W</mi><mi>X</mi><mo>−</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>W</mi><mo stretchy=\"false\">^</mo></mover></mrow><mi>X</mi><msubsup><mo fence=\"false\" stretchy=\"false\">‖</mo><mn>2</mn><mn>2</mn></msubsup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-43\">\\min_{\\hat{W}} \\| WX - \\hat{W}X \\|_2^2</script>\n      </li>\n      <li>\n        <p>Quantization is performed column-by-column (i.e., per weight vector), and compensation is applied to unquantized weights to preserve the overall output fidelity.</p>\n      </li>\n    </ul>\n<p>For a linear layer with weight matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-364\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-365\"><span class=\"mi\" id=\"MathJax-Span-366\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-41\">W</script> and input activations <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-367\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-368\"><span class=\"mi\" id=\"MathJax-Span-369\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-42\">X</script>, GPTQ minimizes the reconstruction error after quantization:</p>\n<p>Quantization is performed column-by-column (i.e., per weight vector), and compensation is applied to unquantized weights to preserve the overall output fidelity.</p>\n<p><strong>Approximate Second-Order Weight Selection (OBQ Foundation)</strong></p>\n<ul>\n      <li>\n        <p>GPTQ builds on the Optimal Brain Quantization (OBQ) framework, which selects the next weight <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-395\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.1em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-396\"><span class=\"msubsup\" id=\"MathJax-Span-397\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-398\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-399\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">w_q</script> to quantize by minimizing its induced error, scaled by its Hessian diagonal element:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>w</mi><mi>q</mi></msub><mo>=</mo><mi>arg</mi><mo>&amp;#x2061;</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mfrac><mrow><mo stretchy=&quot;false&quot;>(</mo><mtext>quant</mtext><mo stretchy=&quot;false&quot;>(</mo><msub><mi>w</mi><mi>q</mi></msub><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><msub><mi>w</mi><mi>q</mi></msub><msup><mo stretchy=&quot;false&quot;>)</mo><mn>2</mn></msup></mrow><mrow><mo stretchy=&quot;false&quot;>[</mo><msup><mi>H</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><msub><mo stretchy=&quot;false&quot;>]</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>q</mi><mi>q</mi></mrow></msub></mrow></mfrac><mo>,</mo><mspace width=&quot;1em&quot; /><mi>&amp;#x03B4;</mi><mo>=</mo><mo>&amp;#x2212;</mo><mfrac><mrow><msub><mi>w</mi><mi>q</mi></msub><mo>&amp;#x2212;</mo><mtext>quant</mtext><mo stretchy=&quot;false&quot;>(</mo><msub><mi>w</mi><mi>q</mi></msub><mo stretchy=&quot;false&quot;>)</mo></mrow><mrow><mo stretchy=&quot;false&quot;>[</mo><msup><mi>H</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><msub><mo stretchy=&quot;false&quot;>]</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>q</mi><mi>q</mi></mrow></msub></mrow></mfrac><mo stretchy=&quot;false&quot;>(</mo><msup><mi>H</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><msub><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>:</mo><mo>,</mo><mi>q</mi></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-400\" style=\"width: 32.451em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 27.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.419em, 1027.03em, 3.388em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-401\"><span class=\"msubsup\" id=\"MathJax-Span-402\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-403\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-404\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-405\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-406\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">arg</span><span class=\"mo\" id=\"MathJax-Span-407\"></span><span class=\"mo\" id=\"MathJax-Span-408\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">min</span><span class=\"mfrac\" id=\"MathJax-Span-409\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 7.503em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.023em, 1007.4em, 4.482em, -999.997em); top: -4.789em; left: 50%; margin-left: -3.695em;\"><span class=\"mrow\" id=\"MathJax-Span-410\"><span class=\"mo\" id=\"MathJax-Span-411\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mtext\" id=\"MathJax-Span-412\" style=\"font-family: STIXGeneral-Regular;\">quant</span><span class=\"mo\" id=\"MathJax-Span-413\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-414\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-415\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-416\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-417\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-418\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-419\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-420\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-421\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-422\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-423\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.315em;\"><span class=\"mn\" id=\"MathJax-Span-424\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.076em, 1003.18em, 4.482em, -999.997em); top: -3.279em; left: 50%; margin-left: -1.612em;\"><span class=\"mrow\" id=\"MathJax-Span-425\"><span class=\"mo\" id=\"MathJax-Span-426\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"msubsup\" id=\"MathJax-Span-427\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-428\" style=\"font-family: STIXGeneral-Italic;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-429\"><span class=\"mrow\" id=\"MathJax-Span-430\"><span class=\"mo\" id=\"MathJax-Span-431\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-432\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-433\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-434\" style=\"font-family: STIXGeneral-Regular;\">]</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-435\"><span class=\"mrow\" id=\"MathJax-Span-436\"><span class=\"mi\" id=\"MathJax-Span-437\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span class=\"mi\" id=\"MathJax-Span-438\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1007.5em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 7.503em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-439\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-440\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-441\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">δ</span><span class=\"mo\" id=\"MathJax-Span-442\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-443\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">−</span><span class=\"mfrac\" id=\"MathJax-Span-444\"><span style=\"display: inline-block; position: relative; width: 6.409em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1006.25em, 4.482em, -999.997em); top: -4.789em; left: 50%; margin-left: -3.122em;\"><span class=\"mrow\" id=\"MathJax-Span-445\"><span class=\"msubsup\" id=\"MathJax-Span-446\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-447\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-448\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-449\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mtext\" id=\"MathJax-Span-450\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">quant</span><span class=\"mo\" id=\"MathJax-Span-451\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-452\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-453\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-454\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-455\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.076em, 1003.18em, 4.482em, -999.997em); top: -3.279em; left: 50%; margin-left: -1.612em;\"><span class=\"mrow\" id=\"MathJax-Span-456\"><span class=\"mo\" id=\"MathJax-Span-457\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"msubsup\" id=\"MathJax-Span-458\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-459\" style=\"font-family: STIXGeneral-Italic;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-460\"><span class=\"mrow\" id=\"MathJax-Span-461\"><span class=\"mo\" id=\"MathJax-Span-462\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-463\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-464\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-465\" style=\"font-family: STIXGeneral-Regular;\">]</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-466\"><span class=\"mrow\" id=\"MathJax-Span-467\"><span class=\"mi\" id=\"MathJax-Span-468\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span class=\"mi\" id=\"MathJax-Span-469\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1006.41em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 6.409em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-470\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-471\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-472\" style=\"font-family: STIXGeneral-Italic;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-473\"><span class=\"mrow\" id=\"MathJax-Span-474\"><span class=\"mo\" id=\"MathJax-Span-475\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-476\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-477\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-478\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-479\"><span class=\"mrow\" id=\"MathJax-Span-480\"><span class=\"mo\" id=\"MathJax-Span-481\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mo\" id=\"MathJax-Span-482\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-483\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.309em; border-left: 0px solid; width: 0px; height: 3.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>w</mi><mi>q</mi></msub><mo>=</mo><mi>arg</mi><mo>⁡</mo><mo movablelimits=\"true\" form=\"prefix\">min</mo><mfrac><mrow><mo stretchy=\"false\">(</mo><mtext>quant</mtext><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>q</mi></msub><mo stretchy=\"false\">)</mo><mo>−</mo><msub><mi>w</mi><mi>q</mi></msub><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup></mrow><mrow><mo stretchy=\"false\">[</mo><msup><mi>H</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup><msub><mo stretchy=\"false\">]</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>q</mi><mi>q</mi></mrow></msub></mrow></mfrac><mo>,</mo><mspace width=\"1em\"></mspace><mi>δ</mi><mo>=</mo><mo>−</mo><mfrac><mrow><msub><mi>w</mi><mi>q</mi></msub><mo>−</mo><mtext>quant</mtext><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>q</mi></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mo stretchy=\"false\">[</mo><msup><mi>H</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup><msub><mo stretchy=\"false\">]</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>q</mi><mi>q</mi></mrow></msub></mrow></mfrac><mo stretchy=\"false\">(</mo><msup><mi>H</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup><msub><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo>:</mo><mo>,</mo><mi>q</mi></mrow></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-45\">w_q = \\arg\\min \\frac{(\\text{quant}(w_q) - w_q)^2}{[H^{-1}]_{qq}}, \\quad \\delta = -\\frac{w_q - \\text{quant}(w_q)}{[H^{-1}]_{qq}} (H^{-1})_{:,q}</script>\n      </li>\n      <li>\n        <p>The inverse Hessian <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>H</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mn>2</mn><mi>X</mi><msup><mi>X</mi><mi mathvariant=&quot;normal&quot;>&amp;#x22A4;</mi></msup><mo>+</mo><mi>&amp;#x03BB;</mi><mi>I</mi><msup><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-484\" style=\"width: 10.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1009.12em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-485\"><span class=\"msubsup\" id=\"MathJax-Span-486\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-487\" style=\"font-family: STIXGeneral-Italic;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-488\"><span class=\"mrow\" id=\"MathJax-Span-489\"><span class=\"mo\" id=\"MathJax-Span-490\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-491\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-492\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-493\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mn\" id=\"MathJax-Span-494\" style=\"font-family: STIXGeneral-Regular;\">2</span><span class=\"mi\" id=\"MathJax-Span-495\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-496\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-497\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-498\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⊤</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-499\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-500\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">λ</span><span class=\"mi\" id=\"MathJax-Span-501\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-502\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-503\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-504\"><span class=\"mrow\" id=\"MathJax-Span-505\"><span class=\"mo\" id=\"MathJax-Span-506\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-507\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>H</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mo stretchy=\"false\">(</mo><mn>2</mn><mi>X</mi><msup><mi>X</mi><mi mathvariant=\"normal\">⊤</mi></msup><mo>+</mo><mi>λ</mi><mi>I</mi><msup><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">H^{-1} = (2X X^\\top + \\lambda I)^{-1}</script> captures sensitivity of the layer outputs to changes in weights. This allows for compensation of quantization-induced error by adjusting the remaining unquantized weights.</p>\n      </li>\n    </ul>\n<p>GPTQ builds on the Optimal Brain Quantization (OBQ) framework, which selects the next weight <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mi>q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-395\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.1em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-396\"><span class=\"msubsup\" id=\"MathJax-Span-397\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-398\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-399\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mi>q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">w_q</script> to quantize by minimizing its induced error, scaled by its Hessian diagonal element:</p>\n<p>The inverse Hessian <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>H</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mn>2</mn><mi>X</mi><msup><mi>X</mi><mi mathvariant=&quot;normal&quot;>&amp;#x22A4;</mi></msup><mo>+</mo><mi>&amp;#x03BB;</mi><mi>I</mi><msup><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-484\" style=\"width: 10.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1009.12em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-485\"><span class=\"msubsup\" id=\"MathJax-Span-486\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-487\" style=\"font-family: STIXGeneral-Italic;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-488\"><span class=\"mrow\" id=\"MathJax-Span-489\"><span class=\"mo\" id=\"MathJax-Span-490\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-491\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-492\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-493\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mn\" id=\"MathJax-Span-494\" style=\"font-family: STIXGeneral-Regular;\">2</span><span class=\"mi\" id=\"MathJax-Span-495\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-496\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-497\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-498\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⊤</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-499\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-500\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">λ</span><span class=\"mi\" id=\"MathJax-Span-501\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-502\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-503\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-504\"><span class=\"mrow\" id=\"MathJax-Span-505\"><span class=\"mo\" id=\"MathJax-Span-506\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-507\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>H</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mo stretchy=\"false\">(</mo><mn>2</mn><mi>X</mi><msup><mi>X</mi><mi mathvariant=\"normal\">⊤</mi></msup><mo>+</mo><mi>λ</mi><mi>I</mi><msup><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">H^{-1} = (2X X^\\top + \\lambda I)^{-1}</script> captures sensitivity of the layer outputs to changes in weights. This allows for compensation of quantization-induced error by adjusting the remaining unquantized weights.</p>\n<p><strong>Blockwise Column Quantization with Shared Hessian</strong></p>\n<ul>\n      <li>\n        <p>GPTQ introduces the insight that, in large layers, quantizing all rows in a fixed column order yields nearly the same accuracy as a greedy per-weight order.</p>\n      </li>\n      <li>\n        <p>This allows sharing the Hessian across rows and amortizing its computation—resulting in a complexity reduction from <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>row</mtext></mrow></msub><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>col</mtext></mrow><mn>3</mn></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-508\" style=\"width: 6.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1005.42em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-509\"><span class=\"mi\" id=\"MathJax-Span-510\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-511\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-512\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-513\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-514\"><span class=\"mrow\" id=\"MathJax-Span-515\"><span class=\"mtext\" id=\"MathJax-Span-516\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">row</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-517\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-518\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-519\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-520\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-521\"><span class=\"mrow\" id=\"MathJax-Span-522\"><span class=\"mtext\" id=\"MathJax-Span-523\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">col</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-524\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>row</mtext></mrow></msub><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>col</mtext></mrow><mn>3</mn></msubsup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">O(d_{\\text{row}} \\cdot d_{\\text{col}}^3)</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>row</mtext></mrow></msub><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>col</mtext></mrow><mn>2</mn></msubsup><mo>,</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>col</mtext></mrow><mn>3</mn></msubsup><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-525\" style=\"width: 11.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.794em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1009.74em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-526\"><span class=\"mi\" id=\"MathJax-Span-527\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-528\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mo\" id=\"MathJax-Span-529\" style=\"font-family: STIXGeneral-Regular;\">max</span><span class=\"mo\" id=\"MathJax-Span-530\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-531\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-532\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-533\"><span class=\"mrow\" id=\"MathJax-Span-534\"><span class=\"mtext\" id=\"MathJax-Span-535\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">row</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-536\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-537\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-538\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-539\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-540\"><span class=\"mrow\" id=\"MathJax-Span-541\"><span class=\"mtext\" id=\"MathJax-Span-542\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">col</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-543\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-544\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-545\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-546\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-547\"><span class=\"mrow\" id=\"MathJax-Span-548\"><span class=\"mtext\" id=\"MathJax-Span-549\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">col</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-550\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-551\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mo movablelimits=\"true\" form=\"prefix\">max</mo><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>row</mtext></mrow></msub><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>col</mtext></mrow><mn>2</mn></msubsup><mo>,</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>col</mtext></mrow><mn>3</mn></msubsup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">O(\\max(d_{\\text{row}} \\cdot d_{\\text{col}}^2, d_{\\text{col}}^3))</script>.</p>\n      </li>\n      <li>\n        <p>The following figure (<a href=\"https://arxiv.org/abs/2210.17323\">source</a>) illustrates the GPTQ quantization procedure. Blocks of consecutive columns (bolded) are quantized at a given step, using the inverse Hessian information stored in the Cholesky decomposition, and the remaining weights (blue) are updated at the end of the step. The quantization procedure is applied recursively inside each block: the white middle column is currently being quantized.</p>\n      </li>\n    </ul>\n<p>GPTQ introduces the insight that, in large layers, quantizing all rows in a fixed column order yields nearly the same accuracy as a greedy per-weight order.</p>\n<p>This allows sharing the Hessian across rows and amortizing its computation—resulting in a complexity reduction from <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>row</mtext></mrow></msub><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>col</mtext></mrow><mn>3</mn></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-508\" style=\"width: 6.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1005.42em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-509\"><span class=\"mi\" id=\"MathJax-Span-510\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-511\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-512\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-513\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-514\"><span class=\"mrow\" id=\"MathJax-Span-515\"><span class=\"mtext\" id=\"MathJax-Span-516\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">row</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-517\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-518\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-519\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-520\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-521\"><span class=\"mrow\" id=\"MathJax-Span-522\"><span class=\"mtext\" id=\"MathJax-Span-523\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">col</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-524\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>row</mtext></mrow></msub><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>col</mtext></mrow><mn>3</mn></msubsup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">O(d_{\\text{row}} \\cdot d_{\\text{col}}^3)</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>row</mtext></mrow></msub><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>col</mtext></mrow><mn>2</mn></msubsup><mo>,</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>col</mtext></mrow><mn>3</mn></msubsup><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-525\" style=\"width: 11.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.794em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1009.74em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-526\"><span class=\"mi\" id=\"MathJax-Span-527\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-528\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mo\" id=\"MathJax-Span-529\" style=\"font-family: STIXGeneral-Regular;\">max</span><span class=\"mo\" id=\"MathJax-Span-530\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-531\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-532\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-533\"><span class=\"mrow\" id=\"MathJax-Span-534\"><span class=\"mtext\" id=\"MathJax-Span-535\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">row</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-536\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-537\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-538\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-539\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-540\"><span class=\"mrow\" id=\"MathJax-Span-541\"><span class=\"mtext\" id=\"MathJax-Span-542\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">col</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-543\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-544\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-545\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-546\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-547\"><span class=\"mrow\" id=\"MathJax-Span-548\"><span class=\"mtext\" id=\"MathJax-Span-549\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">col</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-550\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-551\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mo movablelimits=\"true\" form=\"prefix\">max</mo><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>row</mtext></mrow></msub><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>col</mtext></mrow><mn>2</mn></msubsup><mo>,</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>col</mtext></mrow><mn>3</mn></msubsup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">O(\\max(d_{\\text{row}} \\cdot d_{\\text{col}}^2, d_{\\text{col}}^3))</script>.</p>\n<p>The following figure (<a href=\"https://arxiv.org/abs/2210.17323\">source</a>) illustrates the GPTQ quantization procedure. Blocks of consecutive columns (bolded) are quantized at a given step, using the inverse Hessian information stored in the Cholesky decomposition, and the remaining weights (blue) are updated at the end of the step. The quantization procedure is applied recursively inside each block: the white middle column is currently being quantized.</p>\n<p><img src=\"../assets/model-compression/GPTQ.jpg\" alt=\"GPTQ Figure 2 - Quantization Procedure\"></p>\n<p><strong>Lazy Batch Updates for GPU Efficiency</strong></p>\n<ul>\n      <li>To alleviate the memory-bandwidth bottleneck of GPU kernels, GPTQ processes batches of columns (e.g., 128) before updating the full weight matrix.</li>\n      <li>This “lazy update” scheme postpones weight and Hessian modifications until a full block has been processed, improving throughput and parallelism.</li>\n    </ul>\n<p><strong>Cholesky-Based Inversion for Numerical Stability</strong></p>\n<ul>\n      <li>\n        <p>To avoid numerical instability from repeatedly inverting Hessians during block updates, GPTQ reformulates the update rule using a Cholesky decomposition:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>H</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><mo>=</mo><mi>L</mi><msup><mi>L</mi><mi mathvariant=&quot;normal&quot;>&amp;#x22A4;</mi></msup><mo>,</mo><mspace width=&quot;1em&quot; /><mtext>computed once per block</mtext></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-552\" style=\"width: 19.69em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 16.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1016.41em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-553\"><span class=\"msubsup\" id=\"MathJax-Span-554\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-555\" style=\"font-family: STIXGeneral-Italic;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-556\"><span class=\"mrow\" id=\"MathJax-Span-557\"><span class=\"mo\" id=\"MathJax-Span-558\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-559\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-560\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-561\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-562\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-563\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-564\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⊤</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-565\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-566\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mtext\" id=\"MathJax-Span-567\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">computed once per block</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>H</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup><mo>=</mo><mi>L</mi><msup><mi>L</mi><mi mathvariant=\"normal\">⊤</mi></msup><mo>,</mo><mspace width=\"1em\"></mspace><mtext>computed once per block</mtext></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-49\">H^{-1} = L L^\\top, \\quad \\text{computed once per block}</script>\n      </li>\n      <li>\n        <p>Combined with dampening (adding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03BB;</mi><mi>I</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-568\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.89em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-569\"><span class=\"mi\" id=\"MathJax-Span-570\" style=\"font-family: STIXGeneral-Italic;\">λ</span><span class=\"mi\" id=\"MathJax-Span-571\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>λ</mi><mi>I</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">\\lambda I</script> to the Hessian), this ensures stability across very large models (e.g., &gt;100B parameters).</p>\n      </li>\n    </ul>\n<p>To avoid numerical instability from repeatedly inverting Hessians during block updates, GPTQ reformulates the update rule using a Cholesky decomposition:</p>\n<p>Combined with dampening (adding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03BB;</mi><mi>I</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-568\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.89em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-569\"><span class=\"mi\" id=\"MathJax-Span-570\" style=\"font-family: STIXGeneral-Italic;\">λ</span><span class=\"mi\" id=\"MathJax-Span-571\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>λ</mi><mi>I</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">\\lambda I</script> to the Hessian), this ensures stability across very large models (e.g., &gt;100B parameters).</p>\n<p><strong>Quantization Scheme</strong></p>\n<ul>\n      <li>GPTQ supports asymmetric per-row quantization with <code class=\"language-plaintext highlighter-rouge\">int4</code> or <code class=\"language-plaintext highlighter-rouge\">int3</code> bitwidths.</li>\n      <li>\n        <p>The quantization grid is fixed via min/max values per row, and weights are quantized using:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>i</mi></msub><mo>&amp;#x22C5;</mo><mtext>Round</mtext><mrow><mo>(</mo><mfrac><mrow><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mi>j</mi></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>z</mi><mi>i</mi></msub></mrow><msub><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>i</mi></msub></mfrac><mo>)</mo></mrow><mo>+</mo><msub><mi>z</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-572\" style=\"width: 16.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.596em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.346em, 1013.6em, 4.794em, -999.997em); top: -3.799em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-573\"><span class=\"msubsup\" id=\"MathJax-Span-574\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.128em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-575\"><span class=\"mrow\" id=\"MathJax-Span-576\"><span class=\"munderover\" id=\"MathJax-Span-577\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-578\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.159em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-579\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-580\"><span class=\"mrow\" id=\"MathJax-Span-581\"><span class=\"mi\" id=\"MathJax-Span-582\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-583\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-584\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-585\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-586\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-587\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-588\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mtext\" id=\"MathJax-Span-589\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">Round</span><span class=\"mrow\" id=\"MathJax-Span-590\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-591\" style=\"vertical-align: -0.466em;\"><span><span style=\"font-size: 111%; font-family: STIXSizeTwoSym;\">(</span></span></span><span class=\"mfrac\" id=\"MathJax-Span-592\"><span style=\"display: inline-block; position: relative; width: 3.128em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.284em, 1003.02em, 4.482em, -999.997em); top: -4.789em; left: 50%; margin-left: -1.508em;\"><span class=\"mrow\" id=\"MathJax-Span-593\"><span class=\"msubsup\" id=\"MathJax-Span-594\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-595\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-596\"><span class=\"mrow\" id=\"MathJax-Span-597\"><span class=\"mi\" id=\"MathJax-Span-598\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-599\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-600\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-601\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-602\" style=\"font-family: STIXGeneral-Italic;\">z</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-603\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.326em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.518em;\"><span class=\"msubsup\" id=\"MathJax-Span-604\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-605\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-606\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.13em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.128em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-607\" style=\"vertical-align: -0.466em;\"><span><span style=\"font-size: 111%; font-family: STIXSizeTwoSym;\">)</span></span></span></span><span class=\"mo\" id=\"MathJax-Span-608\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-609\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-610\" style=\"font-family: STIXGeneral-Italic;\">z</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-611\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.805em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.059em; border-left: 0px solid; width: 0px; height: 2.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">^</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi mathvariant=\"normal\">Δ</mi><mi>i</mi></msub><mo>⋅</mo><mtext>Round</mtext><mrow><mo>(</mo><mfrac><mrow><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mi>j</mi></mrow></msub><mo>−</mo><msub><mi>z</mi><mi>i</mi></msub></mrow><msub><mi mathvariant=\"normal\">Δ</mi><mi>i</mi></msub></mfrac><mo>)</mo></mrow><mo>+</mo><msub><mi>z</mi><mi>i</mi></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-51\">\\hat{w}_{ij} = \\Delta_i \\cdot \\text{Round} \\left( \\frac{w_{ij} - z_i}{\\Delta_i} \\right) + z_i</script>\n\n        <ul>\n          <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-612\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-613\"><span class=\"msubsup\" id=\"MathJax-Span-614\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-615\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-616\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi mathvariant=\"normal\">Δ</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">\\Delta_i</script> is the scale and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>z</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-617\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-618\"><span class=\"msubsup\" id=\"MathJax-Span-619\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-620\" style=\"font-family: STIXGeneral-Italic;\">z</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-621\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>z</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">z_i</script> the zero point for row <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-622\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-623\"><span class=\"mi\" id=\"MathJax-Span-624\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-54\">i</script>.</li>\n        </ul>\n      </li>\n    </ul>\n<p>The quantization grid is fixed via min/max values per row, and weights are quantized using:</p>\n<ul>\n          <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-612\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-613\"><span class=\"msubsup\" id=\"MathJax-Span-614\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-615\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-616\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi mathvariant=\"normal\">Δ</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">\\Delta_i</script> is the scale and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>z</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-617\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-618\"><span class=\"msubsup\" id=\"MathJax-Span-619\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-620\" style=\"font-family: STIXGeneral-Italic;\">z</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-621\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>z</mi><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">z_i</script> the zero point for row <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-622\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-623\"><span class=\"mi\" id=\"MathJax-Span-624\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-54\">i</script>.</li>\n        </ul>\n<p><strong>Implementation &amp; Runtime</strong></p>\n<ul>\n      <li>Full quantization of a 175B parameter model (OPT-175B or BLOOM-176B) takes ~4 GPU hours on an NVIDIA A100 using 128 calibration samples from C4.</li>\n      <li>Quantization is applied layer-wise with minimal memory overhead by reloading and processing one transformer block at a time.</li>\n      <li>Models quantized with GPTQ can run on a single GPU, achieving up to 4.5× speedups over <code class=\"language-plaintext highlighter-rouge\">float16</code> baselines.</li>\n    </ul>\n<h5 id=\"pros\">Pros</h5>\n<ul>\n  <li><strong>Highly accurate</strong>: Maintains perplexity within 0.03 of <code class=\"language-plaintext highlighter-rouge\">float16</code> for OPT-175B (<code class=\"language-plaintext highlighter-rouge\">int4</code>) and tolerable degradation at <code class=\"language-plaintext highlighter-rouge\">int3</code>.</li>\n  <li><strong>Scalable</strong>: Efficient enough to quantize 100B+ parameter models using a single GPU.</li>\n  <li><strong>Hardware-efficient</strong>: Enables deployment of massive LLMs on consumer-grade GPUs (e.g., RTX 3090).</li>\n  <li><strong>Open-source tooling</strong>: Supported by AutoGPTQ, Hugging Face Transformers, and integrations with <code class=\"language-plaintext highlighter-rouge\">load_in_4bit=True</code>.</li>\n</ul>\n<h5 id=\"cons\">Cons</h5>\n<ul>\n  <li><strong>Weight-only</strong>: Does not include activation quantization; activation memory remains in <code class=\"language-plaintext highlighter-rouge\">float16</code> unless combined with other methods.</li>\n  <li><strong>Nontrivial math</strong>: Relies on Hessian approximations and matrix inversions, which may complicate custom implementation or adaptation.</li>\n  <li><strong>Challenging for non-standard layers</strong>: Works best with standard linear layers; adaptation for fused or exotic architectures may require modification.</li>\n</ul>\n<h4 id=\"smoothquant\">SmoothQuant</h4>\n<ul>\n  <li>Introduced in <a href=\"https://arxiv.org/abs/2211.10438\">SmoothQuant: Accurate and Efficient Post‑Training Quantization for Large Language Models</a> by Xiao et al. (2022), SmoothQuant enables uniform 8-bit quantization for both weights and activations (W8A8) in LLMs by balancing the quantization difficulty between them. It allows high-accuracy post-training quantization (PTQ) on transformer architectures without requiring fine-tuning. A high-level explanation is also available via <a href=\"https://leimao.github.io/blog/SmoothQuant-LLM-Quantization/\">Lei Mao’s Log Book</a>.</li>\n</ul>\n<h5 id=\"process-1\">Process</h5>\n<ol>\n  <li>\n    <p><strong>Analyze Activation Outliers</strong>:\nActivation tensors in LLMs often have long-tailed distributions, leading to a high dynamic range. These outliers cause large quantization errors when mapping to low-bit formats like <code class=\"language-plaintext highlighter-rouge\">int8</code>.</p>\n  </li>\n  <li>\n    <p><strong>Offline Scaling of Input and Weights</strong>:\nTo reduce activation outliers, SmoothQuant proposes to pre-scale input activations and inversely scale the associated weight matrices before quantization. This is done by computing per-channel maximum absolute values of activation tensors and applying the following scaling transformation:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\"><span class=\"MathJax MathJax_FullWidth\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mspace linebreak=&quot;newline&quot; /><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mi>W</mi><mo>&amp;#x2217;</mo><mi>s</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-625\" style=\"width: 100%; display: inline-block; min-width: 7.711em;\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.867em, 1006.36em, 6.253em, -999.997em); top: -4.112em; left: 0em; width: 100%;\"><span class=\"mrow\" id=\"MathJax-Span-626\"><span style=\"display: inline-block; position: relative; width: 100%; height: 0px;\"><span style=\"position: absolute; clip: rect(2.763em, 1004.48em, 4.846em, -999.997em); top: -4.008em; left: 50%; margin-left: -2.237em;\"><span class=\"msubsup\" id=\"MathJax-Span-627\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-628\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-629\"><span class=\"mrow\" id=\"MathJax-Span-630\"><span class=\"mi\" id=\"MathJax-Span-631\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">s</span><span class=\"mi\" id=\"MathJax-Span-632\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-633\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-634\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-635\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-636\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-637\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-638\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.206em;\"><span class=\"mi\" id=\"MathJax-Span-639\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.206em;\"><span class=\"mi\" id=\"MathJax-Span-640\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.58em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.576em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1006.36em, 4.326em, -999.997em); top: -2.185em; left: 50%; margin-left: -3.174em;\"><span class=\"mspace\" id=\"MathJax-Span-641\" style=\"height: 0em; vertical-align: 0em; width: 0em; display: inline-block; overflow: hidden;\"></span><span class=\"msubsup\" id=\"MathJax-Span-642\"><span style=\"display: inline-block; position: relative; width: 2.763em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-643\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-644\"><span class=\"mrow\" id=\"MathJax-Span-645\"><span class=\"mi\" id=\"MathJax-Span-646\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">s</span><span class=\"mi\" id=\"MathJax-Span-647\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span class=\"mi\" id=\"MathJax-Span-648\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-649\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-650\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-651\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-652\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-653\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-654\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">∗</span><span class=\"mi\" id=\"MathJax-Span-655\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.434em; border-left: 0px solid; width: 0px; height: 3.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mspace linebreak=\"newline\"></mspace><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><mi>W</mi><mo>∗</mo><mi>s</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-55\">x_{scaled} = \\frac{x}{s} \\\\\nW_{scaled} = W * s</script>\n\n    <ul>\n      <li>Here, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-656\" style=\"width: 0.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.37em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-657\"><span class=\"mi\" id=\"MathJax-Span-658\" style=\"font-family: STIXGeneral-Italic;\">s</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">s</script> is the scaling factor (per input channel), <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-659\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-660\"><span class=\"mi\" id=\"MathJax-Span-661\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">x</script> is the activation input, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-662\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-663\"><span class=\"mi\" id=\"MathJax-Span-664\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-58\">W</script> is the weight matrix.</li>\n      <li>\n        <p>This transformation preserves the original linear operation because:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-59-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>@</mo></mrow><mi>W</mi><mo>&amp;#x2248;</mo><mo stretchy=&quot;false&quot;>(</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>@</mo></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>W</mi><mo>&amp;#x2217;</mo><mi>s</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-665\" style=\"width: 10.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.065em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.992em, 1009.01em, 3.076em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-666\"><span class=\"mi\" id=\"MathJax-Span-667\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-668\"><span class=\"mrow\" id=\"MathJax-Span-669\"><span class=\"mo\" id=\"MathJax-Span-670\" style=\"font-family: STIXGeneral-Regular;\">@</span></span></span><span class=\"mi\" id=\"MathJax-Span-671\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-672\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mo\" id=\"MathJax-Span-673\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mfrac\" id=\"MathJax-Span-674\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.206em;\"><span class=\"mi\" id=\"MathJax-Span-675\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.206em;\"><span class=\"mi\" id=\"MathJax-Span-676\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.58em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.576em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-677\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"texatom\" id=\"MathJax-Span-678\"><span class=\"mrow\" id=\"MathJax-Span-679\"><span class=\"mo\" id=\"MathJax-Span-680\" style=\"font-family: STIXGeneral-Regular;\">@</span></span></span><span class=\"mo\" id=\"MathJax-Span-681\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-682\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-683\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">∗</span><span class=\"mi\" id=\"MathJax-Span-684\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">s</span><span class=\"mo\" id=\"MathJax-Span-685\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>@</mo></mrow><mi>W</mi><mo>≈</mo><mo stretchy=\"false\">(</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo>@</mo></mrow><mo stretchy=\"false\">(</mo><mi>W</mi><mo>∗</mo><mi>s</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-59\">x @ W ≈ (\\frac{x}{s}) @ (W * s)</script>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Quantize Scaled Tensors</strong>:\nApply standard post-training quantization (e.g., <code class=\"language-plaintext highlighter-rouge\">torch.quantize_per_tensor</code>) on the scaled weight and activation tensors using uniform <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization.</p>\n  </li>\n  <li>\n    <p><strong>No Runtime Overhead</strong>:\nThe inverse scaling factors <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-60-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-686\" style=\"width: 0.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.37em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-687\"><span class=\"mi\" id=\"MathJax-Span-688\" style=\"font-family: STIXGeneral-Italic;\">s</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-60\">s</script> are folded into the preceding layers during offline preprocessing. At inference, the quantized model does not require additional computation to reverse the scaling—hence, preserving speed.</p>\n  </li>\n</ol>\n<p><strong>Analyze Activation Outliers</strong>:\nActivation tensors in LLMs often have long-tailed distributions, leading to a high dynamic range. These outliers cause large quantization errors when mapping to low-bit formats like <code class=\"language-plaintext highlighter-rouge\">int8</code>.</p>\n<p><strong>Offline Scaling of Input and Weights</strong>:\nTo reduce activation outliers, SmoothQuant proposes to pre-scale input activations and inversely scale the associated weight matrices before quantization. This is done by computing per-channel maximum absolute values of activation tensors and applying the following scaling transformation:</p>\n<ul>\n      <li>Here, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-656\" style=\"width: 0.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.37em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-657\"><span class=\"mi\" id=\"MathJax-Span-658\" style=\"font-family: STIXGeneral-Italic;\">s</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">s</script> is the scaling factor (per input channel), <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-659\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-660\"><span class=\"mi\" id=\"MathJax-Span-661\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">x</script> is the activation input, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-662\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-663\"><span class=\"mi\" id=\"MathJax-Span-664\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-58\">W</script> is the weight matrix.</li>\n      <li>\n        <p>This transformation preserves the original linear operation because:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-59-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>@</mo></mrow><mi>W</mi><mo>&amp;#x2248;</mo><mo stretchy=&quot;false&quot;>(</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>@</mo></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>W</mi><mo>&amp;#x2217;</mo><mi>s</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-665\" style=\"width: 10.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.065em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.992em, 1009.01em, 3.076em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-666\"><span class=\"mi\" id=\"MathJax-Span-667\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"texatom\" id=\"MathJax-Span-668\"><span class=\"mrow\" id=\"MathJax-Span-669\"><span class=\"mo\" id=\"MathJax-Span-670\" style=\"font-family: STIXGeneral-Regular;\">@</span></span></span><span class=\"mi\" id=\"MathJax-Span-671\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-672\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mo\" id=\"MathJax-Span-673\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mfrac\" id=\"MathJax-Span-674\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.206em;\"><span class=\"mi\" id=\"MathJax-Span-675\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.206em;\"><span class=\"mi\" id=\"MathJax-Span-676\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.58em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.576em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-677\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"texatom\" id=\"MathJax-Span-678\"><span class=\"mrow\" id=\"MathJax-Span-679\"><span class=\"mo\" id=\"MathJax-Span-680\" style=\"font-family: STIXGeneral-Regular;\">@</span></span></span><span class=\"mo\" id=\"MathJax-Span-681\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-682\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-683\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">∗</span><span class=\"mi\" id=\"MathJax-Span-684\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">s</span><span class=\"mo\" id=\"MathJax-Span-685\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>@</mo></mrow><mi>W</mi><mo>≈</mo><mo stretchy=\"false\">(</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo>@</mo></mrow><mo stretchy=\"false\">(</mo><mi>W</mi><mo>∗</mo><mi>s</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-59\">x @ W ≈ (\\frac{x}{s}) @ (W * s)</script>\n      </li>\n    </ul>\n<p>This transformation preserves the original linear operation because:</p>\n<p><strong>Quantize Scaled Tensors</strong>:\nApply standard post-training quantization (e.g., <code class=\"language-plaintext highlighter-rouge\">torch.quantize_per_tensor</code>) on the scaled weight and activation tensors using uniform <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization.</p>\n<p><strong>No Runtime Overhead</strong>:\nThe inverse scaling factors <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-60-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-686\" style=\"width: 0.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.37em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-687\"><span class=\"mi\" id=\"MathJax-Span-688\" style=\"font-family: STIXGeneral-Italic;\">s</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-60\">s</script> are folded into the preceding layers during offline preprocessing. At inference, the quantized model does not require additional computation to reverse the scaling—hence, preserving speed.</p>\n<h5 id=\"pros-1\">Pros</h5>\n<ul>\n  <li>Training-free and calibration-light—requires only a few batches of representative data for statistics.</li>\n  <li>Allows fully static W8A8 quantization for transformers, which were previously hard to quantize due to activation outliers.</li>\n  <li>Compatible with major LLMs like Llama, OPT, BLOOM, and GLM.</li>\n  <li>Hardware-friendly: <code class=\"language-plaintext highlighter-rouge\">int8</code> inference is highly optimized on modern CPUs (e.g., VNNI, AMX) and GPUs.</li>\n  <li>\n    <p>Achieves substantial efficiency improvements:</p>\n\n    <ul>\n      <li>~2× reduction in memory footprint.</li>\n      <li>~1.5× to 2× speedup on supported backends.</li>\n      <li>&lt;0.5% accuracy loss on common NLP benchmarks.</li>\n    </ul>\n  </li>\n</ul>\n<p>Achieves substantial efficiency improvements:</p>\n<ul>\n      <li>~2× reduction in memory footprint.</li>\n      <li>~1.5× to 2× speedup on supported backends.</li>\n      <li>&lt;0.5% accuracy loss on common NLP benchmarks.</li>\n    </ul>\n<h5 id=\"cons-1\">Cons</h5>\n<ul>\n  <li>Limited to 8-bit formats—does not address extreme quantization (e.g., 4-bit or binary).</li>\n  <li>Effectiveness depends on the distribution of activations. Improper scaling (e.g., due to poor calibration data) may still degrade performance.</li>\n  <li>Static scale determination can be fragile in models with dynamic context (e.g., prompts of variable length).</li>\n</ul>\n<h4 id=\"activation-aware-weight-quantization-awq\">Activation-Aware Weight Quantization (AWQ)</h4>\n<ul>\n  <li>Introduced in <a href=\"https://arxiv.org/abs/2306.00978\">AWQ: Activation‑aware Weight Quantization for LLM Compression and Acceleration</a> by Lin et al. (2024), AWQ is a post‑training, weight‑only quantization technique tailored for LLMs. It identifies and protects <em>salient channels</em>—those with large activations—via per‑channel scaling derived from calibration, enabling accurate <strong><code class=\"language-plaintext highlighter-rouge\">int4</code>/<code class=\"language-plaintext highlighter-rouge\">int3</code></strong> quantization without retraining. Reference implementation (AutoAWQ and CUDA kernels) is available on <a href=\"https://github.com/mit-han-lab/llm-awq\">GitHub</a>.</li>\n  <li>AWQ offers a highly practical and accurate low-bit, weight-only quantization path. By using activation statistics to protect the most critical channels, it achieves near full-precision accuracy under <code class=\"language-plaintext highlighter-rouge\">int4/3</code> quantization without training. It retains the efficiency of group-wise kernels for deployment while minimizing model size and speeding up inference for LLMs suited to edge and GPU environments.</li>\n</ul>\n<h5 id=\"process-2\">Process</h5>\n<ol>\n  <li>\n    <p><strong>Calibration Pass</strong></p>\n\n    <ul>\n      <li>Run a <em>small calibration dataset</em> through the unquantized model to gather per-channel activation statistics, typically the expected absolute value <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>E</mi></mrow><mo stretchy=&quot;false&quot;>[</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-689\" style=\"width: 3.076em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1002.45em, 2.711em, -999.997em); top: -2.341em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-690\"><span class=\"texatom\" id=\"MathJax-Span-691\"><span class=\"mrow\" id=\"MathJax-Span-692\"><span class=\"mi\" id=\"MathJax-Span-693\" style=\"font-family: STIXGeneral-Regular;\">𝔼</span></span></span><span class=\"mo\" id=\"MathJax-Span-694\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mo\" id=\"MathJax-Span-695\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"msubsup\" id=\"MathJax-Span-696\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-697\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-698\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-699\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-700\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.346em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">E</mi></mrow><mo stretchy=\"false\">[</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence=\"false\" stretchy=\"false\">‖</mo><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">\\mathbb{E}[\\|x_i\\|]</script> of input to each weight channel <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-62-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-701\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-702\"><span class=\"mi\" id=\"MathJax-Span-703\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-62\">i</script>.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Group-Wise Weight Quantization Baseline</strong></p>\n\n    <ul>\n      <li>Use group size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-63-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-704\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-705\"><span class=\"mi\" id=\"MathJax-Span-706\" style=\"font-family: STIXGeneral-Italic;\">G</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-63\">G</script> (e.g. 32 channels) to quantize weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-64-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>w</mi></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-707\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.878em, 1000.73em, 2.659em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-708\"><span class=\"texatom\" id=\"MathJax-Span-709\"><span class=\"mrow\" id=\"MathJax-Span-710\"><span class=\"mi\" id=\"MathJax-Span-711\" style=\"font-family: STIXGeneral; font-weight: bold;\">w</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"bold\">w</mi></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-64\">\\mathbf{w}</script> via a uniform symmetric scheme:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-65-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>Q</mi><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mo>&amp;#x22C5;</mo><mtext>Round</mtext><mrow><mo>(</mo><mrow><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi></mrow><mo>)</mo></mrow><mo>,</mo><mspace width=&quot;1em&quot; /><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mo>=</mo><mfrac><mrow><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow><mrow><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>b</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><mo>&amp;#x2212;</mo><mn>1</mn></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-712\" style=\"width: 21.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 17.815em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.732em, 1017.82em, 3.388em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-713\"><span class=\"mi\" id=\"MathJax-Span-714\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-715\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-716\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-717\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-718\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-719\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">Δ</span><span class=\"mo\" id=\"MathJax-Span-720\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mtext\" id=\"MathJax-Span-721\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">Round</span><span class=\"mrow\" id=\"MathJax-Span-722\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-723\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mrow\" id=\"MathJax-Span-724\"><span class=\"mi\" id=\"MathJax-Span-725\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"texatom\" id=\"MathJax-Span-726\"><span class=\"mrow\" id=\"MathJax-Span-727\"><span class=\"mo\" id=\"MathJax-Span-728\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-729\" style=\"font-family: STIXGeneral-Regular;\">Δ</span></span><span class=\"mo\" id=\"MathJax-Span-730\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span class=\"mo\" id=\"MathJax-Span-731\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"mspace\" id=\"MathJax-Span-732\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-733\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">Δ</span><span class=\"mo\" id=\"MathJax-Span-734\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-735\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.596em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1003.13em, 4.378em, -999.997em); top: -4.685em; left: 50%; margin-left: -1.612em;\"><span class=\"mrow\" id=\"MathJax-Span-736\"><span class=\"mo\" id=\"MathJax-Span-737\" style=\"font-family: STIXGeneral-Regular;\">max</span><span class=\"texatom\" id=\"MathJax-Span-738\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-739\"><span class=\"mo\" id=\"MathJax-Span-740\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"mi\" id=\"MathJax-Span-741\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"texatom\" id=\"MathJax-Span-742\"><span class=\"mrow\" id=\"MathJax-Span-743\"><span class=\"mo\" id=\"MathJax-Span-744\" style=\"font-family: STIXVariants;\">|</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.971em, 1003.39em, 4.326em, -999.997em); top: -3.174em; left: 50%; margin-left: -1.716em;\"><span class=\"mrow\" id=\"MathJax-Span-745\"><span class=\"msubsup\" id=\"MathJax-Span-746\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-747\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-748\"><span class=\"mrow\" id=\"MathJax-Span-749\"><span class=\"mi\" id=\"MathJax-Span-750\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">b</span><span class=\"mo\" id=\"MathJax-Span-751\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-752\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-753\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mn\" id=\"MathJax-Span-754\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.6em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.596em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.247em; border-left: 0px solid; width: 0px; height: 2.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>Q</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi mathvariant=\"normal\">Δ</mi><mo>⋅</mo><mtext>Round</mtext><mrow><mo>(</mo><mrow><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi mathvariant=\"normal\">Δ</mi></mrow><mo>)</mo></mrow><mo>,</mo><mspace width=\"1em\"></mspace><mi mathvariant=\"normal\">Δ</mi><mo>=</mo><mfrac><mrow><mo movablelimits=\"true\" form=\"prefix\">max</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow></mrow><mrow><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>b</mi><mo>−</mo><mn>1</mn></mrow></msup><mo>−</mo><mn>1</mn></mrow></mfrac></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-65\">Q(w) = \\Delta \\cdot \\text{Round}\\left(w / \\Delta\\right), \\quad \\Delta = \\frac{\\max |w|}{2^{b-1}-1}</script>\n\n    <ul>\n      <li>The quantization error in group-wise quantization is proportional to input activation magnitude rather than weight magnitude alone.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Compute Activation-Based Scaling Factors</strong></p>\n\n    <ul>\n      <li>Let <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-66-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>s</mi><mi>i</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo>=</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>E</mi></mrow><mi>x</mi></msub><mo stretchy=&quot;false&quot;>[</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-755\" style=\"width: 6.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.419em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1005.32em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-756\"><span class=\"msubsup\" id=\"MathJax-Span-757\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-758\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.89em, 4.273em, -999.997em); top: -4.477em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-759\"><span class=\"mrow\" id=\"MathJax-Span-760\"><span class=\"mo\" id=\"MathJax-Span-761\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-762\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-763\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-764\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-765\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-766\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-767\"><span class=\"mrow\" id=\"MathJax-Span-768\"><span class=\"mi\" id=\"MathJax-Span-769\" style=\"font-family: STIXGeneral-Regular;\">𝔼</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-770\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-771\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mo\" id=\"MathJax-Span-772\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"msubsup\" id=\"MathJax-Span-773\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-774\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-775\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-776\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-777\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>s</mi><mi>i</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">E</mi></mrow><mi>x</mi></msub><mo stretchy=\"false\">[</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence=\"false\" stretchy=\"false\">‖</mo><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-66\">s_i^{(x)} = \\mathbb{E}_x [\\|x_i\\|]</script> represent average per-channel activation magnitude.</li>\n      <li>For scaling exponent <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-67-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi><mo>&amp;#x2208;</mo><mo stretchy=&quot;false&quot;>[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-778\" style=\"width: 4.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.86em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-779\"><span class=\"mi\" id=\"MathJax-Span-780\" style=\"font-family: STIXGeneral-Italic;\">α</span><span class=\"mo\" id=\"MathJax-Span-781\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"mo\" id=\"MathJax-Span-782\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">[</span><span class=\"mn\" id=\"MathJax-Span-783\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-784\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-785\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">1</span><span class=\"mo\" id=\"MathJax-Span-786\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi><mo>∈</mo><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-67\">\\alpha \\in [0,1]</script>, define:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-68-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msubsup><mi>s</mi><mi>i</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>&amp;#x03B1;</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>s</mi><mi>i</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><msup><mo stretchy=&quot;false&quot;>)</mo><mi>&amp;#x03B1;</mi></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-787\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1004.85em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-788\"><span class=\"msubsup\" id=\"MathJax-Span-789\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-790\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -4.477em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-791\"><span class=\"mrow\" id=\"MathJax-Span-792\"><span class=\"mo\" id=\"MathJax-Span-793\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-794\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">α</span><span class=\"mo\" id=\"MathJax-Span-795\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-796\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-797\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-798\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-799\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-800\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.89em, 4.273em, -999.997em); top: -4.477em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-801\"><span class=\"mrow\" id=\"MathJax-Span-802\"><span class=\"mo\" id=\"MathJax-Span-803\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-804\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-805\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-806\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-807\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-808\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-809\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msubsup><mi>s</mi><mi>i</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>α</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mo stretchy=\"false\">(</mo><msubsup><mi>s</mi><mi>i</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><msup><mo stretchy=\"false\">)</mo><mi>α</mi></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-68\">s_i^{(\\alpha)} = (s_i^{(x)})^\\alpha</script>\n\n    <ul>\n      <li>Use a <strong>small grid search</strong> over <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-69-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-810\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-811\"><span class=\"mi\" id=\"MathJax-Span-812\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-69\">\\alpha</script> to minimize an approximate MSE:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-70-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>E</mi></mrow><mi>x</mi></msub><msup><mrow><mo symmetric=&quot;true&quot;>&amp;#x2016;</mo><mrow><mi>Q</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>w</mi></mrow><mo>&amp;#x22C5;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;normal&quot;>d</mi><mi mathvariant=&quot;normal&quot;>i</mi><mi mathvariant=&quot;normal&quot;>a</mi><mi mathvariant=&quot;normal&quot;>g</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><msup><mi>s</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>&amp;#x03B1;</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-OPEN&quot;><mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;>(</mo></mrow><msup><mi>s</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>&amp;#x03B1;</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><msup><mrow class=&quot;MJX-TeXAtom-CLOSE&quot;><mo maxsize=&quot;1.2em&quot; minsize=&quot;1.2em&quot;>)</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><mi>x</mi><mo>&amp;#x2212;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>w</mi></mrow><mi>x</mi></mrow><mo symmetric=&quot;true&quot;>&amp;#x2016;</mo></mrow><mn>2</mn></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-813\" style=\"width: 18.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 15.211em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.628em, 1015.21em, 2.919em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-814\"><span class=\"msubsup\" id=\"MathJax-Span-815\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-816\"><span class=\"mrow\" id=\"MathJax-Span-817\"><span class=\"mi\" id=\"MathJax-Span-818\" style=\"font-family: STIXGeneral-Regular;\">𝔼</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-819\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-820\"><span style=\"display: inline-block; position: relative; width: 14.221em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.711em, 1013.65em, 4.794em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-821\"><span class=\"mo\" id=\"MathJax-Span-822\" style=\"vertical-align: 1.148em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -3.331em; left: 0em;\">‖<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -2.445em; left: 0em;\">‖<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -2.862em; left: 0em;\">‖<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mrow\" id=\"MathJax-Span-823\"><span class=\"mi\" id=\"MathJax-Span-824\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-825\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-826\"><span class=\"mrow\" id=\"MathJax-Span-827\"><span class=\"mi\" id=\"MathJax-Span-828\" style=\"font-family: STIXGeneral; font-weight: bold;\">w</span></span></span><span class=\"mo\" id=\"MathJax-Span-829\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"texatom\" id=\"MathJax-Span-830\" style=\"padding-left: 0.263em;\"><span class=\"mrow\" id=\"MathJax-Span-831\"><span class=\"mi\" id=\"MathJax-Span-832\" style=\"font-family: STIXGeneral-Regular;\">d</span><span class=\"mi\" id=\"MathJax-Span-833\" style=\"font-family: STIXGeneral-Regular;\">i</span><span class=\"mi\" id=\"MathJax-Span-834\" style=\"font-family: STIXGeneral-Regular;\">a</span><span class=\"mi\" id=\"MathJax-Span-835\" style=\"font-family: STIXGeneral-Regular;\">g</span></span></span><span class=\"mo\" id=\"MathJax-Span-836\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-837\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-838\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-839\"><span class=\"mrow\" id=\"MathJax-Span-840\"><span class=\"mo\" id=\"MathJax-Span-841\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-842\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">α</span><span class=\"mo\" id=\"MathJax-Span-843\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-844\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-845\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"texatom\" id=\"MathJax-Span-846\"><span class=\"mrow\" id=\"MathJax-Span-847\"><span class=\"mo\" id=\"MathJax-Span-848\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">(</span></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-849\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-850\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-851\"><span class=\"mrow\" id=\"MathJax-Span-852\"><span class=\"mo\" id=\"MathJax-Span-853\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-854\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">α</span><span class=\"mo\" id=\"MathJax-Span-855\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-856\" style=\"\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.919em, 1000.37em, 4.586em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-857\" style=\"\"><span class=\"mrow\" id=\"MathJax-Span-858\"><span class=\"mo\" id=\"MathJax-Span-859\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">)</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.685em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-860\"><span class=\"mrow\" id=\"MathJax-Span-861\"><span class=\"mo\" id=\"MathJax-Span-862\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-863\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-864\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-865\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"texatom\" id=\"MathJax-Span-866\" style=\"padding-left: 0.263em;\"><span class=\"mrow\" id=\"MathJax-Span-867\"><span class=\"mi\" id=\"MathJax-Span-868\" style=\"font-family: STIXGeneral; font-weight: bold;\">w</span></span></span><span class=\"mi\" id=\"MathJax-Span-869\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span class=\"mo\" id=\"MathJax-Span-870\" style=\"vertical-align: 1.148em;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -3.331em; left: 0em;\">‖<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -2.445em; left: 0em;\">‖<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"font-family: STIXGeneral-Regular; position: absolute; top: -2.862em; left: 0em;\">‖<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.841em; left: 13.753em;\"><span class=\"mn\" id=\"MathJax-Span-871\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.809em; border-left: 0px solid; width: 0px; height: 2.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">E</mi></mrow><mi>x</mi></msub><msup><mrow><mo symmetric=\"true\">‖</mo><mrow><mi>Q</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"bold\">w</mi></mrow><mo>⋅</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"normal\">d</mi><mi mathvariant=\"normal\">i</mi><mi mathvariant=\"normal\">a</mi><mi mathvariant=\"normal\">g</mi></mrow><mo stretchy=\"false\">(</mo><msup><mi>s</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>α</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-OPEN\"><mo maxsize=\"1.2em\" minsize=\"1.2em\">(</mo></mrow><msup><mi>s</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>α</mi><mo stretchy=\"false\">)</mo></mrow></msup><msup><mrow class=\"MJX-TeXAtom-CLOSE\"><mo maxsize=\"1.2em\" minsize=\"1.2em\">)</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup><mi>x</mi><mo>−</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"bold\">w</mi></mrow><mi>x</mi></mrow><mo symmetric=\"true\">‖</mo></mrow><mn>2</mn></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-70\">\\mathbb{E}_x \\left\\lVert Q(\\mathbf{w} \\cdot \\mathrm{diag}(s^{(\\alpha)}))\\bigl(s^{(\\alpha)}\\bigr)^{-1}x - \\mathbf{w}x \\right\\rVert^2</script>\n\n    <ul>\n      <li>Choose the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-71-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>&amp;#x03B1;</mi><mo>&amp;#x2217;</mo></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-872\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-873\"><span class=\"msubsup\" id=\"MathJax-Span-874\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-875\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-876\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>α</mi><mo>∗</mo></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-71\">\\alpha^*</script> that yields the lowest simulated error (no backprop required).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Scale, Quantize, and Fuse</strong></p>\n\n    <ul>\n      <li>Transform weights and activations as:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-72-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><msub><mi>w</mi><mi>i</mi></msub><mo>&amp;#x22C5;</mo><msubsup><mi>s</mi><mi>i</mi><mo>&amp;#x2217;</mo></msubsup><mo>,</mo><mspace width=&quot;1em&quot; /><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><msub><mi>x</mi><mi>i</mi></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><msubsup><mi>s</mi><mi>i</mi><mo>&amp;#x2217;</mo></msubsup><mspace width=&quot;1em&quot; /><mtext>such that</mtext><mspace width=&quot;1em&quot; /><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mi mathvariant=&quot;normal&quot;>&amp;#x22A4;</mi></msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mo>=</mo><msup><mi>w</mi><mi mathvariant=&quot;normal&quot;>&amp;#x22A4;</mi></msup><mi>x</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-877\" style=\"width: 24.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 20.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1020.78em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-878\"><span class=\"msubsup\" id=\"MathJax-Span-879\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-880\"><span class=\"mrow\" id=\"MathJax-Span-881\"><span class=\"munderover\" id=\"MathJax-Span-882\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-883\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.159em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-884\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-885\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-886\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-887\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-888\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-889\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-890\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-891\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-892\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.47em, 4.169em, -999.997em); top: -4.32em; left: 0.367em;\"><span class=\"mo\" id=\"MathJax-Span-893\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-894\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-895\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-896\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"msubsup\" id=\"MathJax-Span-897\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-898\"><span class=\"mrow\" id=\"MathJax-Span-899\"><span class=\"munderover\" id=\"MathJax-Span-900\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-901\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.107em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-902\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-903\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-904\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-905\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-906\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-907\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-908\"><span class=\"mrow\" id=\"MathJax-Span-909\"><span class=\"mo\" id=\"MathJax-Span-910\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"msubsup\" id=\"MathJax-Span-911\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-912\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.47em, 4.169em, -999.997em); top: -4.32em; left: 0.367em;\"><span class=\"mo\" id=\"MathJax-Span-913\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-914\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mspace\" id=\"MathJax-Span-915\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mtext\" id=\"MathJax-Span-916\" style=\"font-family: STIXGeneral-Regular;\">such that</span><span class=\"mspace\" id=\"MathJax-Span-917\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"msubsup\" id=\"MathJax-Span-918\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-919\"><span class=\"mrow\" id=\"MathJax-Span-920\"><span class=\"munderover\" id=\"MathJax-Span-921\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-922\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.159em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-923\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-924\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⊤</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-925\"><span class=\"mrow\" id=\"MathJax-Span-926\"><span class=\"munderover\" id=\"MathJax-Span-927\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-928\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.107em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-929\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-930\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-931\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-932\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-933\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⊤</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-934\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">~</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><msub><mi>w</mi><mi>i</mi></msub><mo>⋅</mo><msubsup><mi>s</mi><mi>i</mi><mo>∗</mo></msubsup><mo>,</mo><mspace width=\"1em\"></mspace><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">~</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><msub><mi>x</mi><mi>i</mi></msub><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><msubsup><mi>s</mi><mi>i</mi><mo>∗</mo></msubsup><mspace width=\"1em\"></mspace><mtext>such that</mtext><mspace width=\"1em\"></mspace><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">~</mo></mover></mrow><mi mathvariant=\"normal\">⊤</mi></msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo>=</mo><msup><mi>w</mi><mi mathvariant=\"normal\">⊤</mi></msup><mi>x</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-72\">\\tilde w_i = w_i \\cdot s_i^*, \\quad \\tilde x_i = x_i / s_i^* \\quad \\text{such that} \\quad \\tilde w^\\top \\tilde x = w^\\top x</script>\n\n    <ul>\n      <li>Apply <strong><code class=\"language-plaintext highlighter-rouge\">int4</code> or <code class=\"language-plaintext highlighter-rouge\">int3</code> group-wise quantization</strong> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-73-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-935\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-936\"><span class=\"texatom\" id=\"MathJax-Span-937\"><span class=\"mrow\" id=\"MathJax-Span-938\"><span class=\"munderover\" id=\"MathJax-Span-939\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-940\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.159em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-941\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">~</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-73\">\\tilde w</script> using a single scale per group.</li>\n      <li>Fuse the activation scaling <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-74-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>s</mi><mi>i</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-942\" style=\"width: 1.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1001.3em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-943\"><span class=\"msubsup\" id=\"MathJax-Span-944\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-945\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -4.477em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-946\"><span class=\"mrow\" id=\"MathJax-Span-947\"><span class=\"mo\" id=\"MathJax-Span-948\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-949\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-950\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>s</mi><mi>i</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-74\">s_i^{-1}</script> directly into the preceding layer normalization or linear layer, avoiding runtime rescaling and preserving inference speed by leveraging existing CUDA kernels from weight-only quantization libraries.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Deployment Optimization</strong></p>\n\n    <ul>\n      <li>AWQ pairs with <strong>TinyChat</strong>, an inference engine optimized for 4-bit weight-only transformers with kernel fusion and reorder-free dequantization.</li>\n      <li>Uses <strong>platform‑aware weight packing</strong> to maximize throughput on GPUs (observed ~3× speedup over Hugging Face <code class=\"language-plaintext highlighter-rouge\">float16</code> with negligible accuracy drop).</li>\n    </ul>\n  </li>\n</ol>\n<p><strong>Calibration Pass</strong></p>\n<ul>\n      <li>Run a <em>small calibration dataset</em> through the unquantized model to gather per-channel activation statistics, typically the expected absolute value <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>E</mi></mrow><mo stretchy=&quot;false&quot;>[</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-689\" style=\"width: 3.076em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1002.45em, 2.711em, -999.997em); top: -2.341em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-690\"><span class=\"texatom\" id=\"MathJax-Span-691\"><span class=\"mrow\" id=\"MathJax-Span-692\"><span class=\"mi\" id=\"MathJax-Span-693\" style=\"font-family: STIXGeneral-Regular;\">𝔼</span></span></span><span class=\"mo\" id=\"MathJax-Span-694\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mo\" id=\"MathJax-Span-695\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"msubsup\" id=\"MathJax-Span-696\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-697\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-698\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-699\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-700\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.346em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">E</mi></mrow><mo stretchy=\"false\">[</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence=\"false\" stretchy=\"false\">‖</mo><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">\\mathbb{E}[\\|x_i\\|]</script> of input to each weight channel <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-62-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-701\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-702\"><span class=\"mi\" id=\"MathJax-Span-703\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-62\">i</script>.</li>\n    </ul>\n<p><strong>Group-Wise Weight Quantization Baseline</strong></p>\n<ul>\n      <li>Use group size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-63-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-704\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-705\"><span class=\"mi\" id=\"MathJax-Span-706\" style=\"font-family: STIXGeneral-Italic;\">G</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-63\">G</script> (e.g. 32 channels) to quantize weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-64-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>w</mi></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-707\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.878em, 1000.73em, 2.659em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-708\"><span class=\"texatom\" id=\"MathJax-Span-709\"><span class=\"mrow\" id=\"MathJax-Span-710\"><span class=\"mi\" id=\"MathJax-Span-711\" style=\"font-family: STIXGeneral; font-weight: bold;\">w</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"bold\">w</mi></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-64\">\\mathbf{w}</script> via a uniform symmetric scheme:</li>\n    </ul>\n<ul>\n      <li>The quantization error in group-wise quantization is proportional to input activation magnitude rather than weight magnitude alone.</li>\n    </ul>\n<p><strong>Compute Activation-Based Scaling Factors</strong></p>\n<ul>\n      <li>Let <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-66-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>s</mi><mi>i</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo>=</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>E</mi></mrow><mi>x</mi></msub><mo stretchy=&quot;false&quot;>[</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-755\" style=\"width: 6.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.419em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1005.32em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-756\"><span class=\"msubsup\" id=\"MathJax-Span-757\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-758\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.89em, 4.273em, -999.997em); top: -4.477em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-759\"><span class=\"mrow\" id=\"MathJax-Span-760\"><span class=\"mo\" id=\"MathJax-Span-761\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-762\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-763\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-764\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-765\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-766\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-767\"><span class=\"mrow\" id=\"MathJax-Span-768\"><span class=\"mi\" id=\"MathJax-Span-769\" style=\"font-family: STIXGeneral-Regular;\">𝔼</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-770\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-771\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mo\" id=\"MathJax-Span-772\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"msubsup\" id=\"MathJax-Span-773\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-774\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-775\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-776\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mo\" id=\"MathJax-Span-777\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>s</mi><mi>i</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">E</mi></mrow><mi>x</mi></msub><mo stretchy=\"false\">[</mo><mo fence=\"false\" stretchy=\"false\">‖</mo><msub><mi>x</mi><mi>i</mi></msub><mo fence=\"false\" stretchy=\"false\">‖</mo><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-66\">s_i^{(x)} = \\mathbb{E}_x [\\|x_i\\|]</script> represent average per-channel activation magnitude.</li>\n      <li>For scaling exponent <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-67-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi><mo>&amp;#x2208;</mo><mo stretchy=&quot;false&quot;>[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-778\" style=\"width: 4.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.86em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-779\"><span class=\"mi\" id=\"MathJax-Span-780\" style=\"font-family: STIXGeneral-Italic;\">α</span><span class=\"mo\" id=\"MathJax-Span-781\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"mo\" id=\"MathJax-Span-782\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">[</span><span class=\"mn\" id=\"MathJax-Span-783\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-784\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-785\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">1</span><span class=\"mo\" id=\"MathJax-Span-786\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi><mo>∈</mo><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-67\">\\alpha \\in [0,1]</script>, define:</li>\n    </ul>\n<ul>\n      <li>Use a <strong>small grid search</strong> over <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-69-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-810\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-811\"><span class=\"mi\" id=\"MathJax-Span-812\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-69\">\\alpha</script> to minimize an approximate MSE:</li>\n    </ul>\n<ul>\n      <li>Choose the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-71-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>&amp;#x03B1;</mi><mo>&amp;#x2217;</mo></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-872\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-873\"><span class=\"msubsup\" id=\"MathJax-Span-874\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-875\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-876\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>α</mi><mo>∗</mo></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-71\">\\alpha^*</script> that yields the lowest simulated error (no backprop required).</li>\n    </ul>\n<p><strong>Scale, Quantize, and Fuse</strong></p>\n<ul>\n      <li>Transform weights and activations as:</li>\n    </ul>\n<ul>\n      <li>Apply <strong><code class=\"language-plaintext highlighter-rouge\">int4</code> or <code class=\"language-plaintext highlighter-rouge\">int3</code> group-wise quantization</strong> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-73-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-935\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-936\"><span class=\"texatom\" id=\"MathJax-Span-937\"><span class=\"mrow\" id=\"MathJax-Span-938\"><span class=\"munderover\" id=\"MathJax-Span-939\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-940\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.159em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-941\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">~</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-73\">\\tilde w</script> using a single scale per group.</li>\n      <li>Fuse the activation scaling <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-74-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>s</mi><mi>i</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-942\" style=\"width: 1.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1001.3em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-943\"><span class=\"msubsup\" id=\"MathJax-Span-944\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-945\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -4.477em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-946\"><span class=\"mrow\" id=\"MathJax-Span-947\"><span class=\"mo\" id=\"MathJax-Span-948\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-949\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-950\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>s</mi><mi>i</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-74\">s_i^{-1}</script> directly into the preceding layer normalization or linear layer, avoiding runtime rescaling and preserving inference speed by leveraging existing CUDA kernels from weight-only quantization libraries.</li>\n    </ul>\n<p><strong>Deployment Optimization</strong></p>\n<ul>\n      <li>AWQ pairs with <strong>TinyChat</strong>, an inference engine optimized for 4-bit weight-only transformers with kernel fusion and reorder-free dequantization.</li>\n      <li>Uses <strong>platform‑aware weight packing</strong> to maximize throughput on GPUs (observed ~3× speedup over Hugging Face <code class=\"language-plaintext highlighter-rouge\">float16</code> with negligible accuracy drop).</li>\n    </ul>\n<h5 id=\"pros-2\">Pros</h5>\n<ul>\n  <li><strong>Salient-channel preservation</strong>: By scaling up high-activation channels, AWQ protects the most influential weights using only ~1% additive precision, significantly reducing quantization error.</li>\n  <li><strong>Training‑less</strong>: Requires no finetuning or backpropagation—calibration and closed-form scaling search are sufficient, preserving generalization across domains including instruction-tuned and multi-modal LMs.</li>\n  <li><strong>Hardware‑efficient</strong>: Retains group-wise quantization kernels; activation rescaling is fused into existing linear or layer-norm layers, maintaining inference latency and memory efficiency.</li>\n</ul>\n<h5 id=\"cons-2\">Cons</h5>\n<ul>\n  <li><strong>Calibration dependency</strong>: Requires representative activation samples and search over <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-75-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B1;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-951\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-952\"><span class=\"mi\" id=\"MathJax-Span-953\" style=\"font-family: STIXGeneral-Italic;\">α</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>α</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-75\">\\alpha</script>, which adds one preprocessing pass but no training.</li>\n  <li><strong>Limited activation reduction</strong>: Activations are not quantized (typically kept in <code class=\"language-plaintext highlighter-rouge\">float16</code>), so runtime memory use is not halved.</li>\n  <li><strong>Architecture constraints</strong>: Fusion of scaling into preceding layernorm assumes alignment between weight input channels and layernorm channels; may require adaptation for custom architectures.</li>\n</ul>\n<h4 id=\"gguf-quantization-legacy-kquants-iquants\">GGUF Quantization (Legacy, K‑Quants, I‑Quants)</h4>\n<ul>\n  <li>\n    <p>GGUF is a binary format optimized for fast loading and saving of models, making it highly efficient for inference purposes. GGUF is designed for use with GGML and other executors and was developed by <a href=\"https://github.com/ggerganov\">@ggerganov</a>, who also created <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, a widely-used C/C++ LLM inference framework. Models trained in PyTorch or other frameworks can be quantized and converted to GGUF using community tools for deployment on low-resource hardware or CPU-only systems. A detailed summary of GGUF is available in <a href=\"https://www.reddit.com/r/LocalLlama/comments/1ba55rj/overview_of_gguf_quantization_methods/\">this</a> Reddit post.</p>\n  </li>\n  <li>\n    <p>The GGUF file stores tensors and metadata in a compact and readable format that supports a range of quantization methods including legacy quants, K-quants, and I-quants. Quantization blocks encode 256 weights each, along with minimal overhead (e.g., scale, zero-point, or LUT references), and are decoded efficiently during inference using architecture-specific kernels. The format also supports optional importance matrices and tokenizers directly embedded into the file, eliminating external dependencies during inference.</p>\n  </li>\n  <li>\n    <p>The following figure (<a href=\"https://huggingface.co/docs/hub/en/gguf\">source</a>) illustrates the internal structure of a GGUF model file, including the tensor and metadata layout:</p>\n  </li>\n</ul>\n<p>GGUF is a binary format optimized for fast loading and saving of models, making it highly efficient for inference purposes. GGUF is designed for use with GGML and other executors and was developed by <a href=\"https://github.com/ggerganov\">@ggerganov</a>, who also created <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, a widely-used C/C++ LLM inference framework. Models trained in PyTorch or other frameworks can be quantized and converted to GGUF using community tools for deployment on low-resource hardware or CPU-only systems. A detailed summary of GGUF is available in <a href=\"https://www.reddit.com/r/LocalLlama/comments/1ba55rj/overview_of_gguf_quantization_methods/\">this</a> Reddit post.</p>\n<p>The GGUF file stores tensors and metadata in a compact and readable format that supports a range of quantization methods including legacy quants, K-quants, and I-quants. Quantization blocks encode 256 weights each, along with minimal overhead (e.g., scale, zero-point, or LUT references), and are decoded efficiently during inference using architecture-specific kernels. The format also supports optional importance matrices and tokenizers directly embedded into the file, eliminating external dependencies during inference.</p>\n<p>The following figure (<a href=\"https://huggingface.co/docs/hub/en/gguf\">source</a>) illustrates the internal structure of a GGUF model file, including the tensor and metadata layout:</p>\n<p><img src=\"/primers/ai/assets/model-compression/GGUF.jpg\" alt=\"GGUF structure\"></p>\n<h5 id=\"quantization-types\">Quantization Types</h5>\n<ol>\n  <li>\n    <p><strong>Legacy Quants (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-76-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msub><mn>4</mn><mn>0</mn></msub><mo>,</mo><mi>Q</mi><msub><mn>4</mn><mn>1</mn></msub><mo>,</mo><mi>Q</mi><msub><mn>8</mn><mn>0</mn></msub><mo>,</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-954\" style=\"width: 7.441em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.149em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1006.1em, 2.534em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-955\"><span class=\"mi\" id=\"MathJax-Span-956\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-957\"><span style=\"display: inline-block; position: relative; width: 0.932em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.47em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-958\" style=\"font-family: STIXGeneral-Regular;\">4</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"mn\" id=\"MathJax-Span-959\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-960\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-961\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.209em;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-962\"><span style=\"display: inline-block; position: relative; width: 0.932em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.47em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-963\" style=\"font-family: STIXGeneral-Regular;\">4</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"mn\" id=\"MathJax-Span-964\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-965\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-966\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.209em;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-967\"><span style=\"display: inline-block; position: relative; width: 0.932em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.47em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-968\" style=\"font-family: STIXGeneral-Regular;\">8</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"mn\" id=\"MathJax-Span-969\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-970\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msub><mn>4</mn><mn>0</mn></msub><mo>,</mo><mi>Q</mi><msub><mn>4</mn><mn>1</mn></msub><mo>,</mo><mi>Q</mi><msub><mn>8</mn><mn>0</mn></msub><mo>,</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-76\">Q4_0, Q4_1, Q8_0,</script> etc.)</strong></p>\n\n    <ul>\n      <li>Basic block-based quantization where each 256-weight block is encoded with 4 or 8 bits per weight and one (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-77-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msub><mi>x</mi><mn>0</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-971\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.62em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-972\"><span class=\"mi\" id=\"MathJax-Span-973\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-974\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-975\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-976\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msub><mi>x</mi><mn>0</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-77\">Qx_0</script>) or two (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-78-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msub><mi>x</mi><mn>1</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-977\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.62em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-978\"><span class=\"mi\" id=\"MathJax-Span-979\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-980\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-981\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-982\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msub><mi>x</mi><mn>1</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-78\">Qx_1</script>) constants for scaling/offset.</li>\n      <li>Simple bit-unpacking operations (bit shift, AND, multiply) make these formats highly efficient for older hardware and platforms without vector acceleration.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>K-Quants (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-79-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msub><mn>3</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>K</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>S</mi></mrow></msub></mrow></msub><mo>,</mo><mi>Q</mi><msub><mn>5</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>K</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></msub></mrow></msub><mo>,</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-983\" style=\"width: 6.149em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.065em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1005.01em, 2.585em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-984\"><span class=\"mi\" id=\"MathJax-Span-985\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-986\"><span style=\"display: inline-block; position: relative; width: 1.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.42em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-987\" style=\"font-family: STIXGeneral-Regular;\">3</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"texatom\" id=\"MathJax-Span-988\"><span class=\"mrow\" id=\"MathJax-Span-989\"><span class=\"msubsup\" id=\"MathJax-Span-990\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.36em, 1000.52em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-991\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.871em; left: 0.467em;\"><span class=\"texatom\" id=\"MathJax-Span-992\"><span class=\"mrow\" id=\"MathJax-Span-993\"><span class=\"mi\" id=\"MathJax-Span-994\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-995\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-996\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.209em;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-997\"><span style=\"display: inline-block; position: relative; width: 1.552em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.42em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-998\" style=\"font-family: STIXGeneral-Regular;\">5</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"texatom\" id=\"MathJax-Span-999\"><span class=\"mrow\" id=\"MathJax-Span-1000\"><span class=\"msubsup\" id=\"MathJax-Span-1001\"><span style=\"display: inline-block; position: relative; width: 0.984em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.36em, 1000.52em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1002\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.871em; left: 0.467em;\"><span class=\"texatom\" id=\"MathJax-Span-1003\"><span class=\"mrow\" id=\"MathJax-Span-1004\"><span class=\"mi\" id=\"MathJax-Span-1005\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1006\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msub><mn>3</mn><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>K</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>S</mi></mrow></msub></mrow></msub><mo>,</mo><mi>Q</mi><msub><mn>5</mn><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>K</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>M</mi></mrow></msub></mrow></msub><mo>,</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-79\">Q3_{K_{S}}, Q5_{K_{M}},</script> etc.)</strong></p>\n\n    <ul>\n      <li>Smarter allocation of bits across layers or weight blocks, guided by internal heuristics or optional importance matrices.</li>\n      <li>Uses combinations of quantization levels in different layers (XS, S, M), optimizing performance-quality tradeoff.</li>\n      <li>Maintains speed advantages of legacy quants while improving model fidelity and reducing quantization noise.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>I-Quants (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-80-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>I</mi><mi>Q</mi><msub><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>X</mi><mi>X</mi><mi>S</mi></mrow></msub><mo>,</mo><mi>I</mi><mi>Q</mi><msub><mn>3</mn><mi>S</mi></msub><mo>,</mo><mi>I</mi><mi>Q</mi><msub><mn>4</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>X</mi><mi>S</mi></mrow></msub><mo>,</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1007\" style=\"width: 10.54em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.68em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1008.63em, 2.534em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1008\"><span class=\"mi\" id=\"MathJax-Span-1009\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1010\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-1011\"><span style=\"display: inline-block; position: relative; width: 1.862em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.47em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1012\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"texatom\" id=\"MathJax-Span-1013\"><span class=\"mrow\" id=\"MathJax-Span-1014\"><span class=\"mi\" id=\"MathJax-Span-1015\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1016\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1017\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1018\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1019\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.209em;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1020\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-1021\"><span style=\"display: inline-block; position: relative; width: 0.932em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.42em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1022\" style=\"font-family: STIXGeneral-Regular;\">3</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"mi\" id=\"MathJax-Span-1023\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1024\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1025\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.209em;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1026\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-1027\"><span style=\"display: inline-block; position: relative; width: 1.397em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.47em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1028\" style=\"font-family: STIXGeneral-Regular;\">4</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"texatom\" id=\"MathJax-Span-1029\"><span class=\"mrow\" id=\"MathJax-Span-1030\"><span class=\"mi\" id=\"MathJax-Span-1031\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1032\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1033\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>I</mi><mi>Q</mi><msub><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>X</mi><mi>X</mi><mi>S</mi></mrow></msub><mo>,</mo><mi>I</mi><mi>Q</mi><msub><mn>3</mn><mi>S</mi></msub><mo>,</mo><mi>I</mi><mi>Q</mi><msub><mn>4</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>X</mi><mi>S</mi></mrow></msub><mo>,</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-80\">IQ2_{XXS}, IQ3_S, IQ4_{XS},</script> etc.)</strong></p>\n\n    <ul>\n      <li>Advanced block-wise quantization using ideas from QuIP; includes lookup tables to store additional decoding values.</li>\n      <li>Allows lower bpw (2-4) while preserving model quality, especially useful for extremely memory-constrained inference.</li>\n      <li>Lookup-based dequantization introduces more compute overhead and can cause performance regressions on CPU-bound hardware.</li>\n    </ul>\n  </li>\n</ol>\n<p><strong>Legacy Quants (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-76-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msub><mn>4</mn><mn>0</mn></msub><mo>,</mo><mi>Q</mi><msub><mn>4</mn><mn>1</mn></msub><mo>,</mo><mi>Q</mi><msub><mn>8</mn><mn>0</mn></msub><mo>,</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-954\" style=\"width: 7.441em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.149em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1006.1em, 2.534em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-955\"><span class=\"mi\" id=\"MathJax-Span-956\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-957\"><span style=\"display: inline-block; position: relative; width: 0.932em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.47em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-958\" style=\"font-family: STIXGeneral-Regular;\">4</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"mn\" id=\"MathJax-Span-959\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-960\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-961\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.209em;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-962\"><span style=\"display: inline-block; position: relative; width: 0.932em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.47em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-963\" style=\"font-family: STIXGeneral-Regular;\">4</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"mn\" id=\"MathJax-Span-964\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-965\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-966\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.209em;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-967\"><span style=\"display: inline-block; position: relative; width: 0.932em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.47em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-968\" style=\"font-family: STIXGeneral-Regular;\">8</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"mn\" id=\"MathJax-Span-969\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-970\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msub><mn>4</mn><mn>0</mn></msub><mo>,</mo><mi>Q</mi><msub><mn>4</mn><mn>1</mn></msub><mo>,</mo><mi>Q</mi><msub><mn>8</mn><mn>0</mn></msub><mo>,</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-76\">Q4_0, Q4_1, Q8_0,</script> etc.)</strong></p>\n<ul>\n      <li>Basic block-based quantization where each 256-weight block is encoded with 4 or 8 bits per weight and one (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-77-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msub><mi>x</mi><mn>0</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-971\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.62em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-972\"><span class=\"mi\" id=\"MathJax-Span-973\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-974\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-975\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-976\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msub><mi>x</mi><mn>0</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-77\">Qx_0</script>) or two (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-78-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msub><mi>x</mi><mn>1</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-977\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.62em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-978\"><span class=\"mi\" id=\"MathJax-Span-979\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-980\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-981\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mn\" id=\"MathJax-Span-982\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msub><mi>x</mi><mn>1</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-78\">Qx_1</script>) constants for scaling/offset.</li>\n      <li>Simple bit-unpacking operations (bit shift, AND, multiply) make these formats highly efficient for older hardware and platforms without vector acceleration.</li>\n    </ul>\n<p><strong>K-Quants (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-79-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msub><mn>3</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>K</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>S</mi></mrow></msub></mrow></msub><mo>,</mo><mi>Q</mi><msub><mn>5</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>K</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></msub></mrow></msub><mo>,</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-983\" style=\"width: 6.149em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.065em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1005.01em, 2.585em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-984\"><span class=\"mi\" id=\"MathJax-Span-985\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-986\"><span style=\"display: inline-block; position: relative; width: 1.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.42em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-987\" style=\"font-family: STIXGeneral-Regular;\">3</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"texatom\" id=\"MathJax-Span-988\"><span class=\"mrow\" id=\"MathJax-Span-989\"><span class=\"msubsup\" id=\"MathJax-Span-990\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.36em, 1000.52em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-991\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.871em; left: 0.467em;\"><span class=\"texatom\" id=\"MathJax-Span-992\"><span class=\"mrow\" id=\"MathJax-Span-993\"><span class=\"mi\" id=\"MathJax-Span-994\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-995\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-996\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.209em;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-997\"><span style=\"display: inline-block; position: relative; width: 1.552em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.42em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-998\" style=\"font-family: STIXGeneral-Regular;\">5</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"texatom\" id=\"MathJax-Span-999\"><span class=\"mrow\" id=\"MathJax-Span-1000\"><span class=\"msubsup\" id=\"MathJax-Span-1001\"><span style=\"display: inline-block; position: relative; width: 0.984em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.36em, 1000.52em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1002\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.871em; left: 0.467em;\"><span class=\"texatom\" id=\"MathJax-Span-1003\"><span class=\"mrow\" id=\"MathJax-Span-1004\"><span class=\"mi\" id=\"MathJax-Span-1005\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1006\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msub><mn>3</mn><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>K</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>S</mi></mrow></msub></mrow></msub><mo>,</mo><mi>Q</mi><msub><mn>5</mn><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>K</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>M</mi></mrow></msub></mrow></msub><mo>,</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-79\">Q3_{K_{S}}, Q5_{K_{M}},</script> etc.)</strong></p>\n<ul>\n      <li>Smarter allocation of bits across layers or weight blocks, guided by internal heuristics or optional importance matrices.</li>\n      <li>Uses combinations of quantization levels in different layers (XS, S, M), optimizing performance-quality tradeoff.</li>\n      <li>Maintains speed advantages of legacy quants while improving model fidelity and reducing quantization noise.</li>\n    </ul>\n<p><strong>I-Quants (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-80-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>I</mi><mi>Q</mi><msub><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>X</mi><mi>X</mi><mi>S</mi></mrow></msub><mo>,</mo><mi>I</mi><mi>Q</mi><msub><mn>3</mn><mi>S</mi></msub><mo>,</mo><mi>I</mi><mi>Q</mi><msub><mn>4</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>X</mi><mi>S</mi></mrow></msub><mo>,</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1007\" style=\"width: 10.54em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.68em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1008.63em, 2.534em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1008\"><span class=\"mi\" id=\"MathJax-Span-1009\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1010\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-1011\"><span style=\"display: inline-block; position: relative; width: 1.862em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.47em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1012\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"texatom\" id=\"MathJax-Span-1013\"><span class=\"mrow\" id=\"MathJax-Span-1014\"><span class=\"mi\" id=\"MathJax-Span-1015\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1016\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1017\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1018\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1019\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.209em;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1020\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-1021\"><span style=\"display: inline-block; position: relative; width: 0.932em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.42em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1022\" style=\"font-family: STIXGeneral-Regular;\">3</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"mi\" id=\"MathJax-Span-1023\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1024\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1025\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.209em;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1026\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-1027\"><span style=\"display: inline-block; position: relative; width: 1.397em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.47em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1028\" style=\"font-family: STIXGeneral-Regular;\">4</span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"texatom\" id=\"MathJax-Span-1029\"><span class=\"mrow\" id=\"MathJax-Span-1030\"><span class=\"mi\" id=\"MathJax-Span-1031\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1032\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1033\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>I</mi><mi>Q</mi><msub><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>X</mi><mi>X</mi><mi>S</mi></mrow></msub><mo>,</mo><mi>I</mi><mi>Q</mi><msub><mn>3</mn><mi>S</mi></msub><mo>,</mo><mi>I</mi><mi>Q</mi><msub><mn>4</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>X</mi><mi>S</mi></mrow></msub><mo>,</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-80\">IQ2_{XXS}, IQ3_S, IQ4_{XS},</script> etc.)</strong></p>\n<ul>\n      <li>Advanced block-wise quantization using ideas from QuIP; includes lookup tables to store additional decoding values.</li>\n      <li>Allows lower bpw (2-4) while preserving model quality, especially useful for extremely memory-constrained inference.</li>\n      <li>Lookup-based dequantization introduces more compute overhead and can cause performance regressions on CPU-bound hardware.</li>\n    </ul>\n<h5 id=\"gguf-file-layout-and-execution\">GGUF File Layout and Execution</h5>\n<ul>\n  <li>Each GGUF file begins with a magic header and version indicator (<code class=\"language-plaintext highlighter-rouge\">0x47 0x47 0x55 0x46</code> for “GGUF”, currently version 3), followed by two 64-bit integers: the number of tensors and number of metadata key-value pairs.</li>\n  <li>Tensor definitions include name, shape, type, and byte offset. Supported quant types include formats like <code class=\"language-plaintext highlighter-rouge\">GGML_TYPE_Q2_K</code>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-81-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msub><mn>3</mn><mi>K</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1034\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.83em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1035\"><span class=\"mi\" id=\"MathJax-Span-1036\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-1037\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1038\" style=\"font-family: STIXGeneral-Regular;\">3</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-1039\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msub><mn>3</mn><mi>K</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-81\">Q3_K</script>, or <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-82-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>I</mi><mi>Q</mi><msub><mn>4</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>X</mi><mi>S</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1040\" style=\"width: 3.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.5em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1041\"><span class=\"mi\" id=\"MathJax-Span-1042\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1043\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-1044\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1045\" style=\"font-family: STIXGeneral-Regular;\">4</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1046\"><span class=\"mrow\" id=\"MathJax-Span-1047\"><span class=\"mi\" id=\"MathJax-Span-1048\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1049\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>I</mi><mi>Q</mi><msub><mn>4</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>X</mi><mi>S</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-82\">IQ4_{XS}</script>.</li>\n  <li>Metadata stores tokenizer info, architecture name, context length, and any quantization parameters used during export.</li>\n  <li>Tensors are read from disk at inference time via offset pointers—enabling partial loading or memory-mapped inference.</li>\n</ul>\n<h5 id=\"importance-matrix-imatrix\">Importance Matrix (Imatrix)</h5>\n<ul>\n  <li>An optional matrix that prioritizes preserving accuracy in weights deemed most significant based on a calibration pass.</li>\n  <li>Can be used with K-quants and legacy quants, not exclusive to I-quants.</li>\n  <li>Stored directly in the GGUF metadata and silently improves quantization quality with no inference-time cost.</li>\n</ul>\n<h5 id=\"pros-3\">Pros</h5>\n<ul>\n  <li><strong>Efficient deployment format</strong>: GGUF enables fast loading, lightweight inference, and portable packaging across platforms.</li>\n  <li><strong>Flexible quant schemes</strong>: From legacy-friendly <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-83-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msub><mn>8</mn><mn>0</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1050\" style=\"width: 2.034em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.67em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1051\"><span class=\"mi\" id=\"MathJax-Span-1052\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-1053\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1054\" style=\"font-family: STIXGeneral-Regular;\">8</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-1055\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msub><mn>8</mn><mn>0</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-83\">Q8_0</script> to ultra-compressed <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-84-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>I</mi><mi>Q</mi><msub><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>X</mi><mi>X</mi><mi>S</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1056\" style=\"width: 3.596em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.97em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1057\"><span class=\"mi\" id=\"MathJax-Span-1058\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1059\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-1060\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1061\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1062\"><span class=\"mrow\" id=\"MathJax-Span-1063\"><span class=\"mi\" id=\"MathJax-Span-1064\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1065\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1066\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>I</mi><mi>Q</mi><msub><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>X</mi><mi>X</mi><mi>S</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-84\">IQ2_{XXS}</script>, GGUF supports a wide range of bit-widths and precision tradeoffs.</li>\n  <li><strong>All-in-one packaging</strong>: Tokenizers, metadata, and importance matrices are embedded—no need for external configuration files.</li>\n  <li><strong>Community driven</strong>: Supported natively by <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> and increasingly integrated with Hugging Face tools and runners.</li>\n</ul>\n<h5 id=\"cons-3\">Cons</h5>\n<ul>\n  <li><strong>Hardware-specific behavior</strong>: Some quant schemes (especially I-quants) may perform suboptimally on older CPUs or non-VNNI hardware.</li>\n  <li><strong>Naming ambiguity</strong>: Quantization method and imatrix usage are not always visible in the filename; may require manual inspection or re-quantization.</li>\n  <li><strong>Rapid evolution</strong>: Format and tooling are evolving quickly—older GGUF models may need conversion to newer versions.</li>\n</ul>\n<h4 id=\"aweq-activationweight-equalization\">AWEQ: Activation‑Weight Equalization</h4>\n<ul>\n  <li>Introduced in <a href=\"https://arxiv.org/abs/2311.01305\">AWEQ: Post‑Training Quantization with Activation‑Weight Equalization for LLMs</a> (Nov 2023) by Li et al., AWEQ is a training‑free post‑training quantization technique designed to facilitate both ultra‑low‑bit and 8‑bit weight+activation (e.g., W8A8) quantization in large language models such as Llama and OPT. It works by shifting quantization hardness from activations to weights to reduce error.</li>\n  <li>AWEQ effectively balances activation and weight ranges channel-wise via <strong>per-channel equalization</strong> followed by <strong>uniform quantization</strong>, incorporating <strong>bias correction</strong> to reduce residual errors. It achieves significantly improved quantization accuracy—especially for W8A8 floating‑point alternatives—without any training or runtime slow-down, making it an excellent choice for production deployments requiring both efficiency and fidelity.</li>\n</ul>\n<h5 id=\"motivation\">Motivation</h5>\n<ul>\n  <li>Large‑scale LLM activations often exhibit <strong>long‑tailed per‑channel distributions</strong> with large outliers, making activation quantization challenging even at 8 bits. AWEQ addresses this by balancing activation and weight ranges so that differences in range (and therefore quantization difficulty) are harmonized channel‑wise, reducing wastage in the quantization grid and improving uniform quantization performance.</li>\n</ul>\n<h5 id=\"process-implementation-overview\">Process (Implementation Overview)</h5>\n<ol>\n  <li>\n    <p><strong>Channel Range Analysis</strong></p>\n\n    <ul>\n      <li>Run forward passes over a small calibration dataset to compute <strong>per‑channel activation range</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-85-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><msub><mo stretchy=&quot;false&quot;>)</mo><mi>i</mi></msub><mo>=</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1067\" style=\"width: 12.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1010.68em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1068\"><span class=\"mi\" id=\"MathJax-Span-1069\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1070\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1071\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-1072\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1073\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-1074\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1075\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-1076\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">max</span><span class=\"mo\" id=\"MathJax-Span-1077\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1078\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1079\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-1080\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1081\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1082\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mo\" id=\"MathJax-Span-1083\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">min</span><span class=\"mo\" id=\"MathJax-Span-1084\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1085\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1086\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-1087\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1088\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo stretchy=\"false\">(</mo><mi>X</mi><msub><mo stretchy=\"false\">)</mo><mi>i</mi></msub><mo>=</mo><mo movablelimits=\"true\" form=\"prefix\">max</mo><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>−</mo><mo movablelimits=\"true\" form=\"prefix\">min</mo><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-85\">r(X)_i = \\max(X_i) - \\min(X_i)</script> and <strong>weight range</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-86-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><mi>W</mi><msub><mo stretchy=&quot;false&quot;>)</mo><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1089\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.29em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1090\"><span class=\"mi\" id=\"MathJax-Span-1091\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1092\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1093\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-1094\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1095\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-1096\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo stretchy=\"false\">(</mo><mi>W</mi><msub><mo stretchy=\"false\">)</mo><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-86\">r(W)_i</script> for each linear or attention block tensor. Range refers to max minus min values across all elements in that channel.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Equalization Factor Computation</strong></p>\n\n    <ul>\n      <li>\n        <p>Compute a scale vector <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-87-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mi>C</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1097\" style=\"width: 3.544em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.92em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1098\"><span class=\"mi\" id=\"MathJax-Span-1099\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-1100\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1101\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1102\"><span class=\"mrow\" id=\"MathJax-Span-1103\"><span class=\"mi\" id=\"MathJax-Span-1104\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-1105\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mi>C</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-87\">s \\in \\mathbb{R}^C</script> to equalize ranges via:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-88-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>X</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><mfrac><msub><mi>X</mi><mi>i</mi></msub><msub><mi>s</mi><mi>i</mi></msub></mfrac><mo>,</mo><mspace width=&quot;1em&quot; /><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>W</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><msub><mi>s</mi><mi>i</mi></msub><mo>&amp;#x22C5;</mo><msub><mi>W</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1106\" style=\"width: 12.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.107em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.732em, 1010.11em, 3.232em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1107\"><span class=\"msubsup\" id=\"MathJax-Span-1108\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.971em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1109\"><span class=\"mrow\" id=\"MathJax-Span-1110\"><span class=\"munderover\" id=\"MathJax-Span-1111\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1112\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.211em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-1113\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-1114\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1115\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-1116\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.326em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.414em;\"><span class=\"msubsup\" id=\"MathJax-Span-1117\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1118\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-1119\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.68em, 4.326em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.31em;\"><span class=\"msubsup\" id=\"MathJax-Span-1120\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1121\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-1122\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.99em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.992em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1123\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1124\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"msubsup\" id=\"MathJax-Span-1125\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.971em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1126\"><span class=\"mrow\" id=\"MathJax-Span-1127\"><span class=\"munderover\" id=\"MathJax-Span-1128\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1129\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.367em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-1130\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.888em;\"><span class=\"mi\" id=\"MathJax-Span-1131\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1132\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-1133\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1134\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-1135\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1136\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1137\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1138\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1139\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.059em; border-left: 0px solid; width: 0px; height: 2.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>X</mi><mo stretchy=\"false\">~</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><mfrac><msub><mi>X</mi><mi>i</mi></msub><msub><mi>s</mi><mi>i</mi></msub></mfrac><mo>,</mo><mspace width=\"1em\"></mspace><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>W</mi><mo stretchy=\"false\">~</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><msub><mi>s</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>W</mi><mi>i</mi></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-88\">\\tilde{X}_i = \\frac{X_i}{s_i}, \\quad \\tilde{W}_i = s_i \\cdot W_i</script>\n\n        <ul>\n          <li>The objective is typically set such that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-89-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>X</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>W</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1140\" style=\"width: 6.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1005.32em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1141\"><span class=\"mi\" id=\"MathJax-Span-1142\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1143\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1144\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.971em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1145\"><span class=\"mrow\" id=\"MathJax-Span-1146\"><span class=\"munderover\" id=\"MathJax-Span-1147\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1148\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.211em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-1149\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-1150\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1151\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1152\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1153\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1154\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1155\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.971em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1156\"><span class=\"mrow\" id=\"MathJax-Span-1157\"><span class=\"munderover\" id=\"MathJax-Span-1158\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1159\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.367em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-1160\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.888em;\"><span class=\"mi\" id=\"MathJax-Span-1161\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1162\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo stretchy=\"false\">(</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>X</mi><mo stretchy=\"false\">~</mo></mover></mrow><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>r</mi><mo stretchy=\"false\">(</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>W</mi><mo stretchy=\"false\">~</mo></mover></mrow><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-89\">r(\\tilde{X}_i) = r(\\tilde{W}_i)</script> for all <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-90-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1163\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1164\"><span class=\"mi\" id=\"MathJax-Span-1165\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-90\">i</script>, thereby maximizing per‑channel quantization precision as defined through product of normalized ranges (see Equations 3–10 in the original text).</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Tensor Scaling (Fusion)</strong></p>\n\n    <ul>\n      <li>Apply channel‑wise scaling at the input boundaries of transformer modules—such as prior to self‑attention key/value/data and FFN layers.</li>\n      <li>Merge activation scaling into preceding layers (e.g., LayerNorm or Linear) to <strong>eliminate runtime overhead</strong>. For example, transform internal <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-91-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi><mo stretchy=&quot;false&quot;>&amp;#x2190;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;normal&quot;>d</mi><mi mathvariant=&quot;normal&quot;>i</mi><mi mathvariant=&quot;normal&quot;>a</mi><mi mathvariant=&quot;normal&quot;>g</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>s</mi><mo stretchy=&quot;false&quot;>)</mo><mspace width=&quot;thinmathspace&quot; /><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1166\" style=\"width: 7.971em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.62em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1167\"><span class=\"mi\" id=\"MathJax-Span-1168\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1169\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">←</span><span class=\"texatom\" id=\"MathJax-Span-1170\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-1171\"><span class=\"mi\" id=\"MathJax-Span-1172\" style=\"font-family: STIXGeneral-Regular;\">d</span><span class=\"mi\" id=\"MathJax-Span-1173\" style=\"font-family: STIXGeneral-Regular;\">i</span><span class=\"mi\" id=\"MathJax-Span-1174\" style=\"font-family: STIXGeneral-Regular;\">a</span><span class=\"mi\" id=\"MathJax-Span-1175\" style=\"font-family: STIXGeneral-Regular;\">g</span></span></span><span class=\"mo\" id=\"MathJax-Span-1176\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1177\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-1178\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mspace\" id=\"MathJax-Span-1179\" style=\"height: 0em; vertical-align: 0em; width: 0.211em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-1180\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi><mo stretchy=\"false\">←</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"normal\">d</mi><mi mathvariant=\"normal\">i</mi><mi mathvariant=\"normal\">a</mi><mi mathvariant=\"normal\">g</mi></mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo><mspace width=\"thinmathspace\"></mspace><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-91\">W \\leftarrow \\mathrm{diag}(s) \\, W</script>; hence, activations use quantizable ranges without additional scaling logic.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Uniform Quantization</strong></p>\n\n    <ul>\n      <li>Quantize the equalized tensors using <strong>per‑tensor uniform affine quantization</strong> (e.g., 4‑bit or 8‑bit symmetric). Activation quantization thresholds can be fused with the input block for efficient inference.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Quantization Bias Correction (BC)</strong></p>\n\n    <ul>\n      <li>Because quantization after scaling and symmetric clipping can introduce a bias <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-92-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03F5;</mi><mo>=</mo><msub><mi>W</mi><mi>f</mi></msub><mo>&amp;#x2212;</mo><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1181\" style=\"width: 5.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.898em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.9em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1182\"><span class=\"mi\" id=\"MathJax-Span-1183\" style=\"font-family: STIXGeneral-Italic;\">ϵ</span><span class=\"mo\" id=\"MathJax-Span-1184\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-1185\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1186\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1187\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.107em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1188\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-1189\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ϵ</mi><mo>=</mo><msub><mi>W</mi><mi>f</mi></msub><mo>−</mo><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-92\">\\epsilon = W_f - W</script>, AWEQ applies post‑hoc <strong>bias correction</strong>:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-93-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mo>=</mo><msub><mi>y</mi><mi>e</mi></msub><mo>&amp;#x2212;</mo><mi>&amp;#x03F5;</mi><mo>&amp;#x22C5;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>E</mi></mrow><mo stretchy=&quot;false&quot;>[</mo><mi>x</mi><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1190\" style=\"width: 8.076em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.721em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1006.62em, 2.711em, -999.997em); top: -2.341em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1191\"><span class=\"texatom\" id=\"MathJax-Span-1192\"><span class=\"mrow\" id=\"MathJax-Span-1193\"><span class=\"munderover\" id=\"MathJax-Span-1194\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1195\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.055em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-1196\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1197\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-1198\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1199\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1200\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1201\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-1202\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">ϵ</span><span class=\"mo\" id=\"MathJax-Span-1203\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"texatom\" id=\"MathJax-Span-1204\" style=\"padding-left: 0.263em;\"><span class=\"mrow\" id=\"MathJax-Span-1205\"><span class=\"mi\" id=\"MathJax-Span-1206\" style=\"font-family: STIXGeneral-Regular;\">𝔼</span></span></span><span class=\"mo\" id=\"MathJax-Span-1207\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mi\" id=\"MathJax-Span-1208\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1209\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.346em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">~</mo></mover></mrow><mo>=</mo><msub><mi>y</mi><mi>e</mi></msub><mo>−</mo><mi>ϵ</mi><mo>⋅</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">E</mi></mrow><mo stretchy=\"false\">[</mo><mi>x</mi><mo stretchy=\"false\">]</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-93\">\\tilde{y} = y_e - \\epsilon \\cdot \\mathbb{E}[x]</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-94-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>E</mi></mrow><mo stretchy=&quot;false&quot;>[</mo><mi>x</mi><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1210\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1001.67em, 2.659em, -999.997em); top: -2.341em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1211\"><span class=\"texatom\" id=\"MathJax-Span-1212\"><span class=\"mrow\" id=\"MathJax-Span-1213\"><span class=\"mi\" id=\"MathJax-Span-1214\" style=\"font-family: STIXGeneral-Regular;\">𝔼</span></span></span><span class=\"mo\" id=\"MathJax-Span-1215\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mi\" id=\"MathJax-Span-1216\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1217\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.346em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">E</mi></mrow><mo stretchy=\"false\">[</mo><mi>x</mi><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-94\">\\mathbb{E}[x]</script> is estimated over calibration data, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-95-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mi>e</mi></msub><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mi>W</mi><mo>+</mo><mi>&amp;#x03F5;</mi><mo stretchy=&quot;false&quot;>)</mo><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1218\" style=\"width: 6.826em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.68em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1219\"><span class=\"msubsup\" id=\"MathJax-Span-1220\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1221\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1222\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1223\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-1224\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mi\" id=\"MathJax-Span-1225\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1226\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-1227\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">ϵ</span><span class=\"mo\" id=\"MathJax-Span-1228\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mi\" id=\"MathJax-Span-1229\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mi>e</mi></msub><mo>=</mo><mo stretchy=\"false\">(</mo><mi>W</mi><mo>+</mo><mi>ϵ</mi><mo stretchy=\"false\">)</mo><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-95\">y_e = (W + \\epsilon)x</script>. This corrects the expected error per layer without changing runtime performance, enhancing stability in deep LLMs without BatchNorm.</li>\n    </ul>\n  </li>\n</ol>\n<p><strong>Channel Range Analysis</strong></p>\n<ul>\n      <li>Run forward passes over a small calibration dataset to compute <strong>per‑channel activation range</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-85-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><msub><mo stretchy=&quot;false&quot;>)</mo><mi>i</mi></msub><mo>=</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1067\" style=\"width: 12.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1010.68em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1068\"><span class=\"mi\" id=\"MathJax-Span-1069\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1070\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1071\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-1072\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1073\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-1074\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1075\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-1076\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">max</span><span class=\"mo\" id=\"MathJax-Span-1077\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1078\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1079\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-1080\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1081\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1082\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mo\" id=\"MathJax-Span-1083\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">min</span><span class=\"mo\" id=\"MathJax-Span-1084\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1085\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1086\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-1087\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1088\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo stretchy=\"false\">(</mo><mi>X</mi><msub><mo stretchy=\"false\">)</mo><mi>i</mi></msub><mo>=</mo><mo movablelimits=\"true\" form=\"prefix\">max</mo><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>−</mo><mo movablelimits=\"true\" form=\"prefix\">min</mo><mo stretchy=\"false\">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-85\">r(X)_i = \\max(X_i) - \\min(X_i)</script> and <strong>weight range</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-86-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><mi>W</mi><msub><mo stretchy=&quot;false&quot;>)</mo><mi>i</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1089\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.29em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1090\"><span class=\"mi\" id=\"MathJax-Span-1091\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1092\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1093\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-1094\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.26em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1095\" style=\"font-family: STIXGeneral-Regular;\">)</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"mi\" id=\"MathJax-Span-1096\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo stretchy=\"false\">(</mo><mi>W</mi><msub><mo stretchy=\"false\">)</mo><mi>i</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-86\">r(W)_i</script> for each linear or attention block tensor. Range refers to max minus min values across all elements in that channel.</li>\n    </ul>\n<p><strong>Equalization Factor Computation</strong></p>\n<ul>\n      <li>\n        <p>Compute a scale vector <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-87-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mi>C</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1097\" style=\"width: 3.544em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.92em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1098\"><span class=\"mi\" id=\"MathJax-Span-1099\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-1100\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1101\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1102\"><span class=\"mrow\" id=\"MathJax-Span-1103\"><span class=\"mi\" id=\"MathJax-Span-1104\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-1105\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mi>C</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-87\">s \\in \\mathbb{R}^C</script> to equalize ranges via:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-88-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>X</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><mfrac><msub><mi>X</mi><mi>i</mi></msub><msub><mi>s</mi><mi>i</mi></msub></mfrac><mo>,</mo><mspace width=&quot;1em&quot; /><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>W</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><msub><mi>s</mi><mi>i</mi></msub><mo>&amp;#x22C5;</mo><msub><mi>W</mi><mi>i</mi></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1106\" style=\"width: 12.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.107em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.732em, 1010.11em, 3.232em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1107\"><span class=\"msubsup\" id=\"MathJax-Span-1108\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.971em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1109\"><span class=\"mrow\" id=\"MathJax-Span-1110\"><span class=\"munderover\" id=\"MathJax-Span-1111\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1112\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.211em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-1113\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-1114\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1115\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-1116\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.326em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.414em;\"><span class=\"msubsup\" id=\"MathJax-Span-1117\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1118\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-1119\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.68em, 4.326em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.31em;\"><span class=\"msubsup\" id=\"MathJax-Span-1120\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1121\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-1122\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.99em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.992em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1123\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1124\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"msubsup\" id=\"MathJax-Span-1125\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.971em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1126\"><span class=\"mrow\" id=\"MathJax-Span-1127\"><span class=\"munderover\" id=\"MathJax-Span-1128\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1129\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.367em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-1130\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.888em;\"><span class=\"mi\" id=\"MathJax-Span-1131\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1132\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-1133\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1134\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-1135\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1136\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1137\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1138\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1139\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.059em; border-left: 0px solid; width: 0px; height: 2.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>X</mi><mo stretchy=\"false\">~</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><mfrac><msub><mi>X</mi><mi>i</mi></msub><msub><mi>s</mi><mi>i</mi></msub></mfrac><mo>,</mo><mspace width=\"1em\"></mspace><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>W</mi><mo stretchy=\"false\">~</mo></mover></mrow><mi>i</mi></msub><mo>=</mo><msub><mi>s</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>W</mi><mi>i</mi></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-88\">\\tilde{X}_i = \\frac{X_i}{s_i}, \\quad \\tilde{W}_i = s_i \\cdot W_i</script>\n\n        <ul>\n          <li>The objective is typically set such that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-89-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>X</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>W</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1140\" style=\"width: 6.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1005.32em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1141\"><span class=\"mi\" id=\"MathJax-Span-1142\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1143\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1144\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.971em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1145\"><span class=\"mrow\" id=\"MathJax-Span-1146\"><span class=\"munderover\" id=\"MathJax-Span-1147\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1148\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.211em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-1149\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-1150\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1151\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1152\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1153\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1154\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1155\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.971em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1156\"><span class=\"mrow\" id=\"MathJax-Span-1157\"><span class=\"munderover\" id=\"MathJax-Span-1158\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1159\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.367em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-1160\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.888em;\"><span class=\"mi\" id=\"MathJax-Span-1161\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1162\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo stretchy=\"false\">(</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>X</mi><mo stretchy=\"false\">~</mo></mover></mrow><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>r</mi><mo stretchy=\"false\">(</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>W</mi><mo stretchy=\"false\">~</mo></mover></mrow><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-89\">r(\\tilde{X}_i) = r(\\tilde{W}_i)</script> for all <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-90-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1163\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1164\"><span class=\"mi\" id=\"MathJax-Span-1165\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-90\">i</script>, thereby maximizing per‑channel quantization precision as defined through product of normalized ranges (see Equations 3–10 in the original text).</li>\n        </ul>\n      </li>\n    </ul>\n<p>Compute a scale vector <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-87-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mi>C</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1097\" style=\"width: 3.544em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.92em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1098\"><span class=\"mi\" id=\"MathJax-Span-1099\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-1100\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1101\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1102\"><span class=\"mrow\" id=\"MathJax-Span-1103\"><span class=\"mi\" id=\"MathJax-Span-1104\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-1105\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mi>C</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-87\">s \\in \\mathbb{R}^C</script> to equalize ranges via:</p>\n<ul>\n          <li>The objective is typically set such that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-89-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>X</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>W</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mi>i</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1140\" style=\"width: 6.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1005.32em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1141\"><span class=\"mi\" id=\"MathJax-Span-1142\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1143\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1144\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.971em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1145\"><span class=\"mrow\" id=\"MathJax-Span-1146\"><span class=\"munderover\" id=\"MathJax-Span-1147\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1148\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.211em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-1149\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-1150\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1151\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1152\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1153\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1154\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1155\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.971em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1156\"><span class=\"mrow\" id=\"MathJax-Span-1157\"><span class=\"munderover\" id=\"MathJax-Span-1158\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1159\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.367em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-1160\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.888em;\"><span class=\"mi\" id=\"MathJax-Span-1161\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1162\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo stretchy=\"false\">(</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>X</mi><mo stretchy=\"false\">~</mo></mover></mrow><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>r</mi><mo stretchy=\"false\">(</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>W</mi><mo stretchy=\"false\">~</mo></mover></mrow><mi>i</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-89\">r(\\tilde{X}_i) = r(\\tilde{W}_i)</script> for all <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-90-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1163\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1164\"><span class=\"mi\" id=\"MathJax-Span-1165\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-90\">i</script>, thereby maximizing per‑channel quantization precision as defined through product of normalized ranges (see Equations 3–10 in the original text).</li>\n        </ul>\n<p><strong>Tensor Scaling (Fusion)</strong></p>\n<ul>\n      <li>Apply channel‑wise scaling at the input boundaries of transformer modules—such as prior to self‑attention key/value/data and FFN layers.</li>\n      <li>Merge activation scaling into preceding layers (e.g., LayerNorm or Linear) to <strong>eliminate runtime overhead</strong>. For example, transform internal <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-91-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi><mo stretchy=&quot;false&quot;>&amp;#x2190;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;normal&quot;>d</mi><mi mathvariant=&quot;normal&quot;>i</mi><mi mathvariant=&quot;normal&quot;>a</mi><mi mathvariant=&quot;normal&quot;>g</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>s</mi><mo stretchy=&quot;false&quot;>)</mo><mspace width=&quot;thinmathspace&quot; /><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1166\" style=\"width: 7.971em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.62em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1167\"><span class=\"mi\" id=\"MathJax-Span-1168\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1169\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">←</span><span class=\"texatom\" id=\"MathJax-Span-1170\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-1171\"><span class=\"mi\" id=\"MathJax-Span-1172\" style=\"font-family: STIXGeneral-Regular;\">d</span><span class=\"mi\" id=\"MathJax-Span-1173\" style=\"font-family: STIXGeneral-Regular;\">i</span><span class=\"mi\" id=\"MathJax-Span-1174\" style=\"font-family: STIXGeneral-Regular;\">a</span><span class=\"mi\" id=\"MathJax-Span-1175\" style=\"font-family: STIXGeneral-Regular;\">g</span></span></span><span class=\"mo\" id=\"MathJax-Span-1176\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1177\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-1178\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mspace\" id=\"MathJax-Span-1179\" style=\"height: 0em; vertical-align: 0em; width: 0.211em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-1180\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi><mo stretchy=\"false\">←</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"normal\">d</mi><mi mathvariant=\"normal\">i</mi><mi mathvariant=\"normal\">a</mi><mi mathvariant=\"normal\">g</mi></mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo><mspace width=\"thinmathspace\"></mspace><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-91\">W \\leftarrow \\mathrm{diag}(s) \\, W</script>; hence, activations use quantizable ranges without additional scaling logic.</li>\n    </ul>\n<p><strong>Uniform Quantization</strong></p>\n<ul>\n      <li>Quantize the equalized tensors using <strong>per‑tensor uniform affine quantization</strong> (e.g., 4‑bit or 8‑bit symmetric). Activation quantization thresholds can be fused with the input block for efficient inference.</li>\n    </ul>\n<p><strong>Quantization Bias Correction (BC)</strong></p>\n<ul>\n      <li>Because quantization after scaling and symmetric clipping can introduce a bias <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-92-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03F5;</mi><mo>=</mo><msub><mi>W</mi><mi>f</mi></msub><mo>&amp;#x2212;</mo><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1181\" style=\"width: 5.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.898em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.9em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1182\"><span class=\"mi\" id=\"MathJax-Span-1183\" style=\"font-family: STIXGeneral-Italic;\">ϵ</span><span class=\"mo\" id=\"MathJax-Span-1184\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-1185\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1186\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-1187\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.107em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1188\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-1189\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ϵ</mi><mo>=</mo><msub><mi>W</mi><mi>f</mi></msub><mo>−</mo><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-92\">\\epsilon = W_f - W</script>, AWEQ applies post‑hoc <strong>bias correction</strong>:</li>\n    </ul>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-94-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>E</mi></mrow><mo stretchy=&quot;false&quot;>[</mo><mi>x</mi><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1210\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1001.67em, 2.659em, -999.997em); top: -2.341em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1211\"><span class=\"texatom\" id=\"MathJax-Span-1212\"><span class=\"mrow\" id=\"MathJax-Span-1213\"><span class=\"mi\" id=\"MathJax-Span-1214\" style=\"font-family: STIXGeneral-Regular;\">𝔼</span></span></span><span class=\"mo\" id=\"MathJax-Span-1215\" style=\"font-family: STIXGeneral-Regular;\">[</span><span class=\"mi\" id=\"MathJax-Span-1216\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1217\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.346em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">E</mi></mrow><mo stretchy=\"false\">[</mo><mi>x</mi><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-94\">\\mathbb{E}[x]</script> is estimated over calibration data, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-95-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mi>e</mi></msub><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mi>W</mi><mo>+</mo><mi>&amp;#x03F5;</mi><mo stretchy=&quot;false&quot;>)</mo><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1218\" style=\"width: 6.826em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.68em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1219\"><span class=\"msubsup\" id=\"MathJax-Span-1220\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1221\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1222\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1223\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-1224\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mi\" id=\"MathJax-Span-1225\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1226\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-1227\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">ϵ</span><span class=\"mo\" id=\"MathJax-Span-1228\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mi\" id=\"MathJax-Span-1229\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mi>e</mi></msub><mo>=</mo><mo stretchy=\"false\">(</mo><mi>W</mi><mo>+</mo><mi>ϵ</mi><mo stretchy=\"false\">)</mo><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-95\">y_e = (W + \\epsilon)x</script>. This corrects the expected error per layer without changing runtime performance, enhancing stability in deep LLMs without BatchNorm.</li>\n    </ul>\n<h5 id=\"pros-4\">Pros</h5>\n<ul>\n  <li><strong>Training‑free</strong>, with no need for quantization-aware training or gradient-based fine-tuning.</li>\n  <li>Supports both <strong>W8A8 activation quantization</strong> and <strong>ultra‑low-bit weight-only quantization</strong>, including <code class=\"language-plaintext highlighter-rouge\">int/4</code> with robust performance.</li>\n  <li><strong>Hardware-friendly</strong>, as it avoids dynamic scaling during inference; changes are statically fused before deployment.</li>\n  <li>Demonstrates <strong>best-in-class accuracy</strong> on tasks such as zero‑shot Llama 2 7B evaluation (e.g., average: 70.38% over PIQA, HellaSwag, WinoGrande, ARC‑e—all within &lt;0.01 absolute from <code class=\"language-plaintext highlighter-rouge\">float16</code>).</li>\n</ul>\n<h5 id=\"cons-4\">Cons</h5>\n<ul>\n  <li>Requires <strong>representative calibration data</strong> to compute statistics and activation range profiles.</li>\n  <li><strong>Per‑tensor quantization</strong> may not perform as well as per‑channel for certain weight distributions, though the equalization helps mitigate this.</li>\n  <li>Slight overhead in computing equalization factors and bias expectations at quantization time (calibration phase only).</li>\n</ul>\n<h4 id=\"exl2-quantization\">EXL2 Quantization</h4>\n<ul>\n  <li>\n    <p><a href=\"https://github.com/turboderp-org/exllamav2#exl2-quantization\">ExLlamaV2</a>, commonly known as EXL2, is a flexible, weight-only quantization scheme developed specifically for local inference of large language models on consumer GPUs. It supports mixed-precision quantization with bit-widths from 2 to 8 bits, and can dynamically allocate precision per weight group to optimize model accuracy at a target average bitrate. This makes it suitable for extreme compression of LLMs such as Llama2-70B, enabling execution on GPUs with as little as 24 GB VRAM.</p>\n  </li>\n  <li>\n    <p>EXL2 builds upon the GPTQ framework but introduces finer-grained control over quantization allocation, using an error-minimization strategy driven by calibration data. Unlike uniform quantization, EXL2 allows important weights to retain higher precision while compressing less critical ones more aggressively. This is implemented without significant performance penalties due to tight integration with ExLlama’s inference engine and CUDA backend.</p>\n  </li>\n</ul>\n<p><a href=\"https://github.com/turboderp-org/exllamav2#exl2-quantization\">ExLlamaV2</a>, commonly known as EXL2, is a flexible, weight-only quantization scheme developed specifically for local inference of large language models on consumer GPUs. It supports mixed-precision quantization with bit-widths from 2 to 8 bits, and can dynamically allocate precision per weight group to optimize model accuracy at a target average bitrate. This makes it suitable for extreme compression of LLMs such as Llama2-70B, enabling execution on GPUs with as little as 24 GB VRAM.</p>\n<p>EXL2 builds upon the GPTQ framework but introduces finer-grained control over quantization allocation, using an error-minimization strategy driven by calibration data. Unlike uniform quantization, EXL2 allows important weights to retain higher precision while compressing less critical ones more aggressively. This is implemented without significant performance penalties due to tight integration with ExLlama’s inference engine and CUDA backend.</p>\n<h5 id=\"process-3\">Process</h5>\n<ol>\n  <li>\n    <p><strong>Calibration and Error Evaluation</strong>:</p>\n\n    <ul>\n      <li>Begin by passing a small calibration dataset through the model to obtain representative statistics.</li>\n      <li>For each linear layer weight matrix, the EXL2 pipeline quantizes the matrix multiple times using different bit-width configurations (e.g., 2, 3, 4, 5, 6, or 8 bits).</li>\n      <li>After each quantization trial, compute the quantization error between the original and quantized matrix multiplied by the calibration input. The maximum per-layer error across all trials is tracked.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Bitrate-Constrained Optimization</strong>:</p>\n\n    <ul>\n      <li>A greedy or grid-based search selects the bit-width assignment that minimizes the <strong>maximum layer-wise error</strong> while satisfying a user-defined <strong>average bitrate target</strong> (e.g., 2.55 bits per weight).</li>\n      <li>This allows for <strong>non-uniform quantization within each matrix</strong>, so important rows or columns (typically corresponding to high-magnitude weights or activations) may receive higher precision.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Mixed-Bit Packing and Storage Format</strong>:</p>\n\n    <ul>\n      <li>Each matrix is stored in a compact format supporting mixed-bit representation. A metadata structure encodes the bit-width used for each group.</li>\n      <li>Group size is typically fixed (e.g., 64 or 128), enabling compatibility with blockwise CUDA kernels.</li>\n      <li>The storage layout ensures efficient memory access and can be interpreted directly by ExLlama’s fast inference kernels.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Inference Support</strong>:</p>\n\n    <ul>\n      <li>At runtime, ExLlamaV2 uses custom CUDA kernels capable of unpacking and computing with mixed-bit quantized weights.</li>\n      <li>There is no need for runtime dequantization to full precision—matmul and sampling are done directly on quantized values.</li>\n      <li>The system also supports <strong>act-order remapping</strong>, allowing reordering of weight matrices to preserve activation alignment in grouped attention layers, which is important for compatibility with GQA architectures and inference speed.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Conversion Pipeline</strong>:</p>\n\n    <ul>\n      <li>\n        <p>A command-line <a href=\"https://github.com/turboderp-org/exllamav2?tab=readme-ov-file#exl2-quantization\">script</a> is provided to convert Hugging Face-format models into EXL2 quantized versions. This script includes:</p>\n\n        <ul>\n          <li>Automatic bit-width search using calibration data.</li>\n          <li>Weight remapping and act-order alignment.</li>\n          <li>Storage into a compact format suitable for ExLlamaV2.</li>\n        </ul>\n      </li>\n      <li>\n        <p>Conversion is computationally intensive, especially for large models, but only needs to be done once.</p>\n      </li>\n    </ul>\n  </li>\n</ol>\n<p><strong>Calibration and Error Evaluation</strong>:</p>\n<ul>\n      <li>Begin by passing a small calibration dataset through the model to obtain representative statistics.</li>\n      <li>For each linear layer weight matrix, the EXL2 pipeline quantizes the matrix multiple times using different bit-width configurations (e.g., 2, 3, 4, 5, 6, or 8 bits).</li>\n      <li>After each quantization trial, compute the quantization error between the original and quantized matrix multiplied by the calibration input. The maximum per-layer error across all trials is tracked.</li>\n    </ul>\n<p><strong>Bitrate-Constrained Optimization</strong>:</p>\n<ul>\n      <li>A greedy or grid-based search selects the bit-width assignment that minimizes the <strong>maximum layer-wise error</strong> while satisfying a user-defined <strong>average bitrate target</strong> (e.g., 2.55 bits per weight).</li>\n      <li>This allows for <strong>non-uniform quantization within each matrix</strong>, so important rows or columns (typically corresponding to high-magnitude weights or activations) may receive higher precision.</li>\n    </ul>\n<p><strong>Mixed-Bit Packing and Storage Format</strong>:</p>\n<ul>\n      <li>Each matrix is stored in a compact format supporting mixed-bit representation. A metadata structure encodes the bit-width used for each group.</li>\n      <li>Group size is typically fixed (e.g., 64 or 128), enabling compatibility with blockwise CUDA kernels.</li>\n      <li>The storage layout ensures efficient memory access and can be interpreted directly by ExLlama’s fast inference kernels.</li>\n    </ul>\n<p><strong>Inference Support</strong>:</p>\n<ul>\n      <li>At runtime, ExLlamaV2 uses custom CUDA kernels capable of unpacking and computing with mixed-bit quantized weights.</li>\n      <li>There is no need for runtime dequantization to full precision—matmul and sampling are done directly on quantized values.</li>\n      <li>The system also supports <strong>act-order remapping</strong>, allowing reordering of weight matrices to preserve activation alignment in grouped attention layers, which is important for compatibility with GQA architectures and inference speed.</li>\n    </ul>\n<p><strong>Conversion Pipeline</strong>:</p>\n<ul>\n      <li>\n        <p>A command-line <a href=\"https://github.com/turboderp-org/exllamav2?tab=readme-ov-file#exl2-quantization\">script</a> is provided to convert Hugging Face-format models into EXL2 quantized versions. This script includes:</p>\n\n        <ul>\n          <li>Automatic bit-width search using calibration data.</li>\n          <li>Weight remapping and act-order alignment.</li>\n          <li>Storage into a compact format suitable for ExLlamaV2.</li>\n        </ul>\n      </li>\n      <li>\n        <p>Conversion is computationally intensive, especially for large models, but only needs to be done once.</p>\n      </li>\n    </ul>\n<p>A command-line <a href=\"https://github.com/turboderp-org/exllamav2?tab=readme-ov-file#exl2-quantization\">script</a> is provided to convert Hugging Face-format models into EXL2 quantized versions. This script includes:</p>\n<ul>\n          <li>Automatic bit-width search using calibration data.</li>\n          <li>Weight remapping and act-order alignment.</li>\n          <li>Storage into a compact format suitable for ExLlamaV2.</li>\n        </ul>\n<p>Conversion is computationally intensive, especially for large models, but only needs to be done once.</p>\n<h5 id=\"pros-5\">Pros</h5>\n<ul>\n  <li><strong>Extreme compression</strong>: Achieves ultra-low bitrates (e.g., 2.5–3.0 bpw) without retraining, enabling 70B models to run on 24 GB GPUs.</li>\n  <li><strong>Layer-aware precision allocation</strong>: Allocates bits where they matter most, reducing perceptual degradation in output quality.</li>\n  <li><strong>Performance-friendly</strong>: Designed for fast execution with minimal overhead through mixed-bit CUDA kernels.</li>\n  <li><strong>Flexible deployment</strong>: Supports a range of bitrates and model sizes, allowing trade-offs between quality and performance.</li>\n</ul>\n<h5 id=\"cons-5\">Cons</h5>\n<ul>\n  <li><strong>Complex conversion</strong>: Requires full model calibration, multiple quantization trials per matrix, and custom tooling.</li>\n  <li><strong>Conversion time</strong>: Large models (13B–70B) take significant time to convert due to exhaustive per-layer bit-width search.</li>\n  <li><strong>Inference compatibility</strong>: Requires ExLlamaV2 backend for proper kernel execution; not compatible with standard PyTorch or ONNX runtimes.</li>\n</ul>\n<h4 id=\"spinquant\">SpinQuant</h4>\n<ul>\n  <li>Introduced in <a href=\"https://arxiv.org/abs/2405.16406\">SpinQuant: LLM Quantization with Learned Rotations</a> by Liu et al. (2024), SpinQuant reduces quantization error by applying <strong>learned orthonormal rotations</strong> to weights, activations, and KV-cache blocks. These rotations normalize tensor distributions, mitigate outliers, and enable accurate <strong>W4A4KV4</strong> quantization. The implementation is available on <a href=\"https://github.com/facebookresearch/SpinQuant\">GitHub</a>.</li>\n</ul>\n<h5 id=\"process-4\">Process</h5>\n<ol>\n  <li>\n    <p><strong>Parameterize rotation matrices</strong>:</p>\n\n    <ul>\n      <li>SpinQuant uses blockwise <strong>orthonormal rotation matrices</strong> initialized using Hadamard, shortcut, or random orthonormal bases.</li>\n      <li>These rotations are applied to groups of weights, activations, and KV-cache blocks (e.g., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-96-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1230\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1231\"><span class=\"mi\" id=\"MathJax-Span-1232\" style=\"font-family: STIXGeneral-Italic;\">Q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-96\">Q</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-97-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1233\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1234\"><span class=\"mi\" id=\"MathJax-Span-1235\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-97\">K</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-98-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1236\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1237\"><span class=\"mi\" id=\"MathJax-Span-1238\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-98\">V</script> matrices or FFN weights), where outliers may exist.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Optimize via Cayley-SGD on the Stiefel manifold</strong>:</p>\n\n    <ul>\n      <li>A small calibration set is passed through a simulated W4A4KV4 pipeline.</li>\n      <li>Quantization error (e.g., MSE or KL divergence) between full-precision and quantized outputs is computed.</li>\n      <li>Gradients are backpropagated through the rotation parameters using <strong>Cayley-SGD</strong>, a method that maintains orthonormality constraints by optimizing directly on the Stiefel manifold.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Fold optimized rotations into model weights</strong>:</p>\n\n    <ul>\n      <li>Once optimized, the learned rotation matrices are <strong>fused into the model weights and biases</strong> (e.g., replacing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-99-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1239\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1240\"><span class=\"mi\" id=\"MathJax-Span-1241\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-99\">W</script> with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-100-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>R</mi><mi>T</mi></msup><mi>W</mi><mi>R</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1242\" style=\"width: 3.388em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.815em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.82em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1243\"><span class=\"msubsup\" id=\"MathJax-Span-1244\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1245\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-1246\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-1247\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1248\" style=\"font-family: STIXGeneral-Italic;\">R</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R</mi><mi>T</mi></msup><mi>W</mi><mi>R</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-100\">R^T W R</script>) during preprocessing.</li>\n      <li>This ensures that no extra computation is introduced at inference time—quantization is performed on the already rotated tensors.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Apply W4A4KV4 quantization</strong>:</p>\n\n    <ul>\n      <li>Post-rotation, weights, activations, and KV-cache blocks are quantized to 4-bit using standard uniform quantization schemes.</li>\n      <li>The rotations have distributed the influence of large-magnitude outliers, allowing for a tighter and more efficient quantization range.</li>\n    </ul>\n  </li>\n</ol>\n<p><strong>Parameterize rotation matrices</strong>:</p>\n<ul>\n      <li>SpinQuant uses blockwise <strong>orthonormal rotation matrices</strong> initialized using Hadamard, shortcut, or random orthonormal bases.</li>\n      <li>These rotations are applied to groups of weights, activations, and KV-cache blocks (e.g., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-96-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1230\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1231\"><span class=\"mi\" id=\"MathJax-Span-1232\" style=\"font-family: STIXGeneral-Italic;\">Q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-96\">Q</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-97-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1233\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1234\"><span class=\"mi\" id=\"MathJax-Span-1235\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-97\">K</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-98-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1236\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1237\"><span class=\"mi\" id=\"MathJax-Span-1238\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-98\">V</script> matrices or FFN weights), where outliers may exist.</li>\n    </ul>\n<p><strong>Optimize via Cayley-SGD on the Stiefel manifold</strong>:</p>\n<ul>\n      <li>A small calibration set is passed through a simulated W4A4KV4 pipeline.</li>\n      <li>Quantization error (e.g., MSE or KL divergence) between full-precision and quantized outputs is computed.</li>\n      <li>Gradients are backpropagated through the rotation parameters using <strong>Cayley-SGD</strong>, a method that maintains orthonormality constraints by optimizing directly on the Stiefel manifold.</li>\n    </ul>\n<p><strong>Fold optimized rotations into model weights</strong>:</p>\n<ul>\n      <li>Once optimized, the learned rotation matrices are <strong>fused into the model weights and biases</strong> (e.g., replacing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-99-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1239\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1240\"><span class=\"mi\" id=\"MathJax-Span-1241\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-99\">W</script> with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-100-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>R</mi><mi>T</mi></msup><mi>W</mi><mi>R</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1242\" style=\"width: 3.388em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.815em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.82em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1243\"><span class=\"msubsup\" id=\"MathJax-Span-1244\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1245\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-1246\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-1247\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1248\" style=\"font-family: STIXGeneral-Italic;\">R</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>R</mi><mi>T</mi></msup><mi>W</mi><mi>R</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-100\">R^T W R</script>) during preprocessing.</li>\n      <li>This ensures that no extra computation is introduced at inference time—quantization is performed on the already rotated tensors.</li>\n    </ul>\n<p><strong>Apply W4A4KV4 quantization</strong>:</p>\n<ul>\n      <li>Post-rotation, weights, activations, and KV-cache blocks are quantized to 4-bit using standard uniform quantization schemes.</li>\n      <li>The rotations have distributed the influence of large-magnitude outliers, allowing for a tighter and more efficient quantization range.</li>\n    </ul>\n<h5 id=\"pros-6\">Pros</h5>\n<ul>\n  <li>\n    <p><strong>Outlier mitigation via distribution normalization</strong>:</p>\n\n    <ul>\n      <li>Rotations “smear” large-magnitude values across dimensions, redistributing peak energies that would otherwise dominate quantization bins.</li>\n      <li>This normalization significantly reduces the impact of extreme values and improves low-bit quantization fidelity.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Accuracy preservation</strong>:</p>\n\n    <ul>\n      <li>Achieves within <strong>~2.9 points</strong> of full precision on Llama 2 (7B) zero-shot tasks.</li>\n      <li>Outperforms existing techniques like AWQ, SmoothQuant, and QuaRot by 19–45% in accuracy retention.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>No runtime overhead</strong>:</p>\n\n    <ul>\n      <li>Unlike some quantization techniques that add inference complexity, SpinQuant’s learned rotations are merged offline.</li>\n      <li>At inference, the model behaves identically to a standard quantized model, with no additional compute.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Outlier mitigation via distribution normalization</strong>:</p>\n<ul>\n      <li>Rotations “smear” large-magnitude values across dimensions, redistributing peak energies that would otherwise dominate quantization bins.</li>\n      <li>This normalization significantly reduces the impact of extreme values and improves low-bit quantization fidelity.</li>\n    </ul>\n<p><strong>Accuracy preservation</strong>:</p>\n<ul>\n      <li>Achieves within <strong>~2.9 points</strong> of full precision on Llama 2 (7B) zero-shot tasks.</li>\n      <li>Outperforms existing techniques like AWQ, SmoothQuant, and QuaRot by 19–45% in accuracy retention.</li>\n    </ul>\n<p><strong>No runtime overhead</strong>:</p>\n<ul>\n      <li>Unlike some quantization techniques that add inference complexity, SpinQuant’s learned rotations are merged offline.</li>\n      <li>At inference, the model behaves identically to a standard quantized model, with no additional compute.</li>\n    </ul>\n<h5 id=\"cons-6\">Cons</h5>\n<ul>\n  <li>\n    <p><strong>Involves optimization and calibration</strong>:</p>\n\n    <ul>\n      <li>Cayley-SGD optimization introduces computational overhead during preprocessing.</li>\n      <li>Requires a small validation set to simulate quantization and compute gradients.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Preprocessing complexity</strong>:</p>\n\n    <ul>\n      <li>Folding rotations into model weights adds engineering complexity, especially when targeting hardware deployment.</li>\n      <li>Though folded offline, the rotated tensors may have slightly increased numerical range, requiring careful scale selection.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Larger intermediate tensors</strong>:</p>\n\n    <ul>\n      <li>While inference cost remains low, merged rotated weights can increase storage slightly due to loss of weight sparsity or alignment.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Involves optimization and calibration</strong>:</p>\n<ul>\n      <li>Cayley-SGD optimization introduces computational overhead during preprocessing.</li>\n      <li>Requires a small validation set to simulate quantization and compute gradients.</li>\n    </ul>\n<p><strong>Preprocessing complexity</strong>:</p>\n<ul>\n      <li>Folding rotations into model weights adds engineering complexity, especially when targeting hardware deployment.</li>\n      <li>Though folded offline, the rotated tensors may have slightly increased numerical range, requiring careful scale selection.</li>\n    </ul>\n<p><strong>Larger intermediate tensors</strong>:</p>\n<ul>\n      <li>While inference cost remains low, merged rotated weights can increase storage slightly due to loss of weight sparsity or alignment.</li>\n    </ul>\n<h4 id=\"fptquant\">FPTQuant</h4>\n<ul>\n  <li>Introduced in <a href=\"https://arxiv.org/abs/2506.04985\">FPTQuant: 4-bit Function‑Preserving Transforms for Transformer PTQ</a> by Pan et al. (2025), Function-Preserving Transforms Quantization (FPTQuant) reshapes transformer activations before quantization to preserve function.</li>\n  <li>A complementary overview is available in <a href=\"https://leimao.github.io/blog/FPTQuant-LLM-Quantization/\">Lei Mao’s Log Book</a>.</li>\n</ul>\n<h5 id=\"process-5\">Process</h5>\n<ol>\n  <li>\n    <p><strong>Function-Preserving Activation Transforms</strong>:</p>\n\n    <ul>\n      <li>\n        <p>FPTQuant applies mathematically invertible transforms to the activations in attention and feedforward blocks to reduce their dynamic range. These include:</p>\n\n        <ul>\n          <li><strong>Logarithmic transforms</strong>: Applied to soften the long-tailed distributions (especially in attention scores or MLP activations).</li>\n          <li><strong>Affine or exponential transforms</strong>: Normalize activations without changing the computation graph logic.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Merging Transforms into Weights</strong>:</p>\n\n    <ul>\n      <li>Since these transforms are invertible, the effect can be canceled out by adjusting the downstream linear weights.</li>\n      <li>\n        <p>Specifically:</p>\n\n        <ul>\n          <li>Let <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-101-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1249\" style=\"width: 4.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.75em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1250\"><span class=\"mi\" id=\"MathJax-Span-1251\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1252\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">→</span><span class=\"mi\" id=\"MathJax-Span-1253\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1254\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1255\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1256\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi><mo stretchy=\"false\">→</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-101\">x \\rightarrow f(x)</script> be the transform applied to activations.</li>\n          <li>Then <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-102-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo><mi>W</mi><msup><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><mo stretchy=&quot;false&quot;>(</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1257\" style=\"width: 9.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.867em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1007.82em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1258\"><span class=\"mi\" id=\"MathJax-Span-1259\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1260\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1261\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">→</span><span class=\"mi\" id=\"MathJax-Span-1262\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-1263\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1264\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1265\"><span class=\"mrow\" id=\"MathJax-Span-1266\"><span class=\"mo\" id=\"MathJax-Span-1267\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1268\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1269\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1270\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1271\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1272\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1273\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1274\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi><mi>x</mi><mo stretchy=\"false\">→</mo><mi>W</mi><msup><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-102\">Wx \\rightarrow Wf^{-1}(f(x))</script> ensures the output remains unchanged.</li>\n          <li>FPTQuant modifies the linear projection weights accordingly so that the transform step is absorbed and the forward function is preserved.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Quantization Step</strong>:</p>\n\n    <ul>\n      <li>With the dynamic range compressed, 4-bit symmetric per-channel quantization is applied to the adjusted weights using PTQ methods.</li>\n      <li>Activations are not explicitly quantized, but their transformed form is compatible with W4A16 runtimes (e.g., vLLM) where only weights are quantized.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>No Runtime Penalty</strong>:</p>\n\n    <ul>\n      <li>All transforms are resolved offline and merged into weights.</li>\n      <li>The runtime model is a standard quantized model with no extra ops introduced during inference.</li>\n    </ul>\n  </li>\n</ol>\n<p><strong>Function-Preserving Activation Transforms</strong>:</p>\n<ul>\n      <li>\n        <p>FPTQuant applies mathematically invertible transforms to the activations in attention and feedforward blocks to reduce their dynamic range. These include:</p>\n\n        <ul>\n          <li><strong>Logarithmic transforms</strong>: Applied to soften the long-tailed distributions (especially in attention scores or MLP activations).</li>\n          <li><strong>Affine or exponential transforms</strong>: Normalize activations without changing the computation graph logic.</li>\n        </ul>\n      </li>\n    </ul>\n<p>FPTQuant applies mathematically invertible transforms to the activations in attention and feedforward blocks to reduce their dynamic range. These include:</p>\n<ul>\n          <li><strong>Logarithmic transforms</strong>: Applied to soften the long-tailed distributions (especially in attention scores or MLP activations).</li>\n          <li><strong>Affine or exponential transforms</strong>: Normalize activations without changing the computation graph logic.</li>\n        </ul>\n<p><strong>Merging Transforms into Weights</strong>:</p>\n<ul>\n      <li>Since these transforms are invertible, the effect can be canceled out by adjusting the downstream linear weights.</li>\n      <li>\n        <p>Specifically:</p>\n\n        <ul>\n          <li>Let <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-101-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1249\" style=\"width: 4.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.75em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1250\"><span class=\"mi\" id=\"MathJax-Span-1251\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1252\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">→</span><span class=\"mi\" id=\"MathJax-Span-1253\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1254\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1255\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1256\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi><mo stretchy=\"false\">→</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-101\">x \\rightarrow f(x)</script> be the transform applied to activations.</li>\n          <li>Then <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-102-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo><mi>W</mi><msup><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><mo stretchy=&quot;false&quot;>(</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1257\" style=\"width: 9.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.867em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1007.82em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1258\"><span class=\"mi\" id=\"MathJax-Span-1259\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1260\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1261\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">→</span><span class=\"mi\" id=\"MathJax-Span-1262\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-1263\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1264\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1265\"><span class=\"mrow\" id=\"MathJax-Span-1266\"><span class=\"mo\" id=\"MathJax-Span-1267\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1268\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1269\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1270\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1271\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1272\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1273\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1274\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi><mi>x</mi><mo stretchy=\"false\">→</mo><mi>W</mi><msup><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-102\">Wx \\rightarrow Wf^{-1}(f(x))</script> ensures the output remains unchanged.</li>\n          <li>FPTQuant modifies the linear projection weights accordingly so that the transform step is absorbed and the forward function is preserved.</li>\n        </ul>\n      </li>\n    </ul>\n<p>Specifically:</p>\n<ul>\n          <li>Let <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-101-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1249\" style=\"width: 4.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.75em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1250\"><span class=\"mi\" id=\"MathJax-Span-1251\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1252\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">→</span><span class=\"mi\" id=\"MathJax-Span-1253\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1254\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1255\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1256\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi><mo stretchy=\"false\">→</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-101\">x \\rightarrow f(x)</script> be the transform applied to activations.</li>\n          <li>Then <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-102-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo><mi>W</mi><msup><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msup><mo stretchy=&quot;false&quot;>(</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1257\" style=\"width: 9.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.867em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1007.82em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1258\"><span class=\"mi\" id=\"MathJax-Span-1259\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1260\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1261\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">→</span><span class=\"mi\" id=\"MathJax-Span-1262\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-1263\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1264\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1265\"><span class=\"mrow\" id=\"MathJax-Span-1266\"><span class=\"mo\" id=\"MathJax-Span-1267\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1268\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1269\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1270\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1271\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1272\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1273\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1274\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi><mi>x</mi><mo stretchy=\"false\">→</mo><mi>W</mi><msup><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-102\">Wx \\rightarrow Wf^{-1}(f(x))</script> ensures the output remains unchanged.</li>\n          <li>FPTQuant modifies the linear projection weights accordingly so that the transform step is absorbed and the forward function is preserved.</li>\n        </ul>\n<p><strong>Quantization Step</strong>:</p>\n<ul>\n      <li>With the dynamic range compressed, 4-bit symmetric per-channel quantization is applied to the adjusted weights using PTQ methods.</li>\n      <li>Activations are not explicitly quantized, but their transformed form is compatible with W4A16 runtimes (e.g., vLLM) where only weights are quantized.</li>\n    </ul>\n<p><strong>No Runtime Penalty</strong>:</p>\n<ul>\n      <li>All transforms are resolved offline and merged into weights.</li>\n      <li>The runtime model is a standard quantized model with no extra ops introduced during inference.</li>\n    </ul>\n<h5 id=\"pros-7\">Pros</h5>\n<ul>\n  <li>Fully <strong>training-free</strong>, <strong>invertible</strong>, and <strong>architecture-agnostic</strong>.</li>\n  <li>Achieves <strong><code class=\"language-plaintext highlighter-rouge\">int4</code> weight-only quantization</strong> with minimal or no accuracy loss, by preserving function through mathematically exact transformation.</li>\n  <li>Compatible with <strong>W4A16 systems</strong> like vLLM, delivering significant memory and latency improvements without major architectural rework.</li>\n</ul>\n<h5 id=\"cons-7\">Cons</h5>\n<ul>\n  <li>Primarily targets <strong>weights</strong>—does not quantize activations directly, limiting total memory benefits.</li>\n  <li>Still an <strong>emerging method</strong>—performance and generalization are under ongoing validation across LLM variants (e.g., Mistral, Gemma).</li>\n  <li>May require per-layer transform tuning based on architecture layout (e.g., attention vs MLP blocks).</li>\n</ul>\n<h4 id=\"palettization-weight-clustering\">Palettization (Weight Clustering)</h4>\n<ul>\n  <li><strong>Palettization</strong>, also known as <strong>weight clustering</strong>, is a quantization scheme that replaces full‑precision weights with low‑bit indices into a small lookup table (LUT). Each weight value is approximated by the nearest centroid in the LUT, enabling efficient storage and retrieval.</li>\n</ul>\n<h5 id=\"process-6\">Process</h5>\n<ol>\n  <li><strong>Clustering</strong>: Collect all float‑format weights for a layer (or group of layers), then run <strong>k-means clustering</strong> to derive a set of centroids (typically, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-103-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1275\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1000.94em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1276\"><span class=\"msubsup\" id=\"MathJax-Span-1277\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1278\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1279\"><span class=\"mrow\" id=\"MathJax-Span-1280\"><span class=\"mi\" id=\"MathJax-Span-1281\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-103\">2^{n}</script> entries for n‑bit palettization).</li>\n  <li><strong>Index Mapping</strong>: Each weight is replaced with an integer index pointing to its closest centroid in the LUT. The original full‑precision value is no longer stored.</li>\n  <li><strong>Granularity Options</strong>:\n    <ul>\n      <li><strong>Per‑tensor granularity</strong>: A single LUT for the entire weight tensor.</li>\n      <li><strong>Per‑group‑channel granularity</strong>: The tensor is divided into groups of channels (defined by <code class=\"language-plaintext highlighter-rouge\">group_size</code>), each with its own LUT—offering a better accuracy/compression trade-off.</li>\n    </ul>\n  </li>\n  <li><strong>Optional Vector Clustering</strong> (<code class=\"language-plaintext highlighter-rouge\">cluster_dim &gt; 1</code>): Enables multi-dimensional centroids by clustering weight vectors instead of scalars, improving approximation quality for some architectures.</li>\n  <li><strong>Post‑processing</strong>: Optionally quantize LUT centroids themselves to a lower precision (e.g. <code class=\"language-plaintext highlighter-rouge\">int8</code>) for additional compression.</li>\n</ol>\n<ul>\n      <li><strong>Per‑tensor granularity</strong>: A single LUT for the entire weight tensor.</li>\n      <li><strong>Per‑group‑channel granularity</strong>: The tensor is divided into groups of channels (defined by <code class=\"language-plaintext highlighter-rouge\">group_size</code>), each with its own LUT—offering a better accuracy/compression trade-off.</li>\n    </ul>\n<h5 id=\"integration-in-workflows\">Integration in Workflows</h5>\n<ul>\n  <li>Available via Apple’s <code class=\"language-plaintext highlighter-rouge\">coremltools.optimize.torch.palettization</code> API, which injects <strong>FakePalettize</strong> layers into the model for <strong>palettization-aware training (PAT)</strong>.</li>\n  <li>During training, k‑means clustering is applied online, and the LUT and indices are learned through gradient steps. After convergence, the <code class=\"language-plaintext highlighter-rouge\">finalize()</code> call folds LUTs and indices into permanent quantized weights.</li>\n</ul>\n<h5 id=\"when-to-use-palettization\">When to Use Palettization</h5>\n<ul>\n  <li><strong>Memory-critical deployment</strong>: Edge devices or mobile apps where weight storage is the bottleneck.</li>\n  <li><strong>Aggressive compression</strong>: Scenarios requiring sub‑4‑bit representation.</li>\n  <li><strong>Architecture flexibility</strong>: Works with both CNNs and transformers when standard affine quantization struggles.</li>\n  <li><strong>Fine‑tunable deployment targets</strong>: Fine-tuning after palettization enables high accuracy while still achieving significant compression ratios.</li>\n</ul>\n<h5 id=\"pros-8\">Pros</h5>\n<ul>\n  <li><strong>Extreme compression:</strong> Supports ultra-low bit‑width representations (e.g. 2–4 bits) for weights.</li>\n  <li><strong>Memory savings:</strong> Offers major memory savings—each weight becomes a small index instead of a float.</li>\n  <li><strong>Vector clustering:</strong> Multidimensional centroids can preserve structure in weight matrices.</li>\n  <li><strong>Flexible granularity:</strong> Per-tensor, per-group, or vector-level control enables tailored compression vs. accuracy trade-offs.</li>\n  <li><strong>Compatible with fine‑tuning:</strong> Compatible via PAT, allowing retention of accuracy through fine‑tuning post-clustering.</li>\n</ul>\n<h5 id=\"cons-8\">Cons</h5>\n<ul>\n  <li>Requires additional training or fine‑tuning steps (PAT) to compensate for quantization error.</li>\n  <li>Clustering and LUT management adds complexity to both training and inference pipelines. In other words, introduces LUT metadata and integer-to-centroid lookup logic in inference.</li>\n  <li>Larger runtime overhead than standard affine quantization, especially with per-channel or per-group palettization which increases storage overhead for multiple LUTs and adds runtime logic to look up indices.</li>\n  <li>Less intuitive and more complex to implement than simple scale-based quantization.</li>\n</ul>\n<h4 id=\"what-to-use-when\">What to Use When?</h4>\n<ul>\n  <li>\n    <p>Selecting the right method depends on deployment goals, model architecture, available compute, and desired trade-offs between accuracy, speed, and memory. Below is a structured guide to help determine <strong>what to use when</strong>.</p>\n  </li>\n  <li>\n    <p><strong>For Ultra-Low Bit Weight-Only Quantization (<code class=\"language-plaintext highlighter-rouge\">int3</code>/<code class=\"language-plaintext highlighter-rouge\">int4</code>) with No Accuracy Drop</strong>:</p>\n\n    <ul>\n      <li>\n        <p><em>Use: AWQ, FPTQuant, GPTQ, EXL2, SpinQuant</em></p>\n      </li>\n      <li><strong>AWQ</strong>: Best for <strong>fast deployment</strong> of LLMs (e.g., Llama) on <strong>edge GPUs</strong> or <strong>low-latency inference</strong> with prebuilt CUDA kernels. No training or tuning required, and integrates well with TinyChat or similar runtimes.</li>\n      <li><strong>FPTQuant</strong>: Ideal when you need <strong>function-preserving <code class=\"language-plaintext highlighter-rouge\">int4</code> compression</strong> with no runtime penalty and full compatibility with transformer architectures. Use for <strong>W4A16</strong> deployment in platforms like <strong>vLLM</strong>.</li>\n      <li><strong>GPTQ</strong>: Recommended for <strong>very large models (13B–175B)</strong> where preserving perplexity is critical. Use when quantization accuracy is a priority and you’re comfortable with modest compute during conversion.</li>\n      <li><strong>EXL2</strong>: Choose when running <strong>massive models (e.g., Llama2-70B)</strong> on <strong>consumer GPUs</strong>. Offers the <strong>best compression-speed balance</strong> via dynamic bit allocation and works well with ExLlama.</li>\n      <li><strong>SpinQuant</strong>: Select when targeting <strong>4-bit quantization of both weights and activations</strong> while retaining high accuracy (e.g., for academic or performance-sensitive production use). Best for <strong>W4A4KV4</strong> targets with pre-deployment compute budget.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>For Full W8A8 Quantization (Weight + Activation)</strong>:</p>\n\n    <ul>\n      <li>\n        <p><em>Use: SmoothQuant, AWEQ</em></p>\n      </li>\n      <li><strong>SmoothQuant</strong>: The best choice for <strong>training-free, full <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization</strong> with minimal setup. Choose for <strong>static quantization</strong> pipelines on <strong>NLP models</strong> like Llama, OPT, or BLOOM where CPU or GPU <code class=\"language-plaintext highlighter-rouge\">int8</code> inference is desired.</li>\n      <li><strong>AWEQ</strong>: Prefer this over SmoothQuant when you need <strong>better accuracy</strong> with <strong>activation-weight balance</strong>, especially for models that are hard to quantize (e.g., with long-tailed distributions). Supports both <strong>W8A8 and ultra-low-bit variants</strong>, and requires <strong>no fine-tuning</strong>.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>For Mixed-Precision or Variable-Bitrate Quantization</strong>:</p>\n\n    <ul>\n      <li>\n        <p><em>Use: EXL2, GGUF (K-Quants, I-Quants)</em></p>\n      </li>\n      <li><strong>EXL2</strong>: Use when deploying models in <strong>memory-constrained</strong> environments but still wanting to preserve <strong>key model behavior</strong> via <strong>bit allocation per group</strong>. Especially useful for <strong>interactive LLMs</strong> on laptops or desktops.</li>\n      <li><strong>GGUF (K/I-Quants)</strong>: Ideal for <strong>offline, file-efficient packaging</strong> and <strong>CPU or mobile inference</strong> with tooling like <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>. Offers a trade-off between compatibility and compression via <strong>predefined quant profiles</strong> (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-104-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msub><mn>3</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>K</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>S</mi></mrow></msub></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1282\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1283\"><span class=\"mi\" id=\"MathJax-Span-1284\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-1285\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1286\" style=\"font-family: STIXGeneral-Regular;\">3</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1287\"><span class=\"mrow\" id=\"MathJax-Span-1288\"><span class=\"msubsup\" id=\"MathJax-Span-1289\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1290\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1291\"><span class=\"mrow\" id=\"MathJax-Span-1292\"><span class=\"mi\" id=\"MathJax-Span-1293\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msub><mn>3</mn><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>K</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>S</mi></mrow></msub></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-104\">Q3_{K_{S}}</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-105-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>I</mi><mi>Q</mi><msub><mn>4</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>X</mi><mi>S</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1294\" style=\"width: 3.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.5em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1295\"><span class=\"mi\" id=\"MathJax-Span-1296\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1297\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-1298\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1299\" style=\"font-family: STIXGeneral-Regular;\">4</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1300\"><span class=\"mrow\" id=\"MathJax-Span-1301\"><span class=\"mi\" id=\"MathJax-Span-1302\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1303\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>I</mi><mi>Q</mi><msub><mn>4</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>X</mi><mi>S</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-105\">IQ4_{XS}</script>, etc.).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>For Extreme Compression with Customization or Training Support</strong>:</p>\n\n    <ul>\n      <li>\n        <p><em>Use: Palettization (Weight Clustering)</em></p>\n      </li>\n      <li>Use palettization when <strong>maximum compression</strong> is needed and <strong>some fine-tuning is acceptable</strong>. Ideal for <strong>mobile deployment</strong> or <strong>experimental architectures</strong> where LUTs and centroid representation can drastically reduce size.</li>\n      <li>Choose <strong>vector clustering</strong> when structure preservation in weight matrices matters (e.g., vision-transformer hybrids or customized transformer blocks).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>For Legacy or Format-Constrained Inference</strong>:</p>\n\n    <ul>\n      <li>\n        <p><em>Use: GGUF (Legacy Quants)</em></p>\n      </li>\n      <li>Best suited for <strong>lightweight, portable LLM inference</strong> on CPU or embedded hardware via <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>.</li>\n      <li>Use when you need <strong>fast loading</strong>, <strong>offline conversion</strong>, and <strong>minimal dependencies</strong>, especially for local LLMs or desktop chatbots.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>If Activation Quantization Is Not Required (Weight-Only Models)</strong>:</p>\n\n    <ul>\n      <li>\n        <p><em>Use: AWQ, GPTQ, FPTQuant, EXL2</em></p>\n      </li>\n      <li>These methods focus on <strong><code class=\"language-plaintext highlighter-rouge\">int3/4</code> weight-only quantization</strong> without modifying activations (typically left in <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>).</li>\n      <li>Use these when <strong>inference memory is not your bottleneck</strong>, and you want the best <strong>latency-to-accuracy</strong> trade-off without model retraining.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Summary Decision Matrix</strong>:</p>\n  </li>\n</ul>\n<p>Selecting the right method depends on deployment goals, model architecture, available compute, and desired trade-offs between accuracy, speed, and memory. Below is a structured guide to help determine <strong>what to use when</strong>.</p>\n<p><strong>For Ultra-Low Bit Weight-Only Quantization (<code class=\"language-plaintext highlighter-rouge\">int3</code>/<code class=\"language-plaintext highlighter-rouge\">int4</code>) with No Accuracy Drop</strong>:</p>\n<ul>\n      <li>\n        <p><em>Use: AWQ, FPTQuant, GPTQ, EXL2, SpinQuant</em></p>\n      </li>\n      <li><strong>AWQ</strong>: Best for <strong>fast deployment</strong> of LLMs (e.g., Llama) on <strong>edge GPUs</strong> or <strong>low-latency inference</strong> with prebuilt CUDA kernels. No training or tuning required, and integrates well with TinyChat or similar runtimes.</li>\n      <li><strong>FPTQuant</strong>: Ideal when you need <strong>function-preserving <code class=\"language-plaintext highlighter-rouge\">int4</code> compression</strong> with no runtime penalty and full compatibility with transformer architectures. Use for <strong>W4A16</strong> deployment in platforms like <strong>vLLM</strong>.</li>\n      <li><strong>GPTQ</strong>: Recommended for <strong>very large models (13B–175B)</strong> where preserving perplexity is critical. Use when quantization accuracy is a priority and you’re comfortable with modest compute during conversion.</li>\n      <li><strong>EXL2</strong>: Choose when running <strong>massive models (e.g., Llama2-70B)</strong> on <strong>consumer GPUs</strong>. Offers the <strong>best compression-speed balance</strong> via dynamic bit allocation and works well with ExLlama.</li>\n      <li><strong>SpinQuant</strong>: Select when targeting <strong>4-bit quantization of both weights and activations</strong> while retaining high accuracy (e.g., for academic or performance-sensitive production use). Best for <strong>W4A4KV4</strong> targets with pre-deployment compute budget.</li>\n    </ul>\n<p><em>Use: AWQ, FPTQuant, GPTQ, EXL2, SpinQuant</em></p>\n<p><strong>For Full W8A8 Quantization (Weight + Activation)</strong>:</p>\n<ul>\n      <li>\n        <p><em>Use: SmoothQuant, AWEQ</em></p>\n      </li>\n      <li><strong>SmoothQuant</strong>: The best choice for <strong>training-free, full <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization</strong> with minimal setup. Choose for <strong>static quantization</strong> pipelines on <strong>NLP models</strong> like Llama, OPT, or BLOOM where CPU or GPU <code class=\"language-plaintext highlighter-rouge\">int8</code> inference is desired.</li>\n      <li><strong>AWEQ</strong>: Prefer this over SmoothQuant when you need <strong>better accuracy</strong> with <strong>activation-weight balance</strong>, especially for models that are hard to quantize (e.g., with long-tailed distributions). Supports both <strong>W8A8 and ultra-low-bit variants</strong>, and requires <strong>no fine-tuning</strong>.</li>\n    </ul>\n<p><em>Use: SmoothQuant, AWEQ</em></p>\n<p><strong>For Mixed-Precision or Variable-Bitrate Quantization</strong>:</p>\n<ul>\n      <li>\n        <p><em>Use: EXL2, GGUF (K-Quants, I-Quants)</em></p>\n      </li>\n      <li><strong>EXL2</strong>: Use when deploying models in <strong>memory-constrained</strong> environments but still wanting to preserve <strong>key model behavior</strong> via <strong>bit allocation per group</strong>. Especially useful for <strong>interactive LLMs</strong> on laptops or desktops.</li>\n      <li><strong>GGUF (K/I-Quants)</strong>: Ideal for <strong>offline, file-efficient packaging</strong> and <strong>CPU or mobile inference</strong> with tooling like <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>. Offers a trade-off between compatibility and compression via <strong>predefined quant profiles</strong> (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-104-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><msub><mn>3</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>K</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>S</mi></mrow></msub></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1282\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1283\"><span class=\"mi\" id=\"MathJax-Span-1284\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-1285\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1286\" style=\"font-family: STIXGeneral-Regular;\">3</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1287\"><span class=\"mrow\" id=\"MathJax-Span-1288\"><span class=\"msubsup\" id=\"MathJax-Span-1289\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1290\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1291\"><span class=\"mrow\" id=\"MathJax-Span-1292\"><span class=\"mi\" id=\"MathJax-Span-1293\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><msub><mn>3</mn><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>K</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>S</mi></mrow></msub></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-104\">Q3_{K_{S}}</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-105-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>I</mi><mi>Q</mi><msub><mn>4</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>X</mi><mi>S</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1294\" style=\"width: 3.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.5em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1295\"><span class=\"mi\" id=\"MathJax-Span-1296\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1297\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-1298\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1299\" style=\"font-family: STIXGeneral-Regular;\">4</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1300\"><span class=\"mrow\" id=\"MathJax-Span-1301\"><span class=\"mi\" id=\"MathJax-Span-1302\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1303\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>I</mi><mi>Q</mi><msub><mn>4</mn><mrow class=\"MJX-TeXAtom-ORD\"><mi>X</mi><mi>S</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-105\">IQ4_{XS}</script>, etc.).</li>\n    </ul>\n<p><em>Use: EXL2, GGUF (K-Quants, I-Quants)</em></p>\n<p><strong>For Extreme Compression with Customization or Training Support</strong>:</p>\n<ul>\n      <li>\n        <p><em>Use: Palettization (Weight Clustering)</em></p>\n      </li>\n      <li>Use palettization when <strong>maximum compression</strong> is needed and <strong>some fine-tuning is acceptable</strong>. Ideal for <strong>mobile deployment</strong> or <strong>experimental architectures</strong> where LUTs and centroid representation can drastically reduce size.</li>\n      <li>Choose <strong>vector clustering</strong> when structure preservation in weight matrices matters (e.g., vision-transformer hybrids or customized transformer blocks).</li>\n    </ul>\n<p><em>Use: Palettization (Weight Clustering)</em></p>\n<p><strong>For Legacy or Format-Constrained Inference</strong>:</p>\n<ul>\n      <li>\n        <p><em>Use: GGUF (Legacy Quants)</em></p>\n      </li>\n      <li>Best suited for <strong>lightweight, portable LLM inference</strong> on CPU or embedded hardware via <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>.</li>\n      <li>Use when you need <strong>fast loading</strong>, <strong>offline conversion</strong>, and <strong>minimal dependencies</strong>, especially for local LLMs or desktop chatbots.</li>\n    </ul>\n<p><em>Use: GGUF (Legacy Quants)</em></p>\n<p><strong>If Activation Quantization Is Not Required (Weight-Only Models)</strong>:</p>\n<ul>\n      <li>\n        <p><em>Use: AWQ, GPTQ, FPTQuant, EXL2</em></p>\n      </li>\n      <li>These methods focus on <strong><code class=\"language-plaintext highlighter-rouge\">int3/4</code> weight-only quantization</strong> without modifying activations (typically left in <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>).</li>\n      <li>Use these when <strong>inference memory is not your bottleneck</strong>, and you want the best <strong>latency-to-accuracy</strong> trade-off without model retraining.</li>\n    </ul>\n<p><em>Use: AWQ, GPTQ, FPTQuant, EXL2</em></p>\n<p><strong>Summary Decision Matrix</strong>:</p>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Goal / Constraint</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Recommended Method(s)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Fastest low-bit inference (no training)</td>\n<td class=\"tg-tleft-valign-second\">AWQ, FPTQuant</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Best W4A4 quantization accuracy</td>\n<td class=\"tg-tleft-valign-second\">SpinQuant</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">W8A8 quant with balanced scaling</td>\n<td class=\"tg-tleft-valign-second\">SmoothQuant, AWEQ</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Largest models on smallest VRAM</td>\n<td class=\"tg-tleft-valign-second\">EXL2, GPTQ</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Mixed precision with dynamic control</td>\n<td class=\"tg-tleft-valign-second\">EXL2, GGUF (K/I-Quants)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">CPU/mobile inference with format support</td>\n<td class=\"tg-tleft-valign-second\">GGUF</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Edge deployment with low compute budget</td>\n<td class=\"tg-tleft-valign-second\">AWQ, GGUF (Legacy)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Extreme compression + retraining allowed</td>\n<td class=\"tg-tleft-valign-second\">Palettization</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Activation-aware optimizations</td>\n<td class=\"tg-tleft-valign-second\">SpinQuant, SmoothQuant, AWEQ</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Goal / Constraint</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Recommended Method(s)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Fastest low-bit inference (no training)</td>\n<td class=\"tg-tleft-valign-second\">AWQ, FPTQuant</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Best W4A4 quantization accuracy</td>\n<td class=\"tg-tleft-valign-second\">SpinQuant</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">W8A8 quant with balanced scaling</td>\n<td class=\"tg-tleft-valign-second\">SmoothQuant, AWEQ</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Largest models on smallest VRAM</td>\n<td class=\"tg-tleft-valign-second\">EXL2, GPTQ</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Mixed precision with dynamic control</td>\n<td class=\"tg-tleft-valign-second\">EXL2, GGUF (K/I-Quants)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">CPU/mobile inference with format support</td>\n<td class=\"tg-tleft-valign-second\">GGUF</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Edge deployment with low compute budget</td>\n<td class=\"tg-tleft-valign-second\">AWQ, GGUF (Legacy)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Extreme compression + retraining allowed</td>\n<td class=\"tg-tleft-valign-second\">Palettization</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Activation-aware optimizations</td>\n<td class=\"tg-tleft-valign-second\">SpinQuant, SmoothQuant, AWEQ</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"comparative-analysis-1\">Comparative Analysis</h4>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Method</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Type</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>What Quantized</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Bits</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Training-Free?</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Key Innovation</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Accuracy Retention</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Introduced</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Uniform Quantization</td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization</td>\n<td class=\"tg-tleft-valign-first\">Weights ± Activations</td>\n<td class=\"tg-tleft-valign-first\">4–8 bit</td>\n<td class=\"tg-tleft-valign-first\">yes</td>\n<td class=\"tg-tleft-valign-first\">Simple affine mapping, per-tensor or per-channel</td>\n<td class=\"tg-tleft-valign-first\">Good for smooth distributions (~2pt drop)</td>\n<td class=\"tg-tleft-valign-second\">(Fundamental baseline)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://arxiv.org/abs/2210.17323\">GPTQ</a></td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization</td>\n<td class=\"tg-tleft-valign-first\">Weights only</td>\n<td class=\"tg-tleft-valign-first\">INT3/4 (also 2-bit)</td>\n<td class=\"tg-tleft-valign-first\">yes</td>\n<td class=\"tg-tleft-valign-first\">Second-order Hessian-based error compensation per-layer</td>\n<td class=\"tg-tleft-valign-first\">Highly accurate (perplexity within ~0.03 of <code>float16</code>)</td>\n<td class=\"tg-tleft-valign-second\">Oct 2022</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://arxiv.org/abs/2211.10438\">SmoothQuant</a></td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization</td>\n<td class=\"tg-tleft-valign-first\">Weights + Activations</td>\n<td class=\"tg-tleft-valign-first\">W8A8</td>\n<td class=\"tg-tleft-valign-first\">yes</td>\n<td class=\"tg-tleft-valign-first\">Scaling activation/weights to balance quantization difficulty</td>\n<td class=\"tg-tleft-valign-first\">Very high (&lt;0.5 % loss)</td>\n<td class=\"tg-tleft-valign-second\">Nov 2022</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://arxiv.org/abs/2306.00978\">AWQ</a></td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization</td>\n<td class=\"tg-tleft-valign-first\">Weights only</td>\n<td class=\"tg-tleft-valign-first\">W4</td>\n<td class=\"tg-tleft-valign-first\">yes</td>\n<td class=\"tg-tleft-valign-first\">Activation-aware per-channel scaling via calibration</td>\n<td class=\"tg-tleft-valign-first\">High (&gt;<code>float16</code>)</td>\n<td class=\"tg-tleft-valign-second\">Jun 2023</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://github.com/ggml-org/llama.cpp\">GGUF</a></td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization</td>\n<td class=\"tg-tleft-valign-first\">Weights only</td>\n<td class=\"tg-tleft-valign-first\">2–8 bit (block-based)</td>\n<td class=\"tg-tleft-valign-first\">yes (quantized offline)</td>\n<td class=\"tg-tleft-valign-first\">Flexible binary format with various schemes, importance matrices embedded</td>\n<td class=\"tg-tleft-valign-first\">Varies by scheme; efficient loading</td>\n<td class=\"tg-tleft-valign-second\">Aug 2023 (as part of `llama.cpp`)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://arxiv.org/abs/2311.01305\">AWEQ</a></td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization</td>\n<td class=\"tg-tleft-valign-first\">Weights + Activations</td>\n<td class=\"tg-tleft-valign-first\">W4 or W8A8</td>\n<td class=\"tg-tleft-valign-first\">yes</td>\n<td class=\"tg-tleft-valign-first\">Activation-weight equalization + bias correction</td>\n<td class=\"tg-tleft-valign-first\">Best-in-class (within &lt;0.01 absolute from <code>float16</code>)</td>\n<td class=\"tg-tleft-valign-second\">Nov 2023</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://github.com/chu-tianxiang/exl2-for-all?utm_source=chatgpt.com\">EXL2 (ExLlamaV2)</a></td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization (dynamic allocation)</td>\n<td class=\"tg-tleft-valign-first\">Weights only</td>\n<td class=\"tg-tleft-valign-first\">Mixed: 2–8 bit</td>\n<td class=\"tg-tleft-valign-first\">yes</td>\n<td class=\"tg-tleft-valign-first\">GPTQ-based mixed-bit allocations per layer via error minimization</td>\n<td class=\"tg-tleft-valign-first\">Very high; Llama2-70B runs on 24 GB GPU</td>\n<td class=\"tg-tleft-valign-second\">Nov 2023</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://arxiv.org/abs/2405.16406\">SpinQuant</a></td>\n<td class=\"tg-tleft-valign-first\">Quantization Aware Training (calibration + optimization)</td>\n<td class=\"tg-tleft-valign-first\">Weights + Activations + KV-cache</td>\n<td class=\"tg-tleft-valign-first\">W4A4KV4</td>\n<td class=\"tg-tleft-valign-first\">no (requires calibration + optimization)</td>\n<td class=\"tg-tleft-valign-first\">Learned orthonormal rotations to normalize distributions</td>\n<td class=\"tg-tleft-valign-first\">Within ~2.9 pt of FP</td>\n<td class=\"tg-tleft-valign-second\">May 2024</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://arxiv.org/abs/2506.04985\">FPTQuant</a></td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization</td>\n<td class=\"tg-tleft-valign-first\">Weights only</td>\n<td class=\"tg-tleft-valign-first\">W4 (weight-only)</td>\n<td class=\"tg-tleft-valign-first\">yes</td>\n<td class=\"tg-tleft-valign-first\">Function-preserving invertible transforms to activations, merged into weights</td>\n<td class=\"tg-tleft-valign-first\">Excellent (minimal loss)</td>\n<td class=\"tg-tleft-valign-second\">Jun 2025</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://apple.github.io/coremltools/docs-guides/source/opt-palettization-overview.html\">Palettization</a></td>\n<td class=\"tg-tleft-valign-first\">Quantization Aware (Training or Fine-tuning)</td>\n<td class=\"tg-tleft-valign-first\">Weights only</td>\n<td class=\"tg-tleft-valign-first\">2–4 bit (LUT index)</td>\n<td class=\"tg-tleft-valign-first\">no (requires fine-tuning or PAT)</td>\n<td class=\"tg-tleft-valign-first\">Weight clustering via k-means with optional vector or group granularity</td>\n<td class=\"tg-tleft-valign-first\">High (with PAT); moderate otherwise</td>\n<td class=\"tg-tleft-valign-second\">2024</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Method</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Type</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>What Quantized</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Bits</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Training-Free?</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Key Innovation</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Accuracy Retention</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Introduced</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Uniform Quantization</td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization</td>\n<td class=\"tg-tleft-valign-first\">Weights ± Activations</td>\n<td class=\"tg-tleft-valign-first\">4–8 bit</td>\n<td class=\"tg-tleft-valign-first\">yes</td>\n<td class=\"tg-tleft-valign-first\">Simple affine mapping, per-tensor or per-channel</td>\n<td class=\"tg-tleft-valign-first\">Good for smooth distributions (~2pt drop)</td>\n<td class=\"tg-tleft-valign-second\">(Fundamental baseline)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://arxiv.org/abs/2210.17323\">GPTQ</a></td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization</td>\n<td class=\"tg-tleft-valign-first\">Weights only</td>\n<td class=\"tg-tleft-valign-first\">INT3/4 (also 2-bit)</td>\n<td class=\"tg-tleft-valign-first\">yes</td>\n<td class=\"tg-tleft-valign-first\">Second-order Hessian-based error compensation per-layer</td>\n<td class=\"tg-tleft-valign-first\">Highly accurate (perplexity within ~0.03 of <code>float16</code>)</td>\n<td class=\"tg-tleft-valign-second\">Oct 2022</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://arxiv.org/abs/2211.10438\">SmoothQuant</a></td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization</td>\n<td class=\"tg-tleft-valign-first\">Weights + Activations</td>\n<td class=\"tg-tleft-valign-first\">W8A8</td>\n<td class=\"tg-tleft-valign-first\">yes</td>\n<td class=\"tg-tleft-valign-first\">Scaling activation/weights to balance quantization difficulty</td>\n<td class=\"tg-tleft-valign-first\">Very high (&lt;0.5 % loss)</td>\n<td class=\"tg-tleft-valign-second\">Nov 2022</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://arxiv.org/abs/2306.00978\">AWQ</a></td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization</td>\n<td class=\"tg-tleft-valign-first\">Weights only</td>\n<td class=\"tg-tleft-valign-first\">W4</td>\n<td class=\"tg-tleft-valign-first\">yes</td>\n<td class=\"tg-tleft-valign-first\">Activation-aware per-channel scaling via calibration</td>\n<td class=\"tg-tleft-valign-first\">High (&gt;<code>float16</code>)</td>\n<td class=\"tg-tleft-valign-second\">Jun 2023</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://github.com/ggml-org/llama.cpp\">GGUF</a></td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization</td>\n<td class=\"tg-tleft-valign-first\">Weights only</td>\n<td class=\"tg-tleft-valign-first\">2–8 bit (block-based)</td>\n<td class=\"tg-tleft-valign-first\">yes (quantized offline)</td>\n<td class=\"tg-tleft-valign-first\">Flexible binary format with various schemes, importance matrices embedded</td>\n<td class=\"tg-tleft-valign-first\">Varies by scheme; efficient loading</td>\n<td class=\"tg-tleft-valign-second\">Aug 2023 (as part of `llama.cpp`)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://arxiv.org/abs/2311.01305\">AWEQ</a></td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization</td>\n<td class=\"tg-tleft-valign-first\">Weights + Activations</td>\n<td class=\"tg-tleft-valign-first\">W4 or W8A8</td>\n<td class=\"tg-tleft-valign-first\">yes</td>\n<td class=\"tg-tleft-valign-first\">Activation-weight equalization + bias correction</td>\n<td class=\"tg-tleft-valign-first\">Best-in-class (within &lt;0.01 absolute from <code>float16</code>)</td>\n<td class=\"tg-tleft-valign-second\">Nov 2023</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://github.com/chu-tianxiang/exl2-for-all?utm_source=chatgpt.com\">EXL2 (ExLlamaV2)</a></td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization (dynamic allocation)</td>\n<td class=\"tg-tleft-valign-first\">Weights only</td>\n<td class=\"tg-tleft-valign-first\">Mixed: 2–8 bit</td>\n<td class=\"tg-tleft-valign-first\">yes</td>\n<td class=\"tg-tleft-valign-first\">GPTQ-based mixed-bit allocations per layer via error minimization</td>\n<td class=\"tg-tleft-valign-first\">Very high; Llama2-70B runs on 24 GB GPU</td>\n<td class=\"tg-tleft-valign-second\">Nov 2023</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://arxiv.org/abs/2405.16406\">SpinQuant</a></td>\n<td class=\"tg-tleft-valign-first\">Quantization Aware Training (calibration + optimization)</td>\n<td class=\"tg-tleft-valign-first\">Weights + Activations + KV-cache</td>\n<td class=\"tg-tleft-valign-first\">W4A4KV4</td>\n<td class=\"tg-tleft-valign-first\">no (requires calibration + optimization)</td>\n<td class=\"tg-tleft-valign-first\">Learned orthonormal rotations to normalize distributions</td>\n<td class=\"tg-tleft-valign-first\">Within ~2.9 pt of FP</td>\n<td class=\"tg-tleft-valign-second\">May 2024</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://arxiv.org/abs/2506.04985\">FPTQuant</a></td>\n<td class=\"tg-tleft-valign-first\">Post Training Quantization</td>\n<td class=\"tg-tleft-valign-first\">Weights only</td>\n<td class=\"tg-tleft-valign-first\">W4 (weight-only)</td>\n<td class=\"tg-tleft-valign-first\">yes</td>\n<td class=\"tg-tleft-valign-first\">Function-preserving invertible transforms to activations, merged into weights</td>\n<td class=\"tg-tleft-valign-first\">Excellent (minimal loss)</td>\n<td class=\"tg-tleft-valign-second\">Jun 2025</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><a href=\"https://apple.github.io/coremltools/docs-guides/source/opt-palettization-overview.html\">Palettization</a></td>\n<td class=\"tg-tleft-valign-first\">Quantization Aware (Training or Fine-tuning)</td>\n<td class=\"tg-tleft-valign-first\">Weights only</td>\n<td class=\"tg-tleft-valign-first\">2–4 bit (LUT index)</td>\n<td class=\"tg-tleft-valign-first\">no (requires fine-tuning or PAT)</td>\n<td class=\"tg-tleft-valign-first\">Weight clustering via k-means with optional vector or group granularity</td>\n<td class=\"tg-tleft-valign-first\">High (with PAT); moderate otherwise</td>\n<td class=\"tg-tleft-valign-second\">2024</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "#### GPTQ: Quantization with Second-Order Error Compensation\n\n*   Introduced in [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323) by Frantar et al. (2023), GPTQ is a high-accuracy post-training quantization (PTQ) method tailored for large-scale transformers. Unlike round-to-nearest schemes, GPTQ minimizes the quantization error using approximate second-order information derived from the Hessian of the loss. It quantizes weights in a layer-wise fashion while updating unquantized weights to compensate for introduced error, achieving efficient `int3/4` quantization of models as large as OPT-175B or BLOOM-176B without finetuning. Practical implementations are available through [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) and [LLM.int4](https://github.com/IST-DASLab/gptq).\n    \n*   GPTQ significantly outperforms simple rounding methods by preserving perplexity under low-bit quantization. Notably, it is one of the few techniques that scales to 100B+ parameter models using modest compute (e.g., a single A100 GPU). While it focuses on weight-only quantization, activation quantization can be layered on top via orthogonal techniques such as SmoothQuant.\n    \n\nIntroduced in [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323) by Frantar et al. (2023), GPTQ is a high-accuracy post-training quantization (PTQ) method tailored for large-scale transformers. Unlike round-to-nearest schemes, GPTQ minimizes the quantization error using approximate second-order information derived from the Hessian of the loss. It quantizes weights in a layer-wise fashion while updating unquantized weights to compensate for introduced error, achieving efficient `int3/4` quantization of models as large as OPT-175B or BLOOM-176B without finetuning. Practical implementations are available through [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) and [LLM.int4](https://github.com/IST-DASLab/gptq).\n\nGPTQ significantly outperforms simple rounding methods by preserving perplexity under low-bit quantization. Notably, it is one of the few techniques that scales to 100B+ parameter models using modest compute (e.g., a single A100 GPU). While it focuses on weight-only quantization, activation quantization can be layered on top via orthogonal techniques such as SmoothQuant.\n\n##### Process\n\n1.  **Layer-Wise Quantization Objective**\n    \n    *   For a linear layer with weight matrix WWW and input activations XXX, GPTQ minimizes the reconstruction error after quantization:\n        \n        minŴ ‖WX−Ŵ X‖22minW^‖WX−W^X‖22\n        \n        \\\\min\\_{\\\\hat{W}} \\\\| WX - \\\\hat{W}X \\\\|\\_2^2\n    *   Quantization is performed column-by-column (i.e., per weight vector), and compensation is applied to unquantized weights to preserve the overall output fidelity.\n        \n2.  **Approximate Second-Order Weight Selection (OBQ Foundation)**\n    \n    *   GPTQ builds on the Optimal Brain Quantization (OBQ) framework, which selects the next weight wqwqw\\_q to quantize by minimizing its induced error, scaled by its Hessian diagonal element:\n        \n        wq\\=argmin(quant(wq)−wq)2\\[H−1\\]qq,δ\\=−wq−quant(wq)\\[H−1\\]qq(H−1):,qwq\\=arg⁡min(quant(wq)−wq)2\\[H−1\\]qq,δ\\=−wq−quant(wq)\\[H−1\\]qq(H−1):,q\n        \n        w\\_q = \\\\arg\\\\min \\\\frac{(\\\\text{quant}(w\\_q) - w\\_q)^2}{\\[H^{-1}\\]\\_{qq}}, \\\\quad \\\\delta = -\\\\frac{w\\_q - \\\\text{quant}(w\\_q)}{\\[H^{-1}\\]\\_{qq}} (H^{-1})\\_{:,q}\n    *   The inverse Hessian H−1\\=(2XX⊤+λI)−1H−1\\=(2XX⊤+λI)−1H^{-1} = (2X X^\\\\top + \\\\lambda I)^{-1} captures sensitivity of the layer outputs to changes in weights. This allows for compensation of quantization-induced error by adjusting the remaining unquantized weights.\n        \n3.  **Blockwise Column Quantization with Shared Hessian**\n    \n    *   GPTQ introduces the insight that, in large layers, quantizing all rows in a fixed column order yields nearly the same accuracy as a greedy per-weight order.\n        \n    *   This allows sharing the Hessian across rows and amortizing its computation—resulting in a complexity reduction from O(drow⋅d3col)O(drow⋅dcol3)O(d\\_{\\\\text{row}} \\\\cdot d\\_{\\\\text{col}}^3) to O(max(drow⋅d2col,d3col))O(max(drow⋅dcol2,dcol3))O(\\\\max(d\\_{\\\\text{row}} \\\\cdot d\\_{\\\\text{col}}^2, d\\_{\\\\text{col}}^3)).\n        \n    *   The following figure ([source](https://arxiv.org/abs/2210.17323)) illustrates the GPTQ quantization procedure. Blocks of consecutive columns (bolded) are quantized at a given step, using the inverse Hessian information stored in the Cholesky decomposition, and the remaining weights (blue) are updated at the end of the step. The quantization procedure is applied recursively inside each block: the white middle column is currently being quantized.\n        \n    \n    ![GPTQ Figure 2 - Quantization Procedure](../assets/model-compression/GPTQ.jpg)\n    \n4.  **Lazy Batch Updates for GPU Efficiency**\n    \n    *   To alleviate the memory-bandwidth bottleneck of GPU kernels, GPTQ processes batches of columns (e.g., 128) before updating the full weight matrix.\n    *   This “lazy update” scheme postpones weight and Hessian modifications until a full block has been processed, improving throughput and parallelism.\n5.  **Cholesky-Based Inversion for Numerical Stability**\n    \n    *   To avoid numerical instability from repeatedly inverting Hessians during block updates, GPTQ reformulates the update rule using a Cholesky decomposition:\n        \n        H−1\\=LL⊤,computed once per blockH−1\\=LL⊤,computed once per block\n        \n        H^{-1} = L L^\\\\top, \\\\quad \\\\text{computed once per block}\n    *   Combined with dampening (adding λIλI\\\\lambda I to the Hessian), this ensures stability across very large models (e.g., >100B parameters).\n        \n6.  **Quantization Scheme**\n    \n    *   GPTQ supports asymmetric per-row quantization with `int4` or `int3` bitwidths.\n    *   The quantization grid is fixed via min/max values per row, and weights are quantized using:\n        \n        ŵ ij\\=Δi⋅Round(wij−ziΔi)+ziw^ij\\=Δi⋅Round(wij−ziΔi)+zi\n        \n        \\\\hat{w}\\_{ij} = \\\\Delta\\_i \\\\cdot \\\\text{Round} \\\\left( \\\\frac{w\\_{ij} - z\\_i}{\\\\Delta\\_i} \\\\right) + z\\_i\n        *   where ΔiΔi\\\\Delta\\_i is the scale and ziziz\\_i the zero point for row iii.\n7.  **Implementation & Runtime**\n    \n    *   Full quantization of a 175B parameter model (OPT-175B or BLOOM-176B) takes ~4 GPU hours on an NVIDIA A100 using 128 calibration samples from C4.\n    *   Quantization is applied layer-wise with minimal memory overhead by reloading and processing one transformer block at a time.\n    *   Models quantized with GPTQ can run on a single GPU, achieving up to 4.5× speedups over `float16` baselines.\n\n**Layer-Wise Quantization Objective**\n\n*   For a linear layer with weight matrix WWW and input activations XXX, GPTQ minimizes the reconstruction error after quantization:\n    \n    minŴ ‖WX−Ŵ X‖22minW^‖WX−W^X‖22\n    \n    \\\\min\\_{\\\\hat{W}} \\\\| WX - \\\\hat{W}X \\\\|\\_2^2\n*   Quantization is performed column-by-column (i.e., per weight vector), and compensation is applied to unquantized weights to preserve the overall output fidelity.\n    \n\nFor a linear layer with weight matrix WWW and input activations XXX, GPTQ minimizes the reconstruction error after quantization:\n\nQuantization is performed column-by-column (i.e., per weight vector), and compensation is applied to unquantized weights to preserve the overall output fidelity.\n\n**Approximate Second-Order Weight Selection (OBQ Foundation)**\n\n*   GPTQ builds on the Optimal Brain Quantization (OBQ) framework, which selects the next weight wqwqw\\_q to quantize by minimizing its induced error, scaled by its Hessian diagonal element:\n    \n    wq\\=argmin(quant(wq)−wq)2\\[H−1\\]qq,δ\\=−wq−quant(wq)\\[H−1\\]qq(H−1):,qwq\\=arg⁡min(quant(wq)−wq)2\\[H−1\\]qq,δ\\=−wq−quant(wq)\\[H−1\\]qq(H−1):,q\n    \n    w\\_q = \\\\arg\\\\min \\\\frac{(\\\\text{quant}(w\\_q) - w\\_q)^2}{\\[H^{-1}\\]\\_{qq}}, \\\\quad \\\\delta = -\\\\frac{w\\_q - \\\\text{quant}(w\\_q)}{\\[H^{-1}\\]\\_{qq}} (H^{-1})\\_{:,q}\n*   The inverse Hessian H−1\\=(2XX⊤+λI)−1H−1\\=(2XX⊤+λI)−1H^{-1} = (2X X^\\\\top + \\\\lambda I)^{-1} captures sensitivity of the layer outputs to changes in weights. This allows for compensation of quantization-induced error by adjusting the remaining unquantized weights.\n    \n\nGPTQ builds on the Optimal Brain Quantization (OBQ) framework, which selects the next weight wqwqw\\_q to quantize by minimizing its induced error, scaled by its Hessian diagonal element:\n\nThe inverse Hessian H−1\\=(2XX⊤+λI)−1H−1\\=(2XX⊤+λI)−1H^{-1} = (2X X^\\\\top + \\\\lambda I)^{-1} captures sensitivity of the layer outputs to changes in weights. This allows for compensation of quantization-induced error by adjusting the remaining unquantized weights.\n\n**Blockwise Column Quantization with Shared Hessian**\n\n*   GPTQ introduces the insight that, in large layers, quantizing all rows in a fixed column order yields nearly the same accuracy as a greedy per-weight order.\n    \n*   This allows sharing the Hessian across rows and amortizing its computation—resulting in a complexity reduction from O(drow⋅d3col)O(drow⋅dcol3)O(d\\_{\\\\text{row}} \\\\cdot d\\_{\\\\text{col}}^3) to O(max(drow⋅d2col,d3col))O(max(drow⋅dcol2,dcol3))O(\\\\max(d\\_{\\\\text{row}} \\\\cdot d\\_{\\\\text{col}}^2, d\\_{\\\\text{col}}^3)).\n    \n*   The following figure ([source](https://arxiv.org/abs/2210.17323)) illustrates the GPTQ quantization procedure. Blocks of consecutive columns (bolded) are quantized at a given step, using the inverse Hessian information stored in the Cholesky decomposition, and the remaining weights (blue) are updated at the end of the step. The quantization procedure is applied recursively inside each block: the white middle column is currently being quantized.\n    \n\nGPTQ introduces the insight that, in large layers, quantizing all rows in a fixed column order yields nearly the same accuracy as a greedy per-weight order.\n\nThis allows sharing the Hessian across rows and amortizing its computation—resulting in a complexity reduction from O(drow⋅d3col)O(drow⋅dcol3)O(d\\_{\\\\text{row}} \\\\cdot d\\_{\\\\text{col}}^3) to O(max(drow⋅d2col,d3col))O(max(drow⋅dcol2,dcol3))O(\\\\max(d\\_{\\\\text{row}} \\\\cdot d\\_{\\\\text{col}}^2, d\\_{\\\\text{col}}^3)).\n\nThe following figure ([source](https://arxiv.org/abs/2210.17323)) illustrates the GPTQ quantization procedure. Blocks of consecutive columns (bolded) are quantized at a given step, using the inverse Hessian information stored in the Cholesky decomposition, and the remaining weights (blue) are updated at the end of the step. The quantization procedure is applied recursively inside each block: the white middle column is currently being quantized.\n\n![GPTQ Figure 2 - Quantization Procedure](../assets/model-compression/GPTQ.jpg)\n\n**Lazy Batch Updates for GPU Efficiency**\n\n*   To alleviate the memory-bandwidth bottleneck of GPU kernels, GPTQ processes batches of columns (e.g., 128) before updating the full weight matrix.\n*   This “lazy update” scheme postpones weight and Hessian modifications until a full block has been processed, improving throughput and parallelism.\n\n**Cholesky-Based Inversion for Numerical Stability**\n\n*   To avoid numerical instability from repeatedly inverting Hessians during block updates, GPTQ reformulates the update rule using a Cholesky decomposition:\n    \n    H−1\\=LL⊤,computed once per blockH−1\\=LL⊤,computed once per block\n    \n    H^{-1} = L L^\\\\top, \\\\quad \\\\text{computed once per block}\n*   Combined with dampening (adding λIλI\\\\lambda I to the Hessian), this ensures stability across very large models (e.g., >100B parameters).\n    \n\nTo avoid numerical instability from repeatedly inverting Hessians during block updates, GPTQ reformulates the update rule using a Cholesky decomposition:\n\nCombined with dampening (adding λIλI\\\\lambda I to the Hessian), this ensures stability across very large models (e.g., >100B parameters).\n\n**Quantization Scheme**\n\n*   GPTQ supports asymmetric per-row quantization with `int4` or `int3` bitwidths.\n*   The quantization grid is fixed via min/max values per row, and weights are quantized using:\n    \n    ŵ ij\\=Δi⋅Round(wij−ziΔi)+ziw^ij\\=Δi⋅Round(wij−ziΔi)+zi\n    \n    \\\\hat{w}\\_{ij} = \\\\Delta\\_i \\\\cdot \\\\text{Round} \\\\left( \\\\frac{w\\_{ij} - z\\_i}{\\\\Delta\\_i} \\\\right) + z\\_i\n    *   where ΔiΔi\\\\Delta\\_i is the scale and ziziz\\_i the zero point for row iii.\n\nThe quantization grid is fixed via min/max values per row, and weights are quantized using:\n\n*   where ΔiΔi\\\\Delta\\_i is the scale and ziziz\\_i the zero point for row iii.\n\n**Implementation & Runtime**\n\n*   Full quantization of a 175B parameter model (OPT-175B or BLOOM-176B) takes ~4 GPU hours on an NVIDIA A100 using 128 calibration samples from C4.\n*   Quantization is applied layer-wise with minimal memory overhead by reloading and processing one transformer block at a time.\n*   Models quantized with GPTQ can run on a single GPU, achieving up to 4.5× speedups over `float16` baselines.\n\n##### Pros\n\n*   **Highly accurate**: Maintains perplexity within 0.03 of `float16` for OPT-175B (`int4`) and tolerable degradation at `int3`.\n*   **Scalable**: Efficient enough to quantize 100B+ parameter models using a single GPU.\n*   **Hardware-efficient**: Enables deployment of massive LLMs on consumer-grade GPUs (e.g., RTX 3090).\n*   **Open-source tooling**: Supported by AutoGPTQ, Hugging Face Transformers, and integrations with `load_in_4bit=True`.\n\n##### Cons\n\n*   **Weight-only**: Does not include activation quantization; activation memory remains in `float16` unless combined with other methods.\n*   **Nontrivial math**: Relies on Hessian approximations and matrix inversions, which may complicate custom implementation or adaptation.\n*   **Challenging for non-standard layers**: Works best with standard linear layers; adaptation for fused or exotic architectures may require modification.\n\n#### SmoothQuant\n\n*   Introduced in [SmoothQuant: Accurate and Efficient Post‑Training Quantization for Large Language Models](https://arxiv.org/abs/2211.10438) by Xiao et al. (2022), SmoothQuant enables uniform 8-bit quantization for both weights and activations (W8A8) in LLMs by balancing the quantization difficulty between them. It allows high-accuracy post-training quantization (PTQ) on transformer architectures without requiring fine-tuning. A high-level explanation is also available via [Lei Mao’s Log Book](https://leimao.github.io/blog/SmoothQuant-LLM-Quantization/).\n\n##### Process\n\n1.  **Analyze Activation Outliers**: Activation tensors in LLMs often have long-tailed distributions, leading to a high dynamic range. These outliers cause large quantization errors when mapping to low-bit formats like `int8`.\n    \n2.  **Offline Scaling of Input and Weights**: To reduce activation outliers, SmoothQuant proposes to pre-scale input activations and inversely scale the associated weight matrices before quantization. This is done by computing per-channel maximum absolute values of activation tensors and applying the following scaling transformation:\n    \n    xscaled\\=xsWscaled\\=W∗sxscaled\\=xsWscaled\\=W∗s\n    \n    x\\_{scaled} = \\\\frac{x}{s} \\\\\\\\ W\\_{scaled} = W \\* s\n    *   Here, sss is the scaling factor (per input channel), xxx is the activation input, and WWW is the weight matrix.\n    *   This transformation preserves the original linear operation because:\n        \n        x@W≈(xs)@(W∗s)x@W≈(xs)@(W∗s)\n        \n        x @ W ≈ (\\\\frac{x}{s}) @ (W \\* s)\n3.  **Quantize Scaled Tensors**: Apply standard post-training quantization (e.g., `torch.quantize_per_tensor`) on the scaled weight and activation tensors using uniform `int8` quantization.\n    \n4.  **No Runtime Overhead**: The inverse scaling factors sss are folded into the preceding layers during offline preprocessing. At inference, the quantized model does not require additional computation to reverse the scaling—hence, preserving speed.\n    \n\n**Analyze Activation Outliers**: Activation tensors in LLMs often have long-tailed distributions, leading to a high dynamic range. These outliers cause large quantization errors when mapping to low-bit formats like `int8`.\n\n**Offline Scaling of Input and Weights**: To reduce activation outliers, SmoothQuant proposes to pre-scale input activations and inversely scale the associated weight matrices before quantization. This is done by computing per-channel maximum absolute values of activation tensors and applying the following scaling transformation:\n\n*   Here, sss is the scaling factor (per input channel), xxx is the activation input, and WWW is the weight matrix.\n*   This transformation preserves the original linear operation because:\n    \n    x@W≈(xs)@(W∗s)x@W≈(xs)@(W∗s)\n    \n    x @ W ≈ (\\\\frac{x}{s}) @ (W \\* s)\n\nThis transformation preserves the original linear operation because:\n\n**Quantize Scaled Tensors**: Apply standard post-training quantization (e.g., `torch.quantize_per_tensor`) on the scaled weight and activation tensors using uniform `int8` quantization.\n\n**No Runtime Overhead**: The inverse scaling factors sss are folded into the preceding layers during offline preprocessing. At inference, the quantized model does not require additional computation to reverse the scaling—hence, preserving speed.\n\n##### Pros\n\n*   Training-free and calibration-light—requires only a few batches of representative data for statistics.\n*   Allows fully static W8A8 quantization for transformers, which were previously hard to quantize due to activation outliers.\n*   Compatible with major LLMs like Llama, OPT, BLOOM, and GLM.\n*   Hardware-friendly: `int8` inference is highly optimized on modern CPUs (e.g., VNNI, AMX) and GPUs.\n*   Achieves substantial efficiency improvements:\n    \n    *   ~2× reduction in memory footprint.\n    *   ~1.5× to 2× speedup on supported backends.\n    *   <0.5% accuracy loss on common NLP benchmarks.\n\nAchieves substantial efficiency improvements:\n\n*   ~2× reduction in memory footprint.\n*   ~1.5× to 2× speedup on supported backends.\n*   <0.5% accuracy loss on common NLP benchmarks.\n\n##### Cons\n\n*   Limited to 8-bit formats—does not address extreme quantization (e.g., 4-bit or binary).\n*   Effectiveness depends on the distribution of activations. Improper scaling (e.g., due to poor calibration data) may still degrade performance.\n*   Static scale determination can be fragile in models with dynamic context (e.g., prompts of variable length).\n\n#### Activation-Aware Weight Quantization (AWQ)\n\n*   Introduced in [AWQ: Activation‑aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978) by Lin et al. (2024), AWQ is a post‑training, weight‑only quantization technique tailored for LLMs. It identifies and protects _salient channels_—those with large activations—via per‑channel scaling derived from calibration, enabling accurate **`int4`/`int3`** quantization without retraining. Reference implementation (AutoAWQ and CUDA kernels) is available on [GitHub](https://github.com/mit-han-lab/llm-awq).\n*   AWQ offers a highly practical and accurate low-bit, weight-only quantization path. By using activation statistics to protect the most critical channels, it achieves near full-precision accuracy under `int4/3` quantization without training. It retains the efficiency of group-wise kernels for deployment while minimizing model size and speeding up inference for LLMs suited to edge and GPU environments.\n\n##### Process\n\n1.  **Calibration Pass**\n    \n    *   Run a _small calibration dataset_ through the unquantized model to gather per-channel activation statistics, typically the expected absolute value 𝔼\\[‖xi‖\\]E\\[‖xi‖\\]\\\\mathbb{E}\\[\\\\|x\\_i\\\\|\\] of input to each weight channel iii.\n2.  **Group-Wise Weight Quantization Baseline**\n    \n    *   Use group size GGG (e.g. 32 channels) to quantize weights ww\\\\mathbf{w} via a uniform symmetric scheme:\n    \n    Q(w)\\=Δ⋅Round(w/Δ),Δ\\=max|w|2b−1−1Q(w)\\=Δ⋅Round(w/Δ),Δ\\=max|w|2b−1−1\n    \n    Q(w) = \\\\Delta \\\\cdot \\\\text{Round}\\\\left(w / \\\\Delta\\\\right), \\\\quad \\\\Delta = \\\\frac{\\\\max |w|}{2^{b-1}-1}\n    *   The quantization error in group-wise quantization is proportional to input activation magnitude rather than weight magnitude alone.\n3.  **Compute Activation-Based Scaling Factors**\n    \n    *   Let s(x)i\\=𝔼x\\[‖xi‖\\]si(x)\\=Ex\\[‖xi‖\\]s\\_i^{(x)} = \\\\mathbb{E}\\_x \\[\\\\|x\\_i\\\\|\\] represent average per-channel activation magnitude.\n    *   For scaling exponent α∈\\[0,1\\]α∈\\[0,1\\]\\\\alpha \\\\in \\[0,1\\], define:\n    \n    s(α)i\\=(s(x)i)αsi(α)\\=(si(x))α\n    \n    s\\_i^{(\\\\alpha)} = (s\\_i^{(x)})^\\\\alpha\n    \n    *   Use a **small grid search** over αα\\\\alpha to minimize an approximate MSE:\n    \n    𝔼x‖‖‖Q(w⋅diag(s(α)))(s(α))−1x−wx‖‖‖2Ex‖Q(w⋅diag(s(α)))(s(α))−1x−wx‖2\n    \n    \\\\mathbb{E}\\_x \\\\left\\\\lVert Q(\\\\mathbf{w} \\\\cdot \\\\mathrm{diag}(s^{(\\\\alpha)}))\\\\bigl(s^{(\\\\alpha)}\\\\bigr)^{-1}x - \\\\mathbf{w}x \\\\right\\\\rVert^2\n    *   Choose the α∗α∗\\\\alpha^\\* that yields the lowest simulated error (no backprop required).\n4.  **Scale, Quantize, and Fuse**\n    \n    *   Transform weights and activations as:\n    \n    w̃ i\\=wi⋅s∗i,x̃ i\\=xi/s∗isuch thatw̃ ⊤x̃ \\=w⊤xw~i\\=wi⋅si∗,x~i\\=xi/si∗such thatw~⊤x~\\=w⊤x\n    \n    \\\\tilde w\\_i = w\\_i \\\\cdot s\\_i^\\*, \\\\quad \\\\tilde x\\_i = x\\_i / s\\_i^\\* \\\\quad \\\\text{such that} \\\\quad \\\\tilde w^\\\\top \\\\tilde x = w^\\\\top x\n    *   Apply **`int4` or `int3` group-wise quantization** to w̃ w~\\\\tilde w using a single scale per group.\n    *   Fuse the activation scaling s−1isi−1s\\_i^{-1} directly into the preceding layer normalization or linear layer, avoiding runtime rescaling and preserving inference speed by leveraging existing CUDA kernels from weight-only quantization libraries.\n5.  **Deployment Optimization**\n    \n    *   AWQ pairs with **TinyChat**, an inference engine optimized for 4-bit weight-only transformers with kernel fusion and reorder-free dequantization.\n    *   Uses **platform‑aware weight packing** to maximize throughput on GPUs (observed ~3× speedup over Hugging Face `float16` with negligible accuracy drop).\n\n**Calibration Pass**\n\n*   Run a _small calibration dataset_ through the unquantized model to gather per-channel activation statistics, typically the expected absolute value 𝔼\\[‖xi‖\\]E\\[‖xi‖\\]\\\\mathbb{E}\\[\\\\|x\\_i\\\\|\\] of input to each weight channel iii.\n\n**Group-Wise Weight Quantization Baseline**\n\n*   Use group size GGG (e.g. 32 channels) to quantize weights ww\\\\mathbf{w} via a uniform symmetric scheme:\n\n*   The quantization error in group-wise quantization is proportional to input activation magnitude rather than weight magnitude alone.\n\n**Compute Activation-Based Scaling Factors**\n\n*   Let s(x)i\\=𝔼x\\[‖xi‖\\]si(x)\\=Ex\\[‖xi‖\\]s\\_i^{(x)} = \\\\mathbb{E}\\_x \\[\\\\|x\\_i\\\\|\\] represent average per-channel activation magnitude.\n*   For scaling exponent α∈\\[0,1\\]α∈\\[0,1\\]\\\\alpha \\\\in \\[0,1\\], define:\n\n*   Use a **small grid search** over αα\\\\alpha to minimize an approximate MSE:\n\n*   Choose the α∗α∗\\\\alpha^\\* that yields the lowest simulated error (no backprop required).\n\n**Scale, Quantize, and Fuse**\n\n*   Transform weights and activations as:\n\n*   Apply **`int4` or `int3` group-wise quantization** to w̃ w~\\\\tilde w using a single scale per group.\n*   Fuse the activation scaling s−1isi−1s\\_i^{-1} directly into the preceding layer normalization or linear layer, avoiding runtime rescaling and preserving inference speed by leveraging existing CUDA kernels from weight-only quantization libraries.\n\n**Deployment Optimization**\n\n*   AWQ pairs with **TinyChat**, an inference engine optimized for 4-bit weight-only transformers with kernel fusion and reorder-free dequantization.\n*   Uses **platform‑aware weight packing** to maximize throughput on GPUs (observed ~3× speedup over Hugging Face `float16` with negligible accuracy drop).\n\n##### Pros\n\n*   **Salient-channel preservation**: By scaling up high-activation channels, AWQ protects the most influential weights using only ~1% additive precision, significantly reducing quantization error.\n*   **Training‑less**: Requires no finetuning or backpropagation—calibration and closed-form scaling search are sufficient, preserving generalization across domains including instruction-tuned and multi-modal LMs.\n*   **Hardware‑efficient**: Retains group-wise quantization kernels; activation rescaling is fused into existing linear or layer-norm layers, maintaining inference latency and memory efficiency.\n\n##### Cons\n\n*   **Calibration dependency**: Requires representative activation samples and search over αα\\\\alpha, which adds one preprocessing pass but no training.\n*   **Limited activation reduction**: Activations are not quantized (typically kept in `float16`), so runtime memory use is not halved.\n*   **Architecture constraints**: Fusion of scaling into preceding layernorm assumes alignment between weight input channels and layernorm channels; may require adaptation for custom architectures.\n\n#### GGUF Quantization (Legacy, K‑Quants, I‑Quants)\n\n*   GGUF is a binary format optimized for fast loading and saving of models, making it highly efficient for inference purposes. GGUF is designed for use with GGML and other executors and was developed by [@ggerganov](https://github.com/ggerganov), who also created `llama.cpp`, a widely-used C/C++ LLM inference framework. Models trained in PyTorch or other frameworks can be quantized and converted to GGUF using community tools for deployment on low-resource hardware or CPU-only systems. A detailed summary of GGUF is available in [this](https://www.reddit.com/r/LocalLlama/comments/1ba55rj/overview_of_gguf_quantization_methods/) Reddit post.\n    \n*   The GGUF file stores tensors and metadata in a compact and readable format that supports a range of quantization methods including legacy quants, K-quants, and I-quants. Quantization blocks encode 256 weights each, along with minimal overhead (e.g., scale, zero-point, or LUT references), and are decoded efficiently during inference using architecture-specific kernels. The format also supports optional importance matrices and tokenizers directly embedded into the file, eliminating external dependencies during inference.\n    \n*   The following figure ([source](https://huggingface.co/docs/hub/en/gguf)) illustrates the internal structure of a GGUF model file, including the tensor and metadata layout:\n    \n\nGGUF is a binary format optimized for fast loading and saving of models, making it highly efficient for inference purposes. GGUF is designed for use with GGML and other executors and was developed by [@ggerganov](https://github.com/ggerganov), who also created `llama.cpp`, a widely-used C/C++ LLM inference framework. Models trained in PyTorch or other frameworks can be quantized and converted to GGUF using community tools for deployment on low-resource hardware or CPU-only systems. A detailed summary of GGUF is available in [this](https://www.reddit.com/r/LocalLlama/comments/1ba55rj/overview_of_gguf_quantization_methods/) Reddit post.\n\nThe GGUF file stores tensors and metadata in a compact and readable format that supports a range of quantization methods including legacy quants, K-quants, and I-quants. Quantization blocks encode 256 weights each, along with minimal overhead (e.g., scale, zero-point, or LUT references), and are decoded efficiently during inference using architecture-specific kernels. The format also supports optional importance matrices and tokenizers directly embedded into the file, eliminating external dependencies during inference.\n\nThe following figure ([source](https://huggingface.co/docs/hub/en/gguf)) illustrates the internal structure of a GGUF model file, including the tensor and metadata layout:\n\n![GGUF structure](/primers/ai/assets/model-compression/GGUF.jpg)\n\n##### Quantization Types\n\n1.  **Legacy Quants (Q40,Q41,Q80,Q40,Q41,Q80,Q4\\_0, Q4\\_1, Q8\\_0, etc.)**\n    \n    *   Basic block-based quantization where each 256-weight block is encoded with 4 or 8 bits per weight and one (Qx0Qx0Qx\\_0) or two (Qx1Qx1Qx\\_1) constants for scaling/offset.\n    *   Simple bit-unpacking operations (bit shift, AND, multiply) make these formats highly efficient for older hardware and platforms without vector acceleration.\n2.  **K-Quants (Q3KS,Q5KM,Q3KS,Q5KM,Q3\\_{K\\_{S}}, Q5\\_{K\\_{M}}, etc.)**\n    \n    *   Smarter allocation of bits across layers or weight blocks, guided by internal heuristics or optional importance matrices.\n    *   Uses combinations of quantization levels in different layers (XS, S, M), optimizing performance-quality tradeoff.\n    *   Maintains speed advantages of legacy quants while improving model fidelity and reducing quantization noise.\n3.  **I-Quants (IQ2XXS,IQ3S,IQ4XS,IQ2XXS,IQ3S,IQ4XS,IQ2\\_{XXS}, IQ3\\_S, IQ4\\_{XS}, etc.)**\n    \n    *   Advanced block-wise quantization using ideas from QuIP; includes lookup tables to store additional decoding values.\n    *   Allows lower bpw (2-4) while preserving model quality, especially useful for extremely memory-constrained inference.\n    *   Lookup-based dequantization introduces more compute overhead and can cause performance regressions on CPU-bound hardware.\n\n**Legacy Quants (Q40,Q41,Q80,Q40,Q41,Q80,Q4\\_0, Q4\\_1, Q8\\_0, etc.)**\n\n*   Basic block-based quantization where each 256-weight block is encoded with 4 or 8 bits per weight and one (Qx0Qx0Qx\\_0) or two (Qx1Qx1Qx\\_1) constants for scaling/offset.\n*   Simple bit-unpacking operations (bit shift, AND, multiply) make these formats highly efficient for older hardware and platforms without vector acceleration.\n\n**K-Quants (Q3KS,Q5KM,Q3KS,Q5KM,Q3\\_{K\\_{S}}, Q5\\_{K\\_{M}}, etc.)**\n\n*   Smarter allocation of bits across layers or weight blocks, guided by internal heuristics or optional importance matrices.\n*   Uses combinations of quantization levels in different layers (XS, S, M), optimizing performance-quality tradeoff.\n*   Maintains speed advantages of legacy quants while improving model fidelity and reducing quantization noise.\n\n**I-Quants (IQ2XXS,IQ3S,IQ4XS,IQ2XXS,IQ3S,IQ4XS,IQ2\\_{XXS}, IQ3\\_S, IQ4\\_{XS}, etc.)**\n\n*   Advanced block-wise quantization using ideas from QuIP; includes lookup tables to store additional decoding values.\n*   Allows lower bpw (2-4) while preserving model quality, especially useful for extremely memory-constrained inference.\n*   Lookup-based dequantization introduces more compute overhead and can cause performance regressions on CPU-bound hardware.\n\n##### GGUF File Layout and Execution\n\n*   Each GGUF file begins with a magic header and version indicator (`0x47 0x47 0x55 0x46` for “GGUF”, currently version 3), followed by two 64-bit integers: the number of tensors and number of metadata key-value pairs.\n*   Tensor definitions include name, shape, type, and byte offset. Supported quant types include formats like `GGML_TYPE_Q2_K`, Q3KQ3KQ3\\_K, or IQ4XSIQ4XSIQ4\\_{XS}.\n*   Metadata stores tokenizer info, architecture name, context length, and any quantization parameters used during export.\n*   Tensors are read from disk at inference time via offset pointers—enabling partial loading or memory-mapped inference.\n\n##### Importance Matrix (Imatrix)\n\n*   An optional matrix that prioritizes preserving accuracy in weights deemed most significant based on a calibration pass.\n*   Can be used with K-quants and legacy quants, not exclusive to I-quants.\n*   Stored directly in the GGUF metadata and silently improves quantization quality with no inference-time cost.\n\n##### Pros\n\n*   **Efficient deployment format**: GGUF enables fast loading, lightweight inference, and portable packaging across platforms.\n*   **Flexible quant schemes**: From legacy-friendly Q80Q80Q8\\_0 to ultra-compressed IQ2XXSIQ2XXSIQ2\\_{XXS}, GGUF supports a wide range of bit-widths and precision tradeoffs.\n*   **All-in-one packaging**: Tokenizers, metadata, and importance matrices are embedded—no need for external configuration files.\n*   **Community driven**: Supported natively by `llama.cpp` and increasingly integrated with Hugging Face tools and runners.\n\n##### Cons\n\n*   **Hardware-specific behavior**: Some quant schemes (especially I-quants) may perform suboptimally on older CPUs or non-VNNI hardware.\n*   **Naming ambiguity**: Quantization method and imatrix usage are not always visible in the filename; may require manual inspection or re-quantization.\n*   **Rapid evolution**: Format and tooling are evolving quickly—older GGUF models may need conversion to newer versions.\n\n#### AWEQ: Activation‑Weight Equalization\n\n*   Introduced in [AWEQ: Post‑Training Quantization with Activation‑Weight Equalization for LLMs](https://arxiv.org/abs/2311.01305) (Nov 2023) by Li et al., AWEQ is a training‑free post‑training quantization technique designed to facilitate both ultra‑low‑bit and 8‑bit weight+activation (e.g., W8A8) quantization in large language models such as Llama and OPT. It works by shifting quantization hardness from activations to weights to reduce error.\n*   AWEQ effectively balances activation and weight ranges channel-wise via **per-channel equalization** followed by **uniform quantization**, incorporating **bias correction** to reduce residual errors. It achieves significantly improved quantization accuracy—especially for W8A8 floating‑point alternatives—without any training or runtime slow-down, making it an excellent choice for production deployments requiring both efficiency and fidelity.\n\n##### Motivation\n\n*   Large‑scale LLM activations often exhibit **long‑tailed per‑channel distributions** with large outliers, making activation quantization challenging even at 8 bits. AWEQ addresses this by balancing activation and weight ranges so that differences in range (and therefore quantization difficulty) are harmonized channel‑wise, reducing wastage in the quantization grid and improving uniform quantization performance.\n\n##### Process (Implementation Overview)\n\n1.  **Channel Range Analysis**\n    \n    *   Run forward passes over a small calibration dataset to compute **per‑channel activation range** r(X)i\\=max(Xi)−min(Xi)r(X)i\\=max(Xi)−min(Xi)r(X)\\_i = \\\\max(X\\_i) - \\\\min(X\\_i) and **weight range** r(W)ir(W)ir(W)\\_i for each linear or attention block tensor. Range refers to max minus min values across all elements in that channel.\n2.  **Equalization Factor Computation**\n    \n    *   Compute a scale vector s∈ℝCs∈RCs \\\\in \\\\mathbb{R}^C to equalize ranges via:\n        \n        X̃ i\\=Xisi,W̃ i\\=si⋅WiX~i\\=Xisi,W~i\\=si⋅Wi\n        \n        \\\\tilde{X}\\_i = \\\\frac{X\\_i}{s\\_i}, \\\\quad \\\\tilde{W}\\_i = s\\_i \\\\cdot W\\_i\n        *   The objective is typically set such that r(X̃ i)\\=r(W̃ i)r(X~i)\\=r(W~i)r(\\\\tilde{X}\\_i) = r(\\\\tilde{W}\\_i) for all iii, thereby maximizing per‑channel quantization precision as defined through product of normalized ranges (see Equations 3–10 in the original text).\n3.  **Tensor Scaling (Fusion)**\n    \n    *   Apply channel‑wise scaling at the input boundaries of transformer modules—such as prior to self‑attention key/value/data and FFN layers.\n    *   Merge activation scaling into preceding layers (e.g., LayerNorm or Linear) to **eliminate runtime overhead**. For example, transform internal W←diag(s)WW←diag(s)WW \\\\leftarrow \\\\mathrm{diag}(s) \\\\, W; hence, activations use quantizable ranges without additional scaling logic.\n4.  **Uniform Quantization**\n    \n    *   Quantize the equalized tensors using **per‑tensor uniform affine quantization** (e.g., 4‑bit or 8‑bit symmetric). Activation quantization thresholds can be fused with the input block for efficient inference.\n5.  **Quantization Bias Correction (BC)**\n    \n    *   Because quantization after scaling and symmetric clipping can introduce a bias ϵ\\=Wf−Wϵ\\=Wf−W\\\\epsilon = W\\_f - W, AWEQ applies post‑hoc **bias correction**:\n    \n    ỹ \\=ye−ϵ⋅𝔼\\[x\\]y~\\=ye−ϵ⋅E\\[x\\]\n    \n    \\\\tilde{y} = y\\_e - \\\\epsilon \\\\cdot \\\\mathbb{E}\\[x\\]\n    *   where 𝔼\\[x\\]E\\[x\\]\\\\mathbb{E}\\[x\\] is estimated over calibration data, and ye\\=(W+ϵ)xye\\=(W+ϵ)xy\\_e = (W + \\\\epsilon)x. This corrects the expected error per layer without changing runtime performance, enhancing stability in deep LLMs without BatchNorm.\n\n**Channel Range Analysis**\n\n*   Run forward passes over a small calibration dataset to compute **per‑channel activation range** r(X)i\\=max(Xi)−min(Xi)r(X)i\\=max(Xi)−min(Xi)r(X)\\_i = \\\\max(X\\_i) - \\\\min(X\\_i) and **weight range** r(W)ir(W)ir(W)\\_i for each linear or attention block tensor. Range refers to max minus min values across all elements in that channel.\n\n**Equalization Factor Computation**\n\n*   Compute a scale vector s∈ℝCs∈RCs \\\\in \\\\mathbb{R}^C to equalize ranges via:\n    \n    X̃ i\\=Xisi,W̃ i\\=si⋅WiX~i\\=Xisi,W~i\\=si⋅Wi\n    \n    \\\\tilde{X}\\_i = \\\\frac{X\\_i}{s\\_i}, \\\\quad \\\\tilde{W}\\_i = s\\_i \\\\cdot W\\_i\n    *   The objective is typically set such that r(X̃ i)\\=r(W̃ i)r(X~i)\\=r(W~i)r(\\\\tilde{X}\\_i) = r(\\\\tilde{W}\\_i) for all iii, thereby maximizing per‑channel quantization precision as defined through product of normalized ranges (see Equations 3–10 in the original text).\n\nCompute a scale vector s∈ℝCs∈RCs \\\\in \\\\mathbb{R}^C to equalize ranges via:\n\n*   The objective is typically set such that r(X̃ i)\\=r(W̃ i)r(X~i)\\=r(W~i)r(\\\\tilde{X}\\_i) = r(\\\\tilde{W}\\_i) for all iii, thereby maximizing per‑channel quantization precision as defined through product of normalized ranges (see Equations 3–10 in the original text).\n\n**Tensor Scaling (Fusion)**\n\n*   Apply channel‑wise scaling at the input boundaries of transformer modules—such as prior to self‑attention key/value/data and FFN layers.\n*   Merge activation scaling into preceding layers (e.g., LayerNorm or Linear) to **eliminate runtime overhead**. For example, transform internal W←diag(s)WW←diag(s)WW \\\\leftarrow \\\\mathrm{diag}(s) \\\\, W; hence, activations use quantizable ranges without additional scaling logic.\n\n**Uniform Quantization**\n\n*   Quantize the equalized tensors using **per‑tensor uniform affine quantization** (e.g., 4‑bit or 8‑bit symmetric). Activation quantization thresholds can be fused with the input block for efficient inference.\n\n**Quantization Bias Correction (BC)**\n\n*   Because quantization after scaling and symmetric clipping can introduce a bias ϵ\\=Wf−Wϵ\\=Wf−W\\\\epsilon = W\\_f - W, AWEQ applies post‑hoc **bias correction**:\n\n*   where 𝔼\\[x\\]E\\[x\\]\\\\mathbb{E}\\[x\\] is estimated over calibration data, and ye\\=(W+ϵ)xye\\=(W+ϵ)xy\\_e = (W + \\\\epsilon)x. This corrects the expected error per layer without changing runtime performance, enhancing stability in deep LLMs without BatchNorm.\n\n##### Pros\n\n*   **Training‑free**, with no need for quantization-aware training or gradient-based fine-tuning.\n*   Supports both **W8A8 activation quantization** and **ultra‑low-bit weight-only quantization**, including `int/4` with robust performance.\n*   **Hardware-friendly**, as it avoids dynamic scaling during inference; changes are statically fused before deployment.\n*   Demonstrates **best-in-class accuracy** on tasks such as zero‑shot Llama 2 7B evaluation (e.g., average: 70.38% over PIQA, HellaSwag, WinoGrande, ARC‑e—all within <0.01 absolute from `float16`).\n\n##### Cons\n\n*   Requires **representative calibration data** to compute statistics and activation range profiles.\n*   **Per‑tensor quantization** may not perform as well as per‑channel for certain weight distributions, though the equalization helps mitigate this.\n*   Slight overhead in computing equalization factors and bias expectations at quantization time (calibration phase only).\n\n#### EXL2 Quantization\n\n*   [ExLlamaV2](https://github.com/turboderp-org/exllamav2#exl2-quantization), commonly known as EXL2, is a flexible, weight-only quantization scheme developed specifically for local inference of large language models on consumer GPUs. It supports mixed-precision quantization with bit-widths from 2 to 8 bits, and can dynamically allocate precision per weight group to optimize model accuracy at a target average bitrate. This makes it suitable for extreme compression of LLMs such as Llama2-70B, enabling execution on GPUs with as little as 24 GB VRAM.\n    \n*   EXL2 builds upon the GPTQ framework but introduces finer-grained control over quantization allocation, using an error-minimization strategy driven by calibration data. Unlike uniform quantization, EXL2 allows important weights to retain higher precision while compressing less critical ones more aggressively. This is implemented without significant performance penalties due to tight integration with ExLlama’s inference engine and CUDA backend.\n    \n\n[ExLlamaV2](https://github.com/turboderp-org/exllamav2#exl2-quantization), commonly known as EXL2, is a flexible, weight-only quantization scheme developed specifically for local inference of large language models on consumer GPUs. It supports mixed-precision quantization with bit-widths from 2 to 8 bits, and can dynamically allocate precision per weight group to optimize model accuracy at a target average bitrate. This makes it suitable for extreme compression of LLMs such as Llama2-70B, enabling execution on GPUs with as little as 24 GB VRAM.\n\nEXL2 builds upon the GPTQ framework but introduces finer-grained control over quantization allocation, using an error-minimization strategy driven by calibration data. Unlike uniform quantization, EXL2 allows important weights to retain higher precision while compressing less critical ones more aggressively. This is implemented without significant performance penalties due to tight integration with ExLlama’s inference engine and CUDA backend.\n\n##### Process\n\n1.  **Calibration and Error Evaluation**:\n    \n    *   Begin by passing a small calibration dataset through the model to obtain representative statistics.\n    *   For each linear layer weight matrix, the EXL2 pipeline quantizes the matrix multiple times using different bit-width configurations (e.g., 2, 3, 4, 5, 6, or 8 bits).\n    *   After each quantization trial, compute the quantization error between the original and quantized matrix multiplied by the calibration input. The maximum per-layer error across all trials is tracked.\n2.  **Bitrate-Constrained Optimization**:\n    \n    *   A greedy or grid-based search selects the bit-width assignment that minimizes the **maximum layer-wise error** while satisfying a user-defined **average bitrate target** (e.g., 2.55 bits per weight).\n    *   This allows for **non-uniform quantization within each matrix**, so important rows or columns (typically corresponding to high-magnitude weights or activations) may receive higher precision.\n3.  **Mixed-Bit Packing and Storage Format**:\n    \n    *   Each matrix is stored in a compact format supporting mixed-bit representation. A metadata structure encodes the bit-width used for each group.\n    *   Group size is typically fixed (e.g., 64 or 128), enabling compatibility with blockwise CUDA kernels.\n    *   The storage layout ensures efficient memory access and can be interpreted directly by ExLlama’s fast inference kernels.\n4.  **Inference Support**:\n    \n    *   At runtime, ExLlamaV2 uses custom CUDA kernels capable of unpacking and computing with mixed-bit quantized weights.\n    *   There is no need for runtime dequantization to full precision—matmul and sampling are done directly on quantized values.\n    *   The system also supports **act-order remapping**, allowing reordering of weight matrices to preserve activation alignment in grouped attention layers, which is important for compatibility with GQA architectures and inference speed.\n5.  **Conversion Pipeline**:\n    \n    *   A command-line [script](https://github.com/turboderp-org/exllamav2?tab=readme-ov-file#exl2-quantization) is provided to convert Hugging Face-format models into EXL2 quantized versions. This script includes:\n        \n        *   Automatic bit-width search using calibration data.\n        *   Weight remapping and act-order alignment.\n        *   Storage into a compact format suitable for ExLlamaV2.\n    *   Conversion is computationally intensive, especially for large models, but only needs to be done once.\n        \n\n**Calibration and Error Evaluation**:\n\n*   Begin by passing a small calibration dataset through the model to obtain representative statistics.\n*   For each linear layer weight matrix, the EXL2 pipeline quantizes the matrix multiple times using different bit-width configurations (e.g., 2, 3, 4, 5, 6, or 8 bits).\n*   After each quantization trial, compute the quantization error between the original and quantized matrix multiplied by the calibration input. The maximum per-layer error across all trials is tracked.\n\n**Bitrate-Constrained Optimization**:\n\n*   A greedy or grid-based search selects the bit-width assignment that minimizes the **maximum layer-wise error** while satisfying a user-defined **average bitrate target** (e.g., 2.55 bits per weight).\n*   This allows for **non-uniform quantization within each matrix**, so important rows or columns (typically corresponding to high-magnitude weights or activations) may receive higher precision.\n\n**Mixed-Bit Packing and Storage Format**:\n\n*   Each matrix is stored in a compact format supporting mixed-bit representation. A metadata structure encodes the bit-width used for each group.\n*   Group size is typically fixed (e.g., 64 or 128), enabling compatibility with blockwise CUDA kernels.\n*   The storage layout ensures efficient memory access and can be interpreted directly by ExLlama’s fast inference kernels.\n\n**Inference Support**:\n\n*   At runtime, ExLlamaV2 uses custom CUDA kernels capable of unpacking and computing with mixed-bit quantized weights.\n*   There is no need for runtime dequantization to full precision—matmul and sampling are done directly on quantized values.\n*   The system also supports **act-order remapping**, allowing reordering of weight matrices to preserve activation alignment in grouped attention layers, which is important for compatibility with GQA architectures and inference speed.\n\n**Conversion Pipeline**:\n\n*   A command-line [script](https://github.com/turboderp-org/exllamav2?tab=readme-ov-file#exl2-quantization) is provided to convert Hugging Face-format models into EXL2 quantized versions. This script includes:\n    \n    *   Automatic bit-width search using calibration data.\n    *   Weight remapping and act-order alignment.\n    *   Storage into a compact format suitable for ExLlamaV2.\n*   Conversion is computationally intensive, especially for large models, but only needs to be done once.\n    \n\nA command-line [script](https://github.com/turboderp-org/exllamav2?tab=readme-ov-file#exl2-quantization) is provided to convert Hugging Face-format models into EXL2 quantized versions. This script includes:\n\n*   Automatic bit-width search using calibration data.\n*   Weight remapping and act-order alignment.\n*   Storage into a compact format suitable for ExLlamaV2.\n\nConversion is computationally intensive, especially for large models, but only needs to be done once.\n\n##### Pros\n\n*   **Extreme compression**: Achieves ultra-low bitrates (e.g., 2.5–3.0 bpw) without retraining, enabling 70B models to run on 24 GB GPUs.\n*   **Layer-aware precision allocation**: Allocates bits where they matter most, reducing perceptual degradation in output quality.\n*   **Performance-friendly**: Designed for fast execution with minimal overhead through mixed-bit CUDA kernels.\n*   **Flexible deployment**: Supports a range of bitrates and model sizes, allowing trade-offs between quality and performance.\n\n##### Cons\n\n*   **Complex conversion**: Requires full model calibration, multiple quantization trials per matrix, and custom tooling.\n*   **Conversion time**: Large models (13B–70B) take significant time to convert due to exhaustive per-layer bit-width search.\n*   **Inference compatibility**: Requires ExLlamaV2 backend for proper kernel execution; not compatible with standard PyTorch or ONNX runtimes.\n\n#### SpinQuant\n\n*   Introduced in [SpinQuant: LLM Quantization with Learned Rotations](https://arxiv.org/abs/2405.16406) by Liu et al. (2024), SpinQuant reduces quantization error by applying **learned orthonormal rotations** to weights, activations, and KV-cache blocks. These rotations normalize tensor distributions, mitigate outliers, and enable accurate **W4A4KV4** quantization. The implementation is available on [GitHub](https://github.com/facebookresearch/SpinQuant).\n\n##### Process\n\n1.  **Parameterize rotation matrices**:\n    \n    *   SpinQuant uses blockwise **orthonormal rotation matrices** initialized using Hadamard, shortcut, or random orthonormal bases.\n    *   These rotations are applied to groups of weights, activations, and KV-cache blocks (e.g., QQQ, KKK, VVV matrices or FFN weights), where outliers may exist.\n2.  **Optimize via Cayley-SGD on the Stiefel manifold**:\n    \n    *   A small calibration set is passed through a simulated W4A4KV4 pipeline.\n    *   Quantization error (e.g., MSE or KL divergence) between full-precision and quantized outputs is computed.\n    *   Gradients are backpropagated through the rotation parameters using **Cayley-SGD**, a method that maintains orthonormality constraints by optimizing directly on the Stiefel manifold.\n3.  **Fold optimized rotations into model weights**:\n    \n    *   Once optimized, the learned rotation matrices are **fused into the model weights and biases** (e.g., replacing WWW with RTWRRTWRR^T W R) during preprocessing.\n    *   This ensures that no extra computation is introduced at inference time—quantization is performed on the already rotated tensors.\n4.  **Apply W4A4KV4 quantization**:\n    \n    *   Post-rotation, weights, activations, and KV-cache blocks are quantized to 4-bit using standard uniform quantization schemes.\n    *   The rotations have distributed the influence of large-magnitude outliers, allowing for a tighter and more efficient quantization range.\n\n**Parameterize rotation matrices**:\n\n*   SpinQuant uses blockwise **orthonormal rotation matrices** initialized using Hadamard, shortcut, or random orthonormal bases.\n*   These rotations are applied to groups of weights, activations, and KV-cache blocks (e.g., QQQ, KKK, VVV matrices or FFN weights), where outliers may exist.\n\n**Optimize via Cayley-SGD on the Stiefel manifold**:\n\n*   A small calibration set is passed through a simulated W4A4KV4 pipeline.\n*   Quantization error (e.g., MSE or KL divergence) between full-precision and quantized outputs is computed.\n*   Gradients are backpropagated through the rotation parameters using **Cayley-SGD**, a method that maintains orthonormality constraints by optimizing directly on the Stiefel manifold.\n\n**Fold optimized rotations into model weights**:\n\n*   Once optimized, the learned rotation matrices are **fused into the model weights and biases** (e.g., replacing WWW with RTWRRTWRR^T W R) during preprocessing.\n*   This ensures that no extra computation is introduced at inference time—quantization is performed on the already rotated tensors.\n\n**Apply W4A4KV4 quantization**:\n\n*   Post-rotation, weights, activations, and KV-cache blocks are quantized to 4-bit using standard uniform quantization schemes.\n*   The rotations have distributed the influence of large-magnitude outliers, allowing for a tighter and more efficient quantization range.\n\n##### Pros\n\n*   **Outlier mitigation via distribution normalization**:\n    \n    *   Rotations “smear” large-magnitude values across dimensions, redistributing peak energies that would otherwise dominate quantization bins.\n    *   This normalization significantly reduces the impact of extreme values and improves low-bit quantization fidelity.\n*   **Accuracy preservation**:\n    \n    *   Achieves within **~2.9 points** of full precision on Llama 2 (7B) zero-shot tasks.\n    *   Outperforms existing techniques like AWQ, SmoothQuant, and QuaRot by 19–45% in accuracy retention.\n*   **No runtime overhead**:\n    \n    *   Unlike some quantization techniques that add inference complexity, SpinQuant’s learned rotations are merged offline.\n    *   At inference, the model behaves identically to a standard quantized model, with no additional compute.\n\n**Outlier mitigation via distribution normalization**:\n\n*   Rotations “smear” large-magnitude values across dimensions, redistributing peak energies that would otherwise dominate quantization bins.\n*   This normalization significantly reduces the impact of extreme values and improves low-bit quantization fidelity.\n\n**Accuracy preservation**:\n\n*   Achieves within **~2.9 points** of full precision on Llama 2 (7B) zero-shot tasks.\n*   Outperforms existing techniques like AWQ, SmoothQuant, and QuaRot by 19–45% in accuracy retention.\n\n**No runtime overhead**:\n\n*   Unlike some quantization techniques that add inference complexity, SpinQuant’s learned rotations are merged offline.\n*   At inference, the model behaves identically to a standard quantized model, with no additional compute.\n\n##### Cons\n\n*   **Involves optimization and calibration**:\n    \n    *   Cayley-SGD optimization introduces computational overhead during preprocessing.\n    *   Requires a small validation set to simulate quantization and compute gradients.\n*   **Preprocessing complexity**:\n    \n    *   Folding rotations into model weights adds engineering complexity, especially when targeting hardware deployment.\n    *   Though folded offline, the rotated tensors may have slightly increased numerical range, requiring careful scale selection.\n*   **Larger intermediate tensors**:\n    \n    *   While inference cost remains low, merged rotated weights can increase storage slightly due to loss of weight sparsity or alignment.\n\n**Involves optimization and calibration**:\n\n*   Cayley-SGD optimization introduces computational overhead during preprocessing.\n*   Requires a small validation set to simulate quantization and compute gradients.\n\n**Preprocessing complexity**:\n\n*   Folding rotations into model weights adds engineering complexity, especially when targeting hardware deployment.\n*   Though folded offline, the rotated tensors may have slightly increased numerical range, requiring careful scale selection.\n\n**Larger intermediate tensors**:\n\n*   While inference cost remains low, merged rotated weights can increase storage slightly due to loss of weight sparsity or alignment.\n\n#### FPTQuant\n\n*   Introduced in [FPTQuant: 4-bit Function‑Preserving Transforms for Transformer PTQ](https://arxiv.org/abs/2506.04985) by Pan et al. (2025), Function-Preserving Transforms Quantization (FPTQuant) reshapes transformer activations before quantization to preserve function.\n*   A complementary overview is available in [Lei Mao’s Log Book](https://leimao.github.io/blog/FPTQuant-LLM-Quantization/).\n\n##### Process\n\n1.  **Function-Preserving Activation Transforms**:\n    \n    *   FPTQuant applies mathematically invertible transforms to the activations in attention and feedforward blocks to reduce their dynamic range. These include:\n        \n        *   **Logarithmic transforms**: Applied to soften the long-tailed distributions (especially in attention scores or MLP activations).\n        *   **Affine or exponential transforms**: Normalize activations without changing the computation graph logic.\n2.  **Merging Transforms into Weights**:\n    \n    *   Since these transforms are invertible, the effect can be canceled out by adjusting the downstream linear weights.\n    *   Specifically:\n        \n        *   Let x→f(x)x→f(x)x \\\\rightarrow f(x) be the transform applied to activations.\n        *   Then Wx→Wf−1(f(x))Wx→Wf−1(f(x))Wx \\\\rightarrow Wf^{-1}(f(x)) ensures the output remains unchanged.\n        *   FPTQuant modifies the linear projection weights accordingly so that the transform step is absorbed and the forward function is preserved.\n3.  **Quantization Step**:\n    \n    *   With the dynamic range compressed, 4-bit symmetric per-channel quantization is applied to the adjusted weights using PTQ methods.\n    *   Activations are not explicitly quantized, but their transformed form is compatible with W4A16 runtimes (e.g., vLLM) where only weights are quantized.\n4.  **No Runtime Penalty**:\n    \n    *   All transforms are resolved offline and merged into weights.\n    *   The runtime model is a standard quantized model with no extra ops introduced during inference.\n\n**Function-Preserving Activation Transforms**:\n\n*   FPTQuant applies mathematically invertible transforms to the activations in attention and feedforward blocks to reduce their dynamic range. These include:\n    \n    *   **Logarithmic transforms**: Applied to soften the long-tailed distributions (especially in attention scores or MLP activations).\n    *   **Affine or exponential transforms**: Normalize activations without changing the computation graph logic.\n\nFPTQuant applies mathematically invertible transforms to the activations in attention and feedforward blocks to reduce their dynamic range. These include:\n\n*   **Logarithmic transforms**: Applied to soften the long-tailed distributions (especially in attention scores or MLP activations).\n*   **Affine or exponential transforms**: Normalize activations without changing the computation graph logic.\n\n**Merging Transforms into Weights**:\n\n*   Since these transforms are invertible, the effect can be canceled out by adjusting the downstream linear weights.\n*   Specifically:\n    \n    *   Let x→f(x)x→f(x)x \\\\rightarrow f(x) be the transform applied to activations.\n    *   Then Wx→Wf−1(f(x))Wx→Wf−1(f(x))Wx \\\\rightarrow Wf^{-1}(f(x)) ensures the output remains unchanged.\n    *   FPTQuant modifies the linear projection weights accordingly so that the transform step is absorbed and the forward function is preserved.\n\nSpecifically:\n\n*   Let x→f(x)x→f(x)x \\\\rightarrow f(x) be the transform applied to activations.\n*   Then Wx→Wf−1(f(x))Wx→Wf−1(f(x))Wx \\\\rightarrow Wf^{-1}(f(x)) ensures the output remains unchanged.\n*   FPTQuant modifies the linear projection weights accordingly so that the transform step is absorbed and the forward function is preserved.\n\n**Quantization Step**:\n\n*   With the dynamic range compressed, 4-bit symmetric per-channel quantization is applied to the adjusted weights using PTQ methods.\n*   Activations are not explicitly quantized, but their transformed form is compatible with W4A16 runtimes (e.g., vLLM) where only weights are quantized.\n\n**No Runtime Penalty**:\n\n*   All transforms are resolved offline and merged into weights.\n*   The runtime model is a standard quantized model with no extra ops introduced during inference.\n\n##### Pros\n\n*   Fully **training-free**, **invertible**, and **architecture-agnostic**.\n*   Achieves **`int4` weight-only quantization** with minimal or no accuracy loss, by preserving function through mathematically exact transformation.\n*   Compatible with **W4A16 systems** like vLLM, delivering significant memory and latency improvements without major architectural rework.\n\n##### Cons\n\n*   Primarily targets **weights**—does not quantize activations directly, limiting total memory benefits.\n*   Still an **emerging method**—performance and generalization are under ongoing validation across LLM variants (e.g., Mistral, Gemma).\n*   May require per-layer transform tuning based on architecture layout (e.g., attention vs MLP blocks).\n\n#### Palettization (Weight Clustering)\n\n*   **Palettization**, also known as **weight clustering**, is a quantization scheme that replaces full‑precision weights with low‑bit indices into a small lookup table (LUT). Each weight value is approximated by the nearest centroid in the LUT, enabling efficient storage and retrieval.\n\n##### Process\n\n1.  **Clustering**: Collect all float‑format weights for a layer (or group of layers), then run **k-means clustering** to derive a set of centroids (typically, 2n2n2^{n} entries for n‑bit palettization).\n2.  **Index Mapping**: Each weight is replaced with an integer index pointing to its closest centroid in the LUT. The original full‑precision value is no longer stored.\n3.  **Granularity Options**:\n    *   **Per‑tensor granularity**: A single LUT for the entire weight tensor.\n    *   **Per‑group‑channel granularity**: The tensor is divided into groups of channels (defined by `group_size`), each with its own LUT—offering a better accuracy/compression trade-off.\n4.  **Optional Vector Clustering** (`cluster_dim > 1`): Enables multi-dimensional centroids by clustering weight vectors instead of scalars, improving approximation quality for some architectures.\n5.  **Post‑processing**: Optionally quantize LUT centroids themselves to a lower precision (e.g. `int8`) for additional compression.\n\n*   **Per‑tensor granularity**: A single LUT for the entire weight tensor.\n*   **Per‑group‑channel granularity**: The tensor is divided into groups of channels (defined by `group_size`), each with its own LUT—offering a better accuracy/compression trade-off.\n\n##### Integration in Workflows\n\n*   Available via Apple’s `coremltools.optimize.torch.palettization` API, which injects **FakePalettize** layers into the model for **palettization-aware training (PAT)**.\n*   During training, k‑means clustering is applied online, and the LUT and indices are learned through gradient steps. After convergence, the `finalize()` call folds LUTs and indices into permanent quantized weights.\n\n##### When to Use Palettization\n\n*   **Memory-critical deployment**: Edge devices or mobile apps where weight storage is the bottleneck.\n*   **Aggressive compression**: Scenarios requiring sub‑4‑bit representation.\n*   **Architecture flexibility**: Works with both CNNs and transformers when standard affine quantization struggles.\n*   **Fine‑tunable deployment targets**: Fine-tuning after palettization enables high accuracy while still achieving significant compression ratios.\n\n##### Pros\n\n*   **Extreme compression:** Supports ultra-low bit‑width representations (e.g. 2–4 bits) for weights.\n*   **Memory savings:** Offers major memory savings—each weight becomes a small index instead of a float.\n*   **Vector clustering:** Multidimensional centroids can preserve structure in weight matrices.\n*   **Flexible granularity:** Per-tensor, per-group, or vector-level control enables tailored compression vs. accuracy trade-offs.\n*   **Compatible with fine‑tuning:** Compatible via PAT, allowing retention of accuracy through fine‑tuning post-clustering.\n\n##### Cons\n\n*   Requires additional training or fine‑tuning steps (PAT) to compensate for quantization error.\n*   Clustering and LUT management adds complexity to both training and inference pipelines. In other words, introduces LUT metadata and integer-to-centroid lookup logic in inference.\n*   Larger runtime overhead than standard affine quantization, especially with per-channel or per-group palettization which increases storage overhead for multiple LUTs and adds runtime logic to look up indices.\n*   Less intuitive and more complex to implement than simple scale-based quantization.\n\n#### What to Use When?\n\n*   Selecting the right method depends on deployment goals, model architecture, available compute, and desired trade-offs between accuracy, speed, and memory. Below is a structured guide to help determine **what to use when**.\n    \n*   **For Ultra-Low Bit Weight-Only Quantization (`int3`/`int4`) with No Accuracy Drop**:\n    \n    *   _Use: AWQ, FPTQuant, GPTQ, EXL2, SpinQuant_\n        \n    *   **AWQ**: Best for **fast deployment** of LLMs (e.g., Llama) on **edge GPUs** or **low-latency inference** with prebuilt CUDA kernels. No training or tuning required, and integrates well with TinyChat or similar runtimes.\n    *   **FPTQuant**: Ideal when you need **function-preserving `int4` compression** with no runtime penalty and full compatibility with transformer architectures. Use for **W4A16** deployment in platforms like **vLLM**.\n    *   **GPTQ**: Recommended for **very large models (13B–175B)** where preserving perplexity is critical. Use when quantization accuracy is a priority and you’re comfortable with modest compute during conversion.\n    *   **EXL2**: Choose when running **massive models (e.g., Llama2-70B)** on **consumer GPUs**. Offers the **best compression-speed balance** via dynamic bit allocation and works well with ExLlama.\n    *   **SpinQuant**: Select when targeting **4-bit quantization of both weights and activations** while retaining high accuracy (e.g., for academic or performance-sensitive production use). Best for **W4A4KV4** targets with pre-deployment compute budget.\n*   **For Full W8A8 Quantization (Weight + Activation)**:\n    \n    *   _Use: SmoothQuant, AWEQ_\n        \n    *   **SmoothQuant**: The best choice for **training-free, full `int8` quantization** with minimal setup. Choose for **static quantization** pipelines on **NLP models** like Llama, OPT, or BLOOM where CPU or GPU `int8` inference is desired.\n    *   **AWEQ**: Prefer this over SmoothQuant when you need **better accuracy** with **activation-weight balance**, especially for models that are hard to quantize (e.g., with long-tailed distributions). Supports both **W8A8 and ultra-low-bit variants**, and requires **no fine-tuning**.\n*   **For Mixed-Precision or Variable-Bitrate Quantization**:\n    \n    *   _Use: EXL2, GGUF (K-Quants, I-Quants)_\n        \n    *   **EXL2**: Use when deploying models in **memory-constrained** environments but still wanting to preserve **key model behavior** via **bit allocation per group**. Especially useful for **interactive LLMs** on laptops or desktops.\n    *   **GGUF (K/I-Quants)**: Ideal for **offline, file-efficient packaging** and **CPU or mobile inference** with tooling like `llama.cpp`. Offers a trade-off between compatibility and compression via **predefined quant profiles** (Q3KSQ3KSQ3\\_{K\\_{S}}, IQ4XSIQ4XSIQ4\\_{XS}, etc.).\n*   **For Extreme Compression with Customization or Training Support**:\n    \n    *   _Use: Palettization (Weight Clustering)_\n        \n    *   Use palettization when **maximum compression** is needed and **some fine-tuning is acceptable**. Ideal for **mobile deployment** or **experimental architectures** where LUTs and centroid representation can drastically reduce size.\n    *   Choose **vector clustering** when structure preservation in weight matrices matters (e.g., vision-transformer hybrids or customized transformer blocks).\n*   **For Legacy or Format-Constrained Inference**:\n    \n    *   _Use: GGUF (Legacy Quants)_\n        \n    *   Best suited for **lightweight, portable LLM inference** on CPU or embedded hardware via `llama.cpp`.\n    *   Use when you need **fast loading**, **offline conversion**, and **minimal dependencies**, especially for local LLMs or desktop chatbots.\n*   **If Activation Quantization Is Not Required (Weight-Only Models)**:\n    \n    *   _Use: AWQ, GPTQ, FPTQuant, EXL2_\n        \n    *   These methods focus on **`int3/4` weight-only quantization** without modifying activations (typically left in `float16` or `bfloat16`).\n    *   Use these when **inference memory is not your bottleneck**, and you want the best **latency-to-accuracy** trade-off without model retraining.\n*   **Summary Decision Matrix**:\n    \n\nSelecting the right method depends on deployment goals, model architecture, available compute, and desired trade-offs between accuracy, speed, and memory. Below is a structured guide to help determine **what to use when**.\n\n**For Ultra-Low Bit Weight-Only Quantization (`int3`/`int4`) with No Accuracy Drop**:\n\n*   _Use: AWQ, FPTQuant, GPTQ, EXL2, SpinQuant_\n    \n*   **AWQ**: Best for **fast deployment** of LLMs (e.g., Llama) on **edge GPUs** or **low-latency inference** with prebuilt CUDA kernels. No training or tuning required, and integrates well with TinyChat or similar runtimes.\n*   **FPTQuant**: Ideal when you need **function-preserving `int4` compression** with no runtime penalty and full compatibility with transformer architectures. Use for **W4A16** deployment in platforms like **vLLM**.\n*   **GPTQ**: Recommended for **very large models (13B–175B)** where preserving perplexity is critical. Use when quantization accuracy is a priority and you’re comfortable with modest compute during conversion.\n*   **EXL2**: Choose when running **massive models (e.g., Llama2-70B)** on **consumer GPUs**. Offers the **best compression-speed balance** via dynamic bit allocation and works well with ExLlama.\n*   **SpinQuant**: Select when targeting **4-bit quantization of both weights and activations** while retaining high accuracy (e.g., for academic or performance-sensitive production use). Best for **W4A4KV4** targets with pre-deployment compute budget.\n\n_Use: AWQ, FPTQuant, GPTQ, EXL2, SpinQuant_\n\n**For Full W8A8 Quantization (Weight + Activation)**:\n\n*   _Use: SmoothQuant, AWEQ_\n    \n*   **SmoothQuant**: The best choice for **training-free, full `int8` quantization** with minimal setup. Choose for **static quantization** pipelines on **NLP models** like Llama, OPT, or BLOOM where CPU or GPU `int8` inference is desired.\n*   **AWEQ**: Prefer this over SmoothQuant when you need **better accuracy** with **activation-weight balance**, especially for models that are hard to quantize (e.g., with long-tailed distributions). Supports both **W8A8 and ultra-low-bit variants**, and requires **no fine-tuning**.\n\n_Use: SmoothQuant, AWEQ_\n\n**For Mixed-Precision or Variable-Bitrate Quantization**:\n\n*   _Use: EXL2, GGUF (K-Quants, I-Quants)_\n    \n*   **EXL2**: Use when deploying models in **memory-constrained** environments but still wanting to preserve **key model behavior** via **bit allocation per group**. Especially useful for **interactive LLMs** on laptops or desktops.\n*   **GGUF (K/I-Quants)**: Ideal for **offline, file-efficient packaging** and **CPU or mobile inference** with tooling like `llama.cpp`. Offers a trade-off between compatibility and compression via **predefined quant profiles** (Q3KSQ3KSQ3\\_{K\\_{S}}, IQ4XSIQ4XSIQ4\\_{XS}, etc.).\n\n_Use: EXL2, GGUF (K-Quants, I-Quants)_\n\n**For Extreme Compression with Customization or Training Support**:\n\n*   _Use: Palettization (Weight Clustering)_\n    \n*   Use palettization when **maximum compression** is needed and **some fine-tuning is acceptable**. Ideal for **mobile deployment** or **experimental architectures** where LUTs and centroid representation can drastically reduce size.\n*   Choose **vector clustering** when structure preservation in weight matrices matters (e.g., vision-transformer hybrids or customized transformer blocks).\n\n_Use: Palettization (Weight Clustering)_\n\n**For Legacy or Format-Constrained Inference**:\n\n*   _Use: GGUF (Legacy Quants)_\n    \n*   Best suited for **lightweight, portable LLM inference** on CPU or embedded hardware via `llama.cpp`.\n*   Use when you need **fast loading**, **offline conversion**, and **minimal dependencies**, especially for local LLMs or desktop chatbots.\n\n_Use: GGUF (Legacy Quants)_\n\n**If Activation Quantization Is Not Required (Weight-Only Models)**:\n\n*   _Use: AWQ, GPTQ, FPTQuant, EXL2_\n    \n*   These methods focus on **`int3/4` weight-only quantization** without modifying activations (typically left in `float16` or `bfloat16`).\n*   Use these when **inference memory is not your bottleneck**, and you want the best **latency-to-accuracy** trade-off without model retraining.\n\n_Use: AWQ, GPTQ, FPTQuant, EXL2_\n\n**Summary Decision Matrix**:\n\n**Goal / Constraint**\n\n**Recommended Method(s)**\n\nFastest low-bit inference (no training)\n\nAWQ, FPTQuant\n\nBest W4A4 quantization accuracy\n\nSpinQuant\n\nW8A8 quant with balanced scaling\n\nSmoothQuant, AWEQ\n\nLargest models on smallest VRAM\n\nEXL2, GPTQ\n\nMixed precision with dynamic control\n\nEXL2, GGUF (K/I-Quants)\n\nCPU/mobile inference with format support\n\nGGUF\n\nEdge deployment with low compute budget\n\nAWQ, GGUF (Legacy)\n\nExtreme compression + retraining allowed\n\nPalettization\n\nActivation-aware optimizations\n\nSpinQuant, SmoothQuant, AWEQ\n\n**Goal / Constraint**\n\n**Recommended Method(s)**\n\nFastest low-bit inference (no training)\n\nAWQ, FPTQuant\n\nBest W4A4 quantization accuracy\n\nSpinQuant\n\nW8A8 quant with balanced scaling\n\nSmoothQuant, AWEQ\n\nLargest models on smallest VRAM\n\nEXL2, GPTQ\n\nMixed precision with dynamic control\n\nEXL2, GGUF (K/I-Quants)\n\nCPU/mobile inference with format support\n\nGGUF\n\nEdge deployment with low compute budget\n\nAWQ, GGUF (Legacy)\n\nExtreme compression + retraining allowed\n\nPalettization\n\nActivation-aware optimizations\n\nSpinQuant, SmoothQuant, AWEQ\n\n#### Comparative Analysis\n\n**Method**\n\n**Type**\n\n**What Quantized**\n\n**Bits**\n\n**Training-Free?**\n\n**Key Innovation**\n\n**Accuracy Retention**\n\n**Introduced**\n\nUniform Quantization\n\nPost Training Quantization\n\nWeights ± Activations\n\n4–8 bit\n\nyes\n\nSimple affine mapping, per-tensor or per-channel\n\nGood for smooth distributions (~2pt drop)\n\n(Fundamental baseline)\n\n[GPTQ](https://arxiv.org/abs/2210.17323)\n\nPost Training Quantization\n\nWeights only\n\nINT3/4 (also 2-bit)\n\nyes\n\nSecond-order Hessian-based error compensation per-layer\n\nHighly accurate (perplexity within ~0.03 of `float16`)\n\nOct 2022\n\n[SmoothQuant](https://arxiv.org/abs/2211.10438)\n\nPost Training Quantization\n\nWeights + Activations\n\nW8A8\n\nyes\n\nScaling activation/weights to balance quantization difficulty\n\nVery high (<0.5 % loss)\n\nNov 2022\n\n[AWQ](https://arxiv.org/abs/2306.00978)\n\nPost Training Quantization\n\nWeights only\n\nW4\n\nyes\n\nActivation-aware per-channel scaling via calibration\n\nHigh (>`float16`)\n\nJun 2023\n\n[GGUF](https://github.com/ggml-org/llama.cpp)\n\nPost Training Quantization\n\nWeights only\n\n2–8 bit (block-based)\n\nyes (quantized offline)\n\nFlexible binary format with various schemes, importance matrices embedded\n\nVaries by scheme; efficient loading\n\nAug 2023 (as part of \\`llama.cpp\\`)\n\n[AWEQ](https://arxiv.org/abs/2311.01305)\n\nPost Training Quantization\n\nWeights + Activations\n\nW4 or W8A8\n\nyes\n\nActivation-weight equalization + bias correction\n\nBest-in-class (within <0.01 absolute from `float16`)\n\nNov 2023\n\n[EXL2 (ExLlamaV2)](https://github.com/chu-tianxiang/exl2-for-all?utm_source=chatgpt.com)\n\nPost Training Quantization (dynamic allocation)\n\nWeights only\n\nMixed: 2–8 bit\n\nyes\n\nGPTQ-based mixed-bit allocations per layer via error minimization\n\nVery high; Llama2-70B runs on 24 GB GPU\n\nNov 2023\n\n[SpinQuant](https://arxiv.org/abs/2405.16406)\n\nQuantization Aware Training (calibration + optimization)\n\nWeights + Activations + KV-cache\n\nW4A4KV4\n\nno (requires calibration + optimization)\n\nLearned orthonormal rotations to normalize distributions\n\nWithin ~2.9 pt of FP\n\nMay 2024\n\n[FPTQuant](https://arxiv.org/abs/2506.04985)\n\nPost Training Quantization\n\nWeights only\n\nW4 (weight-only)\n\nyes\n\nFunction-preserving invertible transforms to activations, merged into weights\n\nExcellent (minimal loss)\n\nJun 2025\n\n[Palettization](https://apple.github.io/coremltools/docs-guides/source/opt-palettization-overview.html)\n\nQuantization Aware (Training or Fine-tuning)\n\nWeights only\n\n2–4 bit (LUT index)\n\nno (requires fine-tuning or PAT)\n\nWeight clustering via k-means with optional vector or group granularity\n\nHigh (with PAT); moderate otherwise\n\n2024\n\n**Method**\n\n**Type**\n\n**What Quantized**\n\n**Bits**\n\n**Training-Free?**\n\n**Key Innovation**\n\n**Accuracy Retention**\n\n**Introduced**\n\nUniform Quantization\n\nPost Training Quantization\n\nWeights ± Activations\n\n4–8 bit\n\nyes\n\nSimple affine mapping, per-tensor or per-channel\n\nGood for smooth distributions (~2pt drop)\n\n(Fundamental baseline)\n\n[GPTQ](https://arxiv.org/abs/2210.17323)\n\nPost Training Quantization\n\nWeights only\n\nINT3/4 (also 2-bit)\n\nyes\n\nSecond-order Hessian-based error compensation per-layer\n\nHighly accurate (perplexity within ~0.03 of `float16`)\n\nOct 2022\n\n[SmoothQuant](https://arxiv.org/abs/2211.10438)\n\nPost Training Quantization\n\nWeights + Activations\n\nW8A8\n\nyes\n\nScaling activation/weights to balance quantization difficulty\n\nVery high (<0.5 % loss)\n\nNov 2022\n\n[AWQ](https://arxiv.org/abs/2306.00978)\n\nPost Training Quantization\n\nWeights only\n\nW4\n\nyes\n\nActivation-aware per-channel scaling via calibration\n\nHigh (>`float16`)\n\nJun 2023\n\n[GGUF](https://github.com/ggml-org/llama.cpp)\n\nPost Training Quantization\n\nWeights only\n\n2–8 bit (block-based)\n\nyes (quantized offline)\n\nFlexible binary format with various schemes, importance matrices embedded\n\nVaries by scheme; efficient loading\n\nAug 2023 (as part of \\`llama.cpp\\`)\n\n[AWEQ](https://arxiv.org/abs/2311.01305)\n\nPost Training Quantization\n\nWeights + Activations\n\nW4 or W8A8\n\nyes\n\nActivation-weight equalization + bias correction\n\nBest-in-class (within <0.01 absolute from `float16`)\n\nNov 2023\n\n[EXL2 (ExLlamaV2)](https://github.com/chu-tianxiang/exl2-for-all?utm_source=chatgpt.com)\n\nPost Training Quantization (dynamic allocation)\n\nWeights only\n\nMixed: 2–8 bit\n\nyes\n\nGPTQ-based mixed-bit allocations per layer via error minimization\n\nVery high; Llama2-70B runs on 24 GB GPU\n\nNov 2023\n\n[SpinQuant](https://arxiv.org/abs/2405.16406)\n\nQuantization Aware Training (calibration + optimization)\n\nWeights + Activations + KV-cache\n\nW4A4KV4\n\nno (requires calibration + optimization)\n\nLearned orthonormal rotations to normalize distributions\n\nWithin ~2.9 pt of FP\n\nMay 2024\n\n[FPTQuant](https://arxiv.org/abs/2506.04985)\n\nPost Training Quantization\n\nWeights only\n\nW4 (weight-only)\n\nyes\n\nFunction-preserving invertible transforms to activations, merged into weights\n\nExcellent (minimal loss)\n\nJun 2025\n\n[Palettization](https://apple.github.io/coremltools/docs-guides/source/opt-palettization-overview.html)\n\nQuantization Aware (Training or Fine-tuning)\n\nWeights only\n\n2–4 bit (LUT index)\n\nno (requires fine-tuning or PAT)\n\nWeight clustering via k-means with optional vector or group granularity\n\nHigh (with PAT); moderate otherwise\n\n2024",
      "order": 13,
      "orderInChapter": 13,
      "difficulty": 5,
      "estimatedMinutes": 48,
      "tags": [
        "ondevice ai",
        "transformer",
        "attention",
        "cnn",
        "gpt",
        "llm",
        "nlp",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": true,
        "wordCount": 9499,
        "contentLength": 557706
      },
      "nextCards": [
        "ai-model-compression-multimodal-quantization-14",
        "ai-model-compression-device-and-operator-support-across-frameworks-15"
      ],
      "relatedCards": [
        "ai-on-device-transformers-embedding-size-times-vocabulary-size-times-depth-26",
        "ai-on-device-transformers-parameter-tuning-recipes-for-ml-runtimes-27",
        "ai-on-device-transformers-sequence-length-and-kv-cache-size-24",
        "ai-on-device-transformers-tokenizer-and-vocabulary-size-22",
        "ai-on-device-transformers-parameter-count-and-model-depth-25"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#modern-quantization-techniques",
      "scrapedAt": "2025-12-28T11:55:50.969Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-multimodal-quantization-14",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Multimodal Quantization",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>Quantizing multimodal LLMs—especially Vision-Language Models (VLMs) such as BLIP-2, LLaVA, or Flamingo—presents unique complexities not encountered in traditional text-only LLMs. These models process and fuse inputs from disparate modalities (e.g., images and text), resulting in heterogeneous model architectures and dynamic activation distributions that resist uniform quantization techniques.</li>\n  <li>The inherent heterogeneity in architecture, distribution, and task metrics makes naive post-training quantization insufficient for production-grade deployment. Mixed-precision and QAT remain the most promising paths forward, especially when combined with robust calibration data and modality-aware loss functions. As VLMs become more prevalent in edge AI and on-device inference, a new generation of quantization-aware toolchains will be essential to unlock their full potential.</li>\n</ul>\n<h4 id=\"why-vlm-quantization-is-more-complex\">Why VLM Quantization is More Complex</h4>\n<ul>\n  <li>\n    <p>Multimodal models are composed of at least two distinct processing pipelines—one for each modality (e.g., image and text)—and often a third for cross-modal alignment or fusion. This architectural heterogeneity introduces the following challenges:</p>\n\n    <ul>\n      <li><strong>Diverse tensor statistics</strong>: Vision and language inputs yield activations with vastly different distributions and dynamic ranges, making uniform quantization impractical across modalities.</li>\n      <li><strong>Cross-modal attention sensitivity</strong>: Cross-attention layers that fuse modalities are especially fragile to precision loss, as they are responsible for preserving semantic alignment between vision and language.</li>\n      <li><strong>Embedding alignment</strong>: Vision embeddings (e.g., image patches from ViTs) and text embeddings must remain aligned for effective fusion. Quantization artifacts can easily disrupt this shared embedding space.</li>\n      <li><strong>Lack of inductive biases</strong>: Unlike CNNs, which offer natural robustness to quantization via spatial weight sharing and locality, ViTs and transformers often rely more heavily on learned long-range dependencies, which are easily degraded by quantization noise.</li>\n      <li><strong>Multi-objective optimization</strong>: A VLM may be used across many tasks (e.g., captioning, VQA, grounding), requiring quantized models to generalize well across domains, not just on language metrics like perplexity.</li>\n    </ul>\n  </li>\n  <li>\n    <p>To address these challenges, quantization of multimodal models typically involves hybrid and adaptive strategies as described below.</p>\n  </li>\n</ul>\n<p>Multimodal models are composed of at least two distinct processing pipelines—one for each modality (e.g., image and text)—and often a third for cross-modal alignment or fusion. This architectural heterogeneity introduces the following challenges:</p>\n<ul>\n      <li><strong>Diverse tensor statistics</strong>: Vision and language inputs yield activations with vastly different distributions and dynamic ranges, making uniform quantization impractical across modalities.</li>\n      <li><strong>Cross-modal attention sensitivity</strong>: Cross-attention layers that fuse modalities are especially fragile to precision loss, as they are responsible for preserving semantic alignment between vision and language.</li>\n      <li><strong>Embedding alignment</strong>: Vision embeddings (e.g., image patches from ViTs) and text embeddings must remain aligned for effective fusion. Quantization artifacts can easily disrupt this shared embedding space.</li>\n      <li><strong>Lack of inductive biases</strong>: Unlike CNNs, which offer natural robustness to quantization via spatial weight sharing and locality, ViTs and transformers often rely more heavily on learned long-range dependencies, which are easily degraded by quantization noise.</li>\n      <li><strong>Multi-objective optimization</strong>: A VLM may be used across many tasks (e.g., captioning, VQA, grounding), requiring quantized models to generalize well across domains, not just on language metrics like perplexity.</li>\n    </ul>\n<p>To address these challenges, quantization of multimodal models typically involves hybrid and adaptive strategies as described below.</p>\n<h4 id=\"quantizing-the-visual-backbone\">Quantizing the Visual Backbone</h4>\n<ol>\n  <li>\n    <p><strong>CNN-based encoders (e.g., ResNet, EfficientNet)</strong>:</p>\n\n    <ul>\n      <li>CNNs are relatively robust to quantization, and standard per-channel <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization (as used in MobileNet) can often be applied.</li>\n      <li>Pre-trained encoders may be frozen and quantized independently of the rest of the model.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Vision Transformers (ViTs)</strong>:</p>\n\n    <ul>\n      <li>More sensitive to quantization due to their reliance on attention mechanisms and lack of inductive biases.</li>\n      <li>Key operations such as softmax and positional embeddings are particularly fragile.</li>\n      <li>Attention maps are harder to compress as they carry spatial relevance crucial for image understanding.</li>\n    </ul>\n\n    <p><strong>Best practices:</strong></p>\n\n    <ul>\n      <li>Use per-head or per-channel quantization for attention weights.</li>\n      <li>Apply post-training quantization (PTQ) carefully, or use quantization-aware training (QAT) for attention-heavy layers.</li>\n      <li>Maintain FP16 or BF16 precision in early layers or attention blocks if task-critical.</li>\n    </ul>\n  </li>\n</ol>\n<p><strong>CNN-based encoders (e.g., ResNet, EfficientNet)</strong>:</p>\n<ul>\n      <li>CNNs are relatively robust to quantization, and standard per-channel <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization (as used in MobileNet) can often be applied.</li>\n      <li>Pre-trained encoders may be frozen and quantized independently of the rest of the model.</li>\n    </ul>\n<p><strong>Vision Transformers (ViTs)</strong>:</p>\n<ul>\n      <li>More sensitive to quantization due to their reliance on attention mechanisms and lack of inductive biases.</li>\n      <li>Key operations such as softmax and positional embeddings are particularly fragile.</li>\n      <li>Attention maps are harder to compress as they carry spatial relevance crucial for image understanding.</li>\n    </ul>\n<p><strong>Best practices:</strong></p>\n<ul>\n      <li>Use per-head or per-channel quantization for attention weights.</li>\n      <li>Apply post-training quantization (PTQ) carefully, or use quantization-aware training (QAT) for attention-heavy layers.</li>\n      <li>Maintain FP16 or BF16 precision in early layers or attention blocks if task-critical.</li>\n    </ul>\n<h4 id=\"quantizing-the-language-backbone\">Quantizing the Language Backbone</h4>\n<ul>\n  <li>\n    <p>Language processing in VLMs is typically transformer-based (e.g., LLaMA, T5, GPT-style decoders). The quantization techniques here are more mature and generally follow:</p>\n\n    <ul>\n      <li><strong><code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">int4</code> quantization</strong> using post-training methods (e.g., GPTQ, AWQ, SmoothQuant).</li>\n      <li><strong>Per-group or per-channel quantization</strong> for MLPs and attention blocks.</li>\n      <li><strong>Mixed-precision inference</strong>, especially keeping attention output or layer norms in FP16 when accuracy is crucial.</li>\n    </ul>\n  </li>\n</ul>\n<p>Language processing in VLMs is typically transformer-based (e.g., LLaMA, T5, GPT-style decoders). The quantization techniques here are more mature and generally follow:</p>\n<ul>\n      <li><strong><code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">int4</code> quantization</strong> using post-training methods (e.g., GPTQ, AWQ, SmoothQuant).</li>\n      <li><strong>Per-group or per-channel quantization</strong> for MLPs and attention blocks.</li>\n      <li><strong>Mixed-precision inference</strong>, especially keeping attention output or layer norms in FP16 when accuracy is crucial.</li>\n    </ul>\n<h4 id=\"cross-modal-projection-and-fusion-layer-quantization\">Cross-Modal Projection and Fusion Layer Quantization</h4>\n<ul>\n  <li>\n    <p>This is the most critical and fragile component in VLMs. The fusion modules align visual and textual embeddings into a shared latent space.</p>\n  </li>\n  <li><strong>Cross-attention layers</strong> are highly sensitive to quantization because they match image regions with textual tokens. Errors here degrade the entire model’s reasoning ability.</li>\n  <li>\n    <p><strong>Query Transformers (Q-formers)</strong>, as used in BLIP-2, process image features into language-style prompts. Quantization here must preserve alignment fidelity.</p>\n  </li>\n  <li>\n    <p><strong>Strategies:</strong></p>\n\n    <ul>\n      <li>Retain cross-modal fusion (specifically, projection and/or cross-attention layers) in FP16 or use higher-precision <code class=\"language-plaintext highlighter-rouge\">int8</code>.</li>\n      <li>Apply QAT to cross-modal components to preserve alignment under quantization-induced rounding and clipping.</li>\n      <li>Use per-tensor calibration based on multimodal datasets to balance activations across modalities.</li>\n    </ul>\n  </li>\n</ul>\n<p>This is the most critical and fragile component in VLMs. The fusion modules align visual and textual embeddings into a shared latent space.</p>\n<p><strong>Query Transformers (Q-formers)</strong>, as used in BLIP-2, process image features into language-style prompts. Quantization here must preserve alignment fidelity.</p>\n<p><strong>Strategies:</strong></p>\n<ul>\n      <li>Retain cross-modal fusion (specifically, projection and/or cross-attention layers) in FP16 or use higher-precision <code class=\"language-plaintext highlighter-rouge\">int8</code>.</li>\n      <li>Apply QAT to cross-modal components to preserve alignment under quantization-induced rounding and clipping.</li>\n      <li>Use per-tensor calibration based on multimodal datasets to balance activations across modalities.</li>\n    </ul>\n<h4 id=\"quantization-aware-training-qat-in-vlms\">Quantization-Aware Training (QAT) in VLMs</h4>\n<ul>\n  <li>\n    <p>Due to sensitivity in fusion and vision branches, QAT is often <em>required</em> for VLMs, unlike in pure-text LLMs where PTQ often suffices.</p>\n  </li>\n  <li>During QAT, fake quantization layers simulate precision loss during both forward and backward passes.</li>\n  <li>\n    <p>Loss functions may include:</p>\n\n    <ul>\n      <li><strong>Cross-modal alignment loss</strong> (cosine similarity of vision/text embeddings)</li>\n      <li><strong>Task-specific loss</strong> (e.g., VQA classification loss)</li>\n      <li><strong>KL divergence or logit-matching</strong> between full-precision and quantized models</li>\n    </ul>\n  </li>\n  <li>\n    <p>Progressive QAT approaches are sometimes used:</p>\n\n    <ul>\n      <li>Freeze the vision encoder</li>\n      <li>Apply quantization noise gradually to fusion layers</li>\n      <li>Fine-tune using diverse tasks to preserve robustness</li>\n    </ul>\n  </li>\n</ul>\n<p>Due to sensitivity in fusion and vision branches, QAT is often <em>required</em> for VLMs, unlike in pure-text LLMs where PTQ often suffices.</p>\n<p>Loss functions may include:</p>\n<ul>\n      <li><strong>Cross-modal alignment loss</strong> (cosine similarity of vision/text embeddings)</li>\n      <li><strong>Task-specific loss</strong> (e.g., VQA classification loss)</li>\n      <li><strong>KL divergence or logit-matching</strong> between full-precision and quantized models</li>\n    </ul>\n<p>Progressive QAT approaches are sometimes used:</p>\n<ul>\n      <li>Freeze the vision encoder</li>\n      <li>Apply quantization noise gradually to fusion layers</li>\n      <li>Fine-tune using diverse tasks to preserve robustness</li>\n    </ul>\n<h4 id=\"calibration-and-evaluation-in-vlms\">Calibration and Evaluation in VLMs</h4>\n<ul>\n  <li>\n    <p>Calibration datasets for VLMs must reflect the multimodal distribution. Suitable datasets include:</p>\n\n    <ul>\n      <li>MS-COCO (captioning)</li>\n      <li>VQAv2 (VQA)</li>\n      <li>GQA, VizWiz, RefCOCO (referring expression comprehension)</li>\n    </ul>\n  </li>\n  <li>\n    <p>Metrics to evaluate quantized VLMs differ from those used in text-only LLMs:</p>\n\n    <ul>\n      <li><strong>Captioning:</strong> CIDEr, BLEU, METEOR</li>\n      <li><strong>VQA:</strong> answer accuracy</li>\n      <li><strong>Localization:</strong> IoU and precision</li>\n    </ul>\n  </li>\n  <li>\n    <p>These tasks are affected differently by quantization; degradation in fusion modules typically leads to sharper accuracy drops than in text-only settings.</p>\n  </li>\n</ul>\n<p>Calibration datasets for VLMs must reflect the multimodal distribution. Suitable datasets include:</p>\n<ul>\n      <li>MS-COCO (captioning)</li>\n      <li>VQAv2 (VQA)</li>\n      <li>GQA, VizWiz, RefCOCO (referring expression comprehension)</li>\n    </ul>\n<p>Metrics to evaluate quantized VLMs differ from those used in text-only LLMs:</p>\n<ul>\n      <li><strong>Captioning:</strong> CIDEr, BLEU, METEOR</li>\n      <li><strong>VQA:</strong> answer accuracy</li>\n      <li><strong>Localization:</strong> IoU and precision</li>\n    </ul>\n<p>These tasks are affected differently by quantization; degradation in fusion modules typically leads to sharper accuracy drops than in text-only settings.</p>\n<h4 id=\"hybrid-and-mixed-precision-quantization\">Hybrid and Mixed-Precision Quantization</h4>\n<ul>\n  <li>Given the disparity in sensitivity across components:\n    <ul>\n      <li>Use <strong><code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">int4</code></strong> for robust modules (e.g., MLPs, FFNs)</li>\n      <li>Use <strong>FP16 or BF16</strong> for:\n        <ul>\n          <li>Cross-modal projections</li>\n          <li>Final projection</li>\n          <li>Optionally, try attention output and embedding normalization layers</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Mixed-precision kernels</strong> in deployment frameworks (e.g., TensorRT, OpenVINO) allow selective high-precision execution.</li>\n</ul>\n<ul>\n      <li>Use <strong><code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">int4</code></strong> for robust modules (e.g., MLPs, FFNs)</li>\n      <li>Use <strong>FP16 or BF16</strong> for:\n        <ul>\n          <li>Cross-modal projections</li>\n          <li>Final projection</li>\n          <li>Optionally, try attention output and embedding normalization layers</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Cross-modal projections</li>\n          <li>Final projection</li>\n          <li>Optionally, try attention output and embedding normalization layers</li>\n        </ul>\n<h4 id=\"tooling-support\">Tooling Support</h4>\n<ul>\n  <li>\n    <p>Quantization tooling for VLMs is still developing. While text-only LLMs enjoy mature and well-integrated support in libraries like <strong>GPTQ</strong> and <strong>AWQ</strong>, multimodal quantization often requires combining custom workflows with early-stage tooling.</p>\n  </li>\n  <li>\n    <p>Current options include:</p>\n\n    <ul>\n      <li>\n        <p><strong><a href=\"https://developer.nvidia.com/tensorrt\">TensorRT</a></strong>: NVIDIA’s inference engine supports mixed-precision ViT and fusion models with custom kernels for <code class=\"language-plaintext highlighter-rouge\">int8</code> and FP16. Requires ONNX export and hardware-specific calibration.</p>\n      </li>\n      <li>\n        <p><strong><a href=\"https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html\">OpenVINO</a></strong>: Intel’s deployment toolkit for CPU/GPU/VPUs. Supports post-training quantization and VLMs exported via ONNX or Hugging Face pipelines.</p>\n      </li>\n      <li>\n        <p><strong><a href=\"https://huggingface.co/docs/optimum/overview\">Hugging Face Optimum</a></strong>: A library bridging Transformers with hardware backends. Supports quantization workflows with <strong>Intel Neural Compressor</strong> and <strong>ONNX Runtime</strong> for vision+language models.</p>\n      </li>\n      <li>\n        <p><strong><a href=\"https://intel.github.io/neural-compressor/\">Intel Neural Compressor</a></strong>: An open-source tool for quantization (PTQ, QAT, dynamic) across PyTorch and TensorFlow. Early support for multimodal calibration and per-layer configuration.</p>\n      </li>\n      <li>\n        <p><strong><a href=\"https://github.com/haotian-liu/LLaVA\">LLaVA Quantized Forks</a></strong>: Some forks of LLaVA apply <strong>GPTQ</strong>-style <code class=\"language-plaintext highlighter-rouge\">int4</code> quantization to the language backbone. However, the vision encoder and fusion modules are often left in FP16 due to sensitivity.</p>\n      </li>\n      <li>\n        <p><strong><a href=\"https://github.com/salesforce/LAVIS\">BLIP-2 Quantized Implementations</a></strong> (via LAVIS): Some unofficial variants quantize the visual backbone and language decoder separately, relying on hybrid strategies. The Q-former is often kept in higher precision.</p>\n      </li>\n      <li>\n        <p>Emerging research tools are adapting <strong><a href=\"https://github.com/IST-DASLab/gptq\">GPTQ</a></strong> and <strong><a href=\"https://github.com/mit-han-lab/llm-awq\">AWQ</a></strong> to support VLMs, although these approaches require careful per-layer calibration and fine-tuning with multimodal datasets.</p>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p>Quantization tooling for VLMs is still developing. While text-only LLMs enjoy mature and well-integrated support in libraries like <strong>GPTQ</strong> and <strong>AWQ</strong>, multimodal quantization often requires combining custom workflows with early-stage tooling.</p>\n<p>Current options include:</p>\n<ul>\n      <li>\n        <p><strong><a href=\"https://developer.nvidia.com/tensorrt\">TensorRT</a></strong>: NVIDIA’s inference engine supports mixed-precision ViT and fusion models with custom kernels for <code class=\"language-plaintext highlighter-rouge\">int8</code> and FP16. Requires ONNX export and hardware-specific calibration.</p>\n      </li>\n      <li>\n        <p><strong><a href=\"https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html\">OpenVINO</a></strong>: Intel’s deployment toolkit for CPU/GPU/VPUs. Supports post-training quantization and VLMs exported via ONNX or Hugging Face pipelines.</p>\n      </li>\n      <li>\n        <p><strong><a href=\"https://huggingface.co/docs/optimum/overview\">Hugging Face Optimum</a></strong>: A library bridging Transformers with hardware backends. Supports quantization workflows with <strong>Intel Neural Compressor</strong> and <strong>ONNX Runtime</strong> for vision+language models.</p>\n      </li>\n      <li>\n        <p><strong><a href=\"https://intel.github.io/neural-compressor/\">Intel Neural Compressor</a></strong>: An open-source tool for quantization (PTQ, QAT, dynamic) across PyTorch and TensorFlow. Early support for multimodal calibration and per-layer configuration.</p>\n      </li>\n      <li>\n        <p><strong><a href=\"https://github.com/haotian-liu/LLaVA\">LLaVA Quantized Forks</a></strong>: Some forks of LLaVA apply <strong>GPTQ</strong>-style <code class=\"language-plaintext highlighter-rouge\">int4</code> quantization to the language backbone. However, the vision encoder and fusion modules are often left in FP16 due to sensitivity.</p>\n      </li>\n      <li>\n        <p><strong><a href=\"https://github.com/salesforce/LAVIS\">BLIP-2 Quantized Implementations</a></strong> (via LAVIS): Some unofficial variants quantize the visual backbone and language decoder separately, relying on hybrid strategies. The Q-former is often kept in higher precision.</p>\n      </li>\n      <li>\n        <p>Emerging research tools are adapting <strong><a href=\"https://github.com/IST-DASLab/gptq\">GPTQ</a></strong> and <strong><a href=\"https://github.com/mit-han-lab/llm-awq\">AWQ</a></strong> to support VLMs, although these approaches require careful per-layer calibration and fine-tuning with multimodal datasets.</p>\n      </li>\n    </ul>\n<p><strong><a href=\"https://developer.nvidia.com/tensorrt\">TensorRT</a></strong>: NVIDIA’s inference engine supports mixed-precision ViT and fusion models with custom kernels for <code class=\"language-plaintext highlighter-rouge\">int8</code> and FP16. Requires ONNX export and hardware-specific calibration.</p>\n<p><strong><a href=\"https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html\">OpenVINO</a></strong>: Intel’s deployment toolkit for CPU/GPU/VPUs. Supports post-training quantization and VLMs exported via ONNX or Hugging Face pipelines.</p>\n<p><strong><a href=\"https://huggingface.co/docs/optimum/overview\">Hugging Face Optimum</a></strong>: A library bridging Transformers with hardware backends. Supports quantization workflows with <strong>Intel Neural Compressor</strong> and <strong>ONNX Runtime</strong> for vision+language models.</p>\n<p><strong><a href=\"https://intel.github.io/neural-compressor/\">Intel Neural Compressor</a></strong>: An open-source tool for quantization (PTQ, QAT, dynamic) across PyTorch and TensorFlow. Early support for multimodal calibration and per-layer configuration.</p>\n<p><strong><a href=\"https://github.com/haotian-liu/LLaVA\">LLaVA Quantized Forks</a></strong>: Some forks of LLaVA apply <strong>GPTQ</strong>-style <code class=\"language-plaintext highlighter-rouge\">int4</code> quantization to the language backbone. However, the vision encoder and fusion modules are often left in FP16 due to sensitivity.</p>\n<p><strong><a href=\"https://github.com/salesforce/LAVIS\">BLIP-2 Quantized Implementations</a></strong> (via LAVIS): Some unofficial variants quantize the visual backbone and language decoder separately, relying on hybrid strategies. The Q-former is often kept in higher precision.</p>\n<p>Emerging research tools are adapting <strong><a href=\"https://github.com/IST-DASLab/gptq\">GPTQ</a></strong> and <strong><a href=\"https://github.com/mit-han-lab/llm-awq\">AWQ</a></strong> to support VLMs, although these approaches require careful per-layer calibration and fine-tuning with multimodal datasets.</p>\n<p>Note: As of now, there is <strong>no unified framework</strong> for full-stack VLM quantization (vision encoder, fusion, and decoder) akin to what exists for LLaMA or GPT models. Most implementations involve manually freezing or partially quantizing the model.</p>\n<h4 id=\"comparative-analysis-of-llms-vs-vlm-quantization\">Comparative Analysis of LLMs vs. VLM Quantization</h4>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Component</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Text LLM (e.g., GPT)</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>VLM (e.g., BLIP-2, LLaVA)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Architecture</td>\n<td class=\"tg-tleft-valign-first\">Uniform transformer</td>\n<td class=\"tg-tleft-valign-second\">Heterogeneous (ViT + LLM + cross-attn)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Sensitivity to quant.</td>\n<td class=\"tg-tleft-valign-first\">Moderate</td>\n<td class=\"tg-tleft-valign-second\">High (esp. fusion layers)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Common quant methods</td>\n<td class=\"tg-tleft-valign-first\">PTQ, QAT, GPTQ, AWQ</td>\n<td class=\"tg-tleft-valign-second\">Mixed-precision, QAT, PTQ (limited)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Vision encoder</td>\n<td class=\"tg-tleft-valign-first\">N/A</td>\n<td class=\"tg-tleft-valign-second\">CNN/ViT, sensitive to <code>int4</code>/<code>int8</code></td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Cross-modal fusion</td>\n<td class=\"tg-tleft-valign-first\">N/A</td>\n<td class=\"tg-tleft-valign-second\">Needs higher precision</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Evaluation metric</td>\n<td class=\"tg-tleft-valign-first\">Perplexity</td>\n<td class=\"tg-tleft-valign-second\">Task-specific (e.g., VQA accuracy)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Tooling maturity</td>\n<td class=\"tg-tleft-valign-first\">High</td>\n<td class=\"tg-tleft-valign-second\">Low to medium</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Component</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Text LLM (e.g., GPT)</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>VLM (e.g., BLIP-2, LLaVA)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Architecture</td>\n<td class=\"tg-tleft-valign-first\">Uniform transformer</td>\n<td class=\"tg-tleft-valign-second\">Heterogeneous (ViT + LLM + cross-attn)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Sensitivity to quant.</td>\n<td class=\"tg-tleft-valign-first\">Moderate</td>\n<td class=\"tg-tleft-valign-second\">High (esp. fusion layers)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Common quant methods</td>\n<td class=\"tg-tleft-valign-first\">PTQ, QAT, GPTQ, AWQ</td>\n<td class=\"tg-tleft-valign-second\">Mixed-precision, QAT, PTQ (limited)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Vision encoder</td>\n<td class=\"tg-tleft-valign-first\">N/A</td>\n<td class=\"tg-tleft-valign-second\">CNN/ViT, sensitive to <code>int4</code>/<code>int8</code></td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Cross-modal fusion</td>\n<td class=\"tg-tleft-valign-first\">N/A</td>\n<td class=\"tg-tleft-valign-second\">Needs higher precision</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Evaluation metric</td>\n<td class=\"tg-tleft-valign-first\">Perplexity</td>\n<td class=\"tg-tleft-valign-second\">Task-specific (e.g., VQA accuracy)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Tooling maturity</td>\n<td class=\"tg-tleft-valign-first\">High</td>\n<td class=\"tg-tleft-valign-second\">Low to medium</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "*   Quantizing multimodal LLMs—especially Vision-Language Models (VLMs) such as BLIP-2, LLaVA, or Flamingo—presents unique complexities not encountered in traditional text-only LLMs. These models process and fuse inputs from disparate modalities (e.g., images and text), resulting in heterogeneous model architectures and dynamic activation distributions that resist uniform quantization techniques.\n*   The inherent heterogeneity in architecture, distribution, and task metrics makes naive post-training quantization insufficient for production-grade deployment. Mixed-precision and QAT remain the most promising paths forward, especially when combined with robust calibration data and modality-aware loss functions. As VLMs become more prevalent in edge AI and on-device inference, a new generation of quantization-aware toolchains will be essential to unlock their full potential.\n\n#### Why VLM Quantization is More Complex\n\n*   Multimodal models are composed of at least two distinct processing pipelines—one for each modality (e.g., image and text)—and often a third for cross-modal alignment or fusion. This architectural heterogeneity introduces the following challenges:\n    \n    *   **Diverse tensor statistics**: Vision and language inputs yield activations with vastly different distributions and dynamic ranges, making uniform quantization impractical across modalities.\n    *   **Cross-modal attention sensitivity**: Cross-attention layers that fuse modalities are especially fragile to precision loss, as they are responsible for preserving semantic alignment between vision and language.\n    *   **Embedding alignment**: Vision embeddings (e.g., image patches from ViTs) and text embeddings must remain aligned for effective fusion. Quantization artifacts can easily disrupt this shared embedding space.\n    *   **Lack of inductive biases**: Unlike CNNs, which offer natural robustness to quantization via spatial weight sharing and locality, ViTs and transformers often rely more heavily on learned long-range dependencies, which are easily degraded by quantization noise.\n    *   **Multi-objective optimization**: A VLM may be used across many tasks (e.g., captioning, VQA, grounding), requiring quantized models to generalize well across domains, not just on language metrics like perplexity.\n*   To address these challenges, quantization of multimodal models typically involves hybrid and adaptive strategies as described below.\n    \n\nMultimodal models are composed of at least two distinct processing pipelines—one for each modality (e.g., image and text)—and often a third for cross-modal alignment or fusion. This architectural heterogeneity introduces the following challenges:\n\n*   **Diverse tensor statistics**: Vision and language inputs yield activations with vastly different distributions and dynamic ranges, making uniform quantization impractical across modalities.\n*   **Cross-modal attention sensitivity**: Cross-attention layers that fuse modalities are especially fragile to precision loss, as they are responsible for preserving semantic alignment between vision and language.\n*   **Embedding alignment**: Vision embeddings (e.g., image patches from ViTs) and text embeddings must remain aligned for effective fusion. Quantization artifacts can easily disrupt this shared embedding space.\n*   **Lack of inductive biases**: Unlike CNNs, which offer natural robustness to quantization via spatial weight sharing and locality, ViTs and transformers often rely more heavily on learned long-range dependencies, which are easily degraded by quantization noise.\n*   **Multi-objective optimization**: A VLM may be used across many tasks (e.g., captioning, VQA, grounding), requiring quantized models to generalize well across domains, not just on language metrics like perplexity.\n\nTo address these challenges, quantization of multimodal models typically involves hybrid and adaptive strategies as described below.\n\n#### Quantizing the Visual Backbone\n\n1.  **CNN-based encoders (e.g., ResNet, EfficientNet)**:\n    \n    *   CNNs are relatively robust to quantization, and standard per-channel `int8` quantization (as used in MobileNet) can often be applied.\n    *   Pre-trained encoders may be frozen and quantized independently of the rest of the model.\n2.  **Vision Transformers (ViTs)**:\n    \n    *   More sensitive to quantization due to their reliance on attention mechanisms and lack of inductive biases.\n    *   Key operations such as softmax and positional embeddings are particularly fragile.\n    *   Attention maps are harder to compress as they carry spatial relevance crucial for image understanding.\n    \n    **Best practices:**\n    \n    *   Use per-head or per-channel quantization for attention weights.\n    *   Apply post-training quantization (PTQ) carefully, or use quantization-aware training (QAT) for attention-heavy layers.\n    *   Maintain FP16 or BF16 precision in early layers or attention blocks if task-critical.\n\n**CNN-based encoders (e.g., ResNet, EfficientNet)**:\n\n*   CNNs are relatively robust to quantization, and standard per-channel `int8` quantization (as used in MobileNet) can often be applied.\n*   Pre-trained encoders may be frozen and quantized independently of the rest of the model.\n\n**Vision Transformers (ViTs)**:\n\n*   More sensitive to quantization due to their reliance on attention mechanisms and lack of inductive biases.\n*   Key operations such as softmax and positional embeddings are particularly fragile.\n*   Attention maps are harder to compress as they carry spatial relevance crucial for image understanding.\n\n**Best practices:**\n\n*   Use per-head or per-channel quantization for attention weights.\n*   Apply post-training quantization (PTQ) carefully, or use quantization-aware training (QAT) for attention-heavy layers.\n*   Maintain FP16 or BF16 precision in early layers or attention blocks if task-critical.\n\n#### Quantizing the Language Backbone\n\n*   Language processing in VLMs is typically transformer-based (e.g., LLaMA, T5, GPT-style decoders). The quantization techniques here are more mature and generally follow:\n    \n    *   **`int8` or `int4` quantization** using post-training methods (e.g., GPTQ, AWQ, SmoothQuant).\n    *   **Per-group or per-channel quantization** for MLPs and attention blocks.\n    *   **Mixed-precision inference**, especially keeping attention output or layer norms in FP16 when accuracy is crucial.\n\nLanguage processing in VLMs is typically transformer-based (e.g., LLaMA, T5, GPT-style decoders). The quantization techniques here are more mature and generally follow:\n\n*   **`int8` or `int4` quantization** using post-training methods (e.g., GPTQ, AWQ, SmoothQuant).\n*   **Per-group or per-channel quantization** for MLPs and attention blocks.\n*   **Mixed-precision inference**, especially keeping attention output or layer norms in FP16 when accuracy is crucial.\n\n#### Cross-Modal Projection and Fusion Layer Quantization\n\n*   This is the most critical and fragile component in VLMs. The fusion modules align visual and textual embeddings into a shared latent space.\n    \n*   **Cross-attention layers** are highly sensitive to quantization because they match image regions with textual tokens. Errors here degrade the entire model’s reasoning ability.\n*   **Query Transformers (Q-formers)**, as used in BLIP-2, process image features into language-style prompts. Quantization here must preserve alignment fidelity.\n    \n*   **Strategies:**\n    \n    *   Retain cross-modal fusion (specifically, projection and/or cross-attention layers) in FP16 or use higher-precision `int8`.\n    *   Apply QAT to cross-modal components to preserve alignment under quantization-induced rounding and clipping.\n    *   Use per-tensor calibration based on multimodal datasets to balance activations across modalities.\n\nThis is the most critical and fragile component in VLMs. The fusion modules align visual and textual embeddings into a shared latent space.\n\n**Query Transformers (Q-formers)**, as used in BLIP-2, process image features into language-style prompts. Quantization here must preserve alignment fidelity.\n\n**Strategies:**\n\n*   Retain cross-modal fusion (specifically, projection and/or cross-attention layers) in FP16 or use higher-precision `int8`.\n*   Apply QAT to cross-modal components to preserve alignment under quantization-induced rounding and clipping.\n*   Use per-tensor calibration based on multimodal datasets to balance activations across modalities.\n\n#### Quantization-Aware Training (QAT) in VLMs\n\n*   Due to sensitivity in fusion and vision branches, QAT is often _required_ for VLMs, unlike in pure-text LLMs where PTQ often suffices.\n    \n*   During QAT, fake quantization layers simulate precision loss during both forward and backward passes.\n*   Loss functions may include:\n    \n    *   **Cross-modal alignment loss** (cosine similarity of vision/text embeddings)\n    *   **Task-specific loss** (e.g., VQA classification loss)\n    *   **KL divergence or logit-matching** between full-precision and quantized models\n*   Progressive QAT approaches are sometimes used:\n    \n    *   Freeze the vision encoder\n    *   Apply quantization noise gradually to fusion layers\n    *   Fine-tune using diverse tasks to preserve robustness\n\nDue to sensitivity in fusion and vision branches, QAT is often _required_ for VLMs, unlike in pure-text LLMs where PTQ often suffices.\n\nLoss functions may include:\n\n*   **Cross-modal alignment loss** (cosine similarity of vision/text embeddings)\n*   **Task-specific loss** (e.g., VQA classification loss)\n*   **KL divergence or logit-matching** between full-precision and quantized models\n\nProgressive QAT approaches are sometimes used:\n\n*   Freeze the vision encoder\n*   Apply quantization noise gradually to fusion layers\n*   Fine-tune using diverse tasks to preserve robustness\n\n#### Calibration and Evaluation in VLMs\n\n*   Calibration datasets for VLMs must reflect the multimodal distribution. Suitable datasets include:\n    \n    *   MS-COCO (captioning)\n    *   VQAv2 (VQA)\n    *   GQA, VizWiz, RefCOCO (referring expression comprehension)\n*   Metrics to evaluate quantized VLMs differ from those used in text-only LLMs:\n    \n    *   **Captioning:** CIDEr, BLEU, METEOR\n    *   **VQA:** answer accuracy\n    *   **Localization:** IoU and precision\n*   These tasks are affected differently by quantization; degradation in fusion modules typically leads to sharper accuracy drops than in text-only settings.\n    \n\nCalibration datasets for VLMs must reflect the multimodal distribution. Suitable datasets include:\n\n*   MS-COCO (captioning)\n*   VQAv2 (VQA)\n*   GQA, VizWiz, RefCOCO (referring expression comprehension)\n\nMetrics to evaluate quantized VLMs differ from those used in text-only LLMs:\n\n*   **Captioning:** CIDEr, BLEU, METEOR\n*   **VQA:** answer accuracy\n*   **Localization:** IoU and precision\n\nThese tasks are affected differently by quantization; degradation in fusion modules typically leads to sharper accuracy drops than in text-only settings.\n\n#### Hybrid and Mixed-Precision Quantization\n\n*   Given the disparity in sensitivity across components:\n    *   Use **`int8` or `int4`** for robust modules (e.g., MLPs, FFNs)\n    *   Use **FP16 or BF16** for:\n        *   Cross-modal projections\n        *   Final projection\n        *   Optionally, try attention output and embedding normalization layers\n*   **Mixed-precision kernels** in deployment frameworks (e.g., TensorRT, OpenVINO) allow selective high-precision execution.\n\n*   Use **`int8` or `int4`** for robust modules (e.g., MLPs, FFNs)\n*   Use **FP16 or BF16** for:\n    *   Cross-modal projections\n    *   Final projection\n    *   Optionally, try attention output and embedding normalization layers\n\n*   Cross-modal projections\n*   Final projection\n*   Optionally, try attention output and embedding normalization layers\n\n#### Tooling Support\n\n*   Quantization tooling for VLMs is still developing. While text-only LLMs enjoy mature and well-integrated support in libraries like **GPTQ** and **AWQ**, multimodal quantization often requires combining custom workflows with early-stage tooling.\n    \n*   Current options include:\n    \n    *   **[TensorRT](https://developer.nvidia.com/tensorrt)**: NVIDIA’s inference engine supports mixed-precision ViT and fusion models with custom kernels for `int8` and FP16. Requires ONNX export and hardware-specific calibration.\n        \n    *   **[OpenVINO](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html)**: Intel’s deployment toolkit for CPU/GPU/VPUs. Supports post-training quantization and VLMs exported via ONNX or Hugging Face pipelines.\n        \n    *   **[Hugging Face Optimum](https://huggingface.co/docs/optimum/overview)**: A library bridging Transformers with hardware backends. Supports quantization workflows with **Intel Neural Compressor** and **ONNX Runtime** for vision+language models.\n        \n    *   **[Intel Neural Compressor](https://intel.github.io/neural-compressor/)**: An open-source tool for quantization (PTQ, QAT, dynamic) across PyTorch and TensorFlow. Early support for multimodal calibration and per-layer configuration.\n        \n    *   **[LLaVA Quantized Forks](https://github.com/haotian-liu/LLaVA)**: Some forks of LLaVA apply **GPTQ**\\-style `int4` quantization to the language backbone. However, the vision encoder and fusion modules are often left in FP16 due to sensitivity.\n        \n    *   **[BLIP-2 Quantized Implementations](https://github.com/salesforce/LAVIS)** (via LAVIS): Some unofficial variants quantize the visual backbone and language decoder separately, relying on hybrid strategies. The Q-former is often kept in higher precision.\n        \n    *   Emerging research tools are adapting **[GPTQ](https://github.com/IST-DASLab/gptq)** and **[AWQ](https://github.com/mit-han-lab/llm-awq)** to support VLMs, although these approaches require careful per-layer calibration and fine-tuning with multimodal datasets.\n        \n\nQuantization tooling for VLMs is still developing. While text-only LLMs enjoy mature and well-integrated support in libraries like **GPTQ** and **AWQ**, multimodal quantization often requires combining custom workflows with early-stage tooling.\n\nCurrent options include:\n\n*   **[TensorRT](https://developer.nvidia.com/tensorrt)**: NVIDIA’s inference engine supports mixed-precision ViT and fusion models with custom kernels for `int8` and FP16. Requires ONNX export and hardware-specific calibration.\n    \n*   **[OpenVINO](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html)**: Intel’s deployment toolkit for CPU/GPU/VPUs. Supports post-training quantization and VLMs exported via ONNX or Hugging Face pipelines.\n    \n*   **[Hugging Face Optimum](https://huggingface.co/docs/optimum/overview)**: A library bridging Transformers with hardware backends. Supports quantization workflows with **Intel Neural Compressor** and **ONNX Runtime** for vision+language models.\n    \n*   **[Intel Neural Compressor](https://intel.github.io/neural-compressor/)**: An open-source tool for quantization (PTQ, QAT, dynamic) across PyTorch and TensorFlow. Early support for multimodal calibration and per-layer configuration.\n    \n*   **[LLaVA Quantized Forks](https://github.com/haotian-liu/LLaVA)**: Some forks of LLaVA apply **GPTQ**\\-style `int4` quantization to the language backbone. However, the vision encoder and fusion modules are often left in FP16 due to sensitivity.\n    \n*   **[BLIP-2 Quantized Implementations](https://github.com/salesforce/LAVIS)** (via LAVIS): Some unofficial variants quantize the visual backbone and language decoder separately, relying on hybrid strategies. The Q-former is often kept in higher precision.\n    \n*   Emerging research tools are adapting **[GPTQ](https://github.com/IST-DASLab/gptq)** and **[AWQ](https://github.com/mit-han-lab/llm-awq)** to support VLMs, although these approaches require careful per-layer calibration and fine-tuning with multimodal datasets.\n    \n\n**[TensorRT](https://developer.nvidia.com/tensorrt)**: NVIDIA’s inference engine supports mixed-precision ViT and fusion models with custom kernels for `int8` and FP16. Requires ONNX export and hardware-specific calibration.\n\n**[OpenVINO](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html)**: Intel’s deployment toolkit for CPU/GPU/VPUs. Supports post-training quantization and VLMs exported via ONNX or Hugging Face pipelines.\n\n**[Hugging Face Optimum](https://huggingface.co/docs/optimum/overview)**: A library bridging Transformers with hardware backends. Supports quantization workflows with **Intel Neural Compressor** and **ONNX Runtime** for vision+language models.\n\n**[Intel Neural Compressor](https://intel.github.io/neural-compressor/)**: An open-source tool for quantization (PTQ, QAT, dynamic) across PyTorch and TensorFlow. Early support for multimodal calibration and per-layer configuration.\n\n**[LLaVA Quantized Forks](https://github.com/haotian-liu/LLaVA)**: Some forks of LLaVA apply **GPTQ**\\-style `int4` quantization to the language backbone. However, the vision encoder and fusion modules are often left in FP16 due to sensitivity.\n\n**[BLIP-2 Quantized Implementations](https://github.com/salesforce/LAVIS)** (via LAVIS): Some unofficial variants quantize the visual backbone and language decoder separately, relying on hybrid strategies. The Q-former is often kept in higher precision.\n\nEmerging research tools are adapting **[GPTQ](https://github.com/IST-DASLab/gptq)** and **[AWQ](https://github.com/mit-han-lab/llm-awq)** to support VLMs, although these approaches require careful per-layer calibration and fine-tuning with multimodal datasets.\n\nNote: As of now, there is **no unified framework** for full-stack VLM quantization (vision encoder, fusion, and decoder) akin to what exists for LLaMA or GPT models. Most implementations involve manually freezing or partially quantizing the model.\n\n#### Comparative Analysis of LLMs vs. VLM Quantization\n\n**Component**\n\n**Text LLM (e.g., GPT)**\n\n**VLM (e.g., BLIP-2, LLaVA)**\n\nArchitecture\n\nUniform transformer\n\nHeterogeneous (ViT + LLM + cross-attn)\n\nSensitivity to quant.\n\nModerate\n\nHigh (esp. fusion layers)\n\nCommon quant methods\n\nPTQ, QAT, GPTQ, AWQ\n\nMixed-precision, QAT, PTQ (limited)\n\nVision encoder\n\nN/A\n\nCNN/ViT, sensitive to `int4`/`int8`\n\nCross-modal fusion\n\nN/A\n\nNeeds higher precision\n\nEvaluation metric\n\nPerplexity\n\nTask-specific (e.g., VQA accuracy)\n\nTooling maturity\n\nHigh\n\nLow to medium\n\n**Component**\n\n**Text LLM (e.g., GPT)**\n\n**VLM (e.g., BLIP-2, LLaVA)**\n\nArchitecture\n\nUniform transformer\n\nHeterogeneous (ViT + LLM + cross-attn)\n\nSensitivity to quant.\n\nModerate\n\nHigh (esp. fusion layers)\n\nCommon quant methods\n\nPTQ, QAT, GPTQ, AWQ\n\nMixed-precision, QAT, PTQ (limited)\n\nVision encoder\n\nN/A\n\nCNN/ViT, sensitive to `int4`/`int8`\n\nCross-modal fusion\n\nN/A\n\nNeeds higher precision\n\nEvaluation metric\n\nPerplexity\n\nTask-specific (e.g., VQA accuracy)\n\nTooling maturity\n\nHigh\n\nLow to medium",
      "order": 14,
      "orderInChapter": 14,
      "difficulty": 4,
      "estimatedMinutes": 12,
      "tags": [
        "ondevice ai",
        "transformer",
        "attention",
        "embedding",
        "cnn",
        "gpt",
        "llm",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 2303,
        "contentLength": 26191
      },
      "nextCards": [
        "ai-model-compression-device-and-operator-support-across-frameworks-15",
        "ai-model-compression-choosing-the-right-quantization-approach-16"
      ],
      "relatedCards": [
        "ai-on-device-transformers-embedding-size-times-vocabulary-size-times-depth-26",
        "ai-on-device-transformers-parameter-tuning-recipes-for-ml-runtimes-27",
        "ai-on-device-transformers-tokenizer-and-vocabulary-size-22",
        "ai-on-device-transformers-parameter-count-and-model-depth-25",
        "ai-on-device-transformers-modelembedding-dimension-23"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#multimodal-quantization",
      "scrapedAt": "2025-12-28T11:55:50.971Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-device-and-operator-support-across-frameworks-15",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Device and Operator Support Across Frameworks",
      "subtitle": "Quantization",
      "contentHtml": "<h4 id=\"pytorch\">PyTorch</h4>\n<ul>\n  <li>\n    <p>Quantization in PyTorch is supported for a limited subset of operators, and the availability of these operators depends on the specific quantization approach being employed—dynamic, PTQ, or QAT. The list of supported quantized operators is not exhaustive and evolves with newer PyTorch releases. For an up-to-date reference, consult the official PyTorch quantization documentation <a href=\"https://pytorch.org/docs/stable/quantization.html\">here</a>.</p>\n  </li>\n  <li>\n    <p>The implementation of quantization in PyTorch is backend-dependent, meaning that both the quantization configuration (which defines how tensors are quantized) and the set of quantized kernels (which define how arithmetic is performed on quantized tensors) vary based on the target hardware. Currently, PyTorch provides official support for quantized inference only on CPUs, specifically for x86 and ARM architectures. These are supported via two primary backends:</p>\n\n    <ul>\n      <li><strong><code class=\"language-plaintext highlighter-rouge\">fbgemm</code></strong>: Optimized for server-class x86 CPUs.</li>\n      <li><strong><code class=\"language-plaintext highlighter-rouge\">qnnpack</code></strong>: Designed for mobile ARM CPUs.</li>\n    </ul>\n  </li>\n  <li>\n    <p>The backend must be explicitly set to ensure compatibility between the model’s quantized representation and the runtime kernels, as shown in the example below:</p>\n\n    <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code11\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code11\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n<span class=\"n\">backend</span> <span class=\"o\">=</span> <span class=\"s\">'fbgemm'</span>  <span class=\"c1\"># 'qnnpack' for ARM/mobile inference\n</span><span class=\"n\">my_model</span><span class=\"p\">.</span><span class=\"n\">qconfig</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">get_default_qconfig</span><span class=\"p\">(</span><span class=\"n\">backend</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Prepare and convert model\n# Set the backend on which the quantized kernels need to be run\n</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">backends</span><span class=\"p\">.</span><span class=\"n\">quantized</span><span class=\"p\">.</span><span class=\"n\">engine</span> <span class=\"o\">=</span> <span class=\"n\">backend</span>\n\n<span class=\"c1\"># Continue with model preparation and conversion steps...\n</span></code></pre></div>    </div>\n  </li>\n  <li>\n    <p>QAT in PyTorch is performed in full-precision (<code class=\"language-plaintext highlighter-rouge\">float32</code>) mode to leverage existing GPU or CPU hardware during training. This technique simulates the effects of quantization during training to improve model robustness when deployed with quantized weights. QAT is particularly beneficial for convolutional neural networks (CNNs), especially lightweight models such as MobileNet, where static or dynamic post-training quantization may result in unacceptable accuracy degradation.</p>\n  </li>\n</ul>\n<p>Quantization in PyTorch is supported for a limited subset of operators, and the availability of these operators depends on the specific quantization approach being employed—dynamic, PTQ, or QAT. The list of supported quantized operators is not exhaustive and evolves with newer PyTorch releases. For an up-to-date reference, consult the official PyTorch quantization documentation <a href=\"https://pytorch.org/docs/stable/quantization.html\">here</a>.</p>\n<p>The implementation of quantization in PyTorch is backend-dependent, meaning that both the quantization configuration (which defines how tensors are quantized) and the set of quantized kernels (which define how arithmetic is performed on quantized tensors) vary based on the target hardware. Currently, PyTorch provides official support for quantized inference only on CPUs, specifically for x86 and ARM architectures. These are supported via two primary backends:</p>\n<ul>\n      <li><strong><code class=\"language-plaintext highlighter-rouge\">fbgemm</code></strong>: Optimized for server-class x86 CPUs.</li>\n      <li><strong><code class=\"language-plaintext highlighter-rouge\">qnnpack</code></strong>: Designed for mobile ARM CPUs.</li>\n    </ul>\n<p>The backend must be explicitly set to ensure compatibility between the model’s quantized representation and the runtime kernels, as shown in the example below:</p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code11\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code11\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n<span class=\"n\">backend</span> <span class=\"o\">=</span> <span class=\"s\">'fbgemm'</span>  <span class=\"c1\"># 'qnnpack' for ARM/mobile inference\n</span><span class=\"n\">my_model</span><span class=\"p\">.</span><span class=\"n\">qconfig</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">quantization</span><span class=\"p\">.</span><span class=\"n\">get_default_qconfig</span><span class=\"p\">(</span><span class=\"n\">backend</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Prepare and convert model\n# Set the backend on which the quantized kernels need to be run\n</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">backends</span><span class=\"p\">.</span><span class=\"n\">quantized</span><span class=\"p\">.</span><span class=\"n\">engine</span> <span class=\"o\">=</span> <span class=\"n\">backend</span>\n\n<span class=\"c1\"># Continue with model preparation and conversion steps...\n</span></code></pre>\n<p>QAT in PyTorch is performed in full-precision (<code class=\"language-plaintext highlighter-rouge\">float32</code>) mode to leverage existing GPU or CPU hardware during training. This technique simulates the effects of quantization during training to improve model robustness when deployed with quantized weights. QAT is particularly beneficial for convolutional neural networks (CNNs), especially lightweight models such as MobileNet, where static or dynamic post-training quantization may result in unacceptable accuracy degradation.</p>\n<h5 id=\"integration-in-torchvision\">Integration in <code class=\"language-plaintext Highlighter-rouge\">torchvision</code></h5>\n<ul>\n  <li>\n    <p>The <code class=\"language-plaintext highlighter-rouge\">torchvision</code> library includes integrated support for quantization in several widely used neural network architectures. These include GoogLeNet, InceptionV3, ResNet (various depths), ResNeXt, MobileNet (V2 and V3), and ShuffleNet. The support is provided in three distinct forms to enable a range of workflows:</p>\n\n    <ol>\n      <li><strong>Pre-trained quantized model weights</strong>: These models are fully quantized and can be used directly for inference without additional fine-tuning.</li>\n      <li><strong>Quantization-ready model definitions</strong>: These are versions of the models with quantization stubs pre-inserted, making them suitable for post-training quantization or QAT.</li>\n      <li><strong>QAT scripts</strong>: Scripts are available to perform QAT on supported models. While these scripts are applicable to all the models listed above, empirical evaluations show that QAT tends to yield significant accuracy benefits primarily for lightweight models like MobileNet.</li>\n    </ol>\n  </li>\n  <li>\n    <p><a href=\"https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/quantized_transfer_learning_tutorial.ipynb\">Here’s</a> a dedicated tutorial demonstrating how to perform transfer learning with quantization using pre-trained models from <code class=\"language-plaintext highlighter-rouge\">torchvision</code>. This enables developers to take advantage of quantized inference while still adapting models to custom datasets and deployment scenarios.</p>\n  </li>\n</ul>\n<p>The <code class=\"language-plaintext highlighter-rouge\">torchvision</code> library includes integrated support for quantization in several widely used neural network architectures. These include GoogLeNet, InceptionV3, ResNet (various depths), ResNeXt, MobileNet (V2 and V3), and ShuffleNet. The support is provided in three distinct forms to enable a range of workflows:</p>\n<ol>\n      <li><strong>Pre-trained quantized model weights</strong>: These models are fully quantized and can be used directly for inference without additional fine-tuning.</li>\n      <li><strong>Quantization-ready model definitions</strong>: These are versions of the models with quantization stubs pre-inserted, making them suitable for post-training quantization or QAT.</li>\n      <li><strong>QAT scripts</strong>: Scripts are available to perform QAT on supported models. While these scripts are applicable to all the models listed above, empirical evaluations show that QAT tends to yield significant accuracy benefits primarily for lightweight models like MobileNet.</li>\n    </ol>\n<p><a href=\"https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/quantized_transfer_learning_tutorial.ipynb\">Here’s</a> a dedicated tutorial demonstrating how to perform transfer learning with quantization using pre-trained models from <code class=\"language-plaintext highlighter-rouge\">torchvision</code>. This enables developers to take advantage of quantized inference while still adapting models to custom datasets and deployment scenarios.</p>\n<h5 id=\"resources\">Resources</h5>\n<ul>\n  <li>To get started on quantizing your models in PyTorch, start with the tutorials on the <a href=\"https://pytorch.org/tutorials/#model-optimization\">PyTorch website</a>.</li>\n  <li>If you are working with sequence data, start with…\n    <ul>\n      <li><a href=\"https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html\">Dynamic quantization for LSTM</a>, or</li>\n      <li><a href=\"https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html\">Dynamic quantization for BERT</a></li>\n    </ul>\n  </li>\n  <li>If you are working with image data then we recommend starting with the <a href=\"https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/quantized_transfer_learning_tutorial.ipynb\">transfer learning with quantization tutorial</a>. Then you can explore <a href=\"https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html\">static post training quantization</a>.\n    <ul>\n      <li>If you find that the accuracy drop with post training quantization is too high, then try <a href=\"https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html\">quantization aware training</a>.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><a href=\"https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html\">Dynamic quantization for LSTM</a>, or</li>\n      <li><a href=\"https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html\">Dynamic quantization for BERT</a></li>\n    </ul>\n<ul>\n      <li>If you find that the accuracy drop with post training quantization is too high, then try <a href=\"https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html\">quantization aware training</a>.</li>\n    </ul>\n<h4 id=\"tensorflow\">TensorFlow</h4>\n<ul>\n  <li>\n    <p>Quantization support in TensorFlow is primarily centered around deployment through TensorFlow Lite (TFLite). Only a subset of TensorFlow operations are supported in quantized form when converting models to run efficiently on edge devices. For a comprehensive list of quantization-compatible operators, refer to the official TFLite operator compatibility documentation <a href=\"https://ai.google.dev/edge/litert/models/ops_compatibility\">here</a>.</p>\n  </li>\n  <li>\n    <p>Quantized inference in TensorFlow is enabled through TensorFlow Lite (TFLite) delegates, which provide optimized execution across various hardware backends. These include:</p>\n\n    <ul>\n      <li><strong>CPU Delegate</strong>: Supports <code class=\"language-plaintext highlighter-rouge\">int8</code> and <code class=\"language-plaintext highlighter-rouge\">float16</code> quantized models using XNNPack kernels, which are enabled by default in modern TFLite runtimes.</li>\n      <li><strong>GPU Delegate</strong>: Accelerates inference on mobile and embedded GPUs. It supports <code class=\"language-plaintext highlighter-rouge\">float16</code> quantization and, in limited cases, <code class=\"language-plaintext highlighter-rouge\">int8</code> precision. The delegate is available on both Android and iOS platforms.</li>\n      <li><strong>NNAPI Delegate</strong> (Android only): Interfaces with on-device hardware acceleration drivers. Quantized <code class=\"language-plaintext highlighter-rouge\">int8</code> models are typically supported and can see performance improvements depending on the device and vendor-specific drivers.</li>\n      <li>\n        <p><strong>Edge TPU Delegate</strong>: Targets Google’s Coral hardware and supports only fully integer quantized models with <code class=\"language-plaintext highlighter-rouge\">int8</code> weights and activations. Due to strict operator and quantization constraints, models must be carefully converted and then compiled using the Edge TPU Compiler.</p>\n      </li>\n      <li>The level of operator support and performance characteristics differ by delegate. For example, the Edge TPU requires that all operations be quantized and supported by its limited op set. Any unsupported operations will result in compilation failure or will require fallback to CPU, which can significantly affect performance. As such, developers must validate operator compatibility prior to deployment by reviewing the <a href=\"https://ai.google.dev/edge/litert/models/ops_compatibility\">TFLite ops compatibility guide</a> and testing with their target delegate.</li>\n    </ul>\n  </li>\n  <li>\n    <p>QAT in TensorFlow is implemented using the <code class=\"language-plaintext highlighter-rouge\">tfmot.quantization.keras.quantize_model</code> API available through the TensorFlow Model Optimization Toolkit (TFMOT). Similar to PyTorch, QAT in TensorFlow is performed in floating point, allowing the model to simulate quantized behavior during training while still leveraging GPU acceleration. This helps preserve accuracy for models that do not respond well to post-training quantization, such as compact architectures like MobileNet or custom CNNs. The general trade-offs between PTQ and QAT in TensorFlow align closely with those in PyTorch, although some feature and operator support mismatches still exist between the two frameworks.</p>\n  </li>\n  <li>\n    <p>When using post-training quantization or QAT, it’s important to validate that all critical model operations are supported in TFLite with quantized equivalents. Unsupported operations may be automatically left in float, potentially degrading the intended performance benefits of quantization.</p>\n  </li>\n</ul>\n<p>Quantization support in TensorFlow is primarily centered around deployment through TensorFlow Lite (TFLite). Only a subset of TensorFlow operations are supported in quantized form when converting models to run efficiently on edge devices. For a comprehensive list of quantization-compatible operators, refer to the official TFLite operator compatibility documentation <a href=\"https://ai.google.dev/edge/litert/models/ops_compatibility\">here</a>.</p>\n<p>Quantized inference in TensorFlow is enabled through TensorFlow Lite (TFLite) delegates, which provide optimized execution across various hardware backends. These include:</p>\n<ul>\n      <li><strong>CPU Delegate</strong>: Supports <code class=\"language-plaintext highlighter-rouge\">int8</code> and <code class=\"language-plaintext highlighter-rouge\">float16</code> quantized models using XNNPack kernels, which are enabled by default in modern TFLite runtimes.</li>\n      <li><strong>GPU Delegate</strong>: Accelerates inference on mobile and embedded GPUs. It supports <code class=\"language-plaintext highlighter-rouge\">float16</code> quantization and, in limited cases, <code class=\"language-plaintext highlighter-rouge\">int8</code> precision. The delegate is available on both Android and iOS platforms.</li>\n      <li><strong>NNAPI Delegate</strong> (Android only): Interfaces with on-device hardware acceleration drivers. Quantized <code class=\"language-plaintext highlighter-rouge\">int8</code> models are typically supported and can see performance improvements depending on the device and vendor-specific drivers.</li>\n      <li>\n        <p><strong>Edge TPU Delegate</strong>: Targets Google’s Coral hardware and supports only fully integer quantized models with <code class=\"language-plaintext highlighter-rouge\">int8</code> weights and activations. Due to strict operator and quantization constraints, models must be carefully converted and then compiled using the Edge TPU Compiler.</p>\n      </li>\n      <li>The level of operator support and performance characteristics differ by delegate. For example, the Edge TPU requires that all operations be quantized and supported by its limited op set. Any unsupported operations will result in compilation failure or will require fallback to CPU, which can significantly affect performance. As such, developers must validate operator compatibility prior to deployment by reviewing the <a href=\"https://ai.google.dev/edge/litert/models/ops_compatibility\">TFLite ops compatibility guide</a> and testing with their target delegate.</li>\n    </ul>\n<p><strong>Edge TPU Delegate</strong>: Targets Google’s Coral hardware and supports only fully integer quantized models with <code class=\"language-plaintext highlighter-rouge\">int8</code> weights and activations. Due to strict operator and quantization constraints, models must be carefully converted and then compiled using the Edge TPU Compiler.</p>\n<p>QAT in TensorFlow is implemented using the <code class=\"language-plaintext highlighter-rouge\">tfmot.quantization.keras.quantize_model</code> API available through the TensorFlow Model Optimization Toolkit (TFMOT). Similar to PyTorch, QAT in TensorFlow is performed in floating point, allowing the model to simulate quantized behavior during training while still leveraging GPU acceleration. This helps preserve accuracy for models that do not respond well to post-training quantization, such as compact architectures like MobileNet or custom CNNs. The general trade-offs between PTQ and QAT in TensorFlow align closely with those in PyTorch, although some feature and operator support mismatches still exist between the two frameworks.</p>\n<p>When using post-training quantization or QAT, it’s important to validate that all critical model operations are supported in TFLite with quantized equivalents. Unsupported operations may be automatically left in float, potentially degrading the intended performance benefits of quantization.</p>\n<h5 id=\"integration-in-tfkerasapplications\">Integration in <code class=\"language-plaintext Highlighter-rouge\">tf.keras.applications</code></h5>\n<ul>\n  <li>\n    <p>While TensorFlow does not provide pre-quantized models in <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications\"><code class=\"language-plaintext highlighter-rouge\">tf.keras.applications</code></a>, the Model Optimization Toolkit provides utilities to quantize these models post-training or prepare them for QAT. Developers can load a model from <code class=\"language-plaintext highlighter-rouge\">tf.keras.applications</code>, apply quantization via TFMOT, and then convert it to TFLite. The process typically involves:</p>\n\n    <ol>\n      <li>Cloning the model with quantization-aware layers using <code class=\"language-plaintext highlighter-rouge\">quantize_model</code>.</li>\n      <li>Fine-tuning the quantized model if needed.</li>\n      <li>Converting the trained model to TFLite using the TFLiteConverter.</li>\n    </ol>\n  </li>\n  <li>\n    <p>TFLite provides tools and guidelines for performing transfer learning with quantized models, though, as with PyTorch, QAT tends to be necessary mainly for accuracy-sensitive lightweight models.</p>\n  </li>\n</ul>\n<p>While TensorFlow does not provide pre-quantized models in <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications\"><code class=\"language-plaintext highlighter-rouge\">tf.keras.applications</code></a>, the Model Optimization Toolkit provides utilities to quantize these models post-training or prepare them for QAT. Developers can load a model from <code class=\"language-plaintext highlighter-rouge\">tf.keras.applications</code>, apply quantization via TFMOT, and then convert it to TFLite. The process typically involves:</p>\n<ol>\n      <li>Cloning the model with quantization-aware layers using <code class=\"language-plaintext highlighter-rouge\">quantize_model</code>.</li>\n      <li>Fine-tuning the quantized model if needed.</li>\n      <li>Converting the trained model to TFLite using the TFLiteConverter.</li>\n    </ol>\n<p>TFLite provides tools and guidelines for performing transfer learning with quantized models, though, as with PyTorch, QAT tends to be necessary mainly for accuracy-sensitive lightweight models.</p>\n<h5 id=\"resources-1\">Resources</h5>\n<ul>\n  <li>\n    <p>TensorFlow’s PTQ techniques are detailed in the <a href=\"https://www.tensorflow.org/model_optimization/guide/quantization/post_training\">post-training quantization guide</a>.</p>\n  </li>\n  <li>\n    <p>QAT is covered in the <a href=\"https://www.tensorflow.org/model_optimization/guide/quantization/training\">QAT guide</a>.</p>\n  </li>\n</ul>\n<p>TensorFlow’s PTQ techniques are detailed in the <a href=\"https://www.tensorflow.org/model_optimization/guide/quantization/post_training\">post-training quantization guide</a>.</p>\n<p>QAT is covered in the <a href=\"https://www.tensorflow.org/model_optimization/guide/quantization/training\">QAT guide</a>.</p>\n<h4 id=\"coreml\">CoreML</h4>\n<ul>\n  <li>\n    <p>Quantization support in CoreML is integrated directly into the CoreML Tools conversion pipeline. Quantization can be applied during model conversion from popular frameworks (such as PyTorch or TensorFlow) to the <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code> format using the <a href=\"https://coremltools.readme.io/\">coremltools</a> Python API. The supported quantization schemes are primarily weight-only quantization, with formats including:</p>\n\n    <ul>\n      <li>\n        <p><strong><code class=\"language-plaintext highlighter-rouge\">float16</code></strong>: Reduces the precision of weights from 32-bit floating point to 16-bit floating point. This is the most common and widely supported quantization type for CoreML, offering significant reductions in model size with minimal accuracy loss. In many cases, Apple hardware (e.g., A-series and M-series chips) executes GPU computations natively in <code class=\"language-plaintext highlighter-rouge\">float16</code>, so <code class=\"language-plaintext highlighter-rouge\">float16</code> quantization primarily benefits memory footprint and model loading speed rather than raw compute throughput.</p>\n      </li>\n      <li>\n        <p><strong>Linear <code class=\"language-plaintext highlighter-rouge\">int8</code> Weight Quantization</strong>: Supported through offline quantization in <code class=\"language-plaintext highlighter-rouge\">coremltools</code>, mapping weights from float to signed 8-bit integers. This reduces storage and potentially improves memory bandwidth efficiency, but operations are still executed in <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">float32</code> internally on GPU/CPU/NPU. Operator and backend support for <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization is more limited compared to <code class=\"language-plaintext highlighter-rouge\">float16</code>.</p>\n      </li>\n      <li>\n        <p><strong>Custom bit-width quantization</strong>: Experimental support exists for 4-bit and other weight-only schemes via coremltools compression APIs, but these formats require manual handling and may only run on the CPU backend.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>Post-training quantization (PTQ) in CoreML is performed by passing additional parameters to <code class=\"language-plaintext highlighter-rouge\">coremltools.convert</code> or applying the <code class=\"language-plaintext highlighter-rouge\">coremltools.models.neural_network.quantization_utils</code> module to an existing <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code>. For example, <code class=\"language-plaintext highlighter-rouge\">float16</code> weight quantization is typically invoked as:</p>\n\n    <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code12\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code12\"><span class=\"kn\">import</span> <span class=\"nn\">coremltools</span> <span class=\"k\">as</span> <span class=\"n\">ct</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span>\n    <span class=\"n\">traced_model</span><span class=\"p\">,</span> \n    <span class=\"n\">convert_to</span><span class=\"o\">=</span><span class=\"s\">\"mlprogram\"</span><span class=\"p\">,</span>\n    <span class=\"n\">compute_units</span><span class=\"o\">=</span><span class=\"n\">ct</span><span class=\"p\">.</span><span class=\"n\">ComputeUnit</span><span class=\"p\">.</span><span class=\"n\">ALL</span>\n<span class=\"p\">)</span>\n<span class=\"n\">quantized_model</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"p\">.</span><span class=\"n\">models</span><span class=\"p\">.</span><span class=\"n\">neural_network</span><span class=\"p\">.</span><span class=\"n\">quantization_utils</span><span class=\"p\">.</span><span class=\"n\">quantize_weights</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">nbits</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">)</span>\n<span class=\"n\">quantized_model</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"model_float16.mlmodel\"</span><span class=\"p\">)</span>\n</code></pre></div>    </div>\n  </li>\n  <li>\n    <p>CoreML does not currently provide a native, framework-integrated Quantization-Aware Training (QAT) pipeline equivalent to TensorFlow Model Optimization Toolkit or PyTorch’s QAT modules. Instead, QAT must be performed in the source framework prior to export, and the resulting quantization parameters must be preserved during conversion—if the target CoreML format and operators support them. In practice, this is mostly applicable to simulated <code class=\"language-plaintext highlighter-rouge\">float16</code> or weight-clipped models, as CoreML conversion generally re-encodes models in its own quantization formats.</p>\n  </li>\n  <li>\n    <p>Hardware execution backends in CoreML include:</p>\n\n    <ul>\n      <li><strong>CPU</strong>: Executes in <code class=\"language-plaintext highlighter-rouge\">float32</code> or <code class=\"language-plaintext highlighter-rouge\">float16</code>, with weight-only <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization supported in some cases.</li>\n      <li><strong>GPU</strong>: Primarily executes in <code class=\"language-plaintext highlighter-rouge\">float16</code> precision. <code class=\"language-plaintext highlighter-rouge\">float16</code> weight quantization typically does not change GPU arithmetic precision but reduces memory usage.</li>\n      <li><strong>Apple Neural Engine (ANE)</strong>: Supports some <code class=\"language-plaintext highlighter-rouge\">int8</code> operations and mixed-precision execution. Operator coverage for quantized <code class=\"language-plaintext highlighter-rouge\">int8</code> is limited and depends on both CoreML runtime version and the specific ANE generation.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Developers should verify operator compatibility after quantization, as unsupported quantized layers will be automatically dequantized and executed in higher precision, potentially negating performance or memory savings. The <a href=\"https://coremltools.readme.io/docs/quantization\">CoreML Tools documentation on quantization</a> provides detailed guidance on supported modes and API usage.</p>\n  </li>\n</ul>\n<p>Quantization support in CoreML is integrated directly into the CoreML Tools conversion pipeline. Quantization can be applied during model conversion from popular frameworks (such as PyTorch or TensorFlow) to the <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code> format using the <a href=\"https://coremltools.readme.io/\">coremltools</a> Python API. The supported quantization schemes are primarily weight-only quantization, with formats including:</p>\n<ul>\n      <li>\n        <p><strong><code class=\"language-plaintext highlighter-rouge\">float16</code></strong>: Reduces the precision of weights from 32-bit floating point to 16-bit floating point. This is the most common and widely supported quantization type for CoreML, offering significant reductions in model size with minimal accuracy loss. In many cases, Apple hardware (e.g., A-series and M-series chips) executes GPU computations natively in <code class=\"language-plaintext highlighter-rouge\">float16</code>, so <code class=\"language-plaintext highlighter-rouge\">float16</code> quantization primarily benefits memory footprint and model loading speed rather than raw compute throughput.</p>\n      </li>\n      <li>\n        <p><strong>Linear <code class=\"language-plaintext highlighter-rouge\">int8</code> Weight Quantization</strong>: Supported through offline quantization in <code class=\"language-plaintext highlighter-rouge\">coremltools</code>, mapping weights from float to signed 8-bit integers. This reduces storage and potentially improves memory bandwidth efficiency, but operations are still executed in <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">float32</code> internally on GPU/CPU/NPU. Operator and backend support for <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization is more limited compared to <code class=\"language-plaintext highlighter-rouge\">float16</code>.</p>\n      </li>\n      <li>\n        <p><strong>Custom bit-width quantization</strong>: Experimental support exists for 4-bit and other weight-only schemes via coremltools compression APIs, but these formats require manual handling and may only run on the CPU backend.</p>\n      </li>\n    </ul>\n<p><strong><code class=\"language-plaintext highlighter-rouge\">float16</code></strong>: Reduces the precision of weights from 32-bit floating point to 16-bit floating point. This is the most common and widely supported quantization type for CoreML, offering significant reductions in model size with minimal accuracy loss. In many cases, Apple hardware (e.g., A-series and M-series chips) executes GPU computations natively in <code class=\"language-plaintext highlighter-rouge\">float16</code>, so <code class=\"language-plaintext highlighter-rouge\">float16</code> quantization primarily benefits memory footprint and model loading speed rather than raw compute throughput.</p>\n<p><strong>Linear <code class=\"language-plaintext highlighter-rouge\">int8</code> Weight Quantization</strong>: Supported through offline quantization in <code class=\"language-plaintext highlighter-rouge\">coremltools</code>, mapping weights from float to signed 8-bit integers. This reduces storage and potentially improves memory bandwidth efficiency, but operations are still executed in <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">float32</code> internally on GPU/CPU/NPU. Operator and backend support for <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization is more limited compared to <code class=\"language-plaintext highlighter-rouge\">float16</code>.</p>\n<p><strong>Custom bit-width quantization</strong>: Experimental support exists for 4-bit and other weight-only schemes via coremltools compression APIs, but these formats require manual handling and may only run on the CPU backend.</p>\n<p>Post-training quantization (PTQ) in CoreML is performed by passing additional parameters to <code class=\"language-plaintext highlighter-rouge\">coremltools.convert</code> or applying the <code class=\"language-plaintext highlighter-rouge\">coremltools.models.neural_network.quantization_utils</code> module to an existing <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code>. For example, <code class=\"language-plaintext highlighter-rouge\">float16</code> weight quantization is typically invoked as:</p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code12\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code12\"><span class=\"kn\">import</span> <span class=\"nn\">coremltools</span> <span class=\"k\">as</span> <span class=\"n\">ct</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span>\n    <span class=\"n\">traced_model</span><span class=\"p\">,</span> \n    <span class=\"n\">convert_to</span><span class=\"o\">=</span><span class=\"s\">\"mlprogram\"</span><span class=\"p\">,</span>\n    <span class=\"n\">compute_units</span><span class=\"o\">=</span><span class=\"n\">ct</span><span class=\"p\">.</span><span class=\"n\">ComputeUnit</span><span class=\"p\">.</span><span class=\"n\">ALL</span>\n<span class=\"p\">)</span>\n<span class=\"n\">quantized_model</span> <span class=\"o\">=</span> <span class=\"n\">ct</span><span class=\"p\">.</span><span class=\"n\">models</span><span class=\"p\">.</span><span class=\"n\">neural_network</span><span class=\"p\">.</span><span class=\"n\">quantization_utils</span><span class=\"p\">.</span><span class=\"n\">quantize_weights</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">nbits</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">)</span>\n<span class=\"n\">quantized_model</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"model_float16.mlmodel\"</span><span class=\"p\">)</span>\n</code></pre>\n<p>CoreML does not currently provide a native, framework-integrated Quantization-Aware Training (QAT) pipeline equivalent to TensorFlow Model Optimization Toolkit or PyTorch’s QAT modules. Instead, QAT must be performed in the source framework prior to export, and the resulting quantization parameters must be preserved during conversion—if the target CoreML format and operators support them. In practice, this is mostly applicable to simulated <code class=\"language-plaintext highlighter-rouge\">float16</code> or weight-clipped models, as CoreML conversion generally re-encodes models in its own quantization formats.</p>\n<p>Hardware execution backends in CoreML include:</p>\n<ul>\n      <li><strong>CPU</strong>: Executes in <code class=\"language-plaintext highlighter-rouge\">float32</code> or <code class=\"language-plaintext highlighter-rouge\">float16</code>, with weight-only <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization supported in some cases.</li>\n      <li><strong>GPU</strong>: Primarily executes in <code class=\"language-plaintext highlighter-rouge\">float16</code> precision. <code class=\"language-plaintext highlighter-rouge\">float16</code> weight quantization typically does not change GPU arithmetic precision but reduces memory usage.</li>\n      <li><strong>Apple Neural Engine (ANE)</strong>: Supports some <code class=\"language-plaintext highlighter-rouge\">int8</code> operations and mixed-precision execution. Operator coverage for quantized <code class=\"language-plaintext highlighter-rouge\">int8</code> is limited and depends on both CoreML runtime version and the specific ANE generation.</li>\n    </ul>\n<p>Developers should verify operator compatibility after quantization, as unsupported quantized layers will be automatically dequantized and executed in higher precision, potentially negating performance or memory savings. The <a href=\"https://coremltools.readme.io/docs/quantization\">CoreML Tools documentation on quantization</a> provides detailed guidance on supported modes and API usage.</p>\n<h5 id=\"integration-with-pytorch-and-tensorflow-models\">Integration with PyTorch and TensorFlow Models</h5>\n<ul>\n  <li>\n    <p>When converting PyTorch models to CoreML using <code class=\"language-plaintext highlighter-rouge\">torch.jit.trace</code> or <code class=\"language-plaintext highlighter-rouge\">torch.jit.script</code>, quantization should generally be applied during or after conversion via <code class=\"language-plaintext highlighter-rouge\">coremltools</code> rather than relying on PyTorch’s native quantization formats, as these may not be mapped directly to CoreML equivalents.</p>\n  </li>\n  <li>\n    <p>TensorFlow models exported via SavedModel or TFLite can be converted to CoreML, but quantized TFLite <code class=\"language-plaintext highlighter-rouge\">int8</code> models are usually re-encoded in <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">float32</code> in the final <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code> unless explicitly mapped to CoreML’s weight-only <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization.</p>\n  </li>\n  <li>\n    <p>In both cases, the recommended process for <code class=\"language-plaintext highlighter-rouge\">float16</code> quantization is:</p>\n\n    <ol>\n      <li>Train or fine-tune the model in the source framework.</li>\n      <li>Export to an intermediate format (TorchScript, ONNX, SavedModel).</li>\n      <li>Convert to CoreML using <code class=\"language-plaintext highlighter-rouge\">coremltools.convert</code>.</li>\n      <li>Apply <code class=\"language-plaintext highlighter-rouge\">float16</code> weight quantization via <code class=\"language-plaintext highlighter-rouge\">quantize_weights</code>.</li>\n      <li>Validate model accuracy and operator execution backend in Xcode or using the CoreML runtime.</li>\n    </ol>\n  </li>\n</ul>\n<p>When converting PyTorch models to CoreML using <code class=\"language-plaintext highlighter-rouge\">torch.jit.trace</code> or <code class=\"language-plaintext highlighter-rouge\">torch.jit.script</code>, quantization should generally be applied during or after conversion via <code class=\"language-plaintext highlighter-rouge\">coremltools</code> rather than relying on PyTorch’s native quantization formats, as these may not be mapped directly to CoreML equivalents.</p>\n<p>TensorFlow models exported via SavedModel or TFLite can be converted to CoreML, but quantized TFLite <code class=\"language-plaintext highlighter-rouge\">int8</code> models are usually re-encoded in <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">float32</code> in the final <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code> unless explicitly mapped to CoreML’s weight-only <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization.</p>\n<p>In both cases, the recommended process for <code class=\"language-plaintext highlighter-rouge\">float16</code> quantization is:</p>\n<ol>\n      <li>Train or fine-tune the model in the source framework.</li>\n      <li>Export to an intermediate format (TorchScript, ONNX, SavedModel).</li>\n      <li>Convert to CoreML using <code class=\"language-plaintext highlighter-rouge\">coremltools.convert</code>.</li>\n      <li>Apply <code class=\"language-plaintext highlighter-rouge\">float16</code> weight quantization via <code class=\"language-plaintext highlighter-rouge\">quantize_weights</code>.</li>\n      <li>Validate model accuracy and operator execution backend in Xcode or using the CoreML runtime.</li>\n    </ol>\n<h5 id=\"resources-2\">Resources</h5>\n<ul>\n  <li><a href=\"https://coremltools.readme.io/docs/quantization\">CoreML Tools Quantization Documentation</a></li>\n  <li><a href=\"https://github.com/apple/coremltools\">CoreML Tools GitHub Repository</a></li>\n</ul>\n<h4 id=\"jax\">JAX</h4>\n<ul>\n  <li>\n    <p>Quantization support in JAX is not built directly into the core library, as JAX is designed to be a high-performance array computation framework with a functional API and just-in-time (JIT) compilation through the XLA compiler (Accelerated Linear Algebra). Instead, quantization workflows in JAX are implemented through external libraries and ecosystem tools that target specific hardware backends. Prominent examples include:</p>\n\n    <ul>\n      <li><strong>Flax</strong>: A neural network library for JAX that provides high-level model definitions but does not offer native quantization APIs. Quantization is typically performed by integrating Flax models with downstream compilers or deployment toolchains.</li>\n      <li><strong><code class=\"language-plaintext highlighter-rouge\">jax.lax</code></strong> and <strong>custom lowering to XLA</strong>: Developers can manually simulate quantization during training by inserting quantization and dequantization operations using <code class=\"language-plaintext highlighter-rouge\">jax.lax</code> primitives. These operations are compiled into the XLA graph and can be mapped to quantized kernels if the target backend supports them.</li>\n      <li>\n        <p><strong>External compilers</strong>:</p>\n\n        <ul>\n          <li><strong>TensorFlow Lite via <code class=\"language-plaintext highlighter-rouge\">jax2tf</code></strong>: Models written in JAX can be converted to TensorFlow using <code class=\"language-plaintext highlighter-rouge\">jax.experimental.jax2tf</code>, and then quantized using TFLite’s post-training or QAT pipelines.</li>\n          <li><strong>XLA backends for TPU/CPU</strong>: Some integer-based execution paths are available for TPUs through XLA, but these are not exposed as a stable, user-facing quantization API in JAX.</li>\n          <li><strong>OpenXLA / IREE</strong>: Experimental support exists for lowering JAX computations to IREE, which can target <code class=\"language-plaintext highlighter-rouge\">int8</code> quantized inference on specific accelerators (e.g., Vulkan, CPU, GPU).</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>Because JAX itself does not provide a quantized operator registry, operator support for quantized execution depends entirely on the downstream backend. For example:</p>\n\n    <ul>\n      <li>When exporting to TFLite, the available quantized ops match those documented in the <a href=\"https://ai.google.dev/edge/litert/models/ops_compatibility\">TFLite operator compatibility guide</a>.</li>\n      <li>When compiling to <code class=\"language-plaintext highlighter-rouge\">XLA:TPU</code> or <code class=\"language-plaintext highlighter-rouge\">XLA:CPU</code>, support for integer quantization is highly backend-specific and often limited to linear and convolution operations.</li>\n      <li>On GPUs, JAX generally runs computations in <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> for mixed-precision training/inference, rather than integer quantization.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Quantization-Aware Training (QAT)</strong> in JAX is typically implemented manually, as there is no built-in helper API equivalent to PyTorch’s <code class=\"language-plaintext highlighter-rouge\">torch.quantization</code> or TensorFlow’s TFMOT. The common workflow is:</p>\n\n    <ol>\n      <li>Insert fake quantization nodes (scale + round + clip) into the model during training to simulate integer precision effects.</li>\n      <li>Train the model using JAX transformations (<code class=\"language-plaintext highlighter-rouge\">jit</code>, <code class=\"language-plaintext highlighter-rouge\">grad</code>, <code class=\"language-plaintext highlighter-rouge\">pmap</code>) as usual, with quantization simulation integrated into the forward pass.</li>\n      <li>Export the trained model to a backend that supports true integer quantized kernels (e.g., TFLite or IREE).</li>\n    </ol>\n\n    <ul>\n      <li>This approach requires careful control over numerical ranges and scale factors, which are not automatically managed by the framework.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hardware backend considerations</strong> for quantized JAX models:</p>\n\n    <ul>\n      <li><strong>CPU (<code class=\"language-plaintext highlighter-rouge\">XLA:CPU</code>)</strong>: Can execute integer operations if the compiled XLA graph contains integer kernels, but full <code class=\"language-plaintext highlighter-rouge\">int8</code> operator coverage is limited compared to float execution.</li>\n      <li><strong>TPU (<code class=\"language-plaintext highlighter-rouge\">XLA:TPU</code>)</strong>: Supports <code class=\"language-plaintext highlighter-rouge\">int8</code> matmul and convolution on newer TPU architectures, but model preparation for TPU quantization requires manual lowering.</li>\n      <li><strong>GPU (<code class=\"language-plaintext highlighter-rouge\">XLA:GPU</code>)</strong>: Typically favors <code class=\"language-plaintext highlighter-rouge\">float16</code>/<code class=\"language-plaintext highlighter-rouge\">bfloat16</code> mixed-precision execution. Integer quantization is not a standard deployment path.</li>\n      <li><strong>Edge/Embedded</strong>: Usually requires exporting to TFLite or another inference framework; quantization support and operator coverage then depend entirely on that target runtime.</li>\n    </ul>\n  </li>\n</ul>\n<p>Quantization support in JAX is not built directly into the core library, as JAX is designed to be a high-performance array computation framework with a functional API and just-in-time (JIT) compilation through the XLA compiler (Accelerated Linear Algebra). Instead, quantization workflows in JAX are implemented through external libraries and ecosystem tools that target specific hardware backends. Prominent examples include:</p>\n<ul>\n      <li><strong>Flax</strong>: A neural network library for JAX that provides high-level model definitions but does not offer native quantization APIs. Quantization is typically performed by integrating Flax models with downstream compilers or deployment toolchains.</li>\n      <li><strong><code class=\"language-plaintext highlighter-rouge\">jax.lax</code></strong> and <strong>custom lowering to XLA</strong>: Developers can manually simulate quantization during training by inserting quantization and dequantization operations using <code class=\"language-plaintext highlighter-rouge\">jax.lax</code> primitives. These operations are compiled into the XLA graph and can be mapped to quantized kernels if the target backend supports them.</li>\n      <li>\n        <p><strong>External compilers</strong>:</p>\n\n        <ul>\n          <li><strong>TensorFlow Lite via <code class=\"language-plaintext highlighter-rouge\">jax2tf</code></strong>: Models written in JAX can be converted to TensorFlow using <code class=\"language-plaintext highlighter-rouge\">jax.experimental.jax2tf</code>, and then quantized using TFLite’s post-training or QAT pipelines.</li>\n          <li><strong>XLA backends for TPU/CPU</strong>: Some integer-based execution paths are available for TPUs through XLA, but these are not exposed as a stable, user-facing quantization API in JAX.</li>\n          <li><strong>OpenXLA / IREE</strong>: Experimental support exists for lowering JAX computations to IREE, which can target <code class=\"language-plaintext highlighter-rouge\">int8</code> quantized inference on specific accelerators (e.g., Vulkan, CPU, GPU).</li>\n        </ul>\n      </li>\n    </ul>\n<p><strong>External compilers</strong>:</p>\n<ul>\n          <li><strong>TensorFlow Lite via <code class=\"language-plaintext highlighter-rouge\">jax2tf</code></strong>: Models written in JAX can be converted to TensorFlow using <code class=\"language-plaintext highlighter-rouge\">jax.experimental.jax2tf</code>, and then quantized using TFLite’s post-training or QAT pipelines.</li>\n          <li><strong>XLA backends for TPU/CPU</strong>: Some integer-based execution paths are available for TPUs through XLA, but these are not exposed as a stable, user-facing quantization API in JAX.</li>\n          <li><strong>OpenXLA / IREE</strong>: Experimental support exists for lowering JAX computations to IREE, which can target <code class=\"language-plaintext highlighter-rouge\">int8</code> quantized inference on specific accelerators (e.g., Vulkan, CPU, GPU).</li>\n        </ul>\n<p>Because JAX itself does not provide a quantized operator registry, operator support for quantized execution depends entirely on the downstream backend. For example:</p>\n<ul>\n      <li>When exporting to TFLite, the available quantized ops match those documented in the <a href=\"https://ai.google.dev/edge/litert/models/ops_compatibility\">TFLite operator compatibility guide</a>.</li>\n      <li>When compiling to <code class=\"language-plaintext highlighter-rouge\">XLA:TPU</code> or <code class=\"language-plaintext highlighter-rouge\">XLA:CPU</code>, support for integer quantization is highly backend-specific and often limited to linear and convolution operations.</li>\n      <li>On GPUs, JAX generally runs computations in <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> for mixed-precision training/inference, rather than integer quantization.</li>\n    </ul>\n<p><strong>Quantization-Aware Training (QAT)</strong> in JAX is typically implemented manually, as there is no built-in helper API equivalent to PyTorch’s <code class=\"language-plaintext highlighter-rouge\">torch.quantization</code> or TensorFlow’s TFMOT. The common workflow is:</p>\n<ol>\n      <li>Insert fake quantization nodes (scale + round + clip) into the model during training to simulate integer precision effects.</li>\n      <li>Train the model using JAX transformations (<code class=\"language-plaintext highlighter-rouge\">jit</code>, <code class=\"language-plaintext highlighter-rouge\">grad</code>, <code class=\"language-plaintext highlighter-rouge\">pmap</code>) as usual, with quantization simulation integrated into the forward pass.</li>\n      <li>Export the trained model to a backend that supports true integer quantized kernels (e.g., TFLite or IREE).</li>\n    </ol>\n<ul>\n      <li>This approach requires careful control over numerical ranges and scale factors, which are not automatically managed by the framework.</li>\n    </ul>\n<p><strong>Hardware backend considerations</strong> for quantized JAX models:</p>\n<ul>\n      <li><strong>CPU (<code class=\"language-plaintext highlighter-rouge\">XLA:CPU</code>)</strong>: Can execute integer operations if the compiled XLA graph contains integer kernels, but full <code class=\"language-plaintext highlighter-rouge\">int8</code> operator coverage is limited compared to float execution.</li>\n      <li><strong>TPU (<code class=\"language-plaintext highlighter-rouge\">XLA:TPU</code>)</strong>: Supports <code class=\"language-plaintext highlighter-rouge\">int8</code> matmul and convolution on newer TPU architectures, but model preparation for TPU quantization requires manual lowering.</li>\n      <li><strong>GPU (<code class=\"language-plaintext highlighter-rouge\">XLA:GPU</code>)</strong>: Typically favors <code class=\"language-plaintext highlighter-rouge\">float16</code>/<code class=\"language-plaintext highlighter-rouge\">bfloat16</code> mixed-precision execution. Integer quantization is not a standard deployment path.</li>\n      <li><strong>Edge/Embedded</strong>: Usually requires exporting to TFLite or another inference framework; quantization support and operator coverage then depend entirely on that target runtime.</li>\n    </ul>\n<h5 id=\"integration-examples\">Integration Examples</h5>\n<ul>\n  <li>\n    <p><strong>Exporting JAX models for quantized deployment via TFLite</strong>:</p>\n\n    <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code13\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code13\"><span class=\"kn\">import</span> <span class=\"nn\">jax</span>\n<span class=\"kn\">import</span> <span class=\"nn\">jax.numpy</span> <span class=\"k\">as</span> <span class=\"n\">jnp</span>\n<span class=\"kn\">from</span> <span class=\"nn\">jax.experimental</span> <span class=\"kn\">import</span> <span class=\"n\">jax2tf</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n\n<span class=\"c1\"># Define a simple JAX function\n</span><span class=\"k\">def</span> <span class=\"nf\">model</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">params</span>\n    <span class=\"k\">return</span> <span class=\"n\">jnp</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n\n<span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">jnp</span><span class=\"p\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">)),</span> <span class=\"n\">jnp</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"mi\">4</span><span class=\"p\">,)))</span>\n<span class=\"n\">tf_model</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">function</span><span class=\"p\">(</span>\n    <span class=\"n\">jax2tf</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">),</span> <span class=\"n\">with_gradient</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">),</span>\n    <span class=\"n\">input_signature</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">TensorSpec</span><span class=\"p\">([</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)]</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Convert to TFLite with quantization\n</span><span class=\"n\">converter</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">lite</span><span class=\"p\">.</span><span class=\"n\">TFLiteConverter</span><span class=\"p\">.</span><span class=\"n\">from_concrete_functions</span><span class=\"p\">([</span><span class=\"n\">tf_model</span><span class=\"p\">.</span><span class=\"n\">get_concrete_function</span><span class=\"p\">()])</span>\n<span class=\"n\">converter</span><span class=\"p\">.</span><span class=\"n\">optimizations</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">lite</span><span class=\"p\">.</span><span class=\"n\">Optimize</span><span class=\"p\">.</span><span class=\"n\">DEFAULT</span><span class=\"p\">]</span>\n<span class=\"n\">tflite_quant_model</span> <span class=\"o\">=</span> <span class=\"n\">converter</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">()</span>\n</code></pre></div>    </div>\n  </li>\n  <li>\n    <p><strong>Manual QAT simulation in JAX</strong>:</p>\n\n    <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code14\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code14\"><span class=\"k\">def</span> <span class=\"nf\">fake_quant</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">scale</span><span class=\"p\">,</span> <span class=\"n\">bits</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">):</span>\n    <span class=\"n\">qmin</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"o\">**</span><span class=\"p\">(</span><span class=\"n\">bits</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">qmax</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"o\">**</span><span class=\"p\">(</span><span class=\"n\">bits</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">))</span> <span class=\"o\">-</span> <span class=\"mi\">1</span>\n    <span class=\"n\">x_scaled</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">/</span> <span class=\"n\">scale</span>\n    <span class=\"n\">x_rounded</span> <span class=\"o\">=</span> <span class=\"n\">jnp</span><span class=\"p\">.</span><span class=\"n\">clip</span><span class=\"p\">(</span><span class=\"n\">jnp</span><span class=\"p\">.</span><span class=\"nb\">round</span><span class=\"p\">(</span><span class=\"n\">x_scaled</span><span class=\"p\">),</span> <span class=\"n\">qmin</span><span class=\"p\">,</span> <span class=\"n\">qmax</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">x_rounded</span> <span class=\"o\">*</span> <span class=\"n\">scale</span>\n</code></pre></div>    </div>\n\n    <ul>\n      <li>This kind of manual fake-quant insertion is common when prototyping QAT in JAX.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Exporting JAX models for quantized deployment via TFLite</strong>:</p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code13\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code13\"><span class=\"kn\">import</span> <span class=\"nn\">jax</span>\n<span class=\"kn\">import</span> <span class=\"nn\">jax.numpy</span> <span class=\"k\">as</span> <span class=\"n\">jnp</span>\n<span class=\"kn\">from</span> <span class=\"nn\">jax.experimental</span> <span class=\"kn\">import</span> <span class=\"n\">jax2tf</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n\n<span class=\"c1\"># Define a simple JAX function\n</span><span class=\"k\">def</span> <span class=\"nf\">model</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">params</span>\n    <span class=\"k\">return</span> <span class=\"n\">jnp</span><span class=\"p\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n\n<span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">jnp</span><span class=\"p\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">)),</span> <span class=\"n\">jnp</span><span class=\"p\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"mi\">4</span><span class=\"p\">,)))</span>\n<span class=\"n\">tf_model</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">function</span><span class=\"p\">(</span>\n    <span class=\"n\">jax2tf</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">),</span> <span class=\"n\">with_gradient</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">),</span>\n    <span class=\"n\">input_signature</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">TensorSpec</span><span class=\"p\">([</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)]</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Convert to TFLite with quantization\n</span><span class=\"n\">converter</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">lite</span><span class=\"p\">.</span><span class=\"n\">TFLiteConverter</span><span class=\"p\">.</span><span class=\"n\">from_concrete_functions</span><span class=\"p\">([</span><span class=\"n\">tf_model</span><span class=\"p\">.</span><span class=\"n\">get_concrete_function</span><span class=\"p\">()])</span>\n<span class=\"n\">converter</span><span class=\"p\">.</span><span class=\"n\">optimizations</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">lite</span><span class=\"p\">.</span><span class=\"n\">Optimize</span><span class=\"p\">.</span><span class=\"n\">DEFAULT</span><span class=\"p\">]</span>\n<span class=\"n\">tflite_quant_model</span> <span class=\"o\">=</span> <span class=\"n\">converter</span><span class=\"p\">.</span><span class=\"n\">convert</span><span class=\"p\">()</span>\n</code></pre>\n<p><strong>Manual QAT simulation in JAX</strong>:</p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code14\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code14\"><span class=\"k\">def</span> <span class=\"nf\">fake_quant</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">scale</span><span class=\"p\">,</span> <span class=\"n\">bits</span><span class=\"o\">=</span><span class=\"mi\">8</span><span class=\"p\">):</span>\n    <span class=\"n\">qmin</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"o\">**</span><span class=\"p\">(</span><span class=\"n\">bits</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">qmax</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"o\">**</span><span class=\"p\">(</span><span class=\"n\">bits</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">))</span> <span class=\"o\">-</span> <span class=\"mi\">1</span>\n    <span class=\"n\">x_scaled</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">/</span> <span class=\"n\">scale</span>\n    <span class=\"n\">x_rounded</span> <span class=\"o\">=</span> <span class=\"n\">jnp</span><span class=\"p\">.</span><span class=\"n\">clip</span><span class=\"p\">(</span><span class=\"n\">jnp</span><span class=\"p\">.</span><span class=\"nb\">round</span><span class=\"p\">(</span><span class=\"n\">x_scaled</span><span class=\"p\">),</span> <span class=\"n\">qmin</span><span class=\"p\">,</span> <span class=\"n\">qmax</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">x_rounded</span> <span class=\"o\">*</span> <span class=\"n\">scale</span>\n</code></pre>\n<ul>\n      <li>This kind of manual fake-quant insertion is common when prototyping QAT in JAX.</li>\n    </ul>\n<h5 id=\"resources-3\">Resources</h5>\n<ul>\n  <li><a href=\"https://jax.readthedocs.io/en/latest/jax2tf.html\">jax2tf Documentation</a> – Guide to converting JAX functions to TensorFlow for deployment.</li>\n  <li><a href=\"https://flax.readthedocs.io/en/latest/\">Flax Documentation</a> – JAX-based neural network library (does not provide quantization directly).</li>\n  <li><a href=\"https://www.tensorflow.org/lite/performance/post_training_quantization\">TFLite Quantization Guide</a> – Relevant for JAX models exported via TensorFlow.</li>\n  <li><a href=\"https://openxla.org/\">OpenXLA Project</a> – Future direction for cross-framework compilation, including quantization support.</li>\n</ul>",
      "contentMarkdown": "#### PyTorch\n\n*   Quantization in PyTorch is supported for a limited subset of operators, and the availability of these operators depends on the specific quantization approach being employed—dynamic, PTQ, or QAT. The list of supported quantized operators is not exhaustive and evolves with newer PyTorch releases. For an up-to-date reference, consult the official PyTorch quantization documentation [here](https://pytorch.org/docs/stable/quantization.html).\n    \n*   The implementation of quantization in PyTorch is backend-dependent, meaning that both the quantization configuration (which defines how tensors are quantized) and the set of quantized kernels (which define how arithmetic is performed on quantized tensors) vary based on the target hardware. Currently, PyTorch provides official support for quantized inference only on CPUs, specifically for x86 and ARM architectures. These are supported via two primary backends:\n    \n    *   **`fbgemm`**: Optimized for server-class x86 CPUs.\n    *   **`qnnpack`**: Designed for mobile ARM CPUs.\n*   The backend must be explicitly set to ensure compatibility between the model’s quantized representation and the runtime kernels, as shown in the example below:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n    `import torch  backend = 'fbgemm'  # 'qnnpack' for ARM/mobile inference my_model.qconfig = torch.quantization.get_default_qconfig(backend)  # Prepare and convert model # Set the backend on which the quantized kernels need to be run torch.backends.quantized.engine = backend  # Continue with model preparation and conversion steps...`\n    \n*   QAT in PyTorch is performed in full-precision (`float32`) mode to leverage existing GPU or CPU hardware during training. This technique simulates the effects of quantization during training to improve model robustness when deployed with quantized weights. QAT is particularly beneficial for convolutional neural networks (CNNs), especially lightweight models such as MobileNet, where static or dynamic post-training quantization may result in unacceptable accuracy degradation.\n    \n\nQuantization in PyTorch is supported for a limited subset of operators, and the availability of these operators depends on the specific quantization approach being employed—dynamic, PTQ, or QAT. The list of supported quantized operators is not exhaustive and evolves with newer PyTorch releases. For an up-to-date reference, consult the official PyTorch quantization documentation [here](https://pytorch.org/docs/stable/quantization.html).\n\nThe implementation of quantization in PyTorch is backend-dependent, meaning that both the quantization configuration (which defines how tensors are quantized) and the set of quantized kernels (which define how arithmetic is performed on quantized tensors) vary based on the target hardware. Currently, PyTorch provides official support for quantized inference only on CPUs, specifically for x86 and ARM architectures. These are supported via two primary backends:\n\n*   **`fbgemm`**: Optimized for server-class x86 CPUs.\n*   **`qnnpack`**: Designed for mobile ARM CPUs.\n\nThe backend must be explicitly set to ensure compatibility between the model’s quantized representation and the runtime kernels, as shown in the example below:\n\n![](https://aman.ai/images/copy.png)\n\n`import torch  backend = 'fbgemm'  # 'qnnpack' for ARM/mobile inference my_model.qconfig = torch.quantization.get_default_qconfig(backend)  # Prepare and convert model # Set the backend on which the quantized kernels need to be run torch.backends.quantized.engine = backend  # Continue with model preparation and conversion steps...`\n\nQAT in PyTorch is performed in full-precision (`float32`) mode to leverage existing GPU or CPU hardware during training. This technique simulates the effects of quantization during training to improve model robustness when deployed with quantized weights. QAT is particularly beneficial for convolutional neural networks (CNNs), especially lightweight models such as MobileNet, where static or dynamic post-training quantization may result in unacceptable accuracy degradation.\n\n##### Integration in `torchvision`\n\n*   The `torchvision` library includes integrated support for quantization in several widely used neural network architectures. These include GoogLeNet, InceptionV3, ResNet (various depths), ResNeXt, MobileNet (V2 and V3), and ShuffleNet. The support is provided in three distinct forms to enable a range of workflows:\n    \n    1.  **Pre-trained quantized model weights**: These models are fully quantized and can be used directly for inference without additional fine-tuning.\n    2.  **Quantization-ready model definitions**: These are versions of the models with quantization stubs pre-inserted, making them suitable for post-training quantization or QAT.\n    3.  **QAT scripts**: Scripts are available to perform QAT on supported models. While these scripts are applicable to all the models listed above, empirical evaluations show that QAT tends to yield significant accuracy benefits primarily for lightweight models like MobileNet.\n*   [Here’s](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/quantized_transfer_learning_tutorial.ipynb) a dedicated tutorial demonstrating how to perform transfer learning with quantization using pre-trained models from `torchvision`. This enables developers to take advantage of quantized inference while still adapting models to custom datasets and deployment scenarios.\n    \n\nThe `torchvision` library includes integrated support for quantization in several widely used neural network architectures. These include GoogLeNet, InceptionV3, ResNet (various depths), ResNeXt, MobileNet (V2 and V3), and ShuffleNet. The support is provided in three distinct forms to enable a range of workflows:\n\n1.  **Pre-trained quantized model weights**: These models are fully quantized and can be used directly for inference without additional fine-tuning.\n2.  **Quantization-ready model definitions**: These are versions of the models with quantization stubs pre-inserted, making them suitable for post-training quantization or QAT.\n3.  **QAT scripts**: Scripts are available to perform QAT on supported models. While these scripts are applicable to all the models listed above, empirical evaluations show that QAT tends to yield significant accuracy benefits primarily for lightweight models like MobileNet.\n\n[Here’s](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/quantized_transfer_learning_tutorial.ipynb) a dedicated tutorial demonstrating how to perform transfer learning with quantization using pre-trained models from `torchvision`. This enables developers to take advantage of quantized inference while still adapting models to custom datasets and deployment scenarios.\n\n##### Resources\n\n*   To get started on quantizing your models in PyTorch, start with the tutorials on the [PyTorch website](https://pytorch.org/tutorials/#model-optimization).\n*   If you are working with sequence data, start with…\n    *   [Dynamic quantization for LSTM](https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html), or\n    *   [Dynamic quantization for BERT](https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html)\n*   If you are working with image data then we recommend starting with the [transfer learning with quantization tutorial](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/quantized_transfer_learning_tutorial.ipynb). Then you can explore [static post training quantization](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html).\n    *   If you find that the accuracy drop with post training quantization is too high, then try [quantization aware training](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html).\n\n*   [Dynamic quantization for LSTM](https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html), or\n*   [Dynamic quantization for BERT](https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html)\n\n*   If you find that the accuracy drop with post training quantization is too high, then try [quantization aware training](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html).\n\n#### TensorFlow\n\n*   Quantization support in TensorFlow is primarily centered around deployment through TensorFlow Lite (TFLite). Only a subset of TensorFlow operations are supported in quantized form when converting models to run efficiently on edge devices. For a comprehensive list of quantization-compatible operators, refer to the official TFLite operator compatibility documentation [here](https://ai.google.dev/edge/litert/models/ops_compatibility).\n    \n*   Quantized inference in TensorFlow is enabled through TensorFlow Lite (TFLite) delegates, which provide optimized execution across various hardware backends. These include:\n    \n    *   **CPU Delegate**: Supports `int8` and `float16` quantized models using XNNPack kernels, which are enabled by default in modern TFLite runtimes.\n    *   **GPU Delegate**: Accelerates inference on mobile and embedded GPUs. It supports `float16` quantization and, in limited cases, `int8` precision. The delegate is available on both Android and iOS platforms.\n    *   **NNAPI Delegate** (Android only): Interfaces with on-device hardware acceleration drivers. Quantized `int8` models are typically supported and can see performance improvements depending on the device and vendor-specific drivers.\n    *   **Edge TPU Delegate**: Targets Google’s Coral hardware and supports only fully integer quantized models with `int8` weights and activations. Due to strict operator and quantization constraints, models must be carefully converted and then compiled using the Edge TPU Compiler.\n        \n    *   The level of operator support and performance characteristics differ by delegate. For example, the Edge TPU requires that all operations be quantized and supported by its limited op set. Any unsupported operations will result in compilation failure or will require fallback to CPU, which can significantly affect performance. As such, developers must validate operator compatibility prior to deployment by reviewing the [TFLite ops compatibility guide](https://ai.google.dev/edge/litert/models/ops_compatibility) and testing with their target delegate.\n*   QAT in TensorFlow is implemented using the `tfmot.quantization.keras.quantize_model` API available through the TensorFlow Model Optimization Toolkit (TFMOT). Similar to PyTorch, QAT in TensorFlow is performed in floating point, allowing the model to simulate quantized behavior during training while still leveraging GPU acceleration. This helps preserve accuracy for models that do not respond well to post-training quantization, such as compact architectures like MobileNet or custom CNNs. The general trade-offs between PTQ and QAT in TensorFlow align closely with those in PyTorch, although some feature and operator support mismatches still exist between the two frameworks.\n    \n*   When using post-training quantization or QAT, it’s important to validate that all critical model operations are supported in TFLite with quantized equivalents. Unsupported operations may be automatically left in float, potentially degrading the intended performance benefits of quantization.\n    \n\nQuantization support in TensorFlow is primarily centered around deployment through TensorFlow Lite (TFLite). Only a subset of TensorFlow operations are supported in quantized form when converting models to run efficiently on edge devices. For a comprehensive list of quantization-compatible operators, refer to the official TFLite operator compatibility documentation [here](https://ai.google.dev/edge/litert/models/ops_compatibility).\n\nQuantized inference in TensorFlow is enabled through TensorFlow Lite (TFLite) delegates, which provide optimized execution across various hardware backends. These include:\n\n*   **CPU Delegate**: Supports `int8` and `float16` quantized models using XNNPack kernels, which are enabled by default in modern TFLite runtimes.\n*   **GPU Delegate**: Accelerates inference on mobile and embedded GPUs. It supports `float16` quantization and, in limited cases, `int8` precision. The delegate is available on both Android and iOS platforms.\n*   **NNAPI Delegate** (Android only): Interfaces with on-device hardware acceleration drivers. Quantized `int8` models are typically supported and can see performance improvements depending on the device and vendor-specific drivers.\n*   **Edge TPU Delegate**: Targets Google’s Coral hardware and supports only fully integer quantized models with `int8` weights and activations. Due to strict operator and quantization constraints, models must be carefully converted and then compiled using the Edge TPU Compiler.\n    \n*   The level of operator support and performance characteristics differ by delegate. For example, the Edge TPU requires that all operations be quantized and supported by its limited op set. Any unsupported operations will result in compilation failure or will require fallback to CPU, which can significantly affect performance. As such, developers must validate operator compatibility prior to deployment by reviewing the [TFLite ops compatibility guide](https://ai.google.dev/edge/litert/models/ops_compatibility) and testing with their target delegate.\n\n**Edge TPU Delegate**: Targets Google’s Coral hardware and supports only fully integer quantized models with `int8` weights and activations. Due to strict operator and quantization constraints, models must be carefully converted and then compiled using the Edge TPU Compiler.\n\nQAT in TensorFlow is implemented using the `tfmot.quantization.keras.quantize_model` API available through the TensorFlow Model Optimization Toolkit (TFMOT). Similar to PyTorch, QAT in TensorFlow is performed in floating point, allowing the model to simulate quantized behavior during training while still leveraging GPU acceleration. This helps preserve accuracy for models that do not respond well to post-training quantization, such as compact architectures like MobileNet or custom CNNs. The general trade-offs between PTQ and QAT in TensorFlow align closely with those in PyTorch, although some feature and operator support mismatches still exist between the two frameworks.\n\nWhen using post-training quantization or QAT, it’s important to validate that all critical model operations are supported in TFLite with quantized equivalents. Unsupported operations may be automatically left in float, potentially degrading the intended performance benefits of quantization.\n\n##### Integration in `tf.keras.applications`\n\n*   While TensorFlow does not provide pre-quantized models in [`tf.keras.applications`](https://www.tensorflow.org/api_docs/python/tf/keras/applications), the Model Optimization Toolkit provides utilities to quantize these models post-training or prepare them for QAT. Developers can load a model from `tf.keras.applications`, apply quantization via TFMOT, and then convert it to TFLite. The process typically involves:\n    \n    1.  Cloning the model with quantization-aware layers using `quantize_model`.\n    2.  Fine-tuning the quantized model if needed.\n    3.  Converting the trained model to TFLite using the TFLiteConverter.\n*   TFLite provides tools and guidelines for performing transfer learning with quantized models, though, as with PyTorch, QAT tends to be necessary mainly for accuracy-sensitive lightweight models.\n    \n\nWhile TensorFlow does not provide pre-quantized models in [`tf.keras.applications`](https://www.tensorflow.org/api_docs/python/tf/keras/applications), the Model Optimization Toolkit provides utilities to quantize these models post-training or prepare them for QAT. Developers can load a model from `tf.keras.applications`, apply quantization via TFMOT, and then convert it to TFLite. The process typically involves:\n\n1.  Cloning the model with quantization-aware layers using `quantize_model`.\n2.  Fine-tuning the quantized model if needed.\n3.  Converting the trained model to TFLite using the TFLiteConverter.\n\nTFLite provides tools and guidelines for performing transfer learning with quantized models, though, as with PyTorch, QAT tends to be necessary mainly for accuracy-sensitive lightweight models.\n\n##### Resources\n\n*   TensorFlow’s PTQ techniques are detailed in the [post-training quantization guide](https://www.tensorflow.org/model_optimization/guide/quantization/post_training).\n    \n*   QAT is covered in the [QAT guide](https://www.tensorflow.org/model_optimization/guide/quantization/training).\n    \n\nTensorFlow’s PTQ techniques are detailed in the [post-training quantization guide](https://www.tensorflow.org/model_optimization/guide/quantization/post_training).\n\nQAT is covered in the [QAT guide](https://www.tensorflow.org/model_optimization/guide/quantization/training).\n\n#### CoreML\n\n*   Quantization support in CoreML is integrated directly into the CoreML Tools conversion pipeline. Quantization can be applied during model conversion from popular frameworks (such as PyTorch or TensorFlow) to the `.mlmodel` format using the [coremltools](https://coremltools.readme.io/) Python API. The supported quantization schemes are primarily weight-only quantization, with formats including:\n    \n    *   **`float16`**: Reduces the precision of weights from 32-bit floating point to 16-bit floating point. This is the most common and widely supported quantization type for CoreML, offering significant reductions in model size with minimal accuracy loss. In many cases, Apple hardware (e.g., A-series and M-series chips) executes GPU computations natively in `float16`, so `float16` quantization primarily benefits memory footprint and model loading speed rather than raw compute throughput.\n        \n    *   **Linear `int8` Weight Quantization**: Supported through offline quantization in `coremltools`, mapping weights from float to signed 8-bit integers. This reduces storage and potentially improves memory bandwidth efficiency, but operations are still executed in `float16` or `float32` internally on GPU/CPU/NPU. Operator and backend support for `int8` quantization is more limited compared to `float16`.\n        \n    *   **Custom bit-width quantization**: Experimental support exists for 4-bit and other weight-only schemes via coremltools compression APIs, but these formats require manual handling and may only run on the CPU backend.\n        \n*   Post-training quantization (PTQ) in CoreML is performed by passing additional parameters to `coremltools.convert` or applying the `coremltools.models.neural_network.quantization_utils` module to an existing `.mlmodel`. For example, `float16` weight quantization is typically invoked as:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n    `import coremltools as ct model = ct.convert(     traced_model,      convert_to=\"mlprogram\",     compute_units=ct.ComputeUnit.ALL ) quantized_model = ct.models.neural_network.quantization_utils.quantize_weights(model, nbits=16) quantized_model.save(\"model_float16.mlmodel\")`\n    \n*   CoreML does not currently provide a native, framework-integrated Quantization-Aware Training (QAT) pipeline equivalent to TensorFlow Model Optimization Toolkit or PyTorch’s QAT modules. Instead, QAT must be performed in the source framework prior to export, and the resulting quantization parameters must be preserved during conversion—if the target CoreML format and operators support them. In practice, this is mostly applicable to simulated `float16` or weight-clipped models, as CoreML conversion generally re-encodes models in its own quantization formats.\n    \n*   Hardware execution backends in CoreML include:\n    \n    *   **CPU**: Executes in `float32` or `float16`, with weight-only `int8` quantization supported in some cases.\n    *   **GPU**: Primarily executes in `float16` precision. `float16` weight quantization typically does not change GPU arithmetic precision but reduces memory usage.\n    *   **Apple Neural Engine (ANE)**: Supports some `int8` operations and mixed-precision execution. Operator coverage for quantized `int8` is limited and depends on both CoreML runtime version and the specific ANE generation.\n*   Developers should verify operator compatibility after quantization, as unsupported quantized layers will be automatically dequantized and executed in higher precision, potentially negating performance or memory savings. The [CoreML Tools documentation on quantization](https://coremltools.readme.io/docs/quantization) provides detailed guidance on supported modes and API usage.\n    \n\nQuantization support in CoreML is integrated directly into the CoreML Tools conversion pipeline. Quantization can be applied during model conversion from popular frameworks (such as PyTorch or TensorFlow) to the `.mlmodel` format using the [coremltools](https://coremltools.readme.io/) Python API. The supported quantization schemes are primarily weight-only quantization, with formats including:\n\n*   **`float16`**: Reduces the precision of weights from 32-bit floating point to 16-bit floating point. This is the most common and widely supported quantization type for CoreML, offering significant reductions in model size with minimal accuracy loss. In many cases, Apple hardware (e.g., A-series and M-series chips) executes GPU computations natively in `float16`, so `float16` quantization primarily benefits memory footprint and model loading speed rather than raw compute throughput.\n    \n*   **Linear `int8` Weight Quantization**: Supported through offline quantization in `coremltools`, mapping weights from float to signed 8-bit integers. This reduces storage and potentially improves memory bandwidth efficiency, but operations are still executed in `float16` or `float32` internally on GPU/CPU/NPU. Operator and backend support for `int8` quantization is more limited compared to `float16`.\n    \n*   **Custom bit-width quantization**: Experimental support exists for 4-bit and other weight-only schemes via coremltools compression APIs, but these formats require manual handling and may only run on the CPU backend.\n    \n\n**`float16`**: Reduces the precision of weights from 32-bit floating point to 16-bit floating point. This is the most common and widely supported quantization type for CoreML, offering significant reductions in model size with minimal accuracy loss. In many cases, Apple hardware (e.g., A-series and M-series chips) executes GPU computations natively in `float16`, so `float16` quantization primarily benefits memory footprint and model loading speed rather than raw compute throughput.\n\n**Linear `int8` Weight Quantization**: Supported through offline quantization in `coremltools`, mapping weights from float to signed 8-bit integers. This reduces storage and potentially improves memory bandwidth efficiency, but operations are still executed in `float16` or `float32` internally on GPU/CPU/NPU. Operator and backend support for `int8` quantization is more limited compared to `float16`.\n\n**Custom bit-width quantization**: Experimental support exists for 4-bit and other weight-only schemes via coremltools compression APIs, but these formats require manual handling and may only run on the CPU backend.\n\nPost-training quantization (PTQ) in CoreML is performed by passing additional parameters to `coremltools.convert` or applying the `coremltools.models.neural_network.quantization_utils` module to an existing `.mlmodel`. For example, `float16` weight quantization is typically invoked as:\n\n![](https://aman.ai/images/copy.png)\n\n`import coremltools as ct model = ct.convert(     traced_model,      convert_to=\"mlprogram\",     compute_units=ct.ComputeUnit.ALL ) quantized_model = ct.models.neural_network.quantization_utils.quantize_weights(model, nbits=16) quantized_model.save(\"model_float16.mlmodel\")`\n\nCoreML does not currently provide a native, framework-integrated Quantization-Aware Training (QAT) pipeline equivalent to TensorFlow Model Optimization Toolkit or PyTorch’s QAT modules. Instead, QAT must be performed in the source framework prior to export, and the resulting quantization parameters must be preserved during conversion—if the target CoreML format and operators support them. In practice, this is mostly applicable to simulated `float16` or weight-clipped models, as CoreML conversion generally re-encodes models in its own quantization formats.\n\nHardware execution backends in CoreML include:\n\n*   **CPU**: Executes in `float32` or `float16`, with weight-only `int8` quantization supported in some cases.\n*   **GPU**: Primarily executes in `float16` precision. `float16` weight quantization typically does not change GPU arithmetic precision but reduces memory usage.\n*   **Apple Neural Engine (ANE)**: Supports some `int8` operations and mixed-precision execution. Operator coverage for quantized `int8` is limited and depends on both CoreML runtime version and the specific ANE generation.\n\nDevelopers should verify operator compatibility after quantization, as unsupported quantized layers will be automatically dequantized and executed in higher precision, potentially negating performance or memory savings. The [CoreML Tools documentation on quantization](https://coremltools.readme.io/docs/quantization) provides detailed guidance on supported modes and API usage.\n\n##### Integration with PyTorch and TensorFlow Models\n\n*   When converting PyTorch models to CoreML using `torch.jit.trace` or `torch.jit.script`, quantization should generally be applied during or after conversion via `coremltools` rather than relying on PyTorch’s native quantization formats, as these may not be mapped directly to CoreML equivalents.\n    \n*   TensorFlow models exported via SavedModel or TFLite can be converted to CoreML, but quantized TFLite `int8` models are usually re-encoded in `float16` or `float32` in the final `.mlmodel` unless explicitly mapped to CoreML’s weight-only `int8` quantization.\n    \n*   In both cases, the recommended process for `float16` quantization is:\n    \n    1.  Train or fine-tune the model in the source framework.\n    2.  Export to an intermediate format (TorchScript, ONNX, SavedModel).\n    3.  Convert to CoreML using `coremltools.convert`.\n    4.  Apply `float16` weight quantization via `quantize_weights`.\n    5.  Validate model accuracy and operator execution backend in Xcode or using the CoreML runtime.\n\nWhen converting PyTorch models to CoreML using `torch.jit.trace` or `torch.jit.script`, quantization should generally be applied during or after conversion via `coremltools` rather than relying on PyTorch’s native quantization formats, as these may not be mapped directly to CoreML equivalents.\n\nTensorFlow models exported via SavedModel or TFLite can be converted to CoreML, but quantized TFLite `int8` models are usually re-encoded in `float16` or `float32` in the final `.mlmodel` unless explicitly mapped to CoreML’s weight-only `int8` quantization.\n\nIn both cases, the recommended process for `float16` quantization is:\n\n1.  Train or fine-tune the model in the source framework.\n2.  Export to an intermediate format (TorchScript, ONNX, SavedModel).\n3.  Convert to CoreML using `coremltools.convert`.\n4.  Apply `float16` weight quantization via `quantize_weights`.\n5.  Validate model accuracy and operator execution backend in Xcode or using the CoreML runtime.\n\n##### Resources\n\n*   [CoreML Tools Quantization Documentation](https://coremltools.readme.io/docs/quantization)\n*   [CoreML Tools GitHub Repository](https://github.com/apple/coremltools)\n\n#### JAX\n\n*   Quantization support in JAX is not built directly into the core library, as JAX is designed to be a high-performance array computation framework with a functional API and just-in-time (JIT) compilation through the XLA compiler (Accelerated Linear Algebra). Instead, quantization workflows in JAX are implemented through external libraries and ecosystem tools that target specific hardware backends. Prominent examples include:\n    \n    *   **Flax**: A neural network library for JAX that provides high-level model definitions but does not offer native quantization APIs. Quantization is typically performed by integrating Flax models with downstream compilers or deployment toolchains.\n    *   **`jax.lax`** and **custom lowering to XLA**: Developers can manually simulate quantization during training by inserting quantization and dequantization operations using `jax.lax` primitives. These operations are compiled into the XLA graph and can be mapped to quantized kernels if the target backend supports them.\n    *   **External compilers**:\n        \n        *   **TensorFlow Lite via `jax2tf`**: Models written in JAX can be converted to TensorFlow using `jax.experimental.jax2tf`, and then quantized using TFLite’s post-training or QAT pipelines.\n        *   **XLA backends for TPU/CPU**: Some integer-based execution paths are available for TPUs through XLA, but these are not exposed as a stable, user-facing quantization API in JAX.\n        *   **OpenXLA / IREE**: Experimental support exists for lowering JAX computations to IREE, which can target `int8` quantized inference on specific accelerators (e.g., Vulkan, CPU, GPU).\n*   Because JAX itself does not provide a quantized operator registry, operator support for quantized execution depends entirely on the downstream backend. For example:\n    \n    *   When exporting to TFLite, the available quantized ops match those documented in the [TFLite operator compatibility guide](https://ai.google.dev/edge/litert/models/ops_compatibility).\n    *   When compiling to `XLA:TPU` or `XLA:CPU`, support for integer quantization is highly backend-specific and often limited to linear and convolution operations.\n    *   On GPUs, JAX generally runs computations in `float16` or `bfloat16` for mixed-precision training/inference, rather than integer quantization.\n*   **Quantization-Aware Training (QAT)** in JAX is typically implemented manually, as there is no built-in helper API equivalent to PyTorch’s `torch.quantization` or TensorFlow’s TFMOT. The common workflow is:\n    \n    1.  Insert fake quantization nodes (scale + round + clip) into the model during training to simulate integer precision effects.\n    2.  Train the model using JAX transformations (`jit`, `grad`, `pmap`) as usual, with quantization simulation integrated into the forward pass.\n    3.  Export the trained model to a backend that supports true integer quantized kernels (e.g., TFLite or IREE).\n    \n    *   This approach requires careful control over numerical ranges and scale factors, which are not automatically managed by the framework.\n*   **Hardware backend considerations** for quantized JAX models:\n    \n    *   **CPU (`XLA:CPU`)**: Can execute integer operations if the compiled XLA graph contains integer kernels, but full `int8` operator coverage is limited compared to float execution.\n    *   **TPU (`XLA:TPU`)**: Supports `int8` matmul and convolution on newer TPU architectures, but model preparation for TPU quantization requires manual lowering.\n    *   **GPU (`XLA:GPU`)**: Typically favors `float16`/`bfloat16` mixed-precision execution. Integer quantization is not a standard deployment path.\n    *   **Edge/Embedded**: Usually requires exporting to TFLite or another inference framework; quantization support and operator coverage then depend entirely on that target runtime.\n\nQuantization support in JAX is not built directly into the core library, as JAX is designed to be a high-performance array computation framework with a functional API and just-in-time (JIT) compilation through the XLA compiler (Accelerated Linear Algebra). Instead, quantization workflows in JAX are implemented through external libraries and ecosystem tools that target specific hardware backends. Prominent examples include:\n\n*   **Flax**: A neural network library for JAX that provides high-level model definitions but does not offer native quantization APIs. Quantization is typically performed by integrating Flax models with downstream compilers or deployment toolchains.\n*   **`jax.lax`** and **custom lowering to XLA**: Developers can manually simulate quantization during training by inserting quantization and dequantization operations using `jax.lax` primitives. These operations are compiled into the XLA graph and can be mapped to quantized kernels if the target backend supports them.\n*   **External compilers**:\n    \n    *   **TensorFlow Lite via `jax2tf`**: Models written in JAX can be converted to TensorFlow using `jax.experimental.jax2tf`, and then quantized using TFLite’s post-training or QAT pipelines.\n    *   **XLA backends for TPU/CPU**: Some integer-based execution paths are available for TPUs through XLA, but these are not exposed as a stable, user-facing quantization API in JAX.\n    *   **OpenXLA / IREE**: Experimental support exists for lowering JAX computations to IREE, which can target `int8` quantized inference on specific accelerators (e.g., Vulkan, CPU, GPU).\n\n**External compilers**:\n\n*   **TensorFlow Lite via `jax2tf`**: Models written in JAX can be converted to TensorFlow using `jax.experimental.jax2tf`, and then quantized using TFLite’s post-training or QAT pipelines.\n*   **XLA backends for TPU/CPU**: Some integer-based execution paths are available for TPUs through XLA, but these are not exposed as a stable, user-facing quantization API in JAX.\n*   **OpenXLA / IREE**: Experimental support exists for lowering JAX computations to IREE, which can target `int8` quantized inference on specific accelerators (e.g., Vulkan, CPU, GPU).\n\nBecause JAX itself does not provide a quantized operator registry, operator support for quantized execution depends entirely on the downstream backend. For example:\n\n*   When exporting to TFLite, the available quantized ops match those documented in the [TFLite operator compatibility guide](https://ai.google.dev/edge/litert/models/ops_compatibility).\n*   When compiling to `XLA:TPU` or `XLA:CPU`, support for integer quantization is highly backend-specific and often limited to linear and convolution operations.\n*   On GPUs, JAX generally runs computations in `float16` or `bfloat16` for mixed-precision training/inference, rather than integer quantization.\n\n**Quantization-Aware Training (QAT)** in JAX is typically implemented manually, as there is no built-in helper API equivalent to PyTorch’s `torch.quantization` or TensorFlow’s TFMOT. The common workflow is:\n\n1.  Insert fake quantization nodes (scale + round + clip) into the model during training to simulate integer precision effects.\n2.  Train the model using JAX transformations (`jit`, `grad`, `pmap`) as usual, with quantization simulation integrated into the forward pass.\n3.  Export the trained model to a backend that supports true integer quantized kernels (e.g., TFLite or IREE).\n\n*   This approach requires careful control over numerical ranges and scale factors, which are not automatically managed by the framework.\n\n**Hardware backend considerations** for quantized JAX models:\n\n*   **CPU (`XLA:CPU`)**: Can execute integer operations if the compiled XLA graph contains integer kernels, but full `int8` operator coverage is limited compared to float execution.\n*   **TPU (`XLA:TPU`)**: Supports `int8` matmul and convolution on newer TPU architectures, but model preparation for TPU quantization requires manual lowering.\n*   **GPU (`XLA:GPU`)**: Typically favors `float16`/`bfloat16` mixed-precision execution. Integer quantization is not a standard deployment path.\n*   **Edge/Embedded**: Usually requires exporting to TFLite or another inference framework; quantization support and operator coverage then depend entirely on that target runtime.\n\n##### Integration Examples\n\n*   **Exporting JAX models for quantized deployment via TFLite**:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n    `import jax import jax.numpy as jnp from jax.experimental import jax2tf import tensorflow as tf  # Define a simple JAX function def model(params, x):     w, b = params     return jnp.dot(x, w) + b  params = (jnp.ones((4, 4)), jnp.zeros((4,))) tf_model = tf.function(     jax2tf.convert(lambda x: model(params, x), with_gradient=False),     input_signature=[tf.TensorSpec([None, 4], tf.float32)] )  # Convert to TFLite with quantization converter = tf.lite.TFLiteConverter.from_concrete_functions([tf_model.get_concrete_function()]) converter.optimizations = [tf.lite.Optimize.DEFAULT] tflite_quant_model = converter.convert()`\n    \n*   **Manual QAT simulation in JAX**:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n    `def fake_quant(x, scale, bits=8):     qmin = -(2**(bits - 1))     qmax = (2**(bits - 1)) - 1     x_scaled = x / scale     x_rounded = jnp.clip(jnp.round(x_scaled), qmin, qmax)     return x_rounded * scale`\n    \n    *   This kind of manual fake-quant insertion is common when prototyping QAT in JAX.\n\n**Exporting JAX models for quantized deployment via TFLite**:\n\n![](https://aman.ai/images/copy.png)\n\n`import jax import jax.numpy as jnp from jax.experimental import jax2tf import tensorflow as tf  # Define a simple JAX function def model(params, x):     w, b = params     return jnp.dot(x, w) + b  params = (jnp.ones((4, 4)), jnp.zeros((4,))) tf_model = tf.function(     jax2tf.convert(lambda x: model(params, x), with_gradient=False),     input_signature=[tf.TensorSpec([None, 4], tf.float32)] )  # Convert to TFLite with quantization converter = tf.lite.TFLiteConverter.from_concrete_functions([tf_model.get_concrete_function()]) converter.optimizations = [tf.lite.Optimize.DEFAULT] tflite_quant_model = converter.convert()`\n\n**Manual QAT simulation in JAX**:\n\n![](https://aman.ai/images/copy.png)\n\n`def fake_quant(x, scale, bits=8):     qmin = -(2**(bits - 1))     qmax = (2**(bits - 1)) - 1     x_scaled = x / scale     x_rounded = jnp.clip(jnp.round(x_scaled), qmin, qmax)     return x_rounded * scale`\n\n*   This kind of manual fake-quant insertion is common when prototyping QAT in JAX.\n\n##### Resources\n\n*   [jax2tf Documentation](https://jax.readthedocs.io/en/latest/jax2tf.html) – Guide to converting JAX functions to TensorFlow for deployment.\n*   [Flax Documentation](https://flax.readthedocs.io/en/latest/) – JAX-based neural network library (does not provide quantization directly).\n*   [TFLite Quantization Guide](https://www.tensorflow.org/lite/performance/post_training_quantization) – Relevant for JAX models exported via TensorFlow.\n*   [OpenXLA Project](https://openxla.org/) – Future direction for cross-framework compilation, including quantization support.",
      "order": 15,
      "orderInChapter": 15,
      "difficulty": 4,
      "estimatedMinutes": 24,
      "tags": [
        "ondevice ai",
        "neural network",
        "convolution",
        "cnn",
        "lstm",
        "bert",
        "optimization",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 4755,
        "contentLength": 65761
      },
      "nextCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17"
      ],
      "relatedCards": [
        "ai-on-device-transformers-tensor-processing-unit-tpu-5",
        "ai-on-device-transformers-balancing-compute-and-memory-prefill-vs-decode-opt-8",
        "ai-ner-summary-2",
        "ai-federated-learning-fedprox-7",
        "ai-federated-learning-scaffold-8"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#device-and-operator-support-across-frameworks",
      "scrapedAt": "2025-12-28T11:55:50.971Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-choosing-the-right-quantization-approach-16",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Choosing the Right Quantization Approach",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>\n    <p>The choice of which scheme to use depends on multiple factors:</p>\n\n    <ul>\n      <li><strong>Model/Target requirements:</strong> Some models might be sensitive to quantization, requiring QAT.</li>\n      <li><strong>Operator/Backend support:</strong> Some backends require fully quantized operators.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Currently, operator coverage in PyTorch is limited and may restrict the choices listed in the table below. The table below from <a href=\"https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\">PyTorch: Introduction to Quantization on PyTorch</a> provides a guideline.</p>\n  </li>\n</ul>\n<p>The choice of which scheme to use depends on multiple factors:</p>\n<ul>\n      <li><strong>Model/Target requirements:</strong> Some models might be sensitive to quantization, requiring QAT.</li>\n      <li><strong>Operator/Backend support:</strong> Some backends require fully quantized operators.</li>\n    </ul>\n<p>Currently, operator coverage in PyTorch is limited and may restrict the choices listed in the table below. The table below from <a href=\"https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\">PyTorch: Introduction to Quantization on PyTorch</a> provides a guideline.</p>\n<p><img src=\"/primers/ai/assets/model-compression/archquant.jpg\" alt=\"\"></p>",
      "contentMarkdown": "*   The choice of which scheme to use depends on multiple factors:\n    \n    *   **Model/Target requirements:** Some models might be sensitive to quantization, requiring QAT.\n    *   **Operator/Backend support:** Some backends require fully quantized operators.\n*   Currently, operator coverage in PyTorch is limited and may restrict the choices listed in the table below. The table below from [PyTorch: Introduction to Quantization on PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/) provides a guideline.\n    \n\nThe choice of which scheme to use depends on multiple factors:\n\n*   **Model/Target requirements:** Some models might be sensitive to quantization, requiring QAT.\n*   **Operator/Backend support:** Some backends require fully quantized operators.\n\nCurrently, operator coverage in PyTorch is limited and may restrict the choices listed in the table below. The table below from [PyTorch: Introduction to Quantization on PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/) provides a guideline.\n\n![](/primers/ai/assets/model-compression/archquant.jpg)",
      "order": 16,
      "orderInChapter": 16,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 127,
        "contentLength": 1330
      },
      "nextCards": [
        "ai-model-compression-performance-results-17",
        "ai-model-compression-accuracy-results-18"
      ],
      "relatedCards": [
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-mime-9",
        "ai-federated-learning-tooling-frameworks-14",
        "ai-federated-learning-lora-in-federated-context-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#choosing-the-right-quantization-approach",
      "scrapedAt": "2025-12-28T11:55:50.971Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-performance-results-17",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Performance Results",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>The table below from <a href=\"https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\">PyTorch: Introduction to Quantization on PyTorch</a> offers some sample results:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/model-compression/perfres.jpg\" alt=\"\"></p>\n<ul>\n  <li>As seen in the table above, quantization yields a substantial reduction in both model loading time and memory footprint, driven by the 4× smaller model size compared to floating-point implementations. Furthermore, it offers a speedup of 2× to 3× compared to floating-point implementations, depending on the hardware platform and the model being benchmarked.</li>\n</ul>",
      "contentMarkdown": "*   The table below from [PyTorch: Introduction to Quantization on PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/) offers some sample results:\n\n![](/primers/ai/assets/model-compression/perfres.jpg)\n\n*   As seen in the table above, quantization yields a substantial reduction in both model loading time and memory footprint, driven by the 4× smaller model size compared to floating-point implementations. Furthermore, it offers a speedup of 2× to 3× compared to floating-point implementations, depending on the hardware platform and the model being benchmarked.",
      "order": 17,
      "orderInChapter": 17,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 70,
        "contentLength": 654
      },
      "nextCards": [
        "ai-model-compression-accuracy-results-18",
        "ai-model-compression-popular-quantization-libraries-19"
      ],
      "relatedCards": [
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-mime-9",
        "ai-federated-learning-tooling-frameworks-14",
        "ai-federated-learning-lora-in-federated-context-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#performance-results",
      "scrapedAt": "2025-12-28T11:55:50.971Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-accuracy-results-18",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Accuracy Results",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>The tables below from <a href=\"https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\">PyTorch: Introduction to Quantization on PyTorch</a> compares the accuracy of static quantized models with the floating point models on Imagenet. For dynamic quantization, we compared the F1 score of BERT on the GLUE benchmark for MRPC.</li>\n</ul>\n<h4 id=\"computer-vision-model-accuracy\">Computer Vision Model Accuracy</h4>\n<p><img src=\"/primers/ai/assets/model-compression/cvmodelacc.jpg\" alt=\"\"></p>\n<h4 id=\"speech-and-nlp-model-accuracy\">Speech and NLP Model Accuracy</h4>\n<p><img src=\"/primers/ai/assets/model-compression/speechnlpmodelacc.jpg\" alt=\"\"></p>",
      "contentMarkdown": "*   The tables below from [PyTorch: Introduction to Quantization on PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/) compares the accuracy of static quantized models with the floating point models on Imagenet. For dynamic quantization, we compared the F1 score of BERT on the GLUE benchmark for MRPC.\n\n#### Computer Vision Model Accuracy\n\n![](/primers/ai/assets/model-compression/cvmodelacc.jpg)\n\n#### Speech and NLP Model Accuracy\n\n![](/primers/ai/assets/model-compression/speechnlpmodelacc.jpg)",
      "order": 18,
      "orderInChapter": 18,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "bert",
        "nlp",
        "computer vision"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 54,
        "contentLength": 669
      },
      "nextCards": [
        "ai-model-compression-popular-quantization-libraries-19",
        "ai-model-compression-how-far-can-quantization-be-pushed-20"
      ],
      "relatedCards": [
        "ai-federated-learning-transportation-self-driving-cars-24",
        "ai-differential-privacy-membership-inference-protection-13",
        "ai-differential-privacy-pros-15",
        "ai-differential-privacy-cons-16",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#accuracy-results",
      "scrapedAt": "2025-12-28T11:55:50.971Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-popular-quantization-libraries-19",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Popular Quantization Libraries",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>This section surveys widely used quantization libraries you’ll encounter in practice, what precision/algorithms they support, how they integrate with common stacks, and caveats that matter on-device or in production.</li>\n</ul>\n<h4 id=\"bitsandbytes\">BitsAndBytes</h4>\n<ul>\n  <li>\n    <p>BitsAndBytes is a CUDA-accelerated library for low-bit matrix multiply and optimizers. It popularized <code class=\"language-plaintext highlighter-rouge\">LLM.int8()</code>, which uses vector-wise 8-bit quantization but routes “outlier” channels through higher precision matmuls, preserving accuracy while halving memory. It also provides 4-bit weight types used by QLoRA (NF4/<code class=\"language-plaintext highlighter-rouge\">float4</code>), plus 8-bit optimizers. In Transformers you typically pass a BitsAndBytesConfig (<code class=\"language-plaintext highlighter-rouge\">load_in_4bit</code> or <code class=\"language-plaintext highlighter-rouge\">load_in_8bit</code>, <code class=\"language-plaintext highlighter-rouge\">bnb_4bit_quant_type=\"nf4\"</code> or <code class=\"language-plaintext highlighter-rouge\">\"fp4\"</code>, <code class=\"language-plaintext highlighter-rouge\">bnb_4bit_use_double_quant</code>, and <code class=\"language-plaintext highlighter-rouge\">bnb_4bit_compute_dtype</code> for BF16/FP16 accumulation).</p>\n  </li>\n  <li>\n    <p><strong>Typical usage (Transformers):</strong></p>\n\n    <ul>\n      <li>Construct BitsAndBytesConfig with <code class=\"language-plaintext highlighter-rouge\">load_in_4bit=True</code> and your preferred compute dtype (often BF16 for stability)</li>\n      <li>Load the model with <code class=\"language-plaintext highlighter-rouge\">device_map=\"auto\"</code>` to shard to available GPUs/CPU if needed</li>\n      <li>When merging adapters back into base weights, dequantize before merging to avoid rounding artifacts, then requantize.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Notes and caveats:</strong></p>\n\n    <ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">LLM.int8</code> is weight-only at matmul inputs; activations remain FP16/BF16, so runtime memory isn’t halved unless your serving stack fuses kernels well.</li>\n      <li>Performance varies by kernel/backend (CUDA vs. ROCm); vendor guides show the expected bottlenecks and setup.</li>\n    </ul>\n  </li>\n</ul>\n<p>BitsAndBytes is a CUDA-accelerated library for low-bit matrix multiply and optimizers. It popularized <code class=\"language-plaintext highlighter-rouge\">LLM.int8()</code>, which uses vector-wise 8-bit quantization but routes “outlier” channels through higher precision matmuls, preserving accuracy while halving memory. It also provides 4-bit weight types used by QLoRA (NF4/<code class=\"language-plaintext highlighter-rouge\">float4</code>), plus 8-bit optimizers. In Transformers you typically pass a BitsAndBytesConfig (<code class=\"language-plaintext highlighter-rouge\">load_in_4bit</code> or <code class=\"language-plaintext highlighter-rouge\">load_in_8bit</code>, <code class=\"language-plaintext highlighter-rouge\">bnb_4bit_quant_type=\"nf4\"</code> or <code class=\"language-plaintext highlighter-rouge\">\"fp4\"</code>, <code class=\"language-plaintext highlighter-rouge\">bnb_4bit_use_double_quant</code>, and <code class=\"language-plaintext highlighter-rouge\">bnb_4bit_compute_dtype</code> for BF16/FP16 accumulation).</p>\n<p><strong>Typical usage (Transformers):</strong></p>\n<ul>\n      <li>Construct BitsAndBytesConfig with <code class=\"language-plaintext highlighter-rouge\">load_in_4bit=True</code> and your preferred compute dtype (often BF16 for stability)</li>\n      <li>Load the model with <code class=\"language-plaintext highlighter-rouge\">device_map=\"auto\"</code>` to shard to available GPUs/CPU if needed</li>\n      <li>When merging adapters back into base weights, dequantize before merging to avoid rounding artifacts, then requantize.</li>\n    </ul>\n<p><strong>Notes and caveats:</strong></p>\n<ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">LLM.int8</code> is weight-only at matmul inputs; activations remain FP16/BF16, so runtime memory isn’t halved unless your serving stack fuses kernels well.</li>\n      <li>Performance varies by kernel/backend (CUDA vs. ROCm); vendor guides show the expected bottlenecks and setup.</li>\n    </ul>\n<h4 id=\"hugging-face-optimum\">Hugging Face Optimum</h4>\n<ul>\n  <li>\n    <p>Optimum is a meta-tooling layer that wraps several backends and exposes unified quantization APIs.</p>\n\n    <ul>\n      <li><strong>Optimum ONNX Runtime:</strong> PTQ/QAT flows via ORTConfig/ORTQuantizer; supports dynamic (weights int8, activations at runtime), static/QLinear/QDQ with calibration, and QAT. Target for CPU, some NPUs, and cross-platform packaging.</li>\n      <li><strong>Optimum Intel (OpenVINO):</strong> hooks into NNCF for <code class=\"language-plaintext highlighter-rouge\">int8</code> PTQ and QAT; good CPU/iGPU latency and small memory footprint; provides export to OpenVINO IR and pipeline examples for LLMs.</li>\n      <li><strong>Optimum Quanto:</strong> a PyTorch quantization backend that supports linear quantization of weights to float8, int8, int4, and even int2; works in eager mode, supports QAT, is device-agnostic, and integrates with torch.compile. Great for quick per-module weight-only experiments.</li>\n      <li><strong>Optimum Habana (Gaudi):</strong> integrates Intel Neural Compressor flows to enable <code class=\"language-plaintext highlighter-rouge\">float8</code>/<code class=\"language-plaintext highlighter-rouge\">uint4</code> inference/training paths on HPU.</li>\n      <li><strong>Transformers integration summary:</strong> Transformers supports AWQ, GPTQ, and bitsandbytes out-of-the-box; Optimum layers sit on top to add ORT/OpenVINO/Quanto/TensorRT flows.</li>\n    </ul>\n  </li>\n</ul>\n<p>Optimum is a meta-tooling layer that wraps several backends and exposes unified quantization APIs.</p>\n<ul>\n      <li><strong>Optimum ONNX Runtime:</strong> PTQ/QAT flows via ORTConfig/ORTQuantizer; supports dynamic (weights int8, activations at runtime), static/QLinear/QDQ with calibration, and QAT. Target for CPU, some NPUs, and cross-platform packaging.</li>\n      <li><strong>Optimum Intel (OpenVINO):</strong> hooks into NNCF for <code class=\"language-plaintext highlighter-rouge\">int8</code> PTQ and QAT; good CPU/iGPU latency and small memory footprint; provides export to OpenVINO IR and pipeline examples for LLMs.</li>\n      <li><strong>Optimum Quanto:</strong> a PyTorch quantization backend that supports linear quantization of weights to float8, int8, int4, and even int2; works in eager mode, supports QAT, is device-agnostic, and integrates with torch.compile. Great for quick per-module weight-only experiments.</li>\n      <li><strong>Optimum Habana (Gaudi):</strong> integrates Intel Neural Compressor flows to enable <code class=\"language-plaintext highlighter-rouge\">float8</code>/<code class=\"language-plaintext highlighter-rouge\">uint4</code> inference/training paths on HPU.</li>\n      <li><strong>Transformers integration summary:</strong> Transformers supports AWQ, GPTQ, and bitsandbytes out-of-the-box; Optimum layers sit on top to add ORT/OpenVINO/Quanto/TensorRT flows.</li>\n    </ul>\n<h4 id=\"onnx-runtime-quantization\">ONNX Runtime Quantization</h4>\n<ul>\n  <li>\n    <p>ONNX Runtime implements 8-bit linear quantization with scale and zero-point per tensor or per channel, using either static calibration or dynamic quantization. The core mapping follows <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-106-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;normal&quot;>f</mi><mi mathvariant=&quot;normal&quot;>p</mi><mn>32</mn></mrow></msub><mo>=</mo><mi>s</mi><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>&amp;#x2212;</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1304\" style=\"width: 8.596em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1007.09em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1305\"><span class=\"msubsup\" id=\"MathJax-Span-1306\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1307\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1308\"><span class=\"mrow\" id=\"MathJax-Span-1309\"><span class=\"mi\" id=\"MathJax-Span-1310\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1311\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">p</span><span class=\"mn\" id=\"MathJax-Span-1312\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">32</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1313\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1314\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">s</span><span class=\"mo\" id=\"MathJax-Span-1315\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-1316\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">(</span><span class=\"mi\" id=\"MathJax-Span-1317\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-1318\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-1319\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">z</span><span class=\"mo\" id=\"MathJax-Span-1320\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"normal\">f</mi><mi mathvariant=\"normal\">p</mi><mn>32</mn></mrow></msub><mo>=</mo><mi>s</mi><mo>⋅</mo><mo stretchy=\"false\">(</mo><mi>q</mi><mo>−</mo><mi>z</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-106\">x_\\mathrm{fp32} = s \\cdot (q - z)</script>; APIs include <code class=\"language-plaintext highlighter-rouge\">quantize_dynamic</code>, <code class=\"language-plaintext highlighter-rouge\">quantize_static</code> (with calibration), and <code class=\"language-plaintext highlighter-rouge\">quantize_qat</code> (for QAT-authored models).</p>\n  </li>\n  <li>\n    <p><strong>Why choose ORT:</strong> stable operator coverage, portable deployment as a single ONNX artifact, and strong CPU performance. Use when you want one model to run across x86/ARM servers and many NPUs with the same runtime.</p>\n  </li>\n</ul>\n<p>ONNX Runtime implements 8-bit linear quantization with scale and zero-point per tensor or per channel, using either static calibration or dynamic quantization. The core mapping follows <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-106-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;normal&quot;>f</mi><mi mathvariant=&quot;normal&quot;>p</mi><mn>32</mn></mrow></msub><mo>=</mo><mi>s</mi><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>&amp;#x2212;</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1304\" style=\"width: 8.596em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1007.09em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1305\"><span class=\"msubsup\" id=\"MathJax-Span-1306\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1307\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1308\"><span class=\"mrow\" id=\"MathJax-Span-1309\"><span class=\"mi\" id=\"MathJax-Span-1310\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1311\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">p</span><span class=\"mn\" id=\"MathJax-Span-1312\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">32</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1313\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1314\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">s</span><span class=\"mo\" id=\"MathJax-Span-1315\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-1316\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">(</span><span class=\"mi\" id=\"MathJax-Span-1317\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-1318\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-1319\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">z</span><span class=\"mo\" id=\"MathJax-Span-1320\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"normal\">f</mi><mi mathvariant=\"normal\">p</mi><mn>32</mn></mrow></msub><mo>=</mo><mi>s</mi><mo>⋅</mo><mo stretchy=\"false\">(</mo><mi>q</mi><mo>−</mo><mi>z</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-106\">x_\\mathrm{fp32} = s \\cdot (q - z)</script>; APIs include <code class=\"language-plaintext highlighter-rouge\">quantize_dynamic</code>, <code class=\"language-plaintext highlighter-rouge\">quantize_static</code> (with calibration), and <code class=\"language-plaintext highlighter-rouge\">quantize_qat</code> (for QAT-authored models).</p>\n<p><strong>Why choose ORT:</strong> stable operator coverage, portable deployment as a single ONNX artifact, and strong CPU performance. Use when you want one model to run across x86/ARM servers and many NPUs with the same runtime.</p>\n<h4 id=\"nvidia-tensorrt--tensorrt-llm\">NVIDIA TensorRT / TensorRT-LLM</h4>\n<ul>\n  <li>\n    <p>TensorRT provides PTQ (with explicit quantization replacing older calibration APIs) and supports QAT. TensorRT-LLM adds LLM-specific kernels and recipes: <code class=\"language-plaintext highlighter-rouge\">float8</code>/<code class=\"language-plaintext highlighter-rouge\">float4</code>, <code class=\"language-plaintext highlighter-rouge\">int4</code> AWQ, and <code class=\"language-plaintext highlighter-rouge\">int8</code> SmoothQuant, plus inflight batching and paged KV cache for high throughput. Use TensorRT-LLM when serving NVIDIA-GPU LLMs with low latency at scale.</p>\n  </li>\n  <li>\n    <p><strong>Practical tip:</strong> for <code class=\"language-plaintext highlighter-rouge\">int8</code> LLMs on NVIDIA GPUs, SmoothQuant (W8A8) with per-channel weight and per-tensor activation scales is a common “safe” baseline; for weight-only speedups, AWQ (W4A16) is widely supported.</p>\n  </li>\n</ul>\n<p>TensorRT provides PTQ (with explicit quantization replacing older calibration APIs) and supports QAT. TensorRT-LLM adds LLM-specific kernels and recipes: <code class=\"language-plaintext highlighter-rouge\">float8</code>/<code class=\"language-plaintext highlighter-rouge\">float4</code>, <code class=\"language-plaintext highlighter-rouge\">int4</code> AWQ, and <code class=\"language-plaintext highlighter-rouge\">int8</code> SmoothQuant, plus inflight batching and paged KV cache for high throughput. Use TensorRT-LLM when serving NVIDIA-GPU LLMs with low latency at scale.</p>\n<p><strong>Practical tip:</strong> for <code class=\"language-plaintext highlighter-rouge\">int8</code> LLMs on NVIDIA GPUs, SmoothQuant (W8A8) with per-channel weight and per-tensor activation scales is a common “safe” baseline; for weight-only speedups, AWQ (W4A16) is widely supported.</p>\n<h4 id=\"intel-neural-compressor-inc\">Intel Neural Compressor (INC)</h4>\n<ul>\n  <li>INC is a framework-agnostic toolkit (PyTorch, TensorFlow, ONNX Runtime, MXNet) offering PTQ, QAT, tuning strategies, and hardware-aware search for accuracy/latency/size trade-offs. It’s also the quantization backend used by several Optimum integrations (e.g., Gaudi). Use INC to automate calibration and accuracy recovery across Intel hardware.</li>\n</ul>\n<h4 id=\"openvino-tooling\">OpenVINO Tooling</h4>\n<ul>\n  <li>OpenVINO provides NNCF-based post-training <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization and QAT, strong CPU/iGPU kernels, and file-size/runtime wins for NLP and CV. Workflows include simple PTQ with a small calibration set and hybrid schemes (e.g., MatMul/Embedding weight-only + activation <code class=\"language-plaintext highlighter-rouge\">int8</code> elsewhere). Good match for desktop/server CPUs and integrated GPUs.</li>\n</ul>\n<h4 id=\"tensorflow-lite\">TensorFlow Lite</h4>\n<ul>\n  <li>TFLite supports dynamic range, full-integer (<code class=\"language-plaintext highlighter-rouge\">int8</code>) PTQ, <code class=\"language-plaintext highlighter-rouge\">float16</code> weight compression, and QAT, with a well-specified int8 scheme (two’s complement; per-tensor/axis). Choose when deploying to mobile/embedded via NNAPI/Metal/Vulkan delegates.</li>\n</ul>\n<h4 id=\"apple-core-ml-tools\">Apple Core ML Tools</h4>\n<ul>\n  <li>Core ML Tools supports weight-only linear quantization and palettization (weight clustering into a LUT) with palettization-aware training APIs that insert fake-quant/palette layers during fine-tuning, then fold them into compact weights at export. Use for native iOS/macOS deployments.</li>\n</ul>\n<h4 id=\"llm-specific-quantizers-gptq-awq-and-their-python-toolkits\">LLM-Specific Quantizers (GPTQ, AWQ) and Their Python Toolkits</h4>\n<ul>\n  <li>GPTQ (weight-only, usually 4-bit): popular for LLMs; Optimum exposes a GPTQQuantizer, and Transformers documents a GPTQModel path. Note AutoGPTQ was archived in 2025; newer stacks may use GPTQModel via Transformers/Optimum.</li>\n  <li>AutoAWQ (AWQ implementation, W4A16): easy 4-bit quantization with fast kernels and wide model coverage; integrates with vLLM and various executors.</li>\n</ul>\n<h4 id=\"rules-of-thumb-for-choosing-a-library\">Rules of Thumb for Choosing a Library</h4>\n<ul>\n  <li><strong>If you’re serving NVIDIA-GPU LLMs at scale:</strong> start with TensorRT-LLM (SmoothQuant W8A8, or AWQ W4A16), then tune.</li>\n  <li><strong>If you need portable CPU inference or a single artifact for many devices:</strong> export to ONNX and quantize with ONNX Runtime (static/QDQ).</li>\n  <li><strong>If you want the fastest “drop-in” memory cut for HF models on a single GPU:</strong> bitsandbytes (<code class=\"language-plaintext highlighter-rouge\">LLM.int8</code> or <a href=\"https://youtu.be/TPcXVJ1VSRI?t=563\"><code class=\"language-plaintext highlighter-rouge\">NF4</code></a>) via Transformers.</li>\n  <li><strong>If you target Intel CPUs/iGPUs or want accuracy-constrained auto-tuning:</strong> Optimum Intel with NNCF or bare INC.</li>\n  <li><strong>If you need pure-PyTorch experimentation across devices with minimal graph rewriting:</strong> Optimum Quanto for weight-only quantization (including float8/int4/int2).</li>\n  <li><strong>If you deploy to mobile:</strong> TFLite (Android) or Core ML Tools (iOS/macOS).</li>\n</ul>\n<h4 id=\"implementation-notes\">Implementation Notes</h4>\n<ul>\n  <li><strong>Calibration for static/PTQ:</strong> for ORT/OpenVINO/TensorRT <code class=\"language-plaintext highlighter-rouge\">int8</code>, pass a representative dataset; a few hundred samples often suffice for stable scales.</li>\n  <li><strong>Algebra and mapping:</strong> the linear quantizer in ORT and many toolkits uses <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-107-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi><mo>=</mo><mi>s</mi><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>&amp;#x2212;</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1321\" style=\"width: 6.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1005.73em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1322\"><span class=\"mi\" id=\"MathJax-Span-1323\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1324\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1325\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">s</span><span class=\"mo\" id=\"MathJax-Span-1326\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-1327\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">(</span><span class=\"mi\" id=\"MathJax-Span-1328\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-1329\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-1330\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">z</span><span class=\"mo\" id=\"MathJax-Span-1331\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi><mo>=</mo><mi>s</mi><mo>⋅</mo><mo stretchy=\"false\">(</mo><mi>q</mi><mo>−</mo><mi>z</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-107\">x = s \\cdot (q - z)</script>. Keep <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-108-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi><mo>&amp;gt;</mo><mn>0</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1332\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1333\"><span class=\"mi\" id=\"MathJax-Span-1334\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-1335\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">&gt;</span><span class=\"mn\" id=\"MathJax-Span-1336\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi><mo>&gt;</mo><mn>0</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-108\">s>0</script>, and prefer per-channel scales for weights in matmuls/convs to reduce error.</li>\n  <li><strong>Mixed precision realities:</strong> many “weight-only” schemes still do FP16/BF16 accumulations; end-to-end memory reduction depends on your kernel fusion and runtime (vLLM/TensorRT-LLM/etc.).</li>\n  <li><strong>Library status:</strong> GPTQ tooling has shifted; AutoGPTQ is archived—prefer the GPTQModel path in Transformers/Optimum for forward-compatibility.</li>\n</ul>",
      "contentMarkdown": "*   This section surveys widely used quantization libraries you’ll encounter in practice, what precision/algorithms they support, how they integrate with common stacks, and caveats that matter on-device or in production.\n\n#### BitsAndBytes\n\n*   BitsAndBytes is a CUDA-accelerated library for low-bit matrix multiply and optimizers. It popularized `LLM.int8()`, which uses vector-wise 8-bit quantization but routes “outlier” channels through higher precision matmuls, preserving accuracy while halving memory. It also provides 4-bit weight types used by QLoRA (NF4/`float4`), plus 8-bit optimizers. In Transformers you typically pass a BitsAndBytesConfig (`load_in_4bit` or `load_in_8bit`, `bnb_4bit_quant_type=\"nf4\"` or `\"fp4\"`, `bnb_4bit_use_double_quant`, and `bnb_4bit_compute_dtype` for BF16/FP16 accumulation).\n    \n*   **Typical usage (Transformers):**\n    \n    *   Construct BitsAndBytesConfig with `load_in_4bit=True` and your preferred compute dtype (often BF16 for stability)\n    *   Load the model with `device_map=\"auto\"`\\` to shard to available GPUs/CPU if needed\n    *   When merging adapters back into base weights, dequantize before merging to avoid rounding artifacts, then requantize.\n*   **Notes and caveats:**\n    \n    *   `LLM.int8` is weight-only at matmul inputs; activations remain FP16/BF16, so runtime memory isn’t halved unless your serving stack fuses kernels well.\n    *   Performance varies by kernel/backend (CUDA vs. ROCm); vendor guides show the expected bottlenecks and setup.\n\nBitsAndBytes is a CUDA-accelerated library for low-bit matrix multiply and optimizers. It popularized `LLM.int8()`, which uses vector-wise 8-bit quantization but routes “outlier” channels through higher precision matmuls, preserving accuracy while halving memory. It also provides 4-bit weight types used by QLoRA (NF4/`float4`), plus 8-bit optimizers. In Transformers you typically pass a BitsAndBytesConfig (`load_in_4bit` or `load_in_8bit`, `bnb_4bit_quant_type=\"nf4\"` or `\"fp4\"`, `bnb_4bit_use_double_quant`, and `bnb_4bit_compute_dtype` for BF16/FP16 accumulation).\n\n**Typical usage (Transformers):**\n\n*   Construct BitsAndBytesConfig with `load_in_4bit=True` and your preferred compute dtype (often BF16 for stability)\n*   Load the model with `device_map=\"auto\"`\\` to shard to available GPUs/CPU if needed\n*   When merging adapters back into base weights, dequantize before merging to avoid rounding artifacts, then requantize.\n\n**Notes and caveats:**\n\n*   `LLM.int8` is weight-only at matmul inputs; activations remain FP16/BF16, so runtime memory isn’t halved unless your serving stack fuses kernels well.\n*   Performance varies by kernel/backend (CUDA vs. ROCm); vendor guides show the expected bottlenecks and setup.\n\n#### Hugging Face Optimum\n\n*   Optimum is a meta-tooling layer that wraps several backends and exposes unified quantization APIs.\n    \n    *   **Optimum ONNX Runtime:** PTQ/QAT flows via ORTConfig/ORTQuantizer; supports dynamic (weights int8, activations at runtime), static/QLinear/QDQ with calibration, and QAT. Target for CPU, some NPUs, and cross-platform packaging.\n    *   **Optimum Intel (OpenVINO):** hooks into NNCF for `int8` PTQ and QAT; good CPU/iGPU latency and small memory footprint; provides export to OpenVINO IR and pipeline examples for LLMs.\n    *   **Optimum Quanto:** a PyTorch quantization backend that supports linear quantization of weights to float8, int8, int4, and even int2; works in eager mode, supports QAT, is device-agnostic, and integrates with torch.compile. Great for quick per-module weight-only experiments.\n    *   **Optimum Habana (Gaudi):** integrates Intel Neural Compressor flows to enable `float8`/`uint4` inference/training paths on HPU.\n    *   **Transformers integration summary:** Transformers supports AWQ, GPTQ, and bitsandbytes out-of-the-box; Optimum layers sit on top to add ORT/OpenVINO/Quanto/TensorRT flows.\n\nOptimum is a meta-tooling layer that wraps several backends and exposes unified quantization APIs.\n\n*   **Optimum ONNX Runtime:** PTQ/QAT flows via ORTConfig/ORTQuantizer; supports dynamic (weights int8, activations at runtime), static/QLinear/QDQ with calibration, and QAT. Target for CPU, some NPUs, and cross-platform packaging.\n*   **Optimum Intel (OpenVINO):** hooks into NNCF for `int8` PTQ and QAT; good CPU/iGPU latency and small memory footprint; provides export to OpenVINO IR and pipeline examples for LLMs.\n*   **Optimum Quanto:** a PyTorch quantization backend that supports linear quantization of weights to float8, int8, int4, and even int2; works in eager mode, supports QAT, is device-agnostic, and integrates with torch.compile. Great for quick per-module weight-only experiments.\n*   **Optimum Habana (Gaudi):** integrates Intel Neural Compressor flows to enable `float8`/`uint4` inference/training paths on HPU.\n*   **Transformers integration summary:** Transformers supports AWQ, GPTQ, and bitsandbytes out-of-the-box; Optimum layers sit on top to add ORT/OpenVINO/Quanto/TensorRT flows.\n\n#### ONNX Runtime Quantization\n\n*   ONNX Runtime implements 8-bit linear quantization with scale and zero-point per tensor or per channel, using either static calibration or dynamic quantization. The core mapping follows xfp32\\=s⋅(q−z)xfp32\\=s⋅(q−z)x\\_\\\\mathrm{fp32} = s \\\\cdot (q - z); APIs include `quantize_dynamic`, `quantize_static` (with calibration), and `quantize_qat` (for QAT-authored models).\n    \n*   **Why choose ORT:** stable operator coverage, portable deployment as a single ONNX artifact, and strong CPU performance. Use when you want one model to run across x86/ARM servers and many NPUs with the same runtime.\n    \n\nONNX Runtime implements 8-bit linear quantization with scale and zero-point per tensor or per channel, using either static calibration or dynamic quantization. The core mapping follows xfp32\\=s⋅(q−z)xfp32\\=s⋅(q−z)x\\_\\\\mathrm{fp32} = s \\\\cdot (q - z); APIs include `quantize_dynamic`, `quantize_static` (with calibration), and `quantize_qat` (for QAT-authored models).\n\n**Why choose ORT:** stable operator coverage, portable deployment as a single ONNX artifact, and strong CPU performance. Use when you want one model to run across x86/ARM servers and many NPUs with the same runtime.\n\n#### NVIDIA TensorRT / TensorRT-LLM\n\n*   TensorRT provides PTQ (with explicit quantization replacing older calibration APIs) and supports QAT. TensorRT-LLM adds LLM-specific kernels and recipes: `float8`/`float4`, `int4` AWQ, and `int8` SmoothQuant, plus inflight batching and paged KV cache for high throughput. Use TensorRT-LLM when serving NVIDIA-GPU LLMs with low latency at scale.\n    \n*   **Practical tip:** for `int8` LLMs on NVIDIA GPUs, SmoothQuant (W8A8) with per-channel weight and per-tensor activation scales is a common “safe” baseline; for weight-only speedups, AWQ (W4A16) is widely supported.\n    \n\nTensorRT provides PTQ (with explicit quantization replacing older calibration APIs) and supports QAT. TensorRT-LLM adds LLM-specific kernels and recipes: `float8`/`float4`, `int4` AWQ, and `int8` SmoothQuant, plus inflight batching and paged KV cache for high throughput. Use TensorRT-LLM when serving NVIDIA-GPU LLMs with low latency at scale.\n\n**Practical tip:** for `int8` LLMs on NVIDIA GPUs, SmoothQuant (W8A8) with per-channel weight and per-tensor activation scales is a common “safe” baseline; for weight-only speedups, AWQ (W4A16) is widely supported.\n\n#### Intel Neural Compressor (INC)\n\n*   INC is a framework-agnostic toolkit (PyTorch, TensorFlow, ONNX Runtime, MXNet) offering PTQ, QAT, tuning strategies, and hardware-aware search for accuracy/latency/size trade-offs. It’s also the quantization backend used by several Optimum integrations (e.g., Gaudi). Use INC to automate calibration and accuracy recovery across Intel hardware.\n\n#### OpenVINO Tooling\n\n*   OpenVINO provides NNCF-based post-training `int8` quantization and QAT, strong CPU/iGPU kernels, and file-size/runtime wins for NLP and CV. Workflows include simple PTQ with a small calibration set and hybrid schemes (e.g., MatMul/Embedding weight-only + activation `int8` elsewhere). Good match for desktop/server CPUs and integrated GPUs.\n\n#### TensorFlow Lite\n\n*   TFLite supports dynamic range, full-integer (`int8`) PTQ, `float16` weight compression, and QAT, with a well-specified int8 scheme (two’s complement; per-tensor/axis). Choose when deploying to mobile/embedded via NNAPI/Metal/Vulkan delegates.\n\n#### Apple Core ML Tools\n\n*   Core ML Tools supports weight-only linear quantization and palettization (weight clustering into a LUT) with palettization-aware training APIs that insert fake-quant/palette layers during fine-tuning, then fold them into compact weights at export. Use for native iOS/macOS deployments.\n\n#### LLM-Specific Quantizers (GPTQ, AWQ) and Their Python Toolkits\n\n*   GPTQ (weight-only, usually 4-bit): popular for LLMs; Optimum exposes a GPTQQuantizer, and Transformers documents a GPTQModel path. Note AutoGPTQ was archived in 2025; newer stacks may use GPTQModel via Transformers/Optimum.\n*   AutoAWQ (AWQ implementation, W4A16): easy 4-bit quantization with fast kernels and wide model coverage; integrates with vLLM and various executors.\n\n#### Rules of Thumb for Choosing a Library\n\n*   **If you’re serving NVIDIA-GPU LLMs at scale:** start with TensorRT-LLM (SmoothQuant W8A8, or AWQ W4A16), then tune.\n*   **If you need portable CPU inference or a single artifact for many devices:** export to ONNX and quantize with ONNX Runtime (static/QDQ).\n*   **If you want the fastest “drop-in” memory cut for HF models on a single GPU:** bitsandbytes (`LLM.int8` or [`NF4`](https://youtu.be/TPcXVJ1VSRI?t=563)) via Transformers.\n*   **If you target Intel CPUs/iGPUs or want accuracy-constrained auto-tuning:** Optimum Intel with NNCF or bare INC.\n*   **If you need pure-PyTorch experimentation across devices with minimal graph rewriting:** Optimum Quanto for weight-only quantization (including float8/int4/int2).\n*   **If you deploy to mobile:** TFLite (Android) or Core ML Tools (iOS/macOS).\n\n#### Implementation Notes\n\n*   **Calibration for static/PTQ:** for ORT/OpenVINO/TensorRT `int8`, pass a representative dataset; a few hundred samples often suffice for stable scales.\n*   **Algebra and mapping:** the linear quantizer in ORT and many toolkits uses x\\=s⋅(q−z)x\\=s⋅(q−z)x = s \\\\cdot (q - z). Keep s\\>0s\\>0s>0, and prefer per-channel scales for weights in matmuls/convs to reduce error.\n*   **Mixed precision realities:** many “weight-only” schemes still do FP16/BF16 accumulations; end-to-end memory reduction depends on your kernel fusion and runtime (vLLM/TensorRT-LLM/etc.).\n*   **Library status:** GPTQ tooling has shifted; AutoGPTQ is archived—prefer the GPTQModel path in Transformers/Optimum for forward-compatibility.",
      "order": 19,
      "orderInChapter": 19,
      "difficulty": 4,
      "estimatedMinutes": 8,
      "tags": [
        "ondevice ai",
        "transformer",
        "embedding",
        "gpt",
        "llm",
        "nlp",
        "activation",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 1407,
        "contentLength": 26223
      },
      "nextCards": [
        "ai-model-compression-how-far-can-quantization-be-pushed-20",
        "ai-model-compression-further-reading-21"
      ],
      "relatedCards": [
        "ai-on-device-transformers-modelembedding-dimension-23",
        "ai-on-device-transformers-tokenizer-and-vocabulary-size-22",
        "ai-on-device-transformers-parameter-count-and-model-depth-25",
        "ai-differential-privacy-fine-tuning-with-dp-sgd-12",
        "ai-on-device-transformers-embedding-size-times-vocabulary-size-times-depth-26"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#popular-quantization-libraries",
      "scrapedAt": "2025-12-28T11:55:50.971Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-how-far-can-quantization-be-pushed-20",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "How Far Can Quantization be Pushed?",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>\n    <p>Quantization can, in theory, be reduced to a single bit per parameter, enabling what are known as <strong>binary neural networks (BNNs)</strong>. In such models, both weights and, in some cases, activations are constrained to binary values (e.g., {−1, +1}), achieving the most extreme point on the accuracy–performance trade-off spectrum. Several notable research efforts have explored this concept, including:</p>\n\n    <ul>\n      <li><strong>BinaryConnect</strong> (<a href=\"https://arxiv.org/abs/1511.00363\">Courbariaux et al., 2015</a>) – Introduced the idea of training networks with binary weights while retaining full-precision activations.</li>\n      <li><strong>XNOR-Net</strong> (<a href=\"https://arxiv.org/abs/1603.05279\">Rastegari et al., 2016</a>) – Extended the approach by binarizing both weights and activations, and introducing scaling factors to reduce the accuracy drop while enabling efficient bitwise operations.</li>\n      <li><strong>XNOR-Net++</strong> (<a href=\"https://arxiv.org/abs/1909.13863\">Bulat &amp; Tzimiropoulos, 2019</a>) – Improved upon XNOR-Net through better gradient approximation techniques and optimized binarization strategies, achieving state-of-the-art results among BNNs at the time. An official PyTorch implementation is available for experimentation.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Implementation details</strong>: BNNs replace standard floating-point arithmetic with bitwise operations such as <code class=\"language-plaintext highlighter-rouge\">XNOR</code> and <code class=\"language-plaintext highlighter-rouge\">popcount</code>, which are significantly faster and require far less memory. For example, multiplying two binary vectors can be performed by an <code class=\"language-plaintext highlighter-rouge\">XNOR</code> followed by a population count, reducing both computation cost and storage by up to 32× compared to <code class=\"language-plaintext highlighter-rouge\">float32</code>. Training typically involves:</p>\n\n    <ol>\n      <li>Maintaining a full-precision copy of weights for gradient updates.</li>\n      <li>Applying a binarization function during forward passes (e.g., <code class=\"language-plaintext highlighter-rouge\">sign</code> function).</li>\n      <li>Using a <strong>straight-through estimator (STE)</strong> to approximate gradients through the non-differentiable binarization step.</li>\n      <li>Incorporating scaling factors to better approximate the dynamic range lost in binarization.</li>\n    </ol>\n  </li>\n  <li>\n    <p>Despite the efficiency gains, BNNs often incur substantial accuracy degradation, particularly on complex tasks such as ImageNet classification. Consequently, they are mostly confined to research contexts or highly resource-constrained applications where extreme performance gains justify the accuracy trade-off.</p>\n  </li>\n</ul>\n<p>Quantization can, in theory, be reduced to a single bit per parameter, enabling what are known as <strong>binary neural networks (BNNs)</strong>. In such models, both weights and, in some cases, activations are constrained to binary values (e.g., {−1, +1}), achieving the most extreme point on the accuracy–performance trade-off spectrum. Several notable research efforts have explored this concept, including:</p>\n<ul>\n      <li><strong>BinaryConnect</strong> (<a href=\"https://arxiv.org/abs/1511.00363\">Courbariaux et al., 2015</a>) – Introduced the idea of training networks with binary weights while retaining full-precision activations.</li>\n      <li><strong>XNOR-Net</strong> (<a href=\"https://arxiv.org/abs/1603.05279\">Rastegari et al., 2016</a>) – Extended the approach by binarizing both weights and activations, and introducing scaling factors to reduce the accuracy drop while enabling efficient bitwise operations.</li>\n      <li><strong>XNOR-Net++</strong> (<a href=\"https://arxiv.org/abs/1909.13863\">Bulat &amp; Tzimiropoulos, 2019</a>) – Improved upon XNOR-Net through better gradient approximation techniques and optimized binarization strategies, achieving state-of-the-art results among BNNs at the time. An official PyTorch implementation is available for experimentation.</li>\n    </ul>\n<p><strong>Implementation details</strong>: BNNs replace standard floating-point arithmetic with bitwise operations such as <code class=\"language-plaintext highlighter-rouge\">XNOR</code> and <code class=\"language-plaintext highlighter-rouge\">popcount</code>, which are significantly faster and require far less memory. For example, multiplying two binary vectors can be performed by an <code class=\"language-plaintext highlighter-rouge\">XNOR</code> followed by a population count, reducing both computation cost and storage by up to 32× compared to <code class=\"language-plaintext highlighter-rouge\">float32</code>. Training typically involves:</p>\n<ol>\n      <li>Maintaining a full-precision copy of weights for gradient updates.</li>\n      <li>Applying a binarization function during forward passes (e.g., <code class=\"language-plaintext highlighter-rouge\">sign</code> function).</li>\n      <li>Using a <strong>straight-through estimator (STE)</strong> to approximate gradients through the non-differentiable binarization step.</li>\n      <li>Incorporating scaling factors to better approximate the dynamic range lost in binarization.</li>\n    </ol>\n<p>Despite the efficiency gains, BNNs often incur substantial accuracy degradation, particularly on complex tasks such as ImageNet classification. Consequently, they are mostly confined to research contexts or highly resource-constrained applications where extreme performance gains justify the accuracy trade-off.</p>",
      "contentMarkdown": "*   Quantization can, in theory, be reduced to a single bit per parameter, enabling what are known as **binary neural networks (BNNs)**. In such models, both weights and, in some cases, activations are constrained to binary values (e.g., {−1, +1}), achieving the most extreme point on the accuracy–performance trade-off spectrum. Several notable research efforts have explored this concept, including:\n    \n    *   **BinaryConnect** ([Courbariaux et al., 2015](https://arxiv.org/abs/1511.00363)) – Introduced the idea of training networks with binary weights while retaining full-precision activations.\n    *   **XNOR-Net** ([Rastegari et al., 2016](https://arxiv.org/abs/1603.05279)) – Extended the approach by binarizing both weights and activations, and introducing scaling factors to reduce the accuracy drop while enabling efficient bitwise operations.\n    *   **XNOR-Net++** ([Bulat & Tzimiropoulos, 2019](https://arxiv.org/abs/1909.13863)) – Improved upon XNOR-Net through better gradient approximation techniques and optimized binarization strategies, achieving state-of-the-art results among BNNs at the time. An official PyTorch implementation is available for experimentation.\n*   **Implementation details**: BNNs replace standard floating-point arithmetic with bitwise operations such as `XNOR` and `popcount`, which are significantly faster and require far less memory. For example, multiplying two binary vectors can be performed by an `XNOR` followed by a population count, reducing both computation cost and storage by up to 32× compared to `float32`. Training typically involves:\n    \n    1.  Maintaining a full-precision copy of weights for gradient updates.\n    2.  Applying a binarization function during forward passes (e.g., `sign` function).\n    3.  Using a **straight-through estimator (STE)** to approximate gradients through the non-differentiable binarization step.\n    4.  Incorporating scaling factors to better approximate the dynamic range lost in binarization.\n*   Despite the efficiency gains, BNNs often incur substantial accuracy degradation, particularly on complex tasks such as ImageNet classification. Consequently, they are mostly confined to research contexts or highly resource-constrained applications where extreme performance gains justify the accuracy trade-off.\n    \n\nQuantization can, in theory, be reduced to a single bit per parameter, enabling what are known as **binary neural networks (BNNs)**. In such models, both weights and, in some cases, activations are constrained to binary values (e.g., {−1, +1}), achieving the most extreme point on the accuracy–performance trade-off spectrum. Several notable research efforts have explored this concept, including:\n\n*   **BinaryConnect** ([Courbariaux et al., 2015](https://arxiv.org/abs/1511.00363)) – Introduced the idea of training networks with binary weights while retaining full-precision activations.\n*   **XNOR-Net** ([Rastegari et al., 2016](https://arxiv.org/abs/1603.05279)) – Extended the approach by binarizing both weights and activations, and introducing scaling factors to reduce the accuracy drop while enabling efficient bitwise operations.\n*   **XNOR-Net++** ([Bulat & Tzimiropoulos, 2019](https://arxiv.org/abs/1909.13863)) – Improved upon XNOR-Net through better gradient approximation techniques and optimized binarization strategies, achieving state-of-the-art results among BNNs at the time. An official PyTorch implementation is available for experimentation.\n\n**Implementation details**: BNNs replace standard floating-point arithmetic with bitwise operations such as `XNOR` and `popcount`, which are significantly faster and require far less memory. For example, multiplying two binary vectors can be performed by an `XNOR` followed by a population count, reducing both computation cost and storage by up to 32× compared to `float32`. Training typically involves:\n\n1.  Maintaining a full-precision copy of weights for gradient updates.\n2.  Applying a binarization function during forward passes (e.g., `sign` function).\n3.  Using a **straight-through estimator (STE)** to approximate gradients through the non-differentiable binarization step.\n4.  Incorporating scaling factors to better approximate the dynamic range lost in binarization.\n\nDespite the efficiency gains, BNNs often incur substantial accuracy degradation, particularly on complex tasks such as ImageNet classification. Consequently, they are mostly confined to research contexts or highly resource-constrained applications where extreme performance gains justify the accuracy trade-off.",
      "order": 20,
      "orderInChapter": 20,
      "difficulty": 4,
      "estimatedMinutes": 3,
      "tags": [
        "ondevice ai",
        "neural network",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 575,
        "contentLength": 5641
      },
      "nextCards": [
        "ai-model-compression-further-reading-21",
        "ai-model-compression-mechanism-22"
      ],
      "relatedCards": [
        "ai-federated-learning-personalization-22",
        "ai-differential-privacy-tightness-3",
        "ai-federated-learning-pros-cons-19",
        "ai-federated-learning-comparison-use-cases-20",
        "ai-on-device-transformers-quantization-48-bit-11"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#how-far-can-quantization-be-pushed?",
      "scrapedAt": "2025-12-28T11:55:50.971Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-further-reading-21",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Quantization",
      "title": "Further Reading",
      "subtitle": "Quantization",
      "contentHtml": "<ul>\n  <li>\n    <p><a href=\"https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\">PyTorch official documentation: Introduction to Quantization on PyTorch</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html\">PyTorch official documentation: Advanced Quantization in PyTorch</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://pytorch.org/docs/stable/quantization.html\">PyTorch official documentation: Quantization</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://coremltools.readme.io/docs/quantization\">CoreML Tools documentation: Quantization</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://github.com/bitsandbytes-foundation/bitsandbytes\">bitsandbytes GitHub (official repo)</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://huggingface.co/docs/transformers/en/quantization/bitsandbytes\">Transformers: bitsandbytes quantization guide</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://huggingface.co/docs/transformers/en/main_classes/quantization\">Transformers: Quantization overview (AWQ, GPTQ, bnb)</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://huggingface.co/docs/optimum/en/onnxruntime/usage_guides/quantization\">Hugging Face Optimum: ONNX Runtime quantization</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html\">ONNX Runtime: Quantize ONNX models</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://docs.openvino.ai/2024/openvino-workflow/model-optimization-guide/quantizing-models-post-training.html\">OpenVINO: Post-training quantization (NNCF)</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://github.com/openvinotoolkit/nncf\">NNCF (Neural Network Compression Framework) repository</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://huggingface.co/docs/transformers/en/quantization/quanto\">Hugging Face Optimum Quanto (Transformers guide)</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://huggingface.co/docs/optimum/en/habana/usage_guides/quantization\">Hugging Face Optimum Habana: Quantization</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://docs.nvidia.com/deeplearning/tensorrt/latest/inference-library/work-quantized-types.html\">NVIDIA TensorRT: Working with quantized types (PTQ/QAT)</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://nvidia.github.io/TensorRT-LLM/\">TensorRT-LLM documentation</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://intel.github.io/neural-compressor/latest/docs/source/quantization.html\">Intel Neural Compressor: Quantization docs</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://www.tensorflow.org/model_optimization/guide/quantization/post_training\">TensorFlow Lite: Post-training quantization guide</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://github.com/casper-hansen/AutoAWQ\">AutoAWQ GitHub (AWQ quantization toolkit)</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://docs.vllm.ai/en/v0.9.1/features/quantization/gptqmodel.html\">vLLM: GPTQModel integration docs</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/model-quantization.html\">AMD ROCm: Model quantization (GPTQ and bitsandbytes)</a></p>\n  </li>\n</ul>\n<p><a href=\"https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\">PyTorch official documentation: Introduction to Quantization on PyTorch</a></p>\n<p><a href=\"https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html\">PyTorch official documentation: Advanced Quantization in PyTorch</a></p>\n<p><a href=\"https://pytorch.org/docs/stable/quantization.html\">PyTorch official documentation: Quantization</a></p>\n<p><a href=\"https://coremltools.readme.io/docs/quantization\">CoreML Tools documentation: Quantization</a></p>\n<p><a href=\"https://github.com/bitsandbytes-foundation/bitsandbytes\">bitsandbytes GitHub (official repo)</a></p>\n<p><a href=\"https://huggingface.co/docs/transformers/en/quantization/bitsandbytes\">Transformers: bitsandbytes quantization guide</a></p>\n<p><a href=\"https://huggingface.co/docs/transformers/en/main_classes/quantization\">Transformers: Quantization overview (AWQ, GPTQ, bnb)</a></p>\n<p><a href=\"https://huggingface.co/docs/optimum/en/onnxruntime/usage_guides/quantization\">Hugging Face Optimum: ONNX Runtime quantization</a></p>\n<p><a href=\"https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html\">ONNX Runtime: Quantize ONNX models</a></p>\n<p><a href=\"https://docs.openvino.ai/2024/openvino-workflow/model-optimization-guide/quantizing-models-post-training.html\">OpenVINO: Post-training quantization (NNCF)</a></p>\n<p><a href=\"https://github.com/openvinotoolkit/nncf\">NNCF (Neural Network Compression Framework) repository</a></p>\n<p><a href=\"https://huggingface.co/docs/transformers/en/quantization/quanto\">Hugging Face Optimum Quanto (Transformers guide)</a></p>\n<p><a href=\"https://huggingface.co/docs/optimum/en/habana/usage_guides/quantization\">Hugging Face Optimum Habana: Quantization</a></p>\n<p><a href=\"https://docs.nvidia.com/deeplearning/tensorrt/latest/inference-library/work-quantized-types.html\">NVIDIA TensorRT: Working with quantized types (PTQ/QAT)</a></p>\n<p><a href=\"https://nvidia.github.io/TensorRT-LLM/\">TensorRT-LLM documentation</a></p>\n<p><a href=\"https://intel.github.io/neural-compressor/latest/docs/source/quantization.html\">Intel Neural Compressor: Quantization docs</a></p>\n<p><a href=\"https://www.tensorflow.org/model_optimization/guide/quantization/post_training\">TensorFlow Lite: Post-training quantization guide</a></p>\n<p><a href=\"https://github.com/casper-hansen/AutoAWQ\">AutoAWQ GitHub (AWQ quantization toolkit)</a></p>\n<p><a href=\"https://docs.vllm.ai/en/v0.9.1/features/quantization/gptqmodel.html\">vLLM: GPTQModel integration docs</a></p>\n<p><a href=\"https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/model-quantization.html\">AMD ROCm: Model quantization (GPTQ and bitsandbytes)</a></p>",
      "contentMarkdown": "*   [PyTorch official documentation: Introduction to Quantization on PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)\n    \n*   [PyTorch official documentation: Advanced Quantization in PyTorch](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)\n    \n*   [PyTorch official documentation: Quantization](https://pytorch.org/docs/stable/quantization.html)\n    \n*   [CoreML Tools documentation: Quantization](https://coremltools.readme.io/docs/quantization)\n    \n*   [bitsandbytes GitHub (official repo)](https://github.com/bitsandbytes-foundation/bitsandbytes)\n    \n*   [Transformers: bitsandbytes quantization guide](https://huggingface.co/docs/transformers/en/quantization/bitsandbytes)\n    \n*   [Transformers: Quantization overview (AWQ, GPTQ, bnb)](https://huggingface.co/docs/transformers/en/main_classes/quantization)\n    \n*   [Hugging Face Optimum: ONNX Runtime quantization](https://huggingface.co/docs/optimum/en/onnxruntime/usage_guides/quantization)\n    \n*   [ONNX Runtime: Quantize ONNX models](https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html)\n    \n*   [OpenVINO: Post-training quantization (NNCF)](https://docs.openvino.ai/2024/openvino-workflow/model-optimization-guide/quantizing-models-post-training.html)\n    \n*   [NNCF (Neural Network Compression Framework) repository](https://github.com/openvinotoolkit/nncf)\n    \n*   [Hugging Face Optimum Quanto (Transformers guide)](https://huggingface.co/docs/transformers/en/quantization/quanto)\n    \n*   [Hugging Face Optimum Habana: Quantization](https://huggingface.co/docs/optimum/en/habana/usage_guides/quantization)\n    \n*   [NVIDIA TensorRT: Working with quantized types (PTQ/QAT)](https://docs.nvidia.com/deeplearning/tensorrt/latest/inference-library/work-quantized-types.html)\n    \n*   [TensorRT-LLM documentation](https://nvidia.github.io/TensorRT-LLM/)\n    \n*   [Intel Neural Compressor: Quantization docs](https://intel.github.io/neural-compressor/latest/docs/source/quantization.html)\n    \n*   [TensorFlow Lite: Post-training quantization guide](https://www.tensorflow.org/model_optimization/guide/quantization/post_training)\n    \n*   [AutoAWQ GitHub (AWQ quantization toolkit)](https://github.com/casper-hansen/AutoAWQ)\n    \n*   [vLLM: GPTQModel integration docs](https://docs.vllm.ai/en/v0.9.1/features/quantization/gptqmodel.html)\n    \n*   [AMD ROCm: Model quantization (GPTQ and bitsandbytes)](https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/model-quantization.html)\n    \n\n[PyTorch official documentation: Introduction to Quantization on PyTorch](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)\n\n[PyTorch official documentation: Advanced Quantization in PyTorch](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)\n\n[PyTorch official documentation: Quantization](https://pytorch.org/docs/stable/quantization.html)\n\n[CoreML Tools documentation: Quantization](https://coremltools.readme.io/docs/quantization)\n\n[bitsandbytes GitHub (official repo)](https://github.com/bitsandbytes-foundation/bitsandbytes)\n\n[Transformers: bitsandbytes quantization guide](https://huggingface.co/docs/transformers/en/quantization/bitsandbytes)\n\n[Transformers: Quantization overview (AWQ, GPTQ, bnb)](https://huggingface.co/docs/transformers/en/main_classes/quantization)\n\n[Hugging Face Optimum: ONNX Runtime quantization](https://huggingface.co/docs/optimum/en/onnxruntime/usage_guides/quantization)\n\n[ONNX Runtime: Quantize ONNX models](https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html)\n\n[OpenVINO: Post-training quantization (NNCF)](https://docs.openvino.ai/2024/openvino-workflow/model-optimization-guide/quantizing-models-post-training.html)\n\n[NNCF (Neural Network Compression Framework) repository](https://github.com/openvinotoolkit/nncf)\n\n[Hugging Face Optimum Quanto (Transformers guide)](https://huggingface.co/docs/transformers/en/quantization/quanto)\n\n[Hugging Face Optimum Habana: Quantization](https://huggingface.co/docs/optimum/en/habana/usage_guides/quantization)\n\n[NVIDIA TensorRT: Working with quantized types (PTQ/QAT)](https://docs.nvidia.com/deeplearning/tensorrt/latest/inference-library/work-quantized-types.html)\n\n[TensorRT-LLM documentation](https://nvidia.github.io/TensorRT-LLM/)\n\n[Intel Neural Compressor: Quantization docs](https://intel.github.io/neural-compressor/latest/docs/source/quantization.html)\n\n[TensorFlow Lite: Post-training quantization guide](https://www.tensorflow.org/model_optimization/guide/quantization/post_training)\n\n[AutoAWQ GitHub (AWQ quantization toolkit)](https://github.com/casper-hansen/AutoAWQ)\n\n[vLLM: GPTQModel integration docs](https://docs.vllm.ai/en/v0.9.1/features/quantization/gptqmodel.html)\n\n[AMD ROCm: Model quantization (GPTQ and bitsandbytes)](https://rocm.docs.amd.com/en/docs-6.2.0/how-to/llm-fine-tuning-optimization/model-quantization.html)",
      "order": 21,
      "orderInChapter": 21,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "neural network",
        "transformer",
        "gpt",
        "llm",
        "optimization",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 228,
        "contentLength": 5854
      },
      "nextCards": [
        "ai-model-compression-mechanism-22",
        "ai-model-compression-types-of-knowledge-distillation-23"
      ],
      "relatedCards": [
        "ai-on-device-transformers-embedding-size-times-vocabulary-size-times-depth-26",
        "ai-on-device-transformers-parameter-tuning-recipes-for-ml-runtimes-27",
        "ai-on-device-transformers-sequence-length-and-kv-cache-size-24",
        "ai-on-device-transformers-balancing-compute-and-memory-prefill-vs-decode-opt-8",
        "ai-on-device-transformers-tokenizer-and-vocabulary-size-22"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#further-reading",
      "scrapedAt": "2025-12-28T11:55:50.971Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-mechanism-22",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Knowledge Distillation",
      "title": "Mechanism",
      "subtitle": "Knowledge Distillation",
      "contentHtml": "<ul>\n  <li>\n    <p>In standard supervised learning, models are trained to predict hard, one-hot labels. In contrast, knowledge distillation augments this with <strong>soft targets</strong>—probability distributions produced by the teacher. These targets encode relative likelihoods across all classes, providing richer supervisory signals.</p>\n  </li>\n  <li>\n    <p>The student is trained using a <strong>composite loss function</strong> that combines:</p>\n\n    <ul>\n      <li><strong>Soft target loss</strong>: Kullback-Leibler (KL) divergence between the teacher’s and student’s softened output distributions.</li>\n      <li><strong>Hard target loss</strong>: Standard cross-entropy loss against ground-truth labels.</li>\n    </ul>\n  </li>\n  <li>\n    <p>The loss function is:</p>\n  </li>\n</ul>\n<p>In standard supervised learning, models are trained to predict hard, one-hot labels. In contrast, knowledge distillation augments this with <strong>soft targets</strong>—probability distributions produced by the teacher. These targets encode relative likelihoods across all classes, providing richer supervisory signals.</p>\n<p>The student is trained using a <strong>composite loss function</strong> that combines:</p>\n<ul>\n      <li><strong>Soft target loss</strong>: Kullback-Leibler (KL) divergence between the teacher’s and student’s softened output distributions.</li>\n      <li><strong>Hard target loss</strong>: Standard cross-entropy loss against ground-truth labels.</li>\n    </ul>\n<p>The loss function is:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-109-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>L</mi><mo>=</mo><mi>&amp;#x03B1;</mi><mo>&amp;#x22C5;</mo><msub><mi>L</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>hard</mtext></mrow></msub><mo>+</mo><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><mi>&amp;#x03B1;</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x22C5;</mo><msub><mi>L</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>distill</mtext></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1337\" style=\"width: 14.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.451em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1012.45em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1338\"><span class=\"mi\" id=\"MathJax-Span-1339\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1340\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1341\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">α</span><span class=\"mo\" id=\"MathJax-Span-1342\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1343\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1344\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-1345\"><span class=\"mrow\" id=\"MathJax-Span-1346\"><span class=\"mtext\" id=\"MathJax-Span-1347\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">hard</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1348\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mo\" id=\"MathJax-Span-1349\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">(</span><span class=\"mn\" id=\"MathJax-Span-1350\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-1351\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-1352\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">α</span><span class=\"mo\" id=\"MathJax-Span-1353\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1354\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1355\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1356\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-1357\"><span class=\"mrow\" id=\"MathJax-Span-1358\"><span class=\"mtext\" id=\"MathJax-Span-1359\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">distill</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>L</mi><mo>=</mo><mi>α</mi><mo>⋅</mo><msub><mi>L</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>hard</mtext></mrow></msub><mo>+</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><msub><mi>L</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>distill</mtext></mrow></msub></math></span></span></div>\n<ul>\n  <li>\n    <p>where:</p>\n\n    <ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-110-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>L</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>hard</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1360\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.88em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1361\"><span class=\"msubsup\" id=\"MathJax-Span-1362\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1363\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-1364\"><span class=\"mrow\" id=\"MathJax-Span-1365\"><span class=\"mtext\" id=\"MathJax-Span-1366\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">hard</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>L</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>hard</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-110\">L_{\\text{hard}}</script> is the cross-entropy loss with hard/true labels.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-111-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>L</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>soft</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1367\" style=\"width: 2.034em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.67em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1368\"><span class=\"msubsup\" id=\"MathJax-Span-1369\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1370\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-1371\"><span class=\"mrow\" id=\"MathJax-Span-1372\"><span class=\"mtext\" id=\"MathJax-Span-1373\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">soft</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>L</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>soft</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-111\">L_{\\text{soft}}</script> is the KL divergence between the temperature-scaled softmax outputs:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-112-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>L</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>soft</mtext></mrow></msub><mo>=</mo><mtext>KL</mtext><mrow><mo>(</mo><mrow><mtext>Softmax</mtext><mo stretchy=&quot;false&quot;>(</mo><msub><mi>z</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>teacher</mtext></mrow></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>T</mi><mo stretchy=&quot;false&quot;>)</mo><mspace width=&quot;thinmathspace&quot; /><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mspace width=&quot;thinmathspace&quot; /><mtext>Softmax</mtext><mo stretchy=&quot;false&quot;>(</mo><msub><mi>z</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>student</mtext></mrow></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mi>T</mi><mo stretchy=&quot;false&quot;>)</mo></mrow><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1374\" style=\"width: 24.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 20.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1020.73em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1375\"><span class=\"msubsup\" id=\"MathJax-Span-1376\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1377\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-1378\"><span class=\"mrow\" id=\"MathJax-Span-1379\"><span class=\"mtext\" id=\"MathJax-Span-1380\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">soft</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1381\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mtext\" id=\"MathJax-Span-1382\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">KL</span><span class=\"mrow\" id=\"MathJax-Span-1383\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-1384\" style=\"vertical-align: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-1385\"><span class=\"mtext\" id=\"MathJax-Span-1386\" style=\"font-family: STIXGeneral-Regular;\">Softmax</span><span class=\"mo\" id=\"MathJax-Span-1387\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1388\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1389\" style=\"font-family: STIXGeneral-Italic;\">z</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-1390\"><span class=\"mrow\" id=\"MathJax-Span-1391\"><span class=\"mtext\" id=\"MathJax-Span-1392\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">teacher</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-1393\"><span class=\"mrow\" id=\"MathJax-Span-1394\"><span class=\"mo\" id=\"MathJax-Span-1395\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-1396\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1397\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mspace\" id=\"MathJax-Span-1398\" style=\"height: 0em; vertical-align: 0em; width: 0.211em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-1399\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mspace\" id=\"MathJax-Span-1400\" style=\"height: 0em; vertical-align: 0em; width: 0.211em; display: inline-block; overflow: hidden;\"></span><span class=\"mtext\" id=\"MathJax-Span-1401\" style=\"font-family: STIXGeneral-Regular;\">Softmax</span><span class=\"mo\" id=\"MathJax-Span-1402\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1403\"><span style=\"display: inline-block; position: relative; width: 2.503em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1404\" style=\"font-family: STIXGeneral-Italic;\">z</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-1405\"><span class=\"mrow\" id=\"MathJax-Span-1406\"><span class=\"mtext\" id=\"MathJax-Span-1407\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">student</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-1408\"><span class=\"mrow\" id=\"MathJax-Span-1409\"><span class=\"mo\" id=\"MathJax-Span-1410\" style=\"font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mi\" id=\"MathJax-Span-1411\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1412\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span class=\"mo\" id=\"MathJax-Span-1413\" style=\"vertical-align: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>L</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>soft</mtext></mrow></msub><mo>=</mo><mtext>KL</mtext><mrow><mo>(</mo><mrow><mtext>Softmax</mtext><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>teacher</mtext></mrow></msub><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>T</mi><mo stretchy=\"false\">)</mo><mspace width=\"thinmathspace\"></mspace><mo fence=\"false\" stretchy=\"false\">‖</mo><mspace width=\"thinmathspace\"></mspace><mtext>Softmax</mtext><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>student</mtext></mrow></msub><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mi>T</mi><mo stretchy=\"false\">)</mo></mrow><mo>)</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-112\">L_{\\text{soft}} = \\text{KL} \\left( \\text{Softmax}(z_{\\text{teacher}} / T) \\,\\|\\, \\text{Softmax}(z_{\\text{student}} / T) \\right)</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-113-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1414\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1415\"><span class=\"mi\" id=\"MathJax-Span-1416\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>T</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-113\">T</script> is the temperature parameter that controls the softness of the distribution. A higher <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-114-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1417\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1418\"><span class=\"mi\" id=\"MathJax-Span-1419\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>T</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-114\">T</script> yields softer probabilities, revealing class similarities.</li>\n    </ul>\n  </li>\n  <li>\n    <p>This dual-loss formulation helps the student generalize better by aligning both label fidelity and model semantics.</p>\n  </li>\n</ul>\n<p>where:</p>\n<ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-110-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>L</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>hard</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1360\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.88em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1361\"><span class=\"msubsup\" id=\"MathJax-Span-1362\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1363\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-1364\"><span class=\"mrow\" id=\"MathJax-Span-1365\"><span class=\"mtext\" id=\"MathJax-Span-1366\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">hard</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>L</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>hard</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-110\">L_{\\text{hard}}</script> is the cross-entropy loss with hard/true labels.</li>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-111-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>L</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>soft</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1367\" style=\"width: 2.034em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.67em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1368\"><span class=\"msubsup\" id=\"MathJax-Span-1369\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1370\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-1371\"><span class=\"mrow\" id=\"MathJax-Span-1372\"><span class=\"mtext\" id=\"MathJax-Span-1373\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">soft</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>L</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>soft</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-111\">L_{\\text{soft}}</script> is the KL divergence between the temperature-scaled softmax outputs:</li>\n    </ul>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-113-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1414\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1415\"><span class=\"mi\" id=\"MathJax-Span-1416\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>T</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-113\">T</script> is the temperature parameter that controls the softness of the distribution. A higher <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-114-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>T</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1417\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1418\"><span class=\"mi\" id=\"MathJax-Span-1419\" style=\"font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>T</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-114\">T</script> yields softer probabilities, revealing class similarities.</li>\n    </ul>\n<p>This dual-loss formulation helps the student generalize better by aligning both label fidelity and model semantics.</p>",
      "contentMarkdown": "*   In standard supervised learning, models are trained to predict hard, one-hot labels. In contrast, knowledge distillation augments this with **soft targets**—probability distributions produced by the teacher. These targets encode relative likelihoods across all classes, providing richer supervisory signals.\n    \n*   The student is trained using a **composite loss function** that combines:\n    \n    *   **Soft target loss**: Kullback-Leibler (KL) divergence between the teacher’s and student’s softened output distributions.\n    *   **Hard target loss**: Standard cross-entropy loss against ground-truth labels.\n*   The loss function is:\n    \n\nIn standard supervised learning, models are trained to predict hard, one-hot labels. In contrast, knowledge distillation augments this with **soft targets**—probability distributions produced by the teacher. These targets encode relative likelihoods across all classes, providing richer supervisory signals.\n\nThe student is trained using a **composite loss function** that combines:\n\n*   **Soft target loss**: Kullback-Leibler (KL) divergence between the teacher’s and student’s softened output distributions.\n*   **Hard target loss**: Standard cross-entropy loss against ground-truth labels.\n\nThe loss function is:\n\nL\\=α⋅Lhard+(1−α)⋅LdistillL\\=α⋅Lhard+(1−α)⋅Ldistill\n\n*   where:\n    \n    *   LhardLhardL\\_{\\\\text{hard}} is the cross-entropy loss with hard/true labels.\n    *   LsoftLsoftL\\_{\\\\text{soft}} is the KL divergence between the temperature-scaled softmax outputs:\n    \n    Lsoft\\=KL(Softmax(zteacher/T)‖Softmax(zstudent/T))Lsoft\\=KL(Softmax(zteacher/T)‖Softmax(zstudent/T))\n    \n    L\\_{\\\\text{soft}} = \\\\text{KL} \\\\left( \\\\text{Softmax}(z\\_{\\\\text{teacher}} / T) \\\\,\\\\|\\\\, \\\\text{Softmax}(z\\_{\\\\text{student}} / T) \\\\right)\n    *   where TTT is the temperature parameter that controls the softness of the distribution. A higher TTT yields softer probabilities, revealing class similarities.\n*   This dual-loss formulation helps the student generalize better by aligning both label fidelity and model semantics.\n    \n\nwhere:\n\n*   LhardLhardL\\_{\\\\text{hard}} is the cross-entropy loss with hard/true labels.\n*   LsoftLsoftL\\_{\\\\text{soft}} is the KL divergence between the temperature-scaled softmax outputs:\n\n*   where TTT is the temperature parameter that controls the softness of the distribution. A higher TTT yields softer probabilities, revealing class similarities.\n\nThis dual-loss formulation helps the student generalize better by aligning both label fidelity and model semantics.",
      "order": 22,
      "orderInChapter": 1,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "supervised learning",
        "loss function"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 295,
        "contentLength": 28724
      },
      "nextCards": [
        "ai-model-compression-types-of-knowledge-distillation-23",
        "ai-model-compression-distillation-modes-24"
      ],
      "relatedCards": [
        "ai-federated-learning-definition-2",
        "ai-federated-learning-mime-9",
        "ai-federated-learning-lora-in-federated-context-17",
        "ai-federated-learning-open-challenges-18",
        "ai-federated-learning-legal-benefits-23"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#mechanism",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-types-of-knowledge-distillation-23",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Knowledge Distillation",
      "title": "Types of Knowledge Distillation",
      "subtitle": "Knowledge Distillation",
      "contentHtml": "<h4 id=\"response-based-distillation\">Response-Based Distillation</h4>\n<ul>\n  <li>The most common and classical form.</li>\n  <li>The student is trained to match the final output probabilities of the teacher model.</li>\n  <li>Computationally simple and widely adopted.</li>\n  <li>Used in frameworks like <strong>DistilBERT</strong>, <strong>TinyBERT</strong>, etc.</li>\n</ul>\n<h4 id=\"feature-based-distillation\">Feature-Based Distillation</h4>\n<ul>\n  <li>The student mimics internal hidden states or feature representations of the teacher.</li>\n  <li>Often involves aligning intermediate activations across corresponding layers.</li>\n  <li>Useful in vision tasks where spatial features are important.</li>\n  <li>Examples include <a href=\"https://arxiv.org/abs/1412.6550\">FitNets: Hints for Thin Deep Nets</a> by Romero et al. (2015).</li>\n</ul>\n<h4 id=\"relation-based-distillation\">Relation-Based Distillation</h4>\n<ul>\n  <li>Focuses on matching relationships (e.g., distance, similarity, or angle) between samples in feature space across models.</li>\n  <li>Encourages the student to learn the structural knowledge encoded in the teacher’s representation space.</li>\n  <li>Often used in metric learning and ranking tasks.</li>\n  <li>Example: <a href=\"https://arxiv.org/abs/1904.05068\">Relational Knowledge Distillation</a> by Park et al. (2019).</li>\n</ul>",
      "contentMarkdown": "#### Response-Based Distillation\n\n*   The most common and classical form.\n*   The student is trained to match the final output probabilities of the teacher model.\n*   Computationally simple and widely adopted.\n*   Used in frameworks like **DistilBERT**, **TinyBERT**, etc.\n\n#### Feature-Based Distillation\n\n*   The student mimics internal hidden states or feature representations of the teacher.\n*   Often involves aligning intermediate activations across corresponding layers.\n*   Useful in vision tasks where spatial features are important.\n*   Examples include [FitNets: Hints for Thin Deep Nets](https://arxiv.org/abs/1412.6550) by Romero et al. (2015).\n\n#### Relation-Based Distillation\n\n*   Focuses on matching relationships (e.g., distance, similarity, or angle) between samples in feature space across models.\n*   Encourages the student to learn the structural knowledge encoded in the teacher’s representation space.\n*   Often used in metric learning and ranking tasks.\n*   Example: [Relational Knowledge Distillation](https://arxiv.org/abs/1904.05068) by Park et al. (2019).",
      "order": 23,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "bert",
        "activation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 142,
        "contentLength": 1354
      },
      "nextCards": [
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-why-use-knowledge-distillation-instead-of-training-25"
      ],
      "relatedCards": [
        "ai-federated-learning-pros-cons-19",
        "ai-federated-learning-comparison-use-cases-20",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5",
        "ai-on-device-transformers-quantization-48-bit-11",
        "ai-federated-learning-what-is-federated-lora-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#types-of-knowledge-distillation",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-distillation-modes-24",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Knowledge Distillation",
      "title": "Distillation Modes",
      "subtitle": "Knowledge Distillation",
      "contentHtml": "<h4 id=\"offline-distillation\">Offline Distillation</h4>\n<ul>\n  <li>The teacher is pre-trained and fixed.</li>\n  <li>The student is trained using the frozen teacher’s outputs.</li>\n  <li>This is the most common paradigm in industry.</li>\n</ul>\n<h4 id=\"online-distillation\">Online Distillation</h4>\n<ul>\n  <li>Teacher and student are trained simultaneously.</li>\n  <li>The teacher may itself be evolving (e.g., ensemble of students).</li>\n  <li>Allows for dynamic refinement of knowledge but adds training complexity.</li>\n</ul>\n<h4 id=\"self-distillation\">Self-Distillation</h4>\n<ul>\n  <li>The teacher and student share the same architecture.</li>\n  <li>The teacher is typically an earlier version of the student (e.g., exponential moving average of weights).</li>\n  <li>Demonstrated to improve performance even without model compression.</li>\n</ul>",
      "contentMarkdown": "#### Offline Distillation\n\n*   The teacher is pre-trained and fixed.\n*   The student is trained using the frozen teacher’s outputs.\n*   This is the most common paradigm in industry.\n\n#### Online Distillation\n\n*   Teacher and student are trained simultaneously.\n*   The teacher may itself be evolving (e.g., ensemble of students).\n*   Allows for dynamic refinement of knowledge but adds training complexity.\n\n#### Self-Distillation\n\n*   The teacher and student share the same architecture.\n*   The teacher is typically an earlier version of the student (e.g., exponential moving average of weights).\n*   Demonstrated to improve performance even without model compression.",
      "order": 24,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 98,
        "contentLength": 847
      },
      "nextCards": [
        "ai-model-compression-why-use-knowledge-distillation-instead-of-training-25",
        "ai-model-compression-why-knowledge-distillation-works-26"
      ],
      "relatedCards": [
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-mime-9",
        "ai-federated-learning-tooling-frameworks-14",
        "ai-federated-learning-lora-in-federated-context-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#distillation-modes",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-why-use-knowledge-distillation-instead-of-training-25",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Knowledge Distillation",
      "title": "Why Use Knowledge Distillation Instead of Training Small Models from Scratch?",
      "subtitle": "Knowledge Distillation",
      "contentHtml": "<ol>\n  <li><strong>Richer Supervision leading to Enhanced Generalization</strong>: The soft labels/targets from the teacher offer added supervisory signal by encoding subtle inter-class similarities (e.g., cat vs. tiger) and uncertainties that hard labels miss. This can guide the student to generalize better.</li>\n  <li><strong>Data Augmentation Effect</strong>: The additional information in soft labels effectively augments the supervision signal without needing more data.</li>\n  <li><strong>Performance Boost</strong>: Student models trained via distillation often outperform the same architecture trained directly on hard labels.</li>\n  <li><strong>Compression with Retention</strong>: Distillation enables substantial reduction in model size and latency, with minimal loss in accuracy.</li>\n  <li><strong>Regularization Effect</strong>: Soft labels (and the resulting dense supervision) lead to smoother gradients, which can act as a form of regularization, improving robustness.</li>\n  <li><strong>Data Efficiency</strong>: The student often requires fewer training epochs and can converge with less labeled data.</li>\n  <li><strong>Architecture Agnosticism</strong>: Students need not replicate the teacher’s structure, offering flexibility in design.</li>\n  <li><strong>Latency Reduction</strong>: Distilled students exhibit significant inference speedups, sometimes halving latency.</li>\n</ol>",
      "contentMarkdown": "1.  **Richer Supervision leading to Enhanced Generalization**: The soft labels/targets from the teacher offer added supervisory signal by encoding subtle inter-class similarities (e.g., cat vs. tiger) and uncertainties that hard labels miss. This can guide the student to generalize better.\n2.  **Data Augmentation Effect**: The additional information in soft labels effectively augments the supervision signal without needing more data.\n3.  **Performance Boost**: Student models trained via distillation often outperform the same architecture trained directly on hard labels.\n4.  **Compression with Retention**: Distillation enables substantial reduction in model size and latency, with minimal loss in accuracy.\n5.  **Regularization Effect**: Soft labels (and the resulting dense supervision) lead to smoother gradients, which can act as a form of regularization, improving robustness.\n6.  **Data Efficiency**: The student often requires fewer training epochs and can converge with less labeled data.\n7.  **Architecture Agnosticism**: Students need not replicate the teacher’s structure, offering flexibility in design.\n8.  **Latency Reduction**: Distilled students exhibit significant inference speedups, sometimes halving latency.",
      "order": 25,
      "orderInChapter": 4,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "regularization",
        "data augmentation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 162,
        "contentLength": 1405
      },
      "nextCards": [
        "ai-model-compression-why-knowledge-distillation-works-26",
        "ai-model-compression-distillation-in-practice-27"
      ],
      "relatedCards": [
        "ai-federated-learning-comparative-analysis-of-algorithmic-variants-11",
        "ai-federated-learning-feddyn-10",
        "ai-federated-learning-mime-9",
        "ai-federated-learning-lora-in-federated-context-17",
        "ai-federated-learning-open-challenges-18"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#why-use-knowledge-distillation-instead-of-training-small-models-from-scratch?",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-why-knowledge-distillation-works-26",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Knowledge Distillation",
      "title": "Why Knowledge Distillation Works",
      "subtitle": "Knowledge Distillation",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Soft targets</strong>: Soft targets offer weak but informative signals, guiding the student toward nuanced generalizations. Suppose a large teacher model is trained on CIFAR-100 and has convolutional filters that respond to features like pointy ears. When shown a Batman mask labeled “mask,” the model might still activate its cat filters slightly. This leads to a 0.1 probability for “cat.” Training the student on this soft distribution imparts a weak but useful signal that improves its understanding of cat features.</p>\n  </li>\n  <li>\n    <p><strong>Ensemble effects</strong>: If an ensemble of models each captures different features—say, one detects pointy ears, another whiskers—distillation into a single student helps consolidate these distinct patterns, enhancing generalization.</p>\n  </li>\n  <li>\n    <p><strong>Multiple views and theoretical foundations</strong>: Distillation behaves like weak supervision or multi-view learning. As explored in <a href=\"https://arxiv.org/abs/2012.09816\">Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning</a> by Allen-Zhu et al. (2023), distilled students can approximate ensemble behavior using soft targets.</p>\n  </li>\n</ul>\n<p><strong>Soft targets</strong>: Soft targets offer weak but informative signals, guiding the student toward nuanced generalizations. Suppose a large teacher model is trained on CIFAR-100 and has convolutional filters that respond to features like pointy ears. When shown a Batman mask labeled “mask,” the model might still activate its cat filters slightly. This leads to a 0.1 probability for “cat.” Training the student on this soft distribution imparts a weak but useful signal that improves its understanding of cat features.</p>\n<p><strong>Ensemble effects</strong>: If an ensemble of models each captures different features—say, one detects pointy ears, another whiskers—distillation into a single student helps consolidate these distinct patterns, enhancing generalization.</p>\n<p><strong>Multiple views and theoretical foundations</strong>: Distillation behaves like weak supervision or multi-view learning. As explored in <a href=\"https://arxiv.org/abs/2012.09816\">Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning</a> by Allen-Zhu et al. (2023), distilled students can approximate ensemble behavior using soft targets.</p>",
      "contentMarkdown": "*   **Soft targets**: Soft targets offer weak but informative signals, guiding the student toward nuanced generalizations. Suppose a large teacher model is trained on CIFAR-100 and has convolutional filters that respond to features like pointy ears. When shown a Batman mask labeled “mask,” the model might still activate its cat filters slightly. This leads to a 0.1 probability for “cat.” Training the student on this soft distribution imparts a weak but useful signal that improves its understanding of cat features.\n    \n*   **Ensemble effects**: If an ensemble of models each captures different features—say, one detects pointy ears, another whiskers—distillation into a single student helps consolidate these distinct patterns, enhancing generalization.\n    \n*   **Multiple views and theoretical foundations**: Distillation behaves like weak supervision or multi-view learning. As explored in [Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning](https://arxiv.org/abs/2012.09816) by Allen-Zhu et al. (2023), distilled students can approximate ensemble behavior using soft targets.\n    \n\n**Soft targets**: Soft targets offer weak but informative signals, guiding the student toward nuanced generalizations. Suppose a large teacher model is trained on CIFAR-100 and has convolutional filters that respond to features like pointy ears. When shown a Batman mask labeled “mask,” the model might still activate its cat filters slightly. This leads to a 0.1 probability for “cat.” Training the student on this soft distribution imparts a weak but useful signal that improves its understanding of cat features.\n\n**Ensemble effects**: If an ensemble of models each captures different features—say, one detects pointy ears, another whiskers—distillation into a single student helps consolidate these distinct patterns, enhancing generalization.\n\n**Multiple views and theoretical foundations**: Distillation behaves like weak supervision or multi-view learning. As explored in [Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning](https://arxiv.org/abs/2012.09816) by Allen-Zhu et al. (2023), distilled students can approximate ensemble behavior using soft targets.",
      "order": 26,
      "orderInChapter": 5,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "deep learning",
        "convolution"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 297,
        "contentLength": 2425
      },
      "nextCards": [
        "ai-model-compression-distillation-in-practice-27",
        "ai-model-compression-reverse-distillation-28"
      ],
      "relatedCards": [
        "ai-differential-privacy-pros-15",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-mime-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#why-knowledge-distillation-works",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-distillation-in-practice-27",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Knowledge Distillation",
      "title": "Distillation in Practice",
      "subtitle": "Knowledge Distillation",
      "contentHtml": "<ul>\n  <li>\n    <p>Knowledge distillation intersects with adversarial robustness, privacy preservation, and transfer learning.</p>\n  </li>\n  <li>\n    <p>Most widely used form is response-based distillation, but feature-based and relation-based variants are active research areas.</p>\n  </li>\n  <li>\n    <p>Implementation nuances—teacher-student architecture differences, scheduling, or layer alignment—often require trial and error.</p>\n  </li>\n  <li>\n    <p>In <a href=\"https://arxiv.org/abs/1910.01348\">On the Efficacy of Knowledge Distillation</a>, Cho and Hariharan (2019) found that large teacher models can hurt student performance if there’s a capacity mismatch. Bigger is not always better.</p>\n  </li>\n  <li>\n    <p>In <a href=\"https://arxiv.org/abs/1902.03393\">Improved Knowledge Distillation via Teacher Assistant</a>, Mirzadeh et al. (2019) emphasized that the gap in capacity between teacher and student should be moderate for best results.</p>\n  </li>\n  <li>\n    <p>Thus, a practical takeaway is to perform offline, response-based distillation using a slightly smaller student model for performance gains with minimal tuning.</p>\n  </li>\n  <li>\n    <p>Recent work such as <a href=\"https://arxiv.org/abs/1908.08962\">Well-Read Students Learn Better</a> by Turc et al. (2019) shows that <strong>Pre-trained Distillation (PD)</strong>—pretraining compact models before distillation—yields better results in NLP tasks. The recommended 3-step process is:</p>\n\n    <ol>\n      <li><strong>Pre-train</strong> the compact model (student) on the same masked language modeling (MLM) objective used in BERT.</li>\n      <li><strong>Distill</strong> from a large, task-specific teacher model using response-based offline distillation. For example, if the downstream task is Natural Language Inference (NLI), use the teacher to produce logits for each class (entailment, contradiction, neutral), and minimize KL divergence with the student’s logits.</li>\n      <li><strong>Fine-tune</strong> the distilled student on the task-specific dataset, such as training on the CoNLL 2003 dataset for Named Entity Recognition (NER).</li>\n    </ol>\n\n    <ul>\n      <li>This procedure has the advantage of being <strong>architecture-agnostic</strong>, making it practical for real-world deployment where the student architecture may differ substantially from the teacher.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Pretrained distilled models are now widely accessible. In NLP, libraries such as Hugging Face provide compact and distilled versions like <strong>DistilBERT</strong> and <strong>TinyBERT</strong>, often with task-specific checkpoints. In computer vision, <a href=\"https://github.com/facebookresearch/d2go\">Facebook’s d2go</a> and <a href=\"https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification/\">DeiT</a> offer mobile-ready image classification models that were distilled from larger vision transformers.</p>\n  </li>\n  <li>\n    <p>Practitioners should consider leveraging these pretrained distilled models when seeking lower latency or deployment efficiency, especially when retraining from scratch is resource-prohibitive.</p>\n  </li>\n</ul>\n<p>Knowledge distillation intersects with adversarial robustness, privacy preservation, and transfer learning.</p>\n<p>Most widely used form is response-based distillation, but feature-based and relation-based variants are active research areas.</p>\n<p>Implementation nuances—teacher-student architecture differences, scheduling, or layer alignment—often require trial and error.</p>\n<p>In <a href=\"https://arxiv.org/abs/1910.01348\">On the Efficacy of Knowledge Distillation</a>, Cho and Hariharan (2019) found that large teacher models can hurt student performance if there’s a capacity mismatch. Bigger is not always better.</p>\n<p>In <a href=\"https://arxiv.org/abs/1902.03393\">Improved Knowledge Distillation via Teacher Assistant</a>, Mirzadeh et al. (2019) emphasized that the gap in capacity between teacher and student should be moderate for best results.</p>\n<p>Thus, a practical takeaway is to perform offline, response-based distillation using a slightly smaller student model for performance gains with minimal tuning.</p>\n<p>Recent work such as <a href=\"https://arxiv.org/abs/1908.08962\">Well-Read Students Learn Better</a> by Turc et al. (2019) shows that <strong>Pre-trained Distillation (PD)</strong>—pretraining compact models before distillation—yields better results in NLP tasks. The recommended 3-step process is:</p>\n<ol>\n      <li><strong>Pre-train</strong> the compact model (student) on the same masked language modeling (MLM) objective used in BERT.</li>\n      <li><strong>Distill</strong> from a large, task-specific teacher model using response-based offline distillation. For example, if the downstream task is Natural Language Inference (NLI), use the teacher to produce logits for each class (entailment, contradiction, neutral), and minimize KL divergence with the student’s logits.</li>\n      <li><strong>Fine-tune</strong> the distilled student on the task-specific dataset, such as training on the CoNLL 2003 dataset for Named Entity Recognition (NER).</li>\n    </ol>\n<ul>\n      <li>This procedure has the advantage of being <strong>architecture-agnostic</strong>, making it practical for real-world deployment where the student architecture may differ substantially from the teacher.</li>\n    </ul>\n<p>Pretrained distilled models are now widely accessible. In NLP, libraries such as Hugging Face provide compact and distilled versions like <strong>DistilBERT</strong> and <strong>TinyBERT</strong>, often with task-specific checkpoints. In computer vision, <a href=\"https://github.com/facebookresearch/d2go\">Facebook’s d2go</a> and <a href=\"https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification/\">DeiT</a> offer mobile-ready image classification models that were distilled from larger vision transformers.</p>\n<p>Practitioners should consider leveraging these pretrained distilled models when seeking lower latency or deployment efficiency, especially when retraining from scratch is resource-prohibitive.</p>",
      "contentMarkdown": "*   Knowledge distillation intersects with adversarial robustness, privacy preservation, and transfer learning.\n    \n*   Most widely used form is response-based distillation, but feature-based and relation-based variants are active research areas.\n    \n*   Implementation nuances—teacher-student architecture differences, scheduling, or layer alignment—often require trial and error.\n    \n*   In [On the Efficacy of Knowledge Distillation](https://arxiv.org/abs/1910.01348), Cho and Hariharan (2019) found that large teacher models can hurt student performance if there’s a capacity mismatch. Bigger is not always better.\n    \n*   In [Improved Knowledge Distillation via Teacher Assistant](https://arxiv.org/abs/1902.03393), Mirzadeh et al. (2019) emphasized that the gap in capacity between teacher and student should be moderate for best results.\n    \n*   Thus, a practical takeaway is to perform offline, response-based distillation using a slightly smaller student model for performance gains with minimal tuning.\n    \n*   Recent work such as [Well-Read Students Learn Better](https://arxiv.org/abs/1908.08962) by Turc et al. (2019) shows that **Pre-trained Distillation (PD)**—pretraining compact models before distillation—yields better results in NLP tasks. The recommended 3-step process is:\n    \n    1.  **Pre-train** the compact model (student) on the same masked language modeling (MLM) objective used in BERT.\n    2.  **Distill** from a large, task-specific teacher model using response-based offline distillation. For example, if the downstream task is Natural Language Inference (NLI), use the teacher to produce logits for each class (entailment, contradiction, neutral), and minimize KL divergence with the student’s logits.\n    3.  **Fine-tune** the distilled student on the task-specific dataset, such as training on the CoNLL 2003 dataset for Named Entity Recognition (NER).\n    \n    *   This procedure has the advantage of being **architecture-agnostic**, making it practical for real-world deployment where the student architecture may differ substantially from the teacher.\n*   Pretrained distilled models are now widely accessible. In NLP, libraries such as Hugging Face provide compact and distilled versions like **DistilBERT** and **TinyBERT**, often with task-specific checkpoints. In computer vision, [Facebook’s d2go](https://github.com/facebookresearch/d2go) and [DeiT](https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification/) offer mobile-ready image classification models that were distilled from larger vision transformers.\n    \n*   Practitioners should consider leveraging these pretrained distilled models when seeking lower latency or deployment efficiency, especially when retraining from scratch is resource-prohibitive.\n    \n\nKnowledge distillation intersects with adversarial robustness, privacy preservation, and transfer learning.\n\nMost widely used form is response-based distillation, but feature-based and relation-based variants are active research areas.\n\nImplementation nuances—teacher-student architecture differences, scheduling, or layer alignment—often require trial and error.\n\nIn [On the Efficacy of Knowledge Distillation](https://arxiv.org/abs/1910.01348), Cho and Hariharan (2019) found that large teacher models can hurt student performance if there’s a capacity mismatch. Bigger is not always better.\n\nIn [Improved Knowledge Distillation via Teacher Assistant](https://arxiv.org/abs/1902.03393), Mirzadeh et al. (2019) emphasized that the gap in capacity between teacher and student should be moderate for best results.\n\nThus, a practical takeaway is to perform offline, response-based distillation using a slightly smaller student model for performance gains with minimal tuning.\n\nRecent work such as [Well-Read Students Learn Better](https://arxiv.org/abs/1908.08962) by Turc et al. (2019) shows that **Pre-trained Distillation (PD)**—pretraining compact models before distillation—yields better results in NLP tasks. The recommended 3-step process is:\n\n1.  **Pre-train** the compact model (student) on the same masked language modeling (MLM) objective used in BERT.\n2.  **Distill** from a large, task-specific teacher model using response-based offline distillation. For example, if the downstream task is Natural Language Inference (NLI), use the teacher to produce logits for each class (entailment, contradiction, neutral), and minimize KL divergence with the student’s logits.\n3.  **Fine-tune** the distilled student on the task-specific dataset, such as training on the CoNLL 2003 dataset for Named Entity Recognition (NER).\n\n*   This procedure has the advantage of being **architecture-agnostic**, making it practical for real-world deployment where the student architecture may differ substantially from the teacher.\n\nPretrained distilled models are now widely accessible. In NLP, libraries such as Hugging Face provide compact and distilled versions like **DistilBERT** and **TinyBERT**, often with task-specific checkpoints. In computer vision, [Facebook’s d2go](https://github.com/facebookresearch/d2go) and [DeiT](https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification/) offer mobile-ready image classification models that were distilled from larger vision transformers.\n\nPractitioners should consider leveraging these pretrained distilled models when seeking lower latency or deployment efficiency, especially when retraining from scratch is resource-prohibitive.",
      "order": 27,
      "orderInChapter": 6,
      "difficulty": 3,
      "estimatedMinutes": 4,
      "tags": [
        "ondevice ai",
        "transformer",
        "bert",
        "nlp",
        "computer vision",
        "transfer learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 659,
        "contentLength": 6199
      },
      "nextCards": [
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-weak-supervision-via-distillation-29"
      ],
      "relatedCards": [
        "ai-on-device-transformers-gpu-deployment-considerations-18",
        "ai-federated-learning-transportation-self-driving-cars-24",
        "ai-differential-privacy-membership-inference-protection-13",
        "ai-differential-privacy-pros-15",
        "ai-differential-privacy-cons-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#distillation-in-practice",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-reverse-distillation-28",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Knowledge Distillation",
      "title": "Reverse Distillation",
      "subtitle": "Knowledge Distillation",
      "contentHtml": "<ul>\n  <li>\n    <p>In reverse distillation, a <strong>small model acts as the teacher</strong> and a <strong>larger model is the student</strong>.</p>\n  </li>\n  <li>\n    <p>Particularly useful in noisy datasets (e.g., CTR prediction) where large models overfit easily.</p>\n  </li>\n  <li>\n    <p>In <a href=\"https://arxiv.org/abs/2208.08003\">Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise</a>, Xue et al. (2024) demonstrates this technique in high-label-noise regimes. Particularly, they show that using soft labels from a smaller model can regularize and stabilize large model training under label noise.</p>\n  </li>\n  <li>\n    <p>Process:</p>\n\n    <ol>\n      <li>Train a small, clean model on a curated subset.</li>\n      <li>Use it to produce soft labels for the noisy data.</li>\n      <li>Train the large model using these regularized targets.</li>\n    </ol>\n  </li>\n</ul>\n<p>In reverse distillation, a <strong>small model acts as the teacher</strong> and a <strong>larger model is the student</strong>.</p>\n<p>Particularly useful in noisy datasets (e.g., CTR prediction) where large models overfit easily.</p>\n<p>In <a href=\"https://arxiv.org/abs/2208.08003\">Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise</a>, Xue et al. (2024) demonstrates this technique in high-label-noise regimes. Particularly, they show that using soft labels from a smaller model can regularize and stabilize large model training under label noise.</p>\n<p>Process:</p>\n<ol>\n      <li>Train a small, clean model on a curated subset.</li>\n      <li>Use it to produce soft labels for the noisy data.</li>\n      <li>Train the large model using these regularized targets.</li>\n    </ol>",
      "contentMarkdown": "*   In reverse distillation, a **small model acts as the teacher** and a **larger model is the student**.\n    \n*   Particularly useful in noisy datasets (e.g., CTR prediction) where large models overfit easily.\n    \n*   In [Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise](https://arxiv.org/abs/2208.08003), Xue et al. (2024) demonstrates this technique in high-label-noise regimes. Particularly, they show that using soft labels from a smaller model can regularize and stabilize large model training under label noise.\n    \n*   Process:\n    \n    1.  Train a small, clean model on a curated subset.\n    2.  Use it to produce soft labels for the noisy data.\n    3.  Train the large model using these regularized targets.\n\nIn reverse distillation, a **small model acts as the teacher** and a **larger model is the student**.\n\nParticularly useful in noisy datasets (e.g., CTR prediction) where large models overfit easily.\n\nIn [Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise](https://arxiv.org/abs/2208.08003), Xue et al. (2024) demonstrates this technique in high-label-noise regimes. Particularly, they show that using soft labels from a smaller model can regularize and stabilize large model training under label noise.\n\nProcess:\n\n1.  Train a small, clean model on a curated subset.\n2.  Use it to produce soft labels for the noisy data.\n3.  Train the large model using these regularized targets.",
      "order": 28,
      "orderInChapter": 7,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 220,
        "contentLength": 1759
      },
      "nextCards": [
        "ai-model-compression-weak-supervision-via-distillation-29",
        "ai-model-compression-compute-vs-memory-bottlenecks-30"
      ],
      "relatedCards": [
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-mime-9",
        "ai-federated-learning-tooling-frameworks-14",
        "ai-federated-learning-lora-in-federated-context-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#reverse-distillation",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-weak-supervision-via-distillation-29",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Knowledge Distillation",
      "title": "Weak Supervision Via Distillation",
      "subtitle": "Knowledge Distillation",
      "contentHtml": "<ul>\n  <li>\n    <p>Distillation enables semi-supervised learning. Here’s the procedure:</p>\n\n    <ol>\n      <li>Train a high-capacity teacher on a small labeled set.</li>\n      <li>Use it to label a larger unlabeled dataset.</li>\n      <li>Train a compact student model on the combined data.</li>\n    </ol>\n  </li>\n  <li>\n    <p>This approach has been successfully used in real-world settings such as <a href=\"https://arxiv.org/pdf/1904.01624.pdf\">Lessons from building acoustic models with a million hours of speech</a> by Parthasarathi and Strom (2019), which trained acoustic models with over a million hours of speech data.</p>\n  </li>\n</ul>\n<p>Distillation enables semi-supervised learning. Here’s the procedure:</p>\n<ol>\n      <li>Train a high-capacity teacher on a small labeled set.</li>\n      <li>Use it to label a larger unlabeled dataset.</li>\n      <li>Train a compact student model on the combined data.</li>\n    </ol>\n<p>This approach has been successfully used in real-world settings such as <a href=\"https://arxiv.org/pdf/1904.01624.pdf\">Lessons from building acoustic models with a million hours of speech</a> by Parthasarathi and Strom (2019), which trained acoustic models with over a million hours of speech data.</p>",
      "contentMarkdown": "*   Distillation enables semi-supervised learning. Here’s the procedure:\n    \n    1.  Train a high-capacity teacher on a small labeled set.\n    2.  Use it to label a larger unlabeled dataset.\n    3.  Train a compact student model on the combined data.\n*   This approach has been successfully used in real-world settings such as [Lessons from building acoustic models with a million hours of speech](https://arxiv.org/pdf/1904.01624.pdf) by Parthasarathi and Strom (2019), which trained acoustic models with over a million hours of speech data.\n    \n\nDistillation enables semi-supervised learning. Here’s the procedure:\n\n1.  Train a high-capacity teacher on a small labeled set.\n2.  Use it to label a larger unlabeled dataset.\n3.  Train a compact student model on the combined data.\n\nThis approach has been successfully used in real-world settings such as [Lessons from building acoustic models with a million hours of speech](https://arxiv.org/pdf/1904.01624.pdf) by Parthasarathi and Strom (2019), which trained acoustic models with over a million hours of speech data.",
      "order": 29,
      "orderInChapter": 8,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "supervised learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 152,
        "contentLength": 1237
      },
      "nextCards": [
        "ai-model-compression-compute-vs-memory-bottlenecks-30",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "relatedCards": [
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-mime-9",
        "ai-federated-learning-tooling-frameworks-14",
        "ai-federated-learning-lora-in-federated-context-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#weak-supervision-via-distillation",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-compute-vs-memory-bottlenecks-30",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Knowledge Distillation",
      "title": "Compute vs. Memory Bottlenecks",
      "subtitle": "Knowledge Distillation",
      "contentHtml": "<ul>\n  <li>\n    <p>The latency gains from deploying a distilled student model depend heavily on whether inference in your target environment is <strong>compute-bound</strong> or <strong>memory-bound</strong>.</p>\n\n    <ul>\n      <li>\n        <p><strong>Compute-bound workloads</strong>:</p>\n\n        <ul>\n          <li>For compute-bound workloads, where the runtime is dominated by arithmetic operations (FLOPs), such as large matrix multiplications, convolutions, activation functions, etc., knowledge distillation can yield substantial improvements here because a well-designed student typically has fewer parameters, fewer layers, and reduced hidden dimensions, directly lowering the FLOP count.</li>\n          <li>These savings are realized as long as the hardware can execute the smaller architecture more efficiently without underutilizing compute units.</li>\n          <li>However, on certain GPU architectures optimized for large, dense operations, very small models may not fully saturate the compute pipeline, leading to less-than-expected speedups.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Memory-bound workloads</strong>:</p>\n\n        <ul>\n          <li>For memory-bound workloads, where the runtime is not dominated by arithmetic, but by the cost of moving data (weights, activations) between memory and compute units, distillation can help if the student model’s parameter footprint is significantly smaller than the teacher’s, reducing weight fetches and intermediate activation storage.</li>\n          <li>This is particularly valuable for deployment on edge devices or accelerators with limited memory bandwidth, where the teacher’s size would otherwise bottleneck inference.</li>\n          <li>Gains are more pronounced if the smaller model fits entirely into faster memory tiers (e.g., GPU cache or on-chip SRAM), reducing costly DRAM accesses.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>In practice, many real-world deployments see <strong>mixed bottlenecks</strong>, and the benefits of knowledge distillation are twofold:</p>\n\n    <ol>\n      <li>Lower FLOPs <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-115-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1420\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1421\"><span class=\"mo\" id=\"MathJax-Span-1422\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-115\">\\rightarrow</script> improved compute-bound performance.</li>\n      <li>Smaller parameter and activation footprint <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-116-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1423\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1424\"><span class=\"mo\" id=\"MathJax-Span-1425\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-116\">\\rightarrow</script> improved memory-bound performance.</li>\n    </ol>\n  </li>\n  <li>\n    <p>Importantly, unlike other model compression techniques such as quantization and pruning—which may keep much of the original model’s execution pattern—distillation produces a <em>new, smaller dense architecture</em>, making it easier for standard inference engines and hardware to exploit the reduction in both compute and memory requirements.</p>\n  </li>\n</ul>\n<p>The latency gains from deploying a distilled student model depend heavily on whether inference in your target environment is <strong>compute-bound</strong> or <strong>memory-bound</strong>.</p>\n<ul>\n      <li>\n        <p><strong>Compute-bound workloads</strong>:</p>\n\n        <ul>\n          <li>For compute-bound workloads, where the runtime is dominated by arithmetic operations (FLOPs), such as large matrix multiplications, convolutions, activation functions, etc., knowledge distillation can yield substantial improvements here because a well-designed student typically has fewer parameters, fewer layers, and reduced hidden dimensions, directly lowering the FLOP count.</li>\n          <li>These savings are realized as long as the hardware can execute the smaller architecture more efficiently without underutilizing compute units.</li>\n          <li>However, on certain GPU architectures optimized for large, dense operations, very small models may not fully saturate the compute pipeline, leading to less-than-expected speedups.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Memory-bound workloads</strong>:</p>\n\n        <ul>\n          <li>For memory-bound workloads, where the runtime is not dominated by arithmetic, but by the cost of moving data (weights, activations) between memory and compute units, distillation can help if the student model’s parameter footprint is significantly smaller than the teacher’s, reducing weight fetches and intermediate activation storage.</li>\n          <li>This is particularly valuable for deployment on edge devices or accelerators with limited memory bandwidth, where the teacher’s size would otherwise bottleneck inference.</li>\n          <li>Gains are more pronounced if the smaller model fits entirely into faster memory tiers (e.g., GPU cache or on-chip SRAM), reducing costly DRAM accesses.</li>\n        </ul>\n      </li>\n    </ul>\n<p><strong>Compute-bound workloads</strong>:</p>\n<ul>\n          <li>For compute-bound workloads, where the runtime is dominated by arithmetic operations (FLOPs), such as large matrix multiplications, convolutions, activation functions, etc., knowledge distillation can yield substantial improvements here because a well-designed student typically has fewer parameters, fewer layers, and reduced hidden dimensions, directly lowering the FLOP count.</li>\n          <li>These savings are realized as long as the hardware can execute the smaller architecture more efficiently without underutilizing compute units.</li>\n          <li>However, on certain GPU architectures optimized for large, dense operations, very small models may not fully saturate the compute pipeline, leading to less-than-expected speedups.</li>\n        </ul>\n<p><strong>Memory-bound workloads</strong>:</p>\n<ul>\n          <li>For memory-bound workloads, where the runtime is not dominated by arithmetic, but by the cost of moving data (weights, activations) between memory and compute units, distillation can help if the student model’s parameter footprint is significantly smaller than the teacher’s, reducing weight fetches and intermediate activation storage.</li>\n          <li>This is particularly valuable for deployment on edge devices or accelerators with limited memory bandwidth, where the teacher’s size would otherwise bottleneck inference.</li>\n          <li>Gains are more pronounced if the smaller model fits entirely into faster memory tiers (e.g., GPU cache or on-chip SRAM), reducing costly DRAM accesses.</li>\n        </ul>\n<p>In practice, many real-world deployments see <strong>mixed bottlenecks</strong>, and the benefits of knowledge distillation are twofold:</p>\n<ol>\n      <li>Lower FLOPs <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-115-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1420\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1421\"><span class=\"mo\" id=\"MathJax-Span-1422\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-115\">\\rightarrow</script> improved compute-bound performance.</li>\n      <li>Smaller parameter and activation footprint <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-116-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1423\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1424\"><span class=\"mo\" id=\"MathJax-Span-1425\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-116\">\\rightarrow</script> improved memory-bound performance.</li>\n    </ol>\n<p>Importantly, unlike other model compression techniques such as quantization and pruning—which may keep much of the original model’s execution pattern—distillation produces a <em>new, smaller dense architecture</em>, making it easier for standard inference engines and hardware to exploit the reduction in both compute and memory requirements.</p>",
      "contentMarkdown": "*   The latency gains from deploying a distilled student model depend heavily on whether inference in your target environment is **compute-bound** or **memory-bound**.\n    \n    *   **Compute-bound workloads**:\n        \n        *   For compute-bound workloads, where the runtime is dominated by arithmetic operations (FLOPs), such as large matrix multiplications, convolutions, activation functions, etc., knowledge distillation can yield substantial improvements here because a well-designed student typically has fewer parameters, fewer layers, and reduced hidden dimensions, directly lowering the FLOP count.\n        *   These savings are realized as long as the hardware can execute the smaller architecture more efficiently without underutilizing compute units.\n        *   However, on certain GPU architectures optimized for large, dense operations, very small models may not fully saturate the compute pipeline, leading to less-than-expected speedups.\n    *   **Memory-bound workloads**:\n        \n        *   For memory-bound workloads, where the runtime is not dominated by arithmetic, but by the cost of moving data (weights, activations) between memory and compute units, distillation can help if the student model’s parameter footprint is significantly smaller than the teacher’s, reducing weight fetches and intermediate activation storage.\n        *   This is particularly valuable for deployment on edge devices or accelerators with limited memory bandwidth, where the teacher’s size would otherwise bottleneck inference.\n        *   Gains are more pronounced if the smaller model fits entirely into faster memory tiers (e.g., GPU cache or on-chip SRAM), reducing costly DRAM accesses.\n*   In practice, many real-world deployments see **mixed bottlenecks**, and the benefits of knowledge distillation are twofold:\n    \n    1.  Lower FLOPs →→\\\\rightarrow improved compute-bound performance.\n    2.  Smaller parameter and activation footprint →→\\\\rightarrow improved memory-bound performance.\n*   Importantly, unlike other model compression techniques such as quantization and pruning—which may keep much of the original model’s execution pattern—distillation produces a _new, smaller dense architecture_, making it easier for standard inference engines and hardware to exploit the reduction in both compute and memory requirements.\n    \n\nThe latency gains from deploying a distilled student model depend heavily on whether inference in your target environment is **compute-bound** or **memory-bound**.\n\n*   **Compute-bound workloads**:\n    \n    *   For compute-bound workloads, where the runtime is dominated by arithmetic operations (FLOPs), such as large matrix multiplications, convolutions, activation functions, etc., knowledge distillation can yield substantial improvements here because a well-designed student typically has fewer parameters, fewer layers, and reduced hidden dimensions, directly lowering the FLOP count.\n    *   These savings are realized as long as the hardware can execute the smaller architecture more efficiently without underutilizing compute units.\n    *   However, on certain GPU architectures optimized for large, dense operations, very small models may not fully saturate the compute pipeline, leading to less-than-expected speedups.\n*   **Memory-bound workloads**:\n    \n    *   For memory-bound workloads, where the runtime is not dominated by arithmetic, but by the cost of moving data (weights, activations) between memory and compute units, distillation can help if the student model’s parameter footprint is significantly smaller than the teacher’s, reducing weight fetches and intermediate activation storage.\n    *   This is particularly valuable for deployment on edge devices or accelerators with limited memory bandwidth, where the teacher’s size would otherwise bottleneck inference.\n    *   Gains are more pronounced if the smaller model fits entirely into faster memory tiers (e.g., GPU cache or on-chip SRAM), reducing costly DRAM accesses.\n\n**Compute-bound workloads**:\n\n*   For compute-bound workloads, where the runtime is dominated by arithmetic operations (FLOPs), such as large matrix multiplications, convolutions, activation functions, etc., knowledge distillation can yield substantial improvements here because a well-designed student typically has fewer parameters, fewer layers, and reduced hidden dimensions, directly lowering the FLOP count.\n*   These savings are realized as long as the hardware can execute the smaller architecture more efficiently without underutilizing compute units.\n*   However, on certain GPU architectures optimized for large, dense operations, very small models may not fully saturate the compute pipeline, leading to less-than-expected speedups.\n\n**Memory-bound workloads**:\n\n*   For memory-bound workloads, where the runtime is not dominated by arithmetic, but by the cost of moving data (weights, activations) between memory and compute units, distillation can help if the student model’s parameter footprint is significantly smaller than the teacher’s, reducing weight fetches and intermediate activation storage.\n*   This is particularly valuable for deployment on edge devices or accelerators with limited memory bandwidth, where the teacher’s size would otherwise bottleneck inference.\n*   Gains are more pronounced if the smaller model fits entirely into faster memory tiers (e.g., GPU cache or on-chip SRAM), reducing costly DRAM accesses.\n\nIn practice, many real-world deployments see **mixed bottlenecks**, and the benefits of knowledge distillation are twofold:\n\n1.  Lower FLOPs →→\\\\rightarrow improved compute-bound performance.\n2.  Smaller parameter and activation footprint →→\\\\rightarrow improved memory-bound performance.\n\nImportantly, unlike other model compression techniques such as quantization and pruning—which may keep much of the original model’s execution pattern—distillation produces a _new, smaller dense architecture_, making it easier for standard inference engines and hardware to exploit the reduction in both compute and memory requirements.",
      "order": 30,
      "orderInChapter": 9,
      "difficulty": 4,
      "estimatedMinutes": 4,
      "tags": [
        "ondevice ai",
        "convolution",
        "activation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 792,
        "contentLength": 11827
      },
      "nextCards": [
        "ai-model-compression-limitations-and-challenges-31",
        "ai-model-compression-formal-definition-32"
      ],
      "relatedCards": [
        "ai-federated-learning-pros-cons-19",
        "ai-federated-learning-comparison-use-cases-20",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5",
        "ai-on-device-transformers-quantization-48-bit-11",
        "ai-federated-learning-mime-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#compute-vs.-memory-bottlenecks",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-limitations-and-challenges-31",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Knowledge Distillation",
      "title": "Limitations and Challenges",
      "subtitle": "Knowledge Distillation",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Capacity Gap</strong>: <a href=\"https://arxiv.org/abs/1910.01348\">On the Efficacy of Knowledge Distillation</a> by Cho and Hariharan (2019) demonstrates that extremely large teacher models can be poor mentors if the student’s capacity is too low—it cannot mimic the teacher effectively. Put simply, if the student model is too weak, distillation may degrade performance. Early stopping of the teacher may yield less “over‑fitted” soft labels that are more deliverable to the student.</p>\n  </li>\n  <li>\n    <p><strong>Architecture Mismatch</strong>: Effectiveness can be reduced when the teacher and student architectures differ substantially.</p>\n  </li>\n  <li>\n    <p><strong>Poor transfer on complex tasks</strong>: Results on datasets like ImageNet often trail behind simpler benchmarks unless carefully tuned.</p>\n  </li>\n  <li>\n    <p><strong>Higher tuning cost</strong>: In contrast to quantization and pruning, distillation often requires more experimentation and task-specific adaptation.</p>\n  </li>\n</ul>\n<p><strong>Capacity Gap</strong>: <a href=\"https://arxiv.org/abs/1910.01348\">On the Efficacy of Knowledge Distillation</a> by Cho and Hariharan (2019) demonstrates that extremely large teacher models can be poor mentors if the student’s capacity is too low—it cannot mimic the teacher effectively. Put simply, if the student model is too weak, distillation may degrade performance. Early stopping of the teacher may yield less “over‑fitted” soft labels that are more deliverable to the student.</p>\n<p><strong>Architecture Mismatch</strong>: Effectiveness can be reduced when the teacher and student architectures differ substantially.</p>\n<p><strong>Poor transfer on complex tasks</strong>: Results on datasets like ImageNet often trail behind simpler benchmarks unless carefully tuned.</p>\n<p><strong>Higher tuning cost</strong>: In contrast to quantization and pruning, distillation often requires more experimentation and task-specific adaptation.</p>",
      "contentMarkdown": "*   **Capacity Gap**: [On the Efficacy of Knowledge Distillation](https://arxiv.org/abs/1910.01348) by Cho and Hariharan (2019) demonstrates that extremely large teacher models can be poor mentors if the student’s capacity is too low—it cannot mimic the teacher effectively. Put simply, if the student model is too weak, distillation may degrade performance. Early stopping of the teacher may yield less “over‑fitted” soft labels that are more deliverable to the student.\n    \n*   **Architecture Mismatch**: Effectiveness can be reduced when the teacher and student architectures differ substantially.\n    \n*   **Poor transfer on complex tasks**: Results on datasets like ImageNet often trail behind simpler benchmarks unless carefully tuned.\n    \n*   **Higher tuning cost**: In contrast to quantization and pruning, distillation often requires more experimentation and task-specific adaptation.\n    \n\n**Capacity Gap**: [On the Efficacy of Knowledge Distillation](https://arxiv.org/abs/1910.01348) by Cho and Hariharan (2019) demonstrates that extremely large teacher models can be poor mentors if the student’s capacity is too low—it cannot mimic the teacher effectively. Put simply, if the student model is too weak, distillation may degrade performance. Early stopping of the teacher may yield less “over‑fitted” soft labels that are more deliverable to the student.\n\n**Architecture Mismatch**: Effectiveness can be reduced when the teacher and student architectures differ substantially.\n\n**Poor transfer on complex tasks**: Results on datasets like ImageNet often trail behind simpler benchmarks unless carefully tuned.\n\n**Higher tuning cost**: In contrast to quantization and pruning, distillation often requires more experimentation and task-specific adaptation.",
      "order": 31,
      "orderInChapter": 10,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 234,
        "contentLength": 1998
      },
      "nextCards": [
        "ai-model-compression-formal-definition-32",
        "ai-model-compression-rationale-and-theoretical-motivation-33"
      ],
      "relatedCards": [
        "ai-federated-learning-mime-9",
        "ai-federated-learning-lora-in-federated-context-17",
        "ai-federated-learning-open-challenges-18",
        "ai-federated-learning-legal-benefits-23",
        "ai-differential-privacy-components-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#limitations-and-challenges",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-formal-definition-32",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Model Pruning",
      "title": "Formal Definition",
      "subtitle": "Model Pruning",
      "contentHtml": "<ul>\n  <li>\n    <p>Let <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-117-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>;</mo><mi>&amp;#x03B8;</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1426\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.66em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1427\"><span class=\"mi\" id=\"MathJax-Span-1428\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1429\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1430\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1431\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"mi\" id=\"MathJax-Span-1432\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1433\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo>;</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-117\">f(x; \\theta)</script> be a trained neural network model parameterized by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-118-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B8;</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mi>n</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1434\" style=\"width: 3.596em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.97em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1435\"><span class=\"mi\" id=\"MathJax-Span-1436\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1437\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1438\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1439\"><span class=\"mrow\" id=\"MathJax-Span-1440\"><span class=\"mi\" id=\"MathJax-Span-1441\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-1442\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>θ</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mi>n</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-118\">\\theta \\in \\mathbb{R}^n</script>. Pruning aims to construct a sparsified parameter vector <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-119-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>&amp;#x03B8;</mi><mo>&amp;#x2032;</mo></msup><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mi>n</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1443\" style=\"width: 3.961em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.284em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1003.28em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1444\"><span class=\"msup\" id=\"MathJax-Span-1445\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1446\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-1447\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1448\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1449\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1450\"><span class=\"mrow\" id=\"MathJax-Span-1451\"><span class=\"mi\" id=\"MathJax-Span-1452\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-1453\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>θ</mi><mo>′</mo></msup><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mi>n</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-119\">\\theta' \\in \\mathbb{R}^n</script> such that:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-120-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msubsup><mi>&amp;#x03B8;</mi><mi>i</mi><mo>&amp;#x2032;</mo></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnalign=&quot;left left&quot; rowspacing=&quot;.2em&quot; columnspacing=&quot;1em&quot; displaystyle=&quot;false&quot;><mtr><mtd><mn>0</mn><mo>,</mo></mtd><mtd><mtext>if&amp;#xA0;</mtext><mi>i</mi><mo>&amp;#x2208;</mo><mi>P</mi></mtd></mtr><mtr><mtd><msub><mi>&amp;#x03B8;</mi><mi>i</mi></msub><mo>,</mo></mtd><mtd><mtext>otherwise</mtext></mtd></mtr></mtable><mo fence=&quot;true&quot; stretchy=&quot;true&quot; symmetric=&quot;true&quot;></mo></mrow><mspace width=&quot;1em&quot; /><mtext>where&amp;#xA0;</mtext><mi>P</mi><mo>&amp;#x2282;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>{</mo><mn>1</mn><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><mi>n</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>}</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1454\" style=\"width: 23.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 19.169em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.294em, 1019.07em, 4.846em, -999.997em); top: -3.799em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1455\"><span class=\"msubsup\" id=\"MathJax-Span-1456\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1457\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.32em, 4.169em, -999.997em); top: -4.32em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-1458\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -3.695em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1459\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1460\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mrow\" id=\"MathJax-Span-1461\" style=\"padding-left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-1462\" style=\"vertical-align: -0.466em;\"><span><span style=\"font-size: 111%; font-family: STIXSizeTwoSym;\">{</span></span></span><span class=\"mtable\" id=\"MathJax-Span-1463\" style=\"padding-right: 0.159em; padding-left: 0.159em;\"><span style=\"display: inline-block; position: relative; width: 5.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.607em, 1000.94em, 5.003em, -999.997em); top: -4.008em; left: 0em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.326em, -999.997em); top: -4.581em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-1464\"><span class=\"mrow\" id=\"MathJax-Span-1465\"><span class=\"mn\" id=\"MathJax-Span-1466\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-1467\" style=\"font-family: STIXGeneral-Regular;\">,</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.94em, 4.326em, -999.997em); top: -3.331em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-1474\"><span class=\"mrow\" id=\"MathJax-Span-1475\"><span class=\"msubsup\" id=\"MathJax-Span-1476\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1477\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1478\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1479\" style=\"font-family: STIXGeneral-Regular;\">,</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.607em, 1003.86em, 4.846em, -999.997em); top: -4.008em; left: 2.138em;\"><span style=\"display: inline-block; position: relative; width: 3.909em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1003.08em, 4.221em, -999.997em); top: -4.581em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-1468\"><span class=\"mrow\" id=\"MathJax-Span-1469\"><span class=\"mtext\" id=\"MathJax-Span-1470\" style=\"font-family: STIXGeneral-Regular;\">if&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-1471\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1472\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"mi\" id=\"MathJax-Span-1473\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">P</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.86em, 4.169em, -999.997em); top: -3.331em; left: 0em;\"><span class=\"mtd\" id=\"MathJax-Span-1480\"><span class=\"mrow\" id=\"MathJax-Span-1481\"><span class=\"mtext\" id=\"MathJax-Span-1482\" style=\"font-family: STIXGeneral-Regular;\">otherwise</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1483\"></span></span><span class=\"mspace\" id=\"MathJax-Span-1484\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mtext\" id=\"MathJax-Span-1485\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">where&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-1486\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-1487\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">⊂</span><span class=\"mo\" id=\"MathJax-Span-1488\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">{</span><span class=\"mn\" id=\"MathJax-Span-1489\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-1490\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-1491\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-1492\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"mi\" id=\"MathJax-Span-1493\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">n</span><span class=\"mo\" id=\"MathJax-Span-1494\" style=\"font-family: STIXGeneral-Regular;\">}</span></span><span style=\"display: inline-block; width: 0px; height: 3.805em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 2.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msubsup><mi>θ</mi><mi>i</mi><mo>′</mo></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnalign=\"left left\" rowspacing=\".2em\" columnspacing=\"1em\" displaystyle=\"false\"><mtr><mtd><mn>0</mn><mo>,</mo></mtd><mtd><mtext>if&nbsp;</mtext><mi>i</mi><mo>∈</mo><mi>P</mi></mtd></mtr><mtr><mtd><msub><mi>θ</mi><mi>i</mi></msub><mo>,</mo></mtd><mtd><mtext>otherwise</mtext></mtd></mtr></mtable><mo fence=\"true\" stretchy=\"true\" symmetric=\"true\"></mo></mrow><mspace width=\"1em\"></mspace><mtext>where&nbsp;</mtext><mi>P</mi><mo>⊂</mo><mo fence=\"false\" stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>n</mi><mo fence=\"false\" stretchy=\"false\">}</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-120\">\\theta'_i = \n\\begin{cases}\n0, & \\text{if } i \\in P \\\\\n\\theta_i, & \\text{otherwise}\n\\end{cases}\n\\quad \\text{where } P \\subset \\{1, \\ldots, n\\}</script>\n\n    <ul>\n      <li>and the pruned model <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-121-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>;</mo><msup><mi>&amp;#x03B8;</mi><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1495\" style=\"width: 3.648em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.023em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1002.97em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1496\"><span class=\"mi\" id=\"MathJax-Span-1497\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1498\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1499\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1500\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"msup\" id=\"MathJax-Span-1501\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1502\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-1503\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1504\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo>;</mo><msup><mi>θ</mi><mo>′</mo></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-121\">f(x; \\theta')</script> satisfies:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-122-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>L</mi><mo stretchy=&quot;false&quot;>(</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>;</mo><msup><mi>&amp;#x03B8;</mi><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2248;</mo><mi>L</mi><mo stretchy=&quot;false&quot;>(</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>;</mo><mi>&amp;#x03B8;</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1505\" style=\"width: 11.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.482em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1009.43em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1506\"><span class=\"mi\" id=\"MathJax-Span-1507\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1508\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1509\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1510\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1511\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1512\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"msup\" id=\"MathJax-Span-1513\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1514\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-1515\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1516\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1517\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1518\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mi\" id=\"MathJax-Span-1519\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1520\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1521\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1522\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1523\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1524\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"mi\" id=\"MathJax-Span-1525\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1526\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1527\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>L</mi><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo>;</mo><msup><mi>θ</mi><mo>′</mo></msup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>≈</mo><mi>L</mi><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo>;</mo><mi>θ</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-122\">L(f(x; \\theta')) \\approx L(f(x; \\theta))</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-123-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1528\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1529\"><span class=\"mi\" id=\"MathJax-Span-1530\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-123\">L</script> denotes the loss function over the task of interest.</li>\n    </ul>\n  </li>\n</ul>\n<p>Let <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-117-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>;</mo><mi>&amp;#x03B8;</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1426\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.66em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1427\"><span class=\"mi\" id=\"MathJax-Span-1428\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1429\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1430\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1431\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"mi\" id=\"MathJax-Span-1432\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1433\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo>;</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-117\">f(x; \\theta)</script> be a trained neural network model parameterized by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-118-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B8;</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mi>n</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1434\" style=\"width: 3.596em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.97em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1435\"><span class=\"mi\" id=\"MathJax-Span-1436\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1437\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1438\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1439\"><span class=\"mrow\" id=\"MathJax-Span-1440\"><span class=\"mi\" id=\"MathJax-Span-1441\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-1442\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>θ</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mi>n</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-118\">\\theta \\in \\mathbb{R}^n</script>. Pruning aims to construct a sparsified parameter vector <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-119-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>&amp;#x03B8;</mi><mo>&amp;#x2032;</mo></msup><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mi>n</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1443\" style=\"width: 3.961em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.284em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1003.28em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1444\"><span class=\"msup\" id=\"MathJax-Span-1445\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1446\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-1447\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1448\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1449\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1450\"><span class=\"mrow\" id=\"MathJax-Span-1451\"><span class=\"mi\" id=\"MathJax-Span-1452\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-1453\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>θ</mi><mo>′</mo></msup><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mi>n</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-119\">\\theta' \\in \\mathbb{R}^n</script> such that:</p>\n<ul>\n      <li>and the pruned model <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-121-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>;</mo><msup><mi>&amp;#x03B8;</mi><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1495\" style=\"width: 3.648em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.023em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1002.97em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1496\"><span class=\"mi\" id=\"MathJax-Span-1497\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1498\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1499\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1500\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"msup\" id=\"MathJax-Span-1501\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1502\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-1503\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1504\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo>;</mo><msup><mi>θ</mi><mo>′</mo></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-121\">f(x; \\theta')</script> satisfies:</li>\n    </ul>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-123-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1528\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1529\"><span class=\"mi\" id=\"MathJax-Span-1530\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-123\">L</script> denotes the loss function over the task of interest.</li>\n    </ul>",
      "contentMarkdown": "*   Let f(x;θ)f(x;θ)f(x; \\\\theta) be a trained neural network model parameterized by θ∈ℝnθ∈Rn\\\\theta \\\\in \\\\mathbb{R}^n. Pruning aims to construct a sparsified parameter vector θ′∈ℝnθ′∈Rn\\\\theta' \\\\in \\\\mathbb{R}^n such that:\n    \n    θ′i\\={0,θi,if i∈Potherwisewhere P⊂{1,…,n}θi′\\={0,if i∈Pθi,otherwisewhere P⊂{1,…,n}\n    \n    \\\\theta'\\_i = \\\\begin{cases} 0, & \\\\text{if } i \\\\in P \\\\\\\\ \\\\theta\\_i, & \\\\text{otherwise} \\\\end{cases} \\\\quad \\\\text{where } P \\\\subset \\\\{1, \\\\ldots, n\\\\}\n    \n    *   and the pruned model f(x;θ′)f(x;θ′)f(x; \\\\theta') satisfies:\n    \n    L(f(x;θ′))≈L(f(x;θ))L(f(x;θ′))≈L(f(x;θ))\n    \n    L(f(x; \\\\theta')) \\\\approx L(f(x; \\\\theta))\n    *   where LLL denotes the loss function over the task of interest.\n\nLet f(x;θ)f(x;θ)f(x; \\\\theta) be a trained neural network model parameterized by θ∈ℝnθ∈Rn\\\\theta \\\\in \\\\mathbb{R}^n. Pruning aims to construct a sparsified parameter vector θ′∈ℝnθ′∈Rn\\\\theta' \\\\in \\\\mathbb{R}^n such that:\n\n*   and the pruned model f(x;θ′)f(x;θ′)f(x; \\\\theta') satisfies:\n\n*   where LLL denotes the loss function over the task of interest.",
      "order": 32,
      "orderInChapter": 1,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "neural network",
        "loss function"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 129,
        "contentLength": 38482
      },
      "nextCards": [
        "ai-model-compression-rationale-and-theoretical-motivation-33",
        "ai-model-compression-types-of-pruning-34"
      ],
      "relatedCards": [
        "ai-federated-learning-personalization-22",
        "ai-differential-privacy-tightness-3",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-lora-in-federated-context-17",
        "ai-on-device-transformers-npu-deployment-considerations-edgesoc-devices-20"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#formal-definition",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-rationale-and-theoretical-motivation-33",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Model Pruning",
      "title": "Rationale and Theoretical Motivation",
      "subtitle": "Model Pruning",
      "contentHtml": "<ul>\n  <li>The core insight underpinning pruning is that modern deep networks are typically overparameterized. Empirical studies, such as <a href=\"https://arxiv.org/abs/1506.02626\">Learning both Weights and Connections for Efficient Neural Networks</a> by Han et al. (2015), demonstrate that up to 90% of weights in large-scale models can be pruned with negligible loss in accuracy. The underlying explanation aligns with the <strong>Lottery Ticket Hypothesis</strong> proposed in <a href=\"https://arxiv.org/abs/1803.03635\">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\n</a> by Frankle &amp; Carbin (2019), which posits that:</li>\n</ul>\n<blockquote>\n  <p>“A dense, randomly-initialized neural network contains a subnetwork that, when trained in isolation, can match the performance of the original network.”</p>\n</blockquote>\n<p>“A dense, randomly-initialized neural network contains a subnetwork that, when trained in isolation, can match the performance of the original network.”</p>\n<ul>\n  <li>This hypothesis supports the idea that training a large model enables discovery of a performant sparse subnetwork, which can be extracted via pruning.</li>\n</ul>",
      "contentMarkdown": "*   The core insight underpinning pruning is that modern deep networks are typically overparameterized. Empirical studies, such as [Learning both Weights and Connections for Efficient Neural Networks](https://arxiv.org/abs/1506.02626) by Han et al. (2015), demonstrate that up to 90% of weights in large-scale models can be pruned with negligible loss in accuracy. The underlying explanation aligns with the **Lottery Ticket Hypothesis** proposed in [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635) by Frankle & Carbin (2019), which posits that:\n\n> “A dense, randomly-initialized neural network contains a subnetwork that, when trained in isolation, can match the performance of the original network.”\n\n“A dense, randomly-initialized neural network contains a subnetwork that, when trained in isolation, can match the performance of the original network.”\n\n*   This hypothesis supports the idea that training a large model enables discovery of a performant sparse subnetwork, which can be extracted via pruning.",
      "order": 33,
      "orderInChapter": 2,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "neural network"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 145,
        "contentLength": 1185
      },
      "nextCards": [
        "ai-model-compression-types-of-pruning-34",
        "ai-model-compression-pruning-workflow-35"
      ],
      "relatedCards": [
        "ai-federated-learning-personalization-22",
        "ai-differential-privacy-tightness-3",
        "ai-federated-learning-mime-9",
        "ai-federated-learning-lora-in-federated-context-17",
        "ai-federated-learning-open-challenges-18"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#rationale-and-theoretical-motivation",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-types-of-pruning-34",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Model Pruning",
      "title": "Types of Pruning",
      "subtitle": "Model Pruning",
      "contentHtml": "<h4 id=\"unstructured-pruning\">Unstructured Pruning</h4>\n<ul>\n  <li>\n    <p>Unstructured pruning eliminates individual weights regardless of their position in the weight tensor. It is formally defined by selecting a mask <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-124-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi><mo>&amp;#x2208;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>{</mo><mn>0</mn><mo>,</mo><mn>1</mn><msup><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>}</mo><mi>n</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1531\" style=\"width: 6.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.003em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1005em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1532\"><span class=\"mi\" id=\"MathJax-Span-1533\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1534\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"mo\" id=\"MathJax-Span-1535\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">{</span><span class=\"mn\" id=\"MathJax-Span-1536\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-1537\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-1538\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">1</span><span class=\"msubsup\" id=\"MathJax-Span-1539\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.37em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1540\" style=\"font-family: STIXGeneral-Regular;\">}</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1541\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi><mo>∈</mo><mo fence=\"false\" stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><msup><mo fence=\"false\" stretchy=\"false\">}</mo><mi>n</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-124\">M \\in \\{0, 1\\}^n</script> applied element-wise to the parameter vector <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-125-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B8;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1542\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1543\"><span class=\"mi\" id=\"MathJax-Span-1544\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>θ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-125\">\\theta</script>, resulting in a sparse model:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-126-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>&amp;#x03B8;</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>M</mi><mo>&amp;#x2299;</mo><mi>&amp;#x03B8;</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1545\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1004.85em, 2.503em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1546\"><span class=\"msup\" id=\"MathJax-Span-1547\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1548\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.576em;\"><span class=\"mo\" id=\"MathJax-Span-1549\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1550\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1551\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1552\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⊙</span><span class=\"mi\" id=\"MathJax-Span-1553\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>θ</mi><mo>′</mo></msup><mo>=</mo><mi>M</mi><mo>⊙</mo><mi>θ</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-126\">\\theta' = M \\odot \\theta</script>\n\n    <ul>\n      <li>\n        <p>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-127-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x2299;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1554\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.73em, 2.503em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1555\"><span class=\"mo\" id=\"MathJax-Span-1556\" style=\"font-family: STIXGeneral-Regular;\">⊙</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>⊙</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-127\">\\odot</script> denotes the Hadamard product. Criteria for zeroing elements commonly include:</p>\n\n        <ul>\n          <li>Magnitude-based pruning (e.g., remove smallest | weights |),</li>\n          <li>Gradient-based importance metrics,</li>\n          <li>Second-order methods (e.g., Hessian-based sensitivity).</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>Although unstructured pruning achieves high sparsity, it often provides limited inference acceleration on conventional hardware due to irregular memory access patterns.</p>\n  </li>\n</ul>\n<p>Unstructured pruning eliminates individual weights regardless of their position in the weight tensor. It is formally defined by selecting a mask <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-124-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi><mo>&amp;#x2208;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>{</mo><mn>0</mn><mo>,</mo><mn>1</mn><msup><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>}</mo><mi>n</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1531\" style=\"width: 6.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.003em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1005em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1532\"><span class=\"mi\" id=\"MathJax-Span-1533\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1534\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"mo\" id=\"MathJax-Span-1535\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">{</span><span class=\"mn\" id=\"MathJax-Span-1536\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-1537\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-1538\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">1</span><span class=\"msubsup\" id=\"MathJax-Span-1539\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.37em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1540\" style=\"font-family: STIXGeneral-Regular;\">}</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-1541\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi><mo>∈</mo><mo fence=\"false\" stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><msup><mo fence=\"false\" stretchy=\"false\">}</mo><mi>n</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-124\">M \\in \\{0, 1\\}^n</script> applied element-wise to the parameter vector <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-125-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B8;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1542\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1543\"><span class=\"mi\" id=\"MathJax-Span-1544\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>θ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-125\">\\theta</script>, resulting in a sparse model:</p>\n<ul>\n      <li>\n        <p>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-127-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x2299;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1554\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.73em, 2.503em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1555\"><span class=\"mo\" id=\"MathJax-Span-1556\" style=\"font-family: STIXGeneral-Regular;\">⊙</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>⊙</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-127\">\\odot</script> denotes the Hadamard product. Criteria for zeroing elements commonly include:</p>\n\n        <ul>\n          <li>Magnitude-based pruning (e.g., remove smallest | weights |),</li>\n          <li>Gradient-based importance metrics,</li>\n          <li>Second-order methods (e.g., Hessian-based sensitivity).</li>\n        </ul>\n      </li>\n    </ul>\n<p>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-127-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x2299;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1554\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.73em, 2.503em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1555\"><span class=\"mo\" id=\"MathJax-Span-1556\" style=\"font-family: STIXGeneral-Regular;\">⊙</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.184em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>⊙</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-127\">\\odot</script> denotes the Hadamard product. Criteria for zeroing elements commonly include:</p>\n<ul>\n          <li>Magnitude-based pruning (e.g., remove smallest | weights |),</li>\n          <li>Gradient-based importance metrics,</li>\n          <li>Second-order methods (e.g., Hessian-based sensitivity).</li>\n        </ul>\n<p>Although unstructured pruning achieves high sparsity, it often provides limited inference acceleration on conventional hardware due to irregular memory access patterns.</p>\n<p><strong>Implementation</strong>:</p>\n<ul>\n  <li><strong>PyTorch</strong>: <code class=\"language-plaintext highlighter-rouge\">torch.nn.utils.prune</code> supports various unstructured pruning strategies.</li>\n  <li>\n    <p><strong>TensorFlow</strong>: <code class=\"language-plaintext highlighter-rouge\">tensorflow_model_optimization.sparsity.keras</code> allows weight pruning during training.</p>\n  </li>\n  <li>Example:</li>\n</ul>\n<p><strong>TensorFlow</strong>: <code class=\"language-plaintext highlighter-rouge\">tensorflow_model_optimization.sparsity.keras</code> allows weight pruning during training.</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code15\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code15\"><span class=\"kn\">import</span> <span class=\"nn\">torch.nn.utils.prune</span> <span class=\"k\">as</span> <span class=\"n\">prune</span>\n<span class=\"n\">prune</span><span class=\"p\">.</span><span class=\"n\">l1_unstructured</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'weight'</span><span class=\"p\">,</span> <span class=\"n\">amount</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code15\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code15\"><span class=\"kn\">import</span> <span class=\"nn\">torch.nn.utils.prune</span> <span class=\"k\">as</span> <span class=\"n\">prune</span>\n<span class=\"n\">prune</span><span class=\"p\">.</span><span class=\"n\">l1_unstructured</span><span class=\"p\">(</span><span class=\"n\">module</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'weight'</span><span class=\"p\">,</span> <span class=\"n\">amount</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">)</span>\n</code></pre>\n<h4 id=\"structured-pruning\">Structured Pruning</h4>\n<ul>\n  <li>\n    <p>Structured pruning removes entire structural components of the model, such as:</p>\n\n    <ul>\n      <li><strong>Filters</strong> in convolutional layers,</li>\n      <li><strong>Neurons</strong> in fully-connected layers,</li>\n      <li><strong>Heads</strong> in transformers,</li>\n      <li><strong>Layers</strong> or blocks in residual networks.</li>\n    </ul>\n  </li>\n  <li>\n    <p>This results in a reduced model size and faster inference due to preservation of dense matrix formats.</p>\n  </li>\n  <li>\n    <p>Common importance metrics include:</p>\n\n    <ul>\n      <li>L1/L2 norm of filters, proposed in <a href=\"https://arxiv.org/abs/1506.02626\">Han et al. (2015)</a>,</li>\n      <li>Average activation magnitude,</li>\n      <li>Taylor approximation of loss change,</li>\n      <li>Shapley values.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Implementation</strong>:</p>\n\n    <ul>\n      <li><strong>TorchPruner</strong> (<a href=\"https://github.com/marcoancona/TorchPruner\">repo</a>) automates structured pruning for linear and convolutional layers.</li>\n      <li><strong>Torch-Pruning</strong> (<a href=\"https://github.com/VainF/Torch-Pruning\">repo</a>) provides advanced pruning methods and dependency tracking across layers.</li>\n    </ul>\n  </li>\n</ul>\n<p>Structured pruning removes entire structural components of the model, such as:</p>\n<ul>\n      <li><strong>Filters</strong> in convolutional layers,</li>\n      <li><strong>Neurons</strong> in fully-connected layers,</li>\n      <li><strong>Heads</strong> in transformers,</li>\n      <li><strong>Layers</strong> or blocks in residual networks.</li>\n    </ul>\n<p>This results in a reduced model size and faster inference due to preservation of dense matrix formats.</p>\n<p>Common importance metrics include:</p>\n<ul>\n      <li>L1/L2 norm of filters, proposed in <a href=\"https://arxiv.org/abs/1506.02626\">Han et al. (2015)</a>,</li>\n      <li>Average activation magnitude,</li>\n      <li>Taylor approximation of loss change,</li>\n      <li>Shapley values.</li>\n    </ul>\n<p><strong>Implementation</strong>:</p>\n<ul>\n      <li><strong>TorchPruner</strong> (<a href=\"https://github.com/marcoancona/TorchPruner\">repo</a>) automates structured pruning for linear and convolutional layers.</li>\n      <li><strong>Torch-Pruning</strong> (<a href=\"https://github.com/VainF/Torch-Pruning\">repo</a>) provides advanced pruning methods and dependency tracking across layers.</li>\n    </ul>",
      "contentMarkdown": "#### Unstructured Pruning\n\n*   Unstructured pruning eliminates individual weights regardless of their position in the weight tensor. It is formally defined by selecting a mask M∈{0,1}nM∈{0,1}nM \\\\in \\\\{0, 1\\\\}^n applied element-wise to the parameter vector θθ\\\\theta, resulting in a sparse model:\n    \n    θ′\\=M⊙θθ′\\=M⊙θ\n    \n    \\\\theta' = M \\\\odot \\\\theta\n    *   where ⊙⊙\\\\odot denotes the Hadamard product. Criteria for zeroing elements commonly include:\n        \n        *   Magnitude-based pruning (e.g., remove smallest | weights |),\n        *   Gradient-based importance metrics,\n        *   Second-order methods (e.g., Hessian-based sensitivity).\n*   Although unstructured pruning achieves high sparsity, it often provides limited inference acceleration on conventional hardware due to irregular memory access patterns.\n    \n\nUnstructured pruning eliminates individual weights regardless of their position in the weight tensor. It is formally defined by selecting a mask M∈{0,1}nM∈{0,1}nM \\\\in \\\\{0, 1\\\\}^n applied element-wise to the parameter vector θθ\\\\theta, resulting in a sparse model:\n\n*   where ⊙⊙\\\\odot denotes the Hadamard product. Criteria for zeroing elements commonly include:\n    \n    *   Magnitude-based pruning (e.g., remove smallest | weights |),\n    *   Gradient-based importance metrics,\n    *   Second-order methods (e.g., Hessian-based sensitivity).\n\nwhere ⊙⊙\\\\odot denotes the Hadamard product. Criteria for zeroing elements commonly include:\n\n*   Magnitude-based pruning (e.g., remove smallest | weights |),\n*   Gradient-based importance metrics,\n*   Second-order methods (e.g., Hessian-based sensitivity).\n\nAlthough unstructured pruning achieves high sparsity, it often provides limited inference acceleration on conventional hardware due to irregular memory access patterns.\n\n**Implementation**:\n\n*   **PyTorch**: `torch.nn.utils.prune` supports various unstructured pruning strategies.\n*   **TensorFlow**: `tensorflow_model_optimization.sparsity.keras` allows weight pruning during training.\n    \n*   Example:\n\n**TensorFlow**: `tensorflow_model_optimization.sparsity.keras` allows weight pruning during training.\n\n![](https://aman.ai/images/copy.png)\n\n`import torch.nn.utils.prune as prune prune.l1_unstructured(module, name='weight', amount=0.8)`\n\n![](https://aman.ai/images/copy.png)\n\n`import torch.nn.utils.prune as prune prune.l1_unstructured(module, name='weight', amount=0.8)`\n\n#### Structured Pruning\n\n*   Structured pruning removes entire structural components of the model, such as:\n    \n    *   **Filters** in convolutional layers,\n    *   **Neurons** in fully-connected layers,\n    *   **Heads** in transformers,\n    *   **Layers** or blocks in residual networks.\n*   This results in a reduced model size and faster inference due to preservation of dense matrix formats.\n    \n*   Common importance metrics include:\n    \n    *   L1/L2 norm of filters, proposed in [Han et al. (2015)](https://arxiv.org/abs/1506.02626),\n    *   Average activation magnitude,\n    *   Taylor approximation of loss change,\n    *   Shapley values.\n*   **Implementation**:\n    \n    *   **TorchPruner** ([repo](https://github.com/marcoancona/TorchPruner)) automates structured pruning for linear and convolutional layers.\n    *   **Torch-Pruning** ([repo](https://github.com/VainF/Torch-Pruning)) provides advanced pruning methods and dependency tracking across layers.\n\nStructured pruning removes entire structural components of the model, such as:\n\n*   **Filters** in convolutional layers,\n*   **Neurons** in fully-connected layers,\n*   **Heads** in transformers,\n*   **Layers** or blocks in residual networks.\n\nThis results in a reduced model size and faster inference due to preservation of dense matrix formats.\n\nCommon importance metrics include:\n\n*   L1/L2 norm of filters, proposed in [Han et al. (2015)](https://arxiv.org/abs/1506.02626),\n*   Average activation magnitude,\n*   Taylor approximation of loss change,\n*   Shapley values.\n\n**Implementation**:\n\n*   **TorchPruner** ([repo](https://github.com/marcoancona/TorchPruner)) automates structured pruning for linear and convolutional layers.\n*   **Torch-Pruning** ([repo](https://github.com/VainF/Torch-Pruning)) provides advanced pruning methods and dependency tracking across layers.",
      "order": 34,
      "orderInChapter": 3,
      "difficulty": 5,
      "estimatedMinutes": 3,
      "tags": [
        "ondevice ai",
        "transformer",
        "convolution",
        "optimization",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": true,
        "wordCount": 473,
        "contentLength": 21475
      },
      "nextCards": [
        "ai-model-compression-pruning-workflow-35",
        "ai-model-compression-compute-vs-memory-bottlenecks-36"
      ],
      "relatedCards": [
        "ai-on-device-transformers-balancing-compute-and-memory-prefill-vs-decode-opt-8",
        "ai-federated-learning-scaffold-8",
        "ai-on-device-transformers-hardware-specific-optimization-notes-16",
        "ai-on-device-transformers-gpu-deployment-considerations-18",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#types-of-pruning",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-pruning-workflow-35",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Model Pruning",
      "title": "Pruning Workflow",
      "subtitle": "Model Pruning",
      "contentHtml": "<ul>\n  <li>A typical pruning pipeline consists of the following stages:</li>\n</ul>\n<h4 id=\"step-1-train-the-full-model\">Step 1: Train the Full Model</h4>\n<ul>\n  <li>Train the original model to convergence using standard procedures.</li>\n</ul>\n<h4 id=\"step-2-apply-pruning-mask\">Step 2: Apply Pruning Mask</h4>\n<ul>\n  <li>\n    <p>Determine and apply pruning masks using one of the supported strategies. This can occur:</p>\n\n    <ul>\n      <li><strong>Post-training</strong>: Prune after the model is fully trained.</li>\n      <li><strong>During training</strong>: Gradually prune weights over several epochs.</li>\n    </ul>\n  </li>\n</ul>\n<p>Determine and apply pruning masks using one of the supported strategies. This can occur:</p>\n<ul>\n      <li><strong>Post-training</strong>: Prune after the model is fully trained.</li>\n      <li><strong>During training</strong>: Gradually prune weights over several epochs.</li>\n    </ul>\n<h4 id=\"step-3-fine-tune-the-pruned-model\">Step 3: Fine-Tune the Pruned Model</h4>\n<ul>\n  <li>\n    <p>Retrain the pruned model to recover lost accuracy. The most effective method is <strong>learning rate rewinding</strong>, where:</p>\n\n    <ul>\n      <li>Training is resumed from an earlier weight checkpoint.</li>\n      <li>The learning rate is reset to a higher value <a href=\"https://arxiv.org/abs/2003.02389\">Comparing Rewinding and Fine-tuning in Neural Network Pruning</a> by Renda et al. (2020).</li>\n    </ul>\n  </li>\n  <li>\n    <p>Alternatively, <strong>weight rewinding</strong> may be used to reset weights of surviving parameters to their earlier values (e.g., 1/3 of training completed).</p>\n  </li>\n</ul>\n<p>Retrain the pruned model to recover lost accuracy. The most effective method is <strong>learning rate rewinding</strong>, where:</p>\n<ul>\n      <li>Training is resumed from an earlier weight checkpoint.</li>\n      <li>The learning rate is reset to a higher value <a href=\"https://arxiv.org/abs/2003.02389\">Comparing Rewinding and Fine-tuning in Neural Network Pruning</a> by Renda et al. (2020).</li>\n    </ul>\n<p>Alternatively, <strong>weight rewinding</strong> may be used to reset weights of surviving parameters to their earlier values (e.g., 1/3 of training completed).</p>",
      "contentMarkdown": "*   A typical pruning pipeline consists of the following stages:\n\n#### Step 1: Train the Full Model\n\n*   Train the original model to convergence using standard procedures.\n\n#### Step 2: Apply Pruning Mask\n\n*   Determine and apply pruning masks using one of the supported strategies. This can occur:\n    \n    *   **Post-training**: Prune after the model is fully trained.\n    *   **During training**: Gradually prune weights over several epochs.\n\nDetermine and apply pruning masks using one of the supported strategies. This can occur:\n\n*   **Post-training**: Prune after the model is fully trained.\n*   **During training**: Gradually prune weights over several epochs.\n\n#### Step 3: Fine-Tune the Pruned Model\n\n*   Retrain the pruned model to recover lost accuracy. The most effective method is **learning rate rewinding**, where:\n    \n    *   Training is resumed from an earlier weight checkpoint.\n    *   The learning rate is reset to a higher value [Comparing Rewinding and Fine-tuning in Neural Network Pruning](https://arxiv.org/abs/2003.02389) by Renda et al. (2020).\n*   Alternatively, **weight rewinding** may be used to reset weights of surviving parameters to their earlier values (e.g., 1/3 of training completed).\n    \n\nRetrain the pruned model to recover lost accuracy. The most effective method is **learning rate rewinding**, where:\n\n*   Training is resumed from an earlier weight checkpoint.\n*   The learning rate is reset to a higher value [Comparing Rewinding and Fine-tuning in Neural Network Pruning](https://arxiv.org/abs/2003.02389) by Renda et al. (2020).\n\nAlternatively, **weight rewinding** may be used to reset weights of surviving parameters to their earlier values (e.g., 1/3 of training completed).",
      "order": 35,
      "orderInChapter": 4,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "neural network",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 247,
        "contentLength": 2229
      },
      "nextCards": [
        "ai-model-compression-compute-vs-memory-bottlenecks-36",
        "ai-model-compression-practical-considerations-37"
      ],
      "relatedCards": [
        "ai-federated-learning-personalization-22",
        "ai-differential-privacy-tightness-3",
        "ai-federated-learning-llm-specific-enhancements-15",
        "ai-federated-learning-pros-cons-19",
        "ai-federated-learning-comparison-use-cases-20"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#pruning-workflow",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-compute-vs-memory-bottlenecks-36",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Model Pruning",
      "title": "Compute vs. Memory Bottlenecks",
      "subtitle": "Model Pruning",
      "contentHtml": "<ul>\n  <li>\n    <p>Whether pruning improves inference latency depends strongly on the <strong>primary performance bottleneck</strong> of the workload:</p>\n\n    <p><strong>Compute-bound workloads</strong>:</p>\n\n    <ul>\n      <li>For compute-bound workloads, where the runtime is dominated by arithmetic operations (FLOPs), such as large matrix multiplications, convolutions, activation functions, etc., reducing model size only helps if pruning directly lowers the number of executed operations <em>and</em> the hardware can exploit the new shape efficiently.</li>\n      <li>For example, moderate structured pruning may still yield negligible gains on GPUs optimized for dense matrix multiplication unless the reduction crosses hardware-friendly dimensions that change kernel execution patterns (e.g., multiples of warp sizes or tensor core tile sizes).</li>\n      <li>Unstructured pruning typically does not lower FLOP count in standard dense kernels, so compute-bound latency may remain unchanged.</li>\n    </ul>\n\n    <p><strong>Memory-bound workloads</strong>:</p>\n\n    <ul>\n      <li>\n        <ul>\n          <li>For memory-bound workloads, where the runtime is not dominated by arithmetic, but by the cost of moving data (weights, activations) between memory and compute units, pruning can help by reducing the total parameter and activation footprint. This reduces the volume of data transferred and ultimately leads to fewer memory accesses and potentially higher throughput—especially when weights and activations no longer exceed cache capacities.</li>\n        </ul>\n      </li>\n      <li>This benefit is more pronounced in unstructured pruning or extreme structured pruning, where parameter reduction meaningfully shrinks memory traffic though this often requires specialized hardware/software support.</li>\n    </ul>\n  </li>\n  <li>\n    <p>In summary, if the system is compute-bound, the time saved by transferring fewer parameters is negligible because the main delay is in processing operations, not in moving data. Moreover, unstructured pruning may not reduce computational cost on dense-optimized hardware without sparse acceleration kernels, meaning FLOP counts—and thus latency—stay similar. Structured pruning fares better but still depends heavily on whether the hardware kernels adapt efficiently to the new tensor shapes.</p>\n  </li>\n</ul>\n<p>Whether pruning improves inference latency depends strongly on the <strong>primary performance bottleneck</strong> of the workload:</p>\n<p><strong>Compute-bound workloads</strong>:</p>\n<ul>\n      <li>For compute-bound workloads, where the runtime is dominated by arithmetic operations (FLOPs), such as large matrix multiplications, convolutions, activation functions, etc., reducing model size only helps if pruning directly lowers the number of executed operations <em>and</em> the hardware can exploit the new shape efficiently.</li>\n      <li>For example, moderate structured pruning may still yield negligible gains on GPUs optimized for dense matrix multiplication unless the reduction crosses hardware-friendly dimensions that change kernel execution patterns (e.g., multiples of warp sizes or tensor core tile sizes).</li>\n      <li>Unstructured pruning typically does not lower FLOP count in standard dense kernels, so compute-bound latency may remain unchanged.</li>\n    </ul>\n<p><strong>Memory-bound workloads</strong>:</p>\n<ul>\n      <li>\n        <ul>\n          <li>For memory-bound workloads, where the runtime is not dominated by arithmetic, but by the cost of moving data (weights, activations) between memory and compute units, pruning can help by reducing the total parameter and activation footprint. This reduces the volume of data transferred and ultimately leads to fewer memory accesses and potentially higher throughput—especially when weights and activations no longer exceed cache capacities.</li>\n        </ul>\n      </li>\n      <li>This benefit is more pronounced in unstructured pruning or extreme structured pruning, where parameter reduction meaningfully shrinks memory traffic though this often requires specialized hardware/software support.</li>\n    </ul>\n<ul>\n          <li>For memory-bound workloads, where the runtime is not dominated by arithmetic, but by the cost of moving data (weights, activations) between memory and compute units, pruning can help by reducing the total parameter and activation footprint. This reduces the volume of data transferred and ultimately leads to fewer memory accesses and potentially higher throughput—especially when weights and activations no longer exceed cache capacities.</li>\n        </ul>\n<p>In summary, if the system is compute-bound, the time saved by transferring fewer parameters is negligible because the main delay is in processing operations, not in moving data. Moreover, unstructured pruning may not reduce computational cost on dense-optimized hardware without sparse acceleration kernels, meaning FLOP counts—and thus latency—stay similar. Structured pruning fares better but still depends heavily on whether the hardware kernels adapt efficiently to the new tensor shapes.</p>",
      "contentMarkdown": "*   Whether pruning improves inference latency depends strongly on the **primary performance bottleneck** of the workload:\n    \n    **Compute-bound workloads**:\n    \n    *   For compute-bound workloads, where the runtime is dominated by arithmetic operations (FLOPs), such as large matrix multiplications, convolutions, activation functions, etc., reducing model size only helps if pruning directly lowers the number of executed operations _and_ the hardware can exploit the new shape efficiently.\n    *   For example, moderate structured pruning may still yield negligible gains on GPUs optimized for dense matrix multiplication unless the reduction crosses hardware-friendly dimensions that change kernel execution patterns (e.g., multiples of warp sizes or tensor core tile sizes).\n    *   Unstructured pruning typically does not lower FLOP count in standard dense kernels, so compute-bound latency may remain unchanged.\n    \n    **Memory-bound workloads**:\n    \n    *   *   For memory-bound workloads, where the runtime is not dominated by arithmetic, but by the cost of moving data (weights, activations) between memory and compute units, pruning can help by reducing the total parameter and activation footprint. This reduces the volume of data transferred and ultimately leads to fewer memory accesses and potentially higher throughput—especially when weights and activations no longer exceed cache capacities.\n    *   This benefit is more pronounced in unstructured pruning or extreme structured pruning, where parameter reduction meaningfully shrinks memory traffic though this often requires specialized hardware/software support.\n*   In summary, if the system is compute-bound, the time saved by transferring fewer parameters is negligible because the main delay is in processing operations, not in moving data. Moreover, unstructured pruning may not reduce computational cost on dense-optimized hardware without sparse acceleration kernels, meaning FLOP counts—and thus latency—stay similar. Structured pruning fares better but still depends heavily on whether the hardware kernels adapt efficiently to the new tensor shapes.\n    \n\nWhether pruning improves inference latency depends strongly on the **primary performance bottleneck** of the workload:\n\n**Compute-bound workloads**:\n\n*   For compute-bound workloads, where the runtime is dominated by arithmetic operations (FLOPs), such as large matrix multiplications, convolutions, activation functions, etc., reducing model size only helps if pruning directly lowers the number of executed operations _and_ the hardware can exploit the new shape efficiently.\n*   For example, moderate structured pruning may still yield negligible gains on GPUs optimized for dense matrix multiplication unless the reduction crosses hardware-friendly dimensions that change kernel execution patterns (e.g., multiples of warp sizes or tensor core tile sizes).\n*   Unstructured pruning typically does not lower FLOP count in standard dense kernels, so compute-bound latency may remain unchanged.\n\n**Memory-bound workloads**:\n\n*   *   For memory-bound workloads, where the runtime is not dominated by arithmetic, but by the cost of moving data (weights, activations) between memory and compute units, pruning can help by reducing the total parameter and activation footprint. This reduces the volume of data transferred and ultimately leads to fewer memory accesses and potentially higher throughput—especially when weights and activations no longer exceed cache capacities.\n*   This benefit is more pronounced in unstructured pruning or extreme structured pruning, where parameter reduction meaningfully shrinks memory traffic though this often requires specialized hardware/software support.\n\n*   For memory-bound workloads, where the runtime is not dominated by arithmetic, but by the cost of moving data (weights, activations) between memory and compute units, pruning can help by reducing the total parameter and activation footprint. This reduces the volume of data transferred and ultimately leads to fewer memory accesses and potentially higher throughput—especially when weights and activations no longer exceed cache capacities.\n\nIn summary, if the system is compute-bound, the time saved by transferring fewer parameters is negligible because the main delay is in processing operations, not in moving data. Moreover, unstructured pruning may not reduce computational cost on dense-optimized hardware without sparse acceleration kernels, meaning FLOP counts—and thus latency—stay similar. Structured pruning fares better but still depends heavily on whether the hardware kernels adapt efficiently to the new tensor shapes.",
      "order": 36,
      "orderInChapter": 5,
      "difficulty": 4,
      "estimatedMinutes": 4,
      "tags": [
        "ondevice ai",
        "convolution",
        "activation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 632,
        "contentLength": 5128
      },
      "nextCards": [
        "ai-model-compression-practical-considerations-37",
        "ai-model-compression-comparative-analysis-38"
      ],
      "relatedCards": [
        "ai-federated-learning-pros-cons-19",
        "ai-federated-learning-comparison-use-cases-20",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5",
        "ai-on-device-transformers-quantization-48-bit-11",
        "ai-federated-learning-mime-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#compute-vs.-memory-bottlenecks",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-practical-considerations-37",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Model Pruning",
      "title": "Practical Considerations",
      "subtitle": "Model Pruning",
      "contentHtml": "<h4 id=\"target-sparsity\">Target Sparsity</h4>\n<ul>\n  <li>Target sparsity (e.g., 80%) must be tuned experimentally. Aggressive sparsity often requires multiple pruning–fine-tuning cycles.</li>\n</ul>\n<h4 id=\"compatibility\">Compatibility</h4>\n<ul>\n  <li>\n    <p>Pruning can be difficult for architectures with:</p>\n\n    <ul>\n      <li><strong>Skip connections</strong> (e.g., ResNets),</li>\n      <li><strong>Attention modules</strong> with tight dimensional constraints.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Custom pruning logic may be required.</p>\n  </li>\n</ul>\n<p>Pruning can be difficult for architectures with:</p>\n<ul>\n      <li><strong>Skip connections</strong> (e.g., ResNets),</li>\n      <li><strong>Attention modules</strong> with tight dimensional constraints.</li>\n    </ul>\n<p>Custom pruning logic may be required.</p>\n<h4 id=\"deployment-readiness\">Deployment Readiness</h4>\n<ul>\n  <li>Sparse inference is not universally supported. Quantization-aware or hardware-specific pruning (e.g., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-128-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mo>:</mo><mi>M</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1557\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.61em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1558\"><span class=\"mi\" id=\"MathJax-Span-1559\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1560\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">:</span><span class=\"mi\" id=\"MathJax-Span-1561\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi><mo>:</mo><mi>M</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-128\">N:M</script> sparsity) may be necessary for real-world acceleration. As noted in <a href=\"https://arxiv.org/abs/2003.03033\">What is the State of Neural Network Pruning?</a> by Blalock et al. (2020), real benefits often depend on framework and hardware support.</li>\n</ul>",
      "contentMarkdown": "#### Target Sparsity\n\n*   Target sparsity (e.g., 80%) must be tuned experimentally. Aggressive sparsity often requires multiple pruning–fine-tuning cycles.\n\n#### Compatibility\n\n*   Pruning can be difficult for architectures with:\n    \n    *   **Skip connections** (e.g., ResNets),\n    *   **Attention modules** with tight dimensional constraints.\n*   Custom pruning logic may be required.\n    \n\nPruning can be difficult for architectures with:\n\n*   **Skip connections** (e.g., ResNets),\n*   **Attention modules** with tight dimensional constraints.\n\nCustom pruning logic may be required.\n\n#### Deployment Readiness\n\n*   Sparse inference is not universally supported. Quantization-aware or hardware-specific pruning (e.g., N:MN:MN:M sparsity) may be necessary for real-world acceleration. As noted in [What is the State of Neural Network Pruning?](https://arxiv.org/abs/2003.03033) by Blalock et al. (2020), real benefits often depend on framework and hardware support.",
      "order": 37,
      "orderInChapter": 6,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "neural network",
        "attention",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 121,
        "contentLength": 2891
      },
      "nextCards": [
        "ai-model-compression-comparative-analysis-38",
        "ai-model-compression-implementing-pruning-in-pytorch-and-tensorflow-39"
      ],
      "relatedCards": [
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-federated-learning-pros-cons-19",
        "ai-on-device-transformers-cpu-deployment-considerations-17",
        "ai-federated-learning-llm-specific-enhancements-15",
        "ai-federated-learning-personalization-22"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#practical-considerations",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-comparative-analysis-38",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Model Pruning",
      "title": "Comparative Analysis",
      "subtitle": "Model Pruning",
      "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Aspect</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Unstructured Pruning</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Structured Pruning</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Targets</td>\n<td class=\"tg-tleft-valign-first\">Individual weights</td>\n<td class=\"tg-tleft-valign-second\">Filters, neurons, heads, layers</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Benefits</td>\n<td class=\"tg-tleft-valign-first\">Compression</td>\n<td class=\"tg-tleft-valign-second\">Compression + inference acceleration</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Implementation Ease</td>\n<td class=\"tg-tleft-valign-first\">High</td>\n<td class=\"tg-tleft-valign-second\">Moderate to low</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Framework Support</td>\n<td class=\"tg-tleft-valign-first\">TensorFlow, PyTorch</td>\n<td class=\"tg-tleft-valign-second\">TorchPruner, Torch-Pruning</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Inference Speedup</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-second\">Significant</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Aspect</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Unstructured Pruning</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Structured Pruning</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Targets</td>\n<td class=\"tg-tleft-valign-first\">Individual weights</td>\n<td class=\"tg-tleft-valign-second\">Filters, neurons, heads, layers</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Benefits</td>\n<td class=\"tg-tleft-valign-first\">Compression</td>\n<td class=\"tg-tleft-valign-second\">Compression + inference acceleration</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Implementation Ease</td>\n<td class=\"tg-tleft-valign-first\">High</td>\n<td class=\"tg-tleft-valign-second\">Moderate to low</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Framework Support</td>\n<td class=\"tg-tleft-valign-first\">TensorFlow, PyTorch</td>\n<td class=\"tg-tleft-valign-second\">TorchPruner, Torch-Pruning</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Inference Speedup</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-second\">Significant</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "**Aspect**\n\n**Unstructured Pruning**\n\n**Structured Pruning**\n\nTargets\n\nIndividual weights\n\nFilters, neurons, heads, layers\n\nBenefits\n\nCompression\n\nCompression + inference acceleration\n\nImplementation Ease\n\nHigh\n\nModerate to low\n\nFramework Support\n\nTensorFlow, PyTorch\n\nTorchPruner, Torch-Pruning\n\nInference Speedup\n\nLimited\n\nSignificant\n\n**Aspect**\n\n**Unstructured Pruning**\n\n**Structured Pruning**\n\nTargets\n\nIndividual weights\n\nFilters, neurons, heads, layers\n\nBenefits\n\nCompression\n\nCompression + inference acceleration\n\nImplementation Ease\n\nHigh\n\nModerate to low\n\nFramework Support\n\nTensorFlow, PyTorch\n\nTorchPruner, Torch-Pruning\n\nInference Speedup\n\nLimited\n\nSignificant",
      "order": 38,
      "orderInChapter": 7,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 68,
        "contentLength": 2429
      },
      "nextCards": [
        "ai-model-compression-implementing-pruning-in-pytorch-and-tensorflow-39",
        "ai-model-compression-overview-40"
      ],
      "relatedCards": [
        "ai-federated-learning-mime-9",
        "ai-federated-learning-lora-in-federated-context-17",
        "ai-federated-learning-open-challenges-18",
        "ai-federated-learning-legal-benefits-23",
        "ai-differential-privacy-components-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#comparative-analysis",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-implementing-pruning-in-pytorch-and-tensorflow-39",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Model Pruning",
      "title": "Implementing Pruning in PyTorch and TensorFlow",
      "subtitle": "Model Pruning",
      "contentHtml": "<ul>\n  <li>Modern deep learning frameworks provide built-in utilities to simplify pruning workflows. Below, we describe practical approaches in both PyTorch and TensorFlow.</li>\n</ul>\n<h4 id=\"pytorch-pruning\">PyTorch Pruning</h4>\n<ul>\n  <li>\n    <p>PyTorch offers flexible pruning utilities via the <code class=\"language-plaintext highlighter-rouge\">torch.nn.utils.prune</code> module. This supports both unstructured and structured pruning.</p>\n  </li>\n  <li>\n    <p><strong>Unstructured Pruning Example (L1-based weight pruning):</strong></p>\n  </li>\n</ul>\n<p>PyTorch offers flexible pruning utilities via the <code class=\"language-plaintext highlighter-rouge\">torch.nn.utils.prune</code> module. This supports both unstructured and structured pruning.</p>\n<p><strong>Unstructured Pruning Example (L1-based weight pruning):</strong></p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code16\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code16\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.utils.prune</span> <span class=\"k\">as</span> <span class=\"n\">prune</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"n\">nn</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n    <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">),</span>\n    <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>\n    <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Prune 50% of weights in the first Linear layer\n</span><span class=\"n\">prune</span><span class=\"p\">.</span><span class=\"n\">l1_unstructured</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'weight'</span><span class=\"p\">,</span> <span class=\"n\">amount</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># To remove the pruning reparameterization and finalize pruning\n</span><span class=\"n\">prune</span><span class=\"p\">.</span><span class=\"n\">remove</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s\">'weight'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code16\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code16\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.utils.prune</span> <span class=\"k\">as</span> <span class=\"n\">prune</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"n\">nn</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Sequential</span><span class=\"p\">(</span>\n    <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">),</span>\n    <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">ReLU</span><span class=\"p\">(),</span>\n    <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Prune 50% of weights in the first Linear layer\n</span><span class=\"n\">prune</span><span class=\"p\">.</span><span class=\"n\">l1_unstructured</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'weight'</span><span class=\"p\">,</span> <span class=\"n\">amount</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># To remove the pruning reparameterization and finalize pruning\n</span><span class=\"n\">prune</span><span class=\"p\">.</span><span class=\"n\">remove</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s\">'weight'</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li><strong>Structured Pruning Example (entire neuron/channel pruning):</strong></li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code17\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code17\"><span class=\"c1\"># Prune 30% of channels (columns) in the second Linear layer\n</span><span class=\"n\">prune</span><span class=\"p\">.</span><span class=\"n\">ln_structured</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'weight'</span><span class=\"p\">,</span> <span class=\"n\">amount</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">prune</span><span class=\"p\">.</span><span class=\"n\">remove</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"s\">'weight'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code17\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code17\"><span class=\"c1\"># Prune 30% of channels (columns) in the second Linear layer\n</span><span class=\"n\">prune</span><span class=\"p\">.</span><span class=\"n\">ln_structured</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">'weight'</span><span class=\"p\">,</span> <span class=\"n\">amount</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">prune</span><span class=\"p\">.</span><span class=\"n\">remove</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"s\">'weight'</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>\n    <p><strong>PyTorch Pruning Resources:</strong></p>\n\n    <ul>\n      <li><a href=\"https://pytorch.org/tutorials/intermediate/pruning_tutorial.html\">PyTorch pruning tutorial</a></li>\n      <li><a href=\"https://github.com/VainF/Torch-Pruning\">Torch-Pruning library</a> – advanced dependency-aware structured pruning</li>\n      <li><a href=\"https://github.com/marcoancona/TorchPruner\">TorchPruner</a> – structured pruning with visual feedback</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>PyTorch Pruning Resources:</strong></p>\n<ul>\n      <li><a href=\"https://pytorch.org/tutorials/intermediate/pruning_tutorial.html\">PyTorch pruning tutorial</a></li>\n      <li><a href=\"https://github.com/VainF/Torch-Pruning\">Torch-Pruning library</a> – advanced dependency-aware structured pruning</li>\n      <li><a href=\"https://github.com/marcoancona/TorchPruner\">TorchPruner</a> – structured pruning with visual feedback</li>\n    </ul>\n<h4 id=\"tensorflow-pruning\">TensorFlow Pruning</h4>\n<ul>\n  <li>\n    <p>TensorFlow provides pruning support via the <code class=\"language-plaintext highlighter-rouge\">tensorflow_model_optimization</code> toolkit. It supports sparsity-aware training by gradually zeroing out weights.</p>\n  </li>\n  <li>\n    <p><strong>Basic Workflow:</strong></p>\n  </li>\n</ul>\n<p>TensorFlow provides pruning support via the <code class=\"language-plaintext highlighter-rouge\">tensorflow_model_optimization</code> toolkit. It supports sparsity-aware training by gradually zeroing out weights.</p>\n<p><strong>Basic Workflow:</strong></p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code18\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code18\"><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow_model_optimization</span> <span class=\"k\">as</span> <span class=\"n\">tfmot</span>\n\n<span class=\"n\">prune_low_magnitude</span> <span class=\"o\">=</span> <span class=\"n\">tfmot</span><span class=\"p\">.</span><span class=\"n\">sparsity</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">prune_low_magnitude</span>\n\n<span class=\"c1\"># Define prunable model\n</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">Sequential</span><span class=\"p\">([</span>\n    <span class=\"n\">prune_low_magnitude</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">),</span> \n                        <span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">,)),</span>\n    <span class=\"n\">prune_low_magnitude</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"p\">])</span>\n\n<span class=\"c1\"># Compile and train\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s\">'adam'</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s\">'sparse_categorical_crossentropy'</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Strip pruning wrappers before saving/export\n</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">tfmot</span><span class=\"p\">.</span><span class=\"n\">sparsity</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">strip_pruning</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code18\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code18\"><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow_model_optimization</span> <span class=\"k\">as</span> <span class=\"n\">tfmot</span>\n\n<span class=\"n\">prune_low_magnitude</span> <span class=\"o\">=</span> <span class=\"n\">tfmot</span><span class=\"p\">.</span><span class=\"n\">sparsity</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">prune_low_magnitude</span>\n\n<span class=\"c1\"># Define prunable model\n</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">Sequential</span><span class=\"p\">([</span>\n    <span class=\"n\">prune_low_magnitude</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">),</span> \n                        <span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">,)),</span>\n    <span class=\"n\">prune_low_magnitude</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"p\">])</span>\n\n<span class=\"c1\"># Compile and train\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s\">'adam'</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s\">'sparse_categorical_crossentropy'</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Strip pruning wrappers before saving/export\n</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">tfmot</span><span class=\"p\">.</span><span class=\"n\">sparsity</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">strip_pruning</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li><strong>Set Pruning Schedule:</strong></li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code19\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code19\"><span class=\"n\">pruning_schedule</span> <span class=\"o\">=</span> <span class=\"n\">tfmot</span><span class=\"p\">.</span><span class=\"n\">sparsity</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">PolynomialDecay</span><span class=\"p\">(</span>\n    <span class=\"n\">initial_sparsity</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">,</span>\n    <span class=\"n\">final_sparsity</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">,</span>\n    <span class=\"n\">begin_step</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">end_step</span><span class=\"o\">=</span><span class=\"mi\">1000</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code19\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code19\"><span class=\"n\">pruning_schedule</span> <span class=\"o\">=</span> <span class=\"n\">tfmot</span><span class=\"p\">.</span><span class=\"n\">sparsity</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">PolynomialDecay</span><span class=\"p\">(</span>\n    <span class=\"n\">initial_sparsity</span><span class=\"o\">=</span><span class=\"mf\">0.0</span><span class=\"p\">,</span>\n    <span class=\"n\">final_sparsity</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">,</span>\n    <span class=\"n\">begin_step</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">end_step</span><span class=\"o\">=</span><span class=\"mi\">1000</span>\n<span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>\n    <p><strong>TensorFlow Pruning Resources:</strong></p>\n\n    <ul>\n      <li><a href=\"https://www.tensorflow.org/model_optimization/guide/pruning\">TensorFlow Model Optimization Guide</a></li>\n      <li>Supports only unstructured pruning during training</li>\n      <li>Can export TFLite-compatible sparse models for edge inference</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>TensorFlow Pruning Resources:</strong></p>\n<ul>\n      <li><a href=\"https://www.tensorflow.org/model_optimization/guide/pruning\">TensorFlow Model Optimization Guide</a></li>\n      <li>Supports only unstructured pruning during training</li>\n      <li>Can export TFLite-compatible sparse models for edge inference</li>\n    </ul>",
      "contentMarkdown": "*   Modern deep learning frameworks provide built-in utilities to simplify pruning workflows. Below, we describe practical approaches in both PyTorch and TensorFlow.\n\n#### PyTorch Pruning\n\n*   PyTorch offers flexible pruning utilities via the `torch.nn.utils.prune` module. This supports both unstructured and structured pruning.\n    \n*   **Unstructured Pruning Example (L1-based weight pruning):**\n    \n\nPyTorch offers flexible pruning utilities via the `torch.nn.utils.prune` module. This supports both unstructured and structured pruning.\n\n**Unstructured Pruning Example (L1-based weight pruning):**\n\n![](https://aman.ai/images/copy.png)\n\n`import torch import torch.nn.utils.prune as prune import torch.nn as nn  model = nn.Sequential(     nn.Linear(512, 256),     nn.ReLU(),     nn.Linear(256, 10) )  # Prune 50% of weights in the first Linear layer prune.l1_unstructured(model[0], name='weight', amount=0.5)  # To remove the pruning reparameterization and finalize pruning prune.remove(model[0], 'weight')`\n\n![](https://aman.ai/images/copy.png)\n\n`import torch import torch.nn.utils.prune as prune import torch.nn as nn  model = nn.Sequential(     nn.Linear(512, 256),     nn.ReLU(),     nn.Linear(256, 10) )  # Prune 50% of weights in the first Linear layer prune.l1_unstructured(model[0], name='weight', amount=0.5)  # To remove the pruning reparameterization and finalize pruning prune.remove(model[0], 'weight')`\n\n*   **Structured Pruning Example (entire neuron/channel pruning):**\n\n![](https://aman.ai/images/copy.png)\n\n`# Prune 30% of channels (columns) in the second Linear layer prune.ln_structured(model[2], name='weight', amount=0.3, n=2, dim=0) prune.remove(model[2], 'weight')`\n\n![](https://aman.ai/images/copy.png)\n\n`# Prune 30% of channels (columns) in the second Linear layer prune.ln_structured(model[2], name='weight', amount=0.3, n=2, dim=0) prune.remove(model[2], 'weight')`\n\n*   **PyTorch Pruning Resources:**\n    \n    *   [PyTorch pruning tutorial](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)\n    *   [Torch-Pruning library](https://github.com/VainF/Torch-Pruning) – advanced dependency-aware structured pruning\n    *   [TorchPruner](https://github.com/marcoancona/TorchPruner) – structured pruning with visual feedback\n\n**PyTorch Pruning Resources:**\n\n*   [PyTorch pruning tutorial](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)\n*   [Torch-Pruning library](https://github.com/VainF/Torch-Pruning) – advanced dependency-aware structured pruning\n*   [TorchPruner](https://github.com/marcoancona/TorchPruner) – structured pruning with visual feedback\n\n#### TensorFlow Pruning\n\n*   TensorFlow provides pruning support via the `tensorflow_model_optimization` toolkit. It supports sparsity-aware training by gradually zeroing out weights.\n    \n*   **Basic Workflow:**\n    \n\nTensorFlow provides pruning support via the `tensorflow_model_optimization` toolkit. It supports sparsity-aware training by gradually zeroing out weights.\n\n**Basic Workflow:**\n\n![](https://aman.ai/images/copy.png)\n\n`import tensorflow as tf import tensorflow_model_optimization as tfmot  prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude  # Define prunable model model = tf.keras.Sequential([     prune_low_magnitude(tf.keras.layers.Dense(256, activation='relu'),                          input_shape=(512,)),     prune_low_magnitude(tf.keras.layers.Dense(10)) ])  # Compile and train model.compile(optimizer='adam', loss='sparse_categorical_crossentropy') model.fit(x_train, y_train, epochs=5)  # Strip pruning wrappers before saving/export model = tfmot.sparsity.keras.strip_pruning(model)`\n\n![](https://aman.ai/images/copy.png)\n\n`import tensorflow as tf import tensorflow_model_optimization as tfmot  prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude  # Define prunable model model = tf.keras.Sequential([     prune_low_magnitude(tf.keras.layers.Dense(256, activation='relu'),                          input_shape=(512,)),     prune_low_magnitude(tf.keras.layers.Dense(10)) ])  # Compile and train model.compile(optimizer='adam', loss='sparse_categorical_crossentropy') model.fit(x_train, y_train, epochs=5)  # Strip pruning wrappers before saving/export model = tfmot.sparsity.keras.strip_pruning(model)`\n\n*   **Set Pruning Schedule:**\n\n![](https://aman.ai/images/copy.png)\n\n`pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(     initial_sparsity=0.0,     final_sparsity=0.8,     begin_step=0,     end_step=1000 )`\n\n![](https://aman.ai/images/copy.png)\n\n`pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(     initial_sparsity=0.0,     final_sparsity=0.8,     begin_step=0,     end_step=1000 )`\n\n*   **TensorFlow Pruning Resources:**\n    \n    *   [TensorFlow Model Optimization Guide](https://www.tensorflow.org/model_optimization/guide/pruning)\n    *   Supports only unstructured pruning during training\n    *   Can export TFLite-compatible sparse models for edge inference\n\n**TensorFlow Pruning Resources:**\n\n*   [TensorFlow Model Optimization Guide](https://www.tensorflow.org/model_optimization/guide/pruning)\n*   Supports only unstructured pruning during training\n*   Can export TFLite-compatible sparse models for edge inference",
      "order": 39,
      "orderInChapter": 8,
      "difficulty": 5,
      "estimatedMinutes": 3,
      "tags": [
        "ondevice ai",
        "deep learning",
        "optimization",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 449,
        "contentLength": 18801
      },
      "nextCards": [
        "ai-model-compression-overview-40",
        "ai-model-compression-how-mixed-precision-training-works-41"
      ],
      "relatedCards": [
        "ai-federated-learning-scaffold-8",
        "ai-on-device-transformers-hardware-specific-optimization-notes-16",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5",
        "ai-federated-learning-feddyn-10",
        "ai-federated-learning-pros-cons-19"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#implementing-pruning-in-pytorch-and-tensorflow",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-overview-40",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Mixed Precision Training",
      "title": "Overview",
      "subtitle": "Mixed Precision Training",
      "contentHtml": "<ul>\n  <li>\n    <p>Mixed precision is a technique used to speed up neural network training by utilizing both 16-bit and 32-bit floating-point types—primarily <code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>, and <code class=\"language-plaintext highlighter-rouge\">float32</code>. Traditionally, models rely on the <code class=\"language-plaintext highlighter-rouge\">float32</code> dtype, which uses 32 bits of memory per value. However, many modern hardware accelerators are optimized to perform faster computations and memory access with 16-bit types. This means that using <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>, which only take 16 bits each, can lead to significant performance gains.</p>\n  </li>\n  <li>\n    <p>The key insight behind mixed precision is that not all parts of a model require the full precision of <code class=\"language-plaintext highlighter-rouge\">float32</code>. For example, many operations can be safely executed using <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> without compromising the final evaluation metrics such as accuracy. This is where <a href=\"https://developer.nvidia.com/automatic-mixed-precision\">Automatic Mixed Precision (AMP)</a> comes in—it automatically determines the most appropriate precision for each operation, balancing performance with numerical stability.</p>\n\n    <ul>\n      <li>The term “numeric stability” refers to how much using lower precision (like <code class=\"language-plaintext highlighter-rouge\">float16</code>) affects a model’s quality. If certain operations are sensitive to precision loss, AMP keeps them in <code class=\"language-plaintext highlighter-rouge\">float32</code> to preserve model accuracy. By assigning the appropriate dtype to each operation, mixed precision reduces the model’s runtime and memory footprint while maintaining comparable performance.</li>\n    </ul>\n  </li>\n  <li>\n    <p>In summary, mixed precision combines the efficiency of 16-bit operations with the robustness of 32-bit computations to optimize deep learning training workflows.</p>\n  </li>\n</ul>\n<p>Mixed precision is a technique used to speed up neural network training by utilizing both 16-bit and 32-bit floating-point types—primarily <code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>, and <code class=\"language-plaintext highlighter-rouge\">float32</code>. Traditionally, models rely on the <code class=\"language-plaintext highlighter-rouge\">float32</code> dtype, which uses 32 bits of memory per value. However, many modern hardware accelerators are optimized to perform faster computations and memory access with 16-bit types. This means that using <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code>, which only take 16 bits each, can lead to significant performance gains.</p>\n<p>The key insight behind mixed precision is that not all parts of a model require the full precision of <code class=\"language-plaintext highlighter-rouge\">float32</code>. For example, many operations can be safely executed using <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> without compromising the final evaluation metrics such as accuracy. This is where <a href=\"https://developer.nvidia.com/automatic-mixed-precision\">Automatic Mixed Precision (AMP)</a> comes in—it automatically determines the most appropriate precision for each operation, balancing performance with numerical stability.</p>\n<ul>\n      <li>The term “numeric stability” refers to how much using lower precision (like <code class=\"language-plaintext highlighter-rouge\">float16</code>) affects a model’s quality. If certain operations are sensitive to precision loss, AMP keeps them in <code class=\"language-plaintext highlighter-rouge\">float32</code> to preserve model accuracy. By assigning the appropriate dtype to each operation, mixed precision reduces the model’s runtime and memory footprint while maintaining comparable performance.</li>\n    </ul>\n<p>In summary, mixed precision combines the efficiency of 16-bit operations with the robustness of 32-bit computations to optimize deep learning training workflows.</p>",
      "contentMarkdown": "*   Mixed precision is a technique used to speed up neural network training by utilizing both 16-bit and 32-bit floating-point types—primarily `float16`, `bfloat16`, and `float32`. Traditionally, models rely on the `float32` dtype, which uses 32 bits of memory per value. However, many modern hardware accelerators are optimized to perform faster computations and memory access with 16-bit types. This means that using `float16` or `bfloat16`, which only take 16 bits each, can lead to significant performance gains.\n    \n*   The key insight behind mixed precision is that not all parts of a model require the full precision of `float32`. For example, many operations can be safely executed using `float16` or `bfloat16` without compromising the final evaluation metrics such as accuracy. This is where [Automatic Mixed Precision (AMP)](https://developer.nvidia.com/automatic-mixed-precision) comes in—it automatically determines the most appropriate precision for each operation, balancing performance with numerical stability.\n    \n    *   The term “numeric stability” refers to how much using lower precision (like `float16`) affects a model’s quality. If certain operations are sensitive to precision loss, AMP keeps them in `float32` to preserve model accuracy. By assigning the appropriate dtype to each operation, mixed precision reduces the model’s runtime and memory footprint while maintaining comparable performance.\n*   In summary, mixed precision combines the efficiency of 16-bit operations with the robustness of 32-bit computations to optimize deep learning training workflows.\n    \n\nMixed precision is a technique used to speed up neural network training by utilizing both 16-bit and 32-bit floating-point types—primarily `float16`, `bfloat16`, and `float32`. Traditionally, models rely on the `float32` dtype, which uses 32 bits of memory per value. However, many modern hardware accelerators are optimized to perform faster computations and memory access with 16-bit types. This means that using `float16` or `bfloat16`, which only take 16 bits each, can lead to significant performance gains.\n\nThe key insight behind mixed precision is that not all parts of a model require the full precision of `float32`. For example, many operations can be safely executed using `float16` or `bfloat16` without compromising the final evaluation metrics such as accuracy. This is where [Automatic Mixed Precision (AMP)](https://developer.nvidia.com/automatic-mixed-precision) comes in—it automatically determines the most appropriate precision for each operation, balancing performance with numerical stability.\n\n*   The term “numeric stability” refers to how much using lower precision (like `float16`) affects a model’s quality. If certain operations are sensitive to precision loss, AMP keeps them in `float32` to preserve model accuracy. By assigning the appropriate dtype to each operation, mixed precision reduces the model’s runtime and memory footprint while maintaining comparable performance.\n\nIn summary, mixed precision combines the efficiency of 16-bit operations with the robustness of 32-bit computations to optimize deep learning training workflows.",
      "order": 40,
      "orderInChapter": 1,
      "difficulty": 5,
      "estimatedMinutes": 3,
      "tags": [
        "ondevice ai",
        "neural network",
        "deep learning"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 437,
        "contentLength": 4556
      },
      "nextCards": [
        "ai-model-compression-how-mixed-precision-training-works-41",
        "ai-model-compression-overview-42"
      ],
      "relatedCards": [
        "ai-federated-learning-personalization-22",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-pros-15",
        "ai-federated-learning-lora-in-federated-context-17",
        "ai-on-device-transformers-npu-deployment-considerations-edgesoc-devices-20"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#overview",
      "scrapedAt": "2025-12-28T11:55:50.972Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-how-mixed-precision-training-works-41",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Mixed Precision Training",
      "title": "How Mixed Precision Training Works",
      "subtitle": "Mixed Precision Training",
      "contentHtml": "<ul>\n  <li>\n    <p>Mixed precision training is a performance optimization technique that accelerates neural network training by leveraging lower-precision arithmetic—primarily half-precision floating-point (<code class=\"language-plaintext highlighter-rouge\">float16</code>)—without compromising model accuracy or convergence stability.</p>\n  </li>\n  <li>\n    <p>At its core, the concept is straightforward: replacing standard single-precision (<code class=\"language-plaintext highlighter-rouge\">float32</code>) operations with half-precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>) can roughly halve memory usage and significantly reduce training time. However, implementing this substitution safely and effectively is non-trivial due to the numerical limitations of lower-precision formats.</p>\n  </li>\n  <li>\n    <p>By combining dual weight representations, selective precision, and dynamic loss scaling, mixed precision training enables significant reductions in training time and memory consumption—often with negligible impact on model accuracy. As demonstrated in <a href=\"https://arxiv.org/pdf/1710.03740.pdf\">Mixed Precision Training</a> by Narang et al. (2018), these methods allow a wide range of models to train to convergence reliably and efficiently using <code class=\"language-plaintext highlighter-rouge\">float16</code> computations.</p>\n  </li>\n  <li>\n    <p><strong>Challenges with Half Precision</strong>:</p>\n\n    <ul>\n      <li>\n        <p>Lower-precision formats like <code class=\"language-plaintext highlighter-rouge\">float16</code> have a reduced dynamic range and lower numerical precision compared to <code class=\"language-plaintext highlighter-rouge\">float32</code>. One critical issue is <em>underflow</em>, where extremely small gradient values become indistinguishable from zero due to rounding errors inherent in the limited precision. This is especially problematic during backpropagation, as many gradient updates are naturally very small but still essential for accurate model convergence. If too many of these values are rounded to zero or become <code class=\"language-plaintext highlighter-rouge\">NaN</code> (Not a Number), the model may fail to learn altogether.</p>\n      </li>\n      <li>\n        <p>The following figure from the <a href=\"https://arxiv.org/pdf/1710.03740.pdf\">Mixed Precision Training</a> paper illustrates a key finding that naïvely switching to <code class=\"language-plaintext highlighter-rouge\">float16</code> causes any gradient smaller than <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-129-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>24</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1562\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1563\"><span class=\"msubsup\" id=\"MathJax-Span-1564\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1565\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1566\"><span class=\"mrow\" id=\"MathJax-Span-1567\"><span class=\"mo\" id=\"MathJax-Span-1568\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1569\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">24</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>24</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-129\">2^{-24}</script> to be “swallowed”—effectively zeroed out. In their experiments, this resulted in approximately 5% of all gradient updates being discarded, severely impeding the training process:</p>\n      </li>\n    </ul>\n\n    <p><img src=\"/primers/ai/assets/model-compression/mpt.avif\" alt=\"\"></p>\n  </li>\n  <li>\n    <p><strong>Techniques for Safe Mixed Precision Training</strong>:</p>\n\n    <ul>\n      <li>\n        <p>To mitigate these numerical instabilities, the authors propose a systematic approach combining three key strategies. When used together, these allow safe and effective training with <code class=\"language-plaintext highlighter-rouge\">float16</code> precision:</p>\n\n        <ol>\n          <li><strong>Maintaining Dual Weight Copies (Master Weights Strategy)</strong>\n            <ul>\n              <li>Each model weight is stored in two formats: a full-precision (<code class=\"language-plaintext highlighter-rouge\">float32</code>) “master copy” and a lower-precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>) copy. During forward and backward passes, computations are performed using the <code class=\"language-plaintext highlighter-rouge\">float16</code> version to benefit from faster execution and lower memory usage. However, the actual weight updates are applied to the <code class=\"language-plaintext highlighter-rouge\">float32</code> master weights using gradients computed in <code class=\"language-plaintext highlighter-rouge\">float16</code> but cast to <code class=\"language-plaintext highlighter-rouge\">float32</code>. This preserves update accuracy and avoids the accumulation of precision errors during training.</li>\n            </ul>\n          </li>\n          <li><strong>Selective Precision Application (Mixed-Dtype Execution)</strong>\n            <ul>\n              <li>Not all neural network operations are equally sensitive to reduced precision. Many element-wise operations (e.g., activation functions or layer normalization) are safe to compute in <code class=\"language-plaintext highlighter-rouge\">float16</code>, while others—such as softmax, batch normalization, and gradient accumulation—require <code class=\"language-plaintext highlighter-rouge\">float32</code> to maintain stability. Mixed precision training selectively applies <code class=\"language-plaintext highlighter-rouge\">float16</code> where safe and retains <code class=\"language-plaintext highlighter-rouge\">float32</code> where necessary. This fine-grained control over data types allows the model to reap performance benefits without sacrificing numerical stability/reliability.</li>\n            </ul>\n          </li>\n          <li><strong>Loss Scaling</strong>\n            <ul>\n              <li>To address the underflow problem, the loss value is multiplied by a scalar factor (commonly 8, 16, or 128) before backpropagation. This process, known as <em>loss scaling</em>, proportionally increases all gradient values, elevating small gradients above the <code class=\"language-plaintext highlighter-rouge\">float16</code> precision threshold of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-130-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>24</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1570\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1571\"><span class=\"msubsup\" id=\"MathJax-Span-1572\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1573\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1574\"><span class=\"mrow\" id=\"MathJax-Span-1575\"><span class=\"mo\" id=\"MathJax-Span-1576\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1577\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">24</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>24</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-130\">2^{-24}</script>. After gradients are computed, the scaling factor is removed (by division) before the optimizer applies the updates.</li>\n              <li>Care must be taken to avoid <em>overflow</em>, which occurs when values exceed the representable range of <code class=\"language-plaintext highlighter-rouge\">float16</code>, leading to <code class=\"language-plaintext highlighter-rouge\">Inf</code> or <code class=\"language-plaintext highlighter-rouge\">NaN</code> values. Adaptive loss scaling strategies—where the scaling factor is dynamically adjusted based on gradient statistics—are often employed to balance between underflow and overflow.</li>\n            </ul>\n          </li>\n        </ol>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p>Mixed precision training is a performance optimization technique that accelerates neural network training by leveraging lower-precision arithmetic—primarily half-precision floating-point (<code class=\"language-plaintext highlighter-rouge\">float16</code>)—without compromising model accuracy or convergence stability.</p>\n<p>At its core, the concept is straightforward: replacing standard single-precision (<code class=\"language-plaintext highlighter-rouge\">float32</code>) operations with half-precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>) can roughly halve memory usage and significantly reduce training time. However, implementing this substitution safely and effectively is non-trivial due to the numerical limitations of lower-precision formats.</p>\n<p>By combining dual weight representations, selective precision, and dynamic loss scaling, mixed precision training enables significant reductions in training time and memory consumption—often with negligible impact on model accuracy. As demonstrated in <a href=\"https://arxiv.org/pdf/1710.03740.pdf\">Mixed Precision Training</a> by Narang et al. (2018), these methods allow a wide range of models to train to convergence reliably and efficiently using <code class=\"language-plaintext highlighter-rouge\">float16</code> computations.</p>\n<p><strong>Challenges with Half Precision</strong>:</p>\n<ul>\n      <li>\n        <p>Lower-precision formats like <code class=\"language-plaintext highlighter-rouge\">float16</code> have a reduced dynamic range and lower numerical precision compared to <code class=\"language-plaintext highlighter-rouge\">float32</code>. One critical issue is <em>underflow</em>, where extremely small gradient values become indistinguishable from zero due to rounding errors inherent in the limited precision. This is especially problematic during backpropagation, as many gradient updates are naturally very small but still essential for accurate model convergence. If too many of these values are rounded to zero or become <code class=\"language-plaintext highlighter-rouge\">NaN</code> (Not a Number), the model may fail to learn altogether.</p>\n      </li>\n      <li>\n        <p>The following figure from the <a href=\"https://arxiv.org/pdf/1710.03740.pdf\">Mixed Precision Training</a> paper illustrates a key finding that naïvely switching to <code class=\"language-plaintext highlighter-rouge\">float16</code> causes any gradient smaller than <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-129-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>24</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1562\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1563\"><span class=\"msubsup\" id=\"MathJax-Span-1564\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1565\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1566\"><span class=\"mrow\" id=\"MathJax-Span-1567\"><span class=\"mo\" id=\"MathJax-Span-1568\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1569\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">24</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>24</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-129\">2^{-24}</script> to be “swallowed”—effectively zeroed out. In their experiments, this resulted in approximately 5% of all gradient updates being discarded, severely impeding the training process:</p>\n      </li>\n    </ul>\n<p>Lower-precision formats like <code class=\"language-plaintext highlighter-rouge\">float16</code> have a reduced dynamic range and lower numerical precision compared to <code class=\"language-plaintext highlighter-rouge\">float32</code>. One critical issue is <em>underflow</em>, where extremely small gradient values become indistinguishable from zero due to rounding errors inherent in the limited precision. This is especially problematic during backpropagation, as many gradient updates are naturally very small but still essential for accurate model convergence. If too many of these values are rounded to zero or become <code class=\"language-plaintext highlighter-rouge\">NaN</code> (Not a Number), the model may fail to learn altogether.</p>\n<p>The following figure from the <a href=\"https://arxiv.org/pdf/1710.03740.pdf\">Mixed Precision Training</a> paper illustrates a key finding that naïvely switching to <code class=\"language-plaintext highlighter-rouge\">float16</code> causes any gradient smaller than <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-129-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>24</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1562\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1563\"><span class=\"msubsup\" id=\"MathJax-Span-1564\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1565\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1566\"><span class=\"mrow\" id=\"MathJax-Span-1567\"><span class=\"mo\" id=\"MathJax-Span-1568\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1569\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">24</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>24</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-129\">2^{-24}</script> to be “swallowed”—effectively zeroed out. In their experiments, this resulted in approximately 5% of all gradient updates being discarded, severely impeding the training process:</p>\n<p><img src=\"/primers/ai/assets/model-compression/mpt.avif\" alt=\"\"></p>\n<p><strong>Techniques for Safe Mixed Precision Training</strong>:</p>\n<ul>\n      <li>\n        <p>To mitigate these numerical instabilities, the authors propose a systematic approach combining three key strategies. When used together, these allow safe and effective training with <code class=\"language-plaintext highlighter-rouge\">float16</code> precision:</p>\n\n        <ol>\n          <li><strong>Maintaining Dual Weight Copies (Master Weights Strategy)</strong>\n            <ul>\n              <li>Each model weight is stored in two formats: a full-precision (<code class=\"language-plaintext highlighter-rouge\">float32</code>) “master copy” and a lower-precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>) copy. During forward and backward passes, computations are performed using the <code class=\"language-plaintext highlighter-rouge\">float16</code> version to benefit from faster execution and lower memory usage. However, the actual weight updates are applied to the <code class=\"language-plaintext highlighter-rouge\">float32</code> master weights using gradients computed in <code class=\"language-plaintext highlighter-rouge\">float16</code> but cast to <code class=\"language-plaintext highlighter-rouge\">float32</code>. This preserves update accuracy and avoids the accumulation of precision errors during training.</li>\n            </ul>\n          </li>\n          <li><strong>Selective Precision Application (Mixed-Dtype Execution)</strong>\n            <ul>\n              <li>Not all neural network operations are equally sensitive to reduced precision. Many element-wise operations (e.g., activation functions or layer normalization) are safe to compute in <code class=\"language-plaintext highlighter-rouge\">float16</code>, while others—such as softmax, batch normalization, and gradient accumulation—require <code class=\"language-plaintext highlighter-rouge\">float32</code> to maintain stability. Mixed precision training selectively applies <code class=\"language-plaintext highlighter-rouge\">float16</code> where safe and retains <code class=\"language-plaintext highlighter-rouge\">float32</code> where necessary. This fine-grained control over data types allows the model to reap performance benefits without sacrificing numerical stability/reliability.</li>\n            </ul>\n          </li>\n          <li><strong>Loss Scaling</strong>\n            <ul>\n              <li>To address the underflow problem, the loss value is multiplied by a scalar factor (commonly 8, 16, or 128) before backpropagation. This process, known as <em>loss scaling</em>, proportionally increases all gradient values, elevating small gradients above the <code class=\"language-plaintext highlighter-rouge\">float16</code> precision threshold of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-130-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>24</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1570\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1571\"><span class=\"msubsup\" id=\"MathJax-Span-1572\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1573\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1574\"><span class=\"mrow\" id=\"MathJax-Span-1575\"><span class=\"mo\" id=\"MathJax-Span-1576\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1577\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">24</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>24</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-130\">2^{-24}</script>. After gradients are computed, the scaling factor is removed (by division) before the optimizer applies the updates.</li>\n              <li>Care must be taken to avoid <em>overflow</em>, which occurs when values exceed the representable range of <code class=\"language-plaintext highlighter-rouge\">float16</code>, leading to <code class=\"language-plaintext highlighter-rouge\">Inf</code> or <code class=\"language-plaintext highlighter-rouge\">NaN</code> values. Adaptive loss scaling strategies—where the scaling factor is dynamically adjusted based on gradient statistics—are often employed to balance between underflow and overflow.</li>\n            </ul>\n          </li>\n        </ol>\n      </li>\n    </ul>\n<p>To mitigate these numerical instabilities, the authors propose a systematic approach combining three key strategies. When used together, these allow safe and effective training with <code class=\"language-plaintext highlighter-rouge\">float16</code> precision:</p>\n<ol>\n          <li><strong>Maintaining Dual Weight Copies (Master Weights Strategy)</strong>\n            <ul>\n              <li>Each model weight is stored in two formats: a full-precision (<code class=\"language-plaintext highlighter-rouge\">float32</code>) “master copy” and a lower-precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>) copy. During forward and backward passes, computations are performed using the <code class=\"language-plaintext highlighter-rouge\">float16</code> version to benefit from faster execution and lower memory usage. However, the actual weight updates are applied to the <code class=\"language-plaintext highlighter-rouge\">float32</code> master weights using gradients computed in <code class=\"language-plaintext highlighter-rouge\">float16</code> but cast to <code class=\"language-plaintext highlighter-rouge\">float32</code>. This preserves update accuracy and avoids the accumulation of precision errors during training.</li>\n            </ul>\n          </li>\n          <li><strong>Selective Precision Application (Mixed-Dtype Execution)</strong>\n            <ul>\n              <li>Not all neural network operations are equally sensitive to reduced precision. Many element-wise operations (e.g., activation functions or layer normalization) are safe to compute in <code class=\"language-plaintext highlighter-rouge\">float16</code>, while others—such as softmax, batch normalization, and gradient accumulation—require <code class=\"language-plaintext highlighter-rouge\">float32</code> to maintain stability. Mixed precision training selectively applies <code class=\"language-plaintext highlighter-rouge\">float16</code> where safe and retains <code class=\"language-plaintext highlighter-rouge\">float32</code> where necessary. This fine-grained control over data types allows the model to reap performance benefits without sacrificing numerical stability/reliability.</li>\n            </ul>\n          </li>\n          <li><strong>Loss Scaling</strong>\n            <ul>\n              <li>To address the underflow problem, the loss value is multiplied by a scalar factor (commonly 8, 16, or 128) before backpropagation. This process, known as <em>loss scaling</em>, proportionally increases all gradient values, elevating small gradients above the <code class=\"language-plaintext highlighter-rouge\">float16</code> precision threshold of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-130-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>24</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1570\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1571\"><span class=\"msubsup\" id=\"MathJax-Span-1572\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1573\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1574\"><span class=\"mrow\" id=\"MathJax-Span-1575\"><span class=\"mo\" id=\"MathJax-Span-1576\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1577\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">24</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>24</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-130\">2^{-24}</script>. After gradients are computed, the scaling factor is removed (by division) before the optimizer applies the updates.</li>\n              <li>Care must be taken to avoid <em>overflow</em>, which occurs when values exceed the representable range of <code class=\"language-plaintext highlighter-rouge\">float16</code>, leading to <code class=\"language-plaintext highlighter-rouge\">Inf</code> or <code class=\"language-plaintext highlighter-rouge\">NaN</code> values. Adaptive loss scaling strategies—where the scaling factor is dynamically adjusted based on gradient statistics—are often employed to balance between underflow and overflow.</li>\n            </ul>\n          </li>\n        </ol>\n<ul>\n              <li>Each model weight is stored in two formats: a full-precision (<code class=\"language-plaintext highlighter-rouge\">float32</code>) “master copy” and a lower-precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>) copy. During forward and backward passes, computations are performed using the <code class=\"language-plaintext highlighter-rouge\">float16</code> version to benefit from faster execution and lower memory usage. However, the actual weight updates are applied to the <code class=\"language-plaintext highlighter-rouge\">float32</code> master weights using gradients computed in <code class=\"language-plaintext highlighter-rouge\">float16</code> but cast to <code class=\"language-plaintext highlighter-rouge\">float32</code>. This preserves update accuracy and avoids the accumulation of precision errors during training.</li>\n            </ul>\n<ul>\n              <li>Not all neural network operations are equally sensitive to reduced precision. Many element-wise operations (e.g., activation functions or layer normalization) are safe to compute in <code class=\"language-plaintext highlighter-rouge\">float16</code>, while others—such as softmax, batch normalization, and gradient accumulation—require <code class=\"language-plaintext highlighter-rouge\">float32</code> to maintain stability. Mixed precision training selectively applies <code class=\"language-plaintext highlighter-rouge\">float16</code> where safe and retains <code class=\"language-plaintext highlighter-rouge\">float32</code> where necessary. This fine-grained control over data types allows the model to reap performance benefits without sacrificing numerical stability/reliability.</li>\n            </ul>\n<ul>\n              <li>To address the underflow problem, the loss value is multiplied by a scalar factor (commonly 8, 16, or 128) before backpropagation. This process, known as <em>loss scaling</em>, proportionally increases all gradient values, elevating small gradients above the <code class=\"language-plaintext highlighter-rouge\">float16</code> precision threshold of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-130-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>24</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1570\" style=\"width: 2.138em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.77em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1571\"><span class=\"msubsup\" id=\"MathJax-Span-1572\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1573\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1574\"><span class=\"mrow\" id=\"MathJax-Span-1575\"><span class=\"mo\" id=\"MathJax-Span-1576\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1577\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">24</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>24</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-130\">2^{-24}</script>. After gradients are computed, the scaling factor is removed (by division) before the optimizer applies the updates.</li>\n              <li>Care must be taken to avoid <em>overflow</em>, which occurs when values exceed the representable range of <code class=\"language-plaintext highlighter-rouge\">float16</code>, leading to <code class=\"language-plaintext highlighter-rouge\">Inf</code> or <code class=\"language-plaintext highlighter-rouge\">NaN</code> values. Adaptive loss scaling strategies—where the scaling factor is dynamically adjusted based on gradient statistics—are often employed to balance between underflow and overflow.</li>\n            </ul>\n<h4 id=\"how-pytorch-automatic-mixed-precision-works\">How PyTorch Automatic Mixed Precision Works</h4>\n<ul>\n  <li>\n    <p>With a solid understanding of mixed precision training established, we can now explore how PyTorch streamlines this powerful optimization technique through its <strong>Automatic Mixed Precision (AMP)</strong> API. While mixed precision training has long been theoretically feasible—typically requiring manual tensor casting to <code class=\"language-plaintext highlighter-rouge\">float16</code> and careful loss scaling—PyTorch removes much of this complexity. Its AMP API makes the process highly accessible, offering a streamlined, production-ready solution that demands only minimal code modifications.</p>\n  </li>\n  <li>\n    <p>PyTorch’s AMP achieves this by abstracting the underlying mechanics through two key components: <code class=\"language-plaintext highlighter-rouge\">autocast</code> and <code class=\"language-plaintext highlighter-rouge\">GradScaler</code>. <code class=\"language-plaintext highlighter-rouge\">autocast</code> enables selective precision execution, automatically determining which operations benefit from half-precision without sacrificing accuracy. Simultaneously, <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> manages dynamic loss scaling, helping to prevent issues like gradient underflow and ensuring stable convergence. This integration offers developers substantial speedups—often reducing training times by <strong>50–60%</strong>—and improves memory efficiency, all without compromising model stability or performance.</p>\n  </li>\n  <li>\n    <p>This practical implementation is a direct evolution of the concepts outlined in the <a href=\"https://arxiv.org/pdf/1710.03740.pdf\"><em>Mixed Precision Training</em></a> research paper. AMP embodies how advanced techniques from cutting-edge research can be distilled into user-friendly tools that enhance real-world machine learning workflows.</p>\n  </li>\n  <li>\n    <p>Prior to AMP, implementing mixed precision was a labor-intensive process. Developers had to manually cast tensors, implement and tune custom loss scalers, and safeguard against the risks of instability. The introduction of PyTorch’s <code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp</code> module represents a major leap forward, encapsulating best practices and democratizing access to high-performance training.</p>\n  </li>\n  <li>\n    <p>AMP is especially effective on modern NVIDIA GPUs—such as those based on Volta, Turing, Ampere, or newer architectures—which include specialized <strong>Tensor Cores</strong> designed for half-precision operations. However, even on older or unsupported hardware, users may still see performance benefits due to more efficient memory usage and reduced data movement.</p>\n  </li>\n  <li>\n    <p>In summary, PyTorch’s AMP bridges the gap between theoretical efficiency and practical deployment, making state-of-the-art training techniques both accessible and impactful across a wide range of hardware and use cases.</p>\n  </li>\n</ul>\n<p>With a solid understanding of mixed precision training established, we can now explore how PyTorch streamlines this powerful optimization technique through its <strong>Automatic Mixed Precision (AMP)</strong> API. While mixed precision training has long been theoretically feasible—typically requiring manual tensor casting to <code class=\"language-plaintext highlighter-rouge\">float16</code> and careful loss scaling—PyTorch removes much of this complexity. Its AMP API makes the process highly accessible, offering a streamlined, production-ready solution that demands only minimal code modifications.</p>\n<p>PyTorch’s AMP achieves this by abstracting the underlying mechanics through two key components: <code class=\"language-plaintext highlighter-rouge\">autocast</code> and <code class=\"language-plaintext highlighter-rouge\">GradScaler</code>. <code class=\"language-plaintext highlighter-rouge\">autocast</code> enables selective precision execution, automatically determining which operations benefit from half-precision without sacrificing accuracy. Simultaneously, <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> manages dynamic loss scaling, helping to prevent issues like gradient underflow and ensuring stable convergence. This integration offers developers substantial speedups—often reducing training times by <strong>50–60%</strong>—and improves memory efficiency, all without compromising model stability or performance.</p>\n<p>This practical implementation is a direct evolution of the concepts outlined in the <a href=\"https://arxiv.org/pdf/1710.03740.pdf\"><em>Mixed Precision Training</em></a> research paper. AMP embodies how advanced techniques from cutting-edge research can be distilled into user-friendly tools that enhance real-world machine learning workflows.</p>\n<p>Prior to AMP, implementing mixed precision was a labor-intensive process. Developers had to manually cast tensors, implement and tune custom loss scalers, and safeguard against the risks of instability. The introduction of PyTorch’s <code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp</code> module represents a major leap forward, encapsulating best practices and democratizing access to high-performance training.</p>\n<p>AMP is especially effective on modern NVIDIA GPUs—such as those based on Volta, Turing, Ampere, or newer architectures—which include specialized <strong>Tensor Cores</strong> designed for half-precision operations. However, even on older or unsupported hardware, users may still see performance benefits due to more efficient memory usage and reduced data movement.</p>\n<p>In summary, PyTorch’s AMP bridges the gap between theoretical efficiency and practical deployment, making state-of-the-art training techniques both accessible and impactful across a wide range of hardware and use cases.</p>\n<h5 id=\"overview-of-amp-components\">Overview of AMP Components</h5>\n<ul>\n  <li>\n    <p>PyTorch’s AMP functionality is implemented via the <code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp</code> module and relies on two key primitives:</p>\n\n    <ol>\n      <li>\n        <p><code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp.autocast</code>: A context manager that automatically casts operations to the appropriate precision (<code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">float32</code>) based on operation type and hardware support. This enables a seamless mix of half-precision and full-precision computations without explicit manual intervention.</p>\n      </li>\n      <li>\n        <p><code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp.GradScaler</code>: A utility that handles <em>dynamic loss scaling</em>. It scales the loss to prevent underflow in gradient computations and then unscales it before applying the optimizer step. The scaler also detects and skips optimizer steps with invalid gradients (e.g., <code class=\"language-plaintext highlighter-rouge\">NaN</code> or <code class=\"language-plaintext highlighter-rouge\">Inf</code>), adjusting the scale factor dynamically to maintain numerical stability.</p>\n      </li>\n    </ol>\n  </li>\n</ul>\n<p>PyTorch’s AMP functionality is implemented via the <code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp</code> module and relies on two key primitives:</p>\n<ol>\n      <li>\n        <p><code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp.autocast</code>: A context manager that automatically casts operations to the appropriate precision (<code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">float32</code>) based on operation type and hardware support. This enables a seamless mix of half-precision and full-precision computations without explicit manual intervention.</p>\n      </li>\n      <li>\n        <p><code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp.GradScaler</code>: A utility that handles <em>dynamic loss scaling</em>. It scales the loss to prevent underflow in gradient computations and then unscales it before applying the optimizer step. The scaler also detects and skips optimizer steps with invalid gradients (e.g., <code class=\"language-plaintext highlighter-rouge\">NaN</code> or <code class=\"language-plaintext highlighter-rouge\">Inf</code>), adjusting the scale factor dynamically to maintain numerical stability.</p>\n      </li>\n    </ol>\n<p><code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp.autocast</code>: A context manager that automatically casts operations to the appropriate precision (<code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">float32</code>) based on operation type and hardware support. This enables a seamless mix of half-precision and full-precision computations without explicit manual intervention.</p>\n<p><code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp.GradScaler</code>: A utility that handles <em>dynamic loss scaling</em>. It scales the loss to prevent underflow in gradient computations and then unscales it before applying the optimizer step. The scaler also detects and skips optimizer steps with invalid gradients (e.g., <code class=\"language-plaintext highlighter-rouge\">NaN</code> or <code class=\"language-plaintext highlighter-rouge\">Inf</code>), adjusting the scale factor dynamically to maintain numerical stability.</p>\n<h5 id=\"practical-implementation-in-a-training-loop\">Practical Implementation in a Training Loop</h5>\n<ul>\n  <li>The following example demonstrates how mixed precision training is incorporated into a standard PyTorch training loop. Lines marked with <code class=\"language-plaintext highlighter-rouge\"># NEW</code> indicate additions or modifications required to enable AMP.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code20\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code20\"><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">optim</span><span class=\"p\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">max_lr</span><span class=\"p\">)</span>\n<span class=\"n\">scheduler</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">optim</span><span class=\"p\">.</span><span class=\"n\">lr_scheduler</span><span class=\"p\">.</span><span class=\"n\">OneCycleLR</span><span class=\"p\">(</span>\n    <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">max_lr</span><span class=\"p\">,</span>\n    <span class=\"n\">cycle_momentum</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span>\n    <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">n_epochs</span><span class=\"p\">,</span>\n    <span class=\"n\">steps_per_epoch</span><span class=\"o\">=</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">ceil</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">batch_size</span><span class=\"p\">)),</span>\n<span class=\"p\">)</span>\n<span class=\"n\">batches</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">utils</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">(</span>\n    <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">utils</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">TensorDataset</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">),</span>\n    <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"bp\">True</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># NEW: Initialize GradScaler for dynamic loss scaling\n</span><span class=\"n\">scaler</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">.</span><span class=\"n\">amp</span><span class=\"p\">.</span><span class=\"n\">GradScaler</span><span class=\"p\">()</span>\n\n<span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">n_epochs</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">X_batch</span><span class=\"p\">,</span> <span class=\"n\">y_batch</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">batches</span><span class=\"p\">):</span>\n        <span class=\"n\">X_batch</span> <span class=\"o\">=</span> <span class=\"n\">X_batch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n        <span class=\"n\">y_batch</span> <span class=\"o\">=</span> <span class=\"n\">y_batch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n        <span class=\"n\">optimizer</span><span class=\"p\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n\n        <span class=\"c1\"># NEW: Forward pass with autocast for mixed precision\n</span>        <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">.</span><span class=\"n\">amp</span><span class=\"p\">.</span><span class=\"n\">autocast</span><span class=\"p\">():</span>\n            <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">X_batch</span><span class=\"p\">).</span><span class=\"n\">squeeze</span><span class=\"p\">()</span>\n            <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">y_pred</span><span class=\"p\">,</span> <span class=\"n\">y_batch</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># NEW: Scale loss and perform backward pass\n</span>        <span class=\"n\">scaler</span><span class=\"p\">.</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"p\">).</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n        <span class=\"n\">lv</span> <span class=\"o\">=</span> <span class=\"n\">loss</span><span class=\"p\">.</span><span class=\"n\">detach</span><span class=\"p\">().</span><span class=\"n\">cpu</span><span class=\"p\">().</span><span class=\"n\">numpy</span><span class=\"p\">()</span>\n        <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Epoch </span><span class=\"si\">{</span><span class=\"n\">epoch</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s\">/</span><span class=\"si\">{</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">n_epochs</span><span class=\"si\">}</span><span class=\"s\">; Batch </span><span class=\"si\">{</span><span class=\"n\">i</span><span class=\"si\">}</span><span class=\"s\">; Loss </span><span class=\"si\">{</span><span class=\"n\">lv</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># NEW: Unscale gradients, perform optimizer step, update scaler\n</span>        <span class=\"n\">scaler</span><span class=\"p\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n        <span class=\"n\">scaler</span><span class=\"p\">.</span><span class=\"n\">update</span><span class=\"p\">()</span>\n\n        <span class=\"n\">scheduler</span><span class=\"p\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code20\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code20\"><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">train</span><span class=\"p\">()</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">optim</span><span class=\"p\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">max_lr</span><span class=\"p\">)</span>\n<span class=\"n\">scheduler</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">optim</span><span class=\"p\">.</span><span class=\"n\">lr_scheduler</span><span class=\"p\">.</span><span class=\"n\">OneCycleLR</span><span class=\"p\">(</span>\n    <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">max_lr</span><span class=\"p\">,</span>\n    <span class=\"n\">cycle_momentum</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span>\n    <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">n_epochs</span><span class=\"p\">,</span>\n    <span class=\"n\">steps_per_epoch</span><span class=\"o\">=</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">ceil</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">batch_size</span><span class=\"p\">)),</span>\n<span class=\"p\">)</span>\n<span class=\"n\">batches</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">utils</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">DataLoader</span><span class=\"p\">(</span>\n    <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">utils</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">TensorDataset</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">),</span>\n    <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">batch_size</span><span class=\"p\">,</span> <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"bp\">True</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># NEW: Initialize GradScaler for dynamic loss scaling\n</span><span class=\"n\">scaler</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">.</span><span class=\"n\">amp</span><span class=\"p\">.</span><span class=\"n\">GradScaler</span><span class=\"p\">()</span>\n\n<span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">n_epochs</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">X_batch</span><span class=\"p\">,</span> <span class=\"n\">y_batch</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">batches</span><span class=\"p\">):</span>\n        <span class=\"n\">X_batch</span> <span class=\"o\">=</span> <span class=\"n\">X_batch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n        <span class=\"n\">y_batch</span> <span class=\"o\">=</span> <span class=\"n\">y_batch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">()</span>\n        <span class=\"n\">optimizer</span><span class=\"p\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n\n        <span class=\"c1\"># NEW: Forward pass with autocast for mixed precision\n</span>        <span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">.</span><span class=\"n\">amp</span><span class=\"p\">.</span><span class=\"n\">autocast</span><span class=\"p\">():</span>\n            <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">X_batch</span><span class=\"p\">).</span><span class=\"n\">squeeze</span><span class=\"p\">()</span>\n            <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">y_pred</span><span class=\"p\">,</span> <span class=\"n\">y_batch</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># NEW: Scale loss and perform backward pass\n</span>        <span class=\"n\">scaler</span><span class=\"p\">.</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"p\">).</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n\n        <span class=\"n\">lv</span> <span class=\"o\">=</span> <span class=\"n\">loss</span><span class=\"p\">.</span><span class=\"n\">detach</span><span class=\"p\">().</span><span class=\"n\">cpu</span><span class=\"p\">().</span><span class=\"n\">numpy</span><span class=\"p\">()</span>\n        <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">\"Epoch </span><span class=\"si\">{</span><span class=\"n\">epoch</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s\">/</span><span class=\"si\">{</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">n_epochs</span><span class=\"si\">}</span><span class=\"s\">; Batch </span><span class=\"si\">{</span><span class=\"n\">i</span><span class=\"si\">}</span><span class=\"s\">; Loss </span><span class=\"si\">{</span><span class=\"n\">lv</span><span class=\"si\">}</span><span class=\"s\">\"</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># NEW: Unscale gradients, perform optimizer step, update scaler\n</span>        <span class=\"n\">scaler</span><span class=\"p\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n        <span class=\"n\">scaler</span><span class=\"p\">.</span><span class=\"n\">update</span><span class=\"p\">()</span>\n\n        <span class=\"n\">scheduler</span><span class=\"p\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n</code></pre>\n<ul>\n  <li>\n    <p><strong>Implementation Notes and Best Practices</strong>:</p>\n\n    <ul>\n      <li>\n        <p><strong>Device compatibility</strong>: AMP is optimized for NVIDIA GPUs with Tensor Cores, particularly those with compute capability ≥ 7.0 (Volta architecture or newer). While it will run on other hardware, performance gains may vary.</p>\n      </li>\n      <li>\n        <p><strong>Model compatibility</strong>: Most standard PyTorch layers (e.g., <code class=\"language-plaintext highlighter-rouge\">nn.Linear</code>, <code class=\"language-plaintext highlighter-rouge\">nn.Conv2d</code>, <code class=\"language-plaintext highlighter-rouge\">nn.ReLU</code>) are AMP-compatible. However, custom operations or third-party libraries may require manual inspection to ensure compatibility or appropriate casting.</p>\n      </li>\n      <li>\n        <p><strong>Gradient stability</strong>: The <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> performs <em>automatic gradient anomaly detection</em>, skipping optimizer steps when gradients contain <code class=\"language-plaintext highlighter-rouge\">Inf</code> or <code class=\"language-plaintext highlighter-rouge\">NaN</code> values. This safeguards training from diverging due to numerical instability.</p>\n      </li>\n      <li>\n        <p><strong>Loss scaling strategy</strong>: The <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> uses <em>dynamic loss scaling</em> by default, which adjusts the scaling factor at runtime based on gradient statistics. This is typically preferred over static scaling for its adaptive robustness.</p>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Implementation Notes and Best Practices</strong>:</p>\n<ul>\n      <li>\n        <p><strong>Device compatibility</strong>: AMP is optimized for NVIDIA GPUs with Tensor Cores, particularly those with compute capability ≥ 7.0 (Volta architecture or newer). While it will run on other hardware, performance gains may vary.</p>\n      </li>\n      <li>\n        <p><strong>Model compatibility</strong>: Most standard PyTorch layers (e.g., <code class=\"language-plaintext highlighter-rouge\">nn.Linear</code>, <code class=\"language-plaintext highlighter-rouge\">nn.Conv2d</code>, <code class=\"language-plaintext highlighter-rouge\">nn.ReLU</code>) are AMP-compatible. However, custom operations or third-party libraries may require manual inspection to ensure compatibility or appropriate casting.</p>\n      </li>\n      <li>\n        <p><strong>Gradient stability</strong>: The <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> performs <em>automatic gradient anomaly detection</em>, skipping optimizer steps when gradients contain <code class=\"language-plaintext highlighter-rouge\">Inf</code> or <code class=\"language-plaintext highlighter-rouge\">NaN</code> values. This safeguards training from diverging due to numerical instability.</p>\n      </li>\n      <li>\n        <p><strong>Loss scaling strategy</strong>: The <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> uses <em>dynamic loss scaling</em> by default, which adjusts the scaling factor at runtime based on gradient statistics. This is typically preferred over static scaling for its adaptive robustness.</p>\n      </li>\n    </ul>\n<p><strong>Device compatibility</strong>: AMP is optimized for NVIDIA GPUs with Tensor Cores, particularly those with compute capability ≥ 7.0 (Volta architecture or newer). While it will run on other hardware, performance gains may vary.</p>\n<p><strong>Model compatibility</strong>: Most standard PyTorch layers (e.g., <code class=\"language-plaintext highlighter-rouge\">nn.Linear</code>, <code class=\"language-plaintext highlighter-rouge\">nn.Conv2d</code>, <code class=\"language-plaintext highlighter-rouge\">nn.ReLU</code>) are AMP-compatible. However, custom operations or third-party libraries may require manual inspection to ensure compatibility or appropriate casting.</p>\n<p><strong>Gradient stability</strong>: The <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> performs <em>automatic gradient anomaly detection</em>, skipping optimizer steps when gradients contain <code class=\"language-plaintext highlighter-rouge\">Inf</code> or <code class=\"language-plaintext highlighter-rouge\">NaN</code> values. This safeguards training from diverging due to numerical instability.</p>\n<p><strong>Loss scaling strategy</strong>: The <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> uses <em>dynamic loss scaling</em> by default, which adjusts the scaling factor at runtime based on gradient statistics. This is typically preferred over static scaling for its adaptive robustness.</p>\n<h5 id=\"loss-and-gradient-scaling-with-gradscaler\">Loss and Gradient Scaling with <code class=\"language-plaintext Highlighter-rouge\">GradScaler</code></h5>\n<ul>\n  <li>\n    <p>A fundamental challenge of half-precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>) training is the limited dynamic range, which can cause small-magnitude gradients to underflow—i.e., round down to zero—during backpropagation. This occurs because when an operation receives <code class=\"language-plaintext highlighter-rouge\">float16</code> inputs in the forward pass, the resulting gradients computed in the backward pass are also in <code class=\"language-plaintext highlighter-rouge\">float16</code>, unless explicitly handled. In deep learning, many gradients—particularly in early layers or at later training stages—can be extremely small, and when these are flushed to zero, their corresponding weight updates are effectively lost, impeding learning.</p>\n  </li>\n  <li>\n    <p>To mitigate this, PyTorch introduces <strong>loss scaling</strong>, a technique that amplifies loss values and their corresponding gradients during the backward pass to avoid underflow. The process works as follows:</p>\n\n    <ol>\n      <li>The loss is multiplied by a scale factor before backpropagation.</li>\n      <li>Gradients are computed on this scaled loss, resulting in proportionally larger values.</li>\n      <li>These gradients are then unscaled before the optimizer applies the update, preserving the intended learning dynamics.</li>\n    </ol>\n  </li>\n  <li>\n    <p>This technique is implemented via the <code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp.GradScaler</code> object, which automates both the scaling and unscaling process, as well as overflow detection and recovery. The goal is to find a balance: a scale factor high enough to preserve small gradients, yet not so high that large gradients overflow and become <code class=\"language-plaintext highlighter-rouge\">inf</code>—maintaining a balance between underflow and overflow.</p>\n  </li>\n</ul>\n<p>A fundamental challenge of half-precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>) training is the limited dynamic range, which can cause small-magnitude gradients to underflow—i.e., round down to zero—during backpropagation. This occurs because when an operation receives <code class=\"language-plaintext highlighter-rouge\">float16</code> inputs in the forward pass, the resulting gradients computed in the backward pass are also in <code class=\"language-plaintext highlighter-rouge\">float16</code>, unless explicitly handled. In deep learning, many gradients—particularly in early layers or at later training stages—can be extremely small, and when these are flushed to zero, their corresponding weight updates are effectively lost, impeding learning.</p>\n<p>To mitigate this, PyTorch introduces <strong>loss scaling</strong>, a technique that amplifies loss values and their corresponding gradients during the backward pass to avoid underflow. The process works as follows:</p>\n<ol>\n      <li>The loss is multiplied by a scale factor before backpropagation.</li>\n      <li>Gradients are computed on this scaled loss, resulting in proportionally larger values.</li>\n      <li>These gradients are then unscaled before the optimizer applies the update, preserving the intended learning dynamics.</li>\n    </ol>\n<p>This technique is implemented via the <code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp.GradScaler</code> object, which automates both the scaling and unscaling process, as well as overflow detection and recovery. The goal is to find a balance: a scale factor high enough to preserve small gradients, yet not so high that large gradients overflow and become <code class=\"language-plaintext highlighter-rouge\">inf</code>—maintaining a balance between underflow and overflow.</p>\n<h6 id=\"dynamic-scaling-with-exponential-backoff\">Dynamic Scaling with Exponential Backoff</h6>\n<ul>\n  <li>\n    <p>There is no single static loss multiplier that suits all models or all stages of training. Gradient magnitudes are typically much larger at the beginning of training and diminish as convergence nears. Rather than asking users to manually tune this value, PyTorch uses an adaptive approach based on <strong>exponential backoff</strong>.</p>\n  </li>\n  <li>\n    <p>The <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> begins with an initial scale (default: 65,536 or <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-131-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>16</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1578\" style=\"width: 1.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.3em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1579\"><span class=\"msubsup\" id=\"MathJax-Span-1580\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1581\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1582\"><span class=\"mrow\" id=\"MathJax-Span-1583\"><span class=\"mn\" id=\"MathJax-Span-1584\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>16</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-131\">2^{16}</script>) and periodically doubles it to maximize numerical range. If an overflow is detected—i.e., any gradient becomes <code class=\"language-plaintext highlighter-rouge\">inf</code> or <code class=\"language-plaintext highlighter-rouge\">NaN</code>—the current update step is skipped, the scale is halved, and a cooldown counter is reset. This approach allows PyTorch to adaptively find a safe and efficient scaling factor over time, much like TCP congestion control adapts network throughput.</p>\n  </li>\n  <li>\n    <p>This behavior can be configured via the <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> constructor:</p>\n\n    <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code21\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code21\"><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">.</span><span class=\"n\">amp</span><span class=\"p\">.</span><span class=\"n\">GradScaler</span><span class=\"p\">(</span>\n    <span class=\"n\">init_scale</span><span class=\"o\">=</span><span class=\"mf\">65536.0</span><span class=\"p\">,</span>\n    <span class=\"n\">growth_factor</span><span class=\"o\">=</span><span class=\"mf\">2.0</span><span class=\"p\">,</span>\n    <span class=\"n\">backoff_factor</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span>\n    <span class=\"n\">growth_interval</span><span class=\"o\">=</span><span class=\"mi\">2000</span><span class=\"p\">,</span>\n    <span class=\"n\">enabled</span><span class=\"o\">=</span><span class=\"bp\">True</span>\n<span class=\"p\">)</span>\n</code></pre></div>    </div>\n\n    <ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">init_scale</code>: The initial scaling factor.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">growth_factor</code>: Multiplicative increase rate when no overflows are detected.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">backoff_factor</code>: Reduction factor when an overflow is detected.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">growth_interval</code>: Number of successful steps before scale growth is attempted.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">enabled</code>: Whether AMP and scaling are active.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Operational Considerations</strong>:</p>\n\n    <ul>\n      <li>\n        <p><code class=\"language-plaintext highlighter-rouge\">GradScaler</code> modifies key parts of the training loop:</p>\n\n        <ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">loss.backward()</code> becomes <code class=\"language-plaintext highlighter-rouge\">scaler.scale(loss).backward()</code></li>\n          <li><code class=\"language-plaintext highlighter-rouge\">optimizer.step()</code> becomes <code class=\"language-plaintext highlighter-rouge\">scaler.step(optimizer)</code></li>\n          <li>The call to <code class=\"language-plaintext highlighter-rouge\">scaler.update()</code> checks for overflows and adjusts the scale as needed.</li>\n        </ul>\n      </li>\n      <li>\n        <p>It is important to note that overflows (<code class=\"language-plaintext highlighter-rouge\">inf</code>) are detectable and trigger corrective behavior. Underflows, however, are silent because zero gradients are not always erroneous. Thus, choosing a very low initial scale or a very long growth interval may cause the network to silently underperform or diverge. PyTorch’s large default <code class=\"language-plaintext highlighter-rouge\">init_scale</code> mitigates this risk.</p>\n      </li>\n      <li>\n        <p>Internally, before the optimizer updates the model weights, the gradients (<code class=\"language-plaintext highlighter-rouge\">.grad</code>) are unscaled to ensure the learning rate and optimizer dynamics remain consistent with those expected in <code class=\"language-plaintext highlighter-rouge\">float32</code> training.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Checkpointing with GradScaler</strong>:</p>\n\n    <ul>\n      <li>Because <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> is a stateful object that adapts over time, it must be saved and restored along with the model and optimizer during checkpointing. PyTorch provides simple APIs for this:</li>\n    </ul>\n\n    <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code22\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code22\"><span class=\"c1\"># Saving\n</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">({</span>\n    <span class=\"s\">'model_state_dict'</span><span class=\"p\">:</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">state_dict</span><span class=\"p\">(),</span>\n    <span class=\"s\">'optimizer_state_dict'</span><span class=\"p\">:</span> <span class=\"n\">optimizer</span><span class=\"p\">.</span><span class=\"n\">state_dict</span><span class=\"p\">(),</span>\n    <span class=\"s\">'scaler_state_dict'</span><span class=\"p\">:</span> <span class=\"n\">scaler</span><span class=\"p\">.</span><span class=\"n\">state_dict</span><span class=\"p\">()</span>\n<span class=\"p\">},</span> <span class=\"s\">'checkpoint.pt'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Loading\n</span><span class=\"n\">checkpoint</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s\">'checkpoint.pt'</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">load_state_dict</span><span class=\"p\">(</span><span class=\"n\">checkpoint</span><span class=\"p\">[</span><span class=\"s\">'model_state_dict'</span><span class=\"p\">])</span>\n<span class=\"n\">optimizer</span><span class=\"p\">.</span><span class=\"n\">load_state_dict</span><span class=\"p\">(</span><span class=\"n\">checkpoint</span><span class=\"p\">[</span><span class=\"s\">'optimizer_state_dict'</span><span class=\"p\">])</span>\n<span class=\"n\">scaler</span><span class=\"p\">.</span><span class=\"n\">load_state_dict</span><span class=\"p\">(</span><span class=\"n\">checkpoint</span><span class=\"p\">[</span><span class=\"s\">'scaler_state_dict'</span><span class=\"p\">])</span>\n</code></pre></div>    </div>\n\n    <ul>\n      <li>By integrating <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> into the training process, PyTorch ensures that the numerical precision limitations of <code class=\"language-plaintext highlighter-rouge\">float16</code> do not compromise convergence, while still allowing significant performance and memory efficiency gains.</li>\n    </ul>\n  </li>\n</ul>\n<p>There is no single static loss multiplier that suits all models or all stages of training. Gradient magnitudes are typically much larger at the beginning of training and diminish as convergence nears. Rather than asking users to manually tune this value, PyTorch uses an adaptive approach based on <strong>exponential backoff</strong>.</p>\n<p>The <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> begins with an initial scale (default: 65,536 or <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-131-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>2</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>16</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1578\" style=\"width: 1.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.3em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1579\"><span class=\"msubsup\" id=\"MathJax-Span-1580\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1581\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1582\"><span class=\"mrow\" id=\"MathJax-Span-1583\"><span class=\"mn\" id=\"MathJax-Span-1584\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">16</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>2</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>16</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-131\">2^{16}</script>) and periodically doubles it to maximize numerical range. If an overflow is detected—i.e., any gradient becomes <code class=\"language-plaintext highlighter-rouge\">inf</code> or <code class=\"language-plaintext highlighter-rouge\">NaN</code>—the current update step is skipped, the scale is halved, and a cooldown counter is reset. This approach allows PyTorch to adaptively find a safe and efficient scaling factor over time, much like TCP congestion control adapts network throughput.</p>\n<p>This behavior can be configured via the <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> constructor:</p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code21\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code21\"><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">.</span><span class=\"n\">amp</span><span class=\"p\">.</span><span class=\"n\">GradScaler</span><span class=\"p\">(</span>\n    <span class=\"n\">init_scale</span><span class=\"o\">=</span><span class=\"mf\">65536.0</span><span class=\"p\">,</span>\n    <span class=\"n\">growth_factor</span><span class=\"o\">=</span><span class=\"mf\">2.0</span><span class=\"p\">,</span>\n    <span class=\"n\">backoff_factor</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span>\n    <span class=\"n\">growth_interval</span><span class=\"o\">=</span><span class=\"mi\">2000</span><span class=\"p\">,</span>\n    <span class=\"n\">enabled</span><span class=\"o\">=</span><span class=\"bp\">True</span>\n<span class=\"p\">)</span>\n</code></pre>\n<ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">init_scale</code>: The initial scaling factor.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">growth_factor</code>: Multiplicative increase rate when no overflows are detected.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">backoff_factor</code>: Reduction factor when an overflow is detected.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">growth_interval</code>: Number of successful steps before scale growth is attempted.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">enabled</code>: Whether AMP and scaling are active.</li>\n    </ul>\n<p><strong>Operational Considerations</strong>:</p>\n<ul>\n      <li>\n        <p><code class=\"language-plaintext highlighter-rouge\">GradScaler</code> modifies key parts of the training loop:</p>\n\n        <ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">loss.backward()</code> becomes <code class=\"language-plaintext highlighter-rouge\">scaler.scale(loss).backward()</code></li>\n          <li><code class=\"language-plaintext highlighter-rouge\">optimizer.step()</code> becomes <code class=\"language-plaintext highlighter-rouge\">scaler.step(optimizer)</code></li>\n          <li>The call to <code class=\"language-plaintext highlighter-rouge\">scaler.update()</code> checks for overflows and adjusts the scale as needed.</li>\n        </ul>\n      </li>\n      <li>\n        <p>It is important to note that overflows (<code class=\"language-plaintext highlighter-rouge\">inf</code>) are detectable and trigger corrective behavior. Underflows, however, are silent because zero gradients are not always erroneous. Thus, choosing a very low initial scale or a very long growth interval may cause the network to silently underperform or diverge. PyTorch’s large default <code class=\"language-plaintext highlighter-rouge\">init_scale</code> mitigates this risk.</p>\n      </li>\n      <li>\n        <p>Internally, before the optimizer updates the model weights, the gradients (<code class=\"language-plaintext highlighter-rouge\">.grad</code>) are unscaled to ensure the learning rate and optimizer dynamics remain consistent with those expected in <code class=\"language-plaintext highlighter-rouge\">float32</code> training.</p>\n      </li>\n    </ul>\n<p><code class=\"language-plaintext highlighter-rouge\">GradScaler</code> modifies key parts of the training loop:</p>\n<ul>\n          <li><code class=\"language-plaintext highlighter-rouge\">loss.backward()</code> becomes <code class=\"language-plaintext highlighter-rouge\">scaler.scale(loss).backward()</code></li>\n          <li><code class=\"language-plaintext highlighter-rouge\">optimizer.step()</code> becomes <code class=\"language-plaintext highlighter-rouge\">scaler.step(optimizer)</code></li>\n          <li>The call to <code class=\"language-plaintext highlighter-rouge\">scaler.update()</code> checks for overflows and adjusts the scale as needed.</li>\n        </ul>\n<p>It is important to note that overflows (<code class=\"language-plaintext highlighter-rouge\">inf</code>) are detectable and trigger corrective behavior. Underflows, however, are silent because zero gradients are not always erroneous. Thus, choosing a very low initial scale or a very long growth interval may cause the network to silently underperform or diverge. PyTorch’s large default <code class=\"language-plaintext highlighter-rouge\">init_scale</code> mitigates this risk.</p>\n<p>Internally, before the optimizer updates the model weights, the gradients (<code class=\"language-plaintext highlighter-rouge\">.grad</code>) are unscaled to ensure the learning rate and optimizer dynamics remain consistent with those expected in <code class=\"language-plaintext highlighter-rouge\">float32</code> training.</p>\n<p><strong>Checkpointing with GradScaler</strong>:</p>\n<ul>\n      <li>Because <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> is a stateful object that adapts over time, it must be saved and restored along with the model and optimizer during checkpointing. PyTorch provides simple APIs for this:</li>\n    </ul>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code22\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code22\"><span class=\"c1\"># Saving\n</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">({</span>\n    <span class=\"s\">'model_state_dict'</span><span class=\"p\">:</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">state_dict</span><span class=\"p\">(),</span>\n    <span class=\"s\">'optimizer_state_dict'</span><span class=\"p\">:</span> <span class=\"n\">optimizer</span><span class=\"p\">.</span><span class=\"n\">state_dict</span><span class=\"p\">(),</span>\n    <span class=\"s\">'scaler_state_dict'</span><span class=\"p\">:</span> <span class=\"n\">scaler</span><span class=\"p\">.</span><span class=\"n\">state_dict</span><span class=\"p\">()</span>\n<span class=\"p\">},</span> <span class=\"s\">'checkpoint.pt'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Loading\n</span><span class=\"n\">checkpoint</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s\">'checkpoint.pt'</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">load_state_dict</span><span class=\"p\">(</span><span class=\"n\">checkpoint</span><span class=\"p\">[</span><span class=\"s\">'model_state_dict'</span><span class=\"p\">])</span>\n<span class=\"n\">optimizer</span><span class=\"p\">.</span><span class=\"n\">load_state_dict</span><span class=\"p\">(</span><span class=\"n\">checkpoint</span><span class=\"p\">[</span><span class=\"s\">'optimizer_state_dict'</span><span class=\"p\">])</span>\n<span class=\"n\">scaler</span><span class=\"p\">.</span><span class=\"n\">load_state_dict</span><span class=\"p\">(</span><span class=\"n\">checkpoint</span><span class=\"p\">[</span><span class=\"s\">'scaler_state_dict'</span><span class=\"p\">])</span>\n</code></pre>\n<ul>\n      <li>By integrating <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> into the training process, PyTorch ensures that the numerical precision limitations of <code class=\"language-plaintext highlighter-rouge\">float16</code> do not compromise convergence, while still allowing significant performance and memory efficiency gains.</li>\n    </ul>\n<h5 id=\"automatic-precision-casting-with-the-autocast-context-manager\">Automatic Precision Casting with the <code class=\"language-plaintext Highlighter-rouge\">autocast</code> Context Manager</h5>\n<ul>\n  <li>\n    <p>The second key component of PyTorch’s AMP system is the <code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp.autocast</code> context manager. While <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> addresses numerical stability during backpropagation via loss scaling, <code class=\"language-plaintext highlighter-rouge\">autocast</code> is responsible for precision control during the forward pass.</p>\n  </li>\n  <li>\n    <p>Mixed precision training derives its speed and memory benefits primarily by executing selected operations in <code class=\"language-plaintext highlighter-rouge\">float16</code> rather than <code class=\"language-plaintext highlighter-rouge\">float32</code>. However, not all operations are equally safe or efficient in half precision. Some are numerically stable and performant when cast to <code class=\"language-plaintext highlighter-rouge\">float16</code>, while others require higher precision to avoid instability or incorrect outputs.</p>\n  </li>\n  <li>\n    <p>The <code class=\"language-plaintext highlighter-rouge\">autocast</code> context manager dynamically casts operations to the most appropriate precision at runtime. This casting is done based on an internal whitelist/blacklist system defined by PyTorch, taking into account both the operation type and the tensor data types involved. This enables users to delegate dtype management to PyTorch, avoiding manual casting and type-checking logic.</p>\n  </li>\n  <li>\n    <p><strong>How <code class=\"language-plaintext highlighter-rouge\">autocast</code> Works Internally</strong>:</p>\n\n    <ul>\n      <li>Operations such as matrix multiplications (<code class=\"language-plaintext highlighter-rouge\">matmul</code>), convolutions (<code class=\"language-plaintext highlighter-rouge\">conv2d</code>), and other linear algebraic operations are generally safe to perform in <code class=\"language-plaintext highlighter-rouge\">float16</code>, and thus are automatically downcast when inside an <code class=\"language-plaintext highlighter-rouge\">autocast</code> context.</li>\n      <li>\n        <p>Conversely, operations that are sensitive to numerical precision—such as logarithms, exponentials, trigonometric functions, and large summations—are retained in <code class=\"language-plaintext highlighter-rouge\">float32</code> to ensure computational accuracy.</p>\n      </li>\n      <li>The following visuals (<a href=\"https://pytorch.org/docs/master/amp.html#autocast-op-reference\">source</a>), summarize these distinctions. The image below outlines common operations that benefit from <code class=\"language-plaintext highlighter-rouge\">float16</code> execution. These include core building blocks of deep learning models like matrix multiplications, dot products, and convolutions. Their stability in half precision makes them ideal candidates for mixed precision acceleration.</li>\n    </ul>\n\n    <p><img src=\"/primers/ai/assets/model-compression/ops_widest.avif\" alt=\"\"></p>\n\n    <ul>\n      <li>In contrast, as shown in the image below (<a href=\"https://pytorch.org/docs/master/amp.html#autocast-op-reference\">source</a>), operations involving logarithms, exponentials, or statistical reductions tend to suffer from rounding errors in <code class=\"language-plaintext highlighter-rouge\">float16</code> and are therefore retained in <code class=\"language-plaintext highlighter-rouge\">float32</code>.</li>\n    </ul>\n\n    <p><img src=\"/primers/ai/assets/model-compression/autocast_f32.avif\" alt=\"\"></p>\n  </li>\n  <li>\n    <p><strong>Implications for Model Layers</strong>:</p>\n\n    <ul>\n      <li>\n        <p>These rules imply that:</p>\n\n        <ul>\n          <li>Most <strong>layers</strong> (e.g., linear, convolutional, attention) benefit substantially from autocasting, due to their reliance on matrix operations.</li>\n          <li>Most <strong>activation functions</strong> and <strong>normalization layers</strong> are less safe in <code class=\"language-plaintext highlighter-rouge\">float16</code>, and autocast will retain full precision where necessary.</li>\n          <li>The <strong>greatest performance gains</strong> are likely in deep CNNs or transformer models with many linear operations and matrix multiplications.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Using <code class=\"language-plaintext highlighter-rouge\">autocast</code> in Practice</strong>:</p>\n\n    <ul>\n      <li>Enabling autocasting is simple and requires wrapping the forward pass in a context manager:</li>\n    </ul>\n\n    <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code23\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code23\"><span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">.</span><span class=\"n\">amp</span><span class=\"p\">.</span><span class=\"n\">autocast</span><span class=\"p\">():</span>\n    <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">X_batch</span><span class=\"p\">).</span><span class=\"n\">squeeze</span><span class=\"p\">()</span>\n    <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">y_pred</span><span class=\"p\">,</span> <span class=\"n\">y_batch</span><span class=\"p\">)</span>\n</code></pre></div>    </div>\n\n    <ul>\n      <li>All operations within the <code class=\"language-plaintext highlighter-rouge\">autocast()</code> context will be executed with optimal mixed precision, determined internally by PyTorch. Importantly, this casting behavior extends to the backward pass automatically—there is no need to wrap <code class=\"language-plaintext highlighter-rouge\">loss.backward()</code>.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Best Practices and Notes</strong>:</p>\n\n    <ul>\n      <li>Autocast respects and supports a wide range of PyTorch operators out-of-the-box. Unless using custom operations or extensions, most models will run correctly without additional intervention.</li>\n      <li>In-place operations (e.g., <code class=\"language-plaintext highlighter-rouge\">.add_()</code> or <code class=\"language-plaintext highlighter-rouge\">.relu_()</code>) can interfere with autocast’s internal precision control. Avoid in-place modifications inside <code class=\"language-plaintext highlighter-rouge\">autocast()</code> blocks unless explicitly supported.</li>\n      <li>Autocast is deterministic and composable. It can be used inside model layers, training loops, or custom modules with consistent behavior.</li>\n      <li>For inference scenarios, autocasting is also beneficial and can be enabled in evaluation mode to reduce memory usage without requiring <code class=\"language-plaintext highlighter-rouge\">GradScaler</code>.</li>\n    </ul>\n  </li>\n</ul>\n<p>The second key component of PyTorch’s AMP system is the <code class=\"language-plaintext highlighter-rouge\">torch.cuda.amp.autocast</code> context manager. While <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> addresses numerical stability during backpropagation via loss scaling, <code class=\"language-plaintext highlighter-rouge\">autocast</code> is responsible for precision control during the forward pass.</p>\n<p>Mixed precision training derives its speed and memory benefits primarily by executing selected operations in <code class=\"language-plaintext highlighter-rouge\">float16</code> rather than <code class=\"language-plaintext highlighter-rouge\">float32</code>. However, not all operations are equally safe or efficient in half precision. Some are numerically stable and performant when cast to <code class=\"language-plaintext highlighter-rouge\">float16</code>, while others require higher precision to avoid instability or incorrect outputs.</p>\n<p>The <code class=\"language-plaintext highlighter-rouge\">autocast</code> context manager dynamically casts operations to the most appropriate precision at runtime. This casting is done based on an internal whitelist/blacklist system defined by PyTorch, taking into account both the operation type and the tensor data types involved. This enables users to delegate dtype management to PyTorch, avoiding manual casting and type-checking logic.</p>\n<p><strong>How <code class=\"language-plaintext highlighter-rouge\">autocast</code> Works Internally</strong>:</p>\n<ul>\n      <li>Operations such as matrix multiplications (<code class=\"language-plaintext highlighter-rouge\">matmul</code>), convolutions (<code class=\"language-plaintext highlighter-rouge\">conv2d</code>), and other linear algebraic operations are generally safe to perform in <code class=\"language-plaintext highlighter-rouge\">float16</code>, and thus are automatically downcast when inside an <code class=\"language-plaintext highlighter-rouge\">autocast</code> context.</li>\n      <li>\n        <p>Conversely, operations that are sensitive to numerical precision—such as logarithms, exponentials, trigonometric functions, and large summations—are retained in <code class=\"language-plaintext highlighter-rouge\">float32</code> to ensure computational accuracy.</p>\n      </li>\n      <li>The following visuals (<a href=\"https://pytorch.org/docs/master/amp.html#autocast-op-reference\">source</a>), summarize these distinctions. The image below outlines common operations that benefit from <code class=\"language-plaintext highlighter-rouge\">float16</code> execution. These include core building blocks of deep learning models like matrix multiplications, dot products, and convolutions. Their stability in half precision makes them ideal candidates for mixed precision acceleration.</li>\n    </ul>\n<p>Conversely, operations that are sensitive to numerical precision—such as logarithms, exponentials, trigonometric functions, and large summations—are retained in <code class=\"language-plaintext highlighter-rouge\">float32</code> to ensure computational accuracy.</p>\n<p><img src=\"/primers/ai/assets/model-compression/ops_widest.avif\" alt=\"\"></p>\n<ul>\n      <li>In contrast, as shown in the image below (<a href=\"https://pytorch.org/docs/master/amp.html#autocast-op-reference\">source</a>), operations involving logarithms, exponentials, or statistical reductions tend to suffer from rounding errors in <code class=\"language-plaintext highlighter-rouge\">float16</code> and are therefore retained in <code class=\"language-plaintext highlighter-rouge\">float32</code>.</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/model-compression/autocast_f32.avif\" alt=\"\"></p>\n<p><strong>Implications for Model Layers</strong>:</p>\n<ul>\n      <li>\n        <p>These rules imply that:</p>\n\n        <ul>\n          <li>Most <strong>layers</strong> (e.g., linear, convolutional, attention) benefit substantially from autocasting, due to their reliance on matrix operations.</li>\n          <li>Most <strong>activation functions</strong> and <strong>normalization layers</strong> are less safe in <code class=\"language-plaintext highlighter-rouge\">float16</code>, and autocast will retain full precision where necessary.</li>\n          <li>The <strong>greatest performance gains</strong> are likely in deep CNNs or transformer models with many linear operations and matrix multiplications.</li>\n        </ul>\n      </li>\n    </ul>\n<p>These rules imply that:</p>\n<ul>\n          <li>Most <strong>layers</strong> (e.g., linear, convolutional, attention) benefit substantially from autocasting, due to their reliance on matrix operations.</li>\n          <li>Most <strong>activation functions</strong> and <strong>normalization layers</strong> are less safe in <code class=\"language-plaintext highlighter-rouge\">float16</code>, and autocast will retain full precision where necessary.</li>\n          <li>The <strong>greatest performance gains</strong> are likely in deep CNNs or transformer models with many linear operations and matrix multiplications.</li>\n        </ul>\n<p><strong>Using <code class=\"language-plaintext highlighter-rouge\">autocast</code> in Practice</strong>:</p>\n<ul>\n      <li>Enabling autocasting is simple and requires wrapping the forward pass in a context manager:</li>\n    </ul>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code23\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code23\"><span class=\"k\">with</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">cuda</span><span class=\"p\">.</span><span class=\"n\">amp</span><span class=\"p\">.</span><span class=\"n\">autocast</span><span class=\"p\">():</span>\n    <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"n\">X_batch</span><span class=\"p\">).</span><span class=\"n\">squeeze</span><span class=\"p\">()</span>\n    <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">y_pred</span><span class=\"p\">,</span> <span class=\"n\">y_batch</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n      <li>All operations within the <code class=\"language-plaintext highlighter-rouge\">autocast()</code> context will be executed with optimal mixed precision, determined internally by PyTorch. Importantly, this casting behavior extends to the backward pass automatically—there is no need to wrap <code class=\"language-plaintext highlighter-rouge\">loss.backward()</code>.</li>\n    </ul>\n<p><strong>Best Practices and Notes</strong>:</p>\n<ul>\n      <li>Autocast respects and supports a wide range of PyTorch operators out-of-the-box. Unless using custom operations or extensions, most models will run correctly without additional intervention.</li>\n      <li>In-place operations (e.g., <code class=\"language-plaintext highlighter-rouge\">.add_()</code> or <code class=\"language-plaintext highlighter-rouge\">.relu_()</code>) can interfere with autocast’s internal precision control. Avoid in-place modifications inside <code class=\"language-plaintext highlighter-rouge\">autocast()</code> blocks unless explicitly supported.</li>\n      <li>Autocast is deterministic and composable. It can be used inside model layers, training loops, or custom modules with consistent behavior.</li>\n      <li>For inference scenarios, autocasting is also beneficial and can be enabled in evaluation mode to reduce memory usage without requiring <code class=\"language-plaintext highlighter-rouge\">GradScaler</code>.</li>\n    </ul>\n<h5 id=\"using-amp-with-multiple-gpus\">Using AMP with Multiple GPUs</h5>\n<ul>\n  <li>\n    <p>PyTorch’s Automatic Mixed Precision (AMP) functionality is fully compatible with multi-GPU training, enabling developers to scale up performance without sacrificing the benefits of mixed precision. Both of PyTorch’s multi-GPU parallelization strategies—<code class=\"language-plaintext highlighter-rouge\">DistributedDataParallel</code> (DDP) and <code class=\"language-plaintext highlighter-rouge\">DataParallel</code>—support autocasting and gradient scaling, with minimal adjustments.</p>\n  </li>\n  <li>\n    <p>AMP’s multi-GPU support is robust and integrates seamlessly into distributed training workflows. With only minor adjustments, developers can leverage both horizontal scaling and mixed precision optimization, achieving faster training with efficient GPU utilization across multiple devices.</p>\n\n    <ul>\n      <li>\n        <p><strong>DistributedDataParallel (DDP)</strong>: AMP works out-of-the-box with DDP, which is the recommended strategy for multi-GPU training. The key requirement is to use one process per GPU, following the standard setup for DDP. This ensures independent autocast and <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> instances per GPU, maintaining stability and efficiency.</p>\n      </li>\n      <li>\n        <p><strong>DataParallel</strong>: AMP also works with <code class=\"language-plaintext highlighter-rouge\">DataParallel</code>, but with a caveat. Since <code class=\"language-plaintext highlighter-rouge\">DataParallel</code> uses a single process to drive multiple devices, it shares the autocast and scaling logic across GPUs. To accommodate this, one small adjustment must be made as outlined in the official <a href=\"https://pytorch.org/docs/master/notes/amp_examples.html#dataparallel-in-a-single-process\">AMP Examples</a> guide. Specifically, ensure that loss scaling is only performed on the output of the model’s <code class=\"language-plaintext highlighter-rouge\">.forward()</code> call on the main device, before broadcasting gradients.</p>\n      </li>\n      <li>\n        <p><strong>Implementation Tips</strong>:</p>\n\n        <ul>\n          <li>Refer to the <a href=\"https://pytorch.org/docs/master/notes/amp_examples.html#working-with-multiple-gpus\">Working with Multiple GPUs</a> section in the PyTorch AMP documentation for detailed examples and best practices.</li>\n          <li>Be mindful of numerical stability when using binary classification loss functions. The AMP documentation recommends <a href=\"https://pytorch.org/docs/master/amp.html#prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy\">preferring binary cross entropy with logits over binary cross entropy</a>, as the logits version is more numerically stable and better suited for mixed precision.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p>PyTorch’s Automatic Mixed Precision (AMP) functionality is fully compatible with multi-GPU training, enabling developers to scale up performance without sacrificing the benefits of mixed precision. Both of PyTorch’s multi-GPU parallelization strategies—<code class=\"language-plaintext highlighter-rouge\">DistributedDataParallel</code> (DDP) and <code class=\"language-plaintext highlighter-rouge\">DataParallel</code>—support autocasting and gradient scaling, with minimal adjustments.</p>\n<p>AMP’s multi-GPU support is robust and integrates seamlessly into distributed training workflows. With only minor adjustments, developers can leverage both horizontal scaling and mixed precision optimization, achieving faster training with efficient GPU utilization across multiple devices.</p>\n<ul>\n      <li>\n        <p><strong>DistributedDataParallel (DDP)</strong>: AMP works out-of-the-box with DDP, which is the recommended strategy for multi-GPU training. The key requirement is to use one process per GPU, following the standard setup for DDP. This ensures independent autocast and <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> instances per GPU, maintaining stability and efficiency.</p>\n      </li>\n      <li>\n        <p><strong>DataParallel</strong>: AMP also works with <code class=\"language-plaintext highlighter-rouge\">DataParallel</code>, but with a caveat. Since <code class=\"language-plaintext highlighter-rouge\">DataParallel</code> uses a single process to drive multiple devices, it shares the autocast and scaling logic across GPUs. To accommodate this, one small adjustment must be made as outlined in the official <a href=\"https://pytorch.org/docs/master/notes/amp_examples.html#dataparallel-in-a-single-process\">AMP Examples</a> guide. Specifically, ensure that loss scaling is only performed on the output of the model’s <code class=\"language-plaintext highlighter-rouge\">.forward()</code> call on the main device, before broadcasting gradients.</p>\n      </li>\n      <li>\n        <p><strong>Implementation Tips</strong>:</p>\n\n        <ul>\n          <li>Refer to the <a href=\"https://pytorch.org/docs/master/notes/amp_examples.html#working-with-multiple-gpus\">Working with Multiple GPUs</a> section in the PyTorch AMP documentation for detailed examples and best practices.</li>\n          <li>Be mindful of numerical stability when using binary classification loss functions. The AMP documentation recommends <a href=\"https://pytorch.org/docs/master/amp.html#prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy\">preferring binary cross entropy with logits over binary cross entropy</a>, as the logits version is more numerically stable and better suited for mixed precision.</li>\n        </ul>\n      </li>\n    </ul>\n<p><strong>DistributedDataParallel (DDP)</strong>: AMP works out-of-the-box with DDP, which is the recommended strategy for multi-GPU training. The key requirement is to use one process per GPU, following the standard setup for DDP. This ensures independent autocast and <code class=\"language-plaintext highlighter-rouge\">GradScaler</code> instances per GPU, maintaining stability and efficiency.</p>\n<p><strong>DataParallel</strong>: AMP also works with <code class=\"language-plaintext highlighter-rouge\">DataParallel</code>, but with a caveat. Since <code class=\"language-plaintext highlighter-rouge\">DataParallel</code> uses a single process to drive multiple devices, it shares the autocast and scaling logic across GPUs. To accommodate this, one small adjustment must be made as outlined in the official <a href=\"https://pytorch.org/docs/master/notes/amp_examples.html#dataparallel-in-a-single-process\">AMP Examples</a> guide. Specifically, ensure that loss scaling is only performed on the output of the model’s <code class=\"language-plaintext highlighter-rouge\">.forward()</code> call on the main device, before broadcasting gradients.</p>\n<p><strong>Implementation Tips</strong>:</p>\n<ul>\n          <li>Refer to the <a href=\"https://pytorch.org/docs/master/notes/amp_examples.html#working-with-multiple-gpus\">Working with Multiple GPUs</a> section in the PyTorch AMP documentation for detailed examples and best practices.</li>\n          <li>Be mindful of numerical stability when using binary classification loss functions. The AMP documentation recommends <a href=\"https://pytorch.org/docs/master/amp.html#prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy\">preferring binary cross entropy with logits over binary cross entropy</a>, as the logits version is more numerically stable and better suited for mixed precision.</li>\n        </ul>\n<h5 id=\"memory-considerations\">Memory Considerations</h5>\n<ul>\n  <li>\n    <p>One of the advertised benefits of mixed precision training, in addition to performance speedups, is reduced GPU memory consumption. As discussed in the earlier section on <a href=\"#how-mixed-precision-works\">How Mixed Precision Works</a>, <code class=\"language-plaintext highlighter-rouge\">float16</code> tensors require half the storage space of their <code class=\"language-plaintext highlighter-rouge\">float32</code> counterparts. This reduction in memory footprint can be particularly advantageous in training large-scale models, where memory constraints often limit batch size or model complexity.</p>\n  </li>\n  <li>\n    <p>Although GPU compute is generally the primary bottleneck in training workloads, optimizing memory usage remains important. Efficient memory utilization enables:</p>\n\n    <ul>\n      <li>Larger batch sizes, which can improve training stability and convergence.</li>\n      <li>The ability to fit deeper or wider models within available hardware constraints.</li>\n      <li>Reduced reliance on gradient checkpointing or memory-efficient architectures.</li>\n    </ul>\n  </li>\n  <li>\n    <p>PyTorch manages GPU memory allocation proactively. At the start of training, it reserves a block of GPU memory that it maintains throughout the training lifecycle. This behavior helps avoid runtime memory fragmentation and preempts crashes caused by other processes occupying memory mid-training. However, it also means that the effect of mixed precision on memory usage may not always be visible in a straightforward manner.</p>\n  </li>\n  <li>\n    <p>The figure below illustrates PyTorch’s memory reservation behavior with and without AMP enabled:</p>\n  </li>\n</ul>\n<p>One of the advertised benefits of mixed precision training, in addition to performance speedups, is reduced GPU memory consumption. As discussed in the earlier section on <a href=\"#how-mixed-precision-works\">How Mixed Precision Works</a>, <code class=\"language-plaintext highlighter-rouge\">float16</code> tensors require half the storage space of their <code class=\"language-plaintext highlighter-rouge\">float32</code> counterparts. This reduction in memory footprint can be particularly advantageous in training large-scale models, where memory constraints often limit batch size or model complexity.</p>\n<p>Although GPU compute is generally the primary bottleneck in training workloads, optimizing memory usage remains important. Efficient memory utilization enables:</p>\n<ul>\n      <li>Larger batch sizes, which can improve training stability and convergence.</li>\n      <li>The ability to fit deeper or wider models within available hardware constraints.</li>\n      <li>Reduced reliance on gradient checkpointing or memory-efficient architectures.</li>\n    </ul>\n<p>PyTorch manages GPU memory allocation proactively. At the start of training, it reserves a block of GPU memory that it maintains throughout the training lifecycle. This behavior helps avoid runtime memory fragmentation and preempts crashes caused by other processes occupying memory mid-training. However, it also means that the effect of mixed precision on memory usage may not always be visible in a straightforward manner.</p>\n<p>The figure below illustrates PyTorch’s memory reservation behavior with and without AMP enabled:</p>\n<p><img src=\"/primers/ai/assets/model-compression/amp_mem.avif\" alt=\"\"></p>\n<ul>\n  <li>\n    <p>Interestingly, while both UNet and BERT models exhibit a reduction in memory usage when AMP is enabled, the gains are model-dependent. UNet, in particular, benefits significantly more than BERT. This discrepancy may result from differences in internal layer composition, memory allocation patterns, or the proportion of operations compatible with <code class=\"language-plaintext highlighter-rouge\">float16</code>. PyTorch’s memory allocator is largely opaque, making it difficult to pinpoint exact causes without in-depth profiling.</p>\n  </li>\n  <li>\n    <p>Nonetheless, practitioners can generally expect mixed precision to reduce overall memory usage, especially in convolution-heavy models like UNet. This makes AMP not only a tool for acceleration but also a practical memory optimization strategy, particularly beneficial for users working within the limits of consumer-grade GPUs or training on multiple models in parallel.</p>\n  </li>\n</ul>\n<p>Interestingly, while both UNet and BERT models exhibit a reduction in memory usage when AMP is enabled, the gains are model-dependent. UNet, in particular, benefits significantly more than BERT. This discrepancy may result from differences in internal layer composition, memory allocation patterns, or the proportion of operations compatible with <code class=\"language-plaintext highlighter-rouge\">float16</code>. PyTorch’s memory allocator is largely opaque, making it difficult to pinpoint exact causes without in-depth profiling.</p>\n<p>Nonetheless, practitioners can generally expect mixed precision to reduce overall memory usage, especially in convolution-heavy models like UNet. This makes AMP not only a tool for acceleration but also a practical memory optimization strategy, particularly beneficial for users working within the limits of consumer-grade GPUs or training on multiple models in parallel.</p>\n<h5 id=\"further-reading-2\">Further Reading</h5>\n<ul>\n  <li>\n    <p>For further reading and in-depth examples, consult the official documentation:</p>\n\n    <ul>\n      <li><a href=\"https://pytorch.org/docs/master/amp.html\">Automatic Mixed Precision package</a></li>\n      <li><a href=\"https://pytorch.org/docs/master/notes/amp_examples.html\">Automatic Mixed Precision examples</a></li>\n    </ul>\n  </li>\n</ul>\n<p>For further reading and in-depth examples, consult the official documentation:</p>\n<ul>\n      <li><a href=\"https://pytorch.org/docs/master/amp.html\">Automatic Mixed Precision package</a></li>\n      <li><a href=\"https://pytorch.org/docs/master/notes/amp_examples.html\">Automatic Mixed Precision examples</a></li>\n    </ul>\n<h4 id=\"how-tensorflow-automatic-mixed-precision-works\">How TensorFlow Automatic Mixed Precision Works</h4>\n<ul>\n  <li>\n    <p>Mixed precision training in TensorFlow is designed to accelerate deep learning workloads by leveraging the efficiency of lower-precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>) arithmetic on supported hardware. With automatic mixed precision (AMP), TensorFlow streamlines the use of mixed-precision computation while preserving numerical stability and minimizing manual intervention.</p>\n  </li>\n  <li>\n    <p>TensorFlow’s AMP support offers a robust, efficient, and production-ready pathway to accelerate model training with minimal code changes, using a hybrid approach similar to PyTorch. Operations are executed in <code class=\"language-plaintext highlighter-rouge\">float16</code> where safe for performance and memory efficiency, while numerically sensitive computations remain in <code class=\"language-plaintext highlighter-rouge\">float32</code> to preserve stability and ensure convergence.</p>\n  </li>\n  <li>\n    <p>Built around the <code class=\"language-plaintext highlighter-rouge\">mixed_precision</code> module, TensorFlow provides a high-level, intuitive interface for enabling efficient mixed-precision training. By leveraging global policies and loss scaling under the hood, TensorFlow abstracts away much of the complexity involved in training with <code class=\"language-plaintext highlighter-rouge\">float16</code>. No manual casting or scaling logic is needed for most models, making integration straightforward for both experimentation and production. These features bring the practical benefits of faster computation and reduced memory usage—especially when training on modern GPUs—while maintaining the reliability of full-precision training. For a complete guide and reference examples, refer to the <a href=\"https://www.tensorflow.org/guide/mixed_precision\">official TensorFlow mixed precision guide</a>.</p>\n  </li>\n</ul>\n<p>Mixed precision training in TensorFlow is designed to accelerate deep learning workloads by leveraging the efficiency of lower-precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>) arithmetic on supported hardware. With automatic mixed precision (AMP), TensorFlow streamlines the use of mixed-precision computation while preserving numerical stability and minimizing manual intervention.</p>\n<p>TensorFlow’s AMP support offers a robust, efficient, and production-ready pathway to accelerate model training with minimal code changes, using a hybrid approach similar to PyTorch. Operations are executed in <code class=\"language-plaintext highlighter-rouge\">float16</code> where safe for performance and memory efficiency, while numerically sensitive computations remain in <code class=\"language-plaintext highlighter-rouge\">float32</code> to preserve stability and ensure convergence.</p>\n<p>Built around the <code class=\"language-plaintext highlighter-rouge\">mixed_precision</code> module, TensorFlow provides a high-level, intuitive interface for enabling efficient mixed-precision training. By leveraging global policies and loss scaling under the hood, TensorFlow abstracts away much of the complexity involved in training with <code class=\"language-plaintext highlighter-rouge\">float16</code>. No manual casting or scaling logic is needed for most models, making integration straightforward for both experimentation and production. These features bring the practical benefits of faster computation and reduced memory usage—especially when training on modern GPUs—while maintaining the reliability of full-precision training. For a complete guide and reference examples, refer to the <a href=\"https://www.tensorflow.org/guide/mixed_precision\">official TensorFlow mixed precision guide</a>.</p>\n<h5 id=\"conceptual-overview\">Conceptual Overview</h5>\n<ul>\n  <li>\n    <p>Mixed precision training in TensorFlow works by executing computations in half-precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>) where safe, and in single-precision (<code class=\"language-plaintext highlighter-rouge\">float32</code>) where required for numerical stability. This selective usage of data types reduces memory bandwidth and speeds up computation, particularly on GPUs equipped with NVIDIA Tensor Cores (e.g., V100, T4, A100). These architectures are specifically designed to handle mixed-precision workloads efficiently.</p>\n  </li>\n  <li>\n    <p>The two core features of TensorFlow’s AMP system are:</p>\n\n    <ul>\n      <li>\n        <p><strong>Global Policy Management</strong>: Mixed precision is enabled by setting a global or per-layer dtype policy to <code class=\"language-plaintext highlighter-rouge\">'mixed_float16'</code>. This instructs TensorFlow to automatically cast eligible operations to <code class=\"language-plaintext highlighter-rouge\">float16</code> while retaining critical variables (e.g., weights, certain accumulators) in <code class=\"language-plaintext highlighter-rouge\">float32</code>.</p>\n      </li>\n      <li>\n        <p><strong>Loss Scaling with <code class=\"language-plaintext highlighter-rouge\">LossScaleOptimizer</code></strong>: To mitigate the risk of underflow—when gradient values fall below the representable range of <code class=\"language-plaintext highlighter-rouge\">float16</code>—TensorFlow introduces automatic loss scaling. This mechanism adaptively maintains numerical stability without manual tuning, multiplying the loss by a scalar factor before backpropagation and reverting it afterward. This is enabled by wrapping a base optimizer with <code class=\"language-plaintext highlighter-rouge\">tf.keras.mixed_precision.LossScaleOptimizer</code>.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>These two features make mixed precision safe for most real-world training scenarios, enabling users to benefit from performance gains without manual tensor casting or custom scaling logic.</p>\n  </li>\n</ul>\n<p>Mixed precision training in TensorFlow works by executing computations in half-precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>) where safe, and in single-precision (<code class=\"language-plaintext highlighter-rouge\">float32</code>) where required for numerical stability. This selective usage of data types reduces memory bandwidth and speeds up computation, particularly on GPUs equipped with NVIDIA Tensor Cores (e.g., V100, T4, A100). These architectures are specifically designed to handle mixed-precision workloads efficiently.</p>\n<p>The two core features of TensorFlow’s AMP system are:</p>\n<ul>\n      <li>\n        <p><strong>Global Policy Management</strong>: Mixed precision is enabled by setting a global or per-layer dtype policy to <code class=\"language-plaintext highlighter-rouge\">'mixed_float16'</code>. This instructs TensorFlow to automatically cast eligible operations to <code class=\"language-plaintext highlighter-rouge\">float16</code> while retaining critical variables (e.g., weights, certain accumulators) in <code class=\"language-plaintext highlighter-rouge\">float32</code>.</p>\n      </li>\n      <li>\n        <p><strong>Loss Scaling with <code class=\"language-plaintext highlighter-rouge\">LossScaleOptimizer</code></strong>: To mitigate the risk of underflow—when gradient values fall below the representable range of <code class=\"language-plaintext highlighter-rouge\">float16</code>—TensorFlow introduces automatic loss scaling. This mechanism adaptively maintains numerical stability without manual tuning, multiplying the loss by a scalar factor before backpropagation and reverting it afterward. This is enabled by wrapping a base optimizer with <code class=\"language-plaintext highlighter-rouge\">tf.keras.mixed_precision.LossScaleOptimizer</code>.</p>\n      </li>\n    </ul>\n<p><strong>Global Policy Management</strong>: Mixed precision is enabled by setting a global or per-layer dtype policy to <code class=\"language-plaintext highlighter-rouge\">'mixed_float16'</code>. This instructs TensorFlow to automatically cast eligible operations to <code class=\"language-plaintext highlighter-rouge\">float16</code> while retaining critical variables (e.g., weights, certain accumulators) in <code class=\"language-plaintext highlighter-rouge\">float32</code>.</p>\n<p><strong>Loss Scaling with <code class=\"language-plaintext highlighter-rouge\">LossScaleOptimizer</code></strong>: To mitigate the risk of underflow—when gradient values fall below the representable range of <code class=\"language-plaintext highlighter-rouge\">float16</code>—TensorFlow introduces automatic loss scaling. This mechanism adaptively maintains numerical stability without manual tuning, multiplying the loss by a scalar factor before backpropagation and reverting it afterward. This is enabled by wrapping a base optimizer with <code class=\"language-plaintext highlighter-rouge\">tf.keras.mixed_precision.LossScaleOptimizer</code>.</p>\n<p>These two features make mixed precision safe for most real-world training scenarios, enabling users to benefit from performance gains without manual tensor casting or custom scaling logic.</p>\n<h5 id=\"practical-implementation-in-a-training-pipeline\">Practical Implementation in a Training Pipeline</h5>\n<ul>\n  <li>TensorFlow’s mixed precision training is designed to be seamless, requiring only a few lines of code to enable. The following example demonstrates a typical setup using the <code class=\"language-plaintext highlighter-rouge\">tf.keras</code> API.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code24\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code24\"><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras</span> <span class=\"kn\">import</span> <span class=\"n\">mixed_precision</span>\n\n<span class=\"c1\"># Enable mixed precision globally\n</span><span class=\"n\">mixed_precision</span><span class=\"p\">.</span><span class=\"n\">set_global_policy</span><span class=\"p\">(</span><span class=\"s\">'mixed_float16'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Define a model (example: simple MLP)\n</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">Sequential</span><span class=\"p\">([</span>\n    <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">),</span>\n    <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">),</span>\n    <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n\n<span class=\"c1\"># Wrap the optimizer with LossScaleOptimizer for stability\n</span><span class=\"n\">base_optimizer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">optimizers</span><span class=\"p\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">)</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">mixed_precision</span><span class=\"p\">.</span><span class=\"n\">LossScaleOptimizer</span><span class=\"p\">(</span><span class=\"n\">base_optimizer</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Compile the model\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s\">'mse'</span><span class=\"p\">,</span> <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'mae'</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Prepare training data\n</span><span class=\"n\">X_train</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">normal</span><span class=\"p\">((</span><span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">))</span>\n<span class=\"n\">y_train</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">normal</span><span class=\"p\">((</span><span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Train the model\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code24\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code24\"><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras</span> <span class=\"kn\">import</span> <span class=\"n\">mixed_precision</span>\n\n<span class=\"c1\"># Enable mixed precision globally\n</span><span class=\"n\">mixed_precision</span><span class=\"p\">.</span><span class=\"n\">set_global_policy</span><span class=\"p\">(</span><span class=\"s\">'mixed_float16'</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Define a model (example: simple MLP)\n</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">Sequential</span><span class=\"p\">([</span>\n    <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">),</span>\n    <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">),</span>\n    <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n\n<span class=\"c1\"># Wrap the optimizer with LossScaleOptimizer for stability\n</span><span class=\"n\">base_optimizer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">optimizers</span><span class=\"p\">.</span><span class=\"n\">Adam</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">)</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">mixed_precision</span><span class=\"p\">.</span><span class=\"n\">LossScaleOptimizer</span><span class=\"p\">(</span><span class=\"n\">base_optimizer</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Compile the model\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s\">'mse'</span><span class=\"p\">,</span> <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'mae'</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># Prepare training data\n</span><span class=\"n\">X_train</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">normal</span><span class=\"p\">((</span><span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">))</span>\n<span class=\"n\">y_train</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">normal</span><span class=\"p\">((</span><span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Train the model\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>\n    <p><strong>Explanation of Key Components:</strong></p>\n\n    <ul>\n      <li>\n        <p><strong><code class=\"language-plaintext highlighter-rouge\">set_global_policy('mixed_float16')</code></strong>: This sets the default computation policy across all layers to use <code class=\"language-plaintext highlighter-rouge\">float16</code> where safe, while storing variables such as model weights in <code class=\"language-plaintext highlighter-rouge\">float32</code> to ensure stability.</p>\n      </li>\n      <li>\n        <p><strong><code class=\"language-plaintext highlighter-rouge\">LossScaleOptimizer</code></strong>: The base optimizer (e.g., <code class=\"language-plaintext highlighter-rouge\">Adam</code>) is wrapped to apply dynamic loss scaling. This prevents numerical underflows by adapting the loss scaling factor based on gradient stability during training.</p>\n      </li>\n      <li>\n        <p><strong>Hardware Requirements</strong>: While AMP can be enabled on any GPU, maximum performance benefits are realized on NVIDIA GPUs with Tensor Cores, such as the Volta (V100), Turing (T4), or Ampere (A100) architectures.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Layer Compatibility and Custom Layers</strong></p>\n\n    <ul>\n      <li>\n        <p>Most built-in TensorFlow and Keras layers support AMP without modification. If you’re using custom layers or third-party code, ensure that:</p>\n\n        <ul>\n          <li>Operations numerically sensitive to precision are forced to <code class=\"language-plaintext highlighter-rouge\">float32</code> if needed (using <code class=\"language-plaintext highlighter-rouge\">tf.cast</code>).</li>\n          <li>Custom gradients are correctly handled, especially in layers using <code class=\"language-plaintext highlighter-rouge\">tf.custom_gradient</code>.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Model Behavior and Performance Considerations</strong>:</p>\n\n    <ul>\n      <li>\n        <p><strong>Storage Format</strong>: Weights are stored in <code class=\"language-plaintext highlighter-rouge\">float32</code> internally, but computations are cast to <code class=\"language-plaintext highlighter-rouge\">float16</code> where safe. This ensures a balance between performance and accuracy.</p>\n      </li>\n      <li>\n        <p><strong>Layer Compatibility</strong>: Most built-in Keras layers are fully compatible with AMP. Custom layers or third-party operations may require manual casting using <code class=\"language-plaintext highlighter-rouge\">tf.cast()</code> or explicit dtype management.</p>\n      </li>\n      <li>\n        <p><strong>Inference</strong>: After training, models trained with AMP can be saved and exported as usual. During inference, the <code class=\"language-plaintext highlighter-rouge\">mixed_float16</code> policy can remain active to reduce latency and memory usage, particularly for large batch sizes.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Best Practices for TensorFlow AMP</strong>:</p>\n\n    <ul>\n      <li>Enable AMP by default (i.e., use <code class=\"language-plaintext highlighter-rouge\">mixed_float16</code>) when training on Tensor Core GPUs, especially for models with substantial compute demands. Use dynamic loss scaling (enabled by default with <code class=\"language-plaintext highlighter-rouge\">LossScaleOptimizer</code>) to maintain stability during training without the need for manual tuning.</li>\n      <li>Monitor training for gradient anomalies (e.g., sudden spikes in loss). Although AMP is robust, occasional divergence may indicate the need for a lower initial loss scale or a refined model architecture.</li>\n      <li>Use benchmark tools (e.g., TensorBoard, NVIDIA Nsight Systems) to validate performance gains and ensure your training benefits from mixed precision on supported hardware.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Explanation of Key Components:</strong></p>\n<ul>\n      <li>\n        <p><strong><code class=\"language-plaintext highlighter-rouge\">set_global_policy('mixed_float16')</code></strong>: This sets the default computation policy across all layers to use <code class=\"language-plaintext highlighter-rouge\">float16</code> where safe, while storing variables such as model weights in <code class=\"language-plaintext highlighter-rouge\">float32</code> to ensure stability.</p>\n      </li>\n      <li>\n        <p><strong><code class=\"language-plaintext highlighter-rouge\">LossScaleOptimizer</code></strong>: The base optimizer (e.g., <code class=\"language-plaintext highlighter-rouge\">Adam</code>) is wrapped to apply dynamic loss scaling. This prevents numerical underflows by adapting the loss scaling factor based on gradient stability during training.</p>\n      </li>\n      <li>\n        <p><strong>Hardware Requirements</strong>: While AMP can be enabled on any GPU, maximum performance benefits are realized on NVIDIA GPUs with Tensor Cores, such as the Volta (V100), Turing (T4), or Ampere (A100) architectures.</p>\n      </li>\n    </ul>\n<p><strong><code class=\"language-plaintext highlighter-rouge\">set_global_policy('mixed_float16')</code></strong>: This sets the default computation policy across all layers to use <code class=\"language-plaintext highlighter-rouge\">float16</code> where safe, while storing variables such as model weights in <code class=\"language-plaintext highlighter-rouge\">float32</code> to ensure stability.</p>\n<p><strong><code class=\"language-plaintext highlighter-rouge\">LossScaleOptimizer</code></strong>: The base optimizer (e.g., <code class=\"language-plaintext highlighter-rouge\">Adam</code>) is wrapped to apply dynamic loss scaling. This prevents numerical underflows by adapting the loss scaling factor based on gradient stability during training.</p>\n<p><strong>Hardware Requirements</strong>: While AMP can be enabled on any GPU, maximum performance benefits are realized on NVIDIA GPUs with Tensor Cores, such as the Volta (V100), Turing (T4), or Ampere (A100) architectures.</p>\n<p><strong>Layer Compatibility and Custom Layers</strong></p>\n<ul>\n      <li>\n        <p>Most built-in TensorFlow and Keras layers support AMP without modification. If you’re using custom layers or third-party code, ensure that:</p>\n\n        <ul>\n          <li>Operations numerically sensitive to precision are forced to <code class=\"language-plaintext highlighter-rouge\">float32</code> if needed (using <code class=\"language-plaintext highlighter-rouge\">tf.cast</code>).</li>\n          <li>Custom gradients are correctly handled, especially in layers using <code class=\"language-plaintext highlighter-rouge\">tf.custom_gradient</code>.</li>\n        </ul>\n      </li>\n    </ul>\n<p>Most built-in TensorFlow and Keras layers support AMP without modification. If you’re using custom layers or third-party code, ensure that:</p>\n<ul>\n          <li>Operations numerically sensitive to precision are forced to <code class=\"language-plaintext highlighter-rouge\">float32</code> if needed (using <code class=\"language-plaintext highlighter-rouge\">tf.cast</code>).</li>\n          <li>Custom gradients are correctly handled, especially in layers using <code class=\"language-plaintext highlighter-rouge\">tf.custom_gradient</code>.</li>\n        </ul>\n<p><strong>Model Behavior and Performance Considerations</strong>:</p>\n<ul>\n      <li>\n        <p><strong>Storage Format</strong>: Weights are stored in <code class=\"language-plaintext highlighter-rouge\">float32</code> internally, but computations are cast to <code class=\"language-plaintext highlighter-rouge\">float16</code> where safe. This ensures a balance between performance and accuracy.</p>\n      </li>\n      <li>\n        <p><strong>Layer Compatibility</strong>: Most built-in Keras layers are fully compatible with AMP. Custom layers or third-party operations may require manual casting using <code class=\"language-plaintext highlighter-rouge\">tf.cast()</code> or explicit dtype management.</p>\n      </li>\n      <li>\n        <p><strong>Inference</strong>: After training, models trained with AMP can be saved and exported as usual. During inference, the <code class=\"language-plaintext highlighter-rouge\">mixed_float16</code> policy can remain active to reduce latency and memory usage, particularly for large batch sizes.</p>\n      </li>\n    </ul>\n<p><strong>Storage Format</strong>: Weights are stored in <code class=\"language-plaintext highlighter-rouge\">float32</code> internally, but computations are cast to <code class=\"language-plaintext highlighter-rouge\">float16</code> where safe. This ensures a balance between performance and accuracy.</p>\n<p><strong>Layer Compatibility</strong>: Most built-in Keras layers are fully compatible with AMP. Custom layers or third-party operations may require manual casting using <code class=\"language-plaintext highlighter-rouge\">tf.cast()</code> or explicit dtype management.</p>\n<p><strong>Inference</strong>: After training, models trained with AMP can be saved and exported as usual. During inference, the <code class=\"language-plaintext highlighter-rouge\">mixed_float16</code> policy can remain active to reduce latency and memory usage, particularly for large batch sizes.</p>\n<p><strong>Best Practices for TensorFlow AMP</strong>:</p>\n<ul>\n      <li>Enable AMP by default (i.e., use <code class=\"language-plaintext highlighter-rouge\">mixed_float16</code>) when training on Tensor Core GPUs, especially for models with substantial compute demands. Use dynamic loss scaling (enabled by default with <code class=\"language-plaintext highlighter-rouge\">LossScaleOptimizer</code>) to maintain stability during training without the need for manual tuning.</li>\n      <li>Monitor training for gradient anomalies (e.g., sudden spikes in loss). Although AMP is robust, occasional divergence may indicate the need for a lower initial loss scale or a refined model architecture.</li>\n      <li>Use benchmark tools (e.g., TensorBoard, NVIDIA Nsight Systems) to validate performance gains and ensure your training benefits from mixed precision on supported hardware.</li>\n    </ul>\n<h5 id=\"performance-benchmarks\">Performance Benchmarks</h5>\n<ul>\n  <li>To evaluate the real-world impact of mixed precision training, consider benchmarks run across three distinct neural network architectures using TensorFlow’s AMP implementation.</li>\n  <li>These benchmarks demonstrate that <strong>automatic mixed precision should be one of the first performance optimizations you apply to your TensorFlow training scripts</strong>. In large-scale models, AMP can lead to dramatic reductions in training time—up to 60%—with minimal code changes (typically under 5 lines). Especially when training on modern GPU architectures, the performance uplift can be essential to reducing costs and iteration times in production-scale machine learning.</li>\n  <li>\n    <p>The experiments were conducted on AWS EC2 instances using both last-generation and current-generation NVIDIA GPUs.</p>\n  </li>\n  <li><strong>Hardware setup</strong>:\n    <ul>\n      <li><strong>V100</strong> (Volta architecture) via <code class=\"language-plaintext highlighter-rouge\">p3.2xlarge</code></li>\n      <li><strong>T4</strong> (Turing architecture) via <code class=\"language-plaintext highlighter-rouge\">g4dn.xlarge</code></li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Framework</strong>: Recent TensorFlow builds with CUDA 10.0, orchestrated using the <a href=\"https://spell.ml/docs/run_overview/\">Spell API</a></p>\n  </li>\n  <li>\n    <p><strong>Models Tested</strong>:</p>\n\n    <ol>\n      <li>\n        <p><strong>Feedforward Network</strong>\nA fully connected feedforward network trained on tabular data from the <a href=\"https://www.kaggle.com/c/rossmann-store-sales\">Rossmann Store Sales</a> Kaggle competition.\n<em>Codebase</em>: <a href=\"https://github.com/ResidentMario/spell-feedforward-rossman\">GitHub repository</a></p>\n      </li>\n      <li>\n        <p><strong>UNet</strong>\nA medium-sized convolutional model used for image segmentation on the <a href=\"https://www.kaggle.com/residentmario/segmented-bob-ross-images\">Segmented Bob Ross Images</a> dataset.\n<em>Codebase</em>: <a href=\"https://github.com/ResidentMario/spell-unet-bob-ross\">GitHub repository</a></p>\n      </li>\n      <li>\n        <p><strong>BERT</strong>\nA large-scale transformer model (<code class=\"language-plaintext highlighter-rouge\">bert-base-uncased</code>) trained on the <a href=\"https://www.kaggle.com/c/tweet-sentiment-extraction\">Tweet Sentiment Extraction</a> dataset using Hugging Face’s Transformers.\n<em>Codebase</em>: <a href=\"https://github.com/ResidentMario/spell-tweet-sentiment-extraction\">GitHub repository</a></p>\n      </li>\n    </ol>\n  </li>\n  <li><strong>Benchmark Results</strong>:</li>\n</ul>\n<p>The experiments were conducted on AWS EC2 instances using both last-generation and current-generation NVIDIA GPUs.</p>\n<ul>\n      <li><strong>V100</strong> (Volta architecture) via <code class=\"language-plaintext highlighter-rouge\">p3.2xlarge</code></li>\n      <li><strong>T4</strong> (Turing architecture) via <code class=\"language-plaintext highlighter-rouge\">g4dn.xlarge</code></li>\n    </ul>\n<p><strong>Framework</strong>: Recent TensorFlow builds with CUDA 10.0, orchestrated using the <a href=\"https://spell.ml/docs/run_overview/\">Spell API</a></p>\n<p><strong>Models Tested</strong>:</p>\n<ol>\n      <li>\n        <p><strong>Feedforward Network</strong>\nA fully connected feedforward network trained on tabular data from the <a href=\"https://www.kaggle.com/c/rossmann-store-sales\">Rossmann Store Sales</a> Kaggle competition.\n<em>Codebase</em>: <a href=\"https://github.com/ResidentMario/spell-feedforward-rossman\">GitHub repository</a></p>\n      </li>\n      <li>\n        <p><strong>UNet</strong>\nA medium-sized convolutional model used for image segmentation on the <a href=\"https://www.kaggle.com/residentmario/segmented-bob-ross-images\">Segmented Bob Ross Images</a> dataset.\n<em>Codebase</em>: <a href=\"https://github.com/ResidentMario/spell-unet-bob-ross\">GitHub repository</a></p>\n      </li>\n      <li>\n        <p><strong>BERT</strong>\nA large-scale transformer model (<code class=\"language-plaintext highlighter-rouge\">bert-base-uncased</code>) trained on the <a href=\"https://www.kaggle.com/c/tweet-sentiment-extraction\">Tweet Sentiment Extraction</a> dataset using Hugging Face’s Transformers.\n<em>Codebase</em>: <a href=\"https://github.com/ResidentMario/spell-tweet-sentiment-extraction\">GitHub repository</a></p>\n      </li>\n    </ol>\n<p><strong>Feedforward Network</strong>\nA fully connected feedforward network trained on tabular data from the <a href=\"https://www.kaggle.com/c/rossmann-store-sales\">Rossmann Store Sales</a> Kaggle competition.\n<em>Codebase</em>: <a href=\"https://github.com/ResidentMario/spell-feedforward-rossman\">GitHub repository</a></p>\n<p><strong>UNet</strong>\nA medium-sized convolutional model used for image segmentation on the <a href=\"https://www.kaggle.com/residentmario/segmented-bob-ross-images\">Segmented Bob Ross Images</a> dataset.\n<em>Codebase</em>: <a href=\"https://github.com/ResidentMario/spell-unet-bob-ross\">GitHub repository</a></p>\n<p><strong>BERT</strong>\nA large-scale transformer model (<code class=\"language-plaintext highlighter-rouge\">bert-base-uncased</code>) trained on the <a href=\"https://www.kaggle.com/c/tweet-sentiment-extraction\">Tweet Sentiment Extraction</a> dataset using Hugging Face’s Transformers.\n<em>Codebase</em>: <a href=\"https://github.com/ResidentMario/spell-tweet-sentiment-extraction\">GitHub repository</a></p>\n<p><img src=\"/primers/ai/assets/model-compression/amp_time.avif\" alt=\"\"></p>\n<ul>\n  <li>\n    <p><strong>Observations</strong>:</p>\n\n    <ul>\n      <li>\n        <p><strong>Feedforward Network</strong>:\nBeing a small model with minimal computational complexity, this architecture saw negligible benefit from mixed precision training. The data throughput and model size are simply too limited to leverage Tensor Core acceleration.</p>\n      </li>\n      <li>\n        <p><strong>UNet (Medium-Scale Model)</strong>:\nWith approximately 7.7 million parameters, UNet showed meaningful improvements in training time. The impact of AMP varied by hardware:</p>\n\n        <ul>\n          <li>V100: ~5% training time reduction</li>\n          <li>T4: ~30% reduction\nThis disparity highlights how more recent GPU architectures (like Turing) extract greater benefit from Tensor Core utilization.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>BERT (Large-Scale Model)</strong>:\nMixed precision provided <em>transformational</em> benefits for BERT:</p>\n\n        <ul>\n          <li>Training time reduced by <strong>50–60%</strong> on both GPU types</li>\n          <li>No degradation in training loss or final model performance\nThis demonstrates that AMP is especially advantageous for large transformer-based models where computational demand is high.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Observations</strong>:</p>\n<ul>\n      <li>\n        <p><strong>Feedforward Network</strong>:\nBeing a small model with minimal computational complexity, this architecture saw negligible benefit from mixed precision training. The data throughput and model size are simply too limited to leverage Tensor Core acceleration.</p>\n      </li>\n      <li>\n        <p><strong>UNet (Medium-Scale Model)</strong>:\nWith approximately 7.7 million parameters, UNet showed meaningful improvements in training time. The impact of AMP varied by hardware:</p>\n\n        <ul>\n          <li>V100: ~5% training time reduction</li>\n          <li>T4: ~30% reduction\nThis disparity highlights how more recent GPU architectures (like Turing) extract greater benefit from Tensor Core utilization.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>BERT (Large-Scale Model)</strong>:\nMixed precision provided <em>transformational</em> benefits for BERT:</p>\n\n        <ul>\n          <li>Training time reduced by <strong>50–60%</strong> on both GPU types</li>\n          <li>No degradation in training loss or final model performance\nThis demonstrates that AMP is especially advantageous for large transformer-based models where computational demand is high.</li>\n        </ul>\n      </li>\n    </ul>\n<p><strong>Feedforward Network</strong>:\nBeing a small model with minimal computational complexity, this architecture saw negligible benefit from mixed precision training. The data throughput and model size are simply too limited to leverage Tensor Core acceleration.</p>\n<p><strong>UNet (Medium-Scale Model)</strong>:\nWith approximately 7.7 million parameters, UNet showed meaningful improvements in training time. The impact of AMP varied by hardware:</p>\n<ul>\n          <li>V100: ~5% training time reduction</li>\n          <li>T4: ~30% reduction\nThis disparity highlights how more recent GPU architectures (like Turing) extract greater benefit from Tensor Core utilization.</li>\n        </ul>\n<p><strong>BERT (Large-Scale Model)</strong>:\nMixed precision provided <em>transformational</em> benefits for BERT:</p>\n<ul>\n          <li>Training time reduced by <strong>50–60%</strong> on both GPU types</li>\n          <li>No degradation in training loss or final model performance\nThis demonstrates that AMP is especially advantageous for large transformer-based models where computational demand is high.</li>\n        </ul>\n<h5 id=\"key-takeaways\">Key Takeaways</h5>\n<ul>\n  <li>\n    <p>TensorFlow’s automatic mixed precision (AMP) support offers a robust, efficient, and production-ready pathway to accelerate model training with minimal code changes. By executing safe operations in <code class=\"language-plaintext highlighter-rouge\">float16</code> while preserving critical numerical precision with <code class=\"language-plaintext highlighter-rouge\">float32</code> where needed, TensorFlow achieves an optimal balance of performance and stability.</p>\n  </li>\n  <li>\n    <p><strong>Ease of Integration</strong>: Mixed precision can be enabled in just a few lines using <code class=\"language-plaintext highlighter-rouge\">mixed_precision.set_global_policy('mixed_float16')</code> and wrapping the optimizer with <code class=\"language-plaintext highlighter-rouge\">LossScaleOptimizer</code>. No manual casting or scaling logic is needed for most models.</p>\n  </li>\n  <li>\n    <p><strong>Hardware Acceleration</strong>: Significant speedups are realized on NVIDIA GPUs with Tensor Cores (e.g., V100, T4, A100). These architectures are specifically designed to handle mixed-precision workloads efficiently.</p>\n  </li>\n  <li>\n    <p><strong>Scalability</strong>: The performance benefits of AMP scale with model size. While small models may see limited gains, medium-to-large models—particularly convolutional networks and transformers—can experience training time reductions of 30–60% or more.</p>\n  </li>\n  <li>\n    <p><strong>Numerical Stability</strong>: Automatic loss scaling ensures that mixed precision does not compromise training convergence. Gradient underflows are mitigated adaptively, making AMP safe for most real-world training scenarios.</p>\n  </li>\n</ul>\n<p>TensorFlow’s automatic mixed precision (AMP) support offers a robust, efficient, and production-ready pathway to accelerate model training with minimal code changes. By executing safe operations in <code class=\"language-plaintext highlighter-rouge\">float16</code> while preserving critical numerical precision with <code class=\"language-plaintext highlighter-rouge\">float32</code> where needed, TensorFlow achieves an optimal balance of performance and stability.</p>\n<p><strong>Ease of Integration</strong>: Mixed precision can be enabled in just a few lines using <code class=\"language-plaintext highlighter-rouge\">mixed_precision.set_global_policy('mixed_float16')</code> and wrapping the optimizer with <code class=\"language-plaintext highlighter-rouge\">LossScaleOptimizer</code>. No manual casting or scaling logic is needed for most models.</p>\n<p><strong>Hardware Acceleration</strong>: Significant speedups are realized on NVIDIA GPUs with Tensor Cores (e.g., V100, T4, A100). These architectures are specifically designed to handle mixed-precision workloads efficiently.</p>\n<p><strong>Scalability</strong>: The performance benefits of AMP scale with model size. While small models may see limited gains, medium-to-large models—particularly convolutional networks and transformers—can experience training time reductions of 30–60% or more.</p>\n<p><strong>Numerical Stability</strong>: Automatic loss scaling ensures that mixed precision does not compromise training convergence. Gradient underflows are mitigated adaptively, making AMP safe for most real-world training scenarios.</p>\n<h5 id=\"recommendations\">Recommendations</h5>\n<ul>\n  <li><strong>Enable AMP by default</strong> when training on Tensor Core GPUs, especially for models with substantial compute demands.</li>\n  <li><strong>Benchmark performance</strong> for your specific model and dataset, as the impact of mixed precision can vary depending on architecture, data pipeline, and hardware.</li>\n  <li><strong>Use dynamic loss scaling</strong> (enabled by default with <code class=\"language-plaintext highlighter-rouge\">LossScaleOptimizer</code>) to maintain stability without the need for manual tuning.</li>\n</ul>",
      "contentMarkdown": "*   Mixed precision training is a performance optimization technique that accelerates neural network training by leveraging lower-precision arithmetic—primarily half-precision floating-point (`float16`)—without compromising model accuracy or convergence stability.\n    \n*   At its core, the concept is straightforward: replacing standard single-precision (`float32`) operations with half-precision (`float16`) can roughly halve memory usage and significantly reduce training time. However, implementing this substitution safely and effectively is non-trivial due to the numerical limitations of lower-precision formats.\n    \n*   By combining dual weight representations, selective precision, and dynamic loss scaling, mixed precision training enables significant reductions in training time and memory consumption—often with negligible impact on model accuracy. As demonstrated in [Mixed Precision Training](https://arxiv.org/pdf/1710.03740.pdf) by Narang et al. (2018), these methods allow a wide range of models to train to convergence reliably and efficiently using `float16` computations.\n    \n*   **Challenges with Half Precision**:\n    \n    *   Lower-precision formats like `float16` have a reduced dynamic range and lower numerical precision compared to `float32`. One critical issue is _underflow_, where extremely small gradient values become indistinguishable from zero due to rounding errors inherent in the limited precision. This is especially problematic during backpropagation, as many gradient updates are naturally very small but still essential for accurate model convergence. If too many of these values are rounded to zero or become `NaN` (Not a Number), the model may fail to learn altogether.\n        \n    *   The following figure from the [Mixed Precision Training](https://arxiv.org/pdf/1710.03740.pdf) paper illustrates a key finding that naïvely switching to `float16` causes any gradient smaller than 2−242−242^{-24} to be “swallowed”—effectively zeroed out. In their experiments, this resulted in approximately 5% of all gradient updates being discarded, severely impeding the training process:\n        \n    \n    ![](/primers/ai/assets/model-compression/mpt.avif)\n    \n*   **Techniques for Safe Mixed Precision Training**:\n    \n    *   To mitigate these numerical instabilities, the authors propose a systematic approach combining three key strategies. When used together, these allow safe and effective training with `float16` precision:\n        \n        1.  **Maintaining Dual Weight Copies (Master Weights Strategy)**\n            *   Each model weight is stored in two formats: a full-precision (`float32`) “master copy” and a lower-precision (`float16`) copy. During forward and backward passes, computations are performed using the `float16` version to benefit from faster execution and lower memory usage. However, the actual weight updates are applied to the `float32` master weights using gradients computed in `float16` but cast to `float32`. This preserves update accuracy and avoids the accumulation of precision errors during training.\n        2.  **Selective Precision Application (Mixed-Dtype Execution)**\n            *   Not all neural network operations are equally sensitive to reduced precision. Many element-wise operations (e.g., activation functions or layer normalization) are safe to compute in `float16`, while others—such as softmax, batch normalization, and gradient accumulation—require `float32` to maintain stability. Mixed precision training selectively applies `float16` where safe and retains `float32` where necessary. This fine-grained control over data types allows the model to reap performance benefits without sacrificing numerical stability/reliability.\n        3.  **Loss Scaling**\n            *   To address the underflow problem, the loss value is multiplied by a scalar factor (commonly 8, 16, or 128) before backpropagation. This process, known as _loss scaling_, proportionally increases all gradient values, elevating small gradients above the `float16` precision threshold of 2−242−242^{-24}. After gradients are computed, the scaling factor is removed (by division) before the optimizer applies the updates.\n            *   Care must be taken to avoid _overflow_, which occurs when values exceed the representable range of `float16`, leading to `Inf` or `NaN` values. Adaptive loss scaling strategies—where the scaling factor is dynamically adjusted based on gradient statistics—are often employed to balance between underflow and overflow.\n\nMixed precision training is a performance optimization technique that accelerates neural network training by leveraging lower-precision arithmetic—primarily half-precision floating-point (`float16`)—without compromising model accuracy or convergence stability.\n\nAt its core, the concept is straightforward: replacing standard single-precision (`float32`) operations with half-precision (`float16`) can roughly halve memory usage and significantly reduce training time. However, implementing this substitution safely and effectively is non-trivial due to the numerical limitations of lower-precision formats.\n\nBy combining dual weight representations, selective precision, and dynamic loss scaling, mixed precision training enables significant reductions in training time and memory consumption—often with negligible impact on model accuracy. As demonstrated in [Mixed Precision Training](https://arxiv.org/pdf/1710.03740.pdf) by Narang et al. (2018), these methods allow a wide range of models to train to convergence reliably and efficiently using `float16` computations.\n\n**Challenges with Half Precision**:\n\n*   Lower-precision formats like `float16` have a reduced dynamic range and lower numerical precision compared to `float32`. One critical issue is _underflow_, where extremely small gradient values become indistinguishable from zero due to rounding errors inherent in the limited precision. This is especially problematic during backpropagation, as many gradient updates are naturally very small but still essential for accurate model convergence. If too many of these values are rounded to zero or become `NaN` (Not a Number), the model may fail to learn altogether.\n    \n*   The following figure from the [Mixed Precision Training](https://arxiv.org/pdf/1710.03740.pdf) paper illustrates a key finding that naïvely switching to `float16` causes any gradient smaller than 2−242−242^{-24} to be “swallowed”—effectively zeroed out. In their experiments, this resulted in approximately 5% of all gradient updates being discarded, severely impeding the training process:\n    \n\nLower-precision formats like `float16` have a reduced dynamic range and lower numerical precision compared to `float32`. One critical issue is _underflow_, where extremely small gradient values become indistinguishable from zero due to rounding errors inherent in the limited precision. This is especially problematic during backpropagation, as many gradient updates are naturally very small but still essential for accurate model convergence. If too many of these values are rounded to zero or become `NaN` (Not a Number), the model may fail to learn altogether.\n\nThe following figure from the [Mixed Precision Training](https://arxiv.org/pdf/1710.03740.pdf) paper illustrates a key finding that naïvely switching to `float16` causes any gradient smaller than 2−242−242^{-24} to be “swallowed”—effectively zeroed out. In their experiments, this resulted in approximately 5% of all gradient updates being discarded, severely impeding the training process:\n\n![](/primers/ai/assets/model-compression/mpt.avif)\n\n**Techniques for Safe Mixed Precision Training**:\n\n*   To mitigate these numerical instabilities, the authors propose a systematic approach combining three key strategies. When used together, these allow safe and effective training with `float16` precision:\n    \n    1.  **Maintaining Dual Weight Copies (Master Weights Strategy)**\n        *   Each model weight is stored in two formats: a full-precision (`float32`) “master copy” and a lower-precision (`float16`) copy. During forward and backward passes, computations are performed using the `float16` version to benefit from faster execution and lower memory usage. However, the actual weight updates are applied to the `float32` master weights using gradients computed in `float16` but cast to `float32`. This preserves update accuracy and avoids the accumulation of precision errors during training.\n    2.  **Selective Precision Application (Mixed-Dtype Execution)**\n        *   Not all neural network operations are equally sensitive to reduced precision. Many element-wise operations (e.g., activation functions or layer normalization) are safe to compute in `float16`, while others—such as softmax, batch normalization, and gradient accumulation—require `float32` to maintain stability. Mixed precision training selectively applies `float16` where safe and retains `float32` where necessary. This fine-grained control over data types allows the model to reap performance benefits without sacrificing numerical stability/reliability.\n    3.  **Loss Scaling**\n        *   To address the underflow problem, the loss value is multiplied by a scalar factor (commonly 8, 16, or 128) before backpropagation. This process, known as _loss scaling_, proportionally increases all gradient values, elevating small gradients above the `float16` precision threshold of 2−242−242^{-24}. After gradients are computed, the scaling factor is removed (by division) before the optimizer applies the updates.\n        *   Care must be taken to avoid _overflow_, which occurs when values exceed the representable range of `float16`, leading to `Inf` or `NaN` values. Adaptive loss scaling strategies—where the scaling factor is dynamically adjusted based on gradient statistics—are often employed to balance between underflow and overflow.\n\nTo mitigate these numerical instabilities, the authors propose a systematic approach combining three key strategies. When used together, these allow safe and effective training with `float16` precision:\n\n1.  **Maintaining Dual Weight Copies (Master Weights Strategy)**\n    *   Each model weight is stored in two formats: a full-precision (`float32`) “master copy” and a lower-precision (`float16`) copy. During forward and backward passes, computations are performed using the `float16` version to benefit from faster execution and lower memory usage. However, the actual weight updates are applied to the `float32` master weights using gradients computed in `float16` but cast to `float32`. This preserves update accuracy and avoids the accumulation of precision errors during training.\n2.  **Selective Precision Application (Mixed-Dtype Execution)**\n    *   Not all neural network operations are equally sensitive to reduced precision. Many element-wise operations (e.g., activation functions or layer normalization) are safe to compute in `float16`, while others—such as softmax, batch normalization, and gradient accumulation—require `float32` to maintain stability. Mixed precision training selectively applies `float16` where safe and retains `float32` where necessary. This fine-grained control over data types allows the model to reap performance benefits without sacrificing numerical stability/reliability.\n3.  **Loss Scaling**\n    *   To address the underflow problem, the loss value is multiplied by a scalar factor (commonly 8, 16, or 128) before backpropagation. This process, known as _loss scaling_, proportionally increases all gradient values, elevating small gradients above the `float16` precision threshold of 2−242−242^{-24}. After gradients are computed, the scaling factor is removed (by division) before the optimizer applies the updates.\n    *   Care must be taken to avoid _overflow_, which occurs when values exceed the representable range of `float16`, leading to `Inf` or `NaN` values. Adaptive loss scaling strategies—where the scaling factor is dynamically adjusted based on gradient statistics—are often employed to balance between underflow and overflow.\n\n*   Each model weight is stored in two formats: a full-precision (`float32`) “master copy” and a lower-precision (`float16`) copy. During forward and backward passes, computations are performed using the `float16` version to benefit from faster execution and lower memory usage. However, the actual weight updates are applied to the `float32` master weights using gradients computed in `float16` but cast to `float32`. This preserves update accuracy and avoids the accumulation of precision errors during training.\n\n*   Not all neural network operations are equally sensitive to reduced precision. Many element-wise operations (e.g., activation functions or layer normalization) are safe to compute in `float16`, while others—such as softmax, batch normalization, and gradient accumulation—require `float32` to maintain stability. Mixed precision training selectively applies `float16` where safe and retains `float32` where necessary. This fine-grained control over data types allows the model to reap performance benefits without sacrificing numerical stability/reliability.\n\n*   To address the underflow problem, the loss value is multiplied by a scalar factor (commonly 8, 16, or 128) before backpropagation. This process, known as _loss scaling_, proportionally increases all gradient values, elevating small gradients above the `float16` precision threshold of 2−242−242^{-24}. After gradients are computed, the scaling factor is removed (by division) before the optimizer applies the updates.\n*   Care must be taken to avoid _overflow_, which occurs when values exceed the representable range of `float16`, leading to `Inf` or `NaN` values. Adaptive loss scaling strategies—where the scaling factor is dynamically adjusted based on gradient statistics—are often employed to balance between underflow and overflow.\n\n#### How PyTorch Automatic Mixed Precision Works\n\n*   With a solid understanding of mixed precision training established, we can now explore how PyTorch streamlines this powerful optimization technique through its **Automatic Mixed Precision (AMP)** API. While mixed precision training has long been theoretically feasible—typically requiring manual tensor casting to `float16` and careful loss scaling—PyTorch removes much of this complexity. Its AMP API makes the process highly accessible, offering a streamlined, production-ready solution that demands only minimal code modifications.\n    \n*   PyTorch’s AMP achieves this by abstracting the underlying mechanics through two key components: `autocast` and `GradScaler`. `autocast` enables selective precision execution, automatically determining which operations benefit from half-precision without sacrificing accuracy. Simultaneously, `GradScaler` manages dynamic loss scaling, helping to prevent issues like gradient underflow and ensuring stable convergence. This integration offers developers substantial speedups—often reducing training times by **50–60%**—and improves memory efficiency, all without compromising model stability or performance.\n    \n*   This practical implementation is a direct evolution of the concepts outlined in the [_Mixed Precision Training_](https://arxiv.org/pdf/1710.03740.pdf) research paper. AMP embodies how advanced techniques from cutting-edge research can be distilled into user-friendly tools that enhance real-world machine learning workflows.\n    \n*   Prior to AMP, implementing mixed precision was a labor-intensive process. Developers had to manually cast tensors, implement and tune custom loss scalers, and safeguard against the risks of instability. The introduction of PyTorch’s `torch.cuda.amp` module represents a major leap forward, encapsulating best practices and democratizing access to high-performance training.\n    \n*   AMP is especially effective on modern NVIDIA GPUs—such as those based on Volta, Turing, Ampere, or newer architectures—which include specialized **Tensor Cores** designed for half-precision operations. However, even on older or unsupported hardware, users may still see performance benefits due to more efficient memory usage and reduced data movement.\n    \n*   In summary, PyTorch’s AMP bridges the gap between theoretical efficiency and practical deployment, making state-of-the-art training techniques both accessible and impactful across a wide range of hardware and use cases.\n    \n\nWith a solid understanding of mixed precision training established, we can now explore how PyTorch streamlines this powerful optimization technique through its **Automatic Mixed Precision (AMP)** API. While mixed precision training has long been theoretically feasible—typically requiring manual tensor casting to `float16` and careful loss scaling—PyTorch removes much of this complexity. Its AMP API makes the process highly accessible, offering a streamlined, production-ready solution that demands only minimal code modifications.\n\nPyTorch’s AMP achieves this by abstracting the underlying mechanics through two key components: `autocast` and `GradScaler`. `autocast` enables selective precision execution, automatically determining which operations benefit from half-precision without sacrificing accuracy. Simultaneously, `GradScaler` manages dynamic loss scaling, helping to prevent issues like gradient underflow and ensuring stable convergence. This integration offers developers substantial speedups—often reducing training times by **50–60%**—and improves memory efficiency, all without compromising model stability or performance.\n\nThis practical implementation is a direct evolution of the concepts outlined in the [_Mixed Precision Training_](https://arxiv.org/pdf/1710.03740.pdf) research paper. AMP embodies how advanced techniques from cutting-edge research can be distilled into user-friendly tools that enhance real-world machine learning workflows.\n\nPrior to AMP, implementing mixed precision was a labor-intensive process. Developers had to manually cast tensors, implement and tune custom loss scalers, and safeguard against the risks of instability. The introduction of PyTorch’s `torch.cuda.amp` module represents a major leap forward, encapsulating best practices and democratizing access to high-performance training.\n\nAMP is especially effective on modern NVIDIA GPUs—such as those based on Volta, Turing, Ampere, or newer architectures—which include specialized **Tensor Cores** designed for half-precision operations. However, even on older or unsupported hardware, users may still see performance benefits due to more efficient memory usage and reduced data movement.\n\nIn summary, PyTorch’s AMP bridges the gap between theoretical efficiency and practical deployment, making state-of-the-art training techniques both accessible and impactful across a wide range of hardware and use cases.\n\n##### Overview of AMP Components\n\n*   PyTorch’s AMP functionality is implemented via the `torch.cuda.amp` module and relies on two key primitives:\n    \n    1.  `torch.cuda.amp.autocast`: A context manager that automatically casts operations to the appropriate precision (`float16` or `float32`) based on operation type and hardware support. This enables a seamless mix of half-precision and full-precision computations without explicit manual intervention.\n        \n    2.  `torch.cuda.amp.GradScaler`: A utility that handles _dynamic loss scaling_. It scales the loss to prevent underflow in gradient computations and then unscales it before applying the optimizer step. The scaler also detects and skips optimizer steps with invalid gradients (e.g., `NaN` or `Inf`), adjusting the scale factor dynamically to maintain numerical stability.\n        \n\nPyTorch’s AMP functionality is implemented via the `torch.cuda.amp` module and relies on two key primitives:\n\n1.  `torch.cuda.amp.autocast`: A context manager that automatically casts operations to the appropriate precision (`float16` or `float32`) based on operation type and hardware support. This enables a seamless mix of half-precision and full-precision computations without explicit manual intervention.\n    \n2.  `torch.cuda.amp.GradScaler`: A utility that handles _dynamic loss scaling_. It scales the loss to prevent underflow in gradient computations and then unscales it before applying the optimizer step. The scaler also detects and skips optimizer steps with invalid gradients (e.g., `NaN` or `Inf`), adjusting the scale factor dynamically to maintain numerical stability.\n    \n\n`torch.cuda.amp.autocast`: A context manager that automatically casts operations to the appropriate precision (`float16` or `float32`) based on operation type and hardware support. This enables a seamless mix of half-precision and full-precision computations without explicit manual intervention.\n\n`torch.cuda.amp.GradScaler`: A utility that handles _dynamic loss scaling_. It scales the loss to prevent underflow in gradient computations and then unscales it before applying the optimizer step. The scaler also detects and skips optimizer steps with invalid gradients (e.g., `NaN` or `Inf`), adjusting the scale factor dynamically to maintain numerical stability.\n\n##### Practical Implementation in a Training Loop\n\n*   The following example demonstrates how mixed precision training is incorporated into a standard PyTorch training loop. Lines marked with `# NEW` indicate additions or modifications required to enable AMP.\n\n![](https://aman.ai/images/copy.png)\n\n`self.train() X = torch.tensor(X, dtype=torch.float32) y = torch.tensor(y, dtype=torch.float32)  optimizer = torch.optim.Adam(self.parameters(), lr=self.max_lr) scheduler = torch.optim.lr_scheduler.OneCycleLR(     optimizer, self.max_lr,     cycle_momentum=False,     epochs=self.n_epochs,     steps_per_epoch=int(np.ceil(len(X) / self.batch_size)), ) batches = torch.utils.data.DataLoader(     torch.utils.data.TensorDataset(X, y),     batch_size=self.batch_size, shuffle=True )  # NEW: Initialize GradScaler for dynamic loss scaling scaler = torch.cuda.amp.GradScaler()  for epoch in range(self.n_epochs):     for i, (X_batch, y_batch) in enumerate(batches):         X_batch = X_batch.cuda()         y_batch = y_batch.cuda()         optimizer.zero_grad()          # NEW: Forward pass with autocast for mixed precision         with torch.cuda.amp.autocast():             y_pred = model(X_batch).squeeze()             loss = self.loss_fn(y_pred, y_batch)          # NEW: Scale loss and perform backward pass         scaler.scale(loss).backward()          lv = loss.detach().cpu().numpy()         if i % 100 == 0:             print(f\"Epoch {epoch + 1}/{self.n_epochs}; Batch {i}; Loss {lv}\")          # NEW: Unscale gradients, perform optimizer step, update scaler         scaler.step(optimizer)         scaler.update()          scheduler.step()`\n\n![](https://aman.ai/images/copy.png)\n\n`self.train() X = torch.tensor(X, dtype=torch.float32) y = torch.tensor(y, dtype=torch.float32)  optimizer = torch.optim.Adam(self.parameters(), lr=self.max_lr) scheduler = torch.optim.lr_scheduler.OneCycleLR(     optimizer, self.max_lr,     cycle_momentum=False,     epochs=self.n_epochs,     steps_per_epoch=int(np.ceil(len(X) / self.batch_size)), ) batches = torch.utils.data.DataLoader(     torch.utils.data.TensorDataset(X, y),     batch_size=self.batch_size, shuffle=True )  # NEW: Initialize GradScaler for dynamic loss scaling scaler = torch.cuda.amp.GradScaler()  for epoch in range(self.n_epochs):     for i, (X_batch, y_batch) in enumerate(batches):         X_batch = X_batch.cuda()         y_batch = y_batch.cuda()         optimizer.zero_grad()          # NEW: Forward pass with autocast for mixed precision         with torch.cuda.amp.autocast():             y_pred = model(X_batch).squeeze()             loss = self.loss_fn(y_pred, y_batch)          # NEW: Scale loss and perform backward pass         scaler.scale(loss).backward()          lv = loss.detach().cpu().numpy()         if i % 100 == 0:             print(f\"Epoch {epoch + 1}/{self.n_epochs}; Batch {i}; Loss {lv}\")          # NEW: Unscale gradients, perform optimizer step, update scaler         scaler.step(optimizer)         scaler.update()          scheduler.step()`\n\n*   **Implementation Notes and Best Practices**:\n    \n    *   **Device compatibility**: AMP is optimized for NVIDIA GPUs with Tensor Cores, particularly those with compute capability ≥ 7.0 (Volta architecture or newer). While it will run on other hardware, performance gains may vary.\n        \n    *   **Model compatibility**: Most standard PyTorch layers (e.g., `nn.Linear`, `nn.Conv2d`, `nn.ReLU`) are AMP-compatible. However, custom operations or third-party libraries may require manual inspection to ensure compatibility or appropriate casting.\n        \n    *   **Gradient stability**: The `GradScaler` performs _automatic gradient anomaly detection_, skipping optimizer steps when gradients contain `Inf` or `NaN` values. This safeguards training from diverging due to numerical instability.\n        \n    *   **Loss scaling strategy**: The `GradScaler` uses _dynamic loss scaling_ by default, which adjusts the scaling factor at runtime based on gradient statistics. This is typically preferred over static scaling for its adaptive robustness.\n        \n\n**Implementation Notes and Best Practices**:\n\n*   **Device compatibility**: AMP is optimized for NVIDIA GPUs with Tensor Cores, particularly those with compute capability ≥ 7.0 (Volta architecture or newer). While it will run on other hardware, performance gains may vary.\n    \n*   **Model compatibility**: Most standard PyTorch layers (e.g., `nn.Linear`, `nn.Conv2d`, `nn.ReLU`) are AMP-compatible. However, custom operations or third-party libraries may require manual inspection to ensure compatibility or appropriate casting.\n    \n*   **Gradient stability**: The `GradScaler` performs _automatic gradient anomaly detection_, skipping optimizer steps when gradients contain `Inf` or `NaN` values. This safeguards training from diverging due to numerical instability.\n    \n*   **Loss scaling strategy**: The `GradScaler` uses _dynamic loss scaling_ by default, which adjusts the scaling factor at runtime based on gradient statistics. This is typically preferred over static scaling for its adaptive robustness.\n    \n\n**Device compatibility**: AMP is optimized for NVIDIA GPUs with Tensor Cores, particularly those with compute capability ≥ 7.0 (Volta architecture or newer). While it will run on other hardware, performance gains may vary.\n\n**Model compatibility**: Most standard PyTorch layers (e.g., `nn.Linear`, `nn.Conv2d`, `nn.ReLU`) are AMP-compatible. However, custom operations or third-party libraries may require manual inspection to ensure compatibility or appropriate casting.\n\n**Gradient stability**: The `GradScaler` performs _automatic gradient anomaly detection_, skipping optimizer steps when gradients contain `Inf` or `NaN` values. This safeguards training from diverging due to numerical instability.\n\n**Loss scaling strategy**: The `GradScaler` uses _dynamic loss scaling_ by default, which adjusts the scaling factor at runtime based on gradient statistics. This is typically preferred over static scaling for its adaptive robustness.\n\n##### Loss and Gradient Scaling with `GradScaler`\n\n*   A fundamental challenge of half-precision (`float16`) training is the limited dynamic range, which can cause small-magnitude gradients to underflow—i.e., round down to zero—during backpropagation. This occurs because when an operation receives `float16` inputs in the forward pass, the resulting gradients computed in the backward pass are also in `float16`, unless explicitly handled. In deep learning, many gradients—particularly in early layers or at later training stages—can be extremely small, and when these are flushed to zero, their corresponding weight updates are effectively lost, impeding learning.\n    \n*   To mitigate this, PyTorch introduces **loss scaling**, a technique that amplifies loss values and their corresponding gradients during the backward pass to avoid underflow. The process works as follows:\n    \n    1.  The loss is multiplied by a scale factor before backpropagation.\n    2.  Gradients are computed on this scaled loss, resulting in proportionally larger values.\n    3.  These gradients are then unscaled before the optimizer applies the update, preserving the intended learning dynamics.\n*   This technique is implemented via the `torch.cuda.amp.GradScaler` object, which automates both the scaling and unscaling process, as well as overflow detection and recovery. The goal is to find a balance: a scale factor high enough to preserve small gradients, yet not so high that large gradients overflow and become `inf`—maintaining a balance between underflow and overflow.\n    \n\nA fundamental challenge of half-precision (`float16`) training is the limited dynamic range, which can cause small-magnitude gradients to underflow—i.e., round down to zero—during backpropagation. This occurs because when an operation receives `float16` inputs in the forward pass, the resulting gradients computed in the backward pass are also in `float16`, unless explicitly handled. In deep learning, many gradients—particularly in early layers or at later training stages—can be extremely small, and when these are flushed to zero, their corresponding weight updates are effectively lost, impeding learning.\n\nTo mitigate this, PyTorch introduces **loss scaling**, a technique that amplifies loss values and their corresponding gradients during the backward pass to avoid underflow. The process works as follows:\n\n1.  The loss is multiplied by a scale factor before backpropagation.\n2.  Gradients are computed on this scaled loss, resulting in proportionally larger values.\n3.  These gradients are then unscaled before the optimizer applies the update, preserving the intended learning dynamics.\n\nThis technique is implemented via the `torch.cuda.amp.GradScaler` object, which automates both the scaling and unscaling process, as well as overflow detection and recovery. The goal is to find a balance: a scale factor high enough to preserve small gradients, yet not so high that large gradients overflow and become `inf`—maintaining a balance between underflow and overflow.\n\n###### Dynamic Scaling with Exponential Backoff\n\n*   There is no single static loss multiplier that suits all models or all stages of training. Gradient magnitudes are typically much larger at the beginning of training and diminish as convergence nears. Rather than asking users to manually tune this value, PyTorch uses an adaptive approach based on **exponential backoff**.\n    \n*   The `GradScaler` begins with an initial scale (default: 65,536 or 2162162^{16}) and periodically doubles it to maximize numerical range. If an overflow is detected—i.e., any gradient becomes `inf` or `NaN`—the current update step is skipped, the scale is halved, and a cooldown counter is reset. This approach allows PyTorch to adaptively find a safe and efficient scaling factor over time, much like TCP congestion control adapts network throughput.\n    \n*   This behavior can be configured via the `GradScaler` constructor:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n    `torch.cuda.amp.GradScaler(     init_scale=65536.0,     growth_factor=2.0,     backoff_factor=0.5,     growth_interval=2000,     enabled=True )`\n    \n    *   `init_scale`: The initial scaling factor.\n    *   `growth_factor`: Multiplicative increase rate when no overflows are detected.\n    *   `backoff_factor`: Reduction factor when an overflow is detected.\n    *   `growth_interval`: Number of successful steps before scale growth is attempted.\n    *   `enabled`: Whether AMP and scaling are active.\n*   **Operational Considerations**:\n    \n    *   `GradScaler` modifies key parts of the training loop:\n        \n        *   `loss.backward()` becomes `scaler.scale(loss).backward()`\n        *   `optimizer.step()` becomes `scaler.step(optimizer)`\n        *   The call to `scaler.update()` checks for overflows and adjusts the scale as needed.\n    *   It is important to note that overflows (`inf`) are detectable and trigger corrective behavior. Underflows, however, are silent because zero gradients are not always erroneous. Thus, choosing a very low initial scale or a very long growth interval may cause the network to silently underperform or diverge. PyTorch’s large default `init_scale` mitigates this risk.\n        \n    *   Internally, before the optimizer updates the model weights, the gradients (`.grad`) are unscaled to ensure the learning rate and optimizer dynamics remain consistent with those expected in `float32` training.\n        \n*   **Checkpointing with GradScaler**:\n    \n    *   Because `GradScaler` is a stateful object that adapts over time, it must be saved and restored along with the model and optimizer during checkpointing. PyTorch provides simple APIs for this:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n    `# Saving torch.save({     'model_state_dict': model.state_dict(),     'optimizer_state_dict': optimizer.state_dict(),     'scaler_state_dict': scaler.state_dict() }, 'checkpoint.pt')  # Loading checkpoint = torch.load('checkpoint.pt') model.load_state_dict(checkpoint['model_state_dict']) optimizer.load_state_dict(checkpoint['optimizer_state_dict']) scaler.load_state_dict(checkpoint['scaler_state_dict'])`\n    \n    *   By integrating `GradScaler` into the training process, PyTorch ensures that the numerical precision limitations of `float16` do not compromise convergence, while still allowing significant performance and memory efficiency gains.\n\nThere is no single static loss multiplier that suits all models or all stages of training. Gradient magnitudes are typically much larger at the beginning of training and diminish as convergence nears. Rather than asking users to manually tune this value, PyTorch uses an adaptive approach based on **exponential backoff**.\n\nThe `GradScaler` begins with an initial scale (default: 65,536 or 2162162^{16}) and periodically doubles it to maximize numerical range. If an overflow is detected—i.e., any gradient becomes `inf` or `NaN`—the current update step is skipped, the scale is halved, and a cooldown counter is reset. This approach allows PyTorch to adaptively find a safe and efficient scaling factor over time, much like TCP congestion control adapts network throughput.\n\nThis behavior can be configured via the `GradScaler` constructor:\n\n![](https://aman.ai/images/copy.png)\n\n`torch.cuda.amp.GradScaler(     init_scale=65536.0,     growth_factor=2.0,     backoff_factor=0.5,     growth_interval=2000,     enabled=True )`\n\n*   `init_scale`: The initial scaling factor.\n*   `growth_factor`: Multiplicative increase rate when no overflows are detected.\n*   `backoff_factor`: Reduction factor when an overflow is detected.\n*   `growth_interval`: Number of successful steps before scale growth is attempted.\n*   `enabled`: Whether AMP and scaling are active.\n\n**Operational Considerations**:\n\n*   `GradScaler` modifies key parts of the training loop:\n    \n    *   `loss.backward()` becomes `scaler.scale(loss).backward()`\n    *   `optimizer.step()` becomes `scaler.step(optimizer)`\n    *   The call to `scaler.update()` checks for overflows and adjusts the scale as needed.\n*   It is important to note that overflows (`inf`) are detectable and trigger corrective behavior. Underflows, however, are silent because zero gradients are not always erroneous. Thus, choosing a very low initial scale or a very long growth interval may cause the network to silently underperform or diverge. PyTorch’s large default `init_scale` mitigates this risk.\n    \n*   Internally, before the optimizer updates the model weights, the gradients (`.grad`) are unscaled to ensure the learning rate and optimizer dynamics remain consistent with those expected in `float32` training.\n    \n\n`GradScaler` modifies key parts of the training loop:\n\n*   `loss.backward()` becomes `scaler.scale(loss).backward()`\n*   `optimizer.step()` becomes `scaler.step(optimizer)`\n*   The call to `scaler.update()` checks for overflows and adjusts the scale as needed.\n\nIt is important to note that overflows (`inf`) are detectable and trigger corrective behavior. Underflows, however, are silent because zero gradients are not always erroneous. Thus, choosing a very low initial scale or a very long growth interval may cause the network to silently underperform or diverge. PyTorch’s large default `init_scale` mitigates this risk.\n\nInternally, before the optimizer updates the model weights, the gradients (`.grad`) are unscaled to ensure the learning rate and optimizer dynamics remain consistent with those expected in `float32` training.\n\n**Checkpointing with GradScaler**:\n\n*   Because `GradScaler` is a stateful object that adapts over time, it must be saved and restored along with the model and optimizer during checkpointing. PyTorch provides simple APIs for this:\n\n![](https://aman.ai/images/copy.png)\n\n`# Saving torch.save({     'model_state_dict': model.state_dict(),     'optimizer_state_dict': optimizer.state_dict(),     'scaler_state_dict': scaler.state_dict() }, 'checkpoint.pt')  # Loading checkpoint = torch.load('checkpoint.pt') model.load_state_dict(checkpoint['model_state_dict']) optimizer.load_state_dict(checkpoint['optimizer_state_dict']) scaler.load_state_dict(checkpoint['scaler_state_dict'])`\n\n*   By integrating `GradScaler` into the training process, PyTorch ensures that the numerical precision limitations of `float16` do not compromise convergence, while still allowing significant performance and memory efficiency gains.\n\n##### Automatic Precision Casting with the `autocast` Context Manager\n\n*   The second key component of PyTorch’s AMP system is the `torch.cuda.amp.autocast` context manager. While `GradScaler` addresses numerical stability during backpropagation via loss scaling, `autocast` is responsible for precision control during the forward pass.\n    \n*   Mixed precision training derives its speed and memory benefits primarily by executing selected operations in `float16` rather than `float32`. However, not all operations are equally safe or efficient in half precision. Some are numerically stable and performant when cast to `float16`, while others require higher precision to avoid instability or incorrect outputs.\n    \n*   The `autocast` context manager dynamically casts operations to the most appropriate precision at runtime. This casting is done based on an internal whitelist/blacklist system defined by PyTorch, taking into account both the operation type and the tensor data types involved. This enables users to delegate dtype management to PyTorch, avoiding manual casting and type-checking logic.\n    \n*   **How `autocast` Works Internally**:\n    \n    *   Operations such as matrix multiplications (`matmul`), convolutions (`conv2d`), and other linear algebraic operations are generally safe to perform in `float16`, and thus are automatically downcast when inside an `autocast` context.\n    *   Conversely, operations that are sensitive to numerical precision—such as logarithms, exponentials, trigonometric functions, and large summations—are retained in `float32` to ensure computational accuracy.\n        \n    *   The following visuals ([source](https://pytorch.org/docs/master/amp.html#autocast-op-reference)), summarize these distinctions. The image below outlines common operations that benefit from `float16` execution. These include core building blocks of deep learning models like matrix multiplications, dot products, and convolutions. Their stability in half precision makes them ideal candidates for mixed precision acceleration.\n    \n    ![](/primers/ai/assets/model-compression/ops_widest.avif)\n    \n    *   In contrast, as shown in the image below ([source](https://pytorch.org/docs/master/amp.html#autocast-op-reference)), operations involving logarithms, exponentials, or statistical reductions tend to suffer from rounding errors in `float16` and are therefore retained in `float32`.\n    \n    ![](/primers/ai/assets/model-compression/autocast_f32.avif)\n    \n*   **Implications for Model Layers**:\n    \n    *   These rules imply that:\n        \n        *   Most **layers** (e.g., linear, convolutional, attention) benefit substantially from autocasting, due to their reliance on matrix operations.\n        *   Most **activation functions** and **normalization layers** are less safe in `float16`, and autocast will retain full precision where necessary.\n        *   The **greatest performance gains** are likely in deep CNNs or transformer models with many linear operations and matrix multiplications.\n*   **Using `autocast` in Practice**:\n    \n    *   Enabling autocasting is simple and requires wrapping the forward pass in a context manager:\n    \n    ![](https://aman.ai/images/copy.png)\n    \n    `with torch.cuda.amp.autocast():     y_pred = model(X_batch).squeeze()     loss = self.loss_fn(y_pred, y_batch)`\n    \n    *   All operations within the `autocast()` context will be executed with optimal mixed precision, determined internally by PyTorch. Importantly, this casting behavior extends to the backward pass automatically—there is no need to wrap `loss.backward()`.\n*   **Best Practices and Notes**:\n    \n    *   Autocast respects and supports a wide range of PyTorch operators out-of-the-box. Unless using custom operations or extensions, most models will run correctly without additional intervention.\n    *   In-place operations (e.g., `.add_()` or `.relu_()`) can interfere with autocast’s internal precision control. Avoid in-place modifications inside `autocast()` blocks unless explicitly supported.\n    *   Autocast is deterministic and composable. It can be used inside model layers, training loops, or custom modules with consistent behavior.\n    *   For inference scenarios, autocasting is also beneficial and can be enabled in evaluation mode to reduce memory usage without requiring `GradScaler`.\n\nThe second key component of PyTorch’s AMP system is the `torch.cuda.amp.autocast` context manager. While `GradScaler` addresses numerical stability during backpropagation via loss scaling, `autocast` is responsible for precision control during the forward pass.\n\nMixed precision training derives its speed and memory benefits primarily by executing selected operations in `float16` rather than `float32`. However, not all operations are equally safe or efficient in half precision. Some are numerically stable and performant when cast to `float16`, while others require higher precision to avoid instability or incorrect outputs.\n\nThe `autocast` context manager dynamically casts operations to the most appropriate precision at runtime. This casting is done based on an internal whitelist/blacklist system defined by PyTorch, taking into account both the operation type and the tensor data types involved. This enables users to delegate dtype management to PyTorch, avoiding manual casting and type-checking logic.\n\n**How `autocast` Works Internally**:\n\n*   Operations such as matrix multiplications (`matmul`), convolutions (`conv2d`), and other linear algebraic operations are generally safe to perform in `float16`, and thus are automatically downcast when inside an `autocast` context.\n*   Conversely, operations that are sensitive to numerical precision—such as logarithms, exponentials, trigonometric functions, and large summations—are retained in `float32` to ensure computational accuracy.\n    \n*   The following visuals ([source](https://pytorch.org/docs/master/amp.html#autocast-op-reference)), summarize these distinctions. The image below outlines common operations that benefit from `float16` execution. These include core building blocks of deep learning models like matrix multiplications, dot products, and convolutions. Their stability in half precision makes them ideal candidates for mixed precision acceleration.\n\nConversely, operations that are sensitive to numerical precision—such as logarithms, exponentials, trigonometric functions, and large summations—are retained in `float32` to ensure computational accuracy.\n\n![](/primers/ai/assets/model-compression/ops_widest.avif)\n\n*   In contrast, as shown in the image below ([source](https://pytorch.org/docs/master/amp.html#autocast-op-reference)), operations involving logarithms, exponentials, or statistical reductions tend to suffer from rounding errors in `float16` and are therefore retained in `float32`.\n\n![](/primers/ai/assets/model-compression/autocast_f32.avif)\n\n**Implications for Model Layers**:\n\n*   These rules imply that:\n    \n    *   Most **layers** (e.g., linear, convolutional, attention) benefit substantially from autocasting, due to their reliance on matrix operations.\n    *   Most **activation functions** and **normalization layers** are less safe in `float16`, and autocast will retain full precision where necessary.\n    *   The **greatest performance gains** are likely in deep CNNs or transformer models with many linear operations and matrix multiplications.\n\nThese rules imply that:\n\n*   Most **layers** (e.g., linear, convolutional, attention) benefit substantially from autocasting, due to their reliance on matrix operations.\n*   Most **activation functions** and **normalization layers** are less safe in `float16`, and autocast will retain full precision where necessary.\n*   The **greatest performance gains** are likely in deep CNNs or transformer models with many linear operations and matrix multiplications.\n\n**Using `autocast` in Practice**:\n\n*   Enabling autocasting is simple and requires wrapping the forward pass in a context manager:\n\n![](https://aman.ai/images/copy.png)\n\n`with torch.cuda.amp.autocast():     y_pred = model(X_batch).squeeze()     loss = self.loss_fn(y_pred, y_batch)`\n\n*   All operations within the `autocast()` context will be executed with optimal mixed precision, determined internally by PyTorch. Importantly, this casting behavior extends to the backward pass automatically—there is no need to wrap `loss.backward()`.\n\n**Best Practices and Notes**:\n\n*   Autocast respects and supports a wide range of PyTorch operators out-of-the-box. Unless using custom operations or extensions, most models will run correctly without additional intervention.\n*   In-place operations (e.g., `.add_()` or `.relu_()`) can interfere with autocast’s internal precision control. Avoid in-place modifications inside `autocast()` blocks unless explicitly supported.\n*   Autocast is deterministic and composable. It can be used inside model layers, training loops, or custom modules with consistent behavior.\n*   For inference scenarios, autocasting is also beneficial and can be enabled in evaluation mode to reduce memory usage without requiring `GradScaler`.\n\n##### Using AMP with Multiple GPUs\n\n*   PyTorch’s Automatic Mixed Precision (AMP) functionality is fully compatible with multi-GPU training, enabling developers to scale up performance without sacrificing the benefits of mixed precision. Both of PyTorch’s multi-GPU parallelization strategies—`DistributedDataParallel` (DDP) and `DataParallel`—support autocasting and gradient scaling, with minimal adjustments.\n    \n*   AMP’s multi-GPU support is robust and integrates seamlessly into distributed training workflows. With only minor adjustments, developers can leverage both horizontal scaling and mixed precision optimization, achieving faster training with efficient GPU utilization across multiple devices.\n    \n    *   **DistributedDataParallel (DDP)**: AMP works out-of-the-box with DDP, which is the recommended strategy for multi-GPU training. The key requirement is to use one process per GPU, following the standard setup for DDP. This ensures independent autocast and `GradScaler` instances per GPU, maintaining stability and efficiency.\n        \n    *   **DataParallel**: AMP also works with `DataParallel`, but with a caveat. Since `DataParallel` uses a single process to drive multiple devices, it shares the autocast and scaling logic across GPUs. To accommodate this, one small adjustment must be made as outlined in the official [AMP Examples](https://pytorch.org/docs/master/notes/amp_examples.html#dataparallel-in-a-single-process) guide. Specifically, ensure that loss scaling is only performed on the output of the model’s `.forward()` call on the main device, before broadcasting gradients.\n        \n    *   **Implementation Tips**:\n        \n        *   Refer to the [Working with Multiple GPUs](https://pytorch.org/docs/master/notes/amp_examples.html#working-with-multiple-gpus) section in the PyTorch AMP documentation for detailed examples and best practices.\n        *   Be mindful of numerical stability when using binary classification loss functions. The AMP documentation recommends [preferring binary cross entropy with logits over binary cross entropy](https://pytorch.org/docs/master/amp.html#prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy), as the logits version is more numerically stable and better suited for mixed precision.\n\nPyTorch’s Automatic Mixed Precision (AMP) functionality is fully compatible with multi-GPU training, enabling developers to scale up performance without sacrificing the benefits of mixed precision. Both of PyTorch’s multi-GPU parallelization strategies—`DistributedDataParallel` (DDP) and `DataParallel`—support autocasting and gradient scaling, with minimal adjustments.\n\nAMP’s multi-GPU support is robust and integrates seamlessly into distributed training workflows. With only minor adjustments, developers can leverage both horizontal scaling and mixed precision optimization, achieving faster training with efficient GPU utilization across multiple devices.\n\n*   **DistributedDataParallel (DDP)**: AMP works out-of-the-box with DDP, which is the recommended strategy for multi-GPU training. The key requirement is to use one process per GPU, following the standard setup for DDP. This ensures independent autocast and `GradScaler` instances per GPU, maintaining stability and efficiency.\n    \n*   **DataParallel**: AMP also works with `DataParallel`, but with a caveat. Since `DataParallel` uses a single process to drive multiple devices, it shares the autocast and scaling logic across GPUs. To accommodate this, one small adjustment must be made as outlined in the official [AMP Examples](https://pytorch.org/docs/master/notes/amp_examples.html#dataparallel-in-a-single-process) guide. Specifically, ensure that loss scaling is only performed on the output of the model’s `.forward()` call on the main device, before broadcasting gradients.\n    \n*   **Implementation Tips**:\n    \n    *   Refer to the [Working with Multiple GPUs](https://pytorch.org/docs/master/notes/amp_examples.html#working-with-multiple-gpus) section in the PyTorch AMP documentation for detailed examples and best practices.\n    *   Be mindful of numerical stability when using binary classification loss functions. The AMP documentation recommends [preferring binary cross entropy with logits over binary cross entropy](https://pytorch.org/docs/master/amp.html#prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy), as the logits version is more numerically stable and better suited for mixed precision.\n\n**DistributedDataParallel (DDP)**: AMP works out-of-the-box with DDP, which is the recommended strategy for multi-GPU training. The key requirement is to use one process per GPU, following the standard setup for DDP. This ensures independent autocast and `GradScaler` instances per GPU, maintaining stability and efficiency.\n\n**DataParallel**: AMP also works with `DataParallel`, but with a caveat. Since `DataParallel` uses a single process to drive multiple devices, it shares the autocast and scaling logic across GPUs. To accommodate this, one small adjustment must be made as outlined in the official [AMP Examples](https://pytorch.org/docs/master/notes/amp_examples.html#dataparallel-in-a-single-process) guide. Specifically, ensure that loss scaling is only performed on the output of the model’s `.forward()` call on the main device, before broadcasting gradients.\n\n**Implementation Tips**:\n\n*   Refer to the [Working with Multiple GPUs](https://pytorch.org/docs/master/notes/amp_examples.html#working-with-multiple-gpus) section in the PyTorch AMP documentation for detailed examples and best practices.\n*   Be mindful of numerical stability when using binary classification loss functions. The AMP documentation recommends [preferring binary cross entropy with logits over binary cross entropy](https://pytorch.org/docs/master/amp.html#prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy), as the logits version is more numerically stable and better suited for mixed precision.\n\n##### Memory Considerations\n\n*   One of the advertised benefits of mixed precision training, in addition to performance speedups, is reduced GPU memory consumption. As discussed in the earlier section on [How Mixed Precision Works](#how-mixed-precision-works), `float16` tensors require half the storage space of their `float32` counterparts. This reduction in memory footprint can be particularly advantageous in training large-scale models, where memory constraints often limit batch size or model complexity.\n    \n*   Although GPU compute is generally the primary bottleneck in training workloads, optimizing memory usage remains important. Efficient memory utilization enables:\n    \n    *   Larger batch sizes, which can improve training stability and convergence.\n    *   The ability to fit deeper or wider models within available hardware constraints.\n    *   Reduced reliance on gradient checkpointing or memory-efficient architectures.\n*   PyTorch manages GPU memory allocation proactively. At the start of training, it reserves a block of GPU memory that it maintains throughout the training lifecycle. This behavior helps avoid runtime memory fragmentation and preempts crashes caused by other processes occupying memory mid-training. However, it also means that the effect of mixed precision on memory usage may not always be visible in a straightforward manner.\n    \n*   The figure below illustrates PyTorch’s memory reservation behavior with and without AMP enabled:\n    \n\nOne of the advertised benefits of mixed precision training, in addition to performance speedups, is reduced GPU memory consumption. As discussed in the earlier section on [How Mixed Precision Works](#how-mixed-precision-works), `float16` tensors require half the storage space of their `float32` counterparts. This reduction in memory footprint can be particularly advantageous in training large-scale models, where memory constraints often limit batch size or model complexity.\n\nAlthough GPU compute is generally the primary bottleneck in training workloads, optimizing memory usage remains important. Efficient memory utilization enables:\n\n*   Larger batch sizes, which can improve training stability and convergence.\n*   The ability to fit deeper or wider models within available hardware constraints.\n*   Reduced reliance on gradient checkpointing or memory-efficient architectures.\n\nPyTorch manages GPU memory allocation proactively. At the start of training, it reserves a block of GPU memory that it maintains throughout the training lifecycle. This behavior helps avoid runtime memory fragmentation and preempts crashes caused by other processes occupying memory mid-training. However, it also means that the effect of mixed precision on memory usage may not always be visible in a straightforward manner.\n\nThe figure below illustrates PyTorch’s memory reservation behavior with and without AMP enabled:\n\n![](/primers/ai/assets/model-compression/amp_mem.avif)\n\n*   Interestingly, while both UNet and BERT models exhibit a reduction in memory usage when AMP is enabled, the gains are model-dependent. UNet, in particular, benefits significantly more than BERT. This discrepancy may result from differences in internal layer composition, memory allocation patterns, or the proportion of operations compatible with `float16`. PyTorch’s memory allocator is largely opaque, making it difficult to pinpoint exact causes without in-depth profiling.\n    \n*   Nonetheless, practitioners can generally expect mixed precision to reduce overall memory usage, especially in convolution-heavy models like UNet. This makes AMP not only a tool for acceleration but also a practical memory optimization strategy, particularly beneficial for users working within the limits of consumer-grade GPUs or training on multiple models in parallel.\n    \n\nInterestingly, while both UNet and BERT models exhibit a reduction in memory usage when AMP is enabled, the gains are model-dependent. UNet, in particular, benefits significantly more than BERT. This discrepancy may result from differences in internal layer composition, memory allocation patterns, or the proportion of operations compatible with `float16`. PyTorch’s memory allocator is largely opaque, making it difficult to pinpoint exact causes without in-depth profiling.\n\nNonetheless, practitioners can generally expect mixed precision to reduce overall memory usage, especially in convolution-heavy models like UNet. This makes AMP not only a tool for acceleration but also a practical memory optimization strategy, particularly beneficial for users working within the limits of consumer-grade GPUs or training on multiple models in parallel.\n\n##### Further Reading\n\n*   For further reading and in-depth examples, consult the official documentation:\n    \n    *   [Automatic Mixed Precision package](https://pytorch.org/docs/master/amp.html)\n    *   [Automatic Mixed Precision examples](https://pytorch.org/docs/master/notes/amp_examples.html)\n\nFor further reading and in-depth examples, consult the official documentation:\n\n*   [Automatic Mixed Precision package](https://pytorch.org/docs/master/amp.html)\n*   [Automatic Mixed Precision examples](https://pytorch.org/docs/master/notes/amp_examples.html)\n\n#### How TensorFlow Automatic Mixed Precision Works\n\n*   Mixed precision training in TensorFlow is designed to accelerate deep learning workloads by leveraging the efficiency of lower-precision (`float16`) arithmetic on supported hardware. With automatic mixed precision (AMP), TensorFlow streamlines the use of mixed-precision computation while preserving numerical stability and minimizing manual intervention.\n    \n*   TensorFlow’s AMP support offers a robust, efficient, and production-ready pathway to accelerate model training with minimal code changes, using a hybrid approach similar to PyTorch. Operations are executed in `float16` where safe for performance and memory efficiency, while numerically sensitive computations remain in `float32` to preserve stability and ensure convergence.\n    \n*   Built around the `mixed_precision` module, TensorFlow provides a high-level, intuitive interface for enabling efficient mixed-precision training. By leveraging global policies and loss scaling under the hood, TensorFlow abstracts away much of the complexity involved in training with `float16`. No manual casting or scaling logic is needed for most models, making integration straightforward for both experimentation and production. These features bring the practical benefits of faster computation and reduced memory usage—especially when training on modern GPUs—while maintaining the reliability of full-precision training. For a complete guide and reference examples, refer to the [official TensorFlow mixed precision guide](https://www.tensorflow.org/guide/mixed_precision).\n    \n\nMixed precision training in TensorFlow is designed to accelerate deep learning workloads by leveraging the efficiency of lower-precision (`float16`) arithmetic on supported hardware. With automatic mixed precision (AMP), TensorFlow streamlines the use of mixed-precision computation while preserving numerical stability and minimizing manual intervention.\n\nTensorFlow’s AMP support offers a robust, efficient, and production-ready pathway to accelerate model training with minimal code changes, using a hybrid approach similar to PyTorch. Operations are executed in `float16` where safe for performance and memory efficiency, while numerically sensitive computations remain in `float32` to preserve stability and ensure convergence.\n\nBuilt around the `mixed_precision` module, TensorFlow provides a high-level, intuitive interface for enabling efficient mixed-precision training. By leveraging global policies and loss scaling under the hood, TensorFlow abstracts away much of the complexity involved in training with `float16`. No manual casting or scaling logic is needed for most models, making integration straightforward for both experimentation and production. These features bring the practical benefits of faster computation and reduced memory usage—especially when training on modern GPUs—while maintaining the reliability of full-precision training. For a complete guide and reference examples, refer to the [official TensorFlow mixed precision guide](https://www.tensorflow.org/guide/mixed_precision).\n\n##### Conceptual Overview\n\n*   Mixed precision training in TensorFlow works by executing computations in half-precision (`float16`) where safe, and in single-precision (`float32`) where required for numerical stability. This selective usage of data types reduces memory bandwidth and speeds up computation, particularly on GPUs equipped with NVIDIA Tensor Cores (e.g., V100, T4, A100). These architectures are specifically designed to handle mixed-precision workloads efficiently.\n    \n*   The two core features of TensorFlow’s AMP system are:\n    \n    *   **Global Policy Management**: Mixed precision is enabled by setting a global or per-layer dtype policy to `'mixed_float16'`. This instructs TensorFlow to automatically cast eligible operations to `float16` while retaining critical variables (e.g., weights, certain accumulators) in `float32`.\n        \n    *   **Loss Scaling with `LossScaleOptimizer`**: To mitigate the risk of underflow—when gradient values fall below the representable range of `float16`—TensorFlow introduces automatic loss scaling. This mechanism adaptively maintains numerical stability without manual tuning, multiplying the loss by a scalar factor before backpropagation and reverting it afterward. This is enabled by wrapping a base optimizer with `tf.keras.mixed_precision.LossScaleOptimizer`.\n        \n*   These two features make mixed precision safe for most real-world training scenarios, enabling users to benefit from performance gains without manual tensor casting or custom scaling logic.\n    \n\nMixed precision training in TensorFlow works by executing computations in half-precision (`float16`) where safe, and in single-precision (`float32`) where required for numerical stability. This selective usage of data types reduces memory bandwidth and speeds up computation, particularly on GPUs equipped with NVIDIA Tensor Cores (e.g., V100, T4, A100). These architectures are specifically designed to handle mixed-precision workloads efficiently.\n\nThe two core features of TensorFlow’s AMP system are:\n\n*   **Global Policy Management**: Mixed precision is enabled by setting a global or per-layer dtype policy to `'mixed_float16'`. This instructs TensorFlow to automatically cast eligible operations to `float16` while retaining critical variables (e.g., weights, certain accumulators) in `float32`.\n    \n*   **Loss Scaling with `LossScaleOptimizer`**: To mitigate the risk of underflow—when gradient values fall below the representable range of `float16`—TensorFlow introduces automatic loss scaling. This mechanism adaptively maintains numerical stability without manual tuning, multiplying the loss by a scalar factor before backpropagation and reverting it afterward. This is enabled by wrapping a base optimizer with `tf.keras.mixed_precision.LossScaleOptimizer`.\n    \n\n**Global Policy Management**: Mixed precision is enabled by setting a global or per-layer dtype policy to `'mixed_float16'`. This instructs TensorFlow to automatically cast eligible operations to `float16` while retaining critical variables (e.g., weights, certain accumulators) in `float32`.\n\n**Loss Scaling with `LossScaleOptimizer`**: To mitigate the risk of underflow—when gradient values fall below the representable range of `float16`—TensorFlow introduces automatic loss scaling. This mechanism adaptively maintains numerical stability without manual tuning, multiplying the loss by a scalar factor before backpropagation and reverting it afterward. This is enabled by wrapping a base optimizer with `tf.keras.mixed_precision.LossScaleOptimizer`.\n\nThese two features make mixed precision safe for most real-world training scenarios, enabling users to benefit from performance gains without manual tensor casting or custom scaling logic.\n\n##### Practical Implementation in a Training Pipeline\n\n*   TensorFlow’s mixed precision training is designed to be seamless, requiring only a few lines of code to enable. The following example demonstrates a typical setup using the `tf.keras` API.\n\n![](https://aman.ai/images/copy.png)\n\n`import tensorflow as tf from tensorflow.keras import mixed_precision  # Enable mixed precision globally mixed_precision.set_global_policy('mixed_float16')  # Define a model (example: simple MLP) model = tf.keras.Sequential([     tf.keras.layers.Dense(256, activation='relu'),     tf.keras.layers.Dense(128, activation='relu'),     tf.keras.layers.Dense(1) ])  # Wrap the optimizer with LossScaleOptimizer for stability base_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) optimizer = mixed_precision.LossScaleOptimizer(base_optimizer)  # Compile the model model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])  # Prepare training data X_train = tf.random.normal((10000, 20)) y_train = tf.random.normal((10000, 1))  # Train the model model.fit(X_train, y_train, batch_size=64, epochs=10)`\n\n![](https://aman.ai/images/copy.png)\n\n`import tensorflow as tf from tensorflow.keras import mixed_precision  # Enable mixed precision globally mixed_precision.set_global_policy('mixed_float16')  # Define a model (example: simple MLP) model = tf.keras.Sequential([     tf.keras.layers.Dense(256, activation='relu'),     tf.keras.layers.Dense(128, activation='relu'),     tf.keras.layers.Dense(1) ])  # Wrap the optimizer with LossScaleOptimizer for stability base_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) optimizer = mixed_precision.LossScaleOptimizer(base_optimizer)  # Compile the model model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])  # Prepare training data X_train = tf.random.normal((10000, 20)) y_train = tf.random.normal((10000, 1))  # Train the model model.fit(X_train, y_train, batch_size=64, epochs=10)`\n\n*   **Explanation of Key Components:**\n    \n    *   **`set_global_policy('mixed_float16')`**: This sets the default computation policy across all layers to use `float16` where safe, while storing variables such as model weights in `float32` to ensure stability.\n        \n    *   **`LossScaleOptimizer`**: The base optimizer (e.g., `Adam`) is wrapped to apply dynamic loss scaling. This prevents numerical underflows by adapting the loss scaling factor based on gradient stability during training.\n        \n    *   **Hardware Requirements**: While AMP can be enabled on any GPU, maximum performance benefits are realized on NVIDIA GPUs with Tensor Cores, such as the Volta (V100), Turing (T4), or Ampere (A100) architectures.\n        \n*   **Layer Compatibility and Custom Layers**\n    \n    *   Most built-in TensorFlow and Keras layers support AMP without modification. If you’re using custom layers or third-party code, ensure that:\n        \n        *   Operations numerically sensitive to precision are forced to `float32` if needed (using `tf.cast`).\n        *   Custom gradients are correctly handled, especially in layers using `tf.custom_gradient`.\n*   **Model Behavior and Performance Considerations**:\n    \n    *   **Storage Format**: Weights are stored in `float32` internally, but computations are cast to `float16` where safe. This ensures a balance between performance and accuracy.\n        \n    *   **Layer Compatibility**: Most built-in Keras layers are fully compatible with AMP. Custom layers or third-party operations may require manual casting using `tf.cast()` or explicit dtype management.\n        \n    *   **Inference**: After training, models trained with AMP can be saved and exported as usual. During inference, the `mixed_float16` policy can remain active to reduce latency and memory usage, particularly for large batch sizes.\n        \n*   **Best Practices for TensorFlow AMP**:\n    \n    *   Enable AMP by default (i.e., use `mixed_float16`) when training on Tensor Core GPUs, especially for models with substantial compute demands. Use dynamic loss scaling (enabled by default with `LossScaleOptimizer`) to maintain stability during training without the need for manual tuning.\n    *   Monitor training for gradient anomalies (e.g., sudden spikes in loss). Although AMP is robust, occasional divergence may indicate the need for a lower initial loss scale or a refined model architecture.\n    *   Use benchmark tools (e.g., TensorBoard, NVIDIA Nsight Systems) to validate performance gains and ensure your training benefits from mixed precision on supported hardware.\n\n**Explanation of Key Components:**\n\n*   **`set_global_policy('mixed_float16')`**: This sets the default computation policy across all layers to use `float16` where safe, while storing variables such as model weights in `float32` to ensure stability.\n    \n*   **`LossScaleOptimizer`**: The base optimizer (e.g., `Adam`) is wrapped to apply dynamic loss scaling. This prevents numerical underflows by adapting the loss scaling factor based on gradient stability during training.\n    \n*   **Hardware Requirements**: While AMP can be enabled on any GPU, maximum performance benefits are realized on NVIDIA GPUs with Tensor Cores, such as the Volta (V100), Turing (T4), or Ampere (A100) architectures.\n    \n\n**`set_global_policy('mixed_float16')`**: This sets the default computation policy across all layers to use `float16` where safe, while storing variables such as model weights in `float32` to ensure stability.\n\n**`LossScaleOptimizer`**: The base optimizer (e.g., `Adam`) is wrapped to apply dynamic loss scaling. This prevents numerical underflows by adapting the loss scaling factor based on gradient stability during training.\n\n**Hardware Requirements**: While AMP can be enabled on any GPU, maximum performance benefits are realized on NVIDIA GPUs with Tensor Cores, such as the Volta (V100), Turing (T4), or Ampere (A100) architectures.\n\n**Layer Compatibility and Custom Layers**\n\n*   Most built-in TensorFlow and Keras layers support AMP without modification. If you’re using custom layers or third-party code, ensure that:\n    \n    *   Operations numerically sensitive to precision are forced to `float32` if needed (using `tf.cast`).\n    *   Custom gradients are correctly handled, especially in layers using `tf.custom_gradient`.\n\nMost built-in TensorFlow and Keras layers support AMP without modification. If you’re using custom layers or third-party code, ensure that:\n\n*   Operations numerically sensitive to precision are forced to `float32` if needed (using `tf.cast`).\n*   Custom gradients are correctly handled, especially in layers using `tf.custom_gradient`.\n\n**Model Behavior and Performance Considerations**:\n\n*   **Storage Format**: Weights are stored in `float32` internally, but computations are cast to `float16` where safe. This ensures a balance between performance and accuracy.\n    \n*   **Layer Compatibility**: Most built-in Keras layers are fully compatible with AMP. Custom layers or third-party operations may require manual casting using `tf.cast()` or explicit dtype management.\n    \n*   **Inference**: After training, models trained with AMP can be saved and exported as usual. During inference, the `mixed_float16` policy can remain active to reduce latency and memory usage, particularly for large batch sizes.\n    \n\n**Storage Format**: Weights are stored in `float32` internally, but computations are cast to `float16` where safe. This ensures a balance between performance and accuracy.\n\n**Layer Compatibility**: Most built-in Keras layers are fully compatible with AMP. Custom layers or third-party operations may require manual casting using `tf.cast()` or explicit dtype management.\n\n**Inference**: After training, models trained with AMP can be saved and exported as usual. During inference, the `mixed_float16` policy can remain active to reduce latency and memory usage, particularly for large batch sizes.\n\n**Best Practices for TensorFlow AMP**:\n\n*   Enable AMP by default (i.e., use `mixed_float16`) when training on Tensor Core GPUs, especially for models with substantial compute demands. Use dynamic loss scaling (enabled by default with `LossScaleOptimizer`) to maintain stability during training without the need for manual tuning.\n*   Monitor training for gradient anomalies (e.g., sudden spikes in loss). Although AMP is robust, occasional divergence may indicate the need for a lower initial loss scale or a refined model architecture.\n*   Use benchmark tools (e.g., TensorBoard, NVIDIA Nsight Systems) to validate performance gains and ensure your training benefits from mixed precision on supported hardware.\n\n##### Performance Benchmarks\n\n*   To evaluate the real-world impact of mixed precision training, consider benchmarks run across three distinct neural network architectures using TensorFlow’s AMP implementation.\n*   These benchmarks demonstrate that **automatic mixed precision should be one of the first performance optimizations you apply to your TensorFlow training scripts**. In large-scale models, AMP can lead to dramatic reductions in training time—up to 60%—with minimal code changes (typically under 5 lines). Especially when training on modern GPU architectures, the performance uplift can be essential to reducing costs and iteration times in production-scale machine learning.\n*   The experiments were conducted on AWS EC2 instances using both last-generation and current-generation NVIDIA GPUs.\n    \n*   **Hardware setup**:\n    *   **V100** (Volta architecture) via `p3.2xlarge`\n    *   **T4** (Turing architecture) via `g4dn.xlarge`\n*   **Framework**: Recent TensorFlow builds with CUDA 10.0, orchestrated using the [Spell API](https://spell.ml/docs/run_overview/)\n    \n*   **Models Tested**:\n    \n    1.  **Feedforward Network** A fully connected feedforward network trained on tabular data from the [Rossmann Store Sales](https://www.kaggle.com/c/rossmann-store-sales) Kaggle competition. _Codebase_: [GitHub repository](https://github.com/ResidentMario/spell-feedforward-rossman)\n        \n    2.  **UNet** A medium-sized convolutional model used for image segmentation on the [Segmented Bob Ross Images](https://www.kaggle.com/residentmario/segmented-bob-ross-images) dataset. _Codebase_: [GitHub repository](https://github.com/ResidentMario/spell-unet-bob-ross)\n        \n    3.  **BERT** A large-scale transformer model (`bert-base-uncased`) trained on the [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction) dataset using Hugging Face’s Transformers. _Codebase_: [GitHub repository](https://github.com/ResidentMario/spell-tweet-sentiment-extraction)\n        \n*   **Benchmark Results**:\n\nThe experiments were conducted on AWS EC2 instances using both last-generation and current-generation NVIDIA GPUs.\n\n*   **V100** (Volta architecture) via `p3.2xlarge`\n*   **T4** (Turing architecture) via `g4dn.xlarge`\n\n**Framework**: Recent TensorFlow builds with CUDA 10.0, orchestrated using the [Spell API](https://spell.ml/docs/run_overview/)\n\n**Models Tested**:\n\n1.  **Feedforward Network** A fully connected feedforward network trained on tabular data from the [Rossmann Store Sales](https://www.kaggle.com/c/rossmann-store-sales) Kaggle competition. _Codebase_: [GitHub repository](https://github.com/ResidentMario/spell-feedforward-rossman)\n    \n2.  **UNet** A medium-sized convolutional model used for image segmentation on the [Segmented Bob Ross Images](https://www.kaggle.com/residentmario/segmented-bob-ross-images) dataset. _Codebase_: [GitHub repository](https://github.com/ResidentMario/spell-unet-bob-ross)\n    \n3.  **BERT** A large-scale transformer model (`bert-base-uncased`) trained on the [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction) dataset using Hugging Face’s Transformers. _Codebase_: [GitHub repository](https://github.com/ResidentMario/spell-tweet-sentiment-extraction)\n    \n\n**Feedforward Network** A fully connected feedforward network trained on tabular data from the [Rossmann Store Sales](https://www.kaggle.com/c/rossmann-store-sales) Kaggle competition. _Codebase_: [GitHub repository](https://github.com/ResidentMario/spell-feedforward-rossman)\n\n**UNet** A medium-sized convolutional model used for image segmentation on the [Segmented Bob Ross Images](https://www.kaggle.com/residentmario/segmented-bob-ross-images) dataset. _Codebase_: [GitHub repository](https://github.com/ResidentMario/spell-unet-bob-ross)\n\n**BERT** A large-scale transformer model (`bert-base-uncased`) trained on the [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction) dataset using Hugging Face’s Transformers. _Codebase_: [GitHub repository](https://github.com/ResidentMario/spell-tweet-sentiment-extraction)\n\n![](/primers/ai/assets/model-compression/amp_time.avif)\n\n*   **Observations**:\n    \n    *   **Feedforward Network**: Being a small model with minimal computational complexity, this architecture saw negligible benefit from mixed precision training. The data throughput and model size are simply too limited to leverage Tensor Core acceleration.\n        \n    *   **UNet (Medium-Scale Model)**: With approximately 7.7 million parameters, UNet showed meaningful improvements in training time. The impact of AMP varied by hardware:\n        \n        *   V100: ~5% training time reduction\n        *   T4: ~30% reduction This disparity highlights how more recent GPU architectures (like Turing) extract greater benefit from Tensor Core utilization.\n    *   **BERT (Large-Scale Model)**: Mixed precision provided _transformational_ benefits for BERT:\n        \n        *   Training time reduced by **50–60%** on both GPU types\n        *   No degradation in training loss or final model performance This demonstrates that AMP is especially advantageous for large transformer-based models where computational demand is high.\n\n**Observations**:\n\n*   **Feedforward Network**: Being a small model with minimal computational complexity, this architecture saw negligible benefit from mixed precision training. The data throughput and model size are simply too limited to leverage Tensor Core acceleration.\n    \n*   **UNet (Medium-Scale Model)**: With approximately 7.7 million parameters, UNet showed meaningful improvements in training time. The impact of AMP varied by hardware:\n    \n    *   V100: ~5% training time reduction\n    *   T4: ~30% reduction This disparity highlights how more recent GPU architectures (like Turing) extract greater benefit from Tensor Core utilization.\n*   **BERT (Large-Scale Model)**: Mixed precision provided _transformational_ benefits for BERT:\n    \n    *   Training time reduced by **50–60%** on both GPU types\n    *   No degradation in training loss or final model performance This demonstrates that AMP is especially advantageous for large transformer-based models where computational demand is high.\n\n**Feedforward Network**: Being a small model with minimal computational complexity, this architecture saw negligible benefit from mixed precision training. The data throughput and model size are simply too limited to leverage Tensor Core acceleration.\n\n**UNet (Medium-Scale Model)**: With approximately 7.7 million parameters, UNet showed meaningful improvements in training time. The impact of AMP varied by hardware:\n\n*   V100: ~5% training time reduction\n*   T4: ~30% reduction This disparity highlights how more recent GPU architectures (like Turing) extract greater benefit from Tensor Core utilization.\n\n**BERT (Large-Scale Model)**: Mixed precision provided _transformational_ benefits for BERT:\n\n*   Training time reduced by **50–60%** on both GPU types\n*   No degradation in training loss or final model performance This demonstrates that AMP is especially advantageous for large transformer-based models where computational demand is high.\n\n##### Key Takeaways\n\n*   TensorFlow’s automatic mixed precision (AMP) support offers a robust, efficient, and production-ready pathway to accelerate model training with minimal code changes. By executing safe operations in `float16` while preserving critical numerical precision with `float32` where needed, TensorFlow achieves an optimal balance of performance and stability.\n    \n*   **Ease of Integration**: Mixed precision can be enabled in just a few lines using `mixed_precision.set_global_policy('mixed_float16')` and wrapping the optimizer with `LossScaleOptimizer`. No manual casting or scaling logic is needed for most models.\n    \n*   **Hardware Acceleration**: Significant speedups are realized on NVIDIA GPUs with Tensor Cores (e.g., V100, T4, A100). These architectures are specifically designed to handle mixed-precision workloads efficiently.\n    \n*   **Scalability**: The performance benefits of AMP scale with model size. While small models may see limited gains, medium-to-large models—particularly convolutional networks and transformers—can experience training time reductions of 30–60% or more.\n    \n*   **Numerical Stability**: Automatic loss scaling ensures that mixed precision does not compromise training convergence. Gradient underflows are mitigated adaptively, making AMP safe for most real-world training scenarios.\n    \n\nTensorFlow’s automatic mixed precision (AMP) support offers a robust, efficient, and production-ready pathway to accelerate model training with minimal code changes. By executing safe operations in `float16` while preserving critical numerical precision with `float32` where needed, TensorFlow achieves an optimal balance of performance and stability.\n\n**Ease of Integration**: Mixed precision can be enabled in just a few lines using `mixed_precision.set_global_policy('mixed_float16')` and wrapping the optimizer with `LossScaleOptimizer`. No manual casting or scaling logic is needed for most models.\n\n**Hardware Acceleration**: Significant speedups are realized on NVIDIA GPUs with Tensor Cores (e.g., V100, T4, A100). These architectures are specifically designed to handle mixed-precision workloads efficiently.\n\n**Scalability**: The performance benefits of AMP scale with model size. While small models may see limited gains, medium-to-large models—particularly convolutional networks and transformers—can experience training time reductions of 30–60% or more.\n\n**Numerical Stability**: Automatic loss scaling ensures that mixed precision does not compromise training convergence. Gradient underflows are mitigated adaptively, making AMP safe for most real-world training scenarios.\n\n##### Recommendations\n\n*   **Enable AMP by default** when training on Tensor Core GPUs, especially for models with substantial compute demands.\n*   **Benchmark performance** for your specific model and dataset, as the impact of mixed precision can vary depending on architecture, data pipeline, and hardware.\n*   **Use dynamic loss scaling** (enabled by default with `LossScaleOptimizer`) to maintain stability without the need for manual tuning.",
      "order": 41,
      "orderInChapter": 2,
      "difficulty": 5,
      "estimatedMinutes": 52,
      "tags": [
        "ondevice ai",
        "neural network",
        "deep learning",
        "machine learning",
        "transformer",
        "attention",
        "convolution",
        "cnn"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": true,
        "wordCount": 10291,
        "contentLength": 160134
      },
      "nextCards": [
        "ai-model-compression-overview-42",
        "ai-model-compression-formal-definition-43"
      ],
      "relatedCards": [
        "ai-on-device-transformers-cpu-deployment-considerations-17",
        "ai-transformers-transformer-core-5",
        "ai-nlp-tasks-methodologies-and-models-36",
        "ai-federated-learning-privacy-21",
        "ai-interview-in-the-past-cnns-were-used-for-translation-explain-103"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#how-mixed-precision-training-works",
      "scrapedAt": "2025-12-28T11:55:50.973Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-overview-42",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Low-Rank Decomposition & Adaptation",
      "title": "Overview",
      "subtitle": "Low-Rank Decomposition & Adaptation",
      "contentHtml": "<ul>\n  <li>\n    <p>Large language models (LLMs) and deep neural networks often rely on massive weight matrices that are expensive to store and compute. However, in many cases, these matrices exhibit redundancy—meaning they can be approximated by lower-rank structures without significantly compromising the model’s predictive performance. Low-rank decomposition exploits this insight by factorizing large matrices into the product of two smaller matrices.</p>\n  </li>\n  <li>\n    <p>The motivation is twofold:</p>\n\n    <ol>\n      <li><strong>Efficiency</strong>: Reducing the number of parameters and operations accelerates training and inference.</li>\n      <li><strong>Compression</strong>: Enables model deployment on memory-constrained devices by lowering storage requirements.</li>\n    </ol>\n  </li>\n  <li>\n    <p>This technique is especially attractive when used with techniques like quantization, pruning, or transfer learning, as it complements them without requiring extensive retraining.</p>\n  </li>\n</ul>\n<p>Large language models (LLMs) and deep neural networks often rely on massive weight matrices that are expensive to store and compute. However, in many cases, these matrices exhibit redundancy—meaning they can be approximated by lower-rank structures without significantly compromising the model’s predictive performance. Low-rank decomposition exploits this insight by factorizing large matrices into the product of two smaller matrices.</p>\n<p>The motivation is twofold:</p>\n<ol>\n      <li><strong>Efficiency</strong>: Reducing the number of parameters and operations accelerates training and inference.</li>\n      <li><strong>Compression</strong>: Enables model deployment on memory-constrained devices by lowering storage requirements.</li>\n    </ol>\n<p>This technique is especially attractive when used with techniques like quantization, pruning, or transfer learning, as it complements them without requiring extensive retraining.</p>",
      "contentMarkdown": "*   Large language models (LLMs) and deep neural networks often rely on massive weight matrices that are expensive to store and compute. However, in many cases, these matrices exhibit redundancy—meaning they can be approximated by lower-rank structures without significantly compromising the model’s predictive performance. Low-rank decomposition exploits this insight by factorizing large matrices into the product of two smaller matrices.\n    \n*   The motivation is twofold:\n    \n    1.  **Efficiency**: Reducing the number of parameters and operations accelerates training and inference.\n    2.  **Compression**: Enables model deployment on memory-constrained devices by lowering storage requirements.\n*   This technique is especially attractive when used with techniques like quantization, pruning, or transfer learning, as it complements them without requiring extensive retraining.\n    \n\nLarge language models (LLMs) and deep neural networks often rely on massive weight matrices that are expensive to store and compute. However, in many cases, these matrices exhibit redundancy—meaning they can be approximated by lower-rank structures without significantly compromising the model’s predictive performance. Low-rank decomposition exploits this insight by factorizing large matrices into the product of two smaller matrices.\n\nThe motivation is twofold:\n\n1.  **Efficiency**: Reducing the number of parameters and operations accelerates training and inference.\n2.  **Compression**: Enables model deployment on memory-constrained devices by lowering storage requirements.\n\nThis technique is especially attractive when used with techniques like quantization, pruning, or transfer learning, as it complements them without requiring extensive retraining.",
      "order": 42,
      "orderInChapter": 1,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "neural network",
        "llm",
        "transfer learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 225,
        "contentLength": 1956
      },
      "nextCards": [
        "ai-model-compression-formal-definition-43",
        "ai-model-compression-concept-44"
      ],
      "relatedCards": [
        "ai-federated-learning-personalization-22",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-cons-16",
        "ai-on-device-transformers-neural-processing-unit-npu-6",
        "ai-federated-learning-what-is-federated-lora-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#overview",
      "scrapedAt": "2025-12-28T11:55:50.974Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-formal-definition-43",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Low-Rank Decomposition & Adaptation",
      "title": "Formal Definition",
      "subtitle": "Low-Rank Decomposition & Adaptation",
      "contentHtml": "<ul>\n  <li>\n    <p>The core idea: replace a large dense weight matrix</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-132-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>W</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1585\" style=\"width: 5.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.22em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1586\"><span class=\"mi\" id=\"MathJax-Span-1587\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1588\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1589\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1590\"><span class=\"mrow\" id=\"MathJax-Span-1591\"><span class=\"mi\" id=\"MathJax-Span-1592\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1593\"><span class=\"mrow\" id=\"MathJax-Span-1594\"><span class=\"mi\" id=\"MathJax-Span-1595\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1596\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1597\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>W</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-132\">W \\in \\mathbb{R}^{d \\times d}</script>\n\n    <ul>\n      <li>with two smaller matrices:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-133-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>A</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></mrow></msup><mo>,</mo><mspace width=&quot;1em&quot; /><mi>B</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1598\" style=\"width: 11.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.273em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1009.27em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1599\"><span class=\"mi\" id=\"MathJax-Span-1600\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-1601\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1602\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1603\"><span class=\"mrow\" id=\"MathJax-Span-1604\"><span class=\"mi\" id=\"MathJax-Span-1605\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1606\"><span class=\"mrow\" id=\"MathJax-Span-1607\"><span class=\"mi\" id=\"MathJax-Span-1608\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1609\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1610\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1611\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1612\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-1613\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">B</span><span class=\"mo\" id=\"MathJax-Span-1614\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1615\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1616\"><span class=\"mrow\" id=\"MathJax-Span-1617\"><span class=\"mi\" id=\"MathJax-Span-1618\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1619\"><span class=\"mrow\" id=\"MathJax-Span-1620\"><span class=\"mi\" id=\"MathJax-Span-1621\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1622\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1623\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>A</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup><mo>,</mo><mspace width=\"1em\"></mspace><mi>B</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-133\">A \\in \\mathbb{R}^{d \\times r}, \\quad B \\in \\mathbb{R}^{r \\times d}</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-134-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>&amp;#x226A;</mo><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1624\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.24em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1625\"><span class=\"mi\" id=\"MathJax-Span-1626\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1627\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≪</span><span class=\"mi\" id=\"MathJax-Span-1628\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>≪</mo><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-134\">r \\ll d</script>. The original weight matrix is then approximated as:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-135-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>W</mi><mo>&amp;#x2248;</mo><mi>A</mi><mi>B</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1629\" style=\"width: 4.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.596em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.6em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1630\"><span class=\"mi\" id=\"MathJax-Span-1631\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1632\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mi\" id=\"MathJax-Span-1633\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">A</span><span class=\"mi\" id=\"MathJax-Span-1634\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>W</mi><mo>≈</mo><mi>A</mi><mi>B</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-135\">W \\approx A B</script>\n  </li>\n  <li>\n    <p>This reduces the parameter count from <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-136-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1635\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1636\"><span class=\"mi\" id=\"MathJax-Span-1637\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-1638\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1639\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1640\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1641\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1642\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-136\">O(d^2)</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-137-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>r</mi><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1643\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.24em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1644\"><span class=\"mi\" id=\"MathJax-Span-1645\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-1646\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1647\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1648\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1649\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>r</mi><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-137\">O(rd)</script>, which is a significant gain when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-138-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>&amp;#x226A;</mo><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1650\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.24em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1651\"><span class=\"mi\" id=\"MathJax-Span-1652\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1653\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≪</span><span class=\"mi\" id=\"MathJax-Span-1654\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>≪</mo><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-138\">r \\ll d</script>. During inference or training, instead of computing:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-139-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>y</mi><mo>=</mo><mi>W</mi><mi>x</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1655\" style=\"width: 3.857em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.18em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.18em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1656\"><span class=\"mi\" id=\"MathJax-Span-1657\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"mo\" id=\"MathJax-Span-1658\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1659\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1660\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>y</mi><mo>=</mo><mi>W</mi><mi>x</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-139\">y = Wx</script>\n\n    <ul>\n      <li>we compute:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-140-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>y</mi><mo>=</mo><mi>A</mi><mo stretchy=&quot;false&quot;>(</mo><mi>B</mi><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1661\" style=\"width: 5.159em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.273em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.22em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1662\"><span class=\"mi\" id=\"MathJax-Span-1663\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"mo\" id=\"MathJax-Span-1664\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1665\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">A</span><span class=\"mo\" id=\"MathJax-Span-1666\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1667\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mi\" id=\"MathJax-Span-1668\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1669\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>y</mi><mo>=</mo><mi>A</mi><mo stretchy=\"false\">(</mo><mi>B</mi><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-140\">y = A (B x)</script>\n  </li>\n  <li>\n    <p>This allows models to retain much of their representational power while becoming faster and more compact.</p>\n  </li>\n</ul>\n<p>The core idea: replace a large dense weight matrix</p>\n<ul>\n      <li>with two smaller matrices:</li>\n    </ul>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-134-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>&amp;#x226A;</mo><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1624\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.24em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1625\"><span class=\"mi\" id=\"MathJax-Span-1626\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1627\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≪</span><span class=\"mi\" id=\"MathJax-Span-1628\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>≪</mo><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-134\">r \\ll d</script>. The original weight matrix is then approximated as:</li>\n    </ul>\n<p>This reduces the parameter count from <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-136-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1635\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1636\"><span class=\"mi\" id=\"MathJax-Span-1637\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-1638\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1639\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1640\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1641\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1642\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-136\">O(d^2)</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-137-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>r</mi><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1643\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.24em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1644\"><span class=\"mi\" id=\"MathJax-Span-1645\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-1646\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1647\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1648\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1649\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>r</mi><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-137\">O(rd)</script>, which is a significant gain when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-138-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi><mo>&amp;#x226A;</mo><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1650\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.24em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1651\"><span class=\"mi\" id=\"MathJax-Span-1652\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1653\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≪</span><span class=\"mi\" id=\"MathJax-Span-1654\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi><mo>≪</mo><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-138\">r \\ll d</script>. During inference or training, instead of computing:</p>\n<ul>\n      <li>we compute:</li>\n    </ul>\n<p>This allows models to retain much of their representational power while becoming faster and more compact.</p>",
      "contentMarkdown": "*   The core idea: replace a large dense weight matrix\n    \n    W∈ℝd×dW∈Rd×d\n    \n    W \\\\in \\\\mathbb{R}^{d \\\\times d}\n    \n    *   with two smaller matrices:\n    \n    A∈ℝd×r,B∈ℝr×dA∈Rd×r,B∈Rr×d\n    \n    A \\\\in \\\\mathbb{R}^{d \\\\times r}, \\\\quad B \\\\in \\\\mathbb{R}^{r \\\\times d}\n    \n    *   where r≪dr≪dr \\\\ll d. The original weight matrix is then approximated as:\n    \n    W≈ABW≈AB\n    \n    W \\\\approx A B\n*   This reduces the parameter count from O(d2)O(d2)O(d^2) to O(rd)O(rd)O(rd), which is a significant gain when r≪dr≪dr \\\\ll d. During inference or training, instead of computing:\n    \n    y\\=Wxy\\=Wx\n    \n    y = Wx\n    \n    *   we compute:\n    \n    y\\=A(Bx)y\\=A(Bx)\n    \n    y = A (B x)\n*   This allows models to retain much of their representational power while becoming faster and more compact.\n    \n\nThe core idea: replace a large dense weight matrix\n\n*   with two smaller matrices:\n\n*   where r≪dr≪dr \\\\ll d. The original weight matrix is then approximated as:\n\nThis reduces the parameter count from O(d2)O(d2)O(d^2) to O(rd)O(rd)O(rd), which is a significant gain when r≪dr≪dr \\\\ll d. During inference or training, instead of computing:\n\n*   we compute:\n\nThis allows models to retain much of their representational power while becoming faster and more compact.",
      "order": 43,
      "orderInChapter": 2,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 178,
        "contentLength": 30876
      },
      "nextCards": [
        "ai-model-compression-concept-44",
        "ai-model-compression-low-rank-correction-for-quantization-45"
      ],
      "relatedCards": [
        "ai-federated-learning-lora-in-federated-context-17",
        "ai-on-device-transformers-npu-deployment-considerations-edgesoc-devices-20",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-mime-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#formal-definition",
      "scrapedAt": "2025-12-28T11:55:50.974Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-concept-44",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Low-Rank Decomposition & Adaptation",
      "title": "Concept",
      "subtitle": "Low-Rank Decomposition & Adaptation",
      "contentHtml": "<ul>\n  <li>\n    <p>Freeze original pretrained weights; add trainable adapters <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-141-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>,</mo><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1670\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.88em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1671\"><span class=\"mi\" id=\"MathJax-Span-1672\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-1673\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1674\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>,</mo><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-141\">A, B</script> for each dense layer such that:</p>\n  </li>\n  <li>Efficiency: Only train few parameters (e.g. <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-142-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>2</mn><mi>d</mi><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1675\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1676\"><span class=\"mn\" id=\"MathJax-Span-1677\" style=\"font-family: STIXGeneral-Regular;\">2</span><span class=\"mi\" id=\"MathJax-Span-1678\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1679\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>2</mn><mi>d</mi><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-142\">2dr</script> per layer). For large LLMs, can reduce fine-tuning parameters by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-143-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mn>3</mn></msup><mo>&amp;#x2013;</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>4</mn><mo>&amp;#xD7;</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1680\" style=\"width: 4.846em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.013em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1004.01em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1681\"><span class=\"msubsup\" id=\"MathJax-Span-1682\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1683\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"mn\" id=\"MathJax-Span-1684\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1685\" style=\"font-family: STIXGeneral-Regular;\">–</span><span class=\"msubsup\" id=\"MathJax-Span-1686\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-1687\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-1688\"><span class=\"mrow\" id=\"MathJax-Span-1689\"><span class=\"mn\" id=\"MathJax-Span-1690\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4</span><span class=\"mo\" id=\"MathJax-Span-1691\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mn>3</mn></msup><mo>–</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mn>4</mn><mo>×</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-143\">10^3 – 10^{4×}</script> while achieving competitive results.</li>\n  <li>Popular in federated settings (see above).</li>\n</ul>\n<p>Freeze original pretrained weights; add trainable adapters <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-141-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>,</mo><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1670\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.88em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1671\"><span class=\"mi\" id=\"MathJax-Span-1672\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-1673\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1674\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>,</mo><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-141\">A, B</script> for each dense layer such that:</p>",
      "contentMarkdown": "*   Freeze original pretrained weights; add trainable adapters A,BA,BA, B for each dense layer such that:\n    \n*   Efficiency: Only train few parameters (e.g. 2dr2dr2dr per layer). For large LLMs, can reduce fine-tuning parameters by 103–104×103–104×10^3 – 10^{4×} while achieving competitive results.\n*   Popular in federated settings (see above).\n\nFreeze original pretrained weights; add trainable adapters A,BA,BA, B for each dense layer such that:",
      "order": 44,
      "orderInChapter": 3,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "llm",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 63,
        "contentLength": 7943
      },
      "nextCards": [
        "ai-model-compression-low-rank-correction-for-quantization-45",
        "ai-model-compression-quantized-low-rank-adaptation-techniques-46"
      ],
      "relatedCards": [
        "ai-differential-privacy-fine-tuning-with-dp-sgd-12",
        "ai-federated-learning-llm-specific-enhancements-15",
        "ai-federated-learning-pros-cons-19",
        "ai-differential-privacy-cons-16",
        "ai-federated-learning-what-is-federated-lora-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#concept",
      "scrapedAt": "2025-12-28T11:55:50.974Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-low-rank-correction-for-quantization-45",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Low-Rank Decomposition & Adaptation",
      "title": "Low-Rank Correction for Quantization",
      "subtitle": "Low-Rank Decomposition & Adaptation",
      "contentHtml": "<ul>\n  <li>\n    <p>Counteract quantization-induced error in activation domains. We approximate the full-precision weight matrix:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-144-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>W</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1692\" style=\"width: 5.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.22em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1693\"><span class=\"mi\" id=\"MathJax-Span-1694\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1695\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1696\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1697\"><span class=\"mrow\" id=\"MathJax-Span-1698\"><span class=\"mi\" id=\"MathJax-Span-1699\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1700\"><span class=\"mrow\" id=\"MathJax-Span-1701\"><span class=\"mi\" id=\"MathJax-Span-1702\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1703\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1704\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>W</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-144\">W \\in \\mathbb{R}^{d \\times d}</script>\n\n    <ul>\n      <li>as the product of two low-rank matrices:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-145-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>W</mi><mo>&amp;#x2248;</mo><mi>A</mi><mi>B</mi><mo>,</mo><mspace width=&quot;1em&quot; /><mtext>where</mtext><mspace width=&quot;1em&quot; /><mi>A</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></mrow></msup><mo>,</mo><mspace width=&quot;1em&quot; /><mi>B</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup><mo>,</mo><mspace width=&quot;1em&quot; /><mtext>and&amp;#xA0;</mtext><mi>r</mi><mo>&amp;#x226A;</mo><mi>d</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1705\" style=\"width: 28.336em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 23.596em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1023.6em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1706\"><span class=\"mi\" id=\"MathJax-Span-1707\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1708\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mi\" id=\"MathJax-Span-1709\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">A</span><span class=\"mi\" id=\"MathJax-Span-1710\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mo\" id=\"MathJax-Span-1711\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1712\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mtext\" id=\"MathJax-Span-1713\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">where</span><span class=\"mspace\" id=\"MathJax-Span-1714\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-1715\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-1716\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1717\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1718\"><span class=\"mrow\" id=\"MathJax-Span-1719\"><span class=\"mi\" id=\"MathJax-Span-1720\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1721\"><span class=\"mrow\" id=\"MathJax-Span-1722\"><span class=\"mi\" id=\"MathJax-Span-1723\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1724\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1725\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1726\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1727\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-1728\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">B</span><span class=\"mo\" id=\"MathJax-Span-1729\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1730\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1731\"><span class=\"mrow\" id=\"MathJax-Span-1732\"><span class=\"mi\" id=\"MathJax-Span-1733\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1734\"><span class=\"mrow\" id=\"MathJax-Span-1735\"><span class=\"mi\" id=\"MathJax-Span-1736\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1737\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1738\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1739\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1740\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mtext\" id=\"MathJax-Span-1741\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">and&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-1742\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1743\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≪</span><span class=\"mi\" id=\"MathJax-Span-1744\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>W</mi><mo>≈</mo><mi>A</mi><mi>B</mi><mo>,</mo><mspace width=\"1em\"></mspace><mtext>where</mtext><mspace width=\"1em\"></mspace><mi>A</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup><mo>,</mo><mspace width=\"1em\"></mspace><mi>B</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mo>×</mo><mi>d</mi></mrow></msup><mo>,</mo><mspace width=\"1em\"></mspace><mtext>and&nbsp;</mtext><mi>r</mi><mo>≪</mo><mi>d</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-145\">W \\approx A B, \\quad \\text{where} \\quad A \\in \\mathbb{R}^{d \\times r}, \\quad B \\in \\mathbb{R}^{r \\times d}, \\quad \\text{and } r \\ll d</script>\n\n    <ul>\n      <li>\n        <p>This reduces the parameter count from <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-146-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1745\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1746\"><span class=\"mi\" id=\"MathJax-Span-1747\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-1748\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1749\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1750\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1751\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1752\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-146\">O(d^2)</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-147-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>r</mi><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1753\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.24em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1754\"><span class=\"mi\" id=\"MathJax-Span-1755\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-1756\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1757\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1758\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1759\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>r</mi><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-147\">O(rd)</script>, significantly lowering storage and compute cost.</p>\n      </li>\n      <li>\n        <p>In quantization-aware settings, this is often extended to:</p>\n      </li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-148-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>W</mi><mo>&amp;#x2248;</mo><mi>Q</mi><mo>+</mo><mi>A</mi><mi>B</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1760\" style=\"width: 6.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.37em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1761\"><span class=\"mi\" id=\"MathJax-Span-1762\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1763\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mi\" id=\"MathJax-Span-1764\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">Q</span><span class=\"mo\" id=\"MathJax-Span-1765\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-1766\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">A</span><span class=\"mi\" id=\"MathJax-Span-1767\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>W</mi><mo>≈</mo><mi>Q</mi><mo>+</mo><mi>A</mi><mi>B</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-148\">W \\approx Q + A B</script>\n\n    <ul>\n      <li>\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-149-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1768\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1769\"><span class=\"mi\" id=\"MathJax-Span-1770\" style=\"font-family: STIXGeneral-Italic;\">Q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-149\">Q</script> is the quantized version of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-150-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1771\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1772\"><span class=\"mi\" id=\"MathJax-Span-1773\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-150\">W</script>, e.g., <code class=\"language-plaintext highlighter-rouge\">int4</code> or <code class=\"language-plaintext highlighter-rouge\">int8</code></li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-151-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1774\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1775\"><span class=\"mi\" id=\"MathJax-Span-1776\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mi\" id=\"MathJax-Span-1777\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-151\">A B</script> is a low-rank full-precision correction term on unquantized activations</li>\n          <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-152-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>A</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></mrow></msup><mo>,</mo><mi>B</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1778\" style=\"width: 9.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.128em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1008.13em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1779\"><span class=\"mi\" id=\"MathJax-Span-1780\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-1781\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1782\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1783\"><span class=\"mrow\" id=\"MathJax-Span-1784\"><span class=\"mi\" id=\"MathJax-Span-1785\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1786\"><span class=\"mrow\" id=\"MathJax-Span-1787\"><span class=\"mi\" id=\"MathJax-Span-1788\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1789\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1790\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1791\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1792\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">B</span><span class=\"mo\" id=\"MathJax-Span-1793\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1794\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1795\"><span class=\"mrow\" id=\"MathJax-Span-1796\"><span class=\"mi\" id=\"MathJax-Span-1797\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1798\"><span class=\"mrow\" id=\"MathJax-Span-1799\"><span class=\"mi\" id=\"MathJax-Span-1800\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1801\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1802\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>A</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup><mo>,</mo><mi>B</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-152\">A \\in \\mathbb{R}^{d \\times r}, B \\in \\mathbb{R}^{r \\times d}</script>\n          </li>\n        </ul>\n      </li>\n      <li>\n        <p>This hybrid scheme (quantized base + low-rank residual) allows retaining much of the accuracy of full-precision models while gaining the memory and speed benefits of quantization.</p>\n      </li>\n    </ul>\n  </li>\n  <li>Solve via joint optimization: alternating minimization to fit both quantized and low-rank components to minimize output reconstruction error. With ranks at 10% of weight size, activation error gaps can be halved; with 30% rank, closed completely.</li>\n  <li>Fits well with post-training quantization pipelines. Works across calibration sets without full retraining.</li>\n</ul>\n<p>Counteract quantization-induced error in activation domains. We approximate the full-precision weight matrix:</p>\n<ul>\n      <li>as the product of two low-rank matrices:</li>\n    </ul>\n<ul>\n      <li>\n        <p>This reduces the parameter count from <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-146-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1745\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1746\"><span class=\"mi\" id=\"MathJax-Span-1747\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-1748\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1749\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1750\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1751\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1752\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-146\">O(d^2)</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-147-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>r</mi><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1753\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.24em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1754\"><span class=\"mi\" id=\"MathJax-Span-1755\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-1756\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1757\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1758\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1759\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>r</mi><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-147\">O(rd)</script>, significantly lowering storage and compute cost.</p>\n      </li>\n      <li>\n        <p>In quantization-aware settings, this is often extended to:</p>\n      </li>\n    </ul>\n<p>This reduces the parameter count from <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-146-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1745\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1746\"><span class=\"mi\" id=\"MathJax-Span-1747\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-1748\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1749\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1750\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1751\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1752\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-146\">O(d^2)</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-147-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>r</mi><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1753\" style=\"width: 2.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.294em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.24em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1754\"><span class=\"mi\" id=\"MathJax-Span-1755\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-1756\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1757\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1758\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1759\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>r</mi><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-147\">O(rd)</script>, significantly lowering storage and compute cost.</p>\n<p>In quantization-aware settings, this is often extended to:</p>\n<ul>\n      <li>\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-149-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1768\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1769\"><span class=\"mi\" id=\"MathJax-Span-1770\" style=\"font-family: STIXGeneral-Italic;\">Q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-149\">Q</script> is the quantized version of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-150-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1771\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1772\"><span class=\"mi\" id=\"MathJax-Span-1773\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-150\">W</script>, e.g., <code class=\"language-plaintext highlighter-rouge\">int4</code> or <code class=\"language-plaintext highlighter-rouge\">int8</code></li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-151-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1774\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1775\"><span class=\"mi\" id=\"MathJax-Span-1776\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mi\" id=\"MathJax-Span-1777\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-151\">A B</script> is a low-rank full-precision correction term on unquantized activations</li>\n          <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-152-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>A</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></mrow></msup><mo>,</mo><mi>B</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1778\" style=\"width: 9.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.128em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1008.13em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1779\"><span class=\"mi\" id=\"MathJax-Span-1780\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-1781\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1782\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1783\"><span class=\"mrow\" id=\"MathJax-Span-1784\"><span class=\"mi\" id=\"MathJax-Span-1785\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1786\"><span class=\"mrow\" id=\"MathJax-Span-1787\"><span class=\"mi\" id=\"MathJax-Span-1788\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1789\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1790\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1791\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1792\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">B</span><span class=\"mo\" id=\"MathJax-Span-1793\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1794\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1795\"><span class=\"mrow\" id=\"MathJax-Span-1796\"><span class=\"mi\" id=\"MathJax-Span-1797\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1798\"><span class=\"mrow\" id=\"MathJax-Span-1799\"><span class=\"mi\" id=\"MathJax-Span-1800\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1801\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1802\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>A</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup><mo>,</mo><mi>B</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-152\">A \\in \\mathbb{R}^{d \\times r}, B \\in \\mathbb{R}^{r \\times d}</script>\n          </li>\n        </ul>\n      </li>\n      <li>\n        <p>This hybrid scheme (quantized base + low-rank residual) allows retaining much of the accuracy of full-precision models while gaining the memory and speed benefits of quantization.</p>\n      </li>\n    </ul>\n<p>where:</p>\n<ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-149-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1768\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1769\"><span class=\"mi\" id=\"MathJax-Span-1770\" style=\"font-family: STIXGeneral-Italic;\">Q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-149\">Q</script> is the quantized version of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-150-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1771\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1772\"><span class=\"mi\" id=\"MathJax-Span-1773\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-150\">W</script>, e.g., <code class=\"language-plaintext highlighter-rouge\">int4</code> or <code class=\"language-plaintext highlighter-rouge\">int8</code></li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-151-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1774\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1775\"><span class=\"mi\" id=\"MathJax-Span-1776\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mi\" id=\"MathJax-Span-1777\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-151\">A B</script> is a low-rank full-precision correction term on unquantized activations</li>\n          <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-152-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>A</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></mrow></msup><mo>,</mo><mi>B</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1778\" style=\"width: 9.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.128em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1008.13em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1779\"><span class=\"mi\" id=\"MathJax-Span-1780\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-1781\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1782\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1783\"><span class=\"mrow\" id=\"MathJax-Span-1784\"><span class=\"mi\" id=\"MathJax-Span-1785\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1786\"><span class=\"mrow\" id=\"MathJax-Span-1787\"><span class=\"mi\" id=\"MathJax-Span-1788\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1789\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1790\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1791\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1792\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">B</span><span class=\"mo\" id=\"MathJax-Span-1793\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1794\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1795\"><span class=\"mrow\" id=\"MathJax-Span-1796\"><span class=\"mi\" id=\"MathJax-Span-1797\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1798\"><span class=\"mrow\" id=\"MathJax-Span-1799\"><span class=\"mi\" id=\"MathJax-Span-1800\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1801\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1802\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>A</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup><mo>,</mo><mi>B</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-152\">A \\in \\mathbb{R}^{d \\times r}, B \\in \\mathbb{R}^{r \\times d}</script>\n          </li>\n        </ul>\n<p>This hybrid scheme (quantized base + low-rank residual) allows retaining much of the accuracy of full-precision models while gaining the memory and speed benefits of quantization.</p>",
      "contentMarkdown": "*   Counteract quantization-induced error in activation domains. We approximate the full-precision weight matrix:\n    \n    W∈ℝd×dW∈Rd×d\n    \n    W \\\\in \\\\mathbb{R}^{d \\\\times d}\n    \n    *   as the product of two low-rank matrices:\n    \n    W≈AB,whereA∈ℝd×r,B∈ℝr×d,and r≪dW≈AB,whereA∈Rd×r,B∈Rr×d,and r≪d\n    \n    W \\\\approx A B, \\\\quad \\\\text{where} \\\\quad A \\\\in \\\\mathbb{R}^{d \\\\times r}, \\\\quad B \\\\in \\\\mathbb{R}^{r \\\\times d}, \\\\quad \\\\text{and } r \\\\ll d\n    \n    *   This reduces the parameter count from O(d2)O(d2)O(d^2) to O(rd)O(rd)O(rd), significantly lowering storage and compute cost.\n        \n    *   In quantization-aware settings, this is often extended to:\n        \n    \n    W≈Q+ABW≈Q+AB\n    \n    W \\\\approx Q + A B\n    *   where:\n        \n        *   QQQ is the quantized version of WWW, e.g., `int4` or `int8`\n        *   ABABA B is a low-rank full-precision correction term on unquantized activations\n        *   A∈ℝd×r,B∈ℝr×dA∈Rd×r,B∈Rr×d\n            \n            A \\\\in \\\\mathbb{R}^{d \\\\times r}, B \\\\in \\\\mathbb{R}^{r \\\\times d}\n    *   This hybrid scheme (quantized base + low-rank residual) allows retaining much of the accuracy of full-precision models while gaining the memory and speed benefits of quantization.\n        \n*   Solve via joint optimization: alternating minimization to fit both quantized and low-rank components to minimize output reconstruction error. With ranks at 10% of weight size, activation error gaps can be halved; with 30% rank, closed completely.\n*   Fits well with post-training quantization pipelines. Works across calibration sets without full retraining.\n\nCounteract quantization-induced error in activation domains. We approximate the full-precision weight matrix:\n\n*   as the product of two low-rank matrices:\n\n*   This reduces the parameter count from O(d2)O(d2)O(d^2) to O(rd)O(rd)O(rd), significantly lowering storage and compute cost.\n    \n*   In quantization-aware settings, this is often extended to:\n    \n\nThis reduces the parameter count from O(d2)O(d2)O(d^2) to O(rd)O(rd)O(rd), significantly lowering storage and compute cost.\n\nIn quantization-aware settings, this is often extended to:\n\n*   where:\n    \n    *   QQQ is the quantized version of WWW, e.g., `int4` or `int8`\n    *   ABABA B is a low-rank full-precision correction term on unquantized activations\n    *   A∈ℝd×r,B∈ℝr×dA∈Rd×r,B∈Rr×d\n        \n        A \\\\in \\\\mathbb{R}^{d \\\\times r}, B \\\\in \\\\mathbb{R}^{r \\\\times d}\n*   This hybrid scheme (quantized base + low-rank residual) allows retaining much of the accuracy of full-precision models while gaining the memory and speed benefits of quantization.\n    \n\nwhere:\n\n*   QQQ is the quantized version of WWW, e.g., `int4` or `int8`\n*   ABABA B is a low-rank full-precision correction term on unquantized activations\n*   A∈ℝd×r,B∈ℝr×dA∈Rd×r,B∈Rr×d\n    \n    A \\\\in \\\\mathbb{R}^{d \\\\times r}, B \\\\in \\\\mathbb{R}^{r \\\\times d}\n\nThis hybrid scheme (quantized base + low-rank residual) allows retaining much of the accuracy of full-precision models while gaining the memory and speed benefits of quantization.",
      "order": 45,
      "orderInChapter": 4,
      "difficulty": 5,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "optimization",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 398,
        "contentLength": 56190
      },
      "nextCards": [
        "ai-model-compression-quantized-low-rank-adaptation-techniques-46",
        "ai-model-compression-pros-cons-47"
      ],
      "relatedCards": [
        "ai-federated-learning-scaffold-8",
        "ai-on-device-transformers-hardware-specific-optimization-notes-16",
        "ai-federated-learning-fedprox-7",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5",
        "ai-federated-learning-feddyn-10"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#low-rank-correction-for-quantization",
      "scrapedAt": "2025-12-28T11:55:50.974Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-quantized-low-rank-adaptation-techniques-46",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Low-Rank Decomposition & Adaptation",
      "title": "Quantized Low-Rank Adaptation Techniques",
      "subtitle": "Low-Rank Decomposition & Adaptation",
      "contentHtml": "<ul>\n  <li>Low-Rank Adaptation (LoRA) techniques allow efficient fine-tuning of large-scale pre-trained models by updating only a small number of additional low-rank matrices while freezing the original weights. Several recent advancements extend this idea by incorporating quantization, yielding memory-efficient and compute-friendly training pipelines. Below, we explore three major variants: <strong>LQ-LoRA</strong>, <strong>QLoRA</strong>, and <strong>QA-LoRA</strong>.</li>\n</ul>\n<h4 id=\"lqlora-quantized--low-rank-adaptation\">LQ‑LoRA: Quantized + Low-Rank Adaptation</h4>\n<ul>\n  <li><strong>LQ-LoRA</strong> was introduced in <a href=\"https://arxiv.org/abs/2309.07063\">Guo et al., 2023</a> as a memory-efficient fine-tuning approach that combines low-bit quantization with learnable low-rank adapters.</li>\n</ul>\n<h5 id=\"overview-3\">Overview</h5>\n<ul>\n  <li>Each full-precision weight matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-153-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1803\" style=\"width: 5.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.221em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.22em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1804\"><span class=\"mi\" id=\"MathJax-Span-1805\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1806\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1807\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1808\"><span class=\"mrow\" id=\"MathJax-Span-1809\"><span class=\"mi\" id=\"MathJax-Span-1810\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1811\"><span class=\"mrow\" id=\"MathJax-Span-1812\"><span class=\"mi\" id=\"MathJax-Span-1813\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1814\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1815\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>W</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-153\">W \\in \\mathbb{R}^{d \\times d}</script> is decomposed into:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-154-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>W</mi><mo>&amp;#x2248;</mo><mi>Q</mi><mo>+</mo><mi>A</mi><mi>B</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1816\" style=\"width: 6.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.37em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1817\"><span class=\"mi\" id=\"MathJax-Span-1818\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1819\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mi\" id=\"MathJax-Span-1820\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">Q</span><span class=\"mo\" id=\"MathJax-Span-1821\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-1822\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">A</span><span class=\"mi\" id=\"MathJax-Span-1823\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>W</mi><mo>≈</mo><mi>Q</mi><mo>+</mo><mi>A</mi><mi>B</mi></math></span></span></div>\n<ul>\n  <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-155-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1824\" style=\"width: 4.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.961em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.96em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1825\"><span class=\"mi\" id=\"MathJax-Span-1826\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-1827\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1828\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1829\"><span class=\"mrow\" id=\"MathJax-Span-1830\"><span class=\"mi\" id=\"MathJax-Span-1831\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1832\"><span class=\"mrow\" id=\"MathJax-Span-1833\"><span class=\"mi\" id=\"MathJax-Span-1834\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1835\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1836\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-155\">Q \\in \\mathbb{R}^{d \\times d}</script>: a low-bit quantized matrix (e.g., 2.75 bits), kept <strong>frozen</strong></li>\n  <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-156-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1837\" style=\"width: 4.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.86em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1838\"><span class=\"mi\" id=\"MathJax-Span-1839\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-1840\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1841\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1842\"><span class=\"mrow\" id=\"MathJax-Span-1843\"><span class=\"mi\" id=\"MathJax-Span-1844\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1845\"><span class=\"mrow\" id=\"MathJax-Span-1846\"><span class=\"mi\" id=\"MathJax-Span-1847\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1848\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1849\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-156\">A \\in \\mathbb{R}^{d \\times r}</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-157-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1850\" style=\"width: 4.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.8em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1851\"><span class=\"mi\" id=\"MathJax-Span-1852\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mo\" id=\"MathJax-Span-1853\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1854\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1855\"><span class=\"mrow\" id=\"MathJax-Span-1856\"><span class=\"mi\" id=\"MathJax-Span-1857\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1858\"><span class=\"mrow\" id=\"MathJax-Span-1859\"><span class=\"mi\" id=\"MathJax-Span-1860\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1861\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1862\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-157\">B \\in \\mathbb{R}^{r \\times d}</script>: low-rank full-precision matrices, <strong>learnable</strong></li>\n</ul>\n<h5 id=\"key-properties\">Key Properties</h5>\n<ul>\n  <li>The quantized base matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-158-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1863\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1864\"><span class=\"mi\" id=\"MathJax-Span-1865\" style=\"font-family: STIXGeneral-Italic;\">Q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-158\">Q</script> captures the main representational power of the original model, without requiring updates during fine-tuning.</li>\n  <li>The low-rank matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-159-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1866\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1867\"><span class=\"mi\" id=\"MathJax-Span-1868\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-159\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-160-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1869\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1870\"><span class=\"mi\" id=\"MathJax-Span-1871\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-160\">B</script> adapt the model to the downstream task.</li>\n  <li>Enables <strong>sub-3-bit quantization</strong> without major degradation in task performance.</li>\n  <li>Requires only <strong>~27 GB</strong> of GPU memory to fine-tune a Llama 2 70B model, enabling large model training on commodity hardware.</li>\n</ul>\n<h5 id=\"pros-9\">Pros</h5>\n<ul>\n  <li>High compression without sacrificing much accuracy.</li>\n  <li>Applicable to ultra-large models (e.g., Llama 2 70B).</li>\n  <li>Training only the LoRA adapters ensures stability even at low precision.</li>\n</ul>\n<h5 id=\"cons-9\">Cons</h5>\n<ul>\n  <li>Fixed quantized base may limit adaptability in highly domain-shifted settings.</li>\n  <li>Quantization granularity and calibration are critical for performance.</li>\n</ul>\n<h4 id=\"qlora-quantized-lora-with-4-bit-base-model\">QLoRA: Quantized LoRA with 4-bit Base Model</h4>\n<ul>\n  <li><strong>QLoRA</strong>, proposed in <a href=\"https://arxiv.org/abs/2305.14314\">Dettmers et al., 2023</a>, is an efficient fine-tuning method using a 4-bit quantized model backbone and LoRA adapters.</li>\n</ul>\n<h5 id=\"overview-4\">Overview</h5>\n<ul>\n  <li>Applies <strong>4-bit NormalFloat (NF4)</strong> quantization to the pretrained weights.</li>\n  <li>Performs <strong>double quantization</strong> to reduce memory further.</li>\n  <li>Freezes the quantized weights and trains <strong>LoRA adapters</strong> over them.</li>\n  <li>Uses paged optimizers and activation checkpointing for memory efficiency.</li>\n</ul>\n<h5 id=\"architecture\">Architecture</h5>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-161-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>finetuned</mtext></mrow></msub><mo>=</mo><msub><mtext>Quantize</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>NF4</mtext></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>pretrained</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>LoRA</mtext></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1872\" style=\"width: 22.398em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 18.648em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1018.65em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1873\"><span class=\"msubsup\" id=\"MathJax-Span-1874\"><span style=\"display: inline-block; position: relative; width: 3.596em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1875\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-1876\"><span class=\"mrow\" id=\"MathJax-Span-1877\"><span class=\"mtext\" id=\"MathJax-Span-1878\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">finetuned</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1879\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-1880\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 4.951em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1003.6em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mtext\" id=\"MathJax-Span-1881\" style=\"font-family: STIXGeneral-Regular;\">Quantize</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.799em; left: 3.596em;\"><span class=\"texatom\" id=\"MathJax-Span-1882\"><span class=\"mrow\" id=\"MathJax-Span-1883\"><span class=\"mtext\" id=\"MathJax-Span-1884\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">NF4</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1885\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1886\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1887\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-1888\"><span class=\"mrow\" id=\"MathJax-Span-1889\"><span class=\"mtext\" id=\"MathJax-Span-1890\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">pretrained</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1891\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1892\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-1893\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">Δ</span><span class=\"msubsup\" id=\"MathJax-Span-1894\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1895\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-1896\"><span class=\"mrow\" id=\"MathJax-Span-1897\"><span class=\"mtext\" id=\"MathJax-Span-1898\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">LoRA</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>finetuned</mtext></mrow></msub><mo>=</mo><msub><mtext>Quantize</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mtext>NF4</mtext></mrow></msub><mo stretchy=\"false\">(</mo><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>pretrained</mtext></mrow></msub><mo stretchy=\"false\">)</mo><mo>+</mo><mi mathvariant=\"normal\">Δ</mi><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>LoRA</mtext></mrow></msub></math></span></span></div>\n<ul>\n  <li>Only <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-162-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>LoRA</mtext></mrow></msub><mo>=</mo><mi>A</mi><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1899\" style=\"width: 7.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.99em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1900\"><span class=\"mi\" id=\"MathJax-Span-1901\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"msubsup\" id=\"MathJax-Span-1902\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1903\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-1904\"><span class=\"mrow\" id=\"MathJax-Span-1905\"><span class=\"mtext\" id=\"MathJax-Span-1906\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">LoRA</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1907\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1908\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">A</span><span class=\"mi\" id=\"MathJax-Span-1909\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Δ</mi><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>LoRA</mtext></mrow></msub><mo>=</mo><mi>A</mi><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-162\">\\Delta W_{\\text{LoRA}} = AB</script> is trained.</li>\n  <li>Entire fine-tuning can be performed in <strong>&lt; 24 GB</strong> of GPU memory for models like Llama‑65B.</li>\n</ul>\n<h5 id=\"pros-10\">Pros</h5>\n<ul>\n  <li>Fully open-source and hardware-efficient.</li>\n  <li>Well-established tools in the ecosystem (e.g., Hugging Face <code class=\"language-plaintext highlighter-rouge\">peft</code> and <code class=\"language-plaintext highlighter-rouge\">bitsandbytes</code>).</li>\n  <li>High accuracy retention even with 4-bit quantization.</li>\n</ul>\n<h5 id=\"cons-10\">Cons</h5>\n<ul>\n  <li>Limited to NF4 quantization scheme.</li>\n  <li>No learnability in the quantized weights themselves.</li>\n</ul>\n<h4 id=\"qa-lora-quantization-aware-lora\">QA-LoRA: Quantization-Aware LoRA</h4>\n<ul>\n  <li><strong>QA-LoRA</strong>, proposed in <a href=\"https://arxiv.org/abs/2309.13680\">Zhang et al., 2023</a>, adds quantization awareness to LoRA training by simulating quantization noise during fine-tuning.</li>\n</ul>\n<h5 id=\"overview-5\">Overview</h5>\n<ul>\n  <li>Quantization-aware noise is <strong>injected</strong> into both the pretrained weights and the LoRA adapters during training.</li>\n  <li>This simulates inference-time quantization effects during training, allowing the adapters to compensate more effectively.</li>\n</ul>\n<h5 id=\"architecture-1\">Architecture</h5>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-163-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>finetuned</mtext></mrow></msub><mo>=</mo><mtext>QuantNoise</mtext><mo stretchy=&quot;false&quot;>(</mo><mi>W</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mi>A</mi><mi>B</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1910\" style=\"width: 16.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1013.7em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1911\"><span class=\"msubsup\" id=\"MathJax-Span-1912\"><span style=\"display: inline-block; position: relative; width: 3.596em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1913\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-1914\"><span class=\"mrow\" id=\"MathJax-Span-1915\"><span class=\"mtext\" id=\"MathJax-Span-1916\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">finetuned</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1917\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mtext\" id=\"MathJax-Span-1918\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">QuantNoise</span><span class=\"mo\" id=\"MathJax-Span-1919\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1920\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1921\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1922\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-1923\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">A</span><span class=\"mi\" id=\"MathJax-Span-1924\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>finetuned</mtext></mrow></msub><mo>=</mo><mtext>QuantNoise</mtext><mo stretchy=\"false\">(</mo><mi>W</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>A</mi><mi>B</mi></math></span></span></div>\n<ul>\n  <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-164-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>QuantNoise</mtext><mo stretchy=&quot;false&quot;>(</mo><mi>W</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1925\" style=\"width: 7.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.461em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.41em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1926\"><span class=\"mtext\" id=\"MathJax-Span-1927\" style=\"font-family: STIXGeneral-Regular;\">QuantNoise</span><span class=\"mo\" id=\"MathJax-Span-1928\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1929\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1930\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>QuantNoise</mtext><mo stretchy=\"false\">(</mo><mi>W</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-164\">\\text{QuantNoise}(W)</script>: Simulates quantization-induced errors on the frozen weights.</li>\n  <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-165-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1931\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1932\"><span class=\"mi\" id=\"MathJax-Span-1933\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mi\" id=\"MathJax-Span-1934\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-165\">AB</script>: LoRA component trained with awareness of quantization.</li>\n</ul>\n<h5 id=\"key-features\">Key Features</h5>\n<ul>\n  <li>Supports ultra-low-bit quantization (e.g., 3- or 4-bit).</li>\n  <li>Enables <strong>quantization-aware training (QAT)</strong> without modifying the original weight update path.</li>\n</ul>\n<h5 id=\"pros-11\">Pros</h5>\n<ul>\n  <li>Improves robustness of LoRA adapters to quantization errors.</li>\n  <li>Achieves lower perplexity and better accuracy than standard LoRA or QLoRA in low-bit settings.</li>\n</ul>\n<h5 id=\"cons-11\">Cons</h5>\n<ul>\n  <li>Adds complexity to training pipeline.</li>\n  <li>May require careful tuning of noise injection parameters.</li>\n</ul>\n<h4 id=\"comparative-analysis-3\">Comparative Analysis</h4>\n<ul>\n  <li>This taxonomy of LoRA variants shows a clear evolution: from memory-focused quantized adapters (<strong>QLoRA</strong>) to ultra-low-bit efficient models (<strong>LQ-LoRA</strong>), to quantization-aware robust fine-tuning (<strong>QA-LoRA</strong>). Choice of method depends on the target compression level, hardware constraints, and sensitivity to quantization-induced artifacts.</li>\n</ul>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Feature</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LQ‑LoRA</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>QLoRA</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>QA‑LoRA</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantization Level</td>\n<td class=\"tg-tleft-valign-first\">≤ 3-bit (e.g., 2.75)</td>\n<td class=\"tg-tleft-valign-first\">4-bit NF4</td>\n<td class=\"tg-tleft-valign-second\">3–4-bit</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Trainable Params</td>\n<td class=\"tg-tleft-valign-first\">Low-rank adapters only</td>\n<td class=\"tg-tleft-valign-first\">LoRA adapters only</td>\n<td class=\"tg-tleft-valign-second\">LoRA adapters with QAT</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantized Weights</td>\n<td class=\"tg-tleft-valign-first\">Frozen, used as base</td>\n<td class=\"tg-tleft-valign-first\">Frozen, used as base</td>\n<td class=\"tg-tleft-valign-second\">Frozen + perturbed during training</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Noise Handling</td>\n<td class=\"tg-tleft-valign-first\">None</td>\n<td class=\"tg-tleft-valign-first\">None</td>\n<td class=\"tg-tleft-valign-second\">Simulated quantization noise</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Memory Efficiency</td>\n<td class=\"tg-tleft-valign-first\">~27 GB for Llama 2 70B</td>\n<td class=\"tg-tleft-valign-first\">~24 GB for Llama 65B</td>\n<td class=\"tg-tleft-valign-second\">Similar to QLoRA</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Complexity</td>\n<td class=\"tg-tleft-valign-first\">Medium</td>\n<td class=\"tg-tleft-valign-first\">Low</td>\n<td class=\"tg-tleft-valign-second\">High</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Best Use Case</td>\n<td class=\"tg-tleft-valign-first\">Ultra-compressed deployment</td>\n<td class=\"tg-tleft-valign-first\">General-purpose fine-tuning</td>\n<td class=\"tg-tleft-valign-second\">Robustness under low-bit QAT</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Feature</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LQ‑LoRA</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>QLoRA</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>QA‑LoRA</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantization Level</td>\n<td class=\"tg-tleft-valign-first\">≤ 3-bit (e.g., 2.75)</td>\n<td class=\"tg-tleft-valign-first\">4-bit NF4</td>\n<td class=\"tg-tleft-valign-second\">3–4-bit</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Trainable Params</td>\n<td class=\"tg-tleft-valign-first\">Low-rank adapters only</td>\n<td class=\"tg-tleft-valign-first\">LoRA adapters only</td>\n<td class=\"tg-tleft-valign-second\">LoRA adapters with QAT</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantized Weights</td>\n<td class=\"tg-tleft-valign-first\">Frozen, used as base</td>\n<td class=\"tg-tleft-valign-first\">Frozen, used as base</td>\n<td class=\"tg-tleft-valign-second\">Frozen + perturbed during training</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Noise Handling</td>\n<td class=\"tg-tleft-valign-first\">None</td>\n<td class=\"tg-tleft-valign-first\">None</td>\n<td class=\"tg-tleft-valign-second\">Simulated quantization noise</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Memory Efficiency</td>\n<td class=\"tg-tleft-valign-first\">~27 GB for Llama 2 70B</td>\n<td class=\"tg-tleft-valign-first\">~24 GB for Llama 65B</td>\n<td class=\"tg-tleft-valign-second\">Similar to QLoRA</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Complexity</td>\n<td class=\"tg-tleft-valign-first\">Medium</td>\n<td class=\"tg-tleft-valign-first\">Low</td>\n<td class=\"tg-tleft-valign-second\">High</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Best Use Case</td>\n<td class=\"tg-tleft-valign-first\">Ultra-compressed deployment</td>\n<td class=\"tg-tleft-valign-first\">General-purpose fine-tuning</td>\n<td class=\"tg-tleft-valign-second\">Robustness under low-bit QAT</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "*   Low-Rank Adaptation (LoRA) techniques allow efficient fine-tuning of large-scale pre-trained models by updating only a small number of additional low-rank matrices while freezing the original weights. Several recent advancements extend this idea by incorporating quantization, yielding memory-efficient and compute-friendly training pipelines. Below, we explore three major variants: **LQ-LoRA**, **QLoRA**, and **QA-LoRA**.\n\n#### LQ‑LoRA: Quantized + Low-Rank Adaptation\n\n*   **LQ-LoRA** was introduced in [Guo et al., 2023](https://arxiv.org/abs/2309.07063) as a memory-efficient fine-tuning approach that combines low-bit quantization with learnable low-rank adapters.\n\n##### Overview\n\n*   Each full-precision weight matrix W∈ℝd×dW∈Rd×dW \\\\in \\\\mathbb{R}^{d \\\\times d} is decomposed into:\n\nW≈Q+ABW≈Q+AB\n\n*   Q∈ℝd×dQ∈Rd×dQ \\\\in \\\\mathbb{R}^{d \\\\times d}: a low-bit quantized matrix (e.g., 2.75 bits), kept **frozen**\n*   A∈ℝd×rA∈Rd×rA \\\\in \\\\mathbb{R}^{d \\\\times r}, B∈ℝr×dB∈Rr×dB \\\\in \\\\mathbb{R}^{r \\\\times d}: low-rank full-precision matrices, **learnable**\n\n##### Key Properties\n\n*   The quantized base matrix QQQ captures the main representational power of the original model, without requiring updates during fine-tuning.\n*   The low-rank matrices AAA and BBB adapt the model to the downstream task.\n*   Enables **sub-3-bit quantization** without major degradation in task performance.\n*   Requires only **~27 GB** of GPU memory to fine-tune a Llama 2 70B model, enabling large model training on commodity hardware.\n\n##### Pros\n\n*   High compression without sacrificing much accuracy.\n*   Applicable to ultra-large models (e.g., Llama 2 70B).\n*   Training only the LoRA adapters ensures stability even at low precision.\n\n##### Cons\n\n*   Fixed quantized base may limit adaptability in highly domain-shifted settings.\n*   Quantization granularity and calibration are critical for performance.\n\n#### QLoRA: Quantized LoRA with 4-bit Base Model\n\n*   **QLoRA**, proposed in [Dettmers et al., 2023](https://arxiv.org/abs/2305.14314), is an efficient fine-tuning method using a 4-bit quantized model backbone and LoRA adapters.\n\n##### Overview\n\n*   Applies **4-bit NormalFloat (NF4)** quantization to the pretrained weights.\n*   Performs **double quantization** to reduce memory further.\n*   Freezes the quantized weights and trains **LoRA adapters** over them.\n*   Uses paged optimizers and activation checkpointing for memory efficiency.\n\n##### Architecture\n\nWfinetuned\\=QuantizeNF4(Wpretrained)+ΔWLoRAWfinetuned\\=QuantizeNF4(Wpretrained)+ΔWLoRA\n\n*   Only ΔWLoRA\\=ABΔWLoRA\\=AB\\\\Delta W\\_{\\\\text{LoRA}} = AB is trained.\n*   Entire fine-tuning can be performed in **< 24 GB** of GPU memory for models like Llama‑65B.\n\n##### Pros\n\n*   Fully open-source and hardware-efficient.\n*   Well-established tools in the ecosystem (e.g., Hugging Face `peft` and `bitsandbytes`).\n*   High accuracy retention even with 4-bit quantization.\n\n##### Cons\n\n*   Limited to NF4 quantization scheme.\n*   No learnability in the quantized weights themselves.\n\n#### QA-LoRA: Quantization-Aware LoRA\n\n*   **QA-LoRA**, proposed in [Zhang et al., 2023](https://arxiv.org/abs/2309.13680), adds quantization awareness to LoRA training by simulating quantization noise during fine-tuning.\n\n##### Overview\n\n*   Quantization-aware noise is **injected** into both the pretrained weights and the LoRA adapters during training.\n*   This simulates inference-time quantization effects during training, allowing the adapters to compensate more effectively.\n\n##### Architecture\n\nWfinetuned\\=QuantNoise(W)+ABWfinetuned\\=QuantNoise(W)+AB\n\n*   QuantNoise(W)QuantNoise(W)\\\\text{QuantNoise}(W): Simulates quantization-induced errors on the frozen weights.\n*   ABABAB: LoRA component trained with awareness of quantization.\n\n##### Key Features\n\n*   Supports ultra-low-bit quantization (e.g., 3- or 4-bit).\n*   Enables **quantization-aware training (QAT)** without modifying the original weight update path.\n\n##### Pros\n\n*   Improves robustness of LoRA adapters to quantization errors.\n*   Achieves lower perplexity and better accuracy than standard LoRA or QLoRA in low-bit settings.\n\n##### Cons\n\n*   Adds complexity to training pipeline.\n*   May require careful tuning of noise injection parameters.\n\n#### Comparative Analysis\n\n*   This taxonomy of LoRA variants shows a clear evolution: from memory-focused quantized adapters (**QLoRA**) to ultra-low-bit efficient models (**LQ-LoRA**), to quantization-aware robust fine-tuning (**QA-LoRA**). Choice of method depends on the target compression level, hardware constraints, and sensitivity to quantization-induced artifacts.\n\n**Feature**\n\n**LQ‑LoRA**\n\n**QLoRA**\n\n**QA‑LoRA**\n\nQuantization Level\n\n≤ 3-bit (e.g., 2.75)\n\n4-bit NF4\n\n3–4-bit\n\nTrainable Params\n\nLow-rank adapters only\n\nLoRA adapters only\n\nLoRA adapters with QAT\n\nQuantized Weights\n\nFrozen, used as base\n\nFrozen, used as base\n\nFrozen + perturbed during training\n\nNoise Handling\n\nNone\n\nNone\n\nSimulated quantization noise\n\nMemory Efficiency\n\n~27 GB for Llama 2 70B\n\n~24 GB for Llama 65B\n\nSimilar to QLoRA\n\nComplexity\n\nMedium\n\nLow\n\nHigh\n\nBest Use Case\n\nUltra-compressed deployment\n\nGeneral-purpose fine-tuning\n\nRobustness under low-bit QAT\n\n**Feature**\n\n**LQ‑LoRA**\n\n**QLoRA**\n\n**QA‑LoRA**\n\nQuantization Level\n\n≤ 3-bit (e.g., 2.75)\n\n4-bit NF4\n\n3–4-bit\n\nTrainable Params\n\nLow-rank adapters only\n\nLoRA adapters only\n\nLoRA adapters with QAT\n\nQuantized Weights\n\nFrozen, used as base\n\nFrozen, used as base\n\nFrozen + perturbed during training\n\nNoise Handling\n\nNone\n\nNone\n\nSimulated quantization noise\n\nMemory Efficiency\n\n~27 GB for Llama 2 70B\n\n~24 GB for Llama 65B\n\nSimilar to QLoRA\n\nComplexity\n\nMedium\n\nLow\n\nHigh\n\nBest Use Case\n\nUltra-compressed deployment\n\nGeneral-purpose fine-tuning\n\nRobustness under low-bit QAT",
      "order": 46,
      "orderInChapter": 5,
      "difficulty": 5,
      "estimatedMinutes": 4,
      "tags": [
        "ondevice ai",
        "activation",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 733,
        "contentLength": 42248
      },
      "nextCards": [
        "ai-model-compression-pros-cons-47",
        "ai-model-compression-comparison-use-cases-48"
      ],
      "relatedCards": [
        "ai-federated-learning-pros-cons-19",
        "ai-federated-learning-comparison-use-cases-20",
        "ai-federated-learning-llm-specific-enhancements-15",
        "ai-on-device-transformers-quantization-48-bit-11",
        "ai-federated-learning-lora-in-federated-context-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#quantized-low-rank-adaptation-techniques",
      "scrapedAt": "2025-12-28T11:55:50.974Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-pros-cons-47",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Low-Rank Decomposition & Adaptation",
      "title": "Pros & Cons",
      "subtitle": "Low-Rank Decomposition & Adaptation",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li><strong>Parameter-efficient fine-tuning</strong>: Minimal new parameters needed (e.g., LoRA, QLoRA, QA-LoRA).</li>\n      <li><strong>Quantization synergy</strong>: Works well with 4-bit quantization (QLoRA) or ultra-low-bit regimes (LQ-LoRA).</li>\n      <li><strong>Quantization-aware robustness</strong>: QA-LoRA improves low-bit model accuracy via simulated noise.</li>\n      <li><strong>Adaptable to distributed settings</strong>: LoRA-based updates are lightweight and communication-efficient.</li>\n      <li><strong>Accuracy retention</strong>: Strong accuracy, even under aggressive quantization.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>Effectiveness may degrade if base weights are not sufficiently low-rank (task-dependent).</li>\n      <li>Combining quantization and adaptation (as in QA-LoRA or LQ-LoRA) introduces training complexity.</li>\n      <li>Requires careful tuning of rank, quantization scheme, and noise injection (QA-LoRA).</li>\n      <li>QLoRA assumes compatibility with NF4 quantization and specific tooling (bitsandbytes, Hugging Face PEFT).</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li><strong>Parameter-efficient fine-tuning</strong>: Minimal new parameters needed (e.g., LoRA, QLoRA, QA-LoRA).</li>\n      <li><strong>Quantization synergy</strong>: Works well with 4-bit quantization (QLoRA) or ultra-low-bit regimes (LQ-LoRA).</li>\n      <li><strong>Quantization-aware robustness</strong>: QA-LoRA improves low-bit model accuracy via simulated noise.</li>\n      <li><strong>Adaptable to distributed settings</strong>: LoRA-based updates are lightweight and communication-efficient.</li>\n      <li><strong>Accuracy retention</strong>: Strong accuracy, even under aggressive quantization.</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>Effectiveness may degrade if base weights are not sufficiently low-rank (task-dependent).</li>\n      <li>Combining quantization and adaptation (as in QA-LoRA or LQ-LoRA) introduces training complexity.</li>\n      <li>Requires careful tuning of rank, quantization scheme, and noise injection (QA-LoRA).</li>\n      <li>QLoRA assumes compatibility with NF4 quantization and specific tooling (bitsandbytes, Hugging Face PEFT).</li>\n    </ul>",
      "contentMarkdown": "*   **Pros**:\n    \n    *   **Parameter-efficient fine-tuning**: Minimal new parameters needed (e.g., LoRA, QLoRA, QA-LoRA).\n    *   **Quantization synergy**: Works well with 4-bit quantization (QLoRA) or ultra-low-bit regimes (LQ-LoRA).\n    *   **Quantization-aware robustness**: QA-LoRA improves low-bit model accuracy via simulated noise.\n    *   **Adaptable to distributed settings**: LoRA-based updates are lightweight and communication-efficient.\n    *   **Accuracy retention**: Strong accuracy, even under aggressive quantization.\n*   **Cons**:\n    \n    *   Effectiveness may degrade if base weights are not sufficiently low-rank (task-dependent).\n    *   Combining quantization and adaptation (as in QA-LoRA or LQ-LoRA) introduces training complexity.\n    *   Requires careful tuning of rank, quantization scheme, and noise injection (QA-LoRA).\n    *   QLoRA assumes compatibility with NF4 quantization and specific tooling (bitsandbytes, Hugging Face PEFT).\n\n**Pros**:\n\n*   **Parameter-efficient fine-tuning**: Minimal new parameters needed (e.g., LoRA, QLoRA, QA-LoRA).\n*   **Quantization synergy**: Works well with 4-bit quantization (QLoRA) or ultra-low-bit regimes (LQ-LoRA).\n*   **Quantization-aware robustness**: QA-LoRA improves low-bit model accuracy via simulated noise.\n*   **Adaptable to distributed settings**: LoRA-based updates are lightweight and communication-efficient.\n*   **Accuracy retention**: Strong accuracy, even under aggressive quantization.\n\n**Cons**:\n\n*   Effectiveness may degrade if base weights are not sufficiently low-rank (task-dependent).\n*   Combining quantization and adaptation (as in QA-LoRA or LQ-LoRA) introduces training complexity.\n*   Requires careful tuning of rank, quantization scheme, and noise injection (QA-LoRA).\n*   QLoRA assumes compatibility with NF4 quantization and specific tooling (bitsandbytes, Hugging Face PEFT).",
      "order": 47,
      "orderInChapter": 6,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 218,
        "contentLength": 2350
      },
      "nextCards": [
        "ai-model-compression-comparison-use-cases-48",
        "ai-model-compression-key-takeaways-49"
      ],
      "relatedCards": [
        "ai-federated-learning-llm-specific-enhancements-15",
        "ai-federated-learning-pros-cons-19",
        "ai-federated-learning-comparison-use-cases-20",
        "ai-federated-learning-mime-9",
        "ai-federated-learning-lora-in-federated-context-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#pros-&-cons",
      "scrapedAt": "2025-12-28T11:55:50.974Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-comparison-use-cases-48",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Low-Rank Decomposition & Adaptation",
      "title": "Comparison & Use Cases",
      "subtitle": "Low-Rank Decomposition & Adaptation",
      "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Use Case</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Suggested Strategy</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Benefit</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Parameter-efficient tuning</td>\n<td class=\"tg-tleft-valign-first\">LoRA / QLoRA / ALoRA</td>\n<td class=\"tg-tleft-valign-second\">Reduces compute/memory footprint</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Extreme quantization</td>\n<td class=\"tg-tleft-valign-first\">LQ‑LoRA or Low-Rank Correction</td>\n<td class=\"tg-tleft-valign-second\">Sub‑3‑bit performance retention</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Federated fine‑tuning</td>\n<td class=\"tg-tleft-valign-first\">Federated LoRA / QLoRA adapters</td>\n<td class=\"tg-tleft-valign-second\">Minimal communication cost</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantization-aware training</td>\n<td class=\"tg-tleft-valign-first\">QA‑LoRA + QAT</td>\n<td class=\"tg-tleft-valign-second\">High fidelity under 3–4 bit settings</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">4-bit memory-efficient finetuning</td>\n<td class=\"tg-tleft-valign-first\">QLoRA</td>\n<td class=\"tg-tleft-valign-second\">Near full accuracy with 4-bit NF4</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Robust training under quantization noise</td>\n<td class=\"tg-tleft-valign-first\">QA‑LoRA</td>\n<td class=\"tg-tleft-valign-second\">Noise-aware adapters improve generalization</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Use Case</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Suggested Strategy</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Benefit</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Parameter-efficient tuning</td>\n<td class=\"tg-tleft-valign-first\">LoRA / QLoRA / ALoRA</td>\n<td class=\"tg-tleft-valign-second\">Reduces compute/memory footprint</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Extreme quantization</td>\n<td class=\"tg-tleft-valign-first\">LQ‑LoRA or Low-Rank Correction</td>\n<td class=\"tg-tleft-valign-second\">Sub‑3‑bit performance retention</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Federated fine‑tuning</td>\n<td class=\"tg-tleft-valign-first\">Federated LoRA / QLoRA adapters</td>\n<td class=\"tg-tleft-valign-second\">Minimal communication cost</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantization-aware training</td>\n<td class=\"tg-tleft-valign-first\">QA‑LoRA + QAT</td>\n<td class=\"tg-tleft-valign-second\">High fidelity under 3–4 bit settings</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">4-bit memory-efficient finetuning</td>\n<td class=\"tg-tleft-valign-first\">QLoRA</td>\n<td class=\"tg-tleft-valign-second\">Near full accuracy with 4-bit NF4</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Robust training under quantization noise</td>\n<td class=\"tg-tleft-valign-first\">QA‑LoRA</td>\n<td class=\"tg-tleft-valign-second\">Noise-aware adapters improve generalization</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "**Use Case**\n\n**Suggested Strategy**\n\n**Benefit**\n\nParameter-efficient tuning\n\nLoRA / QLoRA / ALoRA\n\nReduces compute/memory footprint\n\nExtreme quantization\n\nLQ‑LoRA or Low-Rank Correction\n\nSub‑3‑bit performance retention\n\nFederated fine‑tuning\n\nFederated LoRA / QLoRA adapters\n\nMinimal communication cost\n\nQuantization-aware training\n\nQA‑LoRA + QAT\n\nHigh fidelity under 3–4 bit settings\n\n4-bit memory-efficient finetuning\n\nQLoRA\n\nNear full accuracy with 4-bit NF4\n\nRobust training under quantization noise\n\nQA‑LoRA\n\nNoise-aware adapters improve generalization\n\n**Use Case**\n\n**Suggested Strategy**\n\n**Benefit**\n\nParameter-efficient tuning\n\nLoRA / QLoRA / ALoRA\n\nReduces compute/memory footprint\n\nExtreme quantization\n\nLQ‑LoRA or Low-Rank Correction\n\nSub‑3‑bit performance retention\n\nFederated fine‑tuning\n\nFederated LoRA / QLoRA adapters\n\nMinimal communication cost\n\nQuantization-aware training\n\nQA‑LoRA + QAT\n\nHigh fidelity under 3–4 bit settings\n\n4-bit memory-efficient finetuning\n\nQLoRA\n\nNear full accuracy with 4-bit NF4\n\nRobust training under quantization noise\n\nQA‑LoRA\n\nNoise-aware adapters improve generalization",
      "order": 48,
      "orderInChapter": 7,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 130,
        "contentLength": 3127
      },
      "nextCards": [
        "ai-model-compression-key-takeaways-49",
        "ai-model-compression-principles-of-lightweight-design-50"
      ],
      "relatedCards": [
        "ai-federated-learning-mime-9",
        "ai-federated-learning-lora-in-federated-context-17",
        "ai-federated-learning-open-challenges-18",
        "ai-federated-learning-legal-benefits-23",
        "ai-differential-privacy-components-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#comparison-&-use-cases",
      "scrapedAt": "2025-12-28T11:55:50.974Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-key-takeaways-49",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Low-Rank Decomposition & Adaptation",
      "title": "Key Takeaways",
      "subtitle": "Low-Rank Decomposition & Adaptation",
      "contentHtml": "<ul>\n  <li>\n    <p>Low-rank techniques improve the efficiency of training and inference by decomposing full-rank weight matrices into two smaller matrices (e.g., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-166-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mo>&amp;#x00D7;</mo><mi>r</mi></mrow></msup><mtext>&amp;#xA0;and&amp;#xA0;</mtext><mi>B</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>m</mi></mrow></msup><mo>,</mo><mtext>&amp;#xA0;where&amp;#xA0;</mtext><mi>r</mi><mo>&amp;#x226A;</mo><mi>n</mi><mo>,</mo><mi>m</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1935\" style=\"width: 19.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 16.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1016.62em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1936\"><span class=\"mi\" id=\"MathJax-Span-1937\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-1938\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1939\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1940\"><span class=\"mrow\" id=\"MathJax-Span-1941\"><span class=\"mi\" id=\"MathJax-Span-1942\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1943\"><span class=\"mrow\" id=\"MathJax-Span-1944\"><span class=\"mi\" id=\"MathJax-Span-1945\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-1946\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1947\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-1948\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;and&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-1949\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mo\" id=\"MathJax-Span-1950\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1951\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1952\"><span class=\"mrow\" id=\"MathJax-Span-1953\"><span class=\"mi\" id=\"MathJax-Span-1954\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1955\"><span class=\"mrow\" id=\"MathJax-Span-1956\"><span class=\"mi\" id=\"MathJax-Span-1957\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1958\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1959\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1960\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-1961\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">&nbsp;where&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-1962\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1963\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≪</span><span class=\"mi\" id=\"MathJax-Span-1964\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">n</span><span class=\"mo\" id=\"MathJax-Span-1965\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1966\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">m</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mo>×</mo><mi>r</mi></mrow></msup><mtext>&nbsp;and&nbsp;</mtext><mi>B</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mo>×</mo><mi>m</mi></mrow></msup><mo>,</mo><mtext>&nbsp;where&nbsp;</mtext><mi>r</mi><mo>≪</mo><mi>n</mi><mo>,</mo><mi>m</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-166\">A \\in \\mathbb{R}^{n \\times r} \\text{ and } B \\in \\mathbb{R}^{r \\times m}, \\text{ where } r \\ll n, m</script>). These factorizations can replace or augment full-weight updates, as seen in LoRA and its variants.</p>\n\n    <ul>\n      <li><strong>QLoRA</strong> combines low-rank adapters with 4-bit NF4 quantized base models, drastically reducing memory usage during training without sacrificing accuracy. It enables finetuning large LLMs (e.g., 65B+) on consumer hardware.</li>\n      <li><strong>QA-LoRA</strong> extends this further by injecting simulated quantization noise into the training process, making LoRA adapters inherently robust to downstream quantization.</li>\n      <li><strong>LQ-LoRA</strong> targets extremely low-bit regimes (e.g., sub-3-bit) by jointly optimizing low-rank corrections and quantized base weights.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Overall, low-rank decomposition plays a central role in enabling quantization-aware fine-tuning pipelines, federated adaptation, and cross-device deployment, all while maintaining high performance and parameter efficiency.</p>\n  </li>\n</ul>\n<p>Low-rank techniques improve the efficiency of training and inference by decomposing full-rank weight matrices into two smaller matrices (e.g., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-166-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi><mo>&amp;#x00D7;</mo><mi>r</mi></mrow></msup><mtext>&amp;#xA0;and&amp;#xA0;</mtext><mi>B</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>m</mi></mrow></msup><mo>,</mo><mtext>&amp;#xA0;where&amp;#xA0;</mtext><mi>r</mi><mo>&amp;#x226A;</mo><mi>n</mi><mo>,</mo><mi>m</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1935\" style=\"width: 19.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 16.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1016.62em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1936\"><span class=\"mi\" id=\"MathJax-Span-1937\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-1938\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1939\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1940\"><span class=\"mrow\" id=\"MathJax-Span-1941\"><span class=\"mi\" id=\"MathJax-Span-1942\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1943\"><span class=\"mrow\" id=\"MathJax-Span-1944\"><span class=\"mi\" id=\"MathJax-Span-1945\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-1946\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1947\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-1948\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;and&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-1949\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mo\" id=\"MathJax-Span-1950\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-1951\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1952\"><span class=\"mrow\" id=\"MathJax-Span-1953\"><span class=\"mi\" id=\"MathJax-Span-1954\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-1955\"><span class=\"mrow\" id=\"MathJax-Span-1956\"><span class=\"mi\" id=\"MathJax-Span-1957\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1958\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1959\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1960\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-1961\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">&nbsp;where&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-1962\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1963\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≪</span><span class=\"mi\" id=\"MathJax-Span-1964\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">n</span><span class=\"mo\" id=\"MathJax-Span-1965\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-1966\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">m</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi><mo>×</mo><mi>r</mi></mrow></msup><mtext>&nbsp;and&nbsp;</mtext><mi>B</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mo>×</mo><mi>m</mi></mrow></msup><mo>,</mo><mtext>&nbsp;where&nbsp;</mtext><mi>r</mi><mo>≪</mo><mi>n</mi><mo>,</mo><mi>m</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-166\">A \\in \\mathbb{R}^{n \\times r} \\text{ and } B \\in \\mathbb{R}^{r \\times m}, \\text{ where } r \\ll n, m</script>). These factorizations can replace or augment full-weight updates, as seen in LoRA and its variants.</p>\n<ul>\n      <li><strong>QLoRA</strong> combines low-rank adapters with 4-bit NF4 quantized base models, drastically reducing memory usage during training without sacrificing accuracy. It enables finetuning large LLMs (e.g., 65B+) on consumer hardware.</li>\n      <li><strong>QA-LoRA</strong> extends this further by injecting simulated quantization noise into the training process, making LoRA adapters inherently robust to downstream quantization.</li>\n      <li><strong>LQ-LoRA</strong> targets extremely low-bit regimes (e.g., sub-3-bit) by jointly optimizing low-rank corrections and quantized base weights.</li>\n    </ul>\n<p>Overall, low-rank decomposition plays a central role in enabling quantization-aware fine-tuning pipelines, federated adaptation, and cross-device deployment, all while maintaining high performance and parameter efficiency.</p>",
      "contentMarkdown": "*   Low-rank techniques improve the efficiency of training and inference by decomposing full-rank weight matrices into two smaller matrices (e.g., A∈ℝn×r and B∈ℝr×m, where r≪n,mA∈Rn×r and B∈Rr×m, where r≪n,mA \\\\in \\\\mathbb{R}^{n \\\\times r} \\\\text{ and } B \\\\in \\\\mathbb{R}^{r \\\\times m}, \\\\text{ where } r \\\\ll n, m). These factorizations can replace or augment full-weight updates, as seen in LoRA and its variants.\n    \n    *   **QLoRA** combines low-rank adapters with 4-bit NF4 quantized base models, drastically reducing memory usage during training without sacrificing accuracy. It enables finetuning large LLMs (e.g., 65B+) on consumer hardware.\n    *   **QA-LoRA** extends this further by injecting simulated quantization noise into the training process, making LoRA adapters inherently robust to downstream quantization.\n    *   **LQ-LoRA** targets extremely low-bit regimes (e.g., sub-3-bit) by jointly optimizing low-rank corrections and quantized base weights.\n*   Overall, low-rank decomposition plays a central role in enabling quantization-aware fine-tuning pipelines, federated adaptation, and cross-device deployment, all while maintaining high performance and parameter efficiency.\n    \n\nLow-rank techniques improve the efficiency of training and inference by decomposing full-rank weight matrices into two smaller matrices (e.g., A∈ℝn×r and B∈ℝr×m, where r≪n,mA∈Rn×r and B∈Rr×m, where r≪n,mA \\\\in \\\\mathbb{R}^{n \\\\times r} \\\\text{ and } B \\\\in \\\\mathbb{R}^{r \\\\times m}, \\\\text{ where } r \\\\ll n, m). These factorizations can replace or augment full-weight updates, as seen in LoRA and its variants.\n\n*   **QLoRA** combines low-rank adapters with 4-bit NF4 quantized base models, drastically reducing memory usage during training without sacrificing accuracy. It enables finetuning large LLMs (e.g., 65B+) on consumer hardware.\n*   **QA-LoRA** extends this further by injecting simulated quantization noise into the training process, making LoRA adapters inherently robust to downstream quantization.\n*   **LQ-LoRA** targets extremely low-bit regimes (e.g., sub-3-bit) by jointly optimizing low-rank corrections and quantized base weights.\n\nOverall, low-rank decomposition plays a central role in enabling quantization-aware fine-tuning pipelines, federated adaptation, and cross-device deployment, all while maintaining high performance and parameter efficiency.",
      "order": 49,
      "orderInChapter": 8,
      "difficulty": 5,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "llm",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 314,
        "contentLength": 14317
      },
      "nextCards": [
        "ai-model-compression-principles-of-lightweight-design-50",
        "ai-model-compression-design-methodologies-51"
      ],
      "relatedCards": [
        "ai-differential-privacy-fine-tuning-with-dp-sgd-12",
        "ai-federated-learning-llm-specific-enhancements-15",
        "ai-federated-learning-pros-cons-19",
        "ai-differential-privacy-cons-16",
        "ai-federated-learning-what-is-federated-lora-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#key-takeaways",
      "scrapedAt": "2025-12-28T11:55:50.974Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-principles-of-lightweight-design-50",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Lightweight Model Design",
      "title": "Principles of Lightweight Design",
      "subtitle": "Lightweight Model Design",
      "contentHtml": "<ol>\n  <li>\n    <p><strong>Parameter Efficiency</strong>: Instead of large dense layers, lightweight designs emphasize reducing parameter counts through smaller embedding dimensions, narrower feed-forward layers, and compact convolutional kernels. For instance, a conventional convolution with kernel size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-167-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x00D7;</mo><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1967\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.04em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1968\"><span class=\"mo\" id=\"MathJax-Span-1969\" style=\"font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1970\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>×</mo><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-167\">\\times k</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-168-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>in</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1971\" style=\"width: 1.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.3em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1972\"><span class=\"msubsup\" id=\"MathJax-Span-1973\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1974\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1975\"><span class=\"mrow\" id=\"MathJax-Span-1976\"><span class=\"mtext\" id=\"MathJax-Span-1977\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">in</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>in</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-168\">C_{\\text{in}}</script> input channels can be replaced with a depthwise convolution (cost <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-169-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>k</mi><mn>2</mn></msup><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>in</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1978\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1002.24em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1979\"><span class=\"msubsup\" id=\"MathJax-Span-1980\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1981\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-1982\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1983\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1984\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1985\"><span class=\"mrow\" id=\"MathJax-Span-1986\"><span class=\"mtext\" id=\"MathJax-Span-1987\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">in</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>k</mi><mn>2</mn></msup><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>in</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-169\">k^2 C_{\\text{in}}</script>) followed by a pointwise <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-170-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1</mn><mo>&amp;#x00D7;</mo><mn>1</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1988\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1989\"><span class=\"mn\" id=\"MathJax-Span-1990\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-1991\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-1992\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>1</mn><mo>×</mo><mn>1</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-170\">1 \\times 1</script> convolution (cost <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-171-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>in</mtext></mrow></msub><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>out</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1993\" style=\"width: 3.596em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.97em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1994\"><span class=\"msubsup\" id=\"MathJax-Span-1995\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1996\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1997\"><span class=\"mrow\" id=\"MathJax-Span-1998\"><span class=\"mtext\" id=\"MathJax-Span-1999\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">in</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-2000\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-2001\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-2002\"><span class=\"mrow\" id=\"MathJax-Span-2003\"><span class=\"mtext\" id=\"MathJax-Span-2004\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">out</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>in</mtext></mrow></msub><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>out</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-171\">C_{\\text{in}} C_{\\text{out}}</script>), drastically lowering Multiply-Accumulate operations (MACs).</p>\n  </li>\n  <li>\n    <p><strong>Computational Sparsity</strong>: Many architectures employ sparsity patterns directly in their design—such as grouped convolutions, block-sparse attention, or factorized projections—to reduce the number of required operations without relying on post-hoc pruning.</p>\n  </li>\n  <li>\n    <p><strong>Layer Reduction and Structural Reuse</strong>: Models like DistilBERT and TinyBERT achieve compactness by halving the number of transformer layers while using distillation losses to retain semantic fidelity. CNN variants often reuse small building blocks in repeated stages to maintain expressiveness without excessive depth.</p>\n  </li>\n  <li>\n    <p><strong>Activation and Feature Map Optimization</strong>: Activations are often the main source of memory usage during inference. Designs that minimize activation size—via lower resolution feature maps, early downsampling, or reduced channel widths—reduce both memory footprint and bandwidth demands.</p>\n  </li>\n  <li>\n    <p><strong>Weight Sharing</strong>: Weight sharing reduces storage requirements by reusing the same parameter values across multiple parts of the network. Instead of learning a unique weight for every connection, the model maintains a smaller set of shared weights and uses index mapping to assign them where needed.</p>\n    <ul>\n      <li><strong>Vector/Matrix Sharing</strong>: In RNNs and Transformers, the same weight matrix may be used for multiple layers or projections. A notable example is <em>weight tying</em> in language models, where the input embedding matrix and the output softmax weights are shared to both reduce parameters and improve perplexity, as proposed in <a href=\"https://arxiv.org/abs/1608.05859\">Using the Output Embedding to Improve Language Models</a> by Press and Wolf (2017).</li>\n      <li><strong>Hash-Based Sharing</strong>: Parameters are grouped by a hash function into a small number of “buckets,” each storing a single shared weight value.</li>\n      <li><strong>Cyclic or Rotational Sharing</strong>: Convolutional kernels are repeated or rotated across channels or layers, reducing the total number of learned unique values.</li>\n      <li><strong>Benefits</strong>: Dramatically reduces model storage and can improve generalization by limiting overfitting.</li>\n      <li><strong>Trade-offs</strong>: May slightly reduce representational capacity if sharing is too aggressive, requiring careful balancing.</li>\n    </ul>\n  </li>\n</ol>\n<p><strong>Parameter Efficiency</strong>: Instead of large dense layers, lightweight designs emphasize reducing parameter counts through smaller embedding dimensions, narrower feed-forward layers, and compact convolutional kernels. For instance, a conventional convolution with kernel size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-167-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo>&amp;#x00D7;</mo><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1967\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.04em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1968\"><span class=\"mo\" id=\"MathJax-Span-1969\" style=\"font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-1970\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo>×</mo><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-167\">\\times k</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-168-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>in</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1971\" style=\"width: 1.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.3em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1972\"><span class=\"msubsup\" id=\"MathJax-Span-1973\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1974\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1975\"><span class=\"mrow\" id=\"MathJax-Span-1976\"><span class=\"mtext\" id=\"MathJax-Span-1977\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">in</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>in</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-168\">C_{\\text{in}}</script> input channels can be replaced with a depthwise convolution (cost <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-169-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>k</mi><mn>2</mn></msup><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>in</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1978\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1002.24em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1979\"><span class=\"msubsup\" id=\"MathJax-Span-1980\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1981\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-1982\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1983\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1984\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1985\"><span class=\"mrow\" id=\"MathJax-Span-1986\"><span class=\"mtext\" id=\"MathJax-Span-1987\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">in</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>k</mi><mn>2</mn></msup><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>in</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-169\">k^2 C_{\\text{in}}</script>) followed by a pointwise <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-170-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>1</mn><mo>&amp;#x00D7;</mo><mn>1</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1988\" style=\"width: 2.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.98em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1989\"><span class=\"mn\" id=\"MathJax-Span-1990\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-1991\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-1992\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>1</mn><mo>×</mo><mn>1</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-170\">1 \\times 1</script> convolution (cost <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-171-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>in</mtext></mrow></msub><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>out</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1993\" style=\"width: 3.596em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.97em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1994\"><span class=\"msubsup\" id=\"MathJax-Span-1995\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1996\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1997\"><span class=\"mrow\" id=\"MathJax-Span-1998\"><span class=\"mtext\" id=\"MathJax-Span-1999\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">in</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-2000\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-2001\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-2002\"><span class=\"mrow\" id=\"MathJax-Span-2003\"><span class=\"mtext\" id=\"MathJax-Span-2004\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">out</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>in</mtext></mrow></msub><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>out</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-171\">C_{\\text{in}} C_{\\text{out}}</script>), drastically lowering Multiply-Accumulate operations (MACs).</p>\n<p><strong>Computational Sparsity</strong>: Many architectures employ sparsity patterns directly in their design—such as grouped convolutions, block-sparse attention, or factorized projections—to reduce the number of required operations without relying on post-hoc pruning.</p>\n<p><strong>Layer Reduction and Structural Reuse</strong>: Models like DistilBERT and TinyBERT achieve compactness by halving the number of transformer layers while using distillation losses to retain semantic fidelity. CNN variants often reuse small building blocks in repeated stages to maintain expressiveness without excessive depth.</p>\n<p><strong>Activation and Feature Map Optimization</strong>: Activations are often the main source of memory usage during inference. Designs that minimize activation size—via lower resolution feature maps, early downsampling, or reduced channel widths—reduce both memory footprint and bandwidth demands.</p>\n<p><strong>Weight Sharing</strong>: Weight sharing reduces storage requirements by reusing the same parameter values across multiple parts of the network. Instead of learning a unique weight for every connection, the model maintains a smaller set of shared weights and uses index mapping to assign them where needed.</p>\n<ul>\n      <li><strong>Vector/Matrix Sharing</strong>: In RNNs and Transformers, the same weight matrix may be used for multiple layers or projections. A notable example is <em>weight tying</em> in language models, where the input embedding matrix and the output softmax weights are shared to both reduce parameters and improve perplexity, as proposed in <a href=\"https://arxiv.org/abs/1608.05859\">Using the Output Embedding to Improve Language Models</a> by Press and Wolf (2017).</li>\n      <li><strong>Hash-Based Sharing</strong>: Parameters are grouped by a hash function into a small number of “buckets,” each storing a single shared weight value.</li>\n      <li><strong>Cyclic or Rotational Sharing</strong>: Convolutional kernels are repeated or rotated across channels or layers, reducing the total number of learned unique values.</li>\n      <li><strong>Benefits</strong>: Dramatically reduces model storage and can improve generalization by limiting overfitting.</li>\n      <li><strong>Trade-offs</strong>: May slightly reduce representational capacity if sharing is too aggressive, requiring careful balancing.</li>\n    </ul>",
      "contentMarkdown": "1.  **Parameter Efficiency**: Instead of large dense layers, lightweight designs emphasize reducing parameter counts through smaller embedding dimensions, narrower feed-forward layers, and compact convolutional kernels. For instance, a conventional convolution with kernel size ×k×k\\\\times k and CinCinC\\_{\\\\text{in}} input channels can be replaced with a depthwise convolution (cost k2Cink2Cink^2 C\\_{\\\\text{in}}) followed by a pointwise 1×11×11 \\\\times 1 convolution (cost CinCoutCinCoutC\\_{\\\\text{in}} C\\_{\\\\text{out}}), drastically lowering Multiply-Accumulate operations (MACs).\n    \n2.  **Computational Sparsity**: Many architectures employ sparsity patterns directly in their design—such as grouped convolutions, block-sparse attention, or factorized projections—to reduce the number of required operations without relying on post-hoc pruning.\n    \n3.  **Layer Reduction and Structural Reuse**: Models like DistilBERT and TinyBERT achieve compactness by halving the number of transformer layers while using distillation losses to retain semantic fidelity. CNN variants often reuse small building blocks in repeated stages to maintain expressiveness without excessive depth.\n    \n4.  **Activation and Feature Map Optimization**: Activations are often the main source of memory usage during inference. Designs that minimize activation size—via lower resolution feature maps, early downsampling, or reduced channel widths—reduce both memory footprint and bandwidth demands.\n    \n5.  **Weight Sharing**: Weight sharing reduces storage requirements by reusing the same parameter values across multiple parts of the network. Instead of learning a unique weight for every connection, the model maintains a smaller set of shared weights and uses index mapping to assign them where needed.\n    \n    *   **Vector/Matrix Sharing**: In RNNs and Transformers, the same weight matrix may be used for multiple layers or projections. A notable example is _weight tying_ in language models, where the input embedding matrix and the output softmax weights are shared to both reduce parameters and improve perplexity, as proposed in [Using the Output Embedding to Improve Language Models](https://arxiv.org/abs/1608.05859) by Press and Wolf (2017).\n    *   **Hash-Based Sharing**: Parameters are grouped by a hash function into a small number of “buckets,” each storing a single shared weight value.\n    *   **Cyclic or Rotational Sharing**: Convolutional kernels are repeated or rotated across channels or layers, reducing the total number of learned unique values.\n    *   **Benefits**: Dramatically reduces model storage and can improve generalization by limiting overfitting.\n    *   **Trade-offs**: May slightly reduce representational capacity if sharing is too aggressive, requiring careful balancing.\n\n**Parameter Efficiency**: Instead of large dense layers, lightweight designs emphasize reducing parameter counts through smaller embedding dimensions, narrower feed-forward layers, and compact convolutional kernels. For instance, a conventional convolution with kernel size ×k×k\\\\times k and CinCinC\\_{\\\\text{in}} input channels can be replaced with a depthwise convolution (cost k2Cink2Cink^2 C\\_{\\\\text{in}}) followed by a pointwise 1×11×11 \\\\times 1 convolution (cost CinCoutCinCoutC\\_{\\\\text{in}} C\\_{\\\\text{out}}), drastically lowering Multiply-Accumulate operations (MACs).\n\n**Computational Sparsity**: Many architectures employ sparsity patterns directly in their design—such as grouped convolutions, block-sparse attention, or factorized projections—to reduce the number of required operations without relying on post-hoc pruning.\n\n**Layer Reduction and Structural Reuse**: Models like DistilBERT and TinyBERT achieve compactness by halving the number of transformer layers while using distillation losses to retain semantic fidelity. CNN variants often reuse small building blocks in repeated stages to maintain expressiveness without excessive depth.\n\n**Activation and Feature Map Optimization**: Activations are often the main source of memory usage during inference. Designs that minimize activation size—via lower resolution feature maps, early downsampling, or reduced channel widths—reduce both memory footprint and bandwidth demands.\n\n**Weight Sharing**: Weight sharing reduces storage requirements by reusing the same parameter values across multiple parts of the network. Instead of learning a unique weight for every connection, the model maintains a smaller set of shared weights and uses index mapping to assign them where needed.\n\n*   **Vector/Matrix Sharing**: In RNNs and Transformers, the same weight matrix may be used for multiple layers or projections. A notable example is _weight tying_ in language models, where the input embedding matrix and the output softmax weights are shared to both reduce parameters and improve perplexity, as proposed in [Using the Output Embedding to Improve Language Models](https://arxiv.org/abs/1608.05859) by Press and Wolf (2017).\n*   **Hash-Based Sharing**: Parameters are grouped by a hash function into a small number of “buckets,” each storing a single shared weight value.\n*   **Cyclic or Rotational Sharing**: Convolutional kernels are repeated or rotated across channels or layers, reducing the total number of learned unique values.\n*   **Benefits**: Dramatically reduces model storage and can improve generalization by limiting overfitting.\n*   **Trade-offs**: May slightly reduce representational capacity if sharing is too aggressive, requiring careful balancing.",
      "order": 50,
      "orderInChapter": 1,
      "difficulty": 5,
      "estimatedMinutes": 4,
      "tags": [
        "ondevice ai",
        "transformer",
        "attention",
        "embedding",
        "convolution",
        "cnn",
        "rnn",
        "bert"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 719,
        "contentLength": 28109
      },
      "nextCards": [
        "ai-model-compression-design-methodologies-51",
        "ai-model-compression-representative-architectures-52"
      ],
      "relatedCards": [
        "ai-on-device-transformers-tokenizer-and-vocabulary-size-22",
        "ai-on-device-transformers-parameter-count-and-model-depth-25",
        "ai-on-device-transformers-cpu-deployment-considerations-17",
        "ai-on-device-transformers-modelembedding-dimension-23",
        "ai-on-device-transformers-embedding-size-times-vocabulary-size-times-depth-26"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#principles-of-lightweight-design",
      "scrapedAt": "2025-12-28T11:55:50.974Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-design-methodologies-51",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Lightweight Model Design",
      "title": "Design Methodologies",
      "subtitle": "Lightweight Model Design",
      "contentHtml": "<ul>\n  <li><strong>Manual Architecture Engineering</strong>: Historically, lightweight models such as SqueezeNet and MobileNetV1 emerged from manual exploration of kernel sizes, strides, and filter counts to balance accuracy and cost.</li>\n  <li><strong>Neural Architecture Search (NAS)</strong>: Modern approaches leverage latency-aware NAS to discover architectures tailored to specific devices. Search objectives often incorporate hardware-measured inference latency, power draw, or memory footprint in addition to accuracy.</li>\n  <li><strong>Hybrid Approaches</strong>: Many practical deployments combine lightweight design with compression techniques. For example, MobileBERT applies a bottlenecked Transformer architecture and then further compresses it via quantization and distillation.</li>\n</ul>",
      "contentMarkdown": "*   **Manual Architecture Engineering**: Historically, lightweight models such as SqueezeNet and MobileNetV1 emerged from manual exploration of kernel sizes, strides, and filter counts to balance accuracy and cost.\n*   **Neural Architecture Search (NAS)**: Modern approaches leverage latency-aware NAS to discover architectures tailored to specific devices. Search objectives often incorporate hardware-measured inference latency, power draw, or memory footprint in addition to accuracy.\n*   **Hybrid Approaches**: Many practical deployments combine lightweight design with compression techniques. For example, MobileBERT applies a bottlenecked Transformer architecture and then further compresses it via quantization and distillation.",
      "order": 51,
      "orderInChapter": 2,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "transformer",
        "bert"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 90,
        "contentLength": 806
      },
      "nextCards": [
        "ai-model-compression-representative-architectures-52",
        "ai-model-compression-when-to-use-lightweight-models-53"
      ],
      "relatedCards": [
        "ai-on-device-transformers-gpu-deployment-considerations-18",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5",
        "ai-on-device-transformers-neural-processing-unit-npu-6",
        "ai-on-device-transformers-cpu-deployment-considerations-17",
        "ai-federated-learning-what-is-federated-lora-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#design-methodologies",
      "scrapedAt": "2025-12-28T11:55:50.974Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-representative-architectures-52",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Lightweight Model Design",
      "title": "Representative Architectures",
      "subtitle": "Lightweight Model Design",
      "contentHtml": "<ul>\n  <li><strong>MobileNetV2/V3</strong>: Introduced inverted residual blocks with linear bottlenecks and squeeze-and-excitation modules for better accuracy-efficiency trade-offs.</li>\n  <li><strong>EfficientNet-Lite</strong>: Scales network depth, width, and resolution using compound scaling optimized for mobile hardware.</li>\n  <li><strong>DistilBERT</strong>: Retains 97% of BERT-base’s language understanding capability with 40% fewer parameters and 60% faster inference.</li>\n  <li><strong>ConvNeXt-T</strong>: Adapts design elements from vision transformers into a lightweight CNN backbone for efficient vision tasks.</li>\n</ul>",
      "contentMarkdown": "*   **MobileNetV2/V3**: Introduced inverted residual blocks with linear bottlenecks and squeeze-and-excitation modules for better accuracy-efficiency trade-offs.\n*   **EfficientNet-Lite**: Scales network depth, width, and resolution using compound scaling optimized for mobile hardware.\n*   **DistilBERT**: Retains 97% of BERT-base’s language understanding capability with 40% fewer parameters and 60% faster inference.\n*   **ConvNeXt-T**: Adapts design elements from vision transformers into a lightweight CNN backbone for efficient vision tasks.",
      "order": 52,
      "orderInChapter": 3,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "transformer",
        "cnn",
        "bert"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 65,
        "contentLength": 638
      },
      "nextCards": [
        "ai-model-compression-when-to-use-lightweight-models-53"
      ],
      "relatedCards": [
        "ai-on-device-transformers-gpu-deployment-considerations-18",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5",
        "ai-on-device-transformers-neural-processing-unit-npu-6",
        "ai-on-device-transformers-cpu-deployment-considerations-17",
        "ai-federated-learning-what-is-federated-lora-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#representative-architectures",
      "scrapedAt": "2025-12-28T11:55:50.974Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-model-compression-when-to-use-lightweight-models-53",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Model Compression",
      "articleSlug": "model-compression",
      "chapter": "Lightweight Model Design",
      "title": "When to Use Lightweight Models",
      "subtitle": "Lightweight Model Design",
      "contentHtml": "<ul>\n  <li>The target hardware has strict latency or power limits that even heavily compressed large models cannot meet.</li>\n  <li>The deployment pipeline does not support large intermediate activations due to memory constraints.</li>\n  <li>Model training and deployment budgets are limited, making retraining from scratch feasible but large-model compression less practical.</li>\n</ul>",
      "contentMarkdown": "*   The target hardware has strict latency or power limits that even heavily compressed large models cannot meet.\n*   The deployment pipeline does not support large intermediate activations due to memory constraints.\n*   Model training and deployment budgets are limited, making retraining from scratch feasible but large-model compression less practical.",
      "order": 53,
      "orderInChapter": 4,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "activation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 50,
        "contentLength": 387
      },
      "nextCards": [],
      "relatedCards": [
        "ai-federated-learning-pros-cons-19",
        "ai-federated-learning-comparison-use-cases-20",
        "ai-on-device-transformers-quantization-48-bit-11",
        "ai-federated-learning-mime-9",
        "ai-federated-learning-lora-in-federated-context-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-compression/#when-to-use-lightweight-models",
      "scrapedAt": "2025-12-28T11:55:50.974Z",
      "siblings": [
        "ai-model-compression-background-precision-1",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-definition-3",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-dequantization-considerations-5"
      ]
    },
    {
      "id": "ai-federated-learning-overview-1",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Federated Learning (FL)",
      "title": "Overview",
      "subtitle": "Federated Learning (FL)",
      "contentHtml": "<ul>\n  <li>\n    <p>Federated Learning (FL) is a machine learning paradigm designed for scenarios where data is <strong>distributed across multiple devices or organizations</strong>, and privacy or logistical concerns make it undesirable or illegal to centralize that data. FL enables <strong>collaborative model training</strong> while keeping all raw data <strong>local to each client</strong>, thus mitigating privacy risks and reducing communication overhead compared to transferring large datasets.</p>\n  </li>\n  <li>\n    <p>Originally introduced to enable <strong>on-device learning</strong> (e.g., mobile keyboards, IoT devices), FL is now central to many privacy-preserving and distributed AI strategies, particularly in <strong>healthcare, finance, and mobile personalization</strong>.</p>\n  </li>\n</ul>\n<p>Federated Learning (FL) is a machine learning paradigm designed for scenarios where data is <strong>distributed across multiple devices or organizations</strong>, and privacy or logistical concerns make it undesirable or illegal to centralize that data. FL enables <strong>collaborative model training</strong> while keeping all raw data <strong>local to each client</strong>, thus mitigating privacy risks and reducing communication overhead compared to transferring large datasets.</p>\n<p>Originally introduced to enable <strong>on-device learning</strong> (e.g., mobile keyboards, IoT devices), FL is now central to many privacy-preserving and distributed AI strategies, particularly in <strong>healthcare, finance, and mobile personalization</strong>.</p>\n<h4 id=\"motivations\">Motivations</h4>\n<ul>\n  <li><strong>Data privacy regulations</strong> (GDPR, HIPAA) restrict centralized storage of user data.</li>\n  <li><strong>Bandwidth constraints</strong> and <strong>data sovereignty</strong> make uploading large volumes of data impractical or noncompliant.</li>\n  <li>In many real-world cases, data is <strong>non-i.i.d.</strong> (not identically distributed across clients) and can be heavily skewed, e.g., language dialects across users or hospitals with different patient demographics.</li>\n</ul>\n<h4 id=\"privacy-preserving-ai-via-fl\">Privacy-Preserving AI Via FL</h4>\n<ul>\n  <li>\n    <p>FL plays a foundational role in building <strong>privacy-preserving AI</strong> systems by ensuring:</p>\n\n    <ul>\n      <li><strong>Raw user data never leaves its source</strong>. This drastically reduces the risk of data leakage from centralized storage or transit pipelines.</li>\n      <li><strong>Client updates (e.g., gradients or model weights)</strong> can be encrypted and anonymized, further mitigating the risk of inference attacks.</li>\n      <li>FL can be combined with other privacy-preserving techniques such as <strong>Differential Privacy (DP)</strong> and <strong>Secure Aggregation</strong>, forming a robust stack that provides <strong>both empirical and theoretical privacy guarantees</strong>.</li>\n      <li>Enables learning from <strong>sensitive data</strong> such as electronic health records, legal case documents, or personal conversations without exposing them, allowing AI systems to leverage such data responsibly.</li>\n    </ul>\n  </li>\n  <li>\n    <p>This architecture is especially critical for deploying <strong>responsible AI</strong> in regulated or user-facing environments—ensuring compliance, maintaining user trust, and reducing ethical and legal liabilities.</p>\n  </li>\n</ul>\n<p>FL plays a foundational role in building <strong>privacy-preserving AI</strong> systems by ensuring:</p>\n<ul>\n      <li><strong>Raw user data never leaves its source</strong>. This drastically reduces the risk of data leakage from centralized storage or transit pipelines.</li>\n      <li><strong>Client updates (e.g., gradients or model weights)</strong> can be encrypted and anonymized, further mitigating the risk of inference attacks.</li>\n      <li>FL can be combined with other privacy-preserving techniques such as <strong>Differential Privacy (DP)</strong> and <strong>Secure Aggregation</strong>, forming a robust stack that provides <strong>both empirical and theoretical privacy guarantees</strong>.</li>\n      <li>Enables learning from <strong>sensitive data</strong> such as electronic health records, legal case documents, or personal conversations without exposing them, allowing AI systems to leverage such data responsibly.</li>\n    </ul>\n<p>This architecture is especially critical for deploying <strong>responsible AI</strong> in regulated or user-facing environments—ensuring compliance, maintaining user trust, and reducing ethical and legal liabilities.</p>",
      "contentMarkdown": "*   Federated Learning (FL) is a machine learning paradigm designed for scenarios where data is **distributed across multiple devices or organizations**, and privacy or logistical concerns make it undesirable or illegal to centralize that data. FL enables **collaborative model training** while keeping all raw data **local to each client**, thus mitigating privacy risks and reducing communication overhead compared to transferring large datasets.\n    \n*   Originally introduced to enable **on-device learning** (e.g., mobile keyboards, IoT devices), FL is now central to many privacy-preserving and distributed AI strategies, particularly in **healthcare, finance, and mobile personalization**.\n    \n\nFederated Learning (FL) is a machine learning paradigm designed for scenarios where data is **distributed across multiple devices or organizations**, and privacy or logistical concerns make it undesirable or illegal to centralize that data. FL enables **collaborative model training** while keeping all raw data **local to each client**, thus mitigating privacy risks and reducing communication overhead compared to transferring large datasets.\n\nOriginally introduced to enable **on-device learning** (e.g., mobile keyboards, IoT devices), FL is now central to many privacy-preserving and distributed AI strategies, particularly in **healthcare, finance, and mobile personalization**.\n\n#### Motivations\n\n*   **Data privacy regulations** (GDPR, HIPAA) restrict centralized storage of user data.\n*   **Bandwidth constraints** and **data sovereignty** make uploading large volumes of data impractical or noncompliant.\n*   In many real-world cases, data is **non-i.i.d.** (not identically distributed across clients) and can be heavily skewed, e.g., language dialects across users or hospitals with different patient demographics.\n\n#### Privacy-Preserving AI Via FL\n\n*   FL plays a foundational role in building **privacy-preserving AI** systems by ensuring:\n    \n    *   **Raw user data never leaves its source**. This drastically reduces the risk of data leakage from centralized storage or transit pipelines.\n    *   **Client updates (e.g., gradients or model weights)** can be encrypted and anonymized, further mitigating the risk of inference attacks.\n    *   FL can be combined with other privacy-preserving techniques such as **Differential Privacy (DP)** and **Secure Aggregation**, forming a robust stack that provides **both empirical and theoretical privacy guarantees**.\n    *   Enables learning from **sensitive data** such as electronic health records, legal case documents, or personal conversations without exposing them, allowing AI systems to leverage such data responsibly.\n*   This architecture is especially critical for deploying **responsible AI** in regulated or user-facing environments—ensuring compliance, maintaining user trust, and reducing ethical and legal liabilities.\n    \n\nFL plays a foundational role in building **privacy-preserving AI** systems by ensuring:\n\n*   **Raw user data never leaves its source**. This drastically reduces the risk of data leakage from centralized storage or transit pipelines.\n*   **Client updates (e.g., gradients or model weights)** can be encrypted and anonymized, further mitigating the risk of inference attacks.\n*   FL can be combined with other privacy-preserving techniques such as **Differential Privacy (DP)** and **Secure Aggregation**, forming a robust stack that provides **both empirical and theoretical privacy guarantees**.\n*   Enables learning from **sensitive data** such as electronic health records, legal case documents, or personal conversations without exposing them, allowing AI systems to leverage such data responsibly.\n\nThis architecture is especially critical for deploying **responsible AI** in regulated or user-facing environments—ensuring compliance, maintaining user trust, and reducing ethical and legal liabilities.",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 3,
      "tags": [
        "ondevice ai",
        "machine learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 517,
        "contentLength": 4609
      },
      "nextCards": [
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-differential-privacy-components-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#overview",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5",
        "ai-federated-learning-fedavg-federated-averaging-6"
      ]
    },
    {
      "id": "ai-federated-learning-definition-2",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Federated Learning (FL)",
      "title": "Definition",
      "subtitle": "Federated Learning (FL)",
      "contentHtml": "<ul>\n  <li>\n    <p>At its core, FL aims to train a <strong>global model</strong> collaboratively over a network of <strong>K clients</strong>, each with access to its own private dataset. Instead of aggregating raw data centrally, each client performs <strong>local training</strong> and contributes to a shared global model via <strong>periodic updates</strong>.</p>\n  </li>\n  <li>\n    <p>Let the overall learning objective be:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><munder><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>w</mi></mrow></munder><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>K</mi></mrow></munderover><mfrac><msub><mi>n</mi><mi>k</mi></msub><mi>n</mi></mfrac><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 12.034em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.003em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.263em, 1009.95em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"munderover\" id=\"MathJax-Span-3\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.57em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Regular;\">min</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.47em, 4.273em, -999.997em); top: -3.331em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-5\"><span class=\"mrow\" id=\"MathJax-Span-6\"><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-8\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-9\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-10\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-11\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-13\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-14\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.1em, 4.273em, -999.997em); top: -2.862em; left: 0.055em;\"><span class=\"texatom\" id=\"MathJax-Span-15\"><span class=\"mrow\" id=\"MathJax-Span-16\"><span class=\"mi\" id=\"MathJax-Span-17\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-18\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-19\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.419em;\"><span class=\"texatom\" id=\"MathJax-Span-20\"><span class=\"mrow\" id=\"MathJax-Span-21\"><span class=\"mi\" id=\"MathJax-Span-22\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mfrac\" id=\"MathJax-Span-23\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.89em, 4.326em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.466em;\"><span class=\"msubsup\" id=\"MathJax-Span-24\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-25\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1001.04em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.044em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-28\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-30\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-31\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-32\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><munder><mo movablelimits=\"true\" form=\"prefix\">min</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>w</mi></mrow></munder><mi>f</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>K</mi></mrow></munderover><mfrac><msub><mi>n</mi><mi>k</mi></msub><mi>n</mi></mfrac><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-1\">\\min_{w} f(w) = \\sum_{k=1}^{K} \\frac{n_k}{n} F_k(w)</script>\n\n    <ul>\n      <li>\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mi>d</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-34\" style=\"width: 3.753em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.128em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.13em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-35\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-38\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-39\"><span class=\"mrow\" id=\"MathJax-Span-40\"><span class=\"mi\" id=\"MathJax-Span-41\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-42\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mi>d</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">w \\in \\mathbb{R}^d</script>: global model parameters,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-43\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-44\"><span class=\"msubsup\" id=\"MathJax-Span-45\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-47\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-50\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">F_k(w)</script>: the local empirical risk (loss function) on client <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-51\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-52\"><span class=\"mi\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">k</script>,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>n</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-54\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.89em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-55\"><span class=\"msubsup\" id=\"MathJax-Span-56\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-58\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>n</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">n_k</script>: the number of data points on client <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-59\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-60\"><span class=\"mi\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">k</script>,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi><mo>=</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>K</mi></mrow></munderover><msub><mi>n</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-62\" style=\"width: 5.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.951em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1004.95em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-63\"><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-65\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-66\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.58em, 4.169em, -999.997em); top: -4.477em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-68\"><span class=\"mrow\" id=\"MathJax-Span-69\"><span class=\"mi\" id=\"MathJax-Span-70\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.25em, 4.169em, -999.997em); top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-71\"><span class=\"mrow\" id=\"MathJax-Span-72\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-74\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-75\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-76\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-78\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi><mo>=</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>K</mi></mrow></munderover><msub><mi>n</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">n = \\sum_{k=1}^{K} n_k</script>: total number of training samples across all clients.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>Each client updates its local version of the model <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-79\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1001.41em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-80\"><span class=\"msubsup\" id=\"MathJax-Span-81\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.78em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-83\"><span class=\"mrow\" id=\"MathJax-Span-84\"><span class=\"mo\" id=\"MathJax-Span-85\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-86\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-87\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-88\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">w_k^{(t)}</script> using its private dataset <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>D</mi></mrow><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-89\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-90\"><span class=\"msubsup\" id=\"MathJax-Span-91\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-92\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">D</mi></mrow><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">\\mathcal{D}_k</script>, computes the local update (via SGD or other optimizers), and sends either:</p>\n\n    <ul>\n      <li><strong>Model weights</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-96\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1002.24em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-97\"><span class=\"msubsup\" id=\"MathJax-Span-98\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-100\"><span class=\"mrow\" id=\"MathJax-Span-101\"><span class=\"mo\" id=\"MathJax-Span-102\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-105\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-107\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">w_k^{(t+1)}</script>, or</li>\n      <li>\n        <p><strong>Weight deltas</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><msub><mi>w</mi><mi>k</mi></msub><mo>=</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo>&amp;#x2212;</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-108\" style=\"width: 9.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.763em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1007.76em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-109\"><span class=\"mi\" id=\"MathJax-Span-110\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"msubsup\" id=\"MathJax-Span-111\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-113\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-114\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-115\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-117\"><span class=\"mrow\" id=\"MathJax-Span-118\"><span class=\"mo\" id=\"MathJax-Span-119\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-121\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-122\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-123\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-125\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-126\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-127\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-128\"><span class=\"mrow\" id=\"MathJax-Span-129\"><span class=\"mo\" id=\"MathJax-Span-130\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-132\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Δ</mi><msub><mi>w</mi><mi>k</mi></msub><mo>=</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>−</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">\\Delta w_k = w_k^{(t+1)} - w^{(t)}</script></p>\n      </li>\n      <li>… back to a <strong>central server</strong>.</li>\n    </ul>\n  </li>\n  <li>\n    <p>The server then aggregates all received updates using a weighted average:</p>\n  </li>\n</ul>\n<p>At its core, FL aims to train a <strong>global model</strong> collaboratively over a network of <strong>K clients</strong>, each with access to its own private dataset. Instead of aggregating raw data centrally, each client performs <strong>local training</strong> and contributes to a shared global model via <strong>periodic updates</strong>.</p>\n<p>Let the overall learning objective be:</p>\n<ul>\n      <li>\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mi>d</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-34\" style=\"width: 3.753em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.128em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.13em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-35\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-38\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-39\"><span class=\"mrow\" id=\"MathJax-Span-40\"><span class=\"mi\" id=\"MathJax-Span-41\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-42\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mi>d</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">w \\in \\mathbb{R}^d</script>: global model parameters,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-43\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-44\"><span class=\"msubsup\" id=\"MathJax-Span-45\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-47\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-50\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">F_k(w)</script>: the local empirical risk (loss function) on client <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-51\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-52\"><span class=\"mi\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">k</script>,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>n</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-54\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.89em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-55\"><span class=\"msubsup\" id=\"MathJax-Span-56\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-58\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>n</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">n_k</script>: the number of data points on client <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-59\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-60\"><span class=\"mi\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">k</script>,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi><mo>=</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>K</mi></mrow></munderover><msub><mi>n</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-62\" style=\"width: 5.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.951em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1004.95em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-63\"><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-65\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-66\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.58em, 4.169em, -999.997em); top: -4.477em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-68\"><span class=\"mrow\" id=\"MathJax-Span-69\"><span class=\"mi\" id=\"MathJax-Span-70\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.25em, 4.169em, -999.997em); top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-71\"><span class=\"mrow\" id=\"MathJax-Span-72\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-74\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-75\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-76\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-78\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi><mo>=</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>K</mi></mrow></munderover><msub><mi>n</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">n = \\sum_{k=1}^{K} n_k</script>: total number of training samples across all clients.</li>\n        </ul>\n      </li>\n    </ul>\n<p>where:</p>\n<ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mi>d</mi></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-34\" style=\"width: 3.753em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.128em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.13em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-35\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-38\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-39\"><span class=\"mrow\" id=\"MathJax-Span-40\"><span class=\"mi\" id=\"MathJax-Span-41\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-42\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mi>d</mi></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">w \\in \\mathbb{R}^d</script>: global model parameters,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-43\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-44\"><span class=\"msubsup\" id=\"MathJax-Span-45\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-47\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-50\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">F_k(w)</script>: the local empirical risk (loss function) on client <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-51\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-52\"><span class=\"mi\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">k</script>,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>n</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-54\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.89em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-55\"><span class=\"msubsup\" id=\"MathJax-Span-56\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-58\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>n</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">n_k</script>: the number of data points on client <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-59\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-60\"><span class=\"mi\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">k</script>,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi><mo>=</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>K</mi></mrow></munderover><msub><mi>n</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-62\" style=\"width: 5.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.951em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1004.95em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-63\"><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-65\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-66\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.58em, 4.169em, -999.997em); top: -4.477em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-68\"><span class=\"mrow\" id=\"MathJax-Span-69\"><span class=\"mi\" id=\"MathJax-Span-70\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.25em, 4.169em, -999.997em); top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-71\"><span class=\"mrow\" id=\"MathJax-Span-72\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-74\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-75\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-76\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-78\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi><mo>=</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>K</mi></mrow></munderover><msub><mi>n</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">n = \\sum_{k=1}^{K} n_k</script>: total number of training samples across all clients.</li>\n        </ul>\n<p>Each client updates its local version of the model <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-79\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1001.41em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-80\"><span class=\"msubsup\" id=\"MathJax-Span-81\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.78em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-83\"><span class=\"mrow\" id=\"MathJax-Span-84\"><span class=\"mo\" id=\"MathJax-Span-85\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-86\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-87\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-88\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">w_k^{(t)}</script> using its private dataset <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>D</mi></mrow><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-89\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-90\"><span class=\"msubsup\" id=\"MathJax-Span-91\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-92\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">D</mi></mrow><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">\\mathcal{D}_k</script>, computes the local update (via SGD or other optimizers), and sends either:</p>\n<ul>\n      <li><strong>Model weights</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-96\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1002.24em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-97\"><span class=\"msubsup\" id=\"MathJax-Span-98\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-100\"><span class=\"mrow\" id=\"MathJax-Span-101\"><span class=\"mo\" id=\"MathJax-Span-102\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-105\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-107\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">w_k^{(t+1)}</script>, or</li>\n      <li>\n        <p><strong>Weight deltas</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><msub><mi>w</mi><mi>k</mi></msub><mo>=</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo>&amp;#x2212;</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-108\" style=\"width: 9.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.763em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1007.76em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-109\"><span class=\"mi\" id=\"MathJax-Span-110\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"msubsup\" id=\"MathJax-Span-111\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-113\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-114\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-115\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-117\"><span class=\"mrow\" id=\"MathJax-Span-118\"><span class=\"mo\" id=\"MathJax-Span-119\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-121\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-122\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-123\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-125\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-126\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-127\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-128\"><span class=\"mrow\" id=\"MathJax-Span-129\"><span class=\"mo\" id=\"MathJax-Span-130\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-132\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Δ</mi><msub><mi>w</mi><mi>k</mi></msub><mo>=</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>−</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">\\Delta w_k = w_k^{(t+1)} - w^{(t)}</script></p>\n      </li>\n      <li>… back to a <strong>central server</strong>.</li>\n    </ul>\n<p><strong>Weight deltas</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><msub><mi>w</mi><mi>k</mi></msub><mo>=</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo>&amp;#x2212;</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-108\" style=\"width: 9.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.763em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1007.76em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-109\"><span class=\"mi\" id=\"MathJax-Span-110\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"msubsup\" id=\"MathJax-Span-111\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-113\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-114\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-115\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-117\"><span class=\"mrow\" id=\"MathJax-Span-118\"><span class=\"mo\" id=\"MathJax-Span-119\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-121\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-122\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-123\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-125\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-126\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-127\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-128\"><span class=\"mrow\" id=\"MathJax-Span-129\"><span class=\"mo\" id=\"MathJax-Span-130\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-132\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">Δ</mi><msub><mi>w</mi><mi>k</mi></msub><mo>=</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>−</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">\\Delta w_k = w_k^{(t+1)} - w^{(t)}</script></p>\n<p>The server then aggregates all received updates using a weighted average:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo>=</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>K</mi></mrow></munderover><mfrac><msub><mi>n</mi><mi>k</mi></msub><mi>n</mi></mfrac><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-133\" style=\"width: 10.159em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.44em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.263em, 1008.44em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-134\"><span class=\"msubsup\" id=\"MathJax-Span-135\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-136\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-137\"><span class=\"mrow\" id=\"MathJax-Span-138\"><span class=\"mo\" id=\"MathJax-Span-139\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-140\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-141\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-142\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-143\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-145\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-146\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.1em, 4.273em, -999.997em); top: -2.862em; left: 0.055em;\"><span class=\"texatom\" id=\"MathJax-Span-147\"><span class=\"mrow\" id=\"MathJax-Span-148\"><span class=\"mi\" id=\"MathJax-Span-149\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-150\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-151\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.419em;\"><span class=\"texatom\" id=\"MathJax-Span-152\"><span class=\"mrow\" id=\"MathJax-Span-153\"><span class=\"mi\" id=\"MathJax-Span-154\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mfrac\" id=\"MathJax-Span-155\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.89em, 4.326em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.466em;\"><span class=\"msubsup\" id=\"MathJax-Span-156\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-157\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-158\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-159\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1001.04em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.044em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-160\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-161\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-162\"><span class=\"mrow\" id=\"MathJax-Span-163\"><span class=\"mo\" id=\"MathJax-Span-164\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-165\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-166\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-167\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-168\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-169\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>K</mi></mrow></munderover><mfrac><msub><mi>n</mi><mi>k</mi></msub><mi>n</mi></mfrac><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></math></span></span></div>\n<ul>\n  <li>This process, known as <strong>Federated Averaging (FedAvg)</strong>, repeats for multiple rounds until convergence.</li>\n</ul>\n<h4 id=\"key-concepts\">Key Concepts:</h4>\n<ul>\n  <li><strong>Local Training</strong>: Each client performs one or more gradient descent steps on its local dataset.</li>\n  <li><strong>Periodic Aggregation</strong>: Updates are sent to the server only periodically to reduce communication overhead.</li>\n  <li><strong>Weighted Contributions</strong>: Clients with more data contribute proportionally more to the global model.</li>\n</ul>\n<h4 id=\"comparison-to-centralized-and-distributed-learning\">Comparison to Centralized and Distributed Learning:</h4>\n<ul>\n  <li>Unlike traditional <strong>centralized learning</strong>, FL does not transfer any raw data to the server.</li>\n  <li>\n    <p>Unlike <strong>classical distributed training</strong>, FL assumes that client data may be <strong>non-i.i.d.</strong>, <strong>unbalanced</strong>, and held by <strong>unreliable or resource-constrained devices</strong> (e.g., smartphones, hospitals).</p>\n  </li>\n  <li>This definition forms the mathematical backbone of the FL paradigm. In practice, several enhancements—like asynchronous communication, secure aggregation, and personalization—are layered on top to make FL robust and scalable in real-world systems.</li>\n</ul>\n<p>Unlike <strong>classical distributed training</strong>, FL assumes that client data may be <strong>non-i.i.d.</strong>, <strong>unbalanced</strong>, and held by <strong>unreliable or resource-constrained devices</strong> (e.g., smartphones, hospitals).</p>",
      "contentMarkdown": "*   At its core, FL aims to train a **global model** collaboratively over a network of **K clients**, each with access to its own private dataset. Instead of aggregating raw data centrally, each client performs **local training** and contributes to a shared global model via **periodic updates**.\n    \n*   Let the overall learning objective be:\n    \n    minwf(w)\\=∑k\\=1KnknFk(w)minwf(w)\\=∑k\\=1KnknFk(w)\n    \n    \\\\min\\_{w} f(w) = \\\\sum\\_{k=1}^{K} \\\\frac{n\\_k}{n} F\\_k(w)\n    *   where:\n        \n        *   w∈ℝdw∈Rdw \\\\in \\\\mathbb{R}^d: global model parameters,\n        *   Fk(w)Fk(w)F\\_k(w): the local empirical risk (loss function) on client kkk,\n        *   nknkn\\_k: the number of data points on client kkk,\n        *   n\\=∑Kk\\=1nkn\\=∑k\\=1Knkn = \\\\sum\\_{k=1}^{K} n\\_k: total number of training samples across all clients.\n*   Each client updates its local version of the model w(t)kwk(t)w\\_k^{(t)} using its private dataset kDk\\\\mathcal{D}\\_k, computes the local update (via SGD or other optimizers), and sends either:\n    \n    *   **Model weights** w(t+1)kwk(t+1)w\\_k^{(t+1)}, or\n    *   **Weight deltas** Δwk\\=w(t+1)k−w(t)Δwk\\=wk(t+1)−w(t)\\\\Delta w\\_k = w\\_k^{(t+1)} - w^{(t)}\n        \n    *   … back to a **central server**.\n*   The server then aggregates all received updates using a weighted average:\n    \n\nAt its core, FL aims to train a **global model** collaboratively over a network of **K clients**, each with access to its own private dataset. Instead of aggregating raw data centrally, each client performs **local training** and contributes to a shared global model via **periodic updates**.\n\nLet the overall learning objective be:\n\n*   where:\n    \n    *   w∈ℝdw∈Rdw \\\\in \\\\mathbb{R}^d: global model parameters,\n    *   Fk(w)Fk(w)F\\_k(w): the local empirical risk (loss function) on client kkk,\n    *   nknkn\\_k: the number of data points on client kkk,\n    *   n\\=∑Kk\\=1nkn\\=∑k\\=1Knkn = \\\\sum\\_{k=1}^{K} n\\_k: total number of training samples across all clients.\n\nwhere:\n\n*   w∈ℝdw∈Rdw \\\\in \\\\mathbb{R}^d: global model parameters,\n*   Fk(w)Fk(w)F\\_k(w): the local empirical risk (loss function) on client kkk,\n*   nknkn\\_k: the number of data points on client kkk,\n*   n\\=∑Kk\\=1nkn\\=∑k\\=1Knkn = \\\\sum\\_{k=1}^{K} n\\_k: total number of training samples across all clients.\n\nEach client updates its local version of the model w(t)kwk(t)w\\_k^{(t)} using its private dataset kDk\\\\mathcal{D}\\_k, computes the local update (via SGD or other optimizers), and sends either:\n\n*   **Model weights** w(t+1)kwk(t+1)w\\_k^{(t+1)}, or\n*   **Weight deltas** Δwk\\=w(t+1)k−w(t)Δwk\\=wk(t+1)−w(t)\\\\Delta w\\_k = w\\_k^{(t+1)} - w^{(t)}\n    \n*   … back to a **central server**.\n\n**Weight deltas** Δwk\\=w(t+1)k−w(t)Δwk\\=wk(t+1)−w(t)\\\\Delta w\\_k = w\\_k^{(t+1)} - w^{(t)}\n\nThe server then aggregates all received updates using a weighted average:\n\nw(t+1)\\=∑k\\=1Knknw(t+1)kw(t+1)\\=∑k\\=1Knknwk(t+1)\n\n*   This process, known as **Federated Averaging (FedAvg)**, repeats for multiple rounds until convergence.\n\n#### Key Concepts:\n\n*   **Local Training**: Each client performs one or more gradient descent steps on its local dataset.\n*   **Periodic Aggregation**: Updates are sent to the server only periodically to reduce communication overhead.\n*   **Weighted Contributions**: Clients with more data contribute proportionally more to the global model.\n\n#### Comparison to Centralized and Distributed Learning:\n\n*   Unlike traditional **centralized learning**, FL does not transfer any raw data to the server.\n*   Unlike **classical distributed training**, FL assumes that client data may be **non-i.i.d.**, **unbalanced**, and held by **unreliable or resource-constrained devices** (e.g., smartphones, hospitals).\n    \n*   This definition forms the mathematical backbone of the FL paradigm. In practice, several enhancements—like asynchronous communication, secure aggregation, and personalization—are layered on top to make FL robust and scalable in real-world systems.\n\nUnlike **classical distributed training**, FL assumes that client data may be **non-i.i.d.**, **unbalanced**, and held by **unreliable or resource-constrained devices** (e.g., smartphones, hospitals).",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 3,
      "tags": [
        "ondevice ai",
        "gradient descent",
        "loss function"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 534,
        "contentLength": 96230
      },
      "nextCards": [
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4"
      ],
      "relatedCards": [
        "ai-differential-privacy-dpsgd-core-idea-9",
        "ai-model-compression-mechanism-22",
        "ai-model-compression-formal-definition-32",
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#definition",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5",
        "ai-federated-learning-fedavg-federated-averaging-6"
      ]
    },
    {
      "id": "ai-federated-learning-cross-device-federated-learning-3",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Types of Federated Learning",
      "title": "Cross-Device Federated Learning",
      "subtitle": "Types of Federated Learning",
      "contentHtml": "<ul>\n  <li>Involves a massive number of edge devices—e.g., smartphones, wearables, or IoT sensors—that intermittently participate in training.</li>\n  <li>\n    <p>Typical characteristics:</p>\n\n    <ul>\n      <li>Millions of clients.</li>\n      <li>Sparse participation (often &lt;10% active per round).</li>\n      <li>Highly diverse hardware, data distributions, and power/network availability.</li>\n    </ul>\n  </li>\n  <li>Ideal for on-device personalization, mobile input prediction, and private assistant models (e.g., Google’s Gboard).</li>\n</ul>\n<p>Typical characteristics:</p>\n<ul>\n      <li>Millions of clients.</li>\n      <li>Sparse participation (often &lt;10% active per round).</li>\n      <li>Highly diverse hardware, data distributions, and power/network availability.</li>\n    </ul>",
      "contentMarkdown": "*   Involves a massive number of edge devices—e.g., smartphones, wearables, or IoT sensors—that intermittently participate in training.\n*   Typical characteristics:\n    \n    *   Millions of clients.\n    *   Sparse participation (often <10% active per round).\n    *   Highly diverse hardware, data distributions, and power/network availability.\n*   Ideal for on-device personalization, mobile input prediction, and private assistant models (e.g., Google’s Gboard).\n\nTypical characteristics:\n\n*   Millions of clients.\n*   Sparse participation (often <10% active per round).\n*   Highly diverse hardware, data distributions, and power/network availability.",
      "order": 3,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 79,
        "contentLength": 794
      },
      "nextCards": [
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-differential-privacy-components-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#cross-device-federated-learning",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5",
        "ai-federated-learning-fedavg-federated-averaging-6"
      ]
    },
    {
      "id": "ai-federated-learning-cross-silo-federated-learning-4",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Types of Federated Learning",
      "title": "Cross-Silo Federated Learning",
      "subtitle": "Types of Federated Learning",
      "contentHtml": "<ul>\n  <li>Involves a smaller number (typically 2–100) of reliable institutions or data silos such as hospitals, banks, or data centers.</li>\n  <li>\n    <p>Typical characteristics:</p>\n\n    <ul>\n      <li>High bandwidth and computational capacity.</li>\n      <li>Stable, long-lived training participation.</li>\n      <li>Institutional data often exhibits statistical heterogeneity.</li>\n    </ul>\n  </li>\n  <li>Use cases include medical AI across hospitals or fraud detection across banks.</li>\n</ul>\n<p>Typical characteristics:</p>\n<ul>\n      <li>High bandwidth and computational capacity.</li>\n      <li>Stable, long-lived training participation.</li>\n      <li>Institutional data often exhibits statistical heterogeneity.</li>\n    </ul>",
      "contentMarkdown": "*   Involves a smaller number (typically 2–100) of reliable institutions or data silos such as hospitals, banks, or data centers.\n*   Typical characteristics:\n    \n    *   High bandwidth and computational capacity.\n    *   Stable, long-lived training participation.\n    *   Institutional data often exhibits statistical heterogeneity.\n*   Use cases include medical AI across hospitals or fraud detection across banks.\n\nTypical characteristics:\n\n*   High bandwidth and computational capacity.\n*   Stable, long-lived training participation.\n*   Institutional data often exhibits statistical heterogeneity.",
      "order": 4,
      "orderInChapter": 2,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 74,
        "contentLength": 739
      },
      "nextCards": [
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5",
        "ai-federated-learning-fedavg-federated-averaging-6"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-differential-privacy-components-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#cross-silo-federated-learning",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5",
        "ai-federated-learning-fedavg-federated-averaging-6"
      ]
    },
    {
      "id": "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Federation Algorithms",
      "title": "Federated Stochastic Gradient Descent (FedSGD)",
      "subtitle": "Federation Algorithms",
      "contentHtml": "<ul>\n  <li>\n    <p>FedSGD is the most basic form of federated optimization, adapting traditional stochastic gradient descent to a distributed, privacy-sensitive environment. It was first formalized in McMahan et al.’s seminal work on federated learning, <em>Communication-Efficient Learning of Deep Networks from Decentralized Data</em> (<a href=\"https://arxiv.org/abs/1602.05629\">McMahan et al., 2017</a>).</p>\n  </li>\n  <li>\n    <p>In FedSGD, at each communication round, a random subset of clients is selected. Each participating client computes the gradient of the current global model on its full local dataset and transmits this gradient to the central server. Unlike centralized SGD where the gradient is computed over a mini-batch, FedSGD effectively computes local gradients on the entire dataset of each selected client:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msubsup><mi>g</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo>=</mo><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mfrac><mn>1</mn><msub><mi>n</mi><mi>k</mi></msub></mfrac><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>n</mi><mi>k</mi></msub></mrow></munderover><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><mi>&amp;#x2113;</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo>;</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-211\" style=\"width: 18.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 15.159em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.471em, 1015.11em, 3.648em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-212\"><span class=\"msubsup\" id=\"MathJax-Span-213\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-214\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.78em, 4.273em, -999.997em); top: -4.477em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-215\"><span class=\"mrow\" id=\"MathJax-Span-216\"><span class=\"mo\" id=\"MathJax-Span-217\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-218\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-219\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-220\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-221\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-222\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-223\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-224\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-225\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-226\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-227\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-228\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-229\"><span class=\"mrow\" id=\"MathJax-Span-230\"><span class=\"mo\" id=\"MathJax-Span-231\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-232\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-233\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-234\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-235\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-236\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-237\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.89em, 4.326em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.466em;\"><span class=\"msubsup\" id=\"MathJax-Span-238\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-239\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-240\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1001.04em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.044em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-241\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-242\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-243\"><span class=\"mrow\" id=\"MathJax-Span-244\"><span class=\"mi\" id=\"MathJax-Span-245\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-246\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-247\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.273em, -999.997em); top: -5.206em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-248\"><span class=\"mrow\" id=\"MathJax-Span-249\"><span class=\"msubsup\" id=\"MathJax-Span-250\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-251\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-252\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-253\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">∇</span><span class=\"mi\" id=\"MathJax-Span-254\" style=\"font-family: STIXGeneral-Italic;\">ℓ</span><span class=\"mo\" id=\"MathJax-Span-255\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-256\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-257\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-258\"><span class=\"mrow\" id=\"MathJax-Span-259\"><span class=\"mo\" id=\"MathJax-Span-260\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-261\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-262\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-263\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"msubsup\" id=\"MathJax-Span-264\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-265\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-266\"><span class=\"mrow\" id=\"MathJax-Span-267\"><span class=\"mi\" id=\"MathJax-Span-268\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-269\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-270\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-271\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msubsup><mi>g</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><msub><mi>n</mi><mi>k</mi></msub></mfrac><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>n</mi><mi>k</mi></msub></mrow></munderover><mi mathvariant=\"normal\">∇</mi><mi>ℓ</mi><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>;</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-22\">g_k^{(t)} = \\nabla F_k(w^{(t)}) = \\frac{1}{n_k} \\sum_{i=1}^{n_k} \\nabla \\ell(w^{(t)}; x_{i,k})</script>\n  </li>\n  <li>\n    <p>The server then performs a weighted aggregation of the gradients, proportional to the number of local samples <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>n</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-272\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.89em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-273\"><span class=\"msubsup\" id=\"MathJax-Span-274\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-275\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-276\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>n</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">n_k</script>, to form the global gradient:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>&amp;#x2208;</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><mfrac><msub><mi>n</mi><mi>k</mi></msub><msub><mi>n</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>S</mi><mi>t</mi></msub></mrow></msub></mfrac><msubsup><mi>g</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-277\" style=\"width: 8.076em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.721em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.94em, 1006.72em, 3.701em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-278\"><span class=\"msubsup\" id=\"MathJax-Span-279\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-280\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-281\"><span class=\"mrow\" id=\"MathJax-Span-282\"><span class=\"mo\" id=\"MathJax-Span-283\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-284\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-285\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-286\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-287\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-288\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.36em, 4.378em, -999.997em); top: -2.862em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-289\"><span class=\"mrow\" id=\"MathJax-Span-290\"><span class=\"mi\" id=\"MathJax-Span-291\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-292\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-293\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-294\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-295\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mfrac\" id=\"MathJax-Span-296\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.89em, 4.326em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.466em;\"><span class=\"msubsup\" id=\"MathJax-Span-297\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-298\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-299\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1001.15em, 4.43em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.57em;\"><span class=\"msubsup\" id=\"MathJax-Span-300\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-301\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-302\"><span class=\"mrow\" id=\"MathJax-Span-303\"><span class=\"msubsup\" id=\"MathJax-Span-304\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-305\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-306\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1001.25em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.253em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-307\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-308\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.78em, 4.273em, -999.997em); top: -4.477em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-309\"><span class=\"mrow\" id=\"MathJax-Span-310\"><span class=\"mo\" id=\"MathJax-Span-311\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-312\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-313\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-314\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.684em; border-left: 0px solid; width: 0px; height: 3.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>g</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>∈</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><mfrac><msub><mi>n</mi><mi>k</mi></msub><msub><mi>n</mi><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>S</mi><mi>t</mi></msub></mrow></msub></mfrac><msubsup><mi>g</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-24\">g^{(t)} = \\sum_{k \\in S_t} \\frac{n_k}{n_{S_t}} g_k^{(t)}</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>S</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-315\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.78em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-316\"><span class=\"msubsup\" id=\"MathJax-Span-317\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-318\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-319\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>S</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">S_t</script> is the subset of clients participating at round <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-320\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-321\"><span class=\"mi\" id=\"MathJax-Span-322\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">t</script>, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>n</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>S</mi><mi>t</mi></msub></mrow></msub><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>&amp;#x2208;</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><msub><mi>n</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-323\" style=\"width: 6.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1005.78em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-324\"><span class=\"msubsup\" id=\"MathJax-Span-325\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-326\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-327\"><span class=\"mrow\" id=\"MathJax-Span-328\"><span class=\"msubsup\" id=\"MathJax-Span-329\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-330\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-331\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-332\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-333\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-334\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-335\"><span class=\"mrow\" id=\"MathJax-Span-336\"><span class=\"mi\" id=\"MathJax-Span-337\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-338\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-339\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-340\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-341\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-342\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-343\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-344\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>n</mi><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>S</mi><mi>t</mi></msub></mrow></msub><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>∈</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><msub><mi>n</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">n_{S_t} = \\sum_{k \\in S_t} n_k</script>.</li>\n    </ul>\n  </li>\n</ul>\n<p>FedSGD is the most basic form of federated optimization, adapting traditional stochastic gradient descent to a distributed, privacy-sensitive environment. It was first formalized in McMahan et al.’s seminal work on federated learning, <em>Communication-Efficient Learning of Deep Networks from Decentralized Data</em> (<a href=\"https://arxiv.org/abs/1602.05629\">McMahan et al., 2017</a>).</p>\n<p>In FedSGD, at each communication round, a random subset of clients is selected. Each participating client computes the gradient of the current global model on its full local dataset and transmits this gradient to the central server. Unlike centralized SGD where the gradient is computed over a mini-batch, FedSGD effectively computes local gradients on the entire dataset of each selected client:</p>\n<p>The server then performs a weighted aggregation of the gradients, proportional to the number of local samples <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>n</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-272\" style=\"width: 1.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.89em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-273\"><span class=\"msubsup\" id=\"MathJax-Span-274\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-275\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-276\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>n</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">n_k</script>, to form the global gradient:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>S</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-315\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.78em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-316\"><span class=\"msubsup\" id=\"MathJax-Span-317\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-318\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-319\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>S</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">S_t</script> is the subset of clients participating at round <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-320\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-321\"><span class=\"mi\" id=\"MathJax-Span-322\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">t</script>, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>n</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>S</mi><mi>t</mi></msub></mrow></msub><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>&amp;#x2208;</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><msub><mi>n</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-323\" style=\"width: 6.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1005.78em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-324\"><span class=\"msubsup\" id=\"MathJax-Span-325\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-326\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-327\"><span class=\"mrow\" id=\"MathJax-Span-328\"><span class=\"msubsup\" id=\"MathJax-Span-329\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-330\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-331\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-332\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-333\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-334\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-335\"><span class=\"mrow\" id=\"MathJax-Span-336\"><span class=\"mi\" id=\"MathJax-Span-337\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-338\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-339\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-340\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-341\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-342\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-343\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-344\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>n</mi><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>S</mi><mi>t</mi></msub></mrow></msub><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>∈</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><msub><mi>n</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">n_{S_t} = \\sum_{k \\in S_t} n_k</script>.</li>\n    </ul>\n<blockquote>\n  <p>The gradients are thus averaged by the server proportionally to the number of training samples on each node, and used to make a gradient descent step.</p>\n</blockquote>\n<p>The gradients are thus averaged by the server proportionally to the number of training samples on each node, and used to make a gradient descent step.</p>\n<ul>\n  <li>\n    <p>The global model is then updated via:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo>&amp;#x2212;</mo><mi>&amp;#x03B7;</mi><mo>&amp;#x22C5;</mo><msup><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-345\" style=\"width: 10.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1008.7em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-346\"><span class=\"msubsup\" id=\"MathJax-Span-347\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-348\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-349\"><span class=\"mrow\" id=\"MathJax-Span-350\"><span class=\"mo\" id=\"MathJax-Span-351\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-352\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-353\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-354\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-355\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-356\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-357\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-358\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-359\"><span class=\"mrow\" id=\"MathJax-Span-360\"><span class=\"mo\" id=\"MathJax-Span-361\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-362\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-363\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-364\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-365\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">η</span><span class=\"mo\" id=\"MathJax-Span-366\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-367\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-368\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-369\"><span class=\"mrow\" id=\"MathJax-Span-370\"><span class=\"mo\" id=\"MathJax-Span-371\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-372\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-373\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>−</mo><mi>η</mi><mo>⋅</mo><msup><mi>g</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-28\">w^{(t+1)} = w^{(t)} - \\eta \\cdot g^{(t)}</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-374\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-375\"><span class=\"mi\" id=\"MathJax-Span-376\" style=\"font-family: STIXGeneral-Italic;\">η</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">\\eta</script> is the learning rate.</li>\n    </ul>\n  </li>\n  <li>\n    <p>This approach is simple and communication-efficient when clients are assumed to have high computational power and small local datasets. However, it performs only one local update per round and hence can converge slowly in practice, especially when client participation is sparse or datasets are large.</p>\n  </li>\n  <li>\n    <p>The original FedSGD strategy serves as a theoretical foundation and limiting case of the more practical <strong>FedAvg</strong> algorithm, which allows multiple local SGD steps per round to improve convergence without increasing communication frequency.</p>\n  </li>\n</ul>\n<p>The global model is then updated via:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-374\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-375\"><span class=\"mi\" id=\"MathJax-Span-376\" style=\"font-family: STIXGeneral-Italic;\">η</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">\\eta</script> is the learning rate.</li>\n    </ul>\n<p>This approach is simple and communication-efficient when clients are assumed to have high computational power and small local datasets. However, it performs only one local update per round and hence can converge slowly in practice, especially when client participation is sparse or datasets are large.</p>\n<p>The original FedSGD strategy serves as a theoretical foundation and limiting case of the more practical <strong>FedAvg</strong> algorithm, which allows multiple local SGD steps per round to improve convergence without increasing communication frequency.</p>",
      "contentMarkdown": "*   FedSGD is the most basic form of federated optimization, adapting traditional stochastic gradient descent to a distributed, privacy-sensitive environment. It was first formalized in McMahan et al.’s seminal work on federated learning, _Communication-Efficient Learning of Deep Networks from Decentralized Data_ ([McMahan et al., 2017](https://arxiv.org/abs/1602.05629)).\n    \n*   In FedSGD, at each communication round, a random subset of clients is selected. Each participating client computes the gradient of the current global model on its full local dataset and transmits this gradient to the central server. Unlike centralized SGD where the gradient is computed over a mini-batch, FedSGD effectively computes local gradients on the entire dataset of each selected client:\n    \n    g(t)k\\=∇Fk(w(t))\\=1nk∑i\\=1nk∇ℓ(w(t);xi,k)gk(t)\\=∇Fk(w(t))\\=1nk∑i\\=1nk∇ℓ(w(t);xi,k)\n    \n    g\\_k^{(t)} = \\\\nabla F\\_k(w^{(t)}) = \\\\frac{1}{n\\_k} \\\\sum\\_{i=1}^{n\\_k} \\\\nabla \\\\ell(w^{(t)}; x\\_{i,k})\n*   The server then performs a weighted aggregation of the gradients, proportional to the number of local samples nknkn\\_k, to form the global gradient:\n    \n    g(t)\\=∑k∈StnknStg(t)kg(t)\\=∑k∈StnknStgk(t)\n    \n    g^{(t)} = \\\\sum\\_{k \\\\in S\\_t} \\\\frac{n\\_k}{n\\_{S\\_t}} g\\_k^{(t)}\n    *   where StStS\\_t is the subset of clients participating at round ttt, and nSt\\=∑k∈StnknSt\\=∑k∈Stnkn\\_{S\\_t} = \\\\sum\\_{k \\\\in S\\_t} n\\_k.\n\nFedSGD is the most basic form of federated optimization, adapting traditional stochastic gradient descent to a distributed, privacy-sensitive environment. It was first formalized in McMahan et al.’s seminal work on federated learning, _Communication-Efficient Learning of Deep Networks from Decentralized Data_ ([McMahan et al., 2017](https://arxiv.org/abs/1602.05629)).\n\nIn FedSGD, at each communication round, a random subset of clients is selected. Each participating client computes the gradient of the current global model on its full local dataset and transmits this gradient to the central server. Unlike centralized SGD where the gradient is computed over a mini-batch, FedSGD effectively computes local gradients on the entire dataset of each selected client:\n\nThe server then performs a weighted aggregation of the gradients, proportional to the number of local samples nknkn\\_k, to form the global gradient:\n\n*   where StStS\\_t is the subset of clients participating at round ttt, and nSt\\=∑k∈StnknSt\\=∑k∈Stnkn\\_{S\\_t} = \\\\sum\\_{k \\\\in S\\_t} n\\_k.\n\n> The gradients are thus averaged by the server proportionally to the number of training samples on each node, and used to make a gradient descent step.\n\nThe gradients are thus averaged by the server proportionally to the number of training samples on each node, and used to make a gradient descent step.\n\n*   The global model is then updated via:\n    \n    w(t+1)\\=w(t)−η⋅g(t)w(t+1)\\=w(t)−η⋅g(t)\n    \n    w^{(t+1)} = w^{(t)} - \\\\eta \\\\cdot g^{(t)}\n    *   where ηη\\\\eta is the learning rate.\n*   This approach is simple and communication-efficient when clients are assumed to have high computational power and small local datasets. However, it performs only one local update per round and hence can converge slowly in practice, especially when client participation is sparse or datasets are large.\n    \n*   The original FedSGD strategy serves as a theoretical foundation and limiting case of the more practical **FedAvg** algorithm, which allows multiple local SGD steps per round to improve convergence without increasing communication frequency.\n    \n\nThe global model is then updated via:\n\n*   where ηη\\\\eta is the learning rate.\n\nThis approach is simple and communication-efficient when clients are assumed to have high computational power and small local datasets. However, it performs only one local update per round and hence can converge slowly in practice, especially when client participation is sparse or datasets are large.\n\nThe original FedSGD strategy serves as a theoretical foundation and limiting case of the more practical **FedAvg** algorithm, which allows multiple local SGD steps per round to improve convergence without increasing communication frequency.",
      "order": 5,
      "orderInChapter": 1,
      "difficulty": 4,
      "estimatedMinutes": 3,
      "tags": [
        "ondevice ai",
        "optimization",
        "gradient descent"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 564,
        "contentLength": 59454
      },
      "nextCards": [
        "ai-federated-learning-fedavg-federated-averaging-6",
        "ai-federated-learning-fedprox-7"
      ],
      "relatedCards": [
        "ai-differential-privacy-dpsgd-core-idea-9",
        "ai-on-device-transformers-hardware-specific-optimization-notes-16",
        "ai-model-compression-low-rank-correction-for-quantization-45",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-model-compression-implementing-pruning-in-pytorch-and-tensorflow-39"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#federated-stochastic-gradient-descent-(fedsgd)",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-fedavg-federated-averaging-6"
      ]
    },
    {
      "id": "ai-federated-learning-fedavg-federated-averaging-6",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Federation Algorithms",
      "title": "FedAvg (Federated Averaging)",
      "subtitle": "Federation Algorithms",
      "contentHtml": "<ul>\n  <li>\n    <p>This foundational method introduced the concept of federated averaging for decentralized model training, published in <a href=\"https://arxiv.org/abs/1602.05629\">Communication‑Efficient Learning of Deep Networks from Decentralized Data</a> by McMahan et al., 2017.</p>\n  </li>\n  <li>\n    <p>FedAvg combines <strong>local stochastic gradient descent (SGD)</strong> on each client with periodic <strong>model averaging</strong> on a central server. The key insight is that <strong>communication efficiency</strong> can be vastly improved by allowing multiple local updates before averaging.</p>\n  </li>\n  <li>\n    <p><strong>Server Process</strong>:</p>\n\n    <ul>\n      <li>Initialize global model weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mn>0</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-377\" style=\"width: 1.878em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.57em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-378\"><span class=\"msubsup\" id=\"MathJax-Span-379\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-380\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-381\"><span class=\"mrow\" id=\"MathJax-Span-382\"><span class=\"mo\" id=\"MathJax-Span-383\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-384\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-385\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">w^{(0)}</script></li>\n      <li>\n        <p>For each round <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-386\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-387\"><span class=\"mi\" id=\"MathJax-Span-388\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">t</script>:</p>\n\n        <ul>\n          <li>Sample a subset <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>S</mi><mi>t</mi></msub><mo>&amp;#x2286;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>{</mo><mn>1</mn><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><mi>K</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>}</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-389\" style=\"width: 7.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.461em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.36em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-390\"><span class=\"msubsup\" id=\"MathJax-Span-391\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-392\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-393\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-394\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">⊆</span><span class=\"mo\" id=\"MathJax-Span-395\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">{</span><span class=\"mn\" id=\"MathJax-Span-396\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-397\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-398\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-399\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"mi\" id=\"MathJax-Span-400\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-401\" style=\"font-family: STIXGeneral-Regular;\">}</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>S</mi><mi>t</mi></msub><mo>⊆</mo><mo fence=\"false\" stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>K</mi><mo fence=\"false\" stretchy=\"false\">}</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">S_t \\subseteq \\{1, \\dots, K\\}</script> of participating clients.</li>\n          <li>Send current model <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-402\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.41em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-403\"><span class=\"msubsup\" id=\"MathJax-Span-404\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-405\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-406\"><span class=\"mrow\" id=\"MathJax-Span-407\"><span class=\"mo\" id=\"MathJax-Span-408\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-409\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-410\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">w^{(t)}</script> to all selected clients.</li>\n          <li>Receive updated models <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-411\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1002.24em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-412\"><span class=\"msubsup\" id=\"MathJax-Span-413\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-414\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-415\"><span class=\"mrow\" id=\"MathJax-Span-416\"><span class=\"mo\" id=\"MathJax-Span-417\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-418\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-419\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-420\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-421\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-422\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">w_k^{(t+1)}</script> after local training.</li>\n          <li>Aggregate updates as:</li>\n        </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>&amp;#x2208;</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><mfrac><msub><mi>n</mi><mi>k</mi></msub><msub><mi>n</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>S</mi><mi>t</mi></msub></mrow></msub></mfrac><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-423\" style=\"width: 10.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.94em, 1008.7em, 3.701em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-424\"><span class=\"msubsup\" id=\"MathJax-Span-425\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-426\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-427\"><span class=\"mrow\" id=\"MathJax-Span-428\"><span class=\"mo\" id=\"MathJax-Span-429\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-430\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-431\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-432\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-433\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-434\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-435\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-436\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.36em, 4.378em, -999.997em); top: -2.862em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-437\"><span class=\"mrow\" id=\"MathJax-Span-438\"><span class=\"mi\" id=\"MathJax-Span-439\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-440\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-441\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-442\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-443\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mfrac\" id=\"MathJax-Span-444\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.89em, 4.326em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.466em;\"><span class=\"msubsup\" id=\"MathJax-Span-445\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-446\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-447\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1001.15em, 4.43em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.57em;\"><span class=\"msubsup\" id=\"MathJax-Span-448\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-449\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-450\"><span class=\"mrow\" id=\"MathJax-Span-451\"><span class=\"msubsup\" id=\"MathJax-Span-452\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-453\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-454\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1001.25em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.253em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-455\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-456\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-457\"><span class=\"mrow\" id=\"MathJax-Span-458\"><span class=\"mo\" id=\"MathJax-Span-459\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-460\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-461\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-462\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-463\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-464\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.684em; border-left: 0px solid; width: 0px; height: 3.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>∈</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><mfrac><msub><mi>n</mi><mi>k</mi></msub><msub><mi>n</mi><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>S</mi><mi>t</mi></msub></mrow></msub></mfrac><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-35\">w^{(t+1)} = \\sum_{k \\in S_t} \\frac{n_k}{n_{S_t}} w_k^{(t+1)}</script>\n\n        <ul>\n          <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>n</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>S</mi><mi>t</mi></msub></mrow></msub><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>&amp;#x2208;</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><msub><mi>n</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-465\" style=\"width: 6.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1005.78em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-466\"><span class=\"msubsup\" id=\"MathJax-Span-467\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-468\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-469\"><span class=\"mrow\" id=\"MathJax-Span-470\"><span class=\"msubsup\" id=\"MathJax-Span-471\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-472\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-473\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-474\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-475\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-476\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-477\"><span class=\"mrow\" id=\"MathJax-Span-478\"><span class=\"mi\" id=\"MathJax-Span-479\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-480\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-481\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-482\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-483\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-484\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-485\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-486\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>n</mi><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>S</mi><mi>t</mi></msub></mrow></msub><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>∈</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><msub><mi>n</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">n_{S_t} = \\sum_{k \\in S_t} n_k</script></li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Client Process</strong>:</p>\n\n    <ul>\n      <li>Receive global weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-487\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.41em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-488\"><span class=\"msubsup\" id=\"MathJax-Span-489\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-490\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-491\"><span class=\"mrow\" id=\"MathJax-Span-492\"><span class=\"mo\" id=\"MathJax-Span-493\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-494\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-495\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">w^{(t)}</script>.</li>\n      <li>Train locally using SGD for <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-496\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-497\"><span class=\"mi\" id=\"MathJax-Span-498\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">E</script> epochs over local dataset <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>D</mi></mrow><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-499\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-500\"><span class=\"msubsup\" id=\"MathJax-Span-501\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-502\"><span class=\"mrow\" id=\"MathJax-Span-503\"><span class=\"mi\" id=\"MathJax-Span-504\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-505\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">D</mi></mrow><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">\\mathcal{D}_k</script>.</li>\n      <li>Return updated weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-506\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1002.24em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-507\"><span class=\"msubsup\" id=\"MathJax-Span-508\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-509\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-510\"><span class=\"mrow\" id=\"MathJax-Span-511\"><span class=\"mo\" id=\"MathJax-Span-512\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-513\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-514\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-515\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-516\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-517\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">w_k^{(t+1)}</script> to server.</li>\n    </ul>\n  </li>\n  <li>\n    <p>FedAvg generalizes FedSGD by allowing local multiple gradient descent steps per round. This makes it more robust and scalable, especially under communication constraints.</p>\n  </li>\n  <li>\n    <p><strong>Theoretical Insights from McMahan et al. (2017)</strong>:</p>\n\n    <ul>\n      <li>FedAvg performs well even when client data is <strong>non-IID and unbalanced</strong>, although convergence can slow in such settings.</li>\n      <li>Larger local epochs <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-518\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-519\"><span class=\"mi\" id=\"MathJax-Span-520\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-41\">E</script> can reduce communication rounds, but might increase model drift.</li>\n      <li>The paper demonstrates practical convergence across MNIST, CIFAR-10, and language modeling tasks using LSTM models on mobile keyboards.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Advantages</strong>:</p>\n\n    <ul>\n      <li><strong>Simplicity</strong>: Easy to implement and tune.</li>\n      <li><strong>Efficiency</strong>: Reduces communication rounds.</li>\n      <li><strong>Scalability</strong>: Tested on large-scale FL setups like Gboard.</li>\n    </ul>\n  </li>\n</ul>\n<p>This foundational method introduced the concept of federated averaging for decentralized model training, published in <a href=\"https://arxiv.org/abs/1602.05629\">Communication‑Efficient Learning of Deep Networks from Decentralized Data</a> by McMahan et al., 2017.</p>\n<p>FedAvg combines <strong>local stochastic gradient descent (SGD)</strong> on each client with periodic <strong>model averaging</strong> on a central server. The key insight is that <strong>communication efficiency</strong> can be vastly improved by allowing multiple local updates before averaging.</p>\n<p><strong>Server Process</strong>:</p>\n<ul>\n      <li>Initialize global model weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mn>0</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-377\" style=\"width: 1.878em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.57em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-378\"><span class=\"msubsup\" id=\"MathJax-Span-379\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-380\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-381\"><span class=\"mrow\" id=\"MathJax-Span-382\"><span class=\"mo\" id=\"MathJax-Span-383\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-384\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-385\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">w^{(0)}</script></li>\n      <li>\n        <p>For each round <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-386\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-387\"><span class=\"mi\" id=\"MathJax-Span-388\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">t</script>:</p>\n\n        <ul>\n          <li>Sample a subset <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>S</mi><mi>t</mi></msub><mo>&amp;#x2286;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>{</mo><mn>1</mn><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><mi>K</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>}</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-389\" style=\"width: 7.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.461em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.36em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-390\"><span class=\"msubsup\" id=\"MathJax-Span-391\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-392\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-393\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-394\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">⊆</span><span class=\"mo\" id=\"MathJax-Span-395\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">{</span><span class=\"mn\" id=\"MathJax-Span-396\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-397\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-398\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-399\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"mi\" id=\"MathJax-Span-400\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-401\" style=\"font-family: STIXGeneral-Regular;\">}</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>S</mi><mi>t</mi></msub><mo>⊆</mo><mo fence=\"false\" stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>K</mi><mo fence=\"false\" stretchy=\"false\">}</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">S_t \\subseteq \\{1, \\dots, K\\}</script> of participating clients.</li>\n          <li>Send current model <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-402\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.41em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-403\"><span class=\"msubsup\" id=\"MathJax-Span-404\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-405\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-406\"><span class=\"mrow\" id=\"MathJax-Span-407\"><span class=\"mo\" id=\"MathJax-Span-408\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-409\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-410\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">w^{(t)}</script> to all selected clients.</li>\n          <li>Receive updated models <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-411\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1002.24em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-412\"><span class=\"msubsup\" id=\"MathJax-Span-413\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-414\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-415\"><span class=\"mrow\" id=\"MathJax-Span-416\"><span class=\"mo\" id=\"MathJax-Span-417\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-418\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-419\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-420\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-421\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-422\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">w_k^{(t+1)}</script> after local training.</li>\n          <li>Aggregate updates as:</li>\n        </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>&amp;#x2208;</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><mfrac><msub><mi>n</mi><mi>k</mi></msub><msub><mi>n</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>S</mi><mi>t</mi></msub></mrow></msub></mfrac><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-423\" style=\"width: 10.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.94em, 1008.7em, 3.701em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-424\"><span class=\"msubsup\" id=\"MathJax-Span-425\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-426\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-427\"><span class=\"mrow\" id=\"MathJax-Span-428\"><span class=\"mo\" id=\"MathJax-Span-429\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-430\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-431\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-432\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-433\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-434\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-435\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-436\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.36em, 4.378em, -999.997em); top: -2.862em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-437\"><span class=\"mrow\" id=\"MathJax-Span-438\"><span class=\"mi\" id=\"MathJax-Span-439\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-440\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-441\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-442\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-443\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mfrac\" id=\"MathJax-Span-444\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.89em, 4.326em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.466em;\"><span class=\"msubsup\" id=\"MathJax-Span-445\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-446\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-447\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1001.15em, 4.43em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.57em;\"><span class=\"msubsup\" id=\"MathJax-Span-448\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-449\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-450\"><span class=\"mrow\" id=\"MathJax-Span-451\"><span class=\"msubsup\" id=\"MathJax-Span-452\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-453\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-454\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1001.25em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.253em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-455\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-456\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-457\"><span class=\"mrow\" id=\"MathJax-Span-458\"><span class=\"mo\" id=\"MathJax-Span-459\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-460\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-461\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-462\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-463\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-464\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.684em; border-left: 0px solid; width: 0px; height: 3.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>∈</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><mfrac><msub><mi>n</mi><mi>k</mi></msub><msub><mi>n</mi><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>S</mi><mi>t</mi></msub></mrow></msub></mfrac><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-35\">w^{(t+1)} = \\sum_{k \\in S_t} \\frac{n_k}{n_{S_t}} w_k^{(t+1)}</script>\n\n        <ul>\n          <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>n</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>S</mi><mi>t</mi></msub></mrow></msub><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>&amp;#x2208;</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><msub><mi>n</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-465\" style=\"width: 6.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1005.78em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-466\"><span class=\"msubsup\" id=\"MathJax-Span-467\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-468\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-469\"><span class=\"mrow\" id=\"MathJax-Span-470\"><span class=\"msubsup\" id=\"MathJax-Span-471\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-472\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-473\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-474\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-475\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-476\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-477\"><span class=\"mrow\" id=\"MathJax-Span-478\"><span class=\"mi\" id=\"MathJax-Span-479\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-480\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-481\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-482\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-483\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-484\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-485\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-486\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>n</mi><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>S</mi><mi>t</mi></msub></mrow></msub><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>∈</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><msub><mi>n</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">n_{S_t} = \\sum_{k \\in S_t} n_k</script></li>\n        </ul>\n      </li>\n    </ul>\n<p>For each round <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-386\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-387\"><span class=\"mi\" id=\"MathJax-Span-388\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">t</script>:</p>\n<ul>\n          <li>Sample a subset <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>S</mi><mi>t</mi></msub><mo>&amp;#x2286;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>{</mo><mn>1</mn><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><mi>K</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>}</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-389\" style=\"width: 7.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.461em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.36em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-390\"><span class=\"msubsup\" id=\"MathJax-Span-391\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-392\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-393\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-394\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">⊆</span><span class=\"mo\" id=\"MathJax-Span-395\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">{</span><span class=\"mn\" id=\"MathJax-Span-396\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-397\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-398\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-399\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"mi\" id=\"MathJax-Span-400\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-401\" style=\"font-family: STIXGeneral-Regular;\">}</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>S</mi><mi>t</mi></msub><mo>⊆</mo><mo fence=\"false\" stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mo>…</mo><mo>,</mo><mi>K</mi><mo fence=\"false\" stretchy=\"false\">}</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">S_t \\subseteq \\{1, \\dots, K\\}</script> of participating clients.</li>\n          <li>Send current model <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-402\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.41em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-403\"><span class=\"msubsup\" id=\"MathJax-Span-404\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-405\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-406\"><span class=\"mrow\" id=\"MathJax-Span-407\"><span class=\"mo\" id=\"MathJax-Span-408\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-409\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-410\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">w^{(t)}</script> to all selected clients.</li>\n          <li>Receive updated models <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-411\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1002.24em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-412\"><span class=\"msubsup\" id=\"MathJax-Span-413\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-414\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-415\"><span class=\"mrow\" id=\"MathJax-Span-416\"><span class=\"mo\" id=\"MathJax-Span-417\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-418\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-419\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-420\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-421\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-422\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">w_k^{(t+1)}</script> after local training.</li>\n          <li>Aggregate updates as:</li>\n        </ul>\n<ul>\n          <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>n</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>S</mi><mi>t</mi></msub></mrow></msub><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>&amp;#x2208;</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><msub><mi>n</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-465\" style=\"width: 6.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1005.78em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-466\"><span class=\"msubsup\" id=\"MathJax-Span-467\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-468\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-469\"><span class=\"mrow\" id=\"MathJax-Span-470\"><span class=\"msubsup\" id=\"MathJax-Span-471\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-472\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-473\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-474\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-475\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-476\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-477\"><span class=\"mrow\" id=\"MathJax-Span-478\"><span class=\"mi\" id=\"MathJax-Span-479\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-480\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-481\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-482\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-483\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-484\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-485\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-486\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>n</mi><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>S</mi><mi>t</mi></msub></mrow></msub><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>∈</mo><msub><mi>S</mi><mi>t</mi></msub></mrow></munder><msub><mi>n</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">n_{S_t} = \\sum_{k \\in S_t} n_k</script></li>\n        </ul>\n<p><strong>Client Process</strong>:</p>\n<ul>\n      <li>Receive global weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-487\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.41em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-488\"><span class=\"msubsup\" id=\"MathJax-Span-489\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-490\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-491\"><span class=\"mrow\" id=\"MathJax-Span-492\"><span class=\"mo\" id=\"MathJax-Span-493\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-494\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-495\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">w^{(t)}</script>.</li>\n      <li>Train locally using SGD for <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-496\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-497\"><span class=\"mi\" id=\"MathJax-Span-498\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">E</script> epochs over local dataset <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;>D</mi></mrow><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-499\" style=\"width: 1.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.2em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-500\"><span class=\"msubsup\" id=\"MathJax-Span-501\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-502\"><span class=\"mrow\" id=\"MathJax-Span-503\"><span class=\"mi\" id=\"MathJax-Span-504\" style=\"font-family: STIXNonUnicode-Italic;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-505\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">D</mi></mrow><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">\\mathcal{D}_k</script>.</li>\n      <li>Return updated weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-506\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1002.24em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-507\"><span class=\"msubsup\" id=\"MathJax-Span-508\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-509\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-510\"><span class=\"mrow\" id=\"MathJax-Span-511\"><span class=\"mo\" id=\"MathJax-Span-512\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-513\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-514\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-515\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-516\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-517\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">w_k^{(t+1)}</script> to server.</li>\n    </ul>\n<p>FedAvg generalizes FedSGD by allowing local multiple gradient descent steps per round. This makes it more robust and scalable, especially under communication constraints.</p>\n<p><strong>Theoretical Insights from McMahan et al. (2017)</strong>:</p>\n<ul>\n      <li>FedAvg performs well even when client data is <strong>non-IID and unbalanced</strong>, although convergence can slow in such settings.</li>\n      <li>Larger local epochs <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-518\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-519\"><span class=\"mi\" id=\"MathJax-Span-520\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-41\">E</script> can reduce communication rounds, but might increase model drift.</li>\n      <li>The paper demonstrates practical convergence across MNIST, CIFAR-10, and language modeling tasks using LSTM models on mobile keyboards.</li>\n    </ul>\n<p><strong>Advantages</strong>:</p>\n<ul>\n      <li><strong>Simplicity</strong>: Easy to implement and tune.</li>\n      <li><strong>Efficiency</strong>: Reduces communication rounds.</li>\n      <li><strong>Scalability</strong>: Tested on large-scale FL setups like Gboard.</li>\n    </ul>",
      "contentMarkdown": "*   This foundational method introduced the concept of federated averaging for decentralized model training, published in [Communication‑Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/abs/1602.05629) by McMahan et al., 2017.\n    \n*   FedAvg combines **local stochastic gradient descent (SGD)** on each client with periodic **model averaging** on a central server. The key insight is that **communication efficiency** can be vastly improved by allowing multiple local updates before averaging.\n    \n*   **Server Process**:\n    \n    *   Initialize global model weights w(0)w(0)w^{(0)}\n    *   For each round ttt:\n        \n        *   Sample a subset St⊆{1,…,K}St⊆{1,…,K}S\\_t \\\\subseteq \\\\{1, \\\\dots, K\\\\} of participating clients.\n        *   Send current model w(t)w(t)w^{(t)} to all selected clients.\n        *   Receive updated models w(t+1)kwk(t+1)w\\_k^{(t+1)} after local training.\n        *   Aggregate updates as:\n        \n        w(t+1)\\=∑k∈StnknStw(t+1)kw(t+1)\\=∑k∈StnknStwk(t+1)\n        \n        w^{(t+1)} = \\\\sum\\_{k \\\\in S\\_t} \\\\frac{n\\_k}{n\\_{S\\_t}} w\\_k^{(t+1)}\n        *   where nSt\\=∑k∈StnknSt\\=∑k∈Stnkn\\_{S\\_t} = \\\\sum\\_{k \\\\in S\\_t} n\\_k\n*   **Client Process**:\n    \n    *   Receive global weights w(t)w(t)w^{(t)}.\n    *   Train locally using SGD for EEE epochs over local dataset kDk\\\\mathcal{D}\\_k.\n    *   Return updated weights w(t+1)kwk(t+1)w\\_k^{(t+1)} to server.\n*   FedAvg generalizes FedSGD by allowing local multiple gradient descent steps per round. This makes it more robust and scalable, especially under communication constraints.\n    \n*   **Theoretical Insights from McMahan et al. (2017)**:\n    \n    *   FedAvg performs well even when client data is **non-IID and unbalanced**, although convergence can slow in such settings.\n    *   Larger local epochs EEE can reduce communication rounds, but might increase model drift.\n    *   The paper demonstrates practical convergence across MNIST, CIFAR-10, and language modeling tasks using LSTM models on mobile keyboards.\n*   **Advantages**:\n    \n    *   **Simplicity**: Easy to implement and tune.\n    *   **Efficiency**: Reduces communication rounds.\n    *   **Scalability**: Tested on large-scale FL setups like Gboard.\n\nThis foundational method introduced the concept of federated averaging for decentralized model training, published in [Communication‑Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/abs/1602.05629) by McMahan et al., 2017.\n\nFedAvg combines **local stochastic gradient descent (SGD)** on each client with periodic **model averaging** on a central server. The key insight is that **communication efficiency** can be vastly improved by allowing multiple local updates before averaging.\n\n**Server Process**:\n\n*   Initialize global model weights w(0)w(0)w^{(0)}\n*   For each round ttt:\n    \n    *   Sample a subset St⊆{1,…,K}St⊆{1,…,K}S\\_t \\\\subseteq \\\\{1, \\\\dots, K\\\\} of participating clients.\n    *   Send current model w(t)w(t)w^{(t)} to all selected clients.\n    *   Receive updated models w(t+1)kwk(t+1)w\\_k^{(t+1)} after local training.\n    *   Aggregate updates as:\n    \n    w(t+1)\\=∑k∈StnknStw(t+1)kw(t+1)\\=∑k∈StnknStwk(t+1)\n    \n    w^{(t+1)} = \\\\sum\\_{k \\\\in S\\_t} \\\\frac{n\\_k}{n\\_{S\\_t}} w\\_k^{(t+1)}\n    *   where nSt\\=∑k∈StnknSt\\=∑k∈Stnkn\\_{S\\_t} = \\\\sum\\_{k \\\\in S\\_t} n\\_k\n\nFor each round ttt:\n\n*   Sample a subset St⊆{1,…,K}St⊆{1,…,K}S\\_t \\\\subseteq \\\\{1, \\\\dots, K\\\\} of participating clients.\n*   Send current model w(t)w(t)w^{(t)} to all selected clients.\n*   Receive updated models w(t+1)kwk(t+1)w\\_k^{(t+1)} after local training.\n*   Aggregate updates as:\n\n*   where nSt\\=∑k∈StnknSt\\=∑k∈Stnkn\\_{S\\_t} = \\\\sum\\_{k \\\\in S\\_t} n\\_k\n\n**Client Process**:\n\n*   Receive global weights w(t)w(t)w^{(t)}.\n*   Train locally using SGD for EEE epochs over local dataset kDk\\\\mathcal{D}\\_k.\n*   Return updated weights w(t+1)kwk(t+1)w\\_k^{(t+1)} to server.\n\nFedAvg generalizes FedSGD by allowing local multiple gradient descent steps per round. This makes it more robust and scalable, especially under communication constraints.\n\n**Theoretical Insights from McMahan et al. (2017)**:\n\n*   FedAvg performs well even when client data is **non-IID and unbalanced**, although convergence can slow in such settings.\n*   Larger local epochs EEE can reduce communication rounds, but might increase model drift.\n*   The paper demonstrates practical convergence across MNIST, CIFAR-10, and language modeling tasks using LSTM models on mobile keyboards.\n\n**Advantages**:\n\n*   **Simplicity**: Easy to implement and tune.\n*   **Efficiency**: Reduces communication rounds.\n*   **Scalability**: Tested on large-scale FL setups like Gboard.",
      "order": 6,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 3,
      "tags": [
        "ondevice ai",
        "lstm",
        "gradient descent"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 564,
        "contentLength": 100631
      },
      "nextCards": [
        "ai-federated-learning-fedprox-7",
        "ai-federated-learning-scaffold-8"
      ],
      "relatedCards": [
        "ai-differential-privacy-dpsgd-core-idea-9",
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#fedavg-(federated-averaging)",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-fedprox-7",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Federation Algorithms",
      "title": "FedProx",
      "subtitle": "Federation Algorithms",
      "contentHtml": "<ul>\n  <li>\n    <p>FedProx, introduced in <a href=\"https://proceedings.mlsys.org/paper_files/paper/2020/file/1f5fe83998a09396ebe6477d9475ba0c-Paper.pdf\">Federated Optimization in Heterogeneous Networks</a> by Li et al. (2018), is a generalization of FedAvg that incorporates a <strong>proximal term</strong> into the local optimization objective. This addition explicitly addresses the <strong>statistical heterogeneity</strong> and <strong>system variability</strong> often observed in FL settings.</p>\n  </li>\n  <li>\n    <p><strong>Local Objective</strong>:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><munder><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mi>w</mi></munder><mspace width=&quot;thinmathspace&quot; /><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mfrac><mi>&amp;#x03BC;</mi><mn>2</mn></mfrac><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>w</mi><mo>&amp;#x2212;</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><msup><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mn>2</mn></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-521\" style=\"width: 12.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.94em, 1010.73em, 3.128em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-522\"><span class=\"munderover\" id=\"MathJax-Span-523\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.57em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-524\" style=\"font-family: STIXGeneral-Regular;\">min</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.47em, 4.273em, -999.997em); top: -3.331em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-525\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mspace\" id=\"MathJax-Span-526\" style=\"height: 0em; vertical-align: 0em; width: 0.211em; display: inline-block; overflow: hidden;\"></span><span class=\"msubsup\" id=\"MathJax-Span-527\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-528\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-529\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-530\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-531\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-532\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-533\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mfrac\" id=\"MathJax-Span-534\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-535\" style=\"font-family: STIXGeneral-Italic;\">μ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-536\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.63em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.628em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-537\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-538\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-539\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-540\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-541\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-542\"><span class=\"mrow\" id=\"MathJax-Span-543\"><span class=\"mo\" id=\"MathJax-Span-544\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-545\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-546\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-547\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-548\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-549\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.997em; border-left: 0px solid; width: 0px; height: 2.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><munder><mo movablelimits=\"true\" form=\"prefix\">min</mo><mi>w</mi></munder><mspace width=\"thinmathspace\"></mspace><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mfrac><mi>μ</mi><mn>2</mn></mfrac><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>w</mi><mo>−</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><msup><mo fence=\"false\" stretchy=\"false\">‖</mo><mn>2</mn></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-42\">\\min_w \\, F_k(w) + \\frac{\\mu}{2} \\| w - w^{(t)} \\|^2</script>\n\n    <ul>\n      <li>Here, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-550\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-551\"><span class=\"msubsup\" id=\"MathJax-Span-552\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-553\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-554\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-555\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-556\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-557\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-43\">F_k(w)</script> is the local objective on client <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-558\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-559\"><span class=\"mi\" id=\"MathJax-Span-560\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">k</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-561\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.41em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-562\"><span class=\"msubsup\" id=\"MathJax-Span-563\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-564\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-565\"><span class=\"mrow\" id=\"MathJax-Span-566\"><span class=\"mo\" id=\"MathJax-Span-567\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-568\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-569\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-45\">w^{(t)}</script> is the global model at round <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-570\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-571\"><span class=\"mi\" id=\"MathJax-Span-572\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">t</script>, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03BC;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-573\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-574\"><span class=\"mi\" id=\"MathJax-Span-575\" style=\"font-family: STIXGeneral-Italic;\">μ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>μ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">\\mu</script> is the proximal coefficient that controls how much local updates deviate from the global state.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Key Properties</strong>:</p>\n\n    <ul>\n      <li>The proximal term <strong>limits the distance</strong> between local model parameters and the global model during each round.</li>\n      <li>This helps <strong>stabilize updates</strong> from clients with highly non-i.i.d. data or inconsistent computational capabilities.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Theoretical Contributions</strong>:</p>\n\n    <ul>\n      <li>The paper proves convergence rates under <em>mild assumptions</em>, including cases with variable local solvers (e.g. incomplete local convergence).</li>\n      <li>It introduces a <strong>partial participation framework</strong> that captures realistic settings where only a fraction of clients participate per round.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Benefits</strong>:</p>\n\n    <ul>\n      <li>Improved convergence in <strong>heterogeneous environments</strong> where FedAvg fails or is unstable.</li>\n      <li>Allows <strong>flexible local computation</strong>, enabling straggler mitigation without penalizing global model performance.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Implementation Tips</strong>:</p>\n\n    <ul>\n      <li>The proximal coefficient <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03BC;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-576\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-577\"><span class=\"mi\" id=\"MathJax-Span-578\" style=\"font-family: STIXGeneral-Italic;\">μ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>μ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">\\mu</script> should be tuned based on the level of heterogeneity—higher values work better when local data distributions are more divergent.</li>\n      <li>Compatible with many local solvers and can be integrated with mini-batch SGD or adaptive optimizers.</li>\n    </ul>\n  </li>\n</ul>\n<p>FedProx, introduced in <a href=\"https://proceedings.mlsys.org/paper_files/paper/2020/file/1f5fe83998a09396ebe6477d9475ba0c-Paper.pdf\">Federated Optimization in Heterogeneous Networks</a> by Li et al. (2018), is a generalization of FedAvg that incorporates a <strong>proximal term</strong> into the local optimization objective. This addition explicitly addresses the <strong>statistical heterogeneity</strong> and <strong>system variability</strong> often observed in FL settings.</p>\n<p><strong>Local Objective</strong>:</p>\n<ul>\n      <li>Here, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-550\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-551\"><span class=\"msubsup\" id=\"MathJax-Span-552\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-553\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-554\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-555\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-556\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-557\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-43\">F_k(w)</script> is the local objective on client <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-558\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-559\"><span class=\"mi\" id=\"MathJax-Span-560\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">k</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-561\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.41em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-562\"><span class=\"msubsup\" id=\"MathJax-Span-563\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-564\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-565\"><span class=\"mrow\" id=\"MathJax-Span-566\"><span class=\"mo\" id=\"MathJax-Span-567\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-568\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-569\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-45\">w^{(t)}</script> is the global model at round <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-570\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-571\"><span class=\"mi\" id=\"MathJax-Span-572\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">t</script>, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03BC;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-573\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-574\"><span class=\"mi\" id=\"MathJax-Span-575\" style=\"font-family: STIXGeneral-Italic;\">μ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>μ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">\\mu</script> is the proximal coefficient that controls how much local updates deviate from the global state.</li>\n    </ul>\n<p><strong>Key Properties</strong>:</p>\n<ul>\n      <li>The proximal term <strong>limits the distance</strong> between local model parameters and the global model during each round.</li>\n      <li>This helps <strong>stabilize updates</strong> from clients with highly non-i.i.d. data or inconsistent computational capabilities.</li>\n    </ul>\n<p><strong>Theoretical Contributions</strong>:</p>\n<ul>\n      <li>The paper proves convergence rates under <em>mild assumptions</em>, including cases with variable local solvers (e.g. incomplete local convergence).</li>\n      <li>It introduces a <strong>partial participation framework</strong> that captures realistic settings where only a fraction of clients participate per round.</li>\n    </ul>\n<p><strong>Benefits</strong>:</p>\n<ul>\n      <li>Improved convergence in <strong>heterogeneous environments</strong> where FedAvg fails or is unstable.</li>\n      <li>Allows <strong>flexible local computation</strong>, enabling straggler mitigation without penalizing global model performance.</li>\n    </ul>\n<p><strong>Implementation Tips</strong>:</p>\n<ul>\n      <li>The proximal coefficient <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03BC;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-576\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-577\"><span class=\"mi\" id=\"MathJax-Span-578\" style=\"font-family: STIXGeneral-Italic;\">μ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>μ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">\\mu</script> should be tuned based on the level of heterogeneity—higher values work better when local data distributions are more divergent.</li>\n      <li>Compatible with many local solvers and can be integrated with mini-batch SGD or adaptive optimizers.</li>\n    </ul>",
      "contentMarkdown": "*   FedProx, introduced in [Federated Optimization in Heterogeneous Networks](https://proceedings.mlsys.org/paper_files/paper/2020/file/1f5fe83998a09396ebe6477d9475ba0c-Paper.pdf) by Li et al. (2018), is a generalization of FedAvg that incorporates a **proximal term** into the local optimization objective. This addition explicitly addresses the **statistical heterogeneity** and **system variability** often observed in FL settings.\n    \n*   **Local Objective**:\n    \n    minwFk(w)+μ2‖w−w(t)‖2minwFk(w)+μ2‖w−w(t)‖2\n    \n    \\\\min\\_w \\\\, F\\_k(w) + \\\\frac{\\\\mu}{2} \\\\| w - w^{(t)} \\\\|^2\n    *   Here, Fk(w)Fk(w)F\\_k(w) is the local objective on client kkk, w(t)w(t)w^{(t)} is the global model at round ttt, and μμ\\\\mu is the proximal coefficient that controls how much local updates deviate from the global state.\n*   **Key Properties**:\n    \n    *   The proximal term **limits the distance** between local model parameters and the global model during each round.\n    *   This helps **stabilize updates** from clients with highly non-i.i.d. data or inconsistent computational capabilities.\n*   **Theoretical Contributions**:\n    \n    *   The paper proves convergence rates under _mild assumptions_, including cases with variable local solvers (e.g. incomplete local convergence).\n    *   It introduces a **partial participation framework** that captures realistic settings where only a fraction of clients participate per round.\n*   **Benefits**:\n    \n    *   Improved convergence in **heterogeneous environments** where FedAvg fails or is unstable.\n    *   Allows **flexible local computation**, enabling straggler mitigation without penalizing global model performance.\n*   **Implementation Tips**:\n    \n    *   The proximal coefficient μμ\\\\mu should be tuned based on the level of heterogeneity—higher values work better when local data distributions are more divergent.\n    *   Compatible with many local solvers and can be integrated with mini-batch SGD or adaptive optimizers.\n\nFedProx, introduced in [Federated Optimization in Heterogeneous Networks](https://proceedings.mlsys.org/paper_files/paper/2020/file/1f5fe83998a09396ebe6477d9475ba0c-Paper.pdf) by Li et al. (2018), is a generalization of FedAvg that incorporates a **proximal term** into the local optimization objective. This addition explicitly addresses the **statistical heterogeneity** and **system variability** often observed in FL settings.\n\n**Local Objective**:\n\n*   Here, Fk(w)Fk(w)F\\_k(w) is the local objective on client kkk, w(t)w(t)w^{(t)} is the global model at round ttt, and μμ\\\\mu is the proximal coefficient that controls how much local updates deviate from the global state.\n\n**Key Properties**:\n\n*   The proximal term **limits the distance** between local model parameters and the global model during each round.\n*   This helps **stabilize updates** from clients with highly non-i.i.d. data or inconsistent computational capabilities.\n\n**Theoretical Contributions**:\n\n*   The paper proves convergence rates under _mild assumptions_, including cases with variable local solvers (e.g. incomplete local convergence).\n*   It introduces a **partial participation framework** that captures realistic settings where only a fraction of clients participate per round.\n\n**Benefits**:\n\n*   Improved convergence in **heterogeneous environments** where FedAvg fails or is unstable.\n*   Allows **flexible local computation**, enabling straggler mitigation without penalizing global model performance.\n\n**Implementation Tips**:\n\n*   The proximal coefficient μμ\\\\mu should be tuned based on the level of heterogeneity—higher values work better when local data distributions are more divergent.\n*   Compatible with many local solvers and can be integrated with mini-batch SGD or adaptive optimizers.",
      "order": 7,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 3,
      "tags": [
        "ondevice ai",
        "optimization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 465,
        "contentLength": 31775
      },
      "nextCards": [
        "ai-federated-learning-scaffold-8",
        "ai-federated-learning-mime-9"
      ],
      "relatedCards": [
        "ai-on-device-transformers-hardware-specific-optimization-notes-16",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-model-compression-low-rank-correction-for-quantization-45",
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#fedprox",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-scaffold-8",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Federation Algorithms",
      "title": "SCAFFOLD",
      "subtitle": "Federation Algorithms",
      "contentHtml": "<ul>\n  <li>\n    <p>Introduced in <a href=\"https://arxiv.org/abs/1910.06378\">Stochastic Controlled Averaging for Federated Learning</a> by Karimireddy et al. (2020), Stochastic Controlled Averaging for Federated Learning (SCAFFOLD) addresses the slow and unstable convergence often observed in federated settings with non-i.i.d. data distributions. SCAFFOLD applies control variates to mitigate the effects of client drift by correcting the updates made by individual clients during training.</p>\n  </li>\n  <li>\n    <p><strong>Core Idea</strong>: In non-i.i.d. settings, local updates may diverge from the global objective due to statistical heterogeneity. SCAFFOLD introduces a <strong>variance reduction mechanism</strong> using control variates to keep local training aligned with the global direction of optimization.</p>\n  </li>\n  <li>\n    <p><strong>Mechanism</strong>:</p>\n\n    <ul>\n      <li>\n        <p>Each client maintains a <strong>local control variate</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>c</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-579\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-580\"><span class=\"msubsup\" id=\"MathJax-Span-581\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-582\" style=\"font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-583\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>c</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-49\">c_k</script>, while the server holds a <strong>global control variate</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>c</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-584\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-585\"><span class=\"mi\" id=\"MathJax-Span-586\" style=\"font-family: STIXGeneral-Italic;\">c</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>c</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">c</script>. These variates estimate the direction and scale of updates.</p>\n      </li>\n      <li>\n        <p>During local training on client <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-587\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-588\"><span class=\"mi\" id=\"MathJax-Span-589\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-51\">k</script>, the update is modified as follows:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo stretchy=&quot;false&quot;>&amp;#x2190;</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo>&amp;#x2212;</mo><mi>&amp;#x03B7;</mi><mrow><mo>(</mo><mrow><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><msub><mi>c</mi><mi>k</mi></msub><mo>+</mo><mi>c</mi></mrow><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-590\" style=\"width: 18.388em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 15.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.086em, 1015.16em, 3.753em, -999.997em); top: -3.174em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-591\"><span class=\"msubsup\" id=\"MathJax-Span-592\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-593\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-594\"><span class=\"mrow\" id=\"MathJax-Span-595\"><span class=\"mo\" id=\"MathJax-Span-596\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-597\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-598\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-599\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-600\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-601\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-602\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">←</span><span class=\"msubsup\" id=\"MathJax-Span-603\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-604\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.78em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-605\"><span class=\"mrow\" id=\"MathJax-Span-606\"><span class=\"mo\" id=\"MathJax-Span-607\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-608\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-609\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-610\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-611\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-612\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">η</span><span class=\"mrow\" id=\"MathJax-Span-613\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-614\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">(</span></span></span><span class=\"mrow\" id=\"MathJax-Span-615\"><span class=\"mi\" id=\"MathJax-Span-616\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-617\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-618\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-619\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-620\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-621\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-622\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.78em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-623\"><span class=\"mrow\" id=\"MathJax-Span-624\"><span class=\"mo\" id=\"MathJax-Span-625\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-626\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-627\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-628\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-629\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-630\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-631\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-632\" style=\"font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-633\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-634\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-635\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">c</span></span><span class=\"mo\" id=\"MathJax-Span-636\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">)</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.18em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">←</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>−</mo><mi>η</mi><mrow><mo>(</mo><mrow><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo><mo>−</mo><msub><mi>c</mi><mi>k</mi></msub><mo>+</mo><mi>c</mi></mrow><mo>)</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-52\">w_k^{(t+1)} \\leftarrow w_k^{(t)} - \\eta \\left( \\nabla F_k(w_k^{(t)}) - c_k + c \\right)</script>\n\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-637\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-638\"><span class=\"mi\" id=\"MathJax-Span-639\" style=\"font-family: STIXGeneral-Italic;\">η</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">\\eta</script> is the learning rate,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-640\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1003.65em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-641\"><span class=\"mi\" id=\"MathJax-Span-642\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-643\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-644\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-645\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-646\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-647\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-648\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.78em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-649\"><span class=\"mrow\" id=\"MathJax-Span-650\"><span class=\"mo\" id=\"MathJax-Span-651\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-652\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-653\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-654\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-655\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-54\">\\nabla F_k(w_k^{(t)})</script> is the gradient of the local loss,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>c</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-656\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-657\"><span class=\"msubsup\" id=\"MathJax-Span-658\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-659\" style=\"font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-660\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>c</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-55\">c_k</script> is the local control variate,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>c</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-661\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-662\"><span class=\"mi\" id=\"MathJax-Span-663\" style=\"font-family: STIXGeneral-Italic;\">c</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>c</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">c</script> is the global control variate.</li>\n        </ul>\n      </li>\n      <li>\n        <p>After local training, both the local model <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-664\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-665\"><span class=\"msubsup\" id=\"MathJax-Span-666\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-667\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-668\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">w_k</script> and the updated control variate <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>c</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-669\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-670\"><span class=\"msubsup\" id=\"MathJax-Span-671\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-672\" style=\"font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-673\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>c</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-58\">c_k</script> are sent back to the server.</p>\n      </li>\n      <li>\n        <p>The server aggregates the received models and updates the global control variate accordingly.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Theoretical Guarantees</strong>:</p>\n\n    <ul>\n      <li>\n        <p>The authors show that SCAFFOLD can <strong>achieve linear speedup</strong> (with respect to the number of clients) under certain assumptions.</p>\n      </li>\n      <li>\n        <p>It provides a provable reduction in client drift and performs better than FedAvg especially in the <strong>non-i.i.d. regime</strong>.</p>\n      </li>\n      <li>\n        <p>Unlike other methods that require full client participation or proximal terms, SCAFFOLD is <strong>robust to partial participation</strong> and reduces the number of communication rounds required to achieve a target accuracy.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Benefits</strong>:</p>\n\n    <ul>\n      <li>Significantly faster convergence under client heterogeneity.</li>\n      <li>Theoretically grounded variance reduction using control variates.</li>\n      <li>More efficient in both communication and compute under typical federated conditions.</li>\n    </ul>\n  </li>\n</ul>\n<p>Introduced in <a href=\"https://arxiv.org/abs/1910.06378\">Stochastic Controlled Averaging for Federated Learning</a> by Karimireddy et al. (2020), Stochastic Controlled Averaging for Federated Learning (SCAFFOLD) addresses the slow and unstable convergence often observed in federated settings with non-i.i.d. data distributions. SCAFFOLD applies control variates to mitigate the effects of client drift by correcting the updates made by individual clients during training.</p>\n<p><strong>Core Idea</strong>: In non-i.i.d. settings, local updates may diverge from the global objective due to statistical heterogeneity. SCAFFOLD introduces a <strong>variance reduction mechanism</strong> using control variates to keep local training aligned with the global direction of optimization.</p>\n<p><strong>Mechanism</strong>:</p>\n<ul>\n      <li>\n        <p>Each client maintains a <strong>local control variate</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>c</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-579\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-580\"><span class=\"msubsup\" id=\"MathJax-Span-581\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-582\" style=\"font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-583\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>c</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-49\">c_k</script>, while the server holds a <strong>global control variate</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>c</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-584\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-585\"><span class=\"mi\" id=\"MathJax-Span-586\" style=\"font-family: STIXGeneral-Italic;\">c</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>c</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">c</script>. These variates estimate the direction and scale of updates.</p>\n      </li>\n      <li>\n        <p>During local training on client <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-587\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-588\"><span class=\"mi\" id=\"MathJax-Span-589\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-51\">k</script>, the update is modified as follows:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo stretchy=&quot;false&quot;>&amp;#x2190;</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo>&amp;#x2212;</mo><mi>&amp;#x03B7;</mi><mrow><mo>(</mo><mrow><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><msub><mi>c</mi><mi>k</mi></msub><mo>+</mo><mi>c</mi></mrow><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-590\" style=\"width: 18.388em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 15.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.086em, 1015.16em, 3.753em, -999.997em); top: -3.174em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-591\"><span class=\"msubsup\" id=\"MathJax-Span-592\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-593\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-594\"><span class=\"mrow\" id=\"MathJax-Span-595\"><span class=\"mo\" id=\"MathJax-Span-596\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-597\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-598\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-599\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-600\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-601\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-602\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">←</span><span class=\"msubsup\" id=\"MathJax-Span-603\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-604\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.78em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-605\"><span class=\"mrow\" id=\"MathJax-Span-606\"><span class=\"mo\" id=\"MathJax-Span-607\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-608\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-609\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-610\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-611\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-612\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">η</span><span class=\"mrow\" id=\"MathJax-Span-613\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-614\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">(</span></span></span><span class=\"mrow\" id=\"MathJax-Span-615\"><span class=\"mi\" id=\"MathJax-Span-616\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-617\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-618\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-619\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-620\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-621\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-622\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.78em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-623\"><span class=\"mrow\" id=\"MathJax-Span-624\"><span class=\"mo\" id=\"MathJax-Span-625\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-626\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-627\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-628\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-629\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-630\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-631\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-632\" style=\"font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-633\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-634\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-635\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">c</span></span><span class=\"mo\" id=\"MathJax-Span-636\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">)</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.18em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">←</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>−</mo><mi>η</mi><mrow><mo>(</mo><mrow><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo><mo>−</mo><msub><mi>c</mi><mi>k</mi></msub><mo>+</mo><mi>c</mi></mrow><mo>)</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-52\">w_k^{(t+1)} \\leftarrow w_k^{(t)} - \\eta \\left( \\nabla F_k(w_k^{(t)}) - c_k + c \\right)</script>\n\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-637\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-638\"><span class=\"mi\" id=\"MathJax-Span-639\" style=\"font-family: STIXGeneral-Italic;\">η</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">\\eta</script> is the learning rate,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-640\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1003.65em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-641\"><span class=\"mi\" id=\"MathJax-Span-642\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-643\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-644\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-645\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-646\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-647\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-648\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.78em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-649\"><span class=\"mrow\" id=\"MathJax-Span-650\"><span class=\"mo\" id=\"MathJax-Span-651\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-652\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-653\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-654\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-655\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-54\">\\nabla F_k(w_k^{(t)})</script> is the gradient of the local loss,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>c</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-656\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-657\"><span class=\"msubsup\" id=\"MathJax-Span-658\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-659\" style=\"font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-660\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>c</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-55\">c_k</script> is the local control variate,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>c</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-661\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-662\"><span class=\"mi\" id=\"MathJax-Span-663\" style=\"font-family: STIXGeneral-Italic;\">c</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>c</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">c</script> is the global control variate.</li>\n        </ul>\n      </li>\n      <li>\n        <p>After local training, both the local model <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-664\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-665\"><span class=\"msubsup\" id=\"MathJax-Span-666\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-667\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-668\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">w_k</script> and the updated control variate <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>c</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-669\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-670\"><span class=\"msubsup\" id=\"MathJax-Span-671\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-672\" style=\"font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-673\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>c</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-58\">c_k</script> are sent back to the server.</p>\n      </li>\n      <li>\n        <p>The server aggregates the received models and updates the global control variate accordingly.</p>\n      </li>\n    </ul>\n<p>Each client maintains a <strong>local control variate</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>c</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-579\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-580\"><span class=\"msubsup\" id=\"MathJax-Span-581\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-582\" style=\"font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-583\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>c</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-49\">c_k</script>, while the server holds a <strong>global control variate</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>c</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-584\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-585\"><span class=\"mi\" id=\"MathJax-Span-586\" style=\"font-family: STIXGeneral-Italic;\">c</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>c</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">c</script>. These variates estimate the direction and scale of updates.</p>\n<p>During local training on client <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-587\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-588\"><span class=\"mi\" id=\"MathJax-Span-589\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-51\">k</script>, the update is modified as follows:</p>\n<p>where:</p>\n<ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B7;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-637\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-638\"><span class=\"mi\" id=\"MathJax-Span-639\" style=\"font-family: STIXGeneral-Italic;\">η</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>η</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">\\eta</script> is the learning rate,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-640\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1003.65em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-641\"><span class=\"mi\" id=\"MathJax-Span-642\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-643\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-644\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-645\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-646\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-647\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-648\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.78em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-649\"><span class=\"mrow\" id=\"MathJax-Span-650\"><span class=\"mo\" id=\"MathJax-Span-651\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-652\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-653\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-654\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-655\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-54\">\\nabla F_k(w_k^{(t)})</script> is the gradient of the local loss,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>c</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-656\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-657\"><span class=\"msubsup\" id=\"MathJax-Span-658\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-659\" style=\"font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-660\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>c</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-55\">c_k</script> is the local control variate,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>c</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-661\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-662\"><span class=\"mi\" id=\"MathJax-Span-663\" style=\"font-family: STIXGeneral-Italic;\">c</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>c</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">c</script> is the global control variate.</li>\n        </ul>\n<p>After local training, both the local model <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-664\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-665\"><span class=\"msubsup\" id=\"MathJax-Span-666\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-667\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-668\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">w_k</script> and the updated control variate <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>c</mi><mi>k</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-669\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-670\"><span class=\"msubsup\" id=\"MathJax-Span-671\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-672\" style=\"font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-673\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>c</mi><mi>k</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-58\">c_k</script> are sent back to the server.</p>\n<p>The server aggregates the received models and updates the global control variate accordingly.</p>\n<p><strong>Theoretical Guarantees</strong>:</p>\n<ul>\n      <li>\n        <p>The authors show that SCAFFOLD can <strong>achieve linear speedup</strong> (with respect to the number of clients) under certain assumptions.</p>\n      </li>\n      <li>\n        <p>It provides a provable reduction in client drift and performs better than FedAvg especially in the <strong>non-i.i.d. regime</strong>.</p>\n      </li>\n      <li>\n        <p>Unlike other methods that require full client participation or proximal terms, SCAFFOLD is <strong>robust to partial participation</strong> and reduces the number of communication rounds required to achieve a target accuracy.</p>\n      </li>\n    </ul>\n<p>The authors show that SCAFFOLD can <strong>achieve linear speedup</strong> (with respect to the number of clients) under certain assumptions.</p>\n<p>It provides a provable reduction in client drift and performs better than FedAvg especially in the <strong>non-i.i.d. regime</strong>.</p>\n<p>Unlike other methods that require full client participation or proximal terms, SCAFFOLD is <strong>robust to partial participation</strong> and reduces the number of communication rounds required to achieve a target accuracy.</p>\n<p><strong>Benefits</strong>:</p>\n<ul>\n      <li>Significantly faster convergence under client heterogeneity.</li>\n      <li>Theoretically grounded variance reduction using control variates.</li>\n      <li>More efficient in both communication and compute under typical federated conditions.</li>\n    </ul>",
      "contentMarkdown": "*   Introduced in [Stochastic Controlled Averaging for Federated Learning](https://arxiv.org/abs/1910.06378) by Karimireddy et al. (2020), Stochastic Controlled Averaging for Federated Learning (SCAFFOLD) addresses the slow and unstable convergence often observed in federated settings with non-i.i.d. data distributions. SCAFFOLD applies control variates to mitigate the effects of client drift by correcting the updates made by individual clients during training.\n    \n*   **Core Idea**: In non-i.i.d. settings, local updates may diverge from the global objective due to statistical heterogeneity. SCAFFOLD introduces a **variance reduction mechanism** using control variates to keep local training aligned with the global direction of optimization.\n    \n*   **Mechanism**:\n    \n    *   Each client maintains a **local control variate** ckckc\\_k, while the server holds a **global control variate** ccc. These variates estimate the direction and scale of updates.\n        \n    *   During local training on client kkk, the update is modified as follows:\n        \n        w(t+1)k←w(t)k−η(∇Fk(w(t)k)−ck+c)wk(t+1)←wk(t)−η(∇Fk(wk(t))−ck+c)\n        \n        w\\_k^{(t+1)} \\\\leftarrow w\\_k^{(t)} - \\\\eta \\\\left( \\\\nabla F\\_k(w\\_k^{(t)}) - c\\_k + c \\\\right)\n        \n        where:\n        \n        *   ηη\\\\eta is the learning rate,\n        *   ∇Fk(w(t)k)∇Fk(wk(t))\\\\nabla F\\_k(w\\_k^{(t)}) is the gradient of the local loss,\n        *   ckckc\\_k is the local control variate,\n        *   ccc is the global control variate.\n    *   After local training, both the local model wkwkw\\_k and the updated control variate ckckc\\_k are sent back to the server.\n        \n    *   The server aggregates the received models and updates the global control variate accordingly.\n        \n*   **Theoretical Guarantees**:\n    \n    *   The authors show that SCAFFOLD can **achieve linear speedup** (with respect to the number of clients) under certain assumptions.\n        \n    *   It provides a provable reduction in client drift and performs better than FedAvg especially in the **non-i.i.d. regime**.\n        \n    *   Unlike other methods that require full client participation or proximal terms, SCAFFOLD is **robust to partial participation** and reduces the number of communication rounds required to achieve a target accuracy.\n        \n*   **Benefits**:\n    \n    *   Significantly faster convergence under client heterogeneity.\n    *   Theoretically grounded variance reduction using control variates.\n    *   More efficient in both communication and compute under typical federated conditions.\n\nIntroduced in [Stochastic Controlled Averaging for Federated Learning](https://arxiv.org/abs/1910.06378) by Karimireddy et al. (2020), Stochastic Controlled Averaging for Federated Learning (SCAFFOLD) addresses the slow and unstable convergence often observed in federated settings with non-i.i.d. data distributions. SCAFFOLD applies control variates to mitigate the effects of client drift by correcting the updates made by individual clients during training.\n\n**Core Idea**: In non-i.i.d. settings, local updates may diverge from the global objective due to statistical heterogeneity. SCAFFOLD introduces a **variance reduction mechanism** using control variates to keep local training aligned with the global direction of optimization.\n\n**Mechanism**:\n\n*   Each client maintains a **local control variate** ckckc\\_k, while the server holds a **global control variate** ccc. These variates estimate the direction and scale of updates.\n    \n*   During local training on client kkk, the update is modified as follows:\n    \n    w(t+1)k←w(t)k−η(∇Fk(w(t)k)−ck+c)wk(t+1)←wk(t)−η(∇Fk(wk(t))−ck+c)\n    \n    w\\_k^{(t+1)} \\\\leftarrow w\\_k^{(t)} - \\\\eta \\\\left( \\\\nabla F\\_k(w\\_k^{(t)}) - c\\_k + c \\\\right)\n    \n    where:\n    \n    *   ηη\\\\eta is the learning rate,\n    *   ∇Fk(w(t)k)∇Fk(wk(t))\\\\nabla F\\_k(w\\_k^{(t)}) is the gradient of the local loss,\n    *   ckckc\\_k is the local control variate,\n    *   ccc is the global control variate.\n*   After local training, both the local model wkwkw\\_k and the updated control variate ckckc\\_k are sent back to the server.\n    \n*   The server aggregates the received models and updates the global control variate accordingly.\n    \n\nEach client maintains a **local control variate** ckckc\\_k, while the server holds a **global control variate** ccc. These variates estimate the direction and scale of updates.\n\nDuring local training on client kkk, the update is modified as follows:\n\nwhere:\n\n*   ηη\\\\eta is the learning rate,\n*   ∇Fk(w(t)k)∇Fk(wk(t))\\\\nabla F\\_k(w\\_k^{(t)}) is the gradient of the local loss,\n*   ckckc\\_k is the local control variate,\n*   ccc is the global control variate.\n\nAfter local training, both the local model wkwkw\\_k and the updated control variate ckckc\\_k are sent back to the server.\n\nThe server aggregates the received models and updates the global control variate accordingly.\n\n**Theoretical Guarantees**:\n\n*   The authors show that SCAFFOLD can **achieve linear speedup** (with respect to the number of clients) under certain assumptions.\n    \n*   It provides a provable reduction in client drift and performs better than FedAvg especially in the **non-i.i.d. regime**.\n    \n*   Unlike other methods that require full client participation or proximal terms, SCAFFOLD is **robust to partial participation** and reduces the number of communication rounds required to achieve a target accuracy.\n    \n\nThe authors show that SCAFFOLD can **achieve linear speedup** (with respect to the number of clients) under certain assumptions.\n\nIt provides a provable reduction in client drift and performs better than FedAvg especially in the **non-i.i.d. regime**.\n\nUnlike other methods that require full client participation or proximal terms, SCAFFOLD is **robust to partial participation** and reduces the number of communication rounds required to achieve a target accuracy.\n\n**Benefits**:\n\n*   Significantly faster convergence under client heterogeneity.\n*   Theoretically grounded variance reduction using control variates.\n*   More efficient in both communication and compute under typical federated conditions.",
      "order": 8,
      "orderInChapter": 4,
      "difficulty": 4,
      "estimatedMinutes": 5,
      "tags": [
        "ondevice ai",
        "optimization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 802,
        "contentLength": 78614
      },
      "nextCards": [
        "ai-federated-learning-mime-9",
        "ai-federated-learning-feddyn-10"
      ],
      "relatedCards": [
        "ai-on-device-transformers-hardware-specific-optimization-notes-16",
        "ai-model-compression-low-rank-correction-for-quantization-45",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#scaffold",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-mime-9",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Federation Algorithms",
      "title": "MIME",
      "subtitle": "Federation Algorithms",
      "contentHtml": "<ul>\n  <li>\n    <p>Introduced in <a href=\"https://arxiv.org/abs/2008.03606\">Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning</a> by Karimireddy et al. (2021), Mimicking Centralized Stochastic Algorithms (MIME) bridges the gap between centralized and federated training by incorporating global optimizer states like momentum and variance tracking.</p>\n  </li>\n  <li>\n    <p><strong>Key Idea</strong>:</p>\n\n    <ul>\n      <li>Clients mimic centralized SGD behavior by combining local updates with a server-side optimizer (e.g., momentum SGD or Adam).</li>\n      <li>MIME introduces <em>control variates</em> at the server level, capturing gradient differences that correct for stale or drifting local updates.</li>\n      <li>Server sends both the model parameters and a preconditioned gradient to each client. Clients perform local SGD using this guidance to align with centralized dynamics.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Mechanism</strong>:</p>\n\n    <ul>\n      <li>The global server maintains a momentum buffer or preconditioner (e.g., gradient moving average).</li>\n      <li>\n        <p>Each client performs the following at round <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-59-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-674\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-675\"><span class=\"mi\" id=\"MathJax-Span-676\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-59\">t</script>:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-60-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo>=</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo>&amp;#x2212;</mo><mi>&amp;#x03B7;</mi><mrow><mo>(</mo><mrow><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>g</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></mrow><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-677\" style=\"width: 23.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 19.273em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.826em, 1019.12em, 3.44em, -999.997em); top: -2.914em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-678\"><span class=\"msubsup\" id=\"MathJax-Span-679\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-680\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-681\"><span class=\"mrow\" id=\"MathJax-Span-682\"><span class=\"mo\" id=\"MathJax-Span-683\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-684\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-685\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-686\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-687\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-688\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-689\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-690\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-691\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-692\"><span class=\"mrow\" id=\"MathJax-Span-693\"><span class=\"mo\" id=\"MathJax-Span-694\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-695\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-696\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-697\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-698\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">η</span><span class=\"mrow\" id=\"MathJax-Span-699\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-700\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-701\"><span class=\"mi\" id=\"MathJax-Span-702\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-703\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-704\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-705\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-706\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-707\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-708\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-709\"><span class=\"mrow\" id=\"MathJax-Span-710\"><span class=\"mo\" id=\"MathJax-Span-711\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-712\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-713\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-714\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-715\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-716\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-717\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-718\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-719\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-720\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-721\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-722\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-723\"><span class=\"mrow\" id=\"MathJax-Span-724\"><span class=\"mo\" id=\"MathJax-Span-725\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-726\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-727\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-728\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-729\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-730\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-731\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-732\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-733\"><span class=\"mrow\" id=\"MathJax-Span-734\"><span class=\"munderover\" id=\"MathJax-Span-735\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-736\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-737\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-738\"><span class=\"mrow\" id=\"MathJax-Span-739\"><span class=\"mo\" id=\"MathJax-Span-740\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-741\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-742\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-743\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.919em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>−</mo><mi>η</mi><mrow><mo>(</mo><mrow><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo><mo>−</mo><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo><mo>+</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>g</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo>)</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-60\">w_k^{(t+1)} = w^{(t)} - \\eta \\left( \\nabla F_k(w^{(t)}) - \\nabla F_k(w^{(t-1)}) + \\bar{g}^{(t)} \\right)</script>\n\n        <ul>\n          <li>\n            <p>where:</p>\n\n            <ul>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>g</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-744\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.25em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-745\"><span class=\"msubsup\" id=\"MathJax-Span-746\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-747\"><span class=\"mrow\" id=\"MathJax-Span-748\"><span class=\"munderover\" id=\"MathJax-Span-749\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-750\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-751\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-752\"><span class=\"mrow\" id=\"MathJax-Span-753\"><span class=\"mo\" id=\"MathJax-Span-754\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-755\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-756\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>g</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">\\bar{g}^{(t)}</script> is the server-side momentum gradient,</li>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-62-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-757\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-758\"><span class=\"mi\" id=\"MathJax-Span-759\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-760\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-761\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-762\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-763\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-764\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-765\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-766\"><span class=\"mrow\" id=\"MathJax-Span-767\"><span class=\"mo\" id=\"MathJax-Span-768\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-769\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-770\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-771\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-62\">\\nabla F_k(w^{(t)})</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-63-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-772\" style=\"width: 5.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.534em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1004.48em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-773\"><span class=\"mi\" id=\"MathJax-Span-774\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-775\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-776\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-777\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-778\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-779\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-780\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-781\"><span class=\"mrow\" id=\"MathJax-Span-782\"><span class=\"mo\" id=\"MathJax-Span-783\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-784\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-785\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-786\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-787\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-788\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-63\">\\nabla F_k(w^{(t-1)})</script> are local gradients at current and previous iterations.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Benefits</strong>:</p>\n\n    <ul>\n      <li>Effectively reduces client drift in heterogeneous data settings.</li>\n      <li>Improves convergence speed and final model accuracy across various FL benchmarks (e.g., EMNIST, CIFAR-10).</li>\n      <li>Compatible with advanced optimizers like Adam and RMSprop due to its modular design.</li>\n      <li>Empirically shown to outperform FedAvg and FedProx in both IID and non-IID scenarios with fewer communication rounds.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Implementation Notes</strong>:</p>\n\n    <ul>\n      <li>MIME can be implemented by augmenting a FedAvg setup with an extra gradient tracking module at the server.</li>\n      <li>Clients must retain a copy of their previous model state to compute gradient deltas.</li>\n      <li>Performance benefits are highest in regimes with high heterogeneity and sparse client participation.</li>\n    </ul>\n  </li>\n</ul>\n<p>Introduced in <a href=\"https://arxiv.org/abs/2008.03606\">Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning</a> by Karimireddy et al. (2021), Mimicking Centralized Stochastic Algorithms (MIME) bridges the gap between centralized and federated training by incorporating global optimizer states like momentum and variance tracking.</p>\n<p><strong>Key Idea</strong>:</p>\n<ul>\n      <li>Clients mimic centralized SGD behavior by combining local updates with a server-side optimizer (e.g., momentum SGD or Adam).</li>\n      <li>MIME introduces <em>control variates</em> at the server level, capturing gradient differences that correct for stale or drifting local updates.</li>\n      <li>Server sends both the model parameters and a preconditioned gradient to each client. Clients perform local SGD using this guidance to align with centralized dynamics.</li>\n    </ul>\n<p><strong>Mechanism</strong>:</p>\n<ul>\n      <li>The global server maintains a momentum buffer or preconditioner (e.g., gradient moving average).</li>\n      <li>\n        <p>Each client performs the following at round <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-59-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-674\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-675\"><span class=\"mi\" id=\"MathJax-Span-676\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-59\">t</script>:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-60-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msubsup><mi>w</mi><mi>k</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msubsup><mo>=</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo>&amp;#x2212;</mo><mi>&amp;#x03B7;</mi><mrow><mo>(</mo><mrow><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>g</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></mrow><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-677\" style=\"width: 23.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 19.273em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.826em, 1019.12em, 3.44em, -999.997em); top: -2.914em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-678\"><span class=\"msubsup\" id=\"MathJax-Span-679\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-680\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.62em, 4.273em, -999.997em); top: -4.477em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-681\"><span class=\"mrow\" id=\"MathJax-Span-682\"><span class=\"mo\" id=\"MathJax-Span-683\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-684\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-685\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-686\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-687\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-688\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-689\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-690\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-691\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-692\"><span class=\"mrow\" id=\"MathJax-Span-693\"><span class=\"mo\" id=\"MathJax-Span-694\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-695\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-696\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-697\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-698\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">η</span><span class=\"mrow\" id=\"MathJax-Span-699\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-700\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-701\"><span class=\"mi\" id=\"MathJax-Span-702\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-703\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-704\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-705\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-706\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-707\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-708\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-709\"><span class=\"mrow\" id=\"MathJax-Span-710\"><span class=\"mo\" id=\"MathJax-Span-711\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-712\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-713\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-714\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-715\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-716\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-717\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-718\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-719\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-720\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-721\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-722\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-723\"><span class=\"mrow\" id=\"MathJax-Span-724\"><span class=\"mo\" id=\"MathJax-Span-725\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-726\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-727\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-728\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-729\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-730\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-731\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-732\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-733\"><span class=\"mrow\" id=\"MathJax-Span-734\"><span class=\"munderover\" id=\"MathJax-Span-735\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-736\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-737\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-738\"><span class=\"mrow\" id=\"MathJax-Span-739\"><span class=\"mo\" id=\"MathJax-Span-740\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-741\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-742\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-743\" style=\"vertical-align: -0.206em;\"><span style=\"font-family: STIXSizeOneSym;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.919em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msubsup><mi>w</mi><mi>k</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>=</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>−</mo><mi>η</mi><mrow><mo>(</mo><mrow><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo><mo>−</mo><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo><mo>+</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>g</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow><mo>)</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-60\">w_k^{(t+1)} = w^{(t)} - \\eta \\left( \\nabla F_k(w^{(t)}) - \\nabla F_k(w^{(t-1)}) + \\bar{g}^{(t)} \\right)</script>\n\n        <ul>\n          <li>\n            <p>where:</p>\n\n            <ul>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>g</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-744\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.25em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-745\"><span class=\"msubsup\" id=\"MathJax-Span-746\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-747\"><span class=\"mrow\" id=\"MathJax-Span-748\"><span class=\"munderover\" id=\"MathJax-Span-749\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-750\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-751\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-752\"><span class=\"mrow\" id=\"MathJax-Span-753\"><span class=\"mo\" id=\"MathJax-Span-754\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-755\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-756\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>g</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">\\bar{g}^{(t)}</script> is the server-side momentum gradient,</li>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-62-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-757\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-758\"><span class=\"mi\" id=\"MathJax-Span-759\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-760\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-761\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-762\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-763\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-764\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-765\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-766\"><span class=\"mrow\" id=\"MathJax-Span-767\"><span class=\"mo\" id=\"MathJax-Span-768\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-769\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-770\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-771\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-62\">\\nabla F_k(w^{(t)})</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-63-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-772\" style=\"width: 5.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.534em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1004.48em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-773\"><span class=\"mi\" id=\"MathJax-Span-774\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-775\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-776\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-777\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-778\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-779\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-780\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-781\"><span class=\"mrow\" id=\"MathJax-Span-782\"><span class=\"mo\" id=\"MathJax-Span-783\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-784\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-785\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-786\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-787\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-788\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-63\">\\nabla F_k(w^{(t-1)})</script> are local gradients at current and previous iterations.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n    </ul>\n<p>Each client performs the following at round <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-59-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-674\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-675\"><span class=\"mi\" id=\"MathJax-Span-676\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-59\">t</script>:</p>\n<ul>\n          <li>\n            <p>where:</p>\n\n            <ul>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>g</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-744\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.25em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-745\"><span class=\"msubsup\" id=\"MathJax-Span-746\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-747\"><span class=\"mrow\" id=\"MathJax-Span-748\"><span class=\"munderover\" id=\"MathJax-Span-749\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-750\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-751\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-752\"><span class=\"mrow\" id=\"MathJax-Span-753\"><span class=\"mo\" id=\"MathJax-Span-754\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-755\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-756\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>g</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">\\bar{g}^{(t)}</script> is the server-side momentum gradient,</li>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-62-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-757\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-758\"><span class=\"mi\" id=\"MathJax-Span-759\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-760\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-761\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-762\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-763\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-764\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-765\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-766\"><span class=\"mrow\" id=\"MathJax-Span-767\"><span class=\"mo\" id=\"MathJax-Span-768\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-769\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-770\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-771\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-62\">\\nabla F_k(w^{(t)})</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-63-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-772\" style=\"width: 5.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.534em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1004.48em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-773\"><span class=\"mi\" id=\"MathJax-Span-774\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-775\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-776\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-777\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-778\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-779\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-780\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-781\"><span class=\"mrow\" id=\"MathJax-Span-782\"><span class=\"mo\" id=\"MathJax-Span-783\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-784\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-785\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-786\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-787\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-788\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-63\">\\nabla F_k(w^{(t-1)})</script> are local gradients at current and previous iterations.</li>\n            </ul>\n          </li>\n        </ul>\n<p>where:</p>\n<ul>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>g</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-744\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.25em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-745\"><span class=\"msubsup\" id=\"MathJax-Span-746\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-747\"><span class=\"mrow\" id=\"MathJax-Span-748\"><span class=\"munderover\" id=\"MathJax-Span-749\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-750\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-751\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-752\"><span class=\"mrow\" id=\"MathJax-Span-753\"><span class=\"mo\" id=\"MathJax-Span-754\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-755\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-756\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>g</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">\\bar{g}^{(t)}</script> is the server-side momentum gradient,</li>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-62-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-757\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1003.65em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-758\"><span class=\"mi\" id=\"MathJax-Span-759\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-760\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-761\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-762\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-763\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-764\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-765\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-766\"><span class=\"mrow\" id=\"MathJax-Span-767\"><span class=\"mo\" id=\"MathJax-Span-768\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-769\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-770\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-771\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-62\">\\nabla F_k(w^{(t)})</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-63-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></mrow></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-772\" style=\"width: 5.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.534em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1004.48em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-773\"><span class=\"mi\" id=\"MathJax-Span-774\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"msubsup\" id=\"MathJax-Span-775\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-776\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-777\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-778\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-779\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-780\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-781\"><span class=\"mrow\" id=\"MathJax-Span-782\"><span class=\"mo\" id=\"MathJax-Span-783\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-784\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-785\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-786\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-787\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-788\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">∇</mi><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-63\">\\nabla F_k(w^{(t-1)})</script> are local gradients at current and previous iterations.</li>\n            </ul>\n<p><strong>Benefits</strong>:</p>\n<ul>\n      <li>Effectively reduces client drift in heterogeneous data settings.</li>\n      <li>Improves convergence speed and final model accuracy across various FL benchmarks (e.g., EMNIST, CIFAR-10).</li>\n      <li>Compatible with advanced optimizers like Adam and RMSprop due to its modular design.</li>\n      <li>Empirically shown to outperform FedAvg and FedProx in both IID and non-IID scenarios with fewer communication rounds.</li>\n    </ul>\n<p><strong>Implementation Notes</strong>:</p>\n<ul>\n      <li>MIME can be implemented by augmenting a FedAvg setup with an extra gradient tracking module at the server.</li>\n      <li>Clients must retain a copy of their previous model state to compute gradient deltas.</li>\n      <li>Performance benefits are highest in regimes with high heterogeneity and sparse client participation.</li>\n    </ul>",
      "contentMarkdown": "*   Introduced in [Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning](https://arxiv.org/abs/2008.03606) by Karimireddy et al. (2021), Mimicking Centralized Stochastic Algorithms (MIME) bridges the gap between centralized and federated training by incorporating global optimizer states like momentum and variance tracking.\n    \n*   **Key Idea**:\n    \n    *   Clients mimic centralized SGD behavior by combining local updates with a server-side optimizer (e.g., momentum SGD or Adam).\n    *   MIME introduces _control variates_ at the server level, capturing gradient differences that correct for stale or drifting local updates.\n    *   Server sends both the model parameters and a preconditioned gradient to each client. Clients perform local SGD using this guidance to align with centralized dynamics.\n*   **Mechanism**:\n    \n    *   The global server maintains a momentum buffer or preconditioner (e.g., gradient moving average).\n    *   Each client performs the following at round ttt:\n        \n        w(t+1)k\\=w(t)−η(∇Fk(w(t))−∇Fk(w(t−1))+g¯(t))wk(t+1)\\=w(t)−η(∇Fk(w(t))−∇Fk(w(t−1))+g¯(t))\n        \n        w\\_k^{(t+1)} = w^{(t)} - \\\\eta \\\\left( \\\\nabla F\\_k(w^{(t)}) - \\\\nabla F\\_k(w^{(t-1)}) + \\\\bar{g}^{(t)} \\\\right)\n        *   where:\n            \n            *   g¯(t)g¯(t)\\\\bar{g}^{(t)} is the server-side momentum gradient,\n            *   ∇Fk(w(t))∇Fk(w(t))\\\\nabla F\\_k(w^{(t)}) and ∇Fk(w(t−1))∇Fk(w(t−1))\\\\nabla F\\_k(w^{(t-1)}) are local gradients at current and previous iterations.\n*   **Benefits**:\n    \n    *   Effectively reduces client drift in heterogeneous data settings.\n    *   Improves convergence speed and final model accuracy across various FL benchmarks (e.g., EMNIST, CIFAR-10).\n    *   Compatible with advanced optimizers like Adam and RMSprop due to its modular design.\n    *   Empirically shown to outperform FedAvg and FedProx in both IID and non-IID scenarios with fewer communication rounds.\n*   **Implementation Notes**:\n    \n    *   MIME can be implemented by augmenting a FedAvg setup with an extra gradient tracking module at the server.\n    *   Clients must retain a copy of their previous model state to compute gradient deltas.\n    *   Performance benefits are highest in regimes with high heterogeneity and sparse client participation.\n\nIntroduced in [Mime: Mimicking Centralized Stochastic Algorithms in Federated Learning](https://arxiv.org/abs/2008.03606) by Karimireddy et al. (2021), Mimicking Centralized Stochastic Algorithms (MIME) bridges the gap between centralized and federated training by incorporating global optimizer states like momentum and variance tracking.\n\n**Key Idea**:\n\n*   Clients mimic centralized SGD behavior by combining local updates with a server-side optimizer (e.g., momentum SGD or Adam).\n*   MIME introduces _control variates_ at the server level, capturing gradient differences that correct for stale or drifting local updates.\n*   Server sends both the model parameters and a preconditioned gradient to each client. Clients perform local SGD using this guidance to align with centralized dynamics.\n\n**Mechanism**:\n\n*   The global server maintains a momentum buffer or preconditioner (e.g., gradient moving average).\n*   Each client performs the following at round ttt:\n    \n    w(t+1)k\\=w(t)−η(∇Fk(w(t))−∇Fk(w(t−1))+g¯(t))wk(t+1)\\=w(t)−η(∇Fk(w(t))−∇Fk(w(t−1))+g¯(t))\n    \n    w\\_k^{(t+1)} = w^{(t)} - \\\\eta \\\\left( \\\\nabla F\\_k(w^{(t)}) - \\\\nabla F\\_k(w^{(t-1)}) + \\\\bar{g}^{(t)} \\\\right)\n    *   where:\n        \n        *   g¯(t)g¯(t)\\\\bar{g}^{(t)} is the server-side momentum gradient,\n        *   ∇Fk(w(t))∇Fk(w(t))\\\\nabla F\\_k(w^{(t)}) and ∇Fk(w(t−1))∇Fk(w(t−1))\\\\nabla F\\_k(w^{(t-1)}) are local gradients at current and previous iterations.\n\nEach client performs the following at round ttt:\n\n*   where:\n    \n    *   g¯(t)g¯(t)\\\\bar{g}^{(t)} is the server-side momentum gradient,\n    *   ∇Fk(w(t))∇Fk(w(t))\\\\nabla F\\_k(w^{(t)}) and ∇Fk(w(t−1))∇Fk(w(t−1))\\\\nabla F\\_k(w^{(t-1)}) are local gradients at current and previous iterations.\n\nwhere:\n\n*   g¯(t)g¯(t)\\\\bar{g}^{(t)} is the server-side momentum gradient,\n*   ∇Fk(w(t))∇Fk(w(t))\\\\nabla F\\_k(w^{(t)}) and ∇Fk(w(t−1))∇Fk(w(t−1))\\\\nabla F\\_k(w^{(t-1)}) are local gradients at current and previous iterations.\n\n**Benefits**:\n\n*   Effectively reduces client drift in heterogeneous data settings.\n*   Improves convergence speed and final model accuracy across various FL benchmarks (e.g., EMNIST, CIFAR-10).\n*   Compatible with advanced optimizers like Adam and RMSprop due to its modular design.\n*   Empirically shown to outperform FedAvg and FedProx in both IID and non-IID scenarios with fewer communication rounds.\n\n**Implementation Notes**:\n\n*   MIME can be implemented by augmenting a FedAvg setup with an extra gradient tracking module at the server.\n*   Clients must retain a copy of their previous model state to compute gradient deltas.\n*   Performance benefits are highest in regimes with high heterogeneity and sparse client participation.",
      "order": 9,
      "orderInChapter": 5,
      "difficulty": 3,
      "estimatedMinutes": 4,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 606,
        "contentLength": 82421
      },
      "nextCards": [
        "ai-federated-learning-feddyn-10",
        "ai-federated-learning-comparative-analysis-of-algorithmic-variants-11"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#mime",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-feddyn-10",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Federation Algorithms",
      "title": "FedDyn",
      "subtitle": "Federation Algorithms",
      "contentHtml": "<ul>\n  <li>\n    <p>Presented in <a href=\"https://openreview.net/forum?id=B7v4QMR6Z9w\">Federated Learning Based on Dynamic Regularization</a> by Acar et al. (2021), Federated Learning with Dynamic Regularization (FedDyn) addresses the core challenge of objective inconsistency between local client losses and the global optimization objective under data heterogeneity. In typical federated learning scenarios, the optimization problem aims to minimize the global loss across all clients:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-64-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><munder><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>w</mi></mrow></munder><mrow><mo>{</mo><mrow><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mfrac><msub><mi>n</mi><mi>k</mi></msub><mi>n</mi></mfrac><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo></mrow><mo>}</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-789\" style=\"width: 14.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.19em, 1011.98em, 5.576em, -999.997em); top: -4.112em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-790\"><span class=\"munderover\" id=\"MathJax-Span-791\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.57em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-792\" style=\"font-family: STIXGeneral-Regular;\">min</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.47em, 4.273em, -999.997em); top: -3.331em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-793\"><span class=\"mrow\" id=\"MathJax-Span-794\"><span class=\"mi\" id=\"MathJax-Span-795\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mrow\" id=\"MathJax-Span-796\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-797\" style=\"vertical-align: -0.779em;\"><span style=\"font-family: STIXSizeFourSym;\">{</span></span><span class=\"mrow\" id=\"MathJax-Span-798\"><span class=\"mi\" id=\"MathJax-Span-799\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-800\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-801\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-802\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-803\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-804\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-805\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.1em, 4.273em, -999.997em); top: -2.862em; left: 0.055em;\"><span class=\"texatom\" id=\"MathJax-Span-806\"><span class=\"mrow\" id=\"MathJax-Span-807\"><span class=\"mi\" id=\"MathJax-Span-808\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-809\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-810\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.419em;\"><span class=\"mi\" id=\"MathJax-Span-811\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mfrac\" id=\"MathJax-Span-812\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.89em, 4.326em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.466em;\"><span class=\"msubsup\" id=\"MathJax-Span-813\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-814\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-815\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-816\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1001.04em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.044em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-817\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-818\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-819\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-820\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-821\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-822\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span class=\"mo\" id=\"MathJax-Span-823\" style=\"vertical-align: -0.779em;\"><span style=\"font-family: STIXSizeFourSym;\">}</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.622em; border-left: 0px solid; width: 0px; height: 3.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><munder><mo movablelimits=\"true\" form=\"prefix\">min</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>w</mi></mrow></munder><mrow><mo>{</mo><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mfrac><msub><mi>n</mi><mi>k</mi></msub><mi>n</mi></mfrac><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></mrow><mo>}</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-64\">\\min_{w} \\left\\{ f(w) = \\sum_{k=1}^K \\frac{n_k}{n} F_k(w) \\right\\}</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-65-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-824\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-825\"><span class=\"msubsup\" id=\"MathJax-Span-826\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-827\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-828\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-829\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-830\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-831\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-65\">F_k(w)</script> is the empirical loss on client <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-66-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-832\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-833\"><span class=\"mi\" id=\"MathJax-Span-834\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-66\">k</script>. However, in non-i.i.d. settings, minimizing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-67-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-835\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-836\"><span class=\"msubsup\" id=\"MathJax-Span-837\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-838\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-839\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-840\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-841\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-842\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-67\">F_k(w)</script> individually on each client may not lead to convergence of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-68-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-843\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.83em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-844\"><span class=\"mi\" id=\"MathJax-Span-845\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-846\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-847\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-848\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-68\">f(w)</script>.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>FedDyn</strong> introduces a dynamically updated regularization term in each client’s local objective to mitigate this discrepancy. Specifically, FedDyn modifies the local objective function as follows:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-69-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>F</mi><mo stretchy=&quot;false&quot;>&amp;#x007E;</mo></mover></mrow><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x27E8;</mo><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>)</mo><mo>,</mo><mi>w</mi><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x27E9;</mo><mo>+</mo><mfrac><mi>&amp;#x03BB;</mi><mn>2</mn></mfrac><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mi>w</mi><mo>&amp;#x2212;</mo><msub><mi>w</mi><mi>t</mi></msub><msup><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mn>2</mn></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-849\" style=\"width: 21.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 17.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.732em, 1017.66em, 3.076em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-850\"><span class=\"msubsup\" id=\"MathJax-Span-851\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.971em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-852\"><span class=\"mrow\" id=\"MathJax-Span-853\"><span class=\"munderover\" id=\"MathJax-Span-854\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-855\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.211em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.419em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-856\" style=\"font-family: STIXGeneral-Regular;\">̃&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-857\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-858\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-859\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-860\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-861\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-862\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-863\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-864\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-865\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-866\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-867\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-868\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mo\" id=\"MathJax-Span-869\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⟨</span><span class=\"mi\" id=\"MathJax-Span-870\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"mi\" id=\"MathJax-Span-871\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-872\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-873\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-874\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-875\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-876\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-877\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-878\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">w</span><span class=\"mo\" id=\"MathJax-Span-879\" style=\"font-family: STIXGeneral-Regular;\">⟩</span><span class=\"mo\" id=\"MathJax-Span-880\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mfrac\" id=\"MathJax-Span-881\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.206em;\"><span class=\"mi\" id=\"MathJax-Span-882\" style=\"font-family: STIXGeneral-Italic;\">λ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-883\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.63em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.628em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-884\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"mi\" id=\"MathJax-Span-885\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-886\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"msubsup\" id=\"MathJax-Span-887\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-888\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-889\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-890\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-891\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-892\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>F</mi><mo stretchy=\"false\">~</mo></mover></mrow><mi>k</mi></msub><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>−</mo><mo fence=\"false\" stretchy=\"false\">⟨</mo><mi mathvariant=\"normal\">∇</mi><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo><mo>,</mo><mi>w</mi><mo fence=\"false\" stretchy=\"false\">⟩</mo><mo>+</mo><mfrac><mi>λ</mi><mn>2</mn></mfrac><mo fence=\"false\" stretchy=\"false\">‖</mo><mi>w</mi><mo>−</mo><msub><mi>w</mi><mi>t</mi></msub><msup><mo fence=\"false\" stretchy=\"false\">‖</mo><mn>2</mn></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-69\">\\tilde{F}_k(w) = F_k(w) - \\langle \\nabla f(w_t), w \\rangle + \\frac{\\lambda}{2} \\| w - w_t \\|^2</script>\n\n    <ul>\n      <li>\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-70-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-893\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-894\"><span class=\"msubsup\" id=\"MathJax-Span-895\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-896\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-897\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-70\">w_t</script> is the global model at round <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-71-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-898\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-899\"><span class=\"mi\" id=\"MathJax-Span-900\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-71\">t</script>,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-72-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-901\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.66em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-902\"><span class=\"mi\" id=\"MathJax-Span-903\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"mi\" id=\"MathJax-Span-904\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-905\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-906\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-907\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-908\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-909\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">∇</mi><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-72\">\\nabla f(w_t)</script> is an approximation of the global gradient at <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-73-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-910\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-911\"><span class=\"msubsup\" id=\"MathJax-Span-912\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-913\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-914\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-73\">w_t</script>,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-74-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03BB;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-915\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-916\"><span class=\"mi\" id=\"MathJax-Span-917\" style=\"font-family: STIXGeneral-Italic;\">λ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>λ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-74\">\\lambda</script> is a regularization coefficient.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>This regularization acts as a <strong>bias-correction mechanism</strong>, aligning local descent directions with the global objective. By integrating both the history of updates and a proximal term, FedDyn ensures that each client’s optimization path stays consistent with the global descent trajectory.</p>\n  </li>\n  <li>\n    <p><strong>Advantages</strong>:</p>\n\n    <ul>\n      <li>Demonstrates <strong>convergence guarantees</strong> for non-convex losses under arbitrary heterogeneity assumptions.</li>\n      <li>Empirically outperforms other baselines such as FedAvg and FedProx on diverse datasets (e.g., CIFAR-10, FEMNIST, Shakespeare), especially under high data non-i.i.d.-ness.</li>\n      <li>The method reduces <strong>client drift</strong> and improves both convergence speed and final accuracy.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>FedDynOneGD</strong>:</p>\n\n    <ul>\n      <li>A lightweight extension that performs <strong>only one gradient computation per client per round</strong>.</li>\n      <li>Reduces local computation from multiple SGD steps to a single pass, with <strong>linear complexity</strong> in the number of local examples.</li>\n      <li>Suitable for low-power or resource-constrained devices while retaining the convergence behavior of the full FedDyn formulation.</li>\n    </ul>\n  </li>\n  <li>\n    <p>These innovations position FedDyn as a <strong>communication-efficient</strong>, <strong>robust</strong>, and <strong>heterogeneity-aware</strong> alternative for real-world federated learning deployments, where computational and statistical variance across clients is significant.</p>\n  </li>\n</ul>\n<p>Presented in <a href=\"https://openreview.net/forum?id=B7v4QMR6Z9w\">Federated Learning Based on Dynamic Regularization</a> by Acar et al. (2021), Federated Learning with Dynamic Regularization (FedDyn) addresses the core challenge of objective inconsistency between local client losses and the global optimization objective under data heterogeneity. In typical federated learning scenarios, the optimization problem aims to minimize the global loss across all clients:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-65-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-824\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-825\"><span class=\"msubsup\" id=\"MathJax-Span-826\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-827\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-828\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-829\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-830\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-831\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-65\">F_k(w)</script> is the empirical loss on client <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-66-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-832\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-833\"><span class=\"mi\" id=\"MathJax-Span-834\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-66\">k</script>. However, in non-i.i.d. settings, minimizing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-67-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-835\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.35em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-836\"><span class=\"msubsup\" id=\"MathJax-Span-837\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-838\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"mi\" id=\"MathJax-Span-839\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-840\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-841\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-842\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>F</mi><mi>k</mi></msub><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-67\">F_k(w)</script> individually on each client may not lead to convergence of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-68-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-843\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.83em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-844\"><span class=\"mi\" id=\"MathJax-Span-845\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-846\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-847\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-848\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>f</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-68\">f(w)</script>.</li>\n    </ul>\n<p><strong>FedDyn</strong> introduces a dynamically updated regularization term in each client’s local objective to mitigate this discrepancy. Specifically, FedDyn modifies the local objective function as follows:</p>\n<ul>\n      <li>\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-70-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-893\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-894\"><span class=\"msubsup\" id=\"MathJax-Span-895\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-896\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-897\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-70\">w_t</script> is the global model at round <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-71-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-898\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-899\"><span class=\"mi\" id=\"MathJax-Span-900\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-71\">t</script>,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-72-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-901\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.66em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-902\"><span class=\"mi\" id=\"MathJax-Span-903\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"mi\" id=\"MathJax-Span-904\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-905\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-906\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-907\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-908\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-909\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">∇</mi><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-72\">\\nabla f(w_t)</script> is an approximation of the global gradient at <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-73-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-910\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-911\"><span class=\"msubsup\" id=\"MathJax-Span-912\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-913\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-914\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-73\">w_t</script>,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-74-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03BB;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-915\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-916\"><span class=\"mi\" id=\"MathJax-Span-917\" style=\"font-family: STIXGeneral-Italic;\">λ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>λ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-74\">\\lambda</script> is a regularization coefficient.</li>\n        </ul>\n      </li>\n    </ul>\n<p>where:</p>\n<ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-70-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-893\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-894\"><span class=\"msubsup\" id=\"MathJax-Span-895\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-896\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-897\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-70\">w_t</script> is the global model at round <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-71-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-898\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-899\"><span class=\"mi\" id=\"MathJax-Span-900\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-71\">t</script>,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-72-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x2207;</mi><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-901\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1002.66em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-902\"><span class=\"mi\" id=\"MathJax-Span-903\" style=\"font-family: STIXGeneral-Regular;\">∇</span><span class=\"mi\" id=\"MathJax-Span-904\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-905\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-906\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-907\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-908\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-909\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">∇</mi><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi>w</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-72\">\\nabla f(w_t)</script> is an approximation of the global gradient at <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-73-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-910\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-911\"><span class=\"msubsup\" id=\"MathJax-Span-912\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-913\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-914\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-73\">w_t</script>,</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-74-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03BB;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-915\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-916\"><span class=\"mi\" id=\"MathJax-Span-917\" style=\"font-family: STIXGeneral-Italic;\">λ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>λ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-74\">\\lambda</script> is a regularization coefficient.</li>\n        </ul>\n<p>This regularization acts as a <strong>bias-correction mechanism</strong>, aligning local descent directions with the global objective. By integrating both the history of updates and a proximal term, FedDyn ensures that each client’s optimization path stays consistent with the global descent trajectory.</p>\n<p><strong>Advantages</strong>:</p>\n<ul>\n      <li>Demonstrates <strong>convergence guarantees</strong> for non-convex losses under arbitrary heterogeneity assumptions.</li>\n      <li>Empirically outperforms other baselines such as FedAvg and FedProx on diverse datasets (e.g., CIFAR-10, FEMNIST, Shakespeare), especially under high data non-i.i.d.-ness.</li>\n      <li>The method reduces <strong>client drift</strong> and improves both convergence speed and final accuracy.</li>\n    </ul>\n<p><strong>FedDynOneGD</strong>:</p>\n<ul>\n      <li>A lightweight extension that performs <strong>only one gradient computation per client per round</strong>.</li>\n      <li>Reduces local computation from multiple SGD steps to a single pass, with <strong>linear complexity</strong> in the number of local examples.</li>\n      <li>Suitable for low-power or resource-constrained devices while retaining the convergence behavior of the full FedDyn formulation.</li>\n    </ul>\n<p>These innovations position FedDyn as a <strong>communication-efficient</strong>, <strong>robust</strong>, and <strong>heterogeneity-aware</strong> alternative for real-world federated learning deployments, where computational and statistical variance across clients is significant.</p>",
      "contentMarkdown": "*   Presented in [Federated Learning Based on Dynamic Regularization](https://openreview.net/forum?id=B7v4QMR6Z9w) by Acar et al. (2021), Federated Learning with Dynamic Regularization (FedDyn) addresses the core challenge of objective inconsistency between local client losses and the global optimization objective under data heterogeneity. In typical federated learning scenarios, the optimization problem aims to minimize the global loss across all clients:\n    \n    minw{f(w)\\=∑k\\=1KnknFk(w)}minw{f(w)\\=∑k\\=1KnknFk(w)}\n    \n    \\\\min\\_{w} \\\\left\\\\{ f(w) = \\\\sum\\_{k=1}^K \\\\frac{n\\_k}{n} F\\_k(w) \\\\right\\\\}\n    *   where Fk(w)Fk(w)F\\_k(w) is the empirical loss on client kkk. However, in non-i.i.d. settings, minimizing Fk(w)Fk(w)F\\_k(w) individually on each client may not lead to convergence of f(w)f(w)f(w).\n*   **FedDyn** introduces a dynamically updated regularization term in each client’s local objective to mitigate this discrepancy. Specifically, FedDyn modifies the local objective function as follows:\n    \n    F̃ k(w)\\=Fk(w)−⟨∇f(wt),w⟩+λ2‖w−wt‖2F~k(w)\\=Fk(w)−⟨∇f(wt),w⟩+λ2‖w−wt‖2\n    \n    \\\\tilde{F}\\_k(w) = F\\_k(w) - \\\\langle \\\\nabla f(w\\_t), w \\\\rangle + \\\\frac{\\\\lambda}{2} \\\\| w - w\\_t \\\\|^2\n    *   where:\n        \n        *   wtwtw\\_t is the global model at round ttt,\n        *   ∇f(wt)∇f(wt)\\\\nabla f(w\\_t) is an approximation of the global gradient at wtwtw\\_t,\n        *   λλ\\\\lambda is a regularization coefficient.\n*   This regularization acts as a **bias-correction mechanism**, aligning local descent directions with the global objective. By integrating both the history of updates and a proximal term, FedDyn ensures that each client’s optimization path stays consistent with the global descent trajectory.\n    \n*   **Advantages**:\n    \n    *   Demonstrates **convergence guarantees** for non-convex losses under arbitrary heterogeneity assumptions.\n    *   Empirically outperforms other baselines such as FedAvg and FedProx on diverse datasets (e.g., CIFAR-10, FEMNIST, Shakespeare), especially under high data non-i.i.d.-ness.\n    *   The method reduces **client drift** and improves both convergence speed and final accuracy.\n*   **FedDynOneGD**:\n    \n    *   A lightweight extension that performs **only one gradient computation per client per round**.\n    *   Reduces local computation from multiple SGD steps to a single pass, with **linear complexity** in the number of local examples.\n    *   Suitable for low-power or resource-constrained devices while retaining the convergence behavior of the full FedDyn formulation.\n*   These innovations position FedDyn as a **communication-efficient**, **robust**, and **heterogeneity-aware** alternative for real-world federated learning deployments, where computational and statistical variance across clients is significant.\n    \n\nPresented in [Federated Learning Based on Dynamic Regularization](https://openreview.net/forum?id=B7v4QMR6Z9w) by Acar et al. (2021), Federated Learning with Dynamic Regularization (FedDyn) addresses the core challenge of objective inconsistency between local client losses and the global optimization objective under data heterogeneity. In typical federated learning scenarios, the optimization problem aims to minimize the global loss across all clients:\n\n*   where Fk(w)Fk(w)F\\_k(w) is the empirical loss on client kkk. However, in non-i.i.d. settings, minimizing Fk(w)Fk(w)F\\_k(w) individually on each client may not lead to convergence of f(w)f(w)f(w).\n\n**FedDyn** introduces a dynamically updated regularization term in each client’s local objective to mitigate this discrepancy. Specifically, FedDyn modifies the local objective function as follows:\n\n*   where:\n    \n    *   wtwtw\\_t is the global model at round ttt,\n    *   ∇f(wt)∇f(wt)\\\\nabla f(w\\_t) is an approximation of the global gradient at wtwtw\\_t,\n    *   λλ\\\\lambda is a regularization coefficient.\n\nwhere:\n\n*   wtwtw\\_t is the global model at round ttt,\n*   ∇f(wt)∇f(wt)\\\\nabla f(w\\_t) is an approximation of the global gradient at wtwtw\\_t,\n*   λλ\\\\lambda is a regularization coefficient.\n\nThis regularization acts as a **bias-correction mechanism**, aligning local descent directions with the global objective. By integrating both the history of updates and a proximal term, FedDyn ensures that each client’s optimization path stays consistent with the global descent trajectory.\n\n**Advantages**:\n\n*   Demonstrates **convergence guarantees** for non-convex losses under arbitrary heterogeneity assumptions.\n*   Empirically outperforms other baselines such as FedAvg and FedProx on diverse datasets (e.g., CIFAR-10, FEMNIST, Shakespeare), especially under high data non-i.i.d.-ness.\n*   The method reduces **client drift** and improves both convergence speed and final accuracy.\n\n**FedDynOneGD**:\n\n*   A lightweight extension that performs **only one gradient computation per client per round**.\n*   Reduces local computation from multiple SGD steps to a single pass, with **linear complexity** in the number of local examples.\n*   Suitable for low-power or resource-constrained devices while retaining the convergence behavior of the full FedDyn formulation.\n\nThese innovations position FedDyn as a **communication-efficient**, **robust**, and **heterogeneity-aware** alternative for real-world federated learning deployments, where computational and statistical variance across clients is significant.",
      "order": 10,
      "orderInChapter": 6,
      "difficulty": 4,
      "estimatedMinutes": 4,
      "tags": [
        "ondevice ai",
        "optimization",
        "regularization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 665,
        "contentLength": 67748
      },
      "nextCards": [
        "ai-federated-learning-comparative-analysis-of-algorithmic-variants-11",
        "ai-federated-learning-communication-optimizations-12"
      ],
      "relatedCards": [
        "ai-on-device-transformers-hardware-specific-optimization-notes-16",
        "ai-model-compression-why-use-knowledge-distillation-instead-of-training-25",
        "ai-model-compression-low-rank-correction-for-quantization-45",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-model-compression-implementing-pruning-in-pytorch-and-tensorflow-39"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#feddyn",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-comparative-analysis-of-algorithmic-variants-11",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Federation Algorithms",
      "title": "Comparative Analysis of Algorithmic Variants",
      "subtitle": "Federation Algorithms",
      "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Algorithm</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Paper (Year)</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core Innovation</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Heterogeneity Robustness</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">FedSGD</td>\n<td class=\"tg-tleft-valign-first\">McMahan et al. 2017 (<a href=\"https://arxiv.org/abs/1602.05629\">arXiv</a>)</td>\n<td class=\"tg-tleft-valign-first\">Global gradient updates from full local datasets</td>\n<td class=\"tg-tleft-valign-second\">Low</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FedAvg</td>\n<td class=\"tg-tleft-valign-first\">McMahan et al. 2017 (<a href=\"https://flower.ai\">flower.ai</a>, <a href=\"https://en.wikipedia.org/wiki/Federated_learning\">Wikipedia</a>, <a href=\"https://arxiv.org/abs/1602.05629\">arXiv</a>)</td>\n<td class=\"tg-tleft-valign-first\">Simple weighted averaging of local models</td>\n<td class=\"tg-tleft-valign-second\">Moderate</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FedProx</td>\n<td class=\"tg-tleft-valign-first\">Li et al. 2018 (<a href=\"https://arxiv.org/abs/1812.06127\">arXiv</a>, <a href=\"https://proceedings.mlsys.org/paper/2020/file/b5d1a9f8b3d6f56c7ca1c7901ee7ba0c-Paper.pdf\">proceedings.mlsys.org</a>)</td>\n<td class=\"tg-tleft-valign-first\">Local proximal updates to limit drift</td>\n<td class=\"tg-tleft-valign-second\">Improved stability</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">SCAFFOLD</td>\n<td class=\"tg-tleft-valign-first\">Karimireddy et al. 2020 (<a href=\"https://arxiv.org/abs/1910.06378\">arXiv</a>, <a href=\"https://cs.nyu.edu/~yannakakis/FederatedLearning/SCAFFOLD.pdf\">cs.nyu.edu</a>)</td>\n<td class=\"tg-tleft-valign-first\">Control variate correction per client</td>\n<td class=\"tg-tleft-valign-second\">Very robust</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">MIME</td>\n<td class=\"tg-tleft-valign-first\">Karimireddy et al. ICLR 2021 (<a href=\"https://arxiv.org/abs/2008.02668\">arXiv</a>, <a href=\"https://openreview.net/forum?id=0cE7bXq5Gg\">OpenReview</a>)</td>\n<td class=\"tg-tleft-valign-first\">Mimic centralized optimizers with local drift correction</td>\n<td class=\"tg-tleft-valign-second\">Strong</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FedDyn</td>\n<td class=\"tg-tleft-valign-first\">Acar et al. ICLR 2021 (<a href=\"https://arxiv.org/abs/2006.07265\">arXiv</a>, <a href=\"https://openreview.net/forum?id=B7v4QMR6Z9w\">OpenReview</a>)</td>\n<td class=\"tg-tleft-valign-first\">Dynamic loss regularization per-device</td>\n<td class=\"tg-tleft-valign-second\">High robustness</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Algorithm</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Paper (Year)</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core Innovation</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Heterogeneity Robustness</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">FedSGD</td>\n<td class=\"tg-tleft-valign-first\">McMahan et al. 2017 (<a href=\"https://arxiv.org/abs/1602.05629\">arXiv</a>)</td>\n<td class=\"tg-tleft-valign-first\">Global gradient updates from full local datasets</td>\n<td class=\"tg-tleft-valign-second\">Low</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FedAvg</td>\n<td class=\"tg-tleft-valign-first\">McMahan et al. 2017 (<a href=\"https://flower.ai\">flower.ai</a>, <a href=\"https://en.wikipedia.org/wiki/Federated_learning\">Wikipedia</a>, <a href=\"https://arxiv.org/abs/1602.05629\">arXiv</a>)</td>\n<td class=\"tg-tleft-valign-first\">Simple weighted averaging of local models</td>\n<td class=\"tg-tleft-valign-second\">Moderate</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FedProx</td>\n<td class=\"tg-tleft-valign-first\">Li et al. 2018 (<a href=\"https://arxiv.org/abs/1812.06127\">arXiv</a>, <a href=\"https://proceedings.mlsys.org/paper/2020/file/b5d1a9f8b3d6f56c7ca1c7901ee7ba0c-Paper.pdf\">proceedings.mlsys.org</a>)</td>\n<td class=\"tg-tleft-valign-first\">Local proximal updates to limit drift</td>\n<td class=\"tg-tleft-valign-second\">Improved stability</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">SCAFFOLD</td>\n<td class=\"tg-tleft-valign-first\">Karimireddy et al. 2020 (<a href=\"https://arxiv.org/abs/1910.06378\">arXiv</a>, <a href=\"https://cs.nyu.edu/~yannakakis/FederatedLearning/SCAFFOLD.pdf\">cs.nyu.edu</a>)</td>\n<td class=\"tg-tleft-valign-first\">Control variate correction per client</td>\n<td class=\"tg-tleft-valign-second\">Very robust</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">MIME</td>\n<td class=\"tg-tleft-valign-first\">Karimireddy et al. ICLR 2021 (<a href=\"https://arxiv.org/abs/2008.02668\">arXiv</a>, <a href=\"https://openreview.net/forum?id=0cE7bXq5Gg\">OpenReview</a>)</td>\n<td class=\"tg-tleft-valign-first\">Mimic centralized optimizers with local drift correction</td>\n<td class=\"tg-tleft-valign-second\">Strong</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FedDyn</td>\n<td class=\"tg-tleft-valign-first\">Acar et al. ICLR 2021 (<a href=\"https://arxiv.org/abs/2006.07265\">arXiv</a>, <a href=\"https://openreview.net/forum?id=B7v4QMR6Z9w\">OpenReview</a>)</td>\n<td class=\"tg-tleft-valign-first\">Dynamic loss regularization per-device</td>\n<td class=\"tg-tleft-valign-second\">High robustness</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "**Algorithm**\n\n**Paper (Year)**\n\n**Core Innovation**\n\n**Heterogeneity Robustness**\n\nFedSGD\n\nMcMahan et al. 2017 ([arXiv](https://arxiv.org/abs/1602.05629))\n\nGlobal gradient updates from full local datasets\n\nLow\n\nFedAvg\n\nMcMahan et al. 2017 ([flower.ai](https://flower.ai), [Wikipedia](https://en.wikipedia.org/wiki/Federated_learning), [arXiv](https://arxiv.org/abs/1602.05629))\n\nSimple weighted averaging of local models\n\nModerate\n\nFedProx\n\nLi et al. 2018 ([arXiv](https://arxiv.org/abs/1812.06127), [proceedings.mlsys.org](https://proceedings.mlsys.org/paper/2020/file/b5d1a9f8b3d6f56c7ca1c7901ee7ba0c-Paper.pdf))\n\nLocal proximal updates to limit drift\n\nImproved stability\n\nSCAFFOLD\n\nKarimireddy et al. 2020 ([arXiv](https://arxiv.org/abs/1910.06378), [cs.nyu.edu](https://cs.nyu.edu/~yannakakis/FederatedLearning/SCAFFOLD.pdf))\n\nControl variate correction per client\n\nVery robust\n\nMIME\n\nKarimireddy et al. ICLR 2021 ([arXiv](https://arxiv.org/abs/2008.02668), [OpenReview](https://openreview.net/forum?id=0cE7bXq5Gg))\n\nMimic centralized optimizers with local drift correction\n\nStrong\n\nFedDyn\n\nAcar et al. ICLR 2021 ([arXiv](https://arxiv.org/abs/2006.07265), [OpenReview](https://openreview.net/forum?id=B7v4QMR6Z9w))\n\nDynamic loss regularization per-device\n\nHigh robustness\n\n**Algorithm**\n\n**Paper (Year)**\n\n**Core Innovation**\n\n**Heterogeneity Robustness**\n\nFedSGD\n\nMcMahan et al. 2017 ([arXiv](https://arxiv.org/abs/1602.05629))\n\nGlobal gradient updates from full local datasets\n\nLow\n\nFedAvg\n\nMcMahan et al. 2017 ([flower.ai](https://flower.ai), [Wikipedia](https://en.wikipedia.org/wiki/Federated_learning), [arXiv](https://arxiv.org/abs/1602.05629))\n\nSimple weighted averaging of local models\n\nModerate\n\nFedProx\n\nLi et al. 2018 ([arXiv](https://arxiv.org/abs/1812.06127), [proceedings.mlsys.org](https://proceedings.mlsys.org/paper/2020/file/b5d1a9f8b3d6f56c7ca1c7901ee7ba0c-Paper.pdf))\n\nLocal proximal updates to limit drift\n\nImproved stability\n\nSCAFFOLD\n\nKarimireddy et al. 2020 ([arXiv](https://arxiv.org/abs/1910.06378), [cs.nyu.edu](https://cs.nyu.edu/~yannakakis/FederatedLearning/SCAFFOLD.pdf))\n\nControl variate correction per client\n\nVery robust\n\nMIME\n\nKarimireddy et al. ICLR 2021 ([arXiv](https://arxiv.org/abs/2008.02668), [OpenReview](https://openreview.net/forum?id=0cE7bXq5Gg))\n\nMimic centralized optimizers with local drift correction\n\nStrong\n\nFedDyn\n\nAcar et al. ICLR 2021 ([arXiv](https://arxiv.org/abs/2006.07265), [OpenReview](https://openreview.net/forum?id=B7v4QMR6Z9w))\n\nDynamic loss regularization per-device\n\nHigh robustness",
      "order": 11,
      "orderInChapter": 7,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "regularization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 190,
        "contentLength": 5389
      },
      "nextCards": [
        "ai-federated-learning-communication-optimizations-12",
        "ai-federated-learning-privacy-security-13"
      ],
      "relatedCards": [
        "ai-model-compression-why-use-knowledge-distillation-instead-of-training-25",
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#comparative-analysis-of-algorithmic-variants",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-communication-optimizations-12",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Implementation Details",
      "title": "Communication Optimizations",
      "subtitle": "Implementation Details",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Compression &amp; Sparsification</strong>:\nTransmit only top‑k updates (e.g., 1%–10% of gradients), apply quantization (e.g., 8‑bit or ternary encoding), or use sketching techniques like Count Sketch to reduce message size.</p>\n  </li>\n  <li>\n    <p><strong>Asynchronous Updates</strong>:\nIn setups with highly variable client response times, asynchronous FL enables continuous server-side aggregation without stalling. This requires additional mechanisms to ensure consistency (e.g., staleness-aware updates).</p>\n  </li>\n  <li>\n    <p><strong>Dropout Resilience</strong>:\nEnsure robustness to client churn by training with randomly dropping clients each round. This also regularizes the model and avoids overfitting to certain users.</p>\n  </li>\n</ul>\n<p><strong>Compression &amp; Sparsification</strong>:\nTransmit only top‑k updates (e.g., 1%–10% of gradients), apply quantization (e.g., 8‑bit or ternary encoding), or use sketching techniques like Count Sketch to reduce message size.</p>\n<p><strong>Asynchronous Updates</strong>:\nIn setups with highly variable client response times, asynchronous FL enables continuous server-side aggregation without stalling. This requires additional mechanisms to ensure consistency (e.g., staleness-aware updates).</p>\n<p><strong>Dropout Resilience</strong>:\nEnsure robustness to client churn by training with randomly dropping clients each round. This also regularizes the model and avoids overfitting to certain users.</p>",
      "contentMarkdown": "*   **Compression & Sparsification**: Transmit only top‑k updates (e.g., 1%–10% of gradients), apply quantization (e.g., 8‑bit or ternary encoding), or use sketching techniques like Count Sketch to reduce message size.\n    \n*   **Asynchronous Updates**: In setups with highly variable client response times, asynchronous FL enables continuous server-side aggregation without stalling. This requires additional mechanisms to ensure consistency (e.g., staleness-aware updates).\n    \n*   **Dropout Resilience**: Ensure robustness to client churn by training with randomly dropping clients each round. This also regularizes the model and avoids overfitting to certain users.\n    \n\n**Compression & Sparsification**: Transmit only top‑k updates (e.g., 1%–10% of gradients), apply quantization (e.g., 8‑bit or ternary encoding), or use sketching techniques like Count Sketch to reduce message size.\n\n**Asynchronous Updates**: In setups with highly variable client response times, asynchronous FL enables continuous server-side aggregation without stalling. This requires additional mechanisms to ensure consistency (e.g., staleness-aware updates).\n\n**Dropout Resilience**: Ensure robustness to client churn by training with randomly dropping clients each round. This also regularizes the model and avoids overfitting to certain users.",
      "order": 12,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "dropout"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 169,
        "contentLength": 1493
      },
      "nextCards": [
        "ai-federated-learning-privacy-security-13",
        "ai-federated-learning-tooling-frameworks-14"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-differential-privacy-components-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#communication-optimizations",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-privacy-security-13",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Implementation Details",
      "title": "Privacy & Security",
      "subtitle": "Implementation Details",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Differential Privacy (DP-FL)</strong>:\nCombine local gradient clipping with Gaussian noise (via DP-SGD) to bound each client’s contribution to the global model. This yields per‑round guarantees of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-75-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>&amp;#x03B5;</mi><mo>,</mo><mi>&amp;#x03B4;</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-918\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-919\"><span class=\"mo\" id=\"MathJax-Span-920\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-921\" style=\"font-family: STIXGeneral-Italic;\">ε</span><span class=\"mo\" id=\"MathJax-Span-922\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-923\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">δ</span><span class=\"mo\" id=\"MathJax-Span-924\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>ε</mi><mo>,</mo><mi>δ</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-75\">(\\varepsilon, \\delta)</script>-DP, with accounting accumulated over rounds.</p>\n  </li>\n  <li>\n    <p><strong>Secure Aggregation</strong>:\nUse cryptographic techniques (e.g., homomorphic encryption, Shamir’s secret sharing) so that the server can only observe the sum of client updates, not any individual update. Google’s protocol, proposed in <a href=\"https://eprint.iacr.org/2017/281\">Practical Secure Aggregation for Privacy Preserving Machine Learning</a> by Bonawitz et al. (2017).</p>\n  </li>\n  <li>\n    <p><strong>Anomaly Detection</strong>:\nApply outlier detection or robust aggregation (e.g., median, trimmed mean, Krum) to defend against poisoned or adversarial updates.</p>\n  </li>\n</ul>\n<p><strong>Differential Privacy (DP-FL)</strong>:\nCombine local gradient clipping with Gaussian noise (via DP-SGD) to bound each client’s contribution to the global model. This yields per‑round guarantees of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-75-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>&amp;#x03B5;</mi><mo>,</mo><mi>&amp;#x03B4;</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-918\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-919\"><span class=\"mo\" id=\"MathJax-Span-920\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-921\" style=\"font-family: STIXGeneral-Italic;\">ε</span><span class=\"mo\" id=\"MathJax-Span-922\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-923\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">δ</span><span class=\"mo\" id=\"MathJax-Span-924\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>ε</mi><mo>,</mo><mi>δ</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-75\">(\\varepsilon, \\delta)</script>-DP, with accounting accumulated over rounds.</p>\n<p><strong>Secure Aggregation</strong>:\nUse cryptographic techniques (e.g., homomorphic encryption, Shamir’s secret sharing) so that the server can only observe the sum of client updates, not any individual update. Google’s protocol, proposed in <a href=\"https://eprint.iacr.org/2017/281\">Practical Secure Aggregation for Privacy Preserving Machine Learning</a> by Bonawitz et al. (2017).</p>\n<p><strong>Anomaly Detection</strong>:\nApply outlier detection or robust aggregation (e.g., median, trimmed mean, Krum) to defend against poisoned or adversarial updates.</p>",
      "contentMarkdown": "*   **Differential Privacy (DP-FL)**: Combine local gradient clipping with Gaussian noise (via DP-SGD) to bound each client’s contribution to the global model. This yields per‑round guarantees of (ε,δ)(ε,δ)(\\\\varepsilon, \\\\delta)\\-DP, with accounting accumulated over rounds.\n    \n*   **Secure Aggregation**: Use cryptographic techniques (e.g., homomorphic encryption, Shamir’s secret sharing) so that the server can only observe the sum of client updates, not any individual update. Google’s protocol, proposed in [Practical Secure Aggregation for Privacy Preserving Machine Learning](https://eprint.iacr.org/2017/281) by Bonawitz et al. (2017).\n    \n*   **Anomaly Detection**: Apply outlier detection or robust aggregation (e.g., median, trimmed mean, Krum) to defend against poisoned or adversarial updates.\n    \n\n**Differential Privacy (DP-FL)**: Combine local gradient clipping with Gaussian noise (via DP-SGD) to bound each client’s contribution to the global model. This yields per‑round guarantees of (ε,δ)(ε,δ)(\\\\varepsilon, \\\\delta)\\-DP, with accounting accumulated over rounds.\n\n**Secure Aggregation**: Use cryptographic techniques (e.g., homomorphic encryption, Shamir’s secret sharing) so that the server can only observe the sum of client updates, not any individual update. Google’s protocol, proposed in [Practical Secure Aggregation for Privacy Preserving Machine Learning](https://eprint.iacr.org/2017/281) by Bonawitz et al. (2017).\n\n**Anomaly Detection**: Apply outlier detection or robust aggregation (e.g., median, trimmed mean, Krum) to defend against poisoned or adversarial updates.",
      "order": 13,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "machine learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 197,
        "contentLength": 5253
      },
      "nextCards": [
        "ai-federated-learning-tooling-frameworks-14",
        "ai-federated-learning-llm-specific-enhancements-15"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#privacy-&-security",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-tooling-frameworks-14",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Implementation Details",
      "title": "Tooling & Frameworks",
      "subtitle": "Implementation Details",
      "contentHtml": "<ul>\n  <li><strong>Flower</strong>: Flexible framework supporting simulation and real-world deployment, compatible with PyTorch, TensorFlow, JAX.</li>\n  <li><strong>FATE</strong>: Industrial-grade system supporting hetero‑FL, secure computation, and cross‑silo scenarios.</li>\n  <li><strong>TensorFlow Federated</strong>: Google’s research library integrating TF models into FL workflows.</li>\n  <li><strong>PySyft</strong>: Privacy-preserving framework supporting FL, DP, and encrypted ML pipelines.</li>\n</ul>",
      "contentMarkdown": "*   **Flower**: Flexible framework supporting simulation and real-world deployment, compatible with PyTorch, TensorFlow, JAX.\n*   **FATE**: Industrial-grade system supporting hetero‑FL, secure computation, and cross‑silo scenarios.\n*   **TensorFlow Federated**: Google’s research library integrating TF models into FL workflows.\n*   **PySyft**: Privacy-preserving framework supporting FL, DP, and encrypted ML pipelines.",
      "order": 14,
      "orderInChapter": 3,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 48,
        "contentLength": 511
      },
      "nextCards": [
        "ai-federated-learning-llm-specific-enhancements-15",
        "ai-federated-learning-what-is-federated-lora-16"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-differential-privacy-components-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#tooling-&-frameworks",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-llm-specific-enhancements-15",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Implementation Details",
      "title": "LLM-Specific Enhancements",
      "subtitle": "Implementation Details",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>LoRA-based Fine-Tuning</strong>:\nReduces communication and computation by 10×–100×. Each client only updates and shares adapter weights (e.g., &lt;1% of full model).</p>\n  </li>\n  <li>\n    <p><strong>Client Selection &amp; Ranking</strong>:\nClients are often selected based on availability, compute, network, and data quality. Ranking heuristics (e.g., data diversity scores) can further improve convergence.</p>\n  </li>\n  <li>\n    <p><strong>Personalization Layers</strong>:\nClients can locally adapt final model layers post-FL (e.g., bias or head-only fine-tuning) for user-specific performance while still contributing to a shared global backbone.</p>\n  </li>\n</ul>\n<p><strong>LoRA-based Fine-Tuning</strong>:\nReduces communication and computation by 10×–100×. Each client only updates and shares adapter weights (e.g., &lt;1% of full model).</p>\n<p><strong>Client Selection &amp; Ranking</strong>:\nClients are often selected based on availability, compute, network, and data quality. Ranking heuristics (e.g., data diversity scores) can further improve convergence.</p>\n<p><strong>Personalization Layers</strong>:\nClients can locally adapt final model layers post-FL (e.g., bias or head-only fine-tuning) for user-specific performance while still contributing to a shared global backbone.</p>",
      "contentMarkdown": "*   **LoRA-based Fine-Tuning**: Reduces communication and computation by 10×–100×. Each client only updates and shares adapter weights (e.g., <1% of full model).\n    \n*   **Client Selection & Ranking**: Clients are often selected based on availability, compute, network, and data quality. Ranking heuristics (e.g., data diversity scores) can further improve convergence.\n    \n*   **Personalization Layers**: Clients can locally adapt final model layers post-FL (e.g., bias or head-only fine-tuning) for user-specific performance while still contributing to a shared global backbone.\n    \n\n**LoRA-based Fine-Tuning**: Reduces communication and computation by 10×–100×. Each client only updates and shares adapter weights (e.g., <1% of full model).\n\n**Client Selection & Ranking**: Clients are often selected based on availability, compute, network, and data quality. Ranking heuristics (e.g., data diversity scores) can further improve convergence.\n\n**Personalization Layers**: Clients can locally adapt final model layers post-FL (e.g., bias or head-only fine-tuning) for user-specific performance while still contributing to a shared global backbone.",
      "order": 15,
      "orderInChapter": 4,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 149,
        "contentLength": 1323
      },
      "nextCards": [
        "ai-federated-learning-what-is-federated-lora-16",
        "ai-federated-learning-lora-in-federated-context-17"
      ],
      "relatedCards": [
        "ai-model-compression-pros-cons-47",
        "ai-model-compression-pruning-workflow-35",
        "ai-model-compression-concept-44",
        "ai-model-compression-quantized-low-rank-adaptation-techniques-46",
        "ai-model-compression-key-takeaways-49"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#llm-specific-enhancements",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-what-is-federated-lora-16",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Federated Adaptation with LoRA",
      "title": "What is Federated LoRA?",
      "subtitle": "Federated Adaptation with LoRA",
      "contentHtml": "<ul>\n  <li>\n    <p>In <strong>Federated LoRA</strong>, each client receives a frozen copy of the global LLM and fine-tunes only the inserted low-rank matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-76-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-925\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-926\"><span class=\"mi\" id=\"MathJax-Span-927\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-76\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-77-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-928\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-929\"><span class=\"mi\" id=\"MathJax-Span-930\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-77\">B</script> at specific layers:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-78-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0394;</mi><mi>W</mi><mo>&amp;#x2248;</mo><mi>A</mi><mi>B</mi><mspace width=&quot;1em&quot; /><mtext>where&amp;#xA0;</mtext><mi>A</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></mrow></msup><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>B</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>r</mi><mo>&amp;#x00D7;</mo><mi>k</mi></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-931\" style=\"width: 19.794em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 16.461em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1016.46em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-932\"><span class=\"mi\" id=\"MathJax-Span-933\" style=\"font-family: STIXGeneral-Regular;\">Δ</span><span class=\"mi\" id=\"MathJax-Span-934\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-935\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mi\" id=\"MathJax-Span-936\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">A</span><span class=\"mi\" id=\"MathJax-Span-937\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mspace\" id=\"MathJax-Span-938\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mtext\" id=\"MathJax-Span-939\" style=\"font-family: STIXGeneral-Regular;\">where&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-940\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-941\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-942\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-943\"><span class=\"mrow\" id=\"MathJax-Span-944\"><span class=\"mi\" id=\"MathJax-Span-945\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-946\"><span class=\"mrow\" id=\"MathJax-Span-947\"><span class=\"mi\" id=\"MathJax-Span-948\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-949\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-950\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-951\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-952\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-953\" style=\"font-family: STIXGeneral-Italic;\">B</span><span class=\"mo\" id=\"MathJax-Span-954\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-955\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-956\"><span class=\"mrow\" id=\"MathJax-Span-957\"><span class=\"mi\" id=\"MathJax-Span-958\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-959\"><span class=\"mrow\" id=\"MathJax-Span-960\"><span class=\"mi\" id=\"MathJax-Span-961\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-962\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-963\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi mathvariant=\"normal\">Δ</mi><mi>W</mi><mo>≈</mo><mi>A</mi><mi>B</mi><mspace width=\"1em\"></mspace><mtext>where&nbsp;</mtext><mi>A</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup><mo>,</mo><mtext>&nbsp;</mtext><mi>B</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>r</mi><mo>×</mo><mi>k</mi></mrow></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-78\">\\Delta W \\approx A B \\quad \\text{where } A \\in \\mathbb{R}^{d \\times r},\\ B \\in \\mathbb{R}^{r \\times k}</script>\n  </li>\n  <li>\n    <p>These low-rank updates (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-79-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-964\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-965\"><span class=\"mi\" id=\"MathJax-Span-966\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mi\" id=\"MathJax-Span-967\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-79\">AB</script>) are then communicated back to the central server, drastically reducing bandwidth and memory usage.</p>\n  </li>\n  <li>\n    <p><strong>Advantages</strong>:</p>\n\n    <ul>\n      <li>10×–100× fewer parameters shared per client.</li>\n      <li>Easily pluggable into existing LLMs (e.g., GPT, BERT, LLaMA).</li>\n      <li>Empirically shown to converge well even under client heterogeneity.</li>\n    </ul>\n  </li>\n</ul>\n<p>In <strong>Federated LoRA</strong>, each client receives a frozen copy of the global LLM and fine-tunes only the inserted low-rank matrices <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-76-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-925\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-926\"><span class=\"mi\" id=\"MathJax-Span-927\" style=\"font-family: STIXGeneral-Italic;\">A</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-76\">A</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-77-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-928\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-929\"><span class=\"mi\" id=\"MathJax-Span-930\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-77\">B</script> at specific layers:</p>\n<p>These low-rank updates (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-79-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-964\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.41em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-965\"><span class=\"mi\" id=\"MathJax-Span-966\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mi\" id=\"MathJax-Span-967\" style=\"font-family: STIXGeneral-Italic;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-79\">AB</script>) are then communicated back to the central server, drastically reducing bandwidth and memory usage.</p>\n<p><strong>Advantages</strong>:</p>\n<ul>\n      <li>10×–100× fewer parameters shared per client.</li>\n      <li>Easily pluggable into existing LLMs (e.g., GPT, BERT, LLaMA).</li>\n      <li>Empirically shown to converge well even under client heterogeneity.</li>\n    </ul>",
      "contentMarkdown": "*   In **Federated LoRA**, each client receives a frozen copy of the global LLM and fine-tunes only the inserted low-rank matrices AAA and BBB at specific layers:\n    \n    ΔW≈ABwhere A∈ℝd×r, B∈ℝr×kΔW≈ABwhere A∈Rd×r, B∈Rr×k\n    \n    \\\\Delta W \\\\approx A B \\\\quad \\\\text{where } A \\\\in \\\\mathbb{R}^{d \\\\times r},\\\\ B \\\\in \\\\mathbb{R}^{r \\\\times k}\n*   These low-rank updates (ABABAB) are then communicated back to the central server, drastically reducing bandwidth and memory usage.\n    \n*   **Advantages**:\n    \n    *   10×–100× fewer parameters shared per client.\n    *   Easily pluggable into existing LLMs (e.g., GPT, BERT, LLaMA).\n    *   Empirically shown to converge well even under client heterogeneity.\n\nIn **Federated LoRA**, each client receives a frozen copy of the global LLM and fine-tunes only the inserted low-rank matrices AAA and BBB at specific layers:\n\nThese low-rank updates (ABABAB) are then communicated back to the central server, drastically reducing bandwidth and memory usage.\n\n**Advantages**:\n\n*   10×–100× fewer parameters shared per client.\n*   Easily pluggable into existing LLMs (e.g., GPT, BERT, LLaMA).\n*   Empirically shown to converge well even under client heterogeneity.",
      "order": 16,
      "orderInChapter": 1,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "bert",
        "gpt",
        "llm"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 170,
        "contentLength": 15054
      },
      "nextCards": [
        "ai-federated-learning-lora-in-federated-context-17",
        "ai-federated-learning-open-challenges-18"
      ],
      "relatedCards": [
        "ai-on-device-transformers-sequence-length-and-kv-cache-size-24",
        "ai-on-device-transformers-tokenizer-and-vocabulary-size-22",
        "ai-on-device-transformers-parameter-count-and-model-depth-25",
        "ai-model-compression-types-of-knowledge-distillation-23",
        "ai-model-compression-concept-44"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#what-is-federated-lora?",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-lora-in-federated-context-17",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Federated Adaptation with LoRA",
      "title": "LoRA in Federated Context",
      "subtitle": "Federated Adaptation with LoRA",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Federated LoRA Workflow</strong>:</p>\n\n    <ol>\n      <li>Server distributes frozen base model and LoRA config (e.g., rank <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-80-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-968\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-969\"><span class=\"mi\" id=\"MathJax-Span-970\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-80\">r</script>, init values).</li>\n      <li>Clients fine-tune LoRA adapters locally on their data.</li>\n      <li>Clients send only the LoRA parameters <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-81-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B8;</mi><mi>A</mi></msub><mo>,</mo><msub><mi>&amp;#x03B8;</mi><mi>B</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-971\" style=\"width: 2.971em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.451em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.45em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-972\"><span class=\"msubsup\" id=\"MathJax-Span-973\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-974\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-975\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">A</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-976\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-977\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-978\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-979\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">B</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>θ</mi><mi>A</mi></msub><mo>,</mo><msub><mi>θ</mi><mi>B</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-81\">\\theta_A, \\theta_B</script> to the server.</li>\n      <li>Server aggregates adapters and updates the global model (or broadcasts updated adapters).</li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Security</strong>:</p>\n\n    <ul>\n      <li>LoRA matrices are much smaller, reducing exposure in the event of leaks.</li>\n      <li>Combine with <strong>secure aggregation</strong> or <strong>differential privacy</strong> for formal guarantees.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Federated LoRA Workflow</strong>:</p>\n<ol>\n      <li>Server distributes frozen base model and LoRA config (e.g., rank <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-80-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-968\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-969\"><span class=\"mi\" id=\"MathJax-Span-970\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-80\">r</script>, init values).</li>\n      <li>Clients fine-tune LoRA adapters locally on their data.</li>\n      <li>Clients send only the LoRA parameters <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-81-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B8;</mi><mi>A</mi></msub><mo>,</mo><msub><mi>&amp;#x03B8;</mi><mi>B</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-971\" style=\"width: 2.971em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.451em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.45em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-972\"><span class=\"msubsup\" id=\"MathJax-Span-973\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-974\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-975\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">A</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-976\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-977\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-978\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-979\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">B</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>θ</mi><mi>A</mi></msub><mo>,</mo><msub><mi>θ</mi><mi>B</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-81\">\\theta_A, \\theta_B</script> to the server.</li>\n      <li>Server aggregates adapters and updates the global model (or broadcasts updated adapters).</li>\n    </ol>\n<p><strong>Security</strong>:</p>\n<ul>\n      <li>LoRA matrices are much smaller, reducing exposure in the event of leaks.</li>\n      <li>Combine with <strong>secure aggregation</strong> or <strong>differential privacy</strong> for formal guarantees.</li>\n    </ul>",
      "contentMarkdown": "*   **Federated LoRA Workflow**:\n    \n    1.  Server distributes frozen base model and LoRA config (e.g., rank rrr, init values).\n    2.  Clients fine-tune LoRA adapters locally on their data.\n    3.  Clients send only the LoRA parameters θA,θBθA,θB\\\\theta\\_A, \\\\theta\\_B to the server.\n    4.  Server aggregates adapters and updates the global model (or broadcasts updated adapters).\n*   **Security**:\n    \n    *   LoRA matrices are much smaller, reducing exposure in the event of leaks.\n    *   Combine with **secure aggregation** or **differential privacy** for formal guarantees.\n\n**Federated LoRA Workflow**:\n\n1.  Server distributes frozen base model and LoRA config (e.g., rank rrr, init values).\n2.  Clients fine-tune LoRA adapters locally on their data.\n3.  Clients send only the LoRA parameters θA,θBθA,θB\\\\theta\\_A, \\\\theta\\_B to the server.\n4.  Server aggregates adapters and updates the global model (or broadcasts updated adapters).\n\n**Security**:\n\n*   LoRA matrices are much smaller, reducing exposure in the event of leaks.\n*   Combine with **secure aggregation** or **differential privacy** for formal guarantees.",
      "order": 17,
      "orderInChapter": 2,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 154,
        "contentLength": 9760
      },
      "nextCards": [
        "ai-federated-learning-open-challenges-18",
        "ai-federated-learning-pros-cons-19"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#lora-in-federated-context",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-open-challenges-18",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Federated Adaptation with LoRA",
      "title": "Open Challenges",
      "subtitle": "Federated Adaptation with LoRA",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Heterogeneous adapter ranks</strong>:\nClients with different compute budgets may choose different ranks, destabilizing aggregation.</p>\n  </li>\n  <li>\n    <p><strong>Partial participation</strong>:\nAggregation of sparse LoRA updates needs careful design to avoid overfitting to overrepresented clients.</p>\n  </li>\n  <li>\n    <p><strong>Adapter merging</strong>:\nWhen merging LoRA adapters into the base model post-FL, special care must be taken to balance global and client-specific knowledge.</p>\n  </li>\n</ul>\n<p><strong>Heterogeneous adapter ranks</strong>:\nClients with different compute budgets may choose different ranks, destabilizing aggregation.</p>\n<p><strong>Partial participation</strong>:\nAggregation of sparse LoRA updates needs careful design to avoid overfitting to overrepresented clients.</p>\n<p><strong>Adapter merging</strong>:\nWhen merging LoRA adapters into the base model post-FL, special care must be taken to balance global and client-specific knowledge.</p>",
      "contentMarkdown": "*   **Heterogeneous adapter ranks**: Clients with different compute budgets may choose different ranks, destabilizing aggregation.\n    \n*   **Partial participation**: Aggregation of sparse LoRA updates needs careful design to avoid overfitting to overrepresented clients.\n    \n*   **Adapter merging**: When merging LoRA adapters into the base model post-FL, special care must be taken to balance global and client-specific knowledge.\n    \n\n**Heterogeneous adapter ranks**: Clients with different compute budgets may choose different ranks, destabilizing aggregation.\n\n**Partial participation**: Aggregation of sparse LoRA updates needs careful design to avoid overfitting to overrepresented clients.\n\n**Adapter merging**: When merging LoRA adapters into the base model post-FL, special care must be taken to balance global and client-specific knowledge.",
      "order": 18,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 107,
        "contentLength": 1011
      },
      "nextCards": [
        "ai-federated-learning-pros-cons-19",
        "ai-federated-learning-comparison-use-cases-20"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#open-challenges",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-pros-cons-19",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Federated Adaptation with LoRA",
      "title": "Pros & Cons",
      "subtitle": "Federated Adaptation with LoRA",
      "contentHtml": "<h4 id=\"pros\">Pros</h4>\n<ul>\n  <li>\n    <p><strong>Parameter-efficient fine-tuning</strong>: All LoRA-based techniques (LoRA, QLoRA, QA-LoRA, LQ-LoRA) fine-tune only a small set of low-rank parameters, making them ideal for FL environments with limited bandwidth.</p>\n  </li>\n  <li>\n    <p><strong>Adaptable to quantization settings</strong>: LoRA variants like <strong>QLoRA</strong> and <strong>QA-LoRA</strong> support aggressive quantization (e.g., 4-bit), allowing fine-tuning with reduced memory and compute requirements—especially valuable in client-side FL scenarios.</p>\n  </li>\n  <li>\n    <p><strong>Scalable across clients</strong>: Clients send only adapter weights (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-82-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>,</mo><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-980\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.88em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-981\"><span class=\"mi\" id=\"MathJax-Span-982\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-983\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-984\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>,</mo><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-82\">A, B</script>) rather than full models, reducing communication costs by over 90%. This makes federated tuning feasible even on mobile devices.</p>\n  </li>\n  <li>\n    <p><strong>Privacy-enhancing</strong>: By avoiding transmission of full model weights, and optionally combining with <strong>DP-FL</strong>, LoRA-based fine-tuning helps achieve stronger privacy guarantees.</p>\n  </li>\n  <li>\n    <p><strong>Accuracy retention</strong>: LQ-LoRA and QA-LoRA correct quantization-induced degradation via low-rank compensation, maintaining fidelity even at 4-bit or 2-bit precision.</p>\n  </li>\n</ul>\n<p><strong>Parameter-efficient fine-tuning</strong>: All LoRA-based techniques (LoRA, QLoRA, QA-LoRA, LQ-LoRA) fine-tune only a small set of low-rank parameters, making them ideal for FL environments with limited bandwidth.</p>\n<p><strong>Adaptable to quantization settings</strong>: LoRA variants like <strong>QLoRA</strong> and <strong>QA-LoRA</strong> support aggressive quantization (e.g., 4-bit), allowing fine-tuning with reduced memory and compute requirements—especially valuable in client-side FL scenarios.</p>\n<p><strong>Scalable across clients</strong>: Clients send only adapter weights (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-82-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>,</mo><mi>B</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-980\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.88em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-981\"><span class=\"mi\" id=\"MathJax-Span-982\" style=\"font-family: STIXGeneral-Italic;\">A</span><span class=\"mo\" id=\"MathJax-Span-983\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-984\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">B</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>A</mi><mo>,</mo><mi>B</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-82\">A, B</script>) rather than full models, reducing communication costs by over 90%. This makes federated tuning feasible even on mobile devices.</p>\n<p><strong>Privacy-enhancing</strong>: By avoiding transmission of full model weights, and optionally combining with <strong>DP-FL</strong>, LoRA-based fine-tuning helps achieve stronger privacy guarantees.</p>\n<p><strong>Accuracy retention</strong>: LQ-LoRA and QA-LoRA correct quantization-induced degradation via low-rank compensation, maintaining fidelity even at 4-bit or 2-bit precision.</p>\n<h4 id=\"cons\">Cons</h4>\n<ul>\n  <li>\n    <p><strong>Hyperparameter sensitivity</strong>: LoRA’s performance depends on careful tuning of the rank <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-83-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-985\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-986\"><span class=\"mi\" id=\"MathJax-Span-987\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-83\">r</script>, learning rate, and initialization. This gets harder in FL settings with diverse clients.</p>\n  </li>\n  <li>\n    <p><strong>Adapter coordination</strong>: Clients may use different adapter ranks or sparsity patterns, which complicates aggregation of LoRA updates.</p>\n  </li>\n  <li>\n    <p><strong>Quantization complexity</strong>: Variants like LQ-LoRA and QA-LoRA introduce additional steps—e.g., quantizer calibration, activation scaling—which increase implementation burden.</p>\n  </li>\n  <li>\n    <p><strong>Stability under heterogeneity</strong>: In FL, device-specific data distributions may lead to divergent adapter updates, especially for QLoRA and LQ-LoRA. Robust aggregation (e.g., FedProx, FedDyn) is needed.</p>\n  </li>\n</ul>\n<p><strong>Hyperparameter sensitivity</strong>: LoRA’s performance depends on careful tuning of the rank <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-83-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>r</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-985\" style=\"width: 0.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.32em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-986\"><span class=\"mi\" id=\"MathJax-Span-987\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>r</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-83\">r</script>, learning rate, and initialization. This gets harder in FL settings with diverse clients.</p>\n<p><strong>Adapter coordination</strong>: Clients may use different adapter ranks or sparsity patterns, which complicates aggregation of LoRA updates.</p>\n<p><strong>Quantization complexity</strong>: Variants like LQ-LoRA and QA-LoRA introduce additional steps—e.g., quantizer calibration, activation scaling—which increase implementation burden.</p>\n<p><strong>Stability under heterogeneity</strong>: In FL, device-specific data distributions may lead to divergent adapter updates, especially for QLoRA and LQ-LoRA. Robust aggregation (e.g., FedProx, FedDyn) is needed.</p>",
      "contentMarkdown": "#### Pros\n\n*   **Parameter-efficient fine-tuning**: All LoRA-based techniques (LoRA, QLoRA, QA-LoRA, LQ-LoRA) fine-tune only a small set of low-rank parameters, making them ideal for FL environments with limited bandwidth.\n    \n*   **Adaptable to quantization settings**: LoRA variants like **QLoRA** and **QA-LoRA** support aggressive quantization (e.g., 4-bit), allowing fine-tuning with reduced memory and compute requirements—especially valuable in client-side FL scenarios.\n    \n*   **Scalable across clients**: Clients send only adapter weights (A,BA,BA, B) rather than full models, reducing communication costs by over 90%. This makes federated tuning feasible even on mobile devices.\n    \n*   **Privacy-enhancing**: By avoiding transmission of full model weights, and optionally combining with **DP-FL**, LoRA-based fine-tuning helps achieve stronger privacy guarantees.\n    \n*   **Accuracy retention**: LQ-LoRA and QA-LoRA correct quantization-induced degradation via low-rank compensation, maintaining fidelity even at 4-bit or 2-bit precision.\n    \n\n**Parameter-efficient fine-tuning**: All LoRA-based techniques (LoRA, QLoRA, QA-LoRA, LQ-LoRA) fine-tune only a small set of low-rank parameters, making them ideal for FL environments with limited bandwidth.\n\n**Adaptable to quantization settings**: LoRA variants like **QLoRA** and **QA-LoRA** support aggressive quantization (e.g., 4-bit), allowing fine-tuning with reduced memory and compute requirements—especially valuable in client-side FL scenarios.\n\n**Scalable across clients**: Clients send only adapter weights (A,BA,BA, B) rather than full models, reducing communication costs by over 90%. This makes federated tuning feasible even on mobile devices.\n\n**Privacy-enhancing**: By avoiding transmission of full model weights, and optionally combining with **DP-FL**, LoRA-based fine-tuning helps achieve stronger privacy guarantees.\n\n**Accuracy retention**: LQ-LoRA and QA-LoRA correct quantization-induced degradation via low-rank compensation, maintaining fidelity even at 4-bit or 2-bit precision.\n\n#### Cons\n\n*   **Hyperparameter sensitivity**: LoRA’s performance depends on careful tuning of the rank rrr, learning rate, and initialization. This gets harder in FL settings with diverse clients.\n    \n*   **Adapter coordination**: Clients may use different adapter ranks or sparsity patterns, which complicates aggregation of LoRA updates.\n    \n*   **Quantization complexity**: Variants like LQ-LoRA and QA-LoRA introduce additional steps—e.g., quantizer calibration, activation scaling—which increase implementation burden.\n    \n*   **Stability under heterogeneity**: In FL, device-specific data distributions may lead to divergent adapter updates, especially for QLoRA and LQ-LoRA. Robust aggregation (e.g., FedProx, FedDyn) is needed.\n    \n\n**Hyperparameter sensitivity**: LoRA’s performance depends on careful tuning of the rank rrr, learning rate, and initialization. This gets harder in FL settings with diverse clients.\n\n**Adapter coordination**: Clients may use different adapter ranks or sparsity patterns, which complicates aggregation of LoRA updates.\n\n**Quantization complexity**: Variants like LQ-LoRA and QA-LoRA introduce additional steps—e.g., quantizer calibration, activation scaling—which increase implementation burden.\n\n**Stability under heterogeneity**: In FL, device-specific data distributions may lead to divergent adapter updates, especially for QLoRA and LQ-LoRA. Robust aggregation (e.g., FedProx, FedDyn) is needed.",
      "order": 19,
      "orderInChapter": 4,
      "difficulty": 4,
      "estimatedMinutes": 3,
      "tags": [
        "ondevice ai",
        "activation",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 427,
        "contentLength": 9440
      },
      "nextCards": [
        "ai-federated-learning-comparison-use-cases-20",
        "ai-federated-learning-privacy-21"
      ],
      "relatedCards": [
        "ai-model-compression-quantized-low-rank-adaptation-techniques-46",
        "ai-model-compression-dequantization-considerations-5",
        "ai-model-compression-pros-cons-47",
        "ai-model-compression-when-to-use-lightweight-models-53",
        "ai-model-compression-types-of-quantization-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#pros-&-cons",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-comparison-use-cases-20",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Federated Adaptation with LoRA",
      "title": "Comparison & Use Cases",
      "subtitle": "Federated Adaptation with LoRA",
      "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Use Case</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Suggested Strategy</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Benefit</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Parameter-efficient tuning</td>\n<td class=\"tg-tleft-valign-first\">LoRA / QLoRA / QA-LoRA</td>\n<td class=\"tg-tleft-valign-second\">Reduces compute/memory footprint</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Extreme quantization</td>\n<td class=\"tg-tleft-valign-first\">LQ‑LoRA or QA-LoRA</td>\n<td class=\"tg-tleft-valign-second\">4‑bit or 2‑bit tuning with minimal accuracy loss</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Federated fine‑tuning</td>\n<td class=\"tg-tleft-valign-first\">Federated LoRA / QLoRA adapters</td>\n<td class=\"tg-tleft-valign-second\">Minimal communication cost</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantization-aware training</td>\n<td class=\"tg-tleft-valign-first\">QA‑LoRA + QAT</td>\n<td class=\"tg-tleft-valign-second\">Activations + weights in int4 with recovery via low-rank correction</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Memory-constrained fine-tuning</td>\n<td class=\"tg-tleft-valign-first\">QLoRA</td>\n<td class=\"tg-tleft-valign-second\">GPU VRAM usage reduced via paged optimizers + quantized base model</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Private on-device personalization</td>\n<td class=\"tg-tleft-valign-first\">QLoRA + DP-FL</td>\n<td class=\"tg-tleft-valign-second\">Combines quantization, privacy, and adapter sharing</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Use Case</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Suggested Strategy</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Benefit</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Parameter-efficient tuning</td>\n<td class=\"tg-tleft-valign-first\">LoRA / QLoRA / QA-LoRA</td>\n<td class=\"tg-tleft-valign-second\">Reduces compute/memory footprint</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Extreme quantization</td>\n<td class=\"tg-tleft-valign-first\">LQ‑LoRA or QA-LoRA</td>\n<td class=\"tg-tleft-valign-second\">4‑bit or 2‑bit tuning with minimal accuracy loss</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Federated fine‑tuning</td>\n<td class=\"tg-tleft-valign-first\">Federated LoRA / QLoRA adapters</td>\n<td class=\"tg-tleft-valign-second\">Minimal communication cost</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantization-aware training</td>\n<td class=\"tg-tleft-valign-first\">QA‑LoRA + QAT</td>\n<td class=\"tg-tleft-valign-second\">Activations + weights in int4 with recovery via low-rank correction</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Memory-constrained fine-tuning</td>\n<td class=\"tg-tleft-valign-first\">QLoRA</td>\n<td class=\"tg-tleft-valign-second\">GPU VRAM usage reduced via paged optimizers + quantized base model</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Private on-device personalization</td>\n<td class=\"tg-tleft-valign-first\">QLoRA + DP-FL</td>\n<td class=\"tg-tleft-valign-second\">Combines quantization, privacy, and adapter sharing</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "**Use Case**\n\n**Suggested Strategy**\n\n**Benefit**\n\nParameter-efficient tuning\n\nLoRA / QLoRA / QA-LoRA\n\nReduces compute/memory footprint\n\nExtreme quantization\n\nLQ‑LoRA or QA-LoRA\n\n4‑bit or 2‑bit tuning with minimal accuracy loss\n\nFederated fine‑tuning\n\nFederated LoRA / QLoRA adapters\n\nMinimal communication cost\n\nQuantization-aware training\n\nQA‑LoRA + QAT\n\nActivations + weights in int4 with recovery via low-rank correction\n\nMemory-constrained fine-tuning\n\nQLoRA\n\nGPU VRAM usage reduced via paged optimizers + quantized base model\n\nPrivate on-device personalization\n\nQLoRA + DP-FL\n\nCombines quantization, privacy, and adapter sharing\n\n**Use Case**\n\n**Suggested Strategy**\n\n**Benefit**\n\nParameter-efficient tuning\n\nLoRA / QLoRA / QA-LoRA\n\nReduces compute/memory footprint\n\nExtreme quantization\n\nLQ‑LoRA or QA-LoRA\n\n4‑bit or 2‑bit tuning with minimal accuracy loss\n\nFederated fine‑tuning\n\nFederated LoRA / QLoRA adapters\n\nMinimal communication cost\n\nQuantization-aware training\n\nQA‑LoRA + QAT\n\nActivations + weights in int4 with recovery via low-rank correction\n\nMemory-constrained fine-tuning\n\nQLoRA\n\nGPU VRAM usage reduced via paged optimizers + quantized base model\n\nPrivate on-device personalization\n\nQLoRA + DP-FL\n\nCombines quantization, privacy, and adapter sharing",
      "order": 20,
      "orderInChapter": 5,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "activation",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 158,
        "contentLength": 3277
      },
      "nextCards": [
        "ai-federated-learning-privacy-21",
        "ai-federated-learning-personalization-22"
      ],
      "relatedCards": [
        "ai-model-compression-quantized-low-rank-adaptation-techniques-46",
        "ai-model-compression-dequantization-considerations-5",
        "ai-model-compression-pros-cons-47",
        "ai-model-compression-when-to-use-lightweight-models-53",
        "ai-model-compression-types-of-quantization-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#comparison-&-use-cases",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-privacy-21",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Advantages",
      "title": "Privacy",
      "subtitle": "Advantages",
      "contentHtml": "<ul>\n  <li>\n    <p>The main advantage of using federated approaches to machine learning is to ensure data privacy or data secrecy. Indeed, no local data is uploaded externally, concatenated or exchanged. Since the entire database is segmented into local bits, this makes it more difficult to hack into it.</p>\n  </li>\n  <li>\n    <p>With FL, only machine learning parameters are exchanged. In addition, such parameters can be encrypted before sharing between learning rounds to extend privacy and homomorphic encryption schemes can be used to directly make computations on the encrypted data without decrypting them beforehand. Despite such protective measures, these parameters may still leak information about the underlying data samples, for instance, by making multiple specific queries on specific datasets. Querying capability of nodes thus is a major attention point, which can be addressed using <a href=\"../differential-privacy\">differential privacy</a> and secure aggregation.</p>\n  </li>\n  <li>\n    <p>It was found that the privacy issues of FL is often due to running estimates, which hinders the usage of advanced deep learning models.</p>\n  </li>\n</ul>\n<p>The main advantage of using federated approaches to machine learning is to ensure data privacy or data secrecy. Indeed, no local data is uploaded externally, concatenated or exchanged. Since the entire database is segmented into local bits, this makes it more difficult to hack into it.</p>\n<p>With FL, only machine learning parameters are exchanged. In addition, such parameters can be encrypted before sharing between learning rounds to extend privacy and homomorphic encryption schemes can be used to directly make computations on the encrypted data without decrypting them beforehand. Despite such protective measures, these parameters may still leak information about the underlying data samples, for instance, by making multiple specific queries on specific datasets. Querying capability of nodes thus is a major attention point, which can be addressed using <a href=\"../differential-privacy\">differential privacy</a> and secure aggregation.</p>\n<p>It was found that the privacy issues of FL is often due to running estimates, which hinders the usage of advanced deep learning models.</p>\n<blockquote>\n  <p>A Static Batch Normalization (sBN) for optimizing privacy constrained deep neural networks was developed. During the training phase, sBN does not track <strong>running estimates</strong> but simply normalizes batch data. Only the statistics of hidden representations from local data after the model converges are calculated. This method is suitable for the FL framework as local models do not need to upload running estimates during training. <strong>Local models only upload their statistics once after optimization, which significantly reduces data leakage risk</strong>.</p>\n</blockquote>\n<p>A Static Batch Normalization (sBN) for optimizing privacy constrained deep neural networks was developed. During the training phase, sBN does not track <strong>running estimates</strong> but simply normalizes batch data. Only the statistics of hidden representations from local data after the model converges are calculated. This method is suitable for the FL framework as local models do not need to upload running estimates during training. <strong>Local models only upload their statistics once after optimization, which significantly reduces data leakage risk</strong>.</p>",
      "contentMarkdown": "*   The main advantage of using federated approaches to machine learning is to ensure data privacy or data secrecy. Indeed, no local data is uploaded externally, concatenated or exchanged. Since the entire database is segmented into local bits, this makes it more difficult to hack into it.\n    \n*   With FL, only machine learning parameters are exchanged. In addition, such parameters can be encrypted before sharing between learning rounds to extend privacy and homomorphic encryption schemes can be used to directly make computations on the encrypted data without decrypting them beforehand. Despite such protective measures, these parameters may still leak information about the underlying data samples, for instance, by making multiple specific queries on specific datasets. Querying capability of nodes thus is a major attention point, which can be addressed using [differential privacy](../differential-privacy) and secure aggregation.\n    \n*   It was found that the privacy issues of FL is often due to running estimates, which hinders the usage of advanced deep learning models.\n    \n\nThe main advantage of using federated approaches to machine learning is to ensure data privacy or data secrecy. Indeed, no local data is uploaded externally, concatenated or exchanged. Since the entire database is segmented into local bits, this makes it more difficult to hack into it.\n\nWith FL, only machine learning parameters are exchanged. In addition, such parameters can be encrypted before sharing between learning rounds to extend privacy and homomorphic encryption schemes can be used to directly make computations on the encrypted data without decrypting them beforehand. Despite such protective measures, these parameters may still leak information about the underlying data samples, for instance, by making multiple specific queries on specific datasets. Querying capability of nodes thus is a major attention point, which can be addressed using [differential privacy](../differential-privacy) and secure aggregation.\n\nIt was found that the privacy issues of FL is often due to running estimates, which hinders the usage of advanced deep learning models.\n\n> A Static Batch Normalization (sBN) for optimizing privacy constrained deep neural networks was developed. During the training phase, sBN does not track **running estimates** but simply normalizes batch data. Only the statistics of hidden representations from local data after the model converges are calculated. This method is suitable for the FL framework as local models do not need to upload running estimates during training. **Local models only upload their statistics once after optimization, which significantly reduces data leakage risk**.\n\nA Static Batch Normalization (sBN) for optimizing privacy constrained deep neural networks was developed. During the training phase, sBN does not track **running estimates** but simply normalizes batch data. Only the statistics of hidden representations from local data after the model converges are calculated. This method is suitable for the FL framework as local models do not need to upload running estimates during training. **Local models only upload their statistics once after optimization, which significantly reduces data leakage risk**.",
      "order": 21,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 3,
      "tags": [
        "ondevice ai",
        "neural network",
        "deep learning",
        "machine learning",
        "attention",
        "optimization",
        "batch normalization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 476,
        "contentLength": 3454
      },
      "nextCards": [
        "ai-federated-learning-personalization-22",
        "ai-federated-learning-legal-benefits-23"
      ],
      "relatedCards": [
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-model-compression-definition-3",
        "ai-on-device-transformers-key-value-kv-cache-optimization-9",
        "ai-model-compression-weights-vs-activation-quantization-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#privacy",
      "scrapedAt": "2025-12-28T11:56:01.214Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-personalization-22",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Advantages",
      "title": "Personalization",
      "subtitle": "Advantages",
      "contentHtml": "<ul>\n  <li>The generated model delivers insights based on the global patterns of nodes. However, if a participating node wishes to learn from global patterns but also adapt outcomes to its peculiar status, the FL methodology can be adapted to generate two models at once in a multi-task learning framework. In addition, clustering techniques may be applied to aggregate nodes that share some similarities after the learning process is completed. This allows the generalization of the models learned by the nodes according also to their local data.</li>\n</ul>\n<blockquote>\n  <p>In the case of deep neural networks, it is possible to share some layers across the different nodes and keep some of them on each local node. Typically, first layers performing general pattern recognition are shared and trained all datasets. The last layers will <strong>remain on each local node and only be trained on the local node’s dataset</strong>.</p>\n</blockquote>\n<p>In the case of deep neural networks, it is possible to share some layers across the different nodes and keep some of them on each local node. Typically, first layers performing general pattern recognition are shared and trained all datasets. The last layers will <strong>remain on each local node and only be trained on the local node’s dataset</strong>.</p>\n<ul>\n  <li>Early personalization methods often introduce additional computation and communication overhead that may not be necessary. To significantly reduce computation and communication costs in FL, a “Masking Trick” approach was developed. The “Masking Trick” allows local clients to adaptively contribute to the training of global models much more flexibly and efficiently compared with classical FL.</li>\n</ul>",
      "contentMarkdown": "*   The generated model delivers insights based on the global patterns of nodes. However, if a participating node wishes to learn from global patterns but also adapt outcomes to its peculiar status, the FL methodology can be adapted to generate two models at once in a multi-task learning framework. In addition, clustering techniques may be applied to aggregate nodes that share some similarities after the learning process is completed. This allows the generalization of the models learned by the nodes according also to their local data.\n\n> In the case of deep neural networks, it is possible to share some layers across the different nodes and keep some of them on each local node. Typically, first layers performing general pattern recognition are shared and trained all datasets. The last layers will **remain on each local node and only be trained on the local node’s dataset**.\n\nIn the case of deep neural networks, it is possible to share some layers across the different nodes and keep some of them on each local node. Typically, first layers performing general pattern recognition are shared and trained all datasets. The last layers will **remain on each local node and only be trained on the local node’s dataset**.\n\n*   Early personalization methods often introduce additional computation and communication overhead that may not be necessary. To significantly reduce computation and communication costs in FL, a “Masking Trick” approach was developed. The “Masking Trick” allows local clients to adaptively contribute to the training of global models much more flexibly and efficiently compared with classical FL.",
      "order": 22,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "neural network"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 258,
        "contentLength": 1727
      },
      "nextCards": [
        "ai-federated-learning-legal-benefits-23",
        "ai-federated-learning-transportation-self-driving-cars-24"
      ],
      "relatedCards": [
        "ai-model-compression-rationale-and-theoretical-motivation-33",
        "ai-differential-privacy-tightness-3",
        "ai-model-compression-how-far-can-quantization-be-pushed-20",
        "ai-model-compression-pruning-workflow-35",
        "ai-model-compression-formal-definition-32"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#personalization",
      "scrapedAt": "2025-12-28T11:56:01.215Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-legal-benefits-23",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Advantages",
      "title": "Legal Benefits",
      "subtitle": "Advantages",
      "contentHtml": "<ul>\n  <li>Western legal frameworks emphasize more and more on data protection and data traceability. The <a href=\"https://doi.org/10.29012%2Fjpc.v4i2.623\">White House 2012 Report</a> recommended the application of a data minimization principle, which is mentioned in European GDPR. In some cases, it is illegal to transfer data from a country to another (e.g., genomic data), however international consortia are sometimes necessary for scientific advances. In such cases FL brings solutions to train a global model while respecting security constraints.</li>\n</ul>",
      "contentMarkdown": "*   Western legal frameworks emphasize more and more on data protection and data traceability. The [White House 2012 Report](https://doi.org/10.29012%2Fjpc.v4i2.623) recommended the application of a data minimization principle, which is mentioned in European GDPR. In some cases, it is illegal to transfer data from a country to another (e.g., genomic data), however international consortia are sometimes necessary for scientific advances. In such cases FL brings solutions to train a global model while respecting security constraints.",
      "order": 23,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 74,
        "contentLength": 565
      },
      "nextCards": [
        "ai-federated-learning-transportation-self-driving-cars-24",
        "ai-federated-learning-industry-40-smart-manufacturing-25"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#legal-benefits",
      "scrapedAt": "2025-12-28T11:56:01.215Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-transportation-self-driving-cars-24",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Use Cases",
      "title": "Transportation: Self-driving Cars",
      "subtitle": "Use Cases",
      "contentHtml": "<ul>\n  <li>Self-driving cars encapsulate many machine learning technologies to function: computer vision for analyzing obstacles, machine learning for adapting their pace to the environment (e.g., bumpiness of the road). Due to the potential high number of self-driving cars and the need for them to quickly respond to real world situations, traditional cloud approach may generate safety risks. FL can represent a solution for limiting volume of data transfer and accelerating learning processes.</li>\n</ul>",
      "contentMarkdown": "*   Self-driving cars encapsulate many machine learning technologies to function: computer vision for analyzing obstacles, machine learning for adapting their pace to the environment (e.g., bumpiness of the road). Due to the potential high number of self-driving cars and the need for them to quickly respond to real world situations, traditional cloud approach may generate safety risks. FL can represent a solution for limiting volume of data transfer and accelerating learning processes.",
      "order": 24,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "machine learning",
        "computer vision"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 72,
        "contentLength": 508
      },
      "nextCards": [
        "ai-federated-learning-industry-40-smart-manufacturing-25",
        "ai-federated-learning-medicine-digital-health-26"
      ],
      "relatedCards": [
        "ai-model-compression-accuracy-results-18",
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#transportation:-self-driving-cars",
      "scrapedAt": "2025-12-28T11:56:01.215Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-industry-40-smart-manufacturing-25",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Use Cases",
      "title": "Industry 4.0: Smart Manufacturing",
      "subtitle": "Use Cases",
      "contentHtml": "<ul>\n  <li>In Industry 4.0, there is a widespread adoption of machine learning techniques to improve the efficiency and effectiveness of industrial process while guaranteeing a high level of safety. Nevertheless, privacy of sensitive data for industries and manufacturing companies is of paramount importance. FL algorithms can be applied to these problems as they do not disclose any sensitive data. In addition, FL also implemented for PM2.5 prediction to support Smart city sensing applications.</li>\n</ul>",
      "contentMarkdown": "*   In Industry 4.0, there is a widespread adoption of machine learning techniques to improve the efficiency and effectiveness of industrial process while guaranteeing a high level of safety. Nevertheless, privacy of sensitive data for industries and manufacturing companies is of paramount importance. FL algorithms can be applied to these problems as they do not disclose any sensitive data. In addition, FL also implemented for PM2.5 prediction to support Smart city sensing applications.",
      "order": 25,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "machine learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 73,
        "contentLength": 509
      },
      "nextCards": [
        "ai-federated-learning-medicine-digital-health-26"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#industry-4.0:-smart-manufacturing",
      "scrapedAt": "2025-12-28T11:56:01.215Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-federated-learning-medicine-digital-health-26",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Federated Learning",
      "articleSlug": "federated-learning",
      "chapter": "Use Cases",
      "title": "Medicine: Digital Health",
      "subtitle": "Use Cases",
      "contentHtml": "<ul>\n  <li>FL seeks to address the problem of data governance and privacy by training algorithms collaboratively without exchanging the data itself. Today’s standard approach of centralizing data from multiple centers comes at the cost of critical concerns regarding patient privacy and data protection. To solve this problem, the ability to train machine learning models at scale across multiple medical institutions without moving the data is a critical technology.</li>\n  <li>Nature Digital Medicine published the paper <a href=\"https://arxiv.org/abs/2003.08119\">“The Future of Digital Health with Federated Learning”</a> in September 2020, in which the authors explore how FL may provide a solution for the future of digital health, and highlight the challenges and considerations that need to be addressed.</li>\n  <li>Recently, a collaboration of 20 different institutions around the world validated the utility of training AI models using FL.</li>\n  <li>In a paper published in Nature Medicine <a href=\"https://doi.org/10.1038%2Fs41591-021-01506-3\">“Federated learning for predicting clinical outcomes in patients with COVID-19”</a>, they showcased the accuracy and generalizability of a federated AI model for the prediction of oxygen needs in patients with COVID-19 infections.</li>\n  <li>Furthermore, in a published paper <a href=\"https://doi.org/10.3390%2Fapp112311191\">“A Systematic Review of Federated Learning in the Healthcare Area: From the Perspective of Data Properties and Applications”</a>, the authors trying to provide a set of challenges on FL challenges on medical data-centric perspective.</li>\n</ul>",
      "contentMarkdown": "*   FL seeks to address the problem of data governance and privacy by training algorithms collaboratively without exchanging the data itself. Today’s standard approach of centralizing data from multiple centers comes at the cost of critical concerns regarding patient privacy and data protection. To solve this problem, the ability to train machine learning models at scale across multiple medical institutions without moving the data is a critical technology.\n*   Nature Digital Medicine published the paper [“The Future of Digital Health with Federated Learning”](https://arxiv.org/abs/2003.08119) in September 2020, in which the authors explore how FL may provide a solution for the future of digital health, and highlight the challenges and considerations that need to be addressed.\n*   Recently, a collaboration of 20 different institutions around the world validated the utility of training AI models using FL.\n*   In a paper published in Nature Medicine [“Federated learning for predicting clinical outcomes in patients with COVID-19”](https://doi.org/10.1038%2Fs41591-021-01506-3), they showcased the accuracy and generalizability of a federated AI model for the prediction of oxygen needs in patients with COVID-19 infections.\n*   Furthermore, in a published paper [“A Systematic Review of Federated Learning in the Healthcare Area: From the Perspective of Data Properties and Applications”](https://doi.org/10.3390%2Fapp112311191), the authors trying to provide a set of challenges on FL challenges on medical data-centric perspective.",
      "order": 26,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "machine learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 214,
        "contentLength": 1624
      },
      "nextCards": [],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/federated-learning/#medicine:-digital-health",
      "scrapedAt": "2025-12-28T11:56:01.215Z",
      "siblings": [
        "ai-federated-learning-overview-1",
        "ai-federated-learning-definition-2",
        "ai-federated-learning-cross-device-federated-learning-3",
        "ai-federated-learning-cross-silo-federated-learning-4",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ]
    },
    {
      "id": "ai-differential-privacy-components-1",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "Privacy Budget and Definition",
      "title": "Components",
      "subtitle": "Privacy Budget and Definition",
      "contentHtml": "<ul>\n  <li><strong><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-56\" style=\"width: 0.571em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.467em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.552em, 1000.47em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-57\"><span class=\"mi\" id=\"MathJax-Span-58\" style=\"font-family: STIXGeneral-Italic;\">ε</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">\\varepsilon</script> (epsilon)</strong>: The <em>privacy loss parameter</em>. Smaller values imply stronger privacy. A typical range is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi><mo>&amp;#x2208;</mo><mo stretchy=&quot;false&quot;>[</mo><mn>0.01</mn><mo>,</mo><mn>10</mn><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-59\" style=\"width: 6.669em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1005.42em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-60\"><span class=\"mi\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Italic;\">ε</span><span class=\"mo\" id=\"MathJax-Span-62\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"mo\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">[</span><span class=\"mn\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Regular;\">0.01</span><span class=\"mo\" id=\"MathJax-Span-65\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mn\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">10</span><span class=\"mo\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi><mo>∈</mo><mo stretchy=\"false\">[</mo><mn>0.01</mn><mo>,</mo><mn>10</mn><mo stretchy=\"false\">]</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">\\varepsilon \\in [0.01, 10]</script>.</li>\n  <li><strong><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B4;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-68\" style=\"width: 0.726em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.571em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.57em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-69\"><span class=\"mi\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Italic;\">δ</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>δ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">\\delta</script></strong>: The probability that the differential privacy guarantee fails. Typically chosen to be smaller than <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mfrac><mn>1</mn><mi>n</mi></mfrac></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-71\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1000.68em, 2.659em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-72\"><span class=\"mfrac\" id=\"MathJax-Span-73\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.26em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.154em;\"><span class=\"mn\" id=\"MathJax-Span-74\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.154em;\"><span class=\"mi\" id=\"MathJax-Span-75\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.47em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.471em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mfrac><mn>1</mn><mi>n</mi></mfrac></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">\\frac{1}{n}</script>, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-76\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-77\"><span class=\"mi\" id=\"MathJax-Span-78\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">n</script> is the dataset size.</li>\n  <li><strong>Adjacent datasets</strong>: Two datasets are adjacent if they differ in exactly one individual’s data.</li>\n  <li><strong>Randomized algorithm <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-79\" style=\"width: 1.139em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.932em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.93em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-80\"><span class=\"mi\" id=\"MathJax-Span-81\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">M</script></strong>: The core of DP is randomness—achieved by injecting carefully calibrated noise (e.g., Laplace or Gaussian)—so that individual records cannot be inferred from outputs.</li>\n</ul>",
      "contentMarkdown": "*   **εε\\\\varepsilon (epsilon)**: The _privacy loss parameter_. Smaller values imply stronger privacy. A typical range is ε∈\\[0.01,10\\]ε∈\\[0.01,10\\]\\\\varepsilon \\\\in \\[0.01, 10\\].\n*   **δδ\\\\delta**: The probability that the differential privacy guarantee fails. Typically chosen to be smaller than 1n1n\\\\frac{1}{n}, where nnn is the dataset size.\n*   **Adjacent datasets**: Two datasets are adjacent if they differ in exactly one individual’s data.\n*   **Randomized algorithm MMM**: The core of DP is randomness—achieved by injecting carefully calibrated noise (e.g., Laplace or Gaussian)—so that individual records cannot be inferred from outputs.",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 85,
        "contentLength": 9823
      },
      "nextCards": [
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-tightness-3"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#components",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-real-world-example-apple-dp-5",
        "ai-differential-privacy-example-visualization-apple-dp-emoji-analysis-6"
      ]
    },
    {
      "id": "ai-differential-privacy-intuition-2",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "Privacy Budget and Definition",
      "title": "Intuition",
      "subtitle": "Privacy Budget and Definition",
      "contentHtml": "<ul>\n  <li>This definition ensures that the <strong>presence or absence of a single user’s data</strong> does not significantly influence the output distribution of the algorithm. Even a powerful adversary cannot confidently determine if a specific user contributed data, based on the output alone.</li>\n</ul>",
      "contentMarkdown": "*   This definition ensures that the **presence or absence of a single user’s data** does not significantly influence the output distribution of the algorithm. Even a powerful adversary cannot confidently determine if a specific user contributed data, based on the output alone.",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 42,
        "contentLength": 309
      },
      "nextCards": [
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-privacy-budget-in-practice-4"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-federated-learning-cross-device-federated-learning-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#intuition",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-components-1",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-real-world-example-apple-dp-5",
        "ai-differential-privacy-example-visualization-apple-dp-emoji-analysis-6"
      ]
    },
    {
      "id": "ai-differential-privacy-tightness-3",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "Privacy Budget and Definition",
      "title": "Tightness",
      "subtitle": "Privacy Budget and Definition",
      "contentHtml": "<ul>\n  <li>If <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B4;</mi><mo>=</mo><mn>0</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-82\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.24em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-83\"><span class=\"mi\" id=\"MathJax-Span-84\" style=\"font-family: STIXGeneral-Italic;\">δ</span><span class=\"mo\" id=\"MathJax-Span-85\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-86\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>δ</mi><mo>=</mo><mn>0</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">\\delta = 0</script>, we get <strong>pure differential privacy</strong>.</li>\n  <li>If <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B4;</mi><mo>&amp;gt;</mo><mn>0</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-87\" style=\"width: 2.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.24em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-88\"><span class=\"mi\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Italic;\">δ</span><span class=\"mo\" id=\"MathJax-Span-90\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">&gt;</span><span class=\"mn\" id=\"MathJax-Span-91\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">0</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>δ</mi><mo>&gt;</mo><mn>0</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">\\delta > 0</script>, the guarantee becomes <strong>approximate differential privacy</strong>, which is more practical for complex models like deep neural networks.</li>\n</ul>",
      "contentMarkdown": "*   If δ\\=0δ\\=0\\\\delta = 0, we get **pure differential privacy**.\n*   If δ\\>0δ\\>0\\\\delta > 0, the guarantee becomes **approximate differential privacy**, which is more practical for complex models like deep neural networks.",
      "order": 3,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "neural network"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 32,
        "contentLength": 3165
      },
      "nextCards": [
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-real-world-example-apple-dp-5"
      ],
      "relatedCards": [
        "ai-model-compression-rationale-and-theoretical-motivation-33",
        "ai-federated-learning-personalization-22",
        "ai-model-compression-how-far-can-quantization-be-pushed-20",
        "ai-model-compression-pruning-workflow-35",
        "ai-model-compression-formal-definition-32"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#tightness",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-components-1",
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-real-world-example-apple-dp-5",
        "ai-differential-privacy-example-visualization-apple-dp-emoji-analysis-6"
      ]
    },
    {
      "id": "ai-differential-privacy-privacy-budget-in-practice-4",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "Privacy Budget and Definition",
      "title": "Privacy Budget in Practice",
      "subtitle": "Privacy Budget and Definition",
      "contentHtml": "<ul>\n  <li>A typical differential privacy system tracks a user’s <strong>privacy budget</strong>—a measure of how much information about them has been exposed.</li>\n  <li>The budget is often enforced by setting limits on the number of contributions per user and how much noise is injected.</li>\n</ul>",
      "contentMarkdown": "*   A typical differential privacy system tracks a user’s **privacy budget**—a measure of how much information about them has been exposed.\n*   The budget is often enforced by setting limits on the number of contributions per user and how much noise is injected.",
      "order": 4,
      "orderInChapter": 4,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 43,
        "contentLength": 300
      },
      "nextCards": [
        "ai-differential-privacy-real-world-example-apple-dp-5",
        "ai-differential-privacy-example-visualization-apple-dp-emoji-analysis-6"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-federated-learning-cross-device-federated-learning-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#privacy-budget-in-practice",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-components-1",
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-real-world-example-apple-dp-5",
        "ai-differential-privacy-example-visualization-apple-dp-emoji-analysis-6"
      ]
    },
    {
      "id": "ai-differential-privacy-real-world-example-apple-dp-5",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "Privacy Budget and Definition",
      "title": "Real-World Example: Apple DP",
      "subtitle": "Privacy Budget and Definition",
      "contentHtml": "<ul>\n  <li>\n    <p>From <a href=\"https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf\">Apple’s Differential Privacy Technical Overview</a>, Apple uses carefully bounded <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-92\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXGeneral-Italic;\">ε</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">\\varepsilon</script> values and per-day contribution limits to collect aggregate insights without compromising user privacy. Examples include:</p>\n\n    <ul>\n      <li><strong>Emoji suggestions</strong>: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-95\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-96\"><span class=\"mi\" id=\"MathJax-Span-97\" style=\"font-family: STIXGeneral-Italic;\">ε</span><span class=\"mo\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">\\varepsilon = 4</script>, one contribution per day.</li>\n      <li><strong>QuickType</strong>: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi><mo>=</mo><mn>8</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-100\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-101\"><span class=\"mi\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Italic;\">ε</span><span class=\"mo\" id=\"MathJax-Span-103\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-104\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi><mo>=</mo><mn>8</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">\\varepsilon = 8</script>, two donations per day.</li>\n      <li><strong>Safari Auto-play intent detection</strong>: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi><mo>=</mo><mn>8</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-105\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-106\"><span class=\"mi\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Italic;\">ε</span><span class=\"mo\" id=\"MathJax-Span-108\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-109\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi><mo>=</mo><mn>8</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">\\varepsilon = 8</script>, two donations per day.</li>\n      <li><strong>Health Type Usage</strong>: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi><mo>=</mo><mn>2</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-110\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.14em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-111\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-family: STIXGeneral-Italic;\">ε</span><span class=\"mo\" id=\"MathJax-Span-113\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-114\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi><mo>=</mo><mn>2</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">\\varepsilon = 2</script>, one donation per day (only metadata, not raw health info).</li>\n    </ul>\n  </li>\n  <li>\n    <p>These limits and budgets are designed to keep user-specific data unidentifiable—even internally.</p>\n  </li>\n</ul>\n<p>From <a href=\"https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf\">Apple’s Differential Privacy Technical Overview</a>, Apple uses carefully bounded <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-92\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-93\"><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXGeneral-Italic;\">ε</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">\\varepsilon</script> values and per-day contribution limits to collect aggregate insights without compromising user privacy. Examples include:</p>\n<ul>\n      <li><strong>Emoji suggestions</strong>: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi><mo>=</mo><mn>4</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-95\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-96\"><span class=\"mi\" id=\"MathJax-Span-97\" style=\"font-family: STIXGeneral-Italic;\">ε</span><span class=\"mo\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi><mo>=</mo><mn>4</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">\\varepsilon = 4</script>, one contribution per day.</li>\n      <li><strong>QuickType</strong>: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi><mo>=</mo><mn>8</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-100\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-101\"><span class=\"mi\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Italic;\">ε</span><span class=\"mo\" id=\"MathJax-Span-103\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-104\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi><mo>=</mo><mn>8</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">\\varepsilon = 8</script>, two donations per day.</li>\n      <li><strong>Safari Auto-play intent detection</strong>: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi><mo>=</mo><mn>8</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-105\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.09em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-106\"><span class=\"mi\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Italic;\">ε</span><span class=\"mo\" id=\"MathJax-Span-108\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-109\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi><mo>=</mo><mn>8</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">\\varepsilon = 8</script>, two donations per day.</li>\n      <li><strong>Health Type Usage</strong>: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi><mo>=</mo><mn>2</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-110\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.14em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-111\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-family: STIXGeneral-Italic;\">ε</span><span class=\"mo\" id=\"MathJax-Span-113\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-114\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi><mo>=</mo><mn>2</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">\\varepsilon = 2</script>, one donation per day (only metadata, not raw health info).</li>\n    </ul>\n<p>These limits and budgets are designed to keep user-specific data unidentifiable—even internally.</p>",
      "contentMarkdown": "*   From [Apple’s Differential Privacy Technical Overview](https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf), Apple uses carefully bounded εε\\\\varepsilon values and per-day contribution limits to collect aggregate insights without compromising user privacy. Examples include:\n    \n    *   **Emoji suggestions**: ε\\=4ε\\=4\\\\varepsilon = 4, one contribution per day.\n    *   **QuickType**: ε\\=8ε\\=8\\\\varepsilon = 8, two donations per day.\n    *   **Safari Auto-play intent detection**: ε\\=8ε\\=8\\\\varepsilon = 8, two donations per day.\n    *   **Health Type Usage**: ε\\=2ε\\=2\\\\varepsilon = 2, one donation per day (only metadata, not raw health info).\n*   These limits and budgets are designed to keep user-specific data unidentifiable—even internally.\n    \n\nFrom [Apple’s Differential Privacy Technical Overview](https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf), Apple uses carefully bounded εε\\\\varepsilon values and per-day contribution limits to collect aggregate insights without compromising user privacy. Examples include:\n\n*   **Emoji suggestions**: ε\\=4ε\\=4\\\\varepsilon = 4, one contribution per day.\n*   **QuickType**: ε\\=8ε\\=8\\\\varepsilon = 8, two donations per day.\n*   **Safari Auto-play intent detection**: ε\\=8ε\\=8\\\\varepsilon = 8, two donations per day.\n*   **Health Type Usage**: ε\\=2ε\\=2\\\\varepsilon = 2, one donation per day (only metadata, not raw health info).\n\nThese limits and budgets are designed to keep user-specific data unidentifiable—even internally.",
      "order": 5,
      "orderInChapter": 5,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 174,
        "contentLength": 15713
      },
      "nextCards": [
        "ai-differential-privacy-example-visualization-apple-dp-emoji-analysis-6",
        "ai-differential-privacy-count-mean-sketch-cms-7"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#real-world-example:-apple-dp",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-components-1",
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-example-visualization-apple-dp-emoji-analysis-6"
      ]
    },
    {
      "id": "ai-differential-privacy-example-visualization-apple-dp-emoji-analysis-6",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "Privacy Budget and Definition",
      "title": "Example Visualization (Apple DP Emoji Analysis)",
      "subtitle": "Privacy Budget and Definition",
      "contentHtml": "<ul>\n  <li>Apple uses techniques like Count Mean Sketch to estimate popular emoji in a privacy-preserving way. Here’s an example histogram shared in their technical whitepaper:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/differential-privacy/emoji.jpg\" alt=\"\"></p>\n<ul>\n  <li>This visual illustrates how even noisy, privacy-preserving data collection can yield useful aggregate insights.</li>\n</ul>",
      "contentMarkdown": "*   Apple uses techniques like Count Mean Sketch to estimate popular emoji in a privacy-preserving way. Here’s an example histogram shared in their technical whitepaper:\n\n![](/primers/ai/assets/differential-privacy/emoji.jpg)\n\n*   This visual illustrates how even noisy, privacy-preserving data collection can yield useful aggregate insights.",
      "order": 6,
      "orderInChapter": 6,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 41,
        "contentLength": 397
      },
      "nextCards": [
        "ai-differential-privacy-count-mean-sketch-cms-7",
        "ai-differential-privacy-hadamard-count-mean-sketch-hcms-8"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-federated-learning-cross-device-federated-learning-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#example-visualization-(apple-dp-emoji-analysis)",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-components-1",
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-real-world-example-apple-dp-5"
      ]
    },
    {
      "id": "ai-differential-privacy-count-mean-sketch-cms-7",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "Techniques",
      "title": "Count Mean Sketch (CMS)",
      "subtitle": "Techniques",
      "contentHtml": "<ul>\n  <li>This technique creates a <strong>sketch matrix</strong> to represent user inputs compactly.</li>\n  <li>Data is <strong>hashed</strong> using functions like SHA-256 to discretize inputs into vectors.</li>\n  <li>Each coordinate of the hashed vector is then <strong>flipped with probability</strong>:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B5;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>/</mo></mrow><mn>2</mn></mrow></msup></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-115\" style=\"width: 4.169em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.44em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.628em, 1003.44em, 3.076em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-116\"><span class=\"mfrac\" id=\"MathJax-Span-117\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-118\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.076em, 1003.08em, 4.221em, -999.997em); top: -3.279em; left: 50%; margin-left: -1.56em;\"><span class=\"mrow\" id=\"MathJax-Span-119\"><span class=\"mn\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-121\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-122\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-123\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-124\"><span class=\"mrow\" id=\"MathJax-Span-125\"><span class=\"mi\" id=\"MathJax-Span-126\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">ε</span><span class=\"texatom\" id=\"MathJax-Span-127\"><span class=\"mrow\" id=\"MathJax-Span-128\"><span class=\"mo\" id=\"MathJax-Span-129\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">/</span></span></span><span class=\"mn\" id=\"MathJax-Span-130\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.23em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.232em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.997em; border-left: 0px solid; width: 0px; height: 2.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>ε</mi><mrow class=\"MJX-TeXAtom-ORD\"><mo>/</mo></mrow><mn>2</mn></mrow></msup></mrow></mfrac></math></span></span></div>\n<ul>\n  <li>This flipping introduces <strong>uncertainty</strong>, so the server cannot tell whether any specific bit was the result of a user’s actual data or the added noise.</li>\n</ul>\n<h4 id=\"workflow\">Workflow</h4>\n<ul>\n  <li>\n    <p>The workflow for CMS is as follows:</p>\n\n    <ol>\n      <li>User input is hashed and mapped into a vector space.</li>\n      <li>The vector is privatized using random flips.</li>\n      <li>Only <strong>one random row</strong> of the sketch matrix is transmitted to the server to further reduce information leakage.</li>\n      <li>The server <strong>aggregates</strong> noisy vectors from many users and <strong>averages them</strong>, estimating population-level statistics (e.g., the most common emojis or words).</li>\n    </ol>\n  </li>\n  <li>\n    <p>This setup provides good privacy with moderate utility, especially over large user populations.</p>\n  </li>\n</ul>\n<p>The workflow for CMS is as follows:</p>\n<ol>\n      <li>User input is hashed and mapped into a vector space.</li>\n      <li>The vector is privatized using random flips.</li>\n      <li>Only <strong>one random row</strong> of the sketch matrix is transmitted to the server to further reduce information leakage.</li>\n      <li>The server <strong>aggregates</strong> noisy vectors from many users and <strong>averages them</strong>, estimating population-level statistics (e.g., the most common emojis or words).</li>\n    </ol>\n<p>This setup provides good privacy with moderate utility, especially over large user populations.</p>",
      "contentMarkdown": "*   This technique creates a **sketch matrix** to represent user inputs compactly.\n*   Data is **hashed** using functions like SHA-256 to discretize inputs into vectors.\n*   Each coordinate of the hashed vector is then **flipped with probability**:\n\n11+eε/211+eε/2\n\n*   This flipping introduces **uncertainty**, so the server cannot tell whether any specific bit was the result of a user’s actual data or the added noise.\n\n#### Workflow\n\n*   The workflow for CMS is as follows:\n    \n    1.  User input is hashed and mapped into a vector space.\n    2.  The vector is privatized using random flips.\n    3.  Only **one random row** of the sketch matrix is transmitted to the server to further reduce information leakage.\n    4.  The server **aggregates** noisy vectors from many users and **averages them**, estimating population-level statistics (e.g., the most common emojis or words).\n*   This setup provides good privacy with moderate utility, especially over large user populations.\n    \n\nThe workflow for CMS is as follows:\n\n1.  User input is hashed and mapped into a vector space.\n2.  The vector is privatized using random flips.\n3.  Only **one random row** of the sketch matrix is transmitted to the server to further reduce information leakage.\n4.  The server **aggregates** noisy vectors from many users and **averages them**, estimating population-level statistics (e.g., the most common emojis or words).\n\nThis setup provides good privacy with moderate utility, especially over large user populations.",
      "order": 7,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 228,
        "contentLength": 5744
      },
      "nextCards": [
        "ai-differential-privacy-hadamard-count-mean-sketch-hcms-8",
        "ai-differential-privacy-dpsgd-core-idea-9"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#count-mean-sketch-(cms)",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-components-1",
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-real-world-example-apple-dp-5"
      ]
    },
    {
      "id": "ai-differential-privacy-hadamard-count-mean-sketch-hcms-8",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "Techniques",
      "title": "Hadamard Count Mean Sketch (HCMS)",
      "subtitle": "Techniques",
      "contentHtml": "<ul>\n  <li>A refinement over CMS, the HCMS adds a <strong>Hadamard basis transformation</strong> to the vector before privatization.</li>\n  <li>\n    <p>After this transformation:</p>\n\n    <ul>\n      <li>Only <strong>a single bit</strong> is sampled and transmitted to the server per query.</li>\n      <li>This reduces communication cost to <strong>just 1 bit per user per record</strong>, at the cost of slightly reduced accuracy.</li>\n    </ul>\n  </li>\n</ul>\n<p>After this transformation:</p>\n<ul>\n      <li>Only <strong>a single bit</strong> is sampled and transmitted to the server per query.</li>\n      <li>This reduces communication cost to <strong>just 1 bit per user per record</strong>, at the cost of slightly reduced accuracy.</li>\n    </ul>\n<h4 id=\"advantages\">Advantages</h4>\n<ul>\n  <li>More efficient in bandwidth-constrained settings (e.g., mobile devices).</li>\n  <li>\n    <p>Useful for <strong>telemetry collection at scale</strong>, where millions of users contribute daily.</p>\n  </li>\n  <li>Both techniques are used to power <strong>Apple’s differential privacy systems</strong>, as seen in their emoji, QuickType, and Safari data collection pipelines.</li>\n</ul>\n<p>Useful for <strong>telemetry collection at scale</strong>, where millions of users contribute daily.</p>",
      "contentMarkdown": "*   A refinement over CMS, the HCMS adds a **Hadamard basis transformation** to the vector before privatization.\n*   After this transformation:\n    \n    *   Only **a single bit** is sampled and transmitted to the server per query.\n    *   This reduces communication cost to **just 1 bit per user per record**, at the cost of slightly reduced accuracy.\n\nAfter this transformation:\n\n*   Only **a single bit** is sampled and transmitted to the server per query.\n*   This reduces communication cost to **just 1 bit per user per record**, at the cost of slightly reduced accuracy.\n\n#### Advantages\n\n*   More efficient in bandwidth-constrained settings (e.g., mobile devices).\n*   Useful for **telemetry collection at scale**, where millions of users contribute daily.\n    \n*   Both techniques are used to power **Apple’s differential privacy systems**, as seen in their emoji, QuickType, and Safari data collection pipelines.\n\nUseful for **telemetry collection at scale**, where millions of users contribute daily.",
      "order": 8,
      "orderInChapter": 2,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 150,
        "contentLength": 1290
      },
      "nextCards": [
        "ai-differential-privacy-dpsgd-core-idea-9",
        "ai-differential-privacy-pytorch-implementation-with-opacus-10"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-federated-learning-cross-device-federated-learning-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#hadamard-count-mean-sketch-(hcms)",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-components-1",
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-real-world-example-apple-dp-5"
      ]
    },
    {
      "id": "ai-differential-privacy-dpsgd-core-idea-9",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "DP‑SGD and Implementation",
      "title": "DP‑SGD: Core Idea",
      "subtitle": "DP‑SGD and Implementation",
      "contentHtml": "<ul>\n  <li>The standard stochastic gradient descent (SGD) algorithm is modified to ensure that updates do not leak private information. This is done by controlling the sensitivity of each training step via two key operations:</li>\n</ul>\n<ol>\n  <li>\n    <p><strong>Gradient Clipping</strong>:</p>\n\n    <ul>\n      <li>For each training example, compute the per-sample gradient.</li>\n      <li>\n        <p>Clip the gradient to a fixed maximum norm <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>C</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-131\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-132\"><span class=\"mi\" id=\"MathJax-Span-133\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>C</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">C</script>:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>g</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>&amp;#x2190;</mo><msub><mi>g</mi><mi>i</mi></msub><mo>&amp;#x22C5;</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>,</mo><mfrac><mi>C</mi><mrow><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><msub><mi>g</mi><mi>i</mi></msub><msub><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mn>2</mn></msub></mrow></mfrac></mrow><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-134\" style=\"width: 12.971em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.294em, 1010.63em, 4.846em, -999.997em); top: -3.799em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-135\"><span class=\"msubsup\" id=\"MathJax-Span-136\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-138\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">←</span><span class=\"msubsup\" id=\"MathJax-Span-140\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-141\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-142\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">min</span><span class=\"mrow\" id=\"MathJax-Span-145\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-146\" style=\"vertical-align: -0.466em;\"><span><span style=\"font-size: 111%; font-family: STIXSizeTwoSym;\">(</span></span></span><span class=\"mrow\" id=\"MathJax-Span-147\"><span class=\"mn\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-149\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mfrac\" id=\"MathJax-Span-150\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.362em;\"><span class=\"mi\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1002.24em, 4.378em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.143em;\"><span class=\"mrow\" id=\"MathJax-Span-152\"><span class=\"mo\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"msubsup\" id=\"MathJax-Span-154\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-155\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-157\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-158\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-159\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.35em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.346em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-160\" style=\"vertical-align: -0.466em;\"><span><span style=\"font-size: 111%; font-family: STIXSizeTwoSym;\">)</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.805em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 2.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>g</mi><mi>i</mi></msub><mo stretchy=\"false\">←</mo><msub><mi>g</mi><mi>i</mi></msub><mo>⋅</mo><mo movablelimits=\"true\" form=\"prefix\">min</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>,</mo><mfrac><mi>C</mi><mrow><mo fence=\"false\" stretchy=\"false\">‖</mo><msub><mi>g</mi><mi>i</mi></msub><msub><mo fence=\"false\" stretchy=\"false\">‖</mo><mn>2</mn></msub></mrow></mfrac></mrow><mo>)</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-22\">g_i \\leftarrow g_i \\cdot \\min\\left(1, \\frac{C}{\\|g_i\\|_2}\\right)</script>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Noise Addition</strong>:</p>\n\n    <ul>\n      <li>\n        <p>After averaging the clipped gradients, add Gaussian noise:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>g</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munder><mo>&amp;#x2211;</mo><mi>i</mi></munder><msub><mi>g</mi><mi>i</mi></msub><mo>+</mo><mi>N</mi><mo stretchy=&quot;false&quot;>(</mo><mn>0</mn><mo>,</mo><msup><mi>&amp;#x03C3;</mi><mn>2</mn></msup><msup><mi>C</mi><mn>2</mn></msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>I</mi></mrow><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-161\" style=\"width: 13.44em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.992em, 1011.15em, 3.909em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-162\"><span class=\"texatom\" id=\"MathJax-Span-163\"><span class=\"mrow\" id=\"MathJax-Span-164\"><span class=\"munderover\" id=\"MathJax-Span-165\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-166\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-167\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-168\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-169\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-170\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.63em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.628em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-172\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-173\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.273em, -999.997em); top: -2.862em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-174\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-175\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-176\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-177\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-180\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-181\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-182\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-183\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-184\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-185\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-186\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-187\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"mn\" id=\"MathJax-Span-188\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-189\"><span class=\"mrow\" id=\"MathJax-Span-190\"><span class=\"mi\" id=\"MathJax-Span-191\" style=\"font-family: STIXGeneral; font-weight: bold;\">I</span></span></span><span class=\"mo\" id=\"MathJax-Span-192\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>g</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>g</mi><mi>i</mi></msub><mo>+</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><msup><mi>C</mi><mn>2</mn></msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"bold\">I</mi></mrow><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-23\">\\bar{g} = \\frac{1}{n} \\sum_i g_i + N(0, \\sigma^2 C^2 \\mathbf{I})</script>\n      </li>\n      <li>\n        <p>Here, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-193\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-194\"><span class=\"mi\" id=\"MathJax-Span-195\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>σ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">\\sigma</script> is the <strong>noise multiplier</strong> that controls the privacy-utility trade-off.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Privacy Tracking</strong>:</p>\n\n    <ul>\n      <li>A <strong>privacy accountant</strong> (such as Rényi or Moments Accountant) keeps track of the cumulative privacy loss <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-196\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-197\"><span class=\"mi\" id=\"MathJax-Span-198\" style=\"font-family: STIXGeneral-Italic;\">ε</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">\\varepsilon</script> throughout training.</li>\n    </ul>\n  </li>\n</ol>\n<p><strong>Gradient Clipping</strong>:</p>\n<ul>\n      <li>For each training example, compute the per-sample gradient.</li>\n      <li>\n        <p>Clip the gradient to a fixed maximum norm <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>C</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-131\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-132\"><span class=\"mi\" id=\"MathJax-Span-133\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>C</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">C</script>:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>g</mi><mi>i</mi></msub><mo stretchy=&quot;false&quot;>&amp;#x2190;</mo><msub><mi>g</mi><mi>i</mi></msub><mo>&amp;#x22C5;</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>min</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>,</mo><mfrac><mi>C</mi><mrow><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><msub><mi>g</mi><mi>i</mi></msub><msub><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mn>2</mn></msub></mrow></mfrac></mrow><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-134\" style=\"width: 12.971em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.294em, 1010.63em, 4.846em, -999.997em); top: -3.799em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-135\"><span class=\"msubsup\" id=\"MathJax-Span-136\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-138\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">←</span><span class=\"msubsup\" id=\"MathJax-Span-140\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-141\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-142\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">min</span><span class=\"mrow\" id=\"MathJax-Span-145\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-146\" style=\"vertical-align: -0.466em;\"><span><span style=\"font-size: 111%; font-family: STIXSizeTwoSym;\">(</span></span></span><span class=\"mrow\" id=\"MathJax-Span-147\"><span class=\"mn\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-149\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mfrac\" id=\"MathJax-Span-150\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.362em;\"><span class=\"mi\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1002.24em, 4.378em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.143em;\"><span class=\"mrow\" id=\"MathJax-Span-152\"><span class=\"mo\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"msubsup\" id=\"MathJax-Span-154\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-155\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-157\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-158\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-159\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.35em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.346em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-160\" style=\"vertical-align: -0.466em;\"><span><span style=\"font-size: 111%; font-family: STIXSizeTwoSym;\">)</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.805em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.122em; border-left: 0px solid; width: 0px; height: 2.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>g</mi><mi>i</mi></msub><mo stretchy=\"false\">←</mo><msub><mi>g</mi><mi>i</mi></msub><mo>⋅</mo><mo movablelimits=\"true\" form=\"prefix\">min</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>,</mo><mfrac><mi>C</mi><mrow><mo fence=\"false\" stretchy=\"false\">‖</mo><msub><mi>g</mi><mi>i</mi></msub><msub><mo fence=\"false\" stretchy=\"false\">‖</mo><mn>2</mn></msub></mrow></mfrac></mrow><mo>)</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-22\">g_i \\leftarrow g_i \\cdot \\min\\left(1, \\frac{C}{\\|g_i\\|_2}\\right)</script>\n      </li>\n    </ul>\n<p>Clip the gradient to a fixed maximum norm <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>C</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-131\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-132\"><span class=\"mi\" id=\"MathJax-Span-133\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>C</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">C</script>:</p>\n<p><strong>Noise Addition</strong>:</p>\n<ul>\n      <li>\n        <p>After averaging the clipped gradients, add Gaussian noise:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>g</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munder><mo>&amp;#x2211;</mo><mi>i</mi></munder><msub><mi>g</mi><mi>i</mi></msub><mo>+</mo><mi>N</mi><mo stretchy=&quot;false&quot;>(</mo><mn>0</mn><mo>,</mo><msup><mi>&amp;#x03C3;</mi><mn>2</mn></msup><msup><mi>C</mi><mn>2</mn></msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;bold&quot;>I</mi></mrow><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-161\" style=\"width: 13.44em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.201em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.992em, 1011.15em, 3.909em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-162\"><span class=\"texatom\" id=\"MathJax-Span-163\"><span class=\"mrow\" id=\"MathJax-Span-164\"><span class=\"munderover\" id=\"MathJax-Span-165\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-166\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-167\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-168\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-169\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-170\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.63em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.628em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-172\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-173\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.273em, -999.997em); top: -2.862em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-174\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-175\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-176\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-177\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-180\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-181\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-182\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-183\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-184\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-185\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-186\"><span style=\"display: inline-block; position: relative; width: 1.201em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-187\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"mn\" id=\"MathJax-Span-188\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-189\"><span class=\"mrow\" id=\"MathJax-Span-190\"><span class=\"mi\" id=\"MathJax-Span-191\" style=\"font-family: STIXGeneral; font-weight: bold;\">I</span></span></span><span class=\"mo\" id=\"MathJax-Span-192\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>g</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>g</mi><mi>i</mi></msub><mo>+</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><msup><mi>C</mi><mn>2</mn></msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"bold\">I</mi></mrow><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-23\">\\bar{g} = \\frac{1}{n} \\sum_i g_i + N(0, \\sigma^2 C^2 \\mathbf{I})</script>\n      </li>\n      <li>\n        <p>Here, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-193\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-194\"><span class=\"mi\" id=\"MathJax-Span-195\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>σ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">\\sigma</script> is the <strong>noise multiplier</strong> that controls the privacy-utility trade-off.</p>\n      </li>\n    </ul>\n<p>After averaging the clipped gradients, add Gaussian noise:</p>\n<p>Here, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-193\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-194\"><span class=\"mi\" id=\"MathJax-Span-195\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>σ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">\\sigma</script> is the <strong>noise multiplier</strong> that controls the privacy-utility trade-off.</p>\n<p><strong>Privacy Tracking</strong>:</p>\n<ul>\n      <li>A <strong>privacy accountant</strong> (such as Rényi or Moments Accountant) keeps track of the cumulative privacy loss <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-196\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-197\"><span class=\"mi\" id=\"MathJax-Span-198\" style=\"font-family: STIXGeneral-Italic;\">ε</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">\\varepsilon</script> throughout training.</li>\n    </ul>",
      "contentMarkdown": "*   The standard stochastic gradient descent (SGD) algorithm is modified to ensure that updates do not leak private information. This is done by controlling the sensitivity of each training step via two key operations:\n\n1.  **Gradient Clipping**:\n    \n    *   For each training example, compute the per-sample gradient.\n    *   Clip the gradient to a fixed maximum norm CCC:\n        \n        gi←gi⋅min(1,C‖gi‖2)gi←gi⋅min(1,C‖gi‖2)\n        \n        g\\_i \\\\leftarrow g\\_i \\\\cdot \\\\min\\\\left(1, \\\\frac{C}{\\\\|g\\_i\\\\|\\_2}\\\\right)\n2.  **Noise Addition**:\n    \n    *   After averaging the clipped gradients, add Gaussian noise:\n        \n        g¯\\=1n∑igi+N(0,σ2C2I)g¯\\=1n∑igi+N(0,σ2C2I)\n        \n        \\\\bar{g} = \\\\frac{1}{n} \\\\sum\\_i g\\_i + N(0, \\\\sigma^2 C^2 \\\\mathbf{I})\n    *   Here, σσ\\\\sigma is the **noise multiplier** that controls the privacy-utility trade-off.\n        \n3.  **Privacy Tracking**:\n    \n    *   A **privacy accountant** (such as Rényi or Moments Accountant) keeps track of the cumulative privacy loss εε\\\\varepsilon throughout training.\n\n**Gradient Clipping**:\n\n*   For each training example, compute the per-sample gradient.\n*   Clip the gradient to a fixed maximum norm CCC:\n    \n    gi←gi⋅min(1,C‖gi‖2)gi←gi⋅min(1,C‖gi‖2)\n    \n    g\\_i \\\\leftarrow g\\_i \\\\cdot \\\\min\\\\left(1, \\\\frac{C}{\\\\|g\\_i\\\\|\\_2}\\\\right)\n\nClip the gradient to a fixed maximum norm CCC:\n\n**Noise Addition**:\n\n*   After averaging the clipped gradients, add Gaussian noise:\n    \n    g¯\\=1n∑igi+N(0,σ2C2I)g¯\\=1n∑igi+N(0,σ2C2I)\n    \n    \\\\bar{g} = \\\\frac{1}{n} \\\\sum\\_i g\\_i + N(0, \\\\sigma^2 C^2 \\\\mathbf{I})\n*   Here, σσ\\\\sigma is the **noise multiplier** that controls the privacy-utility trade-off.\n    \n\nAfter averaging the clipped gradients, add Gaussian noise:\n\nHere, σσ\\\\sigma is the **noise multiplier** that controls the privacy-utility trade-off.\n\n**Privacy Tracking**:\n\n*   A **privacy accountant** (such as Rényi or Moments Accountant) keeps track of the cumulative privacy loss εε\\\\varepsilon throughout training.",
      "order": 9,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "gradient descent"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 233,
        "contentLength": 42915
      },
      "nextCards": [
        "ai-differential-privacy-pytorch-implementation-with-opacus-10",
        "ai-differential-privacy-dpweights-post-training-differential-privacy-11"
      ],
      "relatedCards": [
        "ai-federated-learning-definition-2",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5",
        "ai-federated-learning-fedavg-federated-averaging-6",
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#dp‑sgd:-core-idea",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-components-1",
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-real-world-example-apple-dp-5"
      ]
    },
    {
      "id": "ai-differential-privacy-pytorch-implementation-with-opacus-10",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "DP‑SGD and Implementation",
      "title": "PyTorch Implementation with Opacus",
      "subtitle": "DP‑SGD and Implementation",
      "contentHtml": "<ul>\n  <li>Facebook’s <a href=\"https://opacus.ai\">Opacus</a> library makes it easy to implement DP‑SGD in PyTorch with minimal code changes:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\"><span class=\"kn\">from</span> <span class=\"nn\">opacus</span> <span class=\"kn\">import</span> <span class=\"n\">PrivacyEngine</span>\n\n<span class=\"n\">privacy_engine</span> <span class=\"o\">=</span> <span class=\"n\">PrivacyEngine</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"p\">,</span>\n    <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">,</span>\n    <span class=\"n\">sample_size</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"p\">),</span>\n    <span class=\"n\">noise_multiplier</span><span class=\"o\">=</span><span class=\"mf\">1.2</span><span class=\"p\">,</span>\n    <span class=\"n\">max_grad_norm</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>\n<span class=\"p\">)</span>\n<span class=\"n\">privacy_engine</span><span class=\"p\">.</span><span class=\"n\">attach</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\"><span class=\"kn\">from</span> <span class=\"nn\">opacus</span> <span class=\"kn\">import</span> <span class=\"n\">PrivacyEngine</span>\n\n<span class=\"n\">privacy_engine</span> <span class=\"o\">=</span> <span class=\"n\">PrivacyEngine</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"p\">,</span>\n    <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">256</span><span class=\"p\">,</span>\n    <span class=\"n\">sample_size</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"p\">),</span>\n    <span class=\"n\">noise_multiplier</span><span class=\"o\">=</span><span class=\"mf\">1.2</span><span class=\"p\">,</span>\n    <span class=\"n\">max_grad_norm</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>\n<span class=\"p\">)</span>\n<span class=\"n\">privacy_engine</span><span class=\"p\">.</span><span class=\"n\">attach</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li><strong><code class=\"language-plaintext highlighter-rouge\">max_grad_norm</code></strong> defines the clipping threshold <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>C</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-199\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-200\"><span class=\"mi\" id=\"MathJax-Span-201\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>C</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">C</script>.</li>\n  <li><strong><code class=\"language-plaintext highlighter-rouge\">noise_multiplier</code></strong> corresponds to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-202\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-203\"><span class=\"mi\" id=\"MathJax-Span-204\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>σ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">\\sigma</script>.</li>\n  <li>Opacus automatically modifies the model to track <strong>per-sample gradients</strong>, apply clipping and noise, and track <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-205\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-206\"><span class=\"mi\" id=\"MathJax-Span-207\" style=\"font-family: STIXGeneral-Italic;\">ε</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">\\varepsilon</script> across epochs.</li>\n</ul>",
      "contentMarkdown": "*   Facebook’s [Opacus](https://opacus.ai) library makes it easy to implement DP‑SGD in PyTorch with minimal code changes:\n\n![](https://aman.ai/images/copy.png)\n\n`from opacus import PrivacyEngine  privacy_engine = PrivacyEngine(     model,     batch_size=256,     sample_size=len(dataset),     noise_multiplier=1.2,     max_grad_norm=1.0 ) privacy_engine.attach(optimizer)`\n\n![](https://aman.ai/images/copy.png)\n\n`from opacus import PrivacyEngine  privacy_engine = PrivacyEngine(     model,     batch_size=256,     sample_size=len(dataset),     noise_multiplier=1.2,     max_grad_norm=1.0 ) privacy_engine.attach(optimizer)`\n\n*   **`max_grad_norm`** defines the clipping threshold CCC.\n*   **`noise_multiplier`** corresponds to σσ\\\\sigma.\n*   Opacus automatically modifies the model to track **per-sample gradients**, apply clipping and noise, and track εε\\\\varepsilon across epochs.",
      "order": 10,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": true,
        "wordCount": 77,
        "contentLength": 6975
      },
      "nextCards": [
        "ai-differential-privacy-dpweights-post-training-differential-privacy-11",
        "ai-differential-privacy-fine-tuning-with-dp-sgd-12"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#pytorch-implementation-with-opacus",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-components-1",
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-real-world-example-apple-dp-5"
      ]
    },
    {
      "id": "ai-differential-privacy-dpweights-post-training-differential-privacy-11",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "DP‑SGD and Implementation",
      "title": "DP‑Weights (Post-training Differential Privacy)",
      "subtitle": "DP‑SGD and Implementation",
      "contentHtml": "<ul>\n  <li>\n    <p>An alternative to DP‑SGD is <strong>DP‑Weights</strong>, where noise is added <strong>after training</strong>:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mi>&amp;#x03B8;</mi><mtext>private</mtext></msup><mo>=</mo><msup><mi>&amp;#x03B8;</mi><mtext>trained</mtext></msup><mo>+</mo><mi>N</mi><mo stretchy=&quot;false&quot;>(</mo><mn>0</mn><mo>,</mo><msup><mi>&amp;#x03C3;</mi><mn>2</mn></msup><mi>I</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-208\" style=\"width: 13.648em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.357em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1011.3em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-209\"><span class=\"msubsup\" id=\"MathJax-Span-210\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-211\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.576em;\"><span class=\"mtext\" id=\"MathJax-Span-212\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">private</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-213\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-214\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-215\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.576em;\"><span class=\"mtext\" id=\"MathJax-Span-216\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">trained</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-217\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-219\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-220\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-221\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-222\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-223\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-224\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-225\" style=\"font-family: STIXGeneral-Italic;\">I<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-226\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mi>θ</mi><mtext>private</mtext></msup><mo>=</mo><msup><mi>θ</mi><mtext>trained</mtext></msup><mo>+</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mi>I</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-29\">\\theta^\\text{private} = \\theta^\\text{trained} + N(0, \\sigma^2 I)</script>\n  </li>\n  <li>\n    <p>This is helpful when retraining is expensive, as it provides some privacy protection with lower overhead.</p>\n  </li>\n  <li>\n    <p>While not as strong as full DP‑SGD, it still <strong>prevents exact memorization</strong> of individual training records and can mitigate simple membership inference attacks.</p>\n  </li>\n</ul>\n<p>An alternative to DP‑SGD is <strong>DP‑Weights</strong>, where noise is added <strong>after training</strong>:</p>\n<p>This is helpful when retraining is expensive, as it provides some privacy protection with lower overhead.</p>\n<p>While not as strong as full DP‑SGD, it still <strong>prevents exact memorization</strong> of individual training records and can mitigate simple membership inference attacks.</p>",
      "contentMarkdown": "*   An alternative to DP‑SGD is **DP‑Weights**, where noise is added **after training**:\n    \n    θprivate\\=θtrained+N(0,σ2I)θprivate\\=θtrained+N(0,σ2I)\n    \n    \\\\theta^\\\\text{private} = \\\\theta^\\\\text{trained} + N(0, \\\\sigma^2 I)\n*   This is helpful when retraining is expensive, as it provides some privacy protection with lower overhead.\n    \n*   While not as strong as full DP‑SGD, it still **prevents exact memorization** of individual training records and can mitigate simple membership inference attacks.\n    \n\nAn alternative to DP‑SGD is **DP‑Weights**, where noise is added **after training**:\n\nThis is helpful when retraining is expensive, as it provides some privacy protection with lower overhead.\n\nWhile not as strong as full DP‑SGD, it still **prevents exact memorization** of individual training records and can mitigate simple membership inference attacks.",
      "order": 11,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 113,
        "contentLength": 6062
      },
      "nextCards": [
        "ai-differential-privacy-fine-tuning-with-dp-sgd-12",
        "ai-differential-privacy-membership-inference-protection-13"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#dp‑weights-(post-training-differential-privacy)",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-components-1",
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-real-world-example-apple-dp-5"
      ]
    },
    {
      "id": "ai-differential-privacy-fine-tuning-with-dp-sgd-12",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "Application to LLMs & NLP",
      "title": "Fine-Tuning with DP-SGD",
      "subtitle": "Application to LLMs & NLP",
      "contentHtml": "<p>When fine-tuning pre-trained LLMs on private datasets (e.g., clinical corpora or enterprise emails), the <strong>DP-SGD algorithm</strong> can be directly applied to protect individual data points. The typical pipeline involves:</p>\n<ol>\n  <li>\n    <p><strong>Privacy Parameter Selection</strong>:</p>\n\n    <ul>\n      <li>Choose an acceptable privacy budget <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-227\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-228\"><span class=\"mi\" id=\"MathJax-Span-229\" style=\"font-family: STIXGeneral-Italic;\">ε</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">\\varepsilon</script> and failure probability <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B4;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-230\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-231\"><span class=\"mi\" id=\"MathJax-Span-232\" style=\"font-family: STIXGeneral-Italic;\">δ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>δ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">\\delta</script>.</li>\n      <li>Common settings in literature: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi><mo>=</mo><mn>1</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-233\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-234\"><span class=\"mi\" id=\"MathJax-Span-235\" style=\"font-family: STIXGeneral-Italic;\">ε</span><span class=\"mo\" id=\"MathJax-Span-236\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-237\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi><mo>=</mo><mn>1</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">\\varepsilon = 1</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>10</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-238\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-239\"><span class=\"mn\" id=\"MathJax-Span-240\" style=\"font-family: STIXGeneral-Regular;\">10</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>10</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">10</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B4;</mi><mo>=</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>5</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-241\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1003.7em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-242\"><span class=\"mi\" id=\"MathJax-Span-243\" style=\"font-family: STIXGeneral-Italic;\">δ</span><span class=\"mo\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-245\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-246\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-247\"><span class=\"mrow\" id=\"MathJax-Span-248\"><span class=\"mo\" id=\"MathJax-Span-249\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-250\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>δ</mi><mo>=</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">\\delta = 10^{-5}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>8</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-251\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.93em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-252\"><span class=\"msubsup\" id=\"MathJax-Span-253\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-254\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-255\"><span class=\"mrow\" id=\"MathJax-Span-256\"><span class=\"mo\" id=\"MathJax-Span-257\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-258\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">8</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>8</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">10^{-8}</script>.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hyperparameter Calibration</strong>:</p>\n\n    <ul>\n      <li>Tune gradient clipping norm <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>C</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-259\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-260\"><span class=\"mi\" id=\"MathJax-Span-261\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>C</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">C</script>, noise multiplier <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-262\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-263\"><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>σ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">\\sigma</script>, batch size, and learning rate.</li>\n      <li>Use a <strong>calibration set</strong> (non-sensitive) to test different combinations that optimize accuracy under a fixed privacy budget.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Training and Privacy Accounting</strong>:</p>\n\n    <ul>\n      <li>Train the model using DP-SGD (e.g., via Opacus), and track the total privacy loss across epochs.</li>\n      <li>Use <strong>moments accountant</strong> or <strong>Rényi differential privacy accountant</strong> for tighter analysis of cumulative <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-265\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-266\"><span class=\"mi\" id=\"MathJax-Span-267\" style=\"font-family: STIXGeneral-Italic;\">ε</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">\\varepsilon</script>.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Model Evaluation</strong>:</p>\n\n    <ul>\n      <li>Evaluate utility trade-off: privacy often comes at the cost of <strong>increased perplexity</strong> or <strong>reduced BLEU/F1 scores</strong> in NLP tasks.</li>\n      <li>Trade-off improves with larger public pretraining and private fine-tuning.</li>\n    </ul>\n  </li>\n</ol>\n<p><strong>Privacy Parameter Selection</strong>:</p>\n<ul>\n      <li>Choose an acceptable privacy budget <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-227\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-228\"><span class=\"mi\" id=\"MathJax-Span-229\" style=\"font-family: STIXGeneral-Italic;\">ε</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">\\varepsilon</script> and failure probability <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B4;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-230\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-231\"><span class=\"mi\" id=\"MathJax-Span-232\" style=\"font-family: STIXGeneral-Italic;\">δ</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>δ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">\\delta</script>.</li>\n      <li>Common settings in literature: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi><mo>=</mo><mn>1</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-233\" style=\"width: 2.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.03em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-234\"><span class=\"mi\" id=\"MathJax-Span-235\" style=\"font-family: STIXGeneral-Italic;\">ε</span><span class=\"mo\" id=\"MathJax-Span-236\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-237\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi><mo>=</mo><mn>1</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">\\varepsilon = 1</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>10</mn></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-238\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.99em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-239\"><span class=\"mn\" id=\"MathJax-Span-240\" style=\"font-family: STIXGeneral-Regular;\">10</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>10</mn></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">10</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B4;</mi><mo>=</mo><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>5</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-241\" style=\"width: 4.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1003.7em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-242\"><span class=\"mi\" id=\"MathJax-Span-243\" style=\"font-family: STIXGeneral-Italic;\">δ</span><span class=\"mo\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-245\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-246\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-247\"><span class=\"mrow\" id=\"MathJax-Span-248\"><span class=\"mo\" id=\"MathJax-Span-249\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-250\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>δ</mi><mo>=</mo><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>5</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">\\delta = 10^{-5}</script> to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mn>10</mn><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x2212;</mo><mn>8</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-251\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1001.93em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-252\"><span class=\"msubsup\" id=\"MathJax-Span-253\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.99em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mn\" id=\"MathJax-Span-254\" style=\"font-family: STIXGeneral-Regular;\">10</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.992em;\"><span class=\"texatom\" id=\"MathJax-Span-255\"><span class=\"mrow\" id=\"MathJax-Span-256\"><span class=\"mo\" id=\"MathJax-Span-257\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-258\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">8</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mn>10</mn><mrow class=\"MJX-TeXAtom-ORD\"><mo>−</mo><mn>8</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">10^{-8}</script>.</li>\n    </ul>\n<p><strong>Hyperparameter Calibration</strong>:</p>\n<ul>\n      <li>Tune gradient clipping norm <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>C</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-259\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-260\"><span class=\"mi\" id=\"MathJax-Span-261\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>C</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">C</script>, noise multiplier <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03C3;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-262\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-263\"><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-family: STIXGeneral-Italic;\">σ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>σ</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">\\sigma</script>, batch size, and learning rate.</li>\n      <li>Use a <strong>calibration set</strong> (non-sensitive) to test different combinations that optimize accuracy under a fixed privacy budget.</li>\n    </ul>\n<p><strong>Training and Privacy Accounting</strong>:</p>\n<ul>\n      <li>Train the model using DP-SGD (e.g., via Opacus), and track the total privacy loss across epochs.</li>\n      <li>Use <strong>moments accountant</strong> or <strong>Rényi differential privacy accountant</strong> for tighter analysis of cumulative <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-265\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-266\"><span class=\"mi\" id=\"MathJax-Span-267\" style=\"font-family: STIXGeneral-Italic;\">ε</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">\\varepsilon</script>.</li>\n    </ul>\n<p><strong>Model Evaluation</strong>:</p>\n<ul>\n      <li>Evaluate utility trade-off: privacy often comes at the cost of <strong>increased perplexity</strong> or <strong>reduced BLEU/F1 scores</strong> in NLP tasks.</li>\n      <li>Trade-off improves with larger public pretraining and private fine-tuning.</li>\n    </ul>",
      "contentMarkdown": "When fine-tuning pre-trained LLMs on private datasets (e.g., clinical corpora or enterprise emails), the **DP-SGD algorithm** can be directly applied to protect individual data points. The typical pipeline involves:\n\n1.  **Privacy Parameter Selection**:\n    \n    *   Choose an acceptable privacy budget εε\\\\varepsilon and failure probability δδ\\\\delta.\n    *   Common settings in literature: ε\\=1ε\\=1\\\\varepsilon = 1 to 101010, δ\\=10−5δ\\=10−5\\\\delta = 10^{-5} to 10−810−810^{-8}.\n2.  **Hyperparameter Calibration**:\n    \n    *   Tune gradient clipping norm CCC, noise multiplier σσ\\\\sigma, batch size, and learning rate.\n    *   Use a **calibration set** (non-sensitive) to test different combinations that optimize accuracy under a fixed privacy budget.\n3.  **Training and Privacy Accounting**:\n    \n    *   Train the model using DP-SGD (e.g., via Opacus), and track the total privacy loss across epochs.\n    *   Use **moments accountant** or **Rényi differential privacy accountant** for tighter analysis of cumulative εε\\\\varepsilon.\n4.  **Model Evaluation**:\n    \n    *   Evaluate utility trade-off: privacy often comes at the cost of **increased perplexity** or **reduced BLEU/F1 scores** in NLP tasks.\n    *   Trade-off improves with larger public pretraining and private fine-tuning.\n\n**Privacy Parameter Selection**:\n\n*   Choose an acceptable privacy budget εε\\\\varepsilon and failure probability δδ\\\\delta.\n*   Common settings in literature: ε\\=1ε\\=1\\\\varepsilon = 1 to 101010, δ\\=10−5δ\\=10−5\\\\delta = 10^{-5} to 10−810−810^{-8}.\n\n**Hyperparameter Calibration**:\n\n*   Tune gradient clipping norm CCC, noise multiplier σσ\\\\sigma, batch size, and learning rate.\n*   Use a **calibration set** (non-sensitive) to test different combinations that optimize accuracy under a fixed privacy budget.\n\n**Training and Privacy Accounting**:\n\n*   Train the model using DP-SGD (e.g., via Opacus), and track the total privacy loss across epochs.\n*   Use **moments accountant** or **Rényi differential privacy accountant** for tighter analysis of cumulative εε\\\\varepsilon.\n\n**Model Evaluation**:\n\n*   Evaluate utility trade-off: privacy often comes at the cost of **increased perplexity** or **reduced BLEU/F1 scores** in NLP tasks.\n*   Trade-off improves with larger public pretraining and private fine-tuning.",
      "order": 12,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "llm",
        "nlp",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 295,
        "contentLength": 29442
      },
      "nextCards": [
        "ai-differential-privacy-membership-inference-protection-13",
        "ai-differential-privacy-recent-strategies-14"
      ],
      "relatedCards": [
        "ai-model-compression-concept-44",
        "ai-model-compression-key-takeaways-49",
        "ai-model-compression-popular-quantization-libraries-19",
        "ai-model-compression-pros-cons-47",
        "ai-federated-learning-llm-specific-enhancements-15"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#fine-tuning-with-dp-sgd",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-components-1",
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-real-world-example-apple-dp-5"
      ]
    },
    {
      "id": "ai-differential-privacy-membership-inference-protection-13",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "Application to LLMs & NLP",
      "title": "Membership Inference & Protection",
      "subtitle": "Application to LLMs & NLP",
      "contentHtml": "<ul>\n  <li>Differential privacy is particularly effective at <strong>mitigating membership inference attacks</strong>—where an attacker queries a model and tries to determine whether specific records were part of its training set.</li>\n  <li>In NLP, these attacks are especially potent: LLMs can <strong>memorize rare sequences</strong> (e.g., uncommon names or numbers), which attackers can prompt into being reproduced.</li>\n  <li>DP-trained models are <strong>less likely to memorize</strong> such outliers, improving privacy robustness in deployment.</li>\n</ul>",
      "contentMarkdown": "*   Differential privacy is particularly effective at **mitigating membership inference attacks**—where an attacker queries a model and tries to determine whether specific records were part of its training set.\n*   In NLP, these attacks are especially potent: LLMs can **memorize rare sequences** (e.g., uncommon names or numbers), which attackers can prompt into being reproduced.\n*   DP-trained models are **less likely to memorize** such outliers, improving privacy robustness in deployment.",
      "order": 13,
      "orderInChapter": 2,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "llm",
        "nlp"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 69,
        "contentLength": 565
      },
      "nextCards": [
        "ai-differential-privacy-recent-strategies-14",
        "ai-differential-privacy-pros-15"
      ],
      "relatedCards": [
        "ai-on-device-transformers-neural-processing-unit-npu-6",
        "ai-model-compression-accuracy-results-18",
        "ai-model-compression-concept-44",
        "ai-model-compression-key-takeaways-49",
        "ai-model-compression-background-precision-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#membership-inference-&-protection",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-components-1",
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-real-world-example-apple-dp-5"
      ]
    },
    {
      "id": "ai-differential-privacy-recent-strategies-14",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "Application to LLMs & NLP",
      "title": "Recent Strategies",
      "subtitle": "Application to LLMs & NLP",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Pre-train with DP, fine-tune with public data</strong>:</p>\n\n    <ul>\n      <li>Train a model under strong DP constraints (e.g., on private corpora).</li>\n      <li>Fine-tune on open-domain datasets (e.g., Wikipedia, C4) to <strong>recover utility</strong> without additional privacy cost.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Public target-tuning</strong>:</p>\n\n    <ul>\n      <li>Fine-tune a public model on public data to a target task.</li>\n      <li>Use DP-SGD on private examples only to adapt the model (e.g., via LoRA adapters) while preserving privacy.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pre-train with DP, fine-tune with public data</strong>:</p>\n<ul>\n      <li>Train a model under strong DP constraints (e.g., on private corpora).</li>\n      <li>Fine-tune on open-domain datasets (e.g., Wikipedia, C4) to <strong>recover utility</strong> without additional privacy cost.</li>\n    </ul>\n<p><strong>Public target-tuning</strong>:</p>\n<ul>\n      <li>Fine-tune a public model on public data to a target task.</li>\n      <li>Use DP-SGD on private examples only to adapt the model (e.g., via LoRA adapters) while preserving privacy.</li>\n    </ul>",
      "contentMarkdown": "*   **Pre-train with DP, fine-tune with public data**:\n    \n    *   Train a model under strong DP constraints (e.g., on private corpora).\n    *   Fine-tune on open-domain datasets (e.g., Wikipedia, C4) to **recover utility** without additional privacy cost.\n*   **Public target-tuning**:\n    \n    *   Fine-tune a public model on public data to a target task.\n    *   Use DP-SGD on private examples only to adapt the model (e.g., via LoRA adapters) while preserving privacy.\n\n**Pre-train with DP, fine-tune with public data**:\n\n*   Train a model under strong DP constraints (e.g., on private corpora).\n*   Fine-tune on open-domain datasets (e.g., Wikipedia, C4) to **recover utility** without additional privacy cost.\n\n**Public target-tuning**:\n\n*   Fine-tune a public model on public data to a target task.\n*   Use DP-SGD on private examples only to adapt the model (e.g., via LoRA adapters) while preserving privacy.",
      "order": 14,
      "orderInChapter": 3,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 134,
        "contentLength": 1192
      },
      "nextCards": [
        "ai-differential-privacy-pros-15",
        "ai-differential-privacy-cons-16"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-federated-learning-cross-device-federated-learning-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#recent-strategies",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-components-1",
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-real-world-example-apple-dp-5"
      ]
    },
    {
      "id": "ai-differential-privacy-pros-15",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "Pros & Cons",
      "title": "Pros",
      "subtitle": "Pros & Cons",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Formal Privacy Guarantees</strong>: DP provides mathematically provable bounds on the risk of individual data exposure—offering strong defense against attacks like membership inference and data reconstruction.</p>\n  </li>\n  <li>\n    <p><strong>Tooling Ecosystem</strong>: Libraries like <strong>Opacus</strong> (PyTorch) and <strong>TensorFlow Privacy</strong> streamline DP integration with deep learning models, supporting gradient clipping, noise addition, and privacy accounting.</p>\n  </li>\n  <li>\n    <p><strong>General Applicability</strong>: Can be applied to various components: model training (via DP-SGD), post-training (via DP-Weights), or user-level contributions (via local DP mechanisms).</p>\n  </li>\n  <li>\n    <p><strong>Defends Against Real-World Threats</strong>: Especially valuable in medical, financial, or sensitive NLP domains where privacy breaches can have serious consequences.</p>\n  </li>\n</ul>\n<p><strong>Formal Privacy Guarantees</strong>: DP provides mathematically provable bounds on the risk of individual data exposure—offering strong defense against attacks like membership inference and data reconstruction.</p>\n<p><strong>Tooling Ecosystem</strong>: Libraries like <strong>Opacus</strong> (PyTorch) and <strong>TensorFlow Privacy</strong> streamline DP integration with deep learning models, supporting gradient clipping, noise addition, and privacy accounting.</p>\n<p><strong>General Applicability</strong>: Can be applied to various components: model training (via DP-SGD), post-training (via DP-Weights), or user-level contributions (via local DP mechanisms).</p>\n<p><strong>Defends Against Real-World Threats</strong>: Especially valuable in medical, financial, or sensitive NLP domains where privacy breaches can have serious consequences.</p>",
      "contentMarkdown": "*   **Formal Privacy Guarantees**: DP provides mathematically provable bounds on the risk of individual data exposure—offering strong defense against attacks like membership inference and data reconstruction.\n    \n*   **Tooling Ecosystem**: Libraries like **Opacus** (PyTorch) and **TensorFlow Privacy** streamline DP integration with deep learning models, supporting gradient clipping, noise addition, and privacy accounting.\n    \n*   **General Applicability**: Can be applied to various components: model training (via DP-SGD), post-training (via DP-Weights), or user-level contributions (via local DP mechanisms).\n    \n*   **Defends Against Real-World Threats**: Especially valuable in medical, financial, or sensitive NLP domains where privacy breaches can have serious consequences.\n    \n\n**Formal Privacy Guarantees**: DP provides mathematically provable bounds on the risk of individual data exposure—offering strong defense against attacks like membership inference and data reconstruction.\n\n**Tooling Ecosystem**: Libraries like **Opacus** (PyTorch) and **TensorFlow Privacy** streamline DP integration with deep learning models, supporting gradient clipping, noise addition, and privacy accounting.\n\n**General Applicability**: Can be applied to various components: model training (via DP-SGD), post-training (via DP-Weights), or user-level contributions (via local DP mechanisms).\n\n**Defends Against Real-World Threats**: Especially valuable in medical, financial, or sensitive NLP domains where privacy breaches can have serious consequences.",
      "order": 15,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "deep learning",
        "nlp"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 186,
        "contentLength": 1812
      },
      "nextCards": [
        "ai-differential-privacy-cons-16"
      ],
      "relatedCards": [
        "ai-model-compression-why-knowledge-distillation-works-26",
        "ai-model-compression-definition-3",
        "ai-model-compression-accuracy-results-18",
        "ai-model-compression-overview-40",
        "ai-model-compression-background-precision-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#pros",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-components-1",
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-real-world-example-apple-dp-5"
      ]
    },
    {
      "id": "ai-differential-privacy-cons-16",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "Differential Privacy",
      "articleSlug": "differential-privacy",
      "chapter": "Pros & Cons",
      "title": "Cons",
      "subtitle": "Pros & Cons",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Utility Degradation</strong>: Injecting noise during training often leads to <strong>higher perplexity</strong>, <strong>lower F1 scores</strong>, or degraded generation quality—especially for smaller datasets or tasks requiring fine detail.</p>\n  </li>\n  <li>\n    <p><strong>Complex Hyperparameter Tuning</strong>: Requires careful balancing of noise multiplier, clipping norms, batch sizes, and learning rate. Poor tuning can nullify either privacy or utility.</p>\n  </li>\n  <li>\n    <p><strong>Privacy Budget Exhaustion</strong>: Each training epoch and query consumes part of the privacy budget <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-268\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-269\"><span class=\"mi\" id=\"MathJax-Span-270\" style=\"font-family: STIXGeneral-Italic;\">ε</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">\\varepsilon</script>; long training or frequent evaluations can <strong>accumulate privacy loss</strong> quickly.</p>\n  </li>\n  <li>\n    <p><strong>Scalability Concerns</strong>: Computing per-sample gradients (needed for DP-SGD) is computationally expensive, especially for large LLMs.</p>\n  </li>\n  <li>\n    <p><strong>Task Sensitivity</strong>: Some NLP tasks (e.g., rare token generation, open-ended dialogue) are more vulnerable to degradation under DP due to their reliance on nuanced memorization.</p>\n  </li>\n</ul>\n<p><strong>Utility Degradation</strong>: Injecting noise during training often leads to <strong>higher perplexity</strong>, <strong>lower F1 scores</strong>, or degraded generation quality—especially for smaller datasets or tasks requiring fine detail.</p>\n<p><strong>Complex Hyperparameter Tuning</strong>: Requires careful balancing of noise multiplier, clipping norms, batch sizes, and learning rate. Poor tuning can nullify either privacy or utility.</p>\n<p><strong>Privacy Budget Exhaustion</strong>: Each training epoch and query consumes part of the privacy budget <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>&amp;#x03B5;</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-268\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-269\"><span class=\"mi\" id=\"MathJax-Span-270\" style=\"font-family: STIXGeneral-Italic;\">ε</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>ε</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">\\varepsilon</script>; long training or frequent evaluations can <strong>accumulate privacy loss</strong> quickly.</p>\n<p><strong>Scalability Concerns</strong>: Computing per-sample gradients (needed for DP-SGD) is computationally expensive, especially for large LLMs.</p>\n<p><strong>Task Sensitivity</strong>: Some NLP tasks (e.g., rare token generation, open-ended dialogue) are more vulnerable to degradation under DP due to their reliance on nuanced memorization.</p>",
      "contentMarkdown": "*   **Utility Degradation**: Injecting noise during training often leads to **higher perplexity**, **lower F1 scores**, or degraded generation quality—especially for smaller datasets or tasks requiring fine detail.\n    \n*   **Complex Hyperparameter Tuning**: Requires careful balancing of noise multiplier, clipping norms, batch sizes, and learning rate. Poor tuning can nullify either privacy or utility.\n    \n*   **Privacy Budget Exhaustion**: Each training epoch and query consumes part of the privacy budget εε\\\\varepsilon; long training or frequent evaluations can **accumulate privacy loss** quickly.\n    \n*   **Scalability Concerns**: Computing per-sample gradients (needed for DP-SGD) is computationally expensive, especially for large LLMs.\n    \n*   **Task Sensitivity**: Some NLP tasks (e.g., rare token generation, open-ended dialogue) are more vulnerable to degradation under DP due to their reliance on nuanced memorization.\n    \n\n**Utility Degradation**: Injecting noise during training often leads to **higher perplexity**, **lower F1 scores**, or degraded generation quality—especially for smaller datasets or tasks requiring fine detail.\n\n**Complex Hyperparameter Tuning**: Requires careful balancing of noise multiplier, clipping norms, batch sizes, and learning rate. Poor tuning can nullify either privacy or utility.\n\n**Privacy Budget Exhaustion**: Each training epoch and query consumes part of the privacy budget εε\\\\varepsilon; long training or frequent evaluations can **accumulate privacy loss** quickly.\n\n**Scalability Concerns**: Computing per-sample gradients (needed for DP-SGD) is computationally expensive, especially for large LLMs.\n\n**Task Sensitivity**: Some NLP tasks (e.g., rare token generation, open-ended dialogue) are more vulnerable to degradation under DP due to their reliance on nuanced memorization.",
      "order": 16,
      "orderInChapter": 2,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "llm",
        "nlp"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 235,
        "contentLength": 4551
      },
      "nextCards": [],
      "relatedCards": [
        "ai-model-compression-concept-44",
        "ai-model-compression-key-takeaways-49",
        "ai-on-device-transformers-neural-processing-unit-npu-6",
        "ai-model-compression-accuracy-results-18",
        "ai-model-compression-overview-42"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/differential-privacy/#cons",
      "scrapedAt": "2025-12-28T11:56:06.248Z",
      "siblings": [
        "ai-differential-privacy-components-1",
        "ai-differential-privacy-intuition-2",
        "ai-differential-privacy-tightness-3",
        "ai-differential-privacy-privacy-budget-in-practice-4",
        "ai-differential-privacy-real-world-example-apple-dp-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-encoder-compute-bound-nature-1",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Architectural Overview and Compute Characteristics of Transformers",
      "title": "Encoder: Compute-bound Nature",
      "subtitle": "Architectural Overview and Compute Characteristics of Transformers",
      "contentHtml": "<ul>\n  <li>\n    <p>The encoder performs operations on entire sequences in parallel. The main computational components are:</p>\n\n    <ol>\n      <li><strong>Multi-Head Self-Attention (MHSA)</strong></li>\n      <li><strong>Feedforward Networks (FFN)</strong></li>\n      <li><strong>LayerNorm / Dropout (lightweight)</strong></li>\n    </ol>\n  </li>\n  <li>\n    <p>The attention operation in the encoder is full-sequence:</p>\n\n    <ul>\n      <li>\n        <p>Attention Score Calculation:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>Attention</mtext><mo stretchy=&quot;false&quot;>(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo>(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo>)</mo></mrow><mi>V</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-7\" style=\"width: 20.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 17.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.19em, 1017.24em, 5.576em, -999.997em); top: -4.112em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-8\"><span class=\"mtext\" id=\"MathJax-Span-9\" style=\"font-family: STIXGeneral-Regular;\">Attention</span><span class=\"mo\" id=\"MathJax-Span-10\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-11\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-13\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-14\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-16\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-17\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mtext\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">softmax</span><span class=\"mrow\" id=\"MathJax-Span-19\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-20\" style=\"vertical-align: -0.779em;\"><span style=\"font-family: STIXSizeFourSym;\">(</span></span><span class=\"mfrac\" id=\"MathJax-Span-21\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.023em, 1002.03em, 4.326em, -999.997em); top: -4.685em; left: 50%; margin-left: -1.039em;\"><span class=\"mrow\" id=\"MathJax-Span-22\"><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-24\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-25\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.971em, 1001.88em, 4.534em, -999.997em); top: -3.122em; left: 50%; margin-left: -0.935em;\"><span class=\"msqrt\" id=\"MathJax-Span-27\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.326em, -999.997em); top: -4.008em; left: 0.94em;\"><span class=\"mrow\" id=\"MathJax-Span-28\"><span class=\"msubsup\" id=\"MathJax-Span-29\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-31\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.023em, 1000.94em, 3.388em, -999.997em); top: -4.06em; left: 0.94em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0.419em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.867em, 1000.94em, 4.43em, -999.997em); top: -3.904em; left: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">√</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.14em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.138em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-32\" style=\"vertical-align: -0.779em;\"><span style=\"font-family: STIXSizeFourSym;\">)</span></span></span><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.622em; border-left: 0px solid; width: 0px; height: 3.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>Attention</mtext><mo stretchy=\"false\">(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo>(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo>)</mo></mrow><mi>V</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-3\">\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)V</script>\n\n        <ul>\n          <li>\n            <p>where:</p>\n\n            <ul>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>Q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-34\" style=\"width: 4.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.12em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-35\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-38\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-39\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-41\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>Q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">Q = XW_Q</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>K</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-42\" style=\"width: 5.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.169em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.17em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-43\"><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-45\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-47\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>K</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">K = XW_K</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>V</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-50\" style=\"width: 5.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.169em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.17em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-51\"><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-55\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-57\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>V</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">V = XW_V</script></li>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-58\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-59\"><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">X</script>: Input sequence</li>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>Q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>K</mi></msub><mo>,</mo><msub><mi>W</mi><mi>V</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-61\" style=\"width: 6.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.159em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.16em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-62\"><span class=\"msubsup\" id=\"MathJax-Span-63\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-67\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-68\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-71\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>Q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>K</mi></msub><mo>,</mo><msub><mi>W</mi><mi>V</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">W_Q, W_K, W_V</script>: Projection matrices</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>This means the compute scales as <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-74\" style=\"width: 4.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1003.8em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-75\"><span class=\"mi\" id=\"MathJax-Span-76\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-77\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-78\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-79\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-80\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-81\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-83\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">O(n^2 \\cdot d)</script>, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-84\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-85\"><span class=\"mi\" id=\"MathJax-Span-86\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">n</script> is the sequence length and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-87\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-88\"><span class=\"mi\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">d</script> is the hidden dimension.</p>\n  </li>\n  <li>\n    <p>FFN consists of two matrix multiplications:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>FFN</mtext><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo><mo stretchy=&quot;false&quot;>(</mo><mn>0</mn><mo>,</mo><mi>x</mi><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo stretchy=&quot;false&quot;>)</mo><msub><mi>W</mi><mn>2</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-90\" style=\"width: 17.607em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.638em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1014.64em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-91\"><span class=\"mtext\" id=\"MathJax-Span-92\" style=\"font-family: STIXGeneral-Regular;\">FFN</span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-96\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-97\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">max</span><span class=\"mo\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-99\" style=\"font-family: STIXGeneral-Regular;\">0</span><span class=\"mo\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-102\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-104\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-105\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-106\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Italic;\">b</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-108\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-109\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"msubsup\" id=\"MathJax-Span-110\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-112\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-113\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"msubsup\" id=\"MathJax-Span-114\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-115\" style=\"font-family: STIXGeneral-Italic;\">b</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-116\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>FFN</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mo movablelimits=\"true\" form=\"prefix\">max</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo><msub><mi>W</mi><mn>2</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-12\">\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2</script>\n\n    <ul>\n      <li>\n        <p>Typically, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>1</mn></msub><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mn>4</mn><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-117\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.85em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-118\"><span class=\"msubsup\" id=\"MathJax-Span-119\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-121\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-122\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-123\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-124\"><span class=\"mrow\" id=\"MathJax-Span-125\"><span class=\"mi\" id=\"MathJax-Span-126\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-127\"><span class=\"mrow\" id=\"MathJax-Span-128\"><span class=\"mi\" id=\"MathJax-Span-129\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-130\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mn\" id=\"MathJax-Span-131\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4</span><span class=\"mi\" id=\"MathJax-Span-132\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>1</mn></msub><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mn>4</mn><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">W_1 \\in \\mathbb{R}^{d \\times 4d}</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>2</mn></msub><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>4</mn><mi>d</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-133\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.85em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-134\"><span class=\"msubsup\" id=\"MathJax-Span-135\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-136\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-137\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-139\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-140\"><span class=\"mrow\" id=\"MathJax-Span-141\"><span class=\"mi\" id=\"MathJax-Span-142\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-143\"><span class=\"mrow\" id=\"MathJax-Span-144\"><span class=\"mn\" id=\"MathJax-Span-145\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4</span><span class=\"mi\" id=\"MathJax-Span-146\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-147\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-148\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>2</mn></msub><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>4</mn><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">W_2 \\in \\mathbb{R}^{4d \\times d}</script></p>\n      </li>\n      <li>\n        <p>Note that the reason there are <strong>two</strong> matrix multiplications is that the feed-forward network inside each transformer block is actually a <strong>two-layer position-wise fully connected network</strong>:</p>\n\n        <ol>\n          <li>The first matrix multiplication with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>1</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-149\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-150\"><span class=\"msubsup\" id=\"MathJax-Span-151\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-153\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>1</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">W_1</script> projects the hidden representation from dimension <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-154\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-155\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">d</script> (the model’s hidden size) <strong>up</strong> to a larger intermediate dimension, typically <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>4</mn><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-157\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.04em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-158\"><span class=\"mn\" id=\"MathJax-Span-159\" style=\"font-family: STIXGeneral-Regular;\">4</span><span class=\"mi\" id=\"MathJax-Span-160\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>4</mn><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">4d</script>, to increase model capacity and introduce a richer representation space.</li>\n          <li>The second matrix multiplication with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>2</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-161\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-162\"><span class=\"msubsup\" id=\"MathJax-Span-163\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-164\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-165\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>2</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">W_2</script> projects the intermediate vector <strong>back down</strong> to dimension <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-166\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-167\"><span class=\"mi\" id=\"MathJax-Span-168\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">d</script>, so that the output shape matches the residual connection input and can be passed to the next transformer layer.</li>\n        </ol>\n      </li>\n      <li>\n        <p>Between these two layers, a non-linearity (often ReLU or GELU) is applied, enabling the FFN to learn complex transformations beyond what a single linear layer could achieve.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>These operations are <strong>highly parallelizable</strong> and benefit from <strong>vectorized matrix operations</strong>, making them well-suited for GPUs and TPUs. Even CPUs can achieve good performance <a href=\"https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html\">Math Kernel Library (oneMKL)</a>/<a href=\"https://github.com/Jakob6174/Intel-DNNL\">Deep Neural Network Library (DNNL)</a> or <a href=\"https://github.com/uxlfoundation/oneDNN\">oneAPI Deep Neural Network Library (oneDNN)</a> optimized kernels — but they are generally slower due to fewer cores and lower memory bandwidth.</p>\n  </li>\n  <li>\n    <p>In short:</p>\n\n    <ul>\n      <li><strong>Memory usage:</strong> Moderate</li>\n      <li><strong>Compute intensity:</strong> High</li>\n      <li><strong>Best suited hardware:</strong> GPU, NPU (CPU feasible for small models or batch sizes)</li>\n    </ul>\n  </li>\n</ul>\n<p>The encoder performs operations on entire sequences in parallel. The main computational components are:</p>\n<ol>\n      <li><strong>Multi-Head Self-Attention (MHSA)</strong></li>\n      <li><strong>Feedforward Networks (FFN)</strong></li>\n      <li><strong>LayerNorm / Dropout (lightweight)</strong></li>\n    </ol>\n<p>The attention operation in the encoder is full-sequence:</p>\n<ul>\n      <li>\n        <p>Attention Score Calculation:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>Attention</mtext><mo stretchy=&quot;false&quot;>(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo>(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo>)</mo></mrow><mi>V</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-7\" style=\"width: 20.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 17.242em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.19em, 1017.24em, 5.576em, -999.997em); top: -4.112em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-8\"><span class=\"mtext\" id=\"MathJax-Span-9\" style=\"font-family: STIXGeneral-Regular;\">Attention</span><span class=\"mo\" id=\"MathJax-Span-10\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-11\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-13\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-14\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-16\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-17\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mtext\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">softmax</span><span class=\"mrow\" id=\"MathJax-Span-19\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-20\" style=\"vertical-align: -0.779em;\"><span style=\"font-family: STIXSizeFourSym;\">(</span></span><span class=\"mfrac\" id=\"MathJax-Span-21\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.023em, 1002.03em, 4.326em, -999.997em); top: -4.685em; left: 50%; margin-left: -1.039em;\"><span class=\"mrow\" id=\"MathJax-Span-22\"><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"msubsup\" id=\"MathJax-Span-24\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-25\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.971em, 1001.88em, 4.534em, -999.997em); top: -3.122em; left: 50%; margin-left: -0.935em;\"><span class=\"msqrt\" id=\"MathJax-Span-27\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.326em, -999.997em); top: -4.008em; left: 0.94em;\"><span class=\"mrow\" id=\"MathJax-Span-28\"><span class=\"msubsup\" id=\"MathJax-Span-29\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-31\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.023em, 1000.94em, 3.388em, -999.997em); top: -4.06em; left: 0.94em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0.419em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.867em, 1000.94em, 4.43em, -999.997em); top: -3.904em; left: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">√</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.14em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.138em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-32\" style=\"vertical-align: -0.779em;\"><span style=\"font-family: STIXSizeFourSym;\">)</span></span></span><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.622em; border-left: 0px solid; width: 0px; height: 3.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>Attention</mtext><mo stretchy=\"false\">(</mo><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo>(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo>)</mo></mrow><mi>V</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-3\">\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)V</script>\n\n        <ul>\n          <li>\n            <p>where:</p>\n\n            <ul>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>Q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-34\" style=\"width: 4.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.12em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-35\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-38\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-39\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-41\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>Q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">Q = XW_Q</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>K</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-42\" style=\"width: 5.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.169em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.17em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-43\"><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-45\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-47\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>K</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">K = XW_K</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>V</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-50\" style=\"width: 5.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.169em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.17em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-51\"><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-55\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-57\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>V</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">V = XW_V</script></li>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-58\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-59\"><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">X</script>: Input sequence</li>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>Q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>K</mi></msub><mo>,</mo><msub><mi>W</mi><mi>V</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-61\" style=\"width: 6.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.159em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.16em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-62\"><span class=\"msubsup\" id=\"MathJax-Span-63\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-67\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-68\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-71\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>Q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>K</mi></msub><mo>,</mo><msub><mi>W</mi><mi>V</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">W_Q, W_K, W_V</script>: Projection matrices</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n    </ul>\n<p>Attention Score Calculation:</p>\n<ul>\n          <li>\n            <p>where:</p>\n\n            <ul>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>Q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-34\" style=\"width: 4.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.12em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-35\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-38\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-39\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-41\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>Q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">Q = XW_Q</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>K</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-42\" style=\"width: 5.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.169em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.17em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-43\"><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-45\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-47\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>K</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">K = XW_K</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>V</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-50\" style=\"width: 5.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.169em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.17em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-51\"><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-55\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-57\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>V</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">V = XW_V</script></li>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-58\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-59\"><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">X</script>: Input sequence</li>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>Q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>K</mi></msub><mo>,</mo><msub><mi>W</mi><mi>V</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-61\" style=\"width: 6.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.159em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.16em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-62\"><span class=\"msubsup\" id=\"MathJax-Span-63\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-67\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-68\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-71\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>Q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>K</mi></msub><mo>,</mo><msub><mi>W</mi><mi>V</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">W_Q, W_K, W_V</script>: Projection matrices</li>\n            </ul>\n          </li>\n        </ul>\n<p>where:</p>\n<ul>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>Q</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-34\" style=\"width: 4.951em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.12em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-35\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-38\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-39\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-41\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>Q</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">Q = XW_Q</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>K</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-42\" style=\"width: 5.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.169em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.17em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-43\"><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-45\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-47\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-49\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>K</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">K = XW_K</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>V</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-50\" style=\"width: 5.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.169em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.17em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-51\"><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-55\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-57\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi><mo>=</mo><mi>X</mi><msub><mi>W</mi><mi>V</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">V = XW_V</script></li>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-58\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-59\"><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">X</script>: Input sequence</li>\n              <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mi>Q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>K</mi></msub><mo>,</mo><msub><mi>W</mi><mi>V</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-61\" style=\"width: 6.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.159em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1005.16em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-62\"><span class=\"msubsup\" id=\"MathJax-Span-63\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-67\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-68\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-70\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-71\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-72\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mi>Q</mi></msub><mo>,</mo><msub><mi>W</mi><mi>K</mi></msub><mo>,</mo><msub><mi>W</mi><mi>V</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">W_Q, W_K, W_V</script>: Projection matrices</li>\n            </ul>\n<p>This means the compute scales as <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-74\" style=\"width: 4.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1003.8em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-75\"><span class=\"mi\" id=\"MathJax-Span-76\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-77\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-78\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-79\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-80\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-81\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-83\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">O(n^2 \\cdot d)</script>, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-84\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-85\"><span class=\"mi\" id=\"MathJax-Span-86\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">n</script> is the sequence length and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-87\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-88\"><span class=\"mi\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">d</script> is the hidden dimension.</p>\n<p>FFN consists of two matrix multiplications:</p>\n<ul>\n      <li>\n        <p>Typically, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>1</mn></msub><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mn>4</mn><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-117\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.85em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-118\"><span class=\"msubsup\" id=\"MathJax-Span-119\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-121\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-122\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-123\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-124\"><span class=\"mrow\" id=\"MathJax-Span-125\"><span class=\"mi\" id=\"MathJax-Span-126\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-127\"><span class=\"mrow\" id=\"MathJax-Span-128\"><span class=\"mi\" id=\"MathJax-Span-129\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-130\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mn\" id=\"MathJax-Span-131\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4</span><span class=\"mi\" id=\"MathJax-Span-132\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>1</mn></msub><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mn>4</mn><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">W_1 \\in \\mathbb{R}^{d \\times 4d}</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>2</mn></msub><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>4</mn><mi>d</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-133\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.85em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-134\"><span class=\"msubsup\" id=\"MathJax-Span-135\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-136\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-137\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-139\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-140\"><span class=\"mrow\" id=\"MathJax-Span-141\"><span class=\"mi\" id=\"MathJax-Span-142\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-143\"><span class=\"mrow\" id=\"MathJax-Span-144\"><span class=\"mn\" id=\"MathJax-Span-145\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4</span><span class=\"mi\" id=\"MathJax-Span-146\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-147\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-148\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>2</mn></msub><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>4</mn><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">W_2 \\in \\mathbb{R}^{4d \\times d}</script></p>\n      </li>\n      <li>\n        <p>Note that the reason there are <strong>two</strong> matrix multiplications is that the feed-forward network inside each transformer block is actually a <strong>two-layer position-wise fully connected network</strong>:</p>\n\n        <ol>\n          <li>The first matrix multiplication with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>1</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-149\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-150\"><span class=\"msubsup\" id=\"MathJax-Span-151\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-153\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>1</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">W_1</script> projects the hidden representation from dimension <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-154\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-155\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">d</script> (the model’s hidden size) <strong>up</strong> to a larger intermediate dimension, typically <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>4</mn><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-157\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.04em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-158\"><span class=\"mn\" id=\"MathJax-Span-159\" style=\"font-family: STIXGeneral-Regular;\">4</span><span class=\"mi\" id=\"MathJax-Span-160\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>4</mn><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">4d</script>, to increase model capacity and introduce a richer representation space.</li>\n          <li>The second matrix multiplication with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>2</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-161\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-162\"><span class=\"msubsup\" id=\"MathJax-Span-163\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-164\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-165\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>2</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">W_2</script> projects the intermediate vector <strong>back down</strong> to dimension <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-166\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-167\"><span class=\"mi\" id=\"MathJax-Span-168\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">d</script>, so that the output shape matches the residual connection input and can be passed to the next transformer layer.</li>\n        </ol>\n      </li>\n      <li>\n        <p>Between these two layers, a non-linearity (often ReLU or GELU) is applied, enabling the FFN to learn complex transformations beyond what a single linear layer could achieve.</p>\n      </li>\n    </ul>\n<p>Typically, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>1</mn></msub><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mn>4</mn><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-117\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.85em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-118\"><span class=\"msubsup\" id=\"MathJax-Span-119\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-121\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-122\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-123\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-124\"><span class=\"mrow\" id=\"MathJax-Span-125\"><span class=\"mi\" id=\"MathJax-Span-126\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-127\"><span class=\"mrow\" id=\"MathJax-Span-128\"><span class=\"mi\" id=\"MathJax-Span-129\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-130\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mn\" id=\"MathJax-Span-131\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4</span><span class=\"mi\" id=\"MathJax-Span-132\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>1</mn></msub><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mn>4</mn><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">W_1 \\in \\mathbb{R}^{d \\times 4d}</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>2</mn></msub><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>4</mn><mi>d</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-133\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1004.85em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-134\"><span class=\"msubsup\" id=\"MathJax-Span-135\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-136\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-137\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-139\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-140\"><span class=\"mrow\" id=\"MathJax-Span-141\"><span class=\"mi\" id=\"MathJax-Span-142\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-143\"><span class=\"mrow\" id=\"MathJax-Span-144\"><span class=\"mn\" id=\"MathJax-Span-145\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">4</span><span class=\"mi\" id=\"MathJax-Span-146\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-147\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-148\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>2</mn></msub><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>4</mn><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">W_2 \\in \\mathbb{R}^{4d \\times d}</script></p>\n<p>Note that the reason there are <strong>two</strong> matrix multiplications is that the feed-forward network inside each transformer block is actually a <strong>two-layer position-wise fully connected network</strong>:</p>\n<ol>\n          <li>The first matrix multiplication with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>1</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-149\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-150\"><span class=\"msubsup\" id=\"MathJax-Span-151\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-153\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>1</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">W_1</script> projects the hidden representation from dimension <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-154\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-155\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">d</script> (the model’s hidden size) <strong>up</strong> to a larger intermediate dimension, typically <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>4</mn><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-157\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.04em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-158\"><span class=\"mn\" id=\"MathJax-Span-159\" style=\"font-family: STIXGeneral-Regular;\">4</span><span class=\"mi\" id=\"MathJax-Span-160\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mn>4</mn><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">4d</script>, to increase model capacity and introduce a richer representation space.</li>\n          <li>The second matrix multiplication with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>2</mn></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-161\" style=\"width: 1.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.25em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-162\"><span class=\"msubsup\" id=\"MathJax-Span-163\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-164\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mn\" id=\"MathJax-Span-165\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mn>2</mn></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">W_2</script> projects the intermediate vector <strong>back down</strong> to dimension <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-166\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-167\"><span class=\"mi\" id=\"MathJax-Span-168\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">d</script>, so that the output shape matches the residual connection input and can be passed to the next transformer layer.</li>\n        </ol>\n<p>Between these two layers, a non-linearity (often ReLU or GELU) is applied, enabling the FFN to learn complex transformations beyond what a single linear layer could achieve.</p>\n<p>These operations are <strong>highly parallelizable</strong> and benefit from <strong>vectorized matrix operations</strong>, making them well-suited for GPUs and TPUs. Even CPUs can achieve good performance <a href=\"https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html\">Math Kernel Library (oneMKL)</a>/<a href=\"https://github.com/Jakob6174/Intel-DNNL\">Deep Neural Network Library (DNNL)</a> or <a href=\"https://github.com/uxlfoundation/oneDNN\">oneAPI Deep Neural Network Library (oneDNN)</a> optimized kernels — but they are generally slower due to fewer cores and lower memory bandwidth.</p>\n<p>In short:</p>\n<ul>\n      <li><strong>Memory usage:</strong> Moderate</li>\n      <li><strong>Compute intensity:</strong> High</li>\n      <li><strong>Best suited hardware:</strong> GPU, NPU (CPU feasible for small models or batch sizes)</li>\n    </ul>",
      "contentMarkdown": "*   The encoder performs operations on entire sequences in parallel. The main computational components are:\n    \n    1.  **Multi-Head Self-Attention (MHSA)**\n    2.  **Feedforward Networks (FFN)**\n    3.  **LayerNorm / Dropout (lightweight)**\n*   The attention operation in the encoder is full-sequence:\n    \n    *   Attention Score Calculation:\n        \n        Attention(Q,K,V)\\=softmax(QKTdk‾‾√)VAttention(Q,K,V)\\=softmax(QKTdk)V\n        \n        \\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left( \\\\frac{QK^T}{\\\\sqrt{d\\_k}} \\\\right)V\n        *   where:\n            \n            *   Q\\=XWQQ\\=XWQQ = XW\\_Q, K\\=XWKK\\=XWKK = XW\\_K, V\\=XWVV\\=XWVV = XW\\_V\n            *   XXX: Input sequence\n            *   WQ,WK,WVWQ,WK,WVW\\_Q, W\\_K, W\\_V: Projection matrices\n*   This means the compute scales as O(n2⋅d)O(n2⋅d)O(n^2 \\\\cdot d), where nnn is the sequence length and ddd is the hidden dimension.\n    \n*   FFN consists of two matrix multiplications:\n    \n    FFN(x)\\=max(0,xW1+b1)W2+b2FFN(x)\\=max(0,xW1+b1)W2+b2\n    \n    \\\\text{FFN}(x) = \\\\max(0, xW\\_1 + b\\_1)W\\_2 + b\\_2\n    *   Typically, W1∈ℝd×4dW1∈Rd×4dW\\_1 \\\\in \\\\mathbb{R}^{d \\\\times 4d}, W2∈ℝ4d×dW2∈R4d×dW\\_2 \\\\in \\\\mathbb{R}^{4d \\\\times d}\n        \n    *   Note that the reason there are **two** matrix multiplications is that the feed-forward network inside each transformer block is actually a **two-layer position-wise fully connected network**:\n        \n        1.  The first matrix multiplication with W1W1W\\_1 projects the hidden representation from dimension ddd (the model’s hidden size) **up** to a larger intermediate dimension, typically 4d4d4d, to increase model capacity and introduce a richer representation space.\n        2.  The second matrix multiplication with W2W2W\\_2 projects the intermediate vector **back down** to dimension ddd, so that the output shape matches the residual connection input and can be passed to the next transformer layer.\n    *   Between these two layers, a non-linearity (often ReLU or GELU) is applied, enabling the FFN to learn complex transformations beyond what a single linear layer could achieve.\n        \n*   These operations are **highly parallelizable** and benefit from **vectorized matrix operations**, making them well-suited for GPUs and TPUs. Even CPUs can achieve good performance [Math Kernel Library (oneMKL)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html)/[Deep Neural Network Library (DNNL)](https://github.com/Jakob6174/Intel-DNNL) or [oneAPI Deep Neural Network Library (oneDNN)](https://github.com/uxlfoundation/oneDNN) optimized kernels — but they are generally slower due to fewer cores and lower memory bandwidth.\n    \n*   In short:\n    \n    *   **Memory usage:** Moderate\n    *   **Compute intensity:** High\n    *   **Best suited hardware:** GPU, NPU (CPU feasible for small models or batch sizes)\n\nThe encoder performs operations on entire sequences in parallel. The main computational components are:\n\n1.  **Multi-Head Self-Attention (MHSA)**\n2.  **Feedforward Networks (FFN)**\n3.  **LayerNorm / Dropout (lightweight)**\n\nThe attention operation in the encoder is full-sequence:\n\n*   Attention Score Calculation:\n    \n    Attention(Q,K,V)\\=softmax(QKTdk‾‾√)VAttention(Q,K,V)\\=softmax(QKTdk)V\n    \n    \\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left( \\\\frac{QK^T}{\\\\sqrt{d\\_k}} \\\\right)V\n    *   where:\n        \n        *   Q\\=XWQQ\\=XWQQ = XW\\_Q, K\\=XWKK\\=XWKK = XW\\_K, V\\=XWVV\\=XWVV = XW\\_V\n        *   XXX: Input sequence\n        *   WQ,WK,WVWQ,WK,WVW\\_Q, W\\_K, W\\_V: Projection matrices\n\nAttention Score Calculation:\n\n*   where:\n    \n    *   Q\\=XWQQ\\=XWQQ = XW\\_Q, K\\=XWKK\\=XWKK = XW\\_K, V\\=XWVV\\=XWVV = XW\\_V\n    *   XXX: Input sequence\n    *   WQ,WK,WVWQ,WK,WVW\\_Q, W\\_K, W\\_V: Projection matrices\n\nwhere:\n\n*   Q\\=XWQQ\\=XWQQ = XW\\_Q, K\\=XWKK\\=XWKK = XW\\_K, V\\=XWVV\\=XWVV = XW\\_V\n*   XXX: Input sequence\n*   WQ,WK,WVWQ,WK,WVW\\_Q, W\\_K, W\\_V: Projection matrices\n\nThis means the compute scales as O(n2⋅d)O(n2⋅d)O(n^2 \\\\cdot d), where nnn is the sequence length and ddd is the hidden dimension.\n\nFFN consists of two matrix multiplications:\n\n*   Typically, W1∈ℝd×4dW1∈Rd×4dW\\_1 \\\\in \\\\mathbb{R}^{d \\\\times 4d}, W2∈ℝ4d×dW2∈R4d×dW\\_2 \\\\in \\\\mathbb{R}^{4d \\\\times d}\n    \n*   Note that the reason there are **two** matrix multiplications is that the feed-forward network inside each transformer block is actually a **two-layer position-wise fully connected network**:\n    \n    1.  The first matrix multiplication with W1W1W\\_1 projects the hidden representation from dimension ddd (the model’s hidden size) **up** to a larger intermediate dimension, typically 4d4d4d, to increase model capacity and introduce a richer representation space.\n    2.  The second matrix multiplication with W2W2W\\_2 projects the intermediate vector **back down** to dimension ddd, so that the output shape matches the residual connection input and can be passed to the next transformer layer.\n*   Between these two layers, a non-linearity (often ReLU or GELU) is applied, enabling the FFN to learn complex transformations beyond what a single linear layer could achieve.\n    \n\nTypically, W1∈ℝd×4dW1∈Rd×4dW\\_1 \\\\in \\\\mathbb{R}^{d \\\\times 4d}, W2∈ℝ4d×dW2∈R4d×dW\\_2 \\\\in \\\\mathbb{R}^{4d \\\\times d}\n\nNote that the reason there are **two** matrix multiplications is that the feed-forward network inside each transformer block is actually a **two-layer position-wise fully connected network**:\n\n1.  The first matrix multiplication with W1W1W\\_1 projects the hidden representation from dimension ddd (the model’s hidden size) **up** to a larger intermediate dimension, typically 4d4d4d, to increase model capacity and introduce a richer representation space.\n2.  The second matrix multiplication with W2W2W\\_2 projects the intermediate vector **back down** to dimension ddd, so that the output shape matches the residual connection input and can be passed to the next transformer layer.\n\nBetween these two layers, a non-linearity (often ReLU or GELU) is applied, enabling the FFN to learn complex transformations beyond what a single linear layer could achieve.\n\nThese operations are **highly parallelizable** and benefit from **vectorized matrix operations**, making them well-suited for GPUs and TPUs. Even CPUs can achieve good performance [Math Kernel Library (oneMKL)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html)/[Deep Neural Network Library (DNNL)](https://github.com/Jakob6174/Intel-DNNL) or [oneAPI Deep Neural Network Library (oneDNN)](https://github.com/uxlfoundation/oneDNN) optimized kernels — but they are generally slower due to fewer cores and lower memory bandwidth.\n\nIn short:\n\n*   **Memory usage:** Moderate\n*   **Compute intensity:** High\n*   **Best suited hardware:** GPU, NPU (CPU feasible for small models or batch sizes)",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 5,
      "tags": [
        "ondevice ai",
        "neural network",
        "transformer",
        "attention",
        "dropout"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 822,
        "contentLength": 136439
      },
      "nextCards": [
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3"
      ],
      "relatedCards": [
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-weights-vs-activation-quantization-9",
        "ai-model-compression-rationale-and-theoretical-motivation-33",
        "ai-federated-learning-communication-optimizations-12",
        "ai-federated-learning-personalization-22"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#encoder:-compute-bound-nature",
      "scrapedAt": "2025-12-28T11:56:11.407Z",
      "siblings": [
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5",
        "ai-on-device-transformers-neural-processing-unit-npu-6"
      ]
    },
    {
      "id": "ai-on-device-transformers-decoder-memory-bound-nature-2",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Architectural Overview and Compute Characteristics of Transformers",
      "title": "Decoder: Memory-bound Nature",
      "subtitle": "Architectural Overview and Compute Characteristics of Transformers",
      "contentHtml": "<ul>\n  <li>\n    <p>The decoder is autoregressive — it generates one token at a time and feeds it back in:</p>\n\n    <ul>\n      <li>Attention involves both <strong>self-attention</strong> (causal) and <strong>encoder-decoder attention</strong></li>\n      <li>Cannot parallelize the same way as the encoder during generation</li>\n      <li>During generation, keys and values grow with each step, leading to large <strong>KV caches</strong></li>\n    </ul>\n  </li>\n  <li>\n    <p>Hence, the decoder is more <strong>memory-bound</strong> than compute-bound during generation:</p>\n\n    <ul>\n      <li>Storing and reading from the <strong>KV cache</strong> dominates the time per token</li>\n      <li>The cost grows linearly with sequence length</li>\n    </ul>\n  </li>\n  <li>\n    <p>At each step <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-169\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-170\"><span class=\"mi\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">t</script>, the model computes:</p>\n  </li>\n</ul>\n<p>The decoder is autoregressive — it generates one token at a time and feeds it back in:</p>\n<ul>\n      <li>Attention involves both <strong>self-attention</strong> (causal) and <strong>encoder-decoder attention</strong></li>\n      <li>Cannot parallelize the same way as the encoder during generation</li>\n      <li>During generation, keys and values grow with each step, leading to large <strong>KV caches</strong></li>\n    </ul>\n<p>Hence, the decoder is more <strong>memory-bound</strong> than compute-bound during generation:</p>\n<ul>\n      <li>Storing and reading from the <strong>KV cache</strong> dominates the time per token</li>\n      <li>The cost grows linearly with sequence length</li>\n    </ul>\n<p>At each step <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>t</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-169\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-170\"><span class=\"mi\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>t</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">t</script>, the model computes:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>Attention</mtext><mo stretchy=&quot;false&quot;>(</mo><msub><mi>Q</mi><mi>t</mi></msub><mo>,</mo><msub><mi>K</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>V</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo>(</mo><mfrac><mrow><msub><mi>Q</mi><mi>t</mi></msub><msubsup><mi>K</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>:</mo><mi>t</mi></mrow><mi>T</mi></msubsup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo>)</mo></mrow><msub><mi>V</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-172\" style=\"width: 24.065em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 20.055em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.19em, 1020.05em, 5.576em, -999.997em); top: -4.112em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-173\"><span class=\"mtext\" id=\"MathJax-Span-174\" style=\"font-family: STIXGeneral-Regular;\">Attention</span><span class=\"mo\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-176\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-177\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-178\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-179\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-180\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-181\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-182\"><span class=\"mrow\" id=\"MathJax-Span-183\"><span class=\"mn\" id=\"MathJax-Span-184\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-185\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mi\" id=\"MathJax-Span-186\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-187\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-188\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-189\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-190\"><span class=\"mrow\" id=\"MathJax-Span-191\"><span class=\"mn\" id=\"MathJax-Span-192\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-193\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mi\" id=\"MathJax-Span-194\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-195\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-196\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mtext\" id=\"MathJax-Span-197\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">softmax</span><span class=\"mrow\" id=\"MathJax-Span-198\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-199\" style=\"vertical-align: -0.779em;\"><span style=\"font-family: STIXSizeFourSym;\">(</span></span><span class=\"mfrac\" id=\"MathJax-Span-200\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.076em, 1002.5em, 4.482em, -999.997em); top: -4.789em; left: 50%; margin-left: -1.247em;\"><span class=\"mrow\" id=\"MathJax-Span-201\"><span class=\"msubsup\" id=\"MathJax-Span-202\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-203\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-204\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-205\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-206\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.372em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-207\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.84em, 4.169em, -999.997em); top: -3.695em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-208\"><span class=\"mrow\" id=\"MathJax-Span-209\"><span class=\"mn\" id=\"MathJax-Span-210\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-211\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mi\" id=\"MathJax-Span-212\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.971em, 1001.88em, 4.534em, -999.997em); top: -3.122em; left: 50%; margin-left: -0.935em;\"><span class=\"msqrt\" id=\"MathJax-Span-213\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.326em, -999.997em); top: -4.008em; left: 0.94em;\"><span class=\"mrow\" id=\"MathJax-Span-214\"><span class=\"msubsup\" id=\"MathJax-Span-215\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-216\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-217\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.023em, 1000.94em, 3.388em, -999.997em); top: -4.06em; left: 0.94em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; font-family: STIXGeneral-Regular; top: -4.008em; left: 0.419em;\">‾<span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.867em, 1000.94em, 4.43em, -999.997em); top: -3.904em; left: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">√</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1002.61em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 2.607em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-218\" style=\"vertical-align: -0.779em;\"><span style=\"font-family: STIXSizeFourSym;\">)</span></span></span><span class=\"msubsup\" id=\"MathJax-Span-219\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-220\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-221\"><span class=\"mrow\" id=\"MathJax-Span-222\"><span class=\"mn\" id=\"MathJax-Span-223\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-224\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mi\" id=\"MathJax-Span-225\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.622em; border-left: 0px solid; width: 0px; height: 3.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>Attention</mtext><mo stretchy=\"false\">(</mo><msub><mi>Q</mi><mi>t</mi></msub><mo>,</mo><msub><mi>K</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>V</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo>(</mo><mfrac><mrow><msub><mi>Q</mi><mi>t</mi></msub><msubsup><mi>K</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>:</mo><mi>t</mi></mrow><mi>T</mi></msubsup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo>)</mo></mrow><msub><mi>V</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub></math></span></span></div>\n<ul>\n  <li>\n    <p><strong>Consequences</strong>:</p>\n\n    <ul>\n      <li>Prefill (initial sequence context embedding) is <strong>parallelizable</strong> and compute-intensive</li>\n      <li><strong>Decode (autoregressive step-by-step)</strong> is <strong>sequential</strong>, leading to limited parallelism and more cache reads</li>\n    </ul>\n  </li>\n  <li>\n    <p>So:</p>\n    <ul>\n      <li><strong>Memory usage:</strong> High (growing with generated sequence)</li>\n      <li><strong>Compute intensity:</strong> Low per step</li>\n      <li><strong>Best suited hardware:</strong> Depends on phase\n        <ul>\n          <li>Prefill: GPU/NPU</li>\n          <li>Decode: CPU often sufficient unless batch generation is needed</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Consequences</strong>:</p>\n<ul>\n      <li>Prefill (initial sequence context embedding) is <strong>parallelizable</strong> and compute-intensive</li>\n      <li><strong>Decode (autoregressive step-by-step)</strong> is <strong>sequential</strong>, leading to limited parallelism and more cache reads</li>\n    </ul>\n<p>So:</p>\n<ul>\n      <li><strong>Memory usage:</strong> High (growing with generated sequence)</li>\n      <li><strong>Compute intensity:</strong> Low per step</li>\n      <li><strong>Best suited hardware:</strong> Depends on phase\n        <ul>\n          <li>Prefill: GPU/NPU</li>\n          <li>Decode: CPU often sufficient unless batch generation is needed</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Prefill: GPU/NPU</li>\n          <li>Decode: CPU often sufficient unless batch generation is needed</li>\n        </ul>",
      "contentMarkdown": "*   The decoder is autoregressive — it generates one token at a time and feeds it back in:\n    \n    *   Attention involves both **self-attention** (causal) and **encoder-decoder attention**\n    *   Cannot parallelize the same way as the encoder during generation\n    *   During generation, keys and values grow with each step, leading to large **KV caches**\n*   Hence, the decoder is more **memory-bound** than compute-bound during generation:\n    \n    *   Storing and reading from the **KV cache** dominates the time per token\n    *   The cost grows linearly with sequence length\n*   At each step ttt, the model computes:\n    \n\nThe decoder is autoregressive — it generates one token at a time and feeds it back in:\n\n*   Attention involves both **self-attention** (causal) and **encoder-decoder attention**\n*   Cannot parallelize the same way as the encoder during generation\n*   During generation, keys and values grow with each step, leading to large **KV caches**\n\nHence, the decoder is more **memory-bound** than compute-bound during generation:\n\n*   Storing and reading from the **KV cache** dominates the time per token\n*   The cost grows linearly with sequence length\n\nAt each step ttt, the model computes:\n\nAttention(Qt,K1:t,V1:t)\\=softmax(QtKT1:tdk‾‾√)V1:tAttention(Qt,K1:t,V1:t)\\=softmax(QtK1:tTdk)V1:t\n\n*   **Consequences**:\n    \n    *   Prefill (initial sequence context embedding) is **parallelizable** and compute-intensive\n    *   **Decode (autoregressive step-by-step)** is **sequential**, leading to limited parallelism and more cache reads\n*   So:\n    \n    *   **Memory usage:** High (growing with generated sequence)\n    *   **Compute intensity:** Low per step\n    *   **Best suited hardware:** Depends on phase\n        *   Prefill: GPU/NPU\n        *   Decode: CPU often sufficient unless batch generation is needed\n\n**Consequences**:\n\n*   Prefill (initial sequence context embedding) is **parallelizable** and compute-intensive\n*   **Decode (autoregressive step-by-step)** is **sequential**, leading to limited parallelism and more cache reads\n\nSo:\n\n*   **Memory usage:** High (growing with generated sequence)\n*   **Compute intensity:** Low per step\n*   **Best suited hardware:** Depends on phase\n    *   Prefill: GPU/NPU\n    *   Decode: CPU often sufficient unless batch generation is needed\n\n*   Prefill: GPU/NPU\n*   Decode: CPU often sufficient unless batch generation is needed",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "attention",
        "embedding"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 319,
        "contentLength": 19168
      },
      "nextCards": [
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4"
      ],
      "relatedCards": [
        "ai-model-compression-compute-vs-memory-bottlenecks-12",
        "ai-model-compression-multimodal-quantization-14",
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#decoder:-memory-bound-nature",
      "scrapedAt": "2025-12-28T11:56:11.407Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5",
        "ai-on-device-transformers-neural-processing-unit-npu-6"
      ]
    },
    {
      "id": "ai-on-device-transformers-central-processing-unit-cpu-3",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Hardware-Specific Characteristics (CPU, GPU, TPU, NPU)",
      "title": "Central Processing Unit (CPU)",
      "subtitle": "Hardware-Specific Characteristics (CPU, GPU, TPU, NPU)",
      "contentHtml": "<h4 id=\"strengths\">Strengths</h4>\n<ul>\n  <li>Excellent single-thread performance</li>\n  <li>Low latency; ideal for on-demand and low-batch inference</li>\n  <li>Flexible memory access and branching logic</li>\n</ul>\n<h4 id=\"limitations\">Limitations</h4>\n<ul>\n  <li>Limited parallelism (typically 4–64 cores vs 1000s of GPU cores)</li>\n  <li>Lower throughput for matrix-heavy operations</li>\n  <li>Cache-friendly but bandwidth-limited compared to accelerators</li>\n</ul>\n<h4 id=\"when-to-use\">When to Use</h4>\n<ul>\n  <li>Autoregressive decoding for small models (e.g., 1–2B parameters)</li>\n  <li>Edge or offline devices with no GPU/NPU</li>\n  <li>Lightweight server-side deployments for on-demand inference</li>\n</ul>\n<h4 id=\"watchouts\">Watchouts</h4>\n<ul>\n  <li>Attention mechanisms (especially multi-head) can become bottlenecks due to cache locality issues</li>\n  <li>Performance highly dependent on software stack: use optimized libraries (e.g., oneDNN, ONNX Runtime, Intel Extension for PyTorch)</li>\n</ul>\n<h4 id=\"best-practices\">Best Practices</h4>\n<ul>\n  <li>Use quantization (<code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">float16</code> if supported)</li>\n  <li>Fuse layers to reduce memory bandwidth needs</li>\n  <li>Apply KV cache optimization aggressively for decoder workloads</li>\n</ul>",
      "contentMarkdown": "#### Strengths\n\n*   Excellent single-thread performance\n*   Low latency; ideal for on-demand and low-batch inference\n*   Flexible memory access and branching logic\n\n#### Limitations\n\n*   Limited parallelism (typically 4–64 cores vs 1000s of GPU cores)\n*   Lower throughput for matrix-heavy operations\n*   Cache-friendly but bandwidth-limited compared to accelerators\n\n#### When to Use\n\n*   Autoregressive decoding for small models (e.g., 1–2B parameters)\n*   Edge or offline devices with no GPU/NPU\n*   Lightweight server-side deployments for on-demand inference\n\n#### Watchouts\n\n*   Attention mechanisms (especially multi-head) can become bottlenecks due to cache locality issues\n*   Performance highly dependent on software stack: use optimized libraries (e.g., oneDNN, ONNX Runtime, Intel Extension for PyTorch)\n\n#### Best Practices\n\n*   Use quantization (`int8` or `float16` if supported)\n*   Fuse layers to reduce memory bandwidth needs\n*   Apply KV cache optimization aggressively for decoder workloads",
      "order": 3,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "attention",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 137,
        "contentLength": 1368
      },
      "nextCards": [
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ],
      "relatedCards": [
        "ai-federated-learning-fedprox-7",
        "ai-federated-learning-scaffold-8",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5",
        "ai-federated-learning-feddyn-10",
        "ai-federated-learning-privacy-21"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#central-processing-unit-(cpu)",
      "scrapedAt": "2025-12-28T11:56:11.407Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5",
        "ai-on-device-transformers-neural-processing-unit-npu-6"
      ]
    },
    {
      "id": "ai-on-device-transformers-graphics-processing-unit-gpu-4",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Hardware-Specific Characteristics (CPU, GPU, TPU, NPU)",
      "title": "Graphics Processing Unit (GPU)",
      "subtitle": "Hardware-Specific Characteristics (CPU, GPU, TPU, NPU)",
      "contentHtml": "<h4 id=\"strengths-1\">Strengths</h4>\n<ul>\n  <li>Massive parallelism via thousands of CUDA cores</li>\n  <li>High-bandwidth memory (HBM), e.g., 600+ GB/s</li>\n  <li>Excellent for batched inference and training</li>\n</ul>\n<h4 id=\"limitations-1\">Limitations</h4>\n<ul>\n  <li>Higher latency per operation than CPUs</li>\n  <li>Can be underutilized for small batch sizes or single-token decoding</li>\n  <li>More complex memory hierarchy</li>\n</ul>\n<h4 id=\"when-to-use-1\">When to Use</h4>\n<ul>\n  <li>Prefill operations where entire input sequence can be parallelized</li>\n  <li>Training and fine-tuning large models</li>\n  <li>Batch decoding (e.g., chat applications with multiple users)</li>\n</ul>\n<h4 id=\"watchouts-1\">Watchouts</h4>\n<ul>\n  <li>Decode stage underutilizes GPU unless using batching or advanced techniques like speculative decoding</li>\n  <li>Need to manage memory carefully — attention cache can become large</li>\n</ul>\n<h4 id=\"best-practices-1\">Best Practices</h4>\n<ul>\n  <li>Use Tensor Cores (e.g., with <code class=\"language-plaintext highlighter-rouge\">float16</code>/<code class=\"language-plaintext highlighter-rouge\">bfloat16</code>) for matrix operations</li>\n  <li>Leverage libraries like NVIDIA’s FasterTransformer or TensorRT</li>\n  <li>Minimize memory copies between host (CPU) and device (GPU)</li>\n</ul>",
      "contentMarkdown": "#### Strengths\n\n*   Massive parallelism via thousands of CUDA cores\n*   High-bandwidth memory (HBM), e.g., 600+ GB/s\n*   Excellent for batched inference and training\n\n#### Limitations\n\n*   Higher latency per operation than CPUs\n*   Can be underutilized for small batch sizes or single-token decoding\n*   More complex memory hierarchy\n\n#### When to Use\n\n*   Prefill operations where entire input sequence can be parallelized\n*   Training and fine-tuning large models\n*   Batch decoding (e.g., chat applications with multiple users)\n\n#### Watchouts\n\n*   Decode stage underutilizes GPU unless using batching or advanced techniques like speculative decoding\n*   Need to manage memory carefully — attention cache can become large\n\n#### Best Practices\n\n*   Use Tensor Cores (e.g., with `float16`/`bfloat16`) for matrix operations\n*   Leverage libraries like NVIDIA’s FasterTransformer or TensorRT\n*   Minimize memory copies between host (CPU) and device (GPU)",
      "order": 4,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "transformer",
        "attention",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 137,
        "contentLength": 1323
      },
      "nextCards": [
        "ai-on-device-transformers-tensor-processing-unit-tpu-5",
        "ai-on-device-transformers-neural-processing-unit-npu-6"
      ],
      "relatedCards": [
        "ai-model-compression-pros-cons-47",
        "ai-federated-learning-llm-specific-enhancements-15",
        "ai-model-compression-practical-considerations-37",
        "ai-model-compression-pruning-workflow-35",
        "ai-model-compression-design-methodologies-51"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#graphics-processing-unit-(gpu)",
      "scrapedAt": "2025-12-28T11:56:11.407Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5",
        "ai-on-device-transformers-neural-processing-unit-npu-6"
      ]
    },
    {
      "id": "ai-on-device-transformers-tensor-processing-unit-tpu-5",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Hardware-Specific Characteristics (CPU, GPU, TPU, NPU)",
      "title": "Tensor Processing Unit (TPU)",
      "subtitle": "Hardware-Specific Characteristics (CPU, GPU, TPU, NPU)",
      "contentHtml": "<h4 id=\"strengths-2\">Strengths</h4>\n<ul>\n  <li>Designed specifically for tensor operations (dense matmuls, convolutions)</li>\n  <li>Excellent for training and large-scale inference</li>\n  <li>Systolic array architecture delivers massive throughput</li>\n</ul>\n<h4 id=\"limitations-2\">Limitations</h4>\n<ul>\n  <li>Limited flexibility (less suited for irregular control flow)</li>\n  <li>Software stack tied to Google ecosystem (JAX, TensorFlow)</li>\n</ul>\n<h4 id=\"when-to-use-2\">When to Use</h4>\n<ul>\n  <li>Training large, deep models with billions to trillions of parameters (e.g., Llama, T5, BERT, etc.)</li>\n  <li>Batched prefill in inference</li>\n</ul>\n<h4 id=\"watchouts-2\">Watchouts</h4>\n<ul>\n  <li>Autoregressive decoding underperforms due to lack of dynamic control flow</li>\n  <li>Fixed size memory buffers limit dynamic sequence handling</li>\n</ul>\n<h4 id=\"best-practices-2\">Best Practices</h4>\n<ul>\n  <li>Keep compute on TPU, avoid offloading to CPU</li>\n  <li>Use <code class=\"language-plaintext highlighter-rouge\">xla</code> compilation to fuse ops and reduce memory overhead</li>\n  <li>Align tensor shapes to match hardware block sizes</li>\n</ul>",
      "contentMarkdown": "#### Strengths\n\n*   Designed specifically for tensor operations (dense matmuls, convolutions)\n*   Excellent for training and large-scale inference\n*   Systolic array architecture delivers massive throughput\n\n#### Limitations\n\n*   Limited flexibility (less suited for irregular control flow)\n*   Software stack tied to Google ecosystem (JAX, TensorFlow)\n\n#### When to Use\n\n*   Training large, deep models with billions to trillions of parameters (e.g., Llama, T5, BERT, etc.)\n*   Batched prefill in inference\n\n#### Watchouts\n\n*   Autoregressive decoding underperforms due to lack of dynamic control flow\n*   Fixed size memory buffers limit dynamic sequence handling\n\n#### Best Practices\n\n*   Keep compute on TPU, avoid offloading to CPU\n*   Use `xla` compilation to fuse ops and reduce memory overhead\n*   Align tensor shapes to match hardware block sizes",
      "order": 5,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "convolution",
        "bert"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 124,
        "contentLength": 1154
      },
      "nextCards": [
        "ai-on-device-transformers-neural-processing-unit-npu-6",
        "ai-on-device-transformers-comparative-analysis-7"
      ],
      "relatedCards": [
        "ai-model-compression-benefits-and-limitations-7",
        "ai-model-compression-types-of-knowledge-distillation-23",
        "ai-model-compression-why-knowledge-distillation-works-26",
        "ai-model-compression-compute-vs-memory-bottlenecks-30",
        "ai-model-compression-compute-vs-memory-bottlenecks-36"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#tensor-processing-unit-(tpu)",
      "scrapedAt": "2025-12-28T11:56:11.407Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-neural-processing-unit-npu-6"
      ]
    },
    {
      "id": "ai-on-device-transformers-neural-processing-unit-npu-6",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Hardware-Specific Characteristics (CPU, GPU, TPU, NPU)",
      "title": "Neural Processing Unit (NPU)",
      "subtitle": "Hardware-Specific Characteristics (CPU, GPU, TPU, NPU)",
      "contentHtml": "<h4 id=\"strengths-3\">Strengths</h4>\n<ul>\n  <li>Optimized for inference on mobile and edge devices</li>\n  <li>Energy-efficient; often integrated with smartphone SoCs</li>\n  <li>Dedicated acceleration for quantized models (e.g., <code class=\"language-plaintext highlighter-rouge\">int8</code>, <code class=\"language-plaintext highlighter-rouge\">float16</code>)</li>\n</ul>\n<h4 id=\"limitations-3\">Limitations</h4>\n<ul>\n  <li>Vary widely in capability and programming APIs (e.g., Apple’s ANE, Qualcomm’s Hexagon, Huawei’s Ascend)</li>\n  <li>Difficult to customize beyond supported ops</li>\n</ul>\n<h4 id=\"when-to-use-3\">When to Use</h4>\n<ul>\n  <li>On-device generation (e.g., real-time voice assistants)</li>\n  <li>Low-latency use-cases with small LLMs or distilled models</li>\n</ul>\n<h4 id=\"watchouts-3\">Watchouts</h4>\n<ul>\n  <li>Must quantize model to match NPU-supported formats</li>\n  <li>Performance depends heavily on vendor-specific SDKs (e.g., CoreML, NNAPI, SNPE)</li>\n</ul>\n<h4 id=\"best-practices-3\">Best Practices</h4>\n<ul>\n  <li>Use static quantization and operation fusing</li>\n  <li>Prune and distill models before deployment</li>\n  <li>Use vendor-optimized Transformer blocks when available</li>\n</ul>",
      "contentMarkdown": "#### Strengths\n\n*   Optimized for inference on mobile and edge devices\n*   Energy-efficient; often integrated with smartphone SoCs\n*   Dedicated acceleration for quantized models (e.g., `int8`, `float16`)\n\n#### Limitations\n\n*   Vary widely in capability and programming APIs (e.g., Apple’s ANE, Qualcomm’s Hexagon, Huawei’s Ascend)\n*   Difficult to customize beyond supported ops\n\n#### When to Use\n\n*   On-device generation (e.g., real-time voice assistants)\n*   Low-latency use-cases with small LLMs or distilled models\n\n#### Watchouts\n\n*   Must quantize model to match NPU-supported formats\n*   Performance depends heavily on vendor-specific SDKs (e.g., CoreML, NNAPI, SNPE)\n\n#### Best Practices\n\n*   Use static quantization and operation fusing\n*   Prune and distill models before deployment\n*   Use vendor-optimized Transformer blocks when available",
      "order": 6,
      "orderInChapter": 4,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "transformer",
        "llm"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 116,
        "contentLength": 1209
      },
      "nextCards": [
        "ai-on-device-transformers-comparative-analysis-7",
        "ai-on-device-transformers-balancing-compute-and-memory-prefill-vs-decode-opt-8"
      ],
      "relatedCards": [
        "ai-model-compression-design-methodologies-51",
        "ai-differential-privacy-membership-inference-protection-13",
        "ai-differential-privacy-cons-16",
        "ai-model-compression-further-reading-21",
        "ai-model-compression-overview-42"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#neural-processing-unit-(npu)",
      "scrapedAt": "2025-12-28T11:56:11.407Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-comparative-analysis-7",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Hardware-Specific Characteristics (CPU, GPU, TPU, NPU)",
      "title": "Comparative Analysis",
      "subtitle": "Hardware-Specific Characteristics (CPU, GPU, TPU, NPU)",
      "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Feature</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>CPU</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>GPU</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TPU</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>NPU (Edge)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Parallelism</td>\n<td class=\"tg-tleft-valign-first\">Low</td>\n<td class=\"tg-tleft-valign-first\">High</td>\n<td class=\"tg-tleft-valign-first\">Very High</td>\n<td class=\"tg-tleft-valign-second\">Moderate</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Prefill Performance</td>\n<td class=\"tg-tleft-valign-first\">Moderate</td>\n<td class=\"tg-tleft-valign-first\">Excellent</td>\n<td class=\"tg-tleft-valign-first\">Excellent</td>\n<td class=\"tg-tleft-valign-second\">Moderate</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Decode Performance</td>\n<td class=\"tg-tleft-valign-first\">Good (low-batch)</td>\n<td class=\"tg-tleft-valign-first\">Moderate</td>\n<td class=\"tg-tleft-valign-first\">Poor</td>\n<td class=\"tg-tleft-valign-second\">Good (quantized)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Memory Bandwidth</td>\n<td class=\"tg-tleft-valign-first\">Low</td>\n<td class=\"tg-tleft-valign-first\">High</td>\n<td class=\"tg-tleft-valign-first\">Very High</td>\n<td class=\"tg-tleft-valign-second\">Low–Moderate</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Flexibility</td>\n<td class=\"tg-tleft-valign-first\">Very High</td>\n<td class=\"tg-tleft-valign-first\">High</td>\n<td class=\"tg-tleft-valign-first\">Low</td>\n<td class=\"tg-tleft-valign-second\">Low</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantization Support</td>\n<td class=\"tg-tleft-valign-first\">Yes (<code>int8</code>)</td>\n<td class=\"tg-tleft-valign-first\">Yes (TensorRT)</td>\n<td class=\"tg-tleft-valign-first\">Yes (<code>bfloat16</code>)</td>\n<td class=\"tg-tleft-valign-second\">Yes (`int8`-only)</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Feature</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>CPU</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>GPU</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TPU</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>NPU (Edge)</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Parallelism</td>\n<td class=\"tg-tleft-valign-first\">Low</td>\n<td class=\"tg-tleft-valign-first\">High</td>\n<td class=\"tg-tleft-valign-first\">Very High</td>\n<td class=\"tg-tleft-valign-second\">Moderate</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Prefill Performance</td>\n<td class=\"tg-tleft-valign-first\">Moderate</td>\n<td class=\"tg-tleft-valign-first\">Excellent</td>\n<td class=\"tg-tleft-valign-first\">Excellent</td>\n<td class=\"tg-tleft-valign-second\">Moderate</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Decode Performance</td>\n<td class=\"tg-tleft-valign-first\">Good (low-batch)</td>\n<td class=\"tg-tleft-valign-first\">Moderate</td>\n<td class=\"tg-tleft-valign-first\">Poor</td>\n<td class=\"tg-tleft-valign-second\">Good (quantized)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Memory Bandwidth</td>\n<td class=\"tg-tleft-valign-first\">Low</td>\n<td class=\"tg-tleft-valign-first\">High</td>\n<td class=\"tg-tleft-valign-first\">Very High</td>\n<td class=\"tg-tleft-valign-second\">Low–Moderate</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Flexibility</td>\n<td class=\"tg-tleft-valign-first\">Very High</td>\n<td class=\"tg-tleft-valign-first\">High</td>\n<td class=\"tg-tleft-valign-first\">Low</td>\n<td class=\"tg-tleft-valign-second\">Low</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Quantization Support</td>\n<td class=\"tg-tleft-valign-first\">Yes (<code>int8</code>)</td>\n<td class=\"tg-tleft-valign-first\">Yes (TensorRT)</td>\n<td class=\"tg-tleft-valign-first\">Yes (<code>bfloat16</code>)</td>\n<td class=\"tg-tleft-valign-second\">Yes (`int8`-only)</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "**Feature**\n\n**CPU**\n\n**GPU**\n\n**TPU**\n\n**NPU (Edge)**\n\nParallelism\n\nLow\n\nHigh\n\nVery High\n\nModerate\n\nPrefill Performance\n\nModerate\n\nExcellent\n\nExcellent\n\nModerate\n\nDecode Performance\n\nGood (low-batch)\n\nModerate\n\nPoor\n\nGood (quantized)\n\nMemory Bandwidth\n\nLow\n\nHigh\n\nVery High\n\nLow–Moderate\n\nFlexibility\n\nVery High\n\nHigh\n\nLow\n\nLow\n\nQuantization Support\n\nYes (`int8`)\n\nYes (TensorRT)\n\nYes (`bfloat16`)\n\nYes (\\`int8\\`-only)\n\n**Feature**\n\n**CPU**\n\n**GPU**\n\n**TPU**\n\n**NPU (Edge)**\n\nParallelism\n\nLow\n\nHigh\n\nVery High\n\nModerate\n\nPrefill Performance\n\nModerate\n\nExcellent\n\nExcellent\n\nModerate\n\nDecode Performance\n\nGood (low-batch)\n\nModerate\n\nPoor\n\nGood (quantized)\n\nMemory Bandwidth\n\nLow\n\nHigh\n\nVery High\n\nLow–Moderate\n\nFlexibility\n\nVery High\n\nHigh\n\nLow\n\nLow\n\nQuantization Support\n\nYes (`int8`)\n\nYes (TensorRT)\n\nYes (`bfloat16`)\n\nYes (\\`int8\\`-only)",
      "order": 7,
      "orderInChapter": 5,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 98,
        "contentLength": 4011
      },
      "nextCards": [
        "ai-on-device-transformers-balancing-compute-and-memory-prefill-vs-decode-opt-8",
        "ai-on-device-transformers-key-value-kv-cache-optimization-9"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#comparative-analysis",
      "scrapedAt": "2025-12-28T11:56:11.407Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-balancing-compute-and-memory-prefill-vs-decode-opt-8",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Optimization Techniques for On-Device Transformers",
      "title": "Balancing Compute and Memory: Prefill vs. Decode Optimization",
      "subtitle": "Optimization Techniques for On-Device Transformers",
      "contentHtml": "<ul>\n  <li>\n    <p>Transformer inference is divided into two distinct phases:</p>\n\n    <ol>\n      <li>\n        <p><strong>Prefill Phase</strong>:</p>\n\n        <ul>\n          <li>Processes the input prompt to generate the initial attention and hidden states.</li>\n          <li>This phase can be highly parallelized across sequence length and batch size.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Decode Phase</strong>:</p>\n\n        <ul>\n          <li>Generates output tokens one at a time in an autoregressive loop.</li>\n          <li>At each step, past tokens must be revisited via the KV cache, leading to repetitive memory access.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p>Because these phases differ in structure, they have different performance bottlenecks and require distinct optimization strategies:</p>\n  </li>\n</ul>\n<p>Transformer inference is divided into two distinct phases:</p>\n<ol>\n      <li>\n        <p><strong>Prefill Phase</strong>:</p>\n\n        <ul>\n          <li>Processes the input prompt to generate the initial attention and hidden states.</li>\n          <li>This phase can be highly parallelized across sequence length and batch size.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Decode Phase</strong>:</p>\n\n        <ul>\n          <li>Generates output tokens one at a time in an autoregressive loop.</li>\n          <li>At each step, past tokens must be revisited via the KV cache, leading to repetitive memory access.</li>\n        </ul>\n      </li>\n    </ol>\n<p><strong>Prefill Phase</strong>:</p>\n<ul>\n          <li>Processes the input prompt to generate the initial attention and hidden states.</li>\n          <li>This phase can be highly parallelized across sequence length and batch size.</li>\n        </ul>\n<p><strong>Decode Phase</strong>:</p>\n<ul>\n          <li>Generates output tokens one at a time in an autoregressive loop.</li>\n          <li>At each step, past tokens must be revisited via the KV cache, leading to repetitive memory access.</li>\n        </ul>\n<p>Because these phases differ in structure, they have different performance bottlenecks and require distinct optimization strategies:</p>\n<h4 id=\"why-different-performance-bottlenecks-require-different-optimization-strategies\">Why Different Performance Bottlenecks Require Different Optimization Strategies</h4>\n<ul>\n  <li>\n    <p><strong>Prefill is compute-bound</strong>:</p>\n\n    <ul>\n      <li>Dominated by large matrix multiplications and attention computations.</li>\n      <li>Performance depends on how much raw compute throughput the device has (e.g., <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> throughput on GPUs/TPUs).</li>\n      <li>\n        <p>To optimize:</p>\n\n        <ul>\n          <li><strong>Dequantize weights early</strong> (from <code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">int4</code> to <code class=\"language-plaintext highlighter-rouge\">float16</code>/<code class=\"language-plaintext highlighter-rouge\">float32</code>) before compute.</li>\n          <li>This ensures clean matrix operations using high-precision compute engines (e.g., Tensor Cores or CPU vector units).</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Decode is memory-bound</strong>:</p>\n\n    <ul>\n      <li>Dominated by repeated memory reads from the KV cache and weights.</li>\n      <li>Each token step requires pulling more memory into cache.</li>\n      <li>\n        <p>To optimize:</p>\n\n        <ul>\n          <li><strong>Delay dequantization</strong> until computation, using compact representations as long as possible.</li>\n          <li>This minimizes bandwidth usage and cache misses, especially on CPUs and NPUs.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Illustration: Optimizing LLM Inference by Adapting to Compute and Memory Bottlenecks (Different Performance Bottlenecks require Different Optimization Strategies)</strong>:</p>\n\n    <ul>\n      <li>The figure below (<a href=\"https://developers.googleblog.com/en/large-language-models-on-device-with-mediapipe-and-tensorflow-lite/\">source</a>) demonstrates how different performance bottlenecks in the LLM inference pipeline lead to distinct optimization strategies for handling dequantization of weights.</li>\n    </ul>\n\n    <p><img src=\"/primers/ai/assets/on-device-transformers/OptimizationStrategiesBottlenecks.jpg\" alt=\"Optimization strategies for prefill vs decode phases\"></p>\n\n    <ul>\n      <li>\n        <p>In the <strong>prefill phase</strong>, which is compute-intensive, the model first <strong>dequantizes</strong> low-bit weights (<code class=\"language-plaintext highlighter-rouge\">int4</code>) into floating-point (<code class=\"language-plaintext highlighter-rouge\">float16</code>) values before passing them to the convolution (<code class=\"language-plaintext highlighter-rouge\">Conv_2D</code>) operator. This ensures optimal performance for heavy computations, as the floating-point operations run more efficiently on modern processors.</p>\n      </li>\n      <li>\n        <p>In contrast, the <strong>decode phase</strong> is memory-intensive. Here, instead of dequantizing weights ahead of time, the model uses <strong>fused Conv_2D with dequantization</strong>, keeping weights in their compact form (<code class=\"language-plaintext highlighter-rouge\">int4</code>) until they are needed. This reduces memory bandwidth usage by avoiding unnecessary data expansion during transfer, which is critical when decoding tokens one at a time.</p>\n      </li>\n      <li>\n        <p>This design highlights a core principle in system optimization: different stages of computation may require tailored approaches based on their unique constraints—compute-bound stages benefit from early dequantization, while memory-bound stages gain from deferred dequantization.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Key takeaways</strong>:</p>\n\n    <ul>\n      <li>\n        <p>Performance optimization isn’t one-size-fits-all. You have to understand where the bottleneck is (compute vs memory) and tailor your strategy to the bottleneck:</p>\n\n        <ul>\n          <li><em>Dequantize early when compute-bound (prefill phase).</em></li>\n          <li><em>Dequantize late when memory-bound (decode phase).</em></li>\n        </ul>\n      </li>\n      <li>\n        <p>This phase-aware optimization is foundational for achieving efficient LLM inference on-device and under constrained compute/memory budgets.</p>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Prefill is compute-bound</strong>:</p>\n<ul>\n      <li>Dominated by large matrix multiplications and attention computations.</li>\n      <li>Performance depends on how much raw compute throughput the device has (e.g., <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> throughput on GPUs/TPUs).</li>\n      <li>\n        <p>To optimize:</p>\n\n        <ul>\n          <li><strong>Dequantize weights early</strong> (from <code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">int4</code> to <code class=\"language-plaintext highlighter-rouge\">float16</code>/<code class=\"language-plaintext highlighter-rouge\">float32</code>) before compute.</li>\n          <li>This ensures clean matrix operations using high-precision compute engines (e.g., Tensor Cores or CPU vector units).</li>\n        </ul>\n      </li>\n    </ul>\n<p>To optimize:</p>\n<ul>\n          <li><strong>Dequantize weights early</strong> (from <code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">int4</code> to <code class=\"language-plaintext highlighter-rouge\">float16</code>/<code class=\"language-plaintext highlighter-rouge\">float32</code>) before compute.</li>\n          <li>This ensures clean matrix operations using high-precision compute engines (e.g., Tensor Cores or CPU vector units).</li>\n        </ul>\n<p><strong>Decode is memory-bound</strong>:</p>\n<ul>\n      <li>Dominated by repeated memory reads from the KV cache and weights.</li>\n      <li>Each token step requires pulling more memory into cache.</li>\n      <li>\n        <p>To optimize:</p>\n\n        <ul>\n          <li><strong>Delay dequantization</strong> until computation, using compact representations as long as possible.</li>\n          <li>This minimizes bandwidth usage and cache misses, especially on CPUs and NPUs.</li>\n        </ul>\n      </li>\n    </ul>\n<p>To optimize:</p>\n<ul>\n          <li><strong>Delay dequantization</strong> until computation, using compact representations as long as possible.</li>\n          <li>This minimizes bandwidth usage and cache misses, especially on CPUs and NPUs.</li>\n        </ul>\n<p><strong>Illustration: Optimizing LLM Inference by Adapting to Compute and Memory Bottlenecks (Different Performance Bottlenecks require Different Optimization Strategies)</strong>:</p>\n<ul>\n      <li>The figure below (<a href=\"https://developers.googleblog.com/en/large-language-models-on-device-with-mediapipe-and-tensorflow-lite/\">source</a>) demonstrates how different performance bottlenecks in the LLM inference pipeline lead to distinct optimization strategies for handling dequantization of weights.</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/on-device-transformers/OptimizationStrategiesBottlenecks.jpg\" alt=\"Optimization strategies for prefill vs decode phases\"></p>\n<ul>\n      <li>\n        <p>In the <strong>prefill phase</strong>, which is compute-intensive, the model first <strong>dequantizes</strong> low-bit weights (<code class=\"language-plaintext highlighter-rouge\">int4</code>) into floating-point (<code class=\"language-plaintext highlighter-rouge\">float16</code>) values before passing them to the convolution (<code class=\"language-plaintext highlighter-rouge\">Conv_2D</code>) operator. This ensures optimal performance for heavy computations, as the floating-point operations run more efficiently on modern processors.</p>\n      </li>\n      <li>\n        <p>In contrast, the <strong>decode phase</strong> is memory-intensive. Here, instead of dequantizing weights ahead of time, the model uses <strong>fused Conv_2D with dequantization</strong>, keeping weights in their compact form (<code class=\"language-plaintext highlighter-rouge\">int4</code>) until they are needed. This reduces memory bandwidth usage by avoiding unnecessary data expansion during transfer, which is critical when decoding tokens one at a time.</p>\n      </li>\n      <li>\n        <p>This design highlights a core principle in system optimization: different stages of computation may require tailored approaches based on their unique constraints—compute-bound stages benefit from early dequantization, while memory-bound stages gain from deferred dequantization.</p>\n      </li>\n    </ul>\n<p>In the <strong>prefill phase</strong>, which is compute-intensive, the model first <strong>dequantizes</strong> low-bit weights (<code class=\"language-plaintext highlighter-rouge\">int4</code>) into floating-point (<code class=\"language-plaintext highlighter-rouge\">float16</code>) values before passing them to the convolution (<code class=\"language-plaintext highlighter-rouge\">Conv_2D</code>) operator. This ensures optimal performance for heavy computations, as the floating-point operations run more efficiently on modern processors.</p>\n<p>In contrast, the <strong>decode phase</strong> is memory-intensive. Here, instead of dequantizing weights ahead of time, the model uses <strong>fused Conv_2D with dequantization</strong>, keeping weights in their compact form (<code class=\"language-plaintext highlighter-rouge\">int4</code>) until they are needed. This reduces memory bandwidth usage by avoiding unnecessary data expansion during transfer, which is critical when decoding tokens one at a time.</p>\n<p>This design highlights a core principle in system optimization: different stages of computation may require tailored approaches based on their unique constraints—compute-bound stages benefit from early dequantization, while memory-bound stages gain from deferred dequantization.</p>\n<p><strong>Key takeaways</strong>:</p>\n<ul>\n      <li>\n        <p>Performance optimization isn’t one-size-fits-all. You have to understand where the bottleneck is (compute vs memory) and tailor your strategy to the bottleneck:</p>\n\n        <ul>\n          <li><em>Dequantize early when compute-bound (prefill phase).</em></li>\n          <li><em>Dequantize late when memory-bound (decode phase).</em></li>\n        </ul>\n      </li>\n      <li>\n        <p>This phase-aware optimization is foundational for achieving efficient LLM inference on-device and under constrained compute/memory budgets.</p>\n      </li>\n    </ul>\n<p>Performance optimization isn’t one-size-fits-all. You have to understand where the bottleneck is (compute vs memory) and tailor your strategy to the bottleneck:</p>\n<ul>\n          <li><em>Dequantize early when compute-bound (prefill phase).</em></li>\n          <li><em>Dequantize late when memory-bound (decode phase).</em></li>\n        </ul>\n<p>This phase-aware optimization is foundational for achieving efficient LLM inference on-device and under constrained compute/memory budgets.</p>",
      "contentMarkdown": "*   Transformer inference is divided into two distinct phases:\n    \n    1.  **Prefill Phase**:\n        \n        *   Processes the input prompt to generate the initial attention and hidden states.\n        *   This phase can be highly parallelized across sequence length and batch size.\n    2.  **Decode Phase**:\n        \n        *   Generates output tokens one at a time in an autoregressive loop.\n        *   At each step, past tokens must be revisited via the KV cache, leading to repetitive memory access.\n*   Because these phases differ in structure, they have different performance bottlenecks and require distinct optimization strategies:\n    \n\nTransformer inference is divided into two distinct phases:\n\n1.  **Prefill Phase**:\n    \n    *   Processes the input prompt to generate the initial attention and hidden states.\n    *   This phase can be highly parallelized across sequence length and batch size.\n2.  **Decode Phase**:\n    \n    *   Generates output tokens one at a time in an autoregressive loop.\n    *   At each step, past tokens must be revisited via the KV cache, leading to repetitive memory access.\n\n**Prefill Phase**:\n\n*   Processes the input prompt to generate the initial attention and hidden states.\n*   This phase can be highly parallelized across sequence length and batch size.\n\n**Decode Phase**:\n\n*   Generates output tokens one at a time in an autoregressive loop.\n*   At each step, past tokens must be revisited via the KV cache, leading to repetitive memory access.\n\nBecause these phases differ in structure, they have different performance bottlenecks and require distinct optimization strategies:\n\n#### Why Different Performance Bottlenecks Require Different Optimization Strategies\n\n*   **Prefill is compute-bound**:\n    \n    *   Dominated by large matrix multiplications and attention computations.\n    *   Performance depends on how much raw compute throughput the device has (e.g., `float16` or `bfloat16` throughput on GPUs/TPUs).\n    *   To optimize:\n        \n        *   **Dequantize weights early** (from `int8`/`int4` to `float16`/`float32`) before compute.\n        *   This ensures clean matrix operations using high-precision compute engines (e.g., Tensor Cores or CPU vector units).\n*   **Decode is memory-bound**:\n    \n    *   Dominated by repeated memory reads from the KV cache and weights.\n    *   Each token step requires pulling more memory into cache.\n    *   To optimize:\n        \n        *   **Delay dequantization** until computation, using compact representations as long as possible.\n        *   This minimizes bandwidth usage and cache misses, especially on CPUs and NPUs.\n*   **Illustration: Optimizing LLM Inference by Adapting to Compute and Memory Bottlenecks (Different Performance Bottlenecks require Different Optimization Strategies)**:\n    \n    *   The figure below ([source](https://developers.googleblog.com/en/large-language-models-on-device-with-mediapipe-and-tensorflow-lite/)) demonstrates how different performance bottlenecks in the LLM inference pipeline lead to distinct optimization strategies for handling dequantization of weights.\n    \n    ![Optimization strategies for prefill vs decode phases](/primers/ai/assets/on-device-transformers/OptimizationStrategiesBottlenecks.jpg)\n    \n    *   In the **prefill phase**, which is compute-intensive, the model first **dequantizes** low-bit weights (`int4`) into floating-point (`float16`) values before passing them to the convolution (`Conv_2D`) operator. This ensures optimal performance for heavy computations, as the floating-point operations run more efficiently on modern processors.\n        \n    *   In contrast, the **decode phase** is memory-intensive. Here, instead of dequantizing weights ahead of time, the model uses **fused Conv\\_2D with dequantization**, keeping weights in their compact form (`int4`) until they are needed. This reduces memory bandwidth usage by avoiding unnecessary data expansion during transfer, which is critical when decoding tokens one at a time.\n        \n    *   This design highlights a core principle in system optimization: different stages of computation may require tailored approaches based on their unique constraints—compute-bound stages benefit from early dequantization, while memory-bound stages gain from deferred dequantization.\n        \n*   **Key takeaways**:\n    \n    *   Performance optimization isn’t one-size-fits-all. You have to understand where the bottleneck is (compute vs memory) and tailor your strategy to the bottleneck:\n        \n        *   _Dequantize early when compute-bound (prefill phase)._\n        *   _Dequantize late when memory-bound (decode phase)._\n    *   This phase-aware optimization is foundational for achieving efficient LLM inference on-device and under constrained compute/memory budgets.\n        \n\n**Prefill is compute-bound**:\n\n*   Dominated by large matrix multiplications and attention computations.\n*   Performance depends on how much raw compute throughput the device has (e.g., `float16` or `bfloat16` throughput on GPUs/TPUs).\n*   To optimize:\n    \n    *   **Dequantize weights early** (from `int8`/`int4` to `float16`/`float32`) before compute.\n    *   This ensures clean matrix operations using high-precision compute engines (e.g., Tensor Cores or CPU vector units).\n\nTo optimize:\n\n*   **Dequantize weights early** (from `int8`/`int4` to `float16`/`float32`) before compute.\n*   This ensures clean matrix operations using high-precision compute engines (e.g., Tensor Cores or CPU vector units).\n\n**Decode is memory-bound**:\n\n*   Dominated by repeated memory reads from the KV cache and weights.\n*   Each token step requires pulling more memory into cache.\n*   To optimize:\n    \n    *   **Delay dequantization** until computation, using compact representations as long as possible.\n    *   This minimizes bandwidth usage and cache misses, especially on CPUs and NPUs.\n\nTo optimize:\n\n*   **Delay dequantization** until computation, using compact representations as long as possible.\n*   This minimizes bandwidth usage and cache misses, especially on CPUs and NPUs.\n\n**Illustration: Optimizing LLM Inference by Adapting to Compute and Memory Bottlenecks (Different Performance Bottlenecks require Different Optimization Strategies)**:\n\n*   The figure below ([source](https://developers.googleblog.com/en/large-language-models-on-device-with-mediapipe-and-tensorflow-lite/)) demonstrates how different performance bottlenecks in the LLM inference pipeline lead to distinct optimization strategies for handling dequantization of weights.\n\n![Optimization strategies for prefill vs decode phases](/primers/ai/assets/on-device-transformers/OptimizationStrategiesBottlenecks.jpg)\n\n*   In the **prefill phase**, which is compute-intensive, the model first **dequantizes** low-bit weights (`int4`) into floating-point (`float16`) values before passing them to the convolution (`Conv_2D`) operator. This ensures optimal performance for heavy computations, as the floating-point operations run more efficiently on modern processors.\n    \n*   In contrast, the **decode phase** is memory-intensive. Here, instead of dequantizing weights ahead of time, the model uses **fused Conv\\_2D with dequantization**, keeping weights in their compact form (`int4`) until they are needed. This reduces memory bandwidth usage by avoiding unnecessary data expansion during transfer, which is critical when decoding tokens one at a time.\n    \n*   This design highlights a core principle in system optimization: different stages of computation may require tailored approaches based on their unique constraints—compute-bound stages benefit from early dequantization, while memory-bound stages gain from deferred dequantization.\n    \n\nIn the **prefill phase**, which is compute-intensive, the model first **dequantizes** low-bit weights (`int4`) into floating-point (`float16`) values before passing them to the convolution (`Conv_2D`) operator. This ensures optimal performance for heavy computations, as the floating-point operations run more efficiently on modern processors.\n\nIn contrast, the **decode phase** is memory-intensive. Here, instead of dequantizing weights ahead of time, the model uses **fused Conv\\_2D with dequantization**, keeping weights in their compact form (`int4`) until they are needed. This reduces memory bandwidth usage by avoiding unnecessary data expansion during transfer, which is critical when decoding tokens one at a time.\n\nThis design highlights a core principle in system optimization: different stages of computation may require tailored approaches based on their unique constraints—compute-bound stages benefit from early dequantization, while memory-bound stages gain from deferred dequantization.\n\n**Key takeaways**:\n\n*   Performance optimization isn’t one-size-fits-all. You have to understand where the bottleneck is (compute vs memory) and tailor your strategy to the bottleneck:\n    \n    *   _Dequantize early when compute-bound (prefill phase)._\n    *   _Dequantize late when memory-bound (decode phase)._\n*   This phase-aware optimization is foundational for achieving efficient LLM inference on-device and under constrained compute/memory budgets.\n    \n\nPerformance optimization isn’t one-size-fits-all. You have to understand where the bottleneck is (compute vs memory) and tailor your strategy to the bottleneck:\n\n*   _Dequantize early when compute-bound (prefill phase)._\n*   _Dequantize late when memory-bound (decode phase)._\n\nThis phase-aware optimization is foundational for achieving efficient LLM inference on-device and under constrained compute/memory budgets.",
      "order": 8,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 6,
      "tags": [
        "ondevice ai",
        "transformer",
        "attention",
        "convolution",
        "llm",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 1197,
        "contentLength": 13431
      },
      "nextCards": [
        "ai-on-device-transformers-key-value-kv-cache-optimization-9",
        "ai-on-device-transformers-speculative-decoding-10"
      ],
      "relatedCards": [
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-quantization-workflows-6",
        "ai-model-compression-weights-vs-activation-quantization-9",
        "ai-model-compression-multimodal-quantization-14",
        "ai-model-compression-mitigation-strategies-8"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#balancing-compute-and-memory:-prefill-vs.-decode-optimization",
      "scrapedAt": "2025-12-28T11:56:11.407Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-key-value-kv-cache-optimization-9",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Optimization Techniques for On-Device Transformers",
      "title": "Key-Value (KV) Cache Optimization",
      "subtitle": "Optimization Techniques for On-Device Transformers",
      "contentHtml": "<ul>\n  <li>\n    <p>In decoder-based models (e.g., GPT), each new token requires computing self-attention with all past tokens. Without optimization, this scales as:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-226\" style=\"width: 4.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1003.8em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-227\"><span class=\"mi\" id=\"MathJax-Span-228\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-229\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-230\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-231\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-232\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-233\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-234\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-235\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-22\">O(n^2 \\cdot d)</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-236\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-237\"><span class=\"mi\" id=\"MathJax-Span-238\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">n</script> is the number of generated tokens and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-239\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-240\"><span class=\"mi\" id=\"MathJax-Span-241\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">d</script> is the hidden dimension.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>KV cache principle:</strong></p>\n\n    <ul>\n      <li>Store the key (K) and value (V) projections for all past tokens during generation.</li>\n      <li>\n        <p>For a new token, compute only its query (Q) and perform:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>Attention</mtext><mo stretchy=&quot;false&quot;>(</mo><msub><mi>Q</mi><mi>t</mi></msub><mo>,</mo><msub><mi>K</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>V</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-242\" style=\"width: 11.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.326em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1009.27em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-243\"><span class=\"mtext\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Regular;\">Attention</span><span class=\"mo\" id=\"MathJax-Span-245\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-246\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-247\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-248\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-249\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-250\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-251\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-252\"><span class=\"mrow\" id=\"MathJax-Span-253\"><span class=\"mn\" id=\"MathJax-Span-254\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-255\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mi\" id=\"MathJax-Span-256\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-257\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-258\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-259\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-260\"><span class=\"mrow\" id=\"MathJax-Span-261\"><span class=\"mn\" id=\"MathJax-Span-262\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-263\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-265\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>Attention</mtext><mo stretchy=\"false\">(</mo><msub><mi>Q</mi><mi>t</mi></msub><mo>,</mo><msub><mi>K</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>V</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-25\">\\text{Attention}(Q_t, K_{1:t}, V_{1:t})</script>\n      </li>\n      <li>\n        <p>This reduces complexity per step to:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-266\" style=\"width: 4.169em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.44em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.39em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-267\"><span class=\"mi\" id=\"MathJax-Span-268\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-269\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-270\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-271\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-272\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-273\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>n</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-26\">O(n \\cdot d)</script>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hardware impact:</strong></p>\n\n    <ul>\n      <li>Reduces compute but increases <strong>memory bandwidth</strong> requirements since past K/V matrices must be accessed repeatedly.</li>\n      <li>Critical for CPU-based decoding; also essential on NPUs with limited memory.</li>\n    </ul>\n  </li>\n  <li>\n    <p>A detailed discourse on this topic is available in our <a href=\"../model-acceleration\">Model Acceleration</a> primer.</p>\n  </li>\n</ul>\n<p>In decoder-based models (e.g., GPT), each new token requires computing self-attention with all past tokens. Without optimization, this scales as:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-236\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-237\"><span class=\"mi\" id=\"MathJax-Span-238\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">n</script> is the number of generated tokens and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-239\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-240\"><span class=\"mi\" id=\"MathJax-Span-241\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">d</script> is the hidden dimension.</li>\n    </ul>\n<p><strong>KV cache principle:</strong></p>\n<ul>\n      <li>Store the key (K) and value (V) projections for all past tokens during generation.</li>\n      <li>\n        <p>For a new token, compute only its query (Q) and perform:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>Attention</mtext><mo stretchy=&quot;false&quot;>(</mo><msub><mi>Q</mi><mi>t</mi></msub><mo>,</mo><msub><mi>K</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>V</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-242\" style=\"width: 11.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.326em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1009.27em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-243\"><span class=\"mtext\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Regular;\">Attention</span><span class=\"mo\" id=\"MathJax-Span-245\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-246\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-247\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-248\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-249\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-250\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-251\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-252\"><span class=\"mrow\" id=\"MathJax-Span-253\"><span class=\"mn\" id=\"MathJax-Span-254\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-255\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mi\" id=\"MathJax-Span-256\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-257\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-258\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-259\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-260\"><span class=\"mrow\" id=\"MathJax-Span-261\"><span class=\"mn\" id=\"MathJax-Span-262\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-263\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-265\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>Attention</mtext><mo stretchy=\"false\">(</mo><msub><mi>Q</mi><mi>t</mi></msub><mo>,</mo><msub><mi>K</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>V</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-25\">\\text{Attention}(Q_t, K_{1:t}, V_{1:t})</script>\n      </li>\n      <li>\n        <p>This reduces complexity per step to:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-266\" style=\"width: 4.169em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.44em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.39em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-267\"><span class=\"mi\" id=\"MathJax-Span-268\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-269\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-270\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-271\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-272\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-273\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>n</mi><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-26\">O(n \\cdot d)</script>\n      </li>\n    </ul>\n<p>For a new token, compute only its query (Q) and perform:</p>\n<p>This reduces complexity per step to:</p>\n<p><strong>Hardware impact:</strong></p>\n<ul>\n      <li>Reduces compute but increases <strong>memory bandwidth</strong> requirements since past K/V matrices must be accessed repeatedly.</li>\n      <li>Critical for CPU-based decoding; also essential on NPUs with limited memory.</li>\n    </ul>\n<p>A detailed discourse on this topic is available in our <a href=\"../model-acceleration\">Model Acceleration</a> primer.</p>",
      "contentMarkdown": "*   In decoder-based models (e.g., GPT), each new token requires computing self-attention with all past tokens. Without optimization, this scales as:\n    \n    O(n2⋅d)O(n2⋅d)\n    \n    O(n^2 \\\\cdot d)\n    *   where nnn is the number of generated tokens and ddd is the hidden dimension.\n*   **KV cache principle:**\n    \n    *   Store the key (K) and value (V) projections for all past tokens during generation.\n    *   For a new token, compute only its query (Q) and perform:\n        \n        Attention(Qt,K1:t,V1:t)Attention(Qt,K1:t,V1:t)\n        \n        \\\\text{Attention}(Q\\_t, K\\_{1:t}, V\\_{1:t})\n    *   This reduces complexity per step to:\n        \n        O(n⋅d)O(n⋅d)\n        \n        O(n \\\\cdot d)\n*   **Hardware impact:**\n    \n    *   Reduces compute but increases **memory bandwidth** requirements since past K/V matrices must be accessed repeatedly.\n    *   Critical for CPU-based decoding; also essential on NPUs with limited memory.\n*   A detailed discourse on this topic is available in our [Model Acceleration](../model-acceleration) primer.\n    \n\nIn decoder-based models (e.g., GPT), each new token requires computing self-attention with all past tokens. Without optimization, this scales as:\n\n*   where nnn is the number of generated tokens and ddd is the hidden dimension.\n\n**KV cache principle:**\n\n*   Store the key (K) and value (V) projections for all past tokens during generation.\n*   For a new token, compute only its query (Q) and perform:\n    \n    Attention(Qt,K1:t,V1:t)Attention(Qt,K1:t,V1:t)\n    \n    \\\\text{Attention}(Q\\_t, K\\_{1:t}, V\\_{1:t})\n*   This reduces complexity per step to:\n    \n    O(n⋅d)O(n⋅d)\n    \n    O(n \\\\cdot d)\n\nFor a new token, compute only its query (Q) and perform:\n\nThis reduces complexity per step to:\n\n**Hardware impact:**\n\n*   Reduces compute but increases **memory bandwidth** requirements since past K/V matrices must be accessed repeatedly.\n*   Critical for CPU-based decoding; also essential on NPUs with limited memory.\n\nA detailed discourse on this topic is available in our [Model Acceleration](../model-acceleration) primer.",
      "order": 9,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "attention",
        "gpt",
        "optimization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 271,
        "contentLength": 25270
      },
      "nextCards": [
        "ai-on-device-transformers-speculative-decoding-10",
        "ai-on-device-transformers-quantization-48-bit-11"
      ],
      "relatedCards": [
        "ai-model-compression-background-matrix-multiplication-in-gpus-2",
        "ai-model-compression-multimodal-quantization-14",
        "ai-federated-learning-fedprox-7",
        "ai-federated-learning-scaffold-8",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#key-value-(kv)-cache-optimization",
      "scrapedAt": "2025-12-28T11:56:11.407Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-speculative-decoding-10",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Optimization Techniques for On-Device Transformers",
      "title": "Speculative Decoding",
      "subtitle": "Optimization Techniques for On-Device Transformers",
      "contentHtml": "<ul>\n  <li>\n    <p>Speculative decoding improves throughput by parallelizing token generation while maintaining correctness.</p>\n  </li>\n  <li>\n    <p><strong>Workflow:</strong></p>\n\n    <ol>\n      <li>Use a small <strong>draft model</strong> to propose multiple tokens ahead.</li>\n      <li>A larger <strong>target model</strong> validates or rejects the proposed tokens in parallel.</li>\n      <li>Accepted tokens are appended to the sequence; rejected tokens trigger fallback to standard autoregressive decoding.</li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Benefit:</strong></p>\n\n    <ul>\n      <li>Reduces total number of sequential steps for the large model.</li>\n      <li>Best suited for GPU or NPU where batched verification of tokens can be parallelized.</li>\n    </ul>\n  </li>\n  <li>\n    <p>A detailed discourse on this topic is available in our <a href=\"../speculative-decoding\">Speculative Decoding</a> primer.</p>\n  </li>\n</ul>\n<p>Speculative decoding improves throughput by parallelizing token generation while maintaining correctness.</p>\n<p><strong>Workflow:</strong></p>\n<ol>\n      <li>Use a small <strong>draft model</strong> to propose multiple tokens ahead.</li>\n      <li>A larger <strong>target model</strong> validates or rejects the proposed tokens in parallel.</li>\n      <li>Accepted tokens are appended to the sequence; rejected tokens trigger fallback to standard autoregressive decoding.</li>\n    </ol>\n<p><strong>Benefit:</strong></p>\n<ul>\n      <li>Reduces total number of sequential steps for the large model.</li>\n      <li>Best suited for GPU or NPU where batched verification of tokens can be parallelized.</li>\n    </ul>\n<p>A detailed discourse on this topic is available in our <a href=\"../speculative-decoding\">Speculative Decoding</a> primer.</p>",
      "contentMarkdown": "*   Speculative decoding improves throughput by parallelizing token generation while maintaining correctness.\n    \n*   **Workflow:**\n    \n    1.  Use a small **draft model** to propose multiple tokens ahead.\n    2.  A larger **target model** validates or rejects the proposed tokens in parallel.\n    3.  Accepted tokens are appended to the sequence; rejected tokens trigger fallback to standard autoregressive decoding.\n*   **Benefit:**\n    \n    *   Reduces total number of sequential steps for the large model.\n    *   Best suited for GPU or NPU where batched verification of tokens can be parallelized.\n*   A detailed discourse on this topic is available in our [Speculative Decoding](../speculative-decoding) primer.\n    \n\nSpeculative decoding improves throughput by parallelizing token generation while maintaining correctness.\n\n**Workflow:**\n\n1.  Use a small **draft model** to propose multiple tokens ahead.\n2.  A larger **target model** validates or rejects the proposed tokens in parallel.\n3.  Accepted tokens are appended to the sequence; rejected tokens trigger fallback to standard autoregressive decoding.\n\n**Benefit:**\n\n*   Reduces total number of sequential steps for the large model.\n*   Best suited for GPU or NPU where batched verification of tokens can be parallelized.\n\nA detailed discourse on this topic is available in our [Speculative Decoding](../speculative-decoding) primer.",
      "order": 10,
      "orderInChapter": 3,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 188,
        "contentLength": 1786
      },
      "nextCards": [
        "ai-on-device-transformers-quantization-48-bit-11",
        "ai-on-device-transformers-knowledge-distillation-12"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-federated-learning-cross-device-federated-learning-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#speculative-decoding",
      "scrapedAt": "2025-12-28T11:56:11.407Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-quantization-48-bit-11",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Optimization Techniques for On-Device Transformers",
      "title": "Quantization (4/8-bit)",
      "subtitle": "Optimization Techniques for On-Device Transformers",
      "contentHtml": "<ul>\n  <li>\n    <p>Quantization reduces model weights and activations from <code class=\"language-plaintext highlighter-rouge\">float32</code> or <code class=\"language-plaintext highlighter-rouge\">float16</code> to lower precision (e.g., <code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">int4</code>).</p>\n\n    <ul>\n      <li><strong>Static Quantization</strong>: Precompute scale factors during calibration on a representative dataset.</li>\n      <li><strong>Dynamic Quantization</strong>: Scales are computed on-the-fly during inference, less accurate but easier to apply.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Equation:</strong></p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>q</mi><mi>u</mi><mi>a</mi><mi>n</mi><mi>t</mi></mrow></msub><mo>=</mo><mtext>round</mtext><mrow><mo>(</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-274\" style=\"width: 9.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.867em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.138em, 1007.76em, 4.326em, -999.997em); top: -3.487em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-275\"><span class=\"msubsup\" id=\"MathJax-Span-276\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-277\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-278\"><span class=\"mrow\" id=\"MathJax-Span-279\"><span class=\"mi\" id=\"MathJax-Span-280\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">q</span><span class=\"mi\" id=\"MathJax-Span-281\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">u</span><span class=\"mi\" id=\"MathJax-Span-282\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-283\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span><span class=\"mi\" id=\"MathJax-Span-284\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-285\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mtext\" id=\"MathJax-Span-286\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">round</span><span class=\"mrow\" id=\"MathJax-Span-287\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-288\" style=\"vertical-align: -0.414em;\"><span style=\"font-family: STIXSizeTwoSym;\">(</span></span><span class=\"mfrac\" id=\"MathJax-Span-289\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.206em;\"><span class=\"mi\" id=\"MathJax-Span-290\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.206em;\"><span class=\"mi\" id=\"MathJax-Span-291\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.58em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.576em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-292\" style=\"vertical-align: -0.414em;\"><span style=\"font-family: STIXSizeTwoSym;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.492em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>q</mi><mi>u</mi><mi>a</mi><mi>n</mi><mi>t</mi></mrow></msub><mo>=</mo><mtext>round</mtext><mrow><mo>(</mo><mfrac><mi>x</mi><mi>s</mi></mfrac><mo>)</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-27\">x_{quant} = \\text{round}\\left(\\frac{x}{s}\\right)</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-293\" style=\"width: 0.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.37em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-294\"><span class=\"mi\" id=\"MathJax-Span-295\" style=\"font-family: STIXGeneral-Italic;\">s</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">s</script> is the scaling factor.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Benefits:</strong></p>\n\n    <ul>\n      <li>Reduces memory footprint by 4–8x.</li>\n      <li>Increases throughput on CPUs (via AVX512/AMX) and NPUs (native <code class=\"language-plaintext highlighter-rouge\">int8</code> support).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Trade-offs:</strong></p>\n\n    <ul>\n      <li>May reduce accuracy, especially for attention layers and small models.</li>\n      <li>Requires per-channel quantization for best results.</li>\n    </ul>\n  </li>\n  <li>\n    <p>A detailed discourse on this topic is available in our <a href=\"../model-compression\">Model Compression</a> primer.</p>\n  </li>\n</ul>\n<p>Quantization reduces model weights and activations from <code class=\"language-plaintext highlighter-rouge\">float32</code> or <code class=\"language-plaintext highlighter-rouge\">float16</code> to lower precision (e.g., <code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">int4</code>).</p>\n<ul>\n      <li><strong>Static Quantization</strong>: Precompute scale factors during calibration on a representative dataset.</li>\n      <li><strong>Dynamic Quantization</strong>: Scales are computed on-the-fly during inference, less accurate but easier to apply.</li>\n    </ul>\n<p><strong>Equation:</strong></p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-293\" style=\"width: 0.471em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.367em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.37em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-294\"><span class=\"mi\" id=\"MathJax-Span-295\" style=\"font-family: STIXGeneral-Italic;\">s</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">s</script> is the scaling factor.</li>\n    </ul>\n<p><strong>Benefits:</strong></p>\n<ul>\n      <li>Reduces memory footprint by 4–8x.</li>\n      <li>Increases throughput on CPUs (via AVX512/AMX) and NPUs (native <code class=\"language-plaintext highlighter-rouge\">int8</code> support).</li>\n    </ul>\n<p><strong>Trade-offs:</strong></p>\n<ul>\n      <li>May reduce accuracy, especially for attention layers and small models.</li>\n      <li>Requires per-channel quantization for best results.</li>\n    </ul>\n<p>A detailed discourse on this topic is available in our <a href=\"../model-compression\">Model Compression</a> primer.</p>",
      "contentMarkdown": "*   Quantization reduces model weights and activations from `float32` or `float16` to lower precision (e.g., `int8` or `int4`).\n    \n    *   **Static Quantization**: Precompute scale factors during calibration on a representative dataset.\n    *   **Dynamic Quantization**: Scales are computed on-the-fly during inference, less accurate but easier to apply.\n*   **Equation:**\n    \n    xquant\\=round(xs)xquant\\=round(xs)\n    \n    x\\_{quant} = \\\\text{round}\\\\left(\\\\frac{x}{s}\\\\right)\n    *   where sss is the scaling factor.\n*   **Benefits:**\n    \n    *   Reduces memory footprint by 4–8x.\n    *   Increases throughput on CPUs (via AVX512/AMX) and NPUs (native `int8` support).\n*   **Trade-offs:**\n    \n    *   May reduce accuracy, especially for attention layers and small models.\n    *   Requires per-channel quantization for best results.\n*   A detailed discourse on this topic is available in our [Model Compression](../model-compression) primer.\n    \n\nQuantization reduces model weights and activations from `float32` or `float16` to lower precision (e.g., `int8` or `int4`).\n\n*   **Static Quantization**: Precompute scale factors during calibration on a representative dataset.\n*   **Dynamic Quantization**: Scales are computed on-the-fly during inference, less accurate but easier to apply.\n\n**Equation:**\n\n*   where sss is the scaling factor.\n\n**Benefits:**\n\n*   Reduces memory footprint by 4–8x.\n*   Increases throughput on CPUs (via AVX512/AMX) and NPUs (native `int8` support).\n\n**Trade-offs:**\n\n*   May reduce accuracy, especially for attention layers and small models.\n*   Requires per-channel quantization for best results.\n\nA detailed discourse on this topic is available in our [Model Compression](../model-compression) primer.",
      "order": 11,
      "orderInChapter": 4,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "attention",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 215,
        "contentLength": 9969
      },
      "nextCards": [
        "ai-on-device-transformers-knowledge-distillation-12",
        "ai-on-device-transformers-weight-pruning-and-low-rank-approximations-13"
      ],
      "relatedCards": [
        "ai-model-compression-dequantization-considerations-5",
        "ai-model-compression-when-to-use-lightweight-models-53",
        "ai-model-compression-types-of-quantization-4",
        "ai-model-compression-how-far-can-quantization-be-pushed-20",
        "ai-model-compression-types-of-knowledge-distillation-23"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#quantization-(4/8-bit)",
      "scrapedAt": "2025-12-28T11:56:11.407Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-knowledge-distillation-12",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Optimization Techniques for On-Device Transformers",
      "title": "Knowledge Distillation",
      "subtitle": "Optimization Techniques for On-Device Transformers",
      "contentHtml": "<ul>\n  <li>\n    <p>Use a large, accurate <strong>teacher model</strong> to train a smaller <strong>student model</strong>:</p>\n  </li>\n  <li>\n    <p>Objective function includes both hard labels and soft probabilities:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>L</mi><mo>=</mo><mi>&amp;#x03B1;</mi><msub><mi>L</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>CE</mtext></mrow></msub><mo>+</mo><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><mi>&amp;#x03B1;</mi><mo stretchy=&quot;false&quot;>)</mo><mspace width=&quot;thinmathspace&quot; /><mtext>KL</mtext><mo stretchy=&quot;false&quot;>(</mo><msub><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>teacher</mtext></mrow></msub><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><msub><mi>p</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>student</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-296\" style=\"width: 18.701em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 15.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1015.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-297\"><span class=\"mi\" id=\"MathJax-Span-298\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-299\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-300\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">α</span><span class=\"msubsup\" id=\"MathJax-Span-301\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-302\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-303\"><span class=\"mrow\" id=\"MathJax-Span-304\"><span class=\"mtext\" id=\"MathJax-Span-305\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">CE</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mo\" id=\"MathJax-Span-307\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">(</span><span class=\"mn\" id=\"MathJax-Span-308\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-309\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-310\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">α</span><span class=\"mo\" id=\"MathJax-Span-311\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mspace\" id=\"MathJax-Span-312\" style=\"height: 0em; vertical-align: 0em; width: 0.211em; display: inline-block; overflow: hidden;\"></span><span class=\"mtext\" id=\"MathJax-Span-313\" style=\"font-family: STIXGeneral-Regular;\">KL</span><span class=\"mo\" id=\"MathJax-Span-314\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-315\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-316\" style=\"font-family: STIXGeneral-Italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-317\"><span class=\"mrow\" id=\"MathJax-Span-318\"><span class=\"mtext\" id=\"MathJax-Span-319\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">teacher</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-320\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"msubsup\" id=\"MathJax-Span-321\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-322\" style=\"font-family: STIXGeneral-Italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-323\"><span class=\"mrow\" id=\"MathJax-Span-324\"><span class=\"mtext\" id=\"MathJax-Span-325\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">student</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-326\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>L</mi><mo>=</mo><mi>α</mi><msub><mi>L</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>CE</mtext></mrow></msub><mo>+</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy=\"false\">)</mo><mspace width=\"thinmathspace\"></mspace><mtext>KL</mtext><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>teacher</mtext></mrow></msub><mo fence=\"false\" stretchy=\"false\">‖</mo><msub><mi>p</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>student</mtext></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-29\">L = \\alpha L_{\\text{CE}} + (1 - \\alpha) \\, \\text{KL}(p_{\\text{teacher}} \\| p_{\\text{student}})</script>\n  </li>\n  <li>\n    <p><strong>Benefit:</strong></p>\n\n    <ul>\n      <li>Student model retains much of the teacher’s performance with fewer parameters.</li>\n      <li>Ideal for NPUs and CPU edge deployments.</li>\n    </ul>\n  </li>\n  <li>\n    <p>A detailed discourse on this topic is available in our <a href=\"../model-compression\">Model Compression</a> primer.</p>\n  </li>\n</ul>\n<p>Use a large, accurate <strong>teacher model</strong> to train a smaller <strong>student model</strong>:</p>\n<p>Objective function includes both hard labels and soft probabilities:</p>\n<p><strong>Benefit:</strong></p>\n<ul>\n      <li>Student model retains much of the teacher’s performance with fewer parameters.</li>\n      <li>Ideal for NPUs and CPU edge deployments.</li>\n    </ul>\n<p>A detailed discourse on this topic is available in our <a href=\"../model-compression\">Model Compression</a> primer.</p>",
      "contentMarkdown": "*   Use a large, accurate **teacher model** to train a smaller **student model**:\n    \n*   Objective function includes both hard labels and soft probabilities:\n    \n    L\\=αLCE+(1−α)KL(pteacher‖pstudent)L\\=αLCE+(1−α)KL(pteacher‖pstudent)\n    \n    L = \\\\alpha L\\_{\\\\text{CE}} + (1 - \\\\alpha) \\\\, \\\\text{KL}(p\\_{\\\\text{teacher}} \\\\| p\\_{\\\\text{student}})\n*   **Benefit:**\n    \n    *   Student model retains much of the teacher’s performance with fewer parameters.\n    *   Ideal for NPUs and CPU edge deployments.\n*   A detailed discourse on this topic is available in our [Model Compression](../model-compression) primer.\n    \n\nUse a large, accurate **teacher model** to train a smaller **student model**:\n\nObjective function includes both hard labels and soft probabilities:\n\n**Benefit:**\n\n*   Student model retains much of the teacher’s performance with fewer parameters.\n*   Ideal for NPUs and CPU edge deployments.\n\nA detailed discourse on this topic is available in our [Model Compression](../model-compression) primer.",
      "order": 12,
      "orderInChapter": 5,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 127,
        "contentLength": 7581
      },
      "nextCards": [
        "ai-on-device-transformers-weight-pruning-and-low-rank-approximations-13",
        "ai-on-device-transformers-operator-and-graph-fusion-14"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#knowledge-distillation",
      "scrapedAt": "2025-12-28T11:56:11.407Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-weight-pruning-and-low-rank-approximations-13",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Optimization Techniques for On-Device Transformers",
      "title": "Weight Pruning and Low-Rank Approximations",
      "subtitle": "Optimization Techniques for On-Device Transformers",
      "contentHtml": "<ul>\n  <li><strong>Pruning</strong> removes less important weights (e.g., structured pruning of entire attention heads).</li>\n  <li>\n    <p><strong>Low-rank factorization</strong> decomposes weight matrices:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>W</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>d</mi></mrow></msup><mo>&amp;#x2248;</mo><mi>U</mi><msup><mi>V</mi><mi>T</mi></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-327\" style=\"width: 9.013em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.503em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1007.5em, 2.451em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-328\"><span class=\"mi\" id=\"MathJax-Span-329\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-330\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-331\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.034em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-332\"><span class=\"mrow\" id=\"MathJax-Span-333\"><span class=\"mi\" id=\"MathJax-Span-334\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-335\"><span class=\"mrow\" id=\"MathJax-Span-336\"><span class=\"mi\" id=\"MathJax-Span-337\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-338\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-339\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-340\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mi\" id=\"MathJax-Span-341\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-342\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-343\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.784em;\"><span class=\"mi\" id=\"MathJax-Span-344\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">T<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.122em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>W</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>d</mi></mrow></msup><mo>≈</mo><mi>U</mi><msup><mi>V</mi><mi>T</mi></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-30\">W \\in \\mathbb{R}^{d \\times d} \\approx U V^T</script>\n\n    <ul>\n      <li>with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>U</mi><mo>,</mo><mi>V</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></mrow></msup><mo>,</mo><mi>r</mi><mo>&amp;#x226A;</mo><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-345\" style=\"width: 9.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.867em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1007.87em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-346\"><span class=\"mi\" id=\"MathJax-Span-347\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-348\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-349\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-350\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-351\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-352\"><span class=\"mrow\" id=\"MathJax-Span-353\"><span class=\"mi\" id=\"MathJax-Span-354\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-355\"><span class=\"mrow\" id=\"MathJax-Span-356\"><span class=\"mi\" id=\"MathJax-Span-357\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-358\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-359\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-360\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-361\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-362\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≪</span><span class=\"mi\" id=\"MathJax-Span-363\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>U</mi><mo>,</mo><mi>V</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup><mo>,</mo><mi>r</mi><mo>≪</mo><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">U, V \\in \\mathbb{R}^{d \\times r}, r \\ll d</script>.</li>\n    </ul>\n  </li>\n  <li>\n    <p>These reduce both memory and FLOPs.</p>\n  </li>\n  <li>A detailed discourse on this topic is available in our <a href=\"../model-compression\">Model Compression</a> primer.</li>\n</ul>\n<p><strong>Low-rank factorization</strong> decomposes weight matrices:</p>\n<ul>\n      <li>with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>U</mi><mo>,</mo><mi>V</mi><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mo>&amp;#x00D7;</mo><mi>r</mi></mrow></msup><mo>,</mo><mi>r</mi><mo>&amp;#x226A;</mo><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-345\" style=\"width: 9.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.867em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1007.87em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-346\"><span class=\"mi\" id=\"MathJax-Span-347\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-348\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-349\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-350\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-351\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-352\"><span class=\"mrow\" id=\"MathJax-Span-353\"><span class=\"mi\" id=\"MathJax-Span-354\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-355\"><span class=\"mrow\" id=\"MathJax-Span-356\"><span class=\"mi\" id=\"MathJax-Span-357\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-358\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-359\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-360\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-361\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-362\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≪</span><span class=\"mi\" id=\"MathJax-Span-363\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>U</mi><mo>,</mo><mi>V</mi><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup><mo>,</mo><mi>r</mi><mo>≪</mo><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">U, V \\in \\mathbb{R}^{d \\times r}, r \\ll d</script>.</li>\n    </ul>\n<p>These reduce both memory and FLOPs.</p>",
      "contentMarkdown": "*   **Pruning** removes less important weights (e.g., structured pruning of entire attention heads).\n*   **Low-rank factorization** decomposes weight matrices:\n    \n    W∈ℝd×d≈UVTW∈Rd×d≈UVT\n    \n    W \\\\in \\\\mathbb{R}^{d \\\\times d} \\\\approx U V^T\n    *   with U,V∈ℝd×r,r≪dU,V∈Rd×r,r≪dU, V \\\\in \\\\mathbb{R}^{d \\\\times r}, r \\\\ll d.\n*   These reduce both memory and FLOPs.\n    \n*   A detailed discourse on this topic is available in our [Model Compression](../model-compression) primer.\n\n**Low-rank factorization** decomposes weight matrices:\n\n*   with U,V∈ℝd×r,r≪dU,V∈Rd×r,r≪dU, V \\\\in \\\\mathbb{R}^{d \\\\times r}, r \\\\ll d.\n\nThese reduce both memory and FLOPs.",
      "order": 13,
      "orderInChapter": 6,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "attention"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 82,
        "contentLength": 13431
      },
      "nextCards": [
        "ai-on-device-transformers-operator-and-graph-fusion-14",
        "ai-on-device-transformers-sequence-length-and-batch-optimizations-15"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#weight-pruning-and-low-rank-approximations",
      "scrapedAt": "2025-12-28T11:56:11.407Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-operator-and-graph-fusion-14",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Optimization Techniques for On-Device Transformers",
      "title": "Operator and Graph Fusion",
      "subtitle": "Optimization Techniques for On-Device Transformers",
      "contentHtml": "<ul>\n  <li>Merge operations like Linear + Bias + LayerNorm to reduce memory reads/writes.</li>\n  <li>Frameworks: TensorRT (GPU), OpenVINO (CPU), CoreML (NPU).</li>\n</ul>",
      "contentMarkdown": "*   Merge operations like Linear + Bias + LayerNorm to reduce memory reads/writes.\n*   Frameworks: TensorRT (GPU), OpenVINO (CPU), CoreML (NPU).",
      "order": 14,
      "orderInChapter": 7,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 21,
        "contentLength": 169
      },
      "nextCards": [
        "ai-on-device-transformers-sequence-length-and-batch-optimizations-15",
        "ai-on-device-transformers-hardware-specific-optimization-notes-16"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-federated-learning-cross-device-federated-learning-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#operator-and-graph-fusion",
      "scrapedAt": "2025-12-28T11:56:11.408Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-sequence-length-and-batch-optimizations-15",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Optimization Techniques for On-Device Transformers",
      "title": "Sequence Length and Batch Optimizations",
      "subtitle": "Optimization Techniques for On-Device Transformers",
      "contentHtml": "<ul>\n  <li>Use <strong>sliding window attention</strong> for long sequences.</li>\n  <li>Tune batch size to match hardware occupancy (especially on GPUs).</li>\n</ul>",
      "contentMarkdown": "*   Use **sliding window attention** for long sequences.\n*   Tune batch size to match hardware occupancy (especially on GPUs).",
      "order": 15,
      "orderInChapter": 8,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "attention"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 19,
        "contentLength": 164
      },
      "nextCards": [
        "ai-on-device-transformers-hardware-specific-optimization-notes-16",
        "ai-on-device-transformers-cpu-deployment-considerations-17"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-federated-learning-cross-device-federated-learning-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#sequence-length-and-batch-optimizations",
      "scrapedAt": "2025-12-28T11:56:11.408Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-hardware-specific-optimization-notes-16",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Optimization Techniques for On-Device Transformers",
      "title": "Hardware-Specific Optimization Notes",
      "subtitle": "Optimization Techniques for On-Device Transformers",
      "contentHtml": "<ul>\n  <li><strong>CPU:</strong> Quantization (<code class=\"language-plaintext highlighter-rouge\">int8</code>), operator fusion, KV caching.</li>\n  <li><strong>GPU:</strong> Mixed precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>/<code class=\"language-plaintext highlighter-rouge\">bfloat16</code>), speculative decoding, batching.</li>\n  <li><strong>TPU:</strong> XLA graph compilation, dense matmul optimizations.</li>\n  <li><strong>NPU:</strong> Static quantization (<code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">int4</code>), model distillation, pruning.</li>\n</ul>",
      "contentMarkdown": "*   **CPU:** Quantization (`int8`), operator fusion, KV caching.\n*   **GPU:** Mixed precision (`float16`/`bfloat16`), speculative decoding, batching.\n*   **TPU:** XLA graph compilation, dense matmul optimizations.\n*   **NPU:** Static quantization (`int8`/`int4`), model distillation, pruning.",
      "order": 16,
      "orderInChapter": 9,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 32,
        "contentLength": 663
      },
      "nextCards": [
        "ai-on-device-transformers-cpu-deployment-considerations-17",
        "ai-on-device-transformers-gpu-deployment-considerations-18"
      ],
      "relatedCards": [
        "ai-federated-learning-fedprox-7",
        "ai-federated-learning-scaffold-8",
        "ai-model-compression-low-rank-correction-for-quantization-45",
        "ai-federated-learning-federated-stochastic-gradient-descent-fedsgd-5",
        "ai-federated-learning-feddyn-10"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#hardware-specific-optimization-notes",
      "scrapedAt": "2025-12-28T11:56:11.408Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-cpu-deployment-considerations-17",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Practical Considerations and Pitfalls When Deploying Transformers on CPU, GPU, and NPUs",
      "title": "CPU Deployment Considerations",
      "subtitle": "Practical Considerations and Pitfalls When Deploying Transformers on CPU, GPU, and NPUs",
      "contentHtml": "<h4 id=\"key-strengths\">Key Strengths</h4>\n<ul>\n  <li>General-purpose flexibility</li>\n  <li>No need for specialized drivers or runtime</li>\n  <li>Lower memory bandwidth than accelerators, but with good cache locality</li>\n</ul>\n<h4 id=\"watch-out-for\">Watch Out for</h4>\n<ol>\n  <li>\n    <p><strong>MatMul Bottlenecks:</strong></p>\n\n    <ul>\n      <li>Transformer layers involve large GEMMs (General Matrix-Matrix Multiplications).</li>\n      <li>Use libraries like Intel MKL-DNN, oneDNN, or OpenBLAS with AVX2/AVX512 instructions.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cache Contention:</strong></p>\n\n    <ul>\n      <li>KV cache can overflow L2/L3 caches, especially with long sequences.</li>\n      <li>Performance drops significantly when cache locality is lost.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Thread Over-subscription:</strong></p>\n\n    <ul>\n      <li>Avoid naive use of multithreading; prefer thread pools and NUMA-aware scheduling.</li>\n      <li>Profile using <code class=\"language-plaintext highlighter-rouge\">perf</code>, Intel VTune, or similar tools.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Memory Bandwidth:</strong></p>\n\n    <ul>\n      <li>CPUs often become memory-bound during decoder operation.</li>\n      <li>Optimize memory access patterns and fuse operations to reduce intermediary transfers.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Quantization Challenges:</strong></p>\n\n    <ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">int8</code> quantization is highly effective, but requires calibration and may degrade attention accuracy.</li>\n      <li>Use dynamic quantization only if static is not viable.</li>\n    </ul>\n  </li>\n</ol>\n<p><strong>MatMul Bottlenecks:</strong></p>\n<ul>\n      <li>Transformer layers involve large GEMMs (General Matrix-Matrix Multiplications).</li>\n      <li>Use libraries like Intel MKL-DNN, oneDNN, or OpenBLAS with AVX2/AVX512 instructions.</li>\n    </ul>\n<p><strong>Cache Contention:</strong></p>\n<ul>\n      <li>KV cache can overflow L2/L3 caches, especially with long sequences.</li>\n      <li>Performance drops significantly when cache locality is lost.</li>\n    </ul>\n<p><strong>Thread Over-subscription:</strong></p>\n<ul>\n      <li>Avoid naive use of multithreading; prefer thread pools and NUMA-aware scheduling.</li>\n      <li>Profile using <code class=\"language-plaintext highlighter-rouge\">perf</code>, Intel VTune, or similar tools.</li>\n    </ul>\n<p><strong>Memory Bandwidth:</strong></p>\n<ul>\n      <li>CPUs often become memory-bound during decoder operation.</li>\n      <li>Optimize memory access patterns and fuse operations to reduce intermediary transfers.</li>\n    </ul>\n<p><strong>Quantization Challenges:</strong></p>\n<ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">int8</code> quantization is highly effective, but requires calibration and may degrade attention accuracy.</li>\n      <li>Use dynamic quantization only if static is not viable.</li>\n    </ul>",
      "contentMarkdown": "#### Key Strengths\n\n*   General-purpose flexibility\n*   No need for specialized drivers or runtime\n*   Lower memory bandwidth than accelerators, but with good cache locality\n\n#### Watch Out for\n\n1.  **MatMul Bottlenecks:**\n    \n    *   Transformer layers involve large GEMMs (General Matrix-Matrix Multiplications).\n    *   Use libraries like Intel MKL-DNN, oneDNN, or OpenBLAS with AVX2/AVX512 instructions.\n2.  **Cache Contention:**\n    \n    *   KV cache can overflow L2/L3 caches, especially with long sequences.\n    *   Performance drops significantly when cache locality is lost.\n3.  **Thread Over-subscription:**\n    \n    *   Avoid naive use of multithreading; prefer thread pools and NUMA-aware scheduling.\n    *   Profile using `perf`, Intel VTune, or similar tools.\n4.  **Memory Bandwidth:**\n    \n    *   CPUs often become memory-bound during decoder operation.\n    *   Optimize memory access patterns and fuse operations to reduce intermediary transfers.\n5.  **Quantization Challenges:**\n    \n    *   `int8` quantization is highly effective, but requires calibration and may degrade attention accuracy.\n    *   Use dynamic quantization only if static is not viable.\n\n**MatMul Bottlenecks:**\n\n*   Transformer layers involve large GEMMs (General Matrix-Matrix Multiplications).\n*   Use libraries like Intel MKL-DNN, oneDNN, or OpenBLAS with AVX2/AVX512 instructions.\n\n**Cache Contention:**\n\n*   KV cache can overflow L2/L3 caches, especially with long sequences.\n*   Performance drops significantly when cache locality is lost.\n\n**Thread Over-subscription:**\n\n*   Avoid naive use of multithreading; prefer thread pools and NUMA-aware scheduling.\n*   Profile using `perf`, Intel VTune, or similar tools.\n\n**Memory Bandwidth:**\n\n*   CPUs often become memory-bound during decoder operation.\n*   Optimize memory access patterns and fuse operations to reduce intermediary transfers.\n\n**Quantization Challenges:**\n\n*   `int8` quantization is highly effective, but requires calibration and may degrade attention accuracy.\n*   Use dynamic quantization only if static is not viable.",
      "order": 17,
      "orderInChapter": 1,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "transformer",
        "attention"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 266,
        "contentLength": 2986
      },
      "nextCards": [
        "ai-on-device-transformers-gpu-deployment-considerations-18",
        "ai-on-device-transformers-tpu-deployment-considerations-19"
      ],
      "relatedCards": [
        "ai-model-compression-design-methodologies-51",
        "ai-model-compression-mitigation-strategies-8",
        "ai-model-compression-compute-vs-memory-bottlenecks-12",
        "ai-model-compression-practical-considerations-37",
        "ai-model-compression-representative-architectures-52"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#cpu-deployment-considerations",
      "scrapedAt": "2025-12-28T11:56:11.408Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-gpu-deployment-considerations-18",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Practical Considerations and Pitfalls When Deploying Transformers on CPU, GPU, and NPUs",
      "title": "GPU Deployment Considerations",
      "subtitle": "Practical Considerations and Pitfalls When Deploying Transformers on CPU, GPU, and NPUs",
      "contentHtml": "<h4 id=\"key-strengths-1\">Key Strengths</h4>\n<ul>\n  <li>Ideal for batched operations and compute-heavy encoder layers</li>\n  <li>High memory bandwidth, many-core architecture</li>\n</ul>\n<h4 id=\"watch-out-for-1\">Watch Out for</h4>\n<ol>\n  <li>\n    <p><strong>Underutilization During Decoding:</strong></p>\n\n    <ul>\n      <li>Single-token decode workloads don’t fully occupy GPU cores.</li>\n      <li>Consider batching requests or speculative decoding to improve efficiency.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Kernel Launch Overhead:</strong></p>\n\n    <ul>\n      <li>GPU launch latency can dominate runtime in step-by-step decoding.</li>\n      <li>Use fused kernels and persistent caches.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Host-Device Memory Transfers:</strong></p>\n\n    <ul>\n      <li>Frequent CPU-GPU synchronization (especially during dynamic decoding) can be costly.</li>\n      <li>Minimize PCIe transfers and pre-load inputs onto device.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Precision Handling:</strong></p>\n\n    <ul>\n      <li>Use <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> for faster inference, but be cautious of numerical stability.</li>\n      <li>Validate that quantized weights preserve generation quality.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Maximize Tensor Core Use:</strong></p>\n\n    <ul>\n      <li>Use NVIDIA’s cuBLAS, cuDNN, or FasterTransformer for optimized linear algebra.</li>\n    </ul>\n  </li>\n</ol>\n<p><strong>Underutilization During Decoding:</strong></p>\n<ul>\n      <li>Single-token decode workloads don’t fully occupy GPU cores.</li>\n      <li>Consider batching requests or speculative decoding to improve efficiency.</li>\n    </ul>\n<p><strong>Kernel Launch Overhead:</strong></p>\n<ul>\n      <li>GPU launch latency can dominate runtime in step-by-step decoding.</li>\n      <li>Use fused kernels and persistent caches.</li>\n    </ul>\n<p><strong>Host-Device Memory Transfers:</strong></p>\n<ul>\n      <li>Frequent CPU-GPU synchronization (especially during dynamic decoding) can be costly.</li>\n      <li>Minimize PCIe transfers and pre-load inputs onto device.</li>\n    </ul>\n<p><strong>Precision Handling:</strong></p>\n<ul>\n      <li>Use <code class=\"language-plaintext highlighter-rouge\">float16</code> or <code class=\"language-plaintext highlighter-rouge\">bfloat16</code> for faster inference, but be cautious of numerical stability.</li>\n      <li>Validate that quantized weights preserve generation quality.</li>\n    </ul>\n<p><strong>Maximize Tensor Core Use:</strong></p>\n<ul>\n      <li>Use NVIDIA’s cuBLAS, cuDNN, or FasterTransformer for optimized linear algebra.</li>\n    </ul>",
      "contentMarkdown": "#### Key Strengths\n\n*   Ideal for batched operations and compute-heavy encoder layers\n*   High memory bandwidth, many-core architecture\n\n#### Watch Out for\n\n1.  **Underutilization During Decoding:**\n    \n    *   Single-token decode workloads don’t fully occupy GPU cores.\n    *   Consider batching requests or speculative decoding to improve efficiency.\n2.  **Kernel Launch Overhead:**\n    \n    *   GPU launch latency can dominate runtime in step-by-step decoding.\n    *   Use fused kernels and persistent caches.\n3.  **Host-Device Memory Transfers:**\n    \n    *   Frequent CPU-GPU synchronization (especially during dynamic decoding) can be costly.\n    *   Minimize PCIe transfers and pre-load inputs onto device.\n4.  **Precision Handling:**\n    \n    *   Use `float16` or `bfloat16` for faster inference, but be cautious of numerical stability.\n    *   Validate that quantized weights preserve generation quality.\n5.  **Maximize Tensor Core Use:**\n    \n    *   Use NVIDIA’s cuBLAS, cuDNN, or FasterTransformer for optimized linear algebra.\n\n**Underutilization During Decoding:**\n\n*   Single-token decode workloads don’t fully occupy GPU cores.\n*   Consider batching requests or speculative decoding to improve efficiency.\n\n**Kernel Launch Overhead:**\n\n*   GPU launch latency can dominate runtime in step-by-step decoding.\n*   Use fused kernels and persistent caches.\n\n**Host-Device Memory Transfers:**\n\n*   Frequent CPU-GPU synchronization (especially during dynamic decoding) can be costly.\n*   Minimize PCIe transfers and pre-load inputs onto device.\n\n**Precision Handling:**\n\n*   Use `float16` or `bfloat16` for faster inference, but be cautious of numerical stability.\n*   Validate that quantized weights preserve generation quality.\n\n**Maximize Tensor Core Use:**\n\n*   Use NVIDIA’s cuBLAS, cuDNN, or FasterTransformer for optimized linear algebra.",
      "order": 18,
      "orderInChapter": 2,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai",
        "transformer"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 235,
        "contentLength": 2737
      },
      "nextCards": [
        "ai-on-device-transformers-tpu-deployment-considerations-19",
        "ai-on-device-transformers-npu-deployment-considerations-edgesoc-devices-20"
      ],
      "relatedCards": [
        "ai-model-compression-design-methodologies-51",
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#gpu-deployment-considerations",
      "scrapedAt": "2025-12-28T11:56:11.408Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-tpu-deployment-considerations-19",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Practical Considerations and Pitfalls When Deploying Transformers on CPU, GPU, and NPUs",
      "title": "TPU Deployment Considerations",
      "subtitle": "Practical Considerations and Pitfalls When Deploying Transformers on CPU, GPU, and NPUs",
      "contentHtml": "<h4 id=\"key-strengths-2\">Key Strengths</h4>\n<ul>\n  <li>Extremely efficient for matrix operations</li>\n  <li>Suitable for large-batch inference and training</li>\n</ul>\n<h4 id=\"watch-out-for-2\">Watch Out for</h4>\n<ol>\n  <li>\n    <p><strong>Poor Fit for Decoding:</strong></p>\n\n    <ul>\n      <li>TPUs struggle with dynamic token-wise decoding due to fixed compute graph.</li>\n      <li>May need to run decode loop on CPU or use alternative methods (e.g., GSPMD)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Static Shape Requirements:</strong></p>\n\n    <ul>\n      <li>Input/output shapes often must be known ahead of time.</li>\n      <li>Makes dynamic batching and long-sequence handling difficult.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Limited Ecosystem:</strong></p>\n\n    <ul>\n      <li>XLA and TensorFlow/JAX integration are essential.</li>\n      <li>Less flexibility than PyTorch or ONNX-based deployments.</li>\n    </ul>\n  </li>\n</ol>\n<p><strong>Poor Fit for Decoding:</strong></p>\n<ul>\n      <li>TPUs struggle with dynamic token-wise decoding due to fixed compute graph.</li>\n      <li>May need to run decode loop on CPU or use alternative methods (e.g., GSPMD)</li>\n    </ul>\n<p><strong>Static Shape Requirements:</strong></p>\n<ul>\n      <li>Input/output shapes often must be known ahead of time.</li>\n      <li>Makes dynamic batching and long-sequence handling difficult.</li>\n    </ul>\n<p><strong>Limited Ecosystem:</strong></p>\n<ul>\n      <li>XLA and TensorFlow/JAX integration are essential.</li>\n      <li>Less flexibility than PyTorch or ONNX-based deployments.</li>\n    </ul>",
      "contentMarkdown": "#### Key Strengths\n\n*   Extremely efficient for matrix operations\n*   Suitable for large-batch inference and training\n\n#### Watch Out for\n\n1.  **Poor Fit for Decoding:**\n    \n    *   TPUs struggle with dynamic token-wise decoding due to fixed compute graph.\n    *   May need to run decode loop on CPU or use alternative methods (e.g., GSPMD)\n2.  **Static Shape Requirements:**\n    \n    *   Input/output shapes often must be known ahead of time.\n    *   Makes dynamic batching and long-sequence handling difficult.\n3.  **Limited Ecosystem:**\n    \n    *   XLA and TensorFlow/JAX integration are essential.\n    *   Less flexibility than PyTorch or ONNX-based deployments.\n\n**Poor Fit for Decoding:**\n\n*   TPUs struggle with dynamic token-wise decoding due to fixed compute graph.\n*   May need to run decode loop on CPU or use alternative methods (e.g., GSPMD)\n\n**Static Shape Requirements:**\n\n*   Input/output shapes often must be known ahead of time.\n*   Makes dynamic batching and long-sequence handling difficult.\n\n**Limited Ecosystem:**\n\n*   XLA and TensorFlow/JAX integration are essential.\n*   Less flexibility than PyTorch or ONNX-based deployments.",
      "order": 19,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 161,
        "contentLength": 1593
      },
      "nextCards": [
        "ai-on-device-transformers-npu-deployment-considerations-edgesoc-devices-20",
        "ai-on-device-transformers-summary-table-key-pitfalls-by-hardware-21"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#tpu-deployment-considerations",
      "scrapedAt": "2025-12-28T11:56:11.408Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-npu-deployment-considerations-edgesoc-devices-20",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Practical Considerations and Pitfalls When Deploying Transformers on CPU, GPU, and NPUs",
      "title": "NPU Deployment Considerations (Edge/SoC Devices)",
      "subtitle": "Practical Considerations and Pitfalls When Deploying Transformers on CPU, GPU, and NPUs",
      "contentHtml": "<h4 id=\"key-strengths-3\">Key Strengths</h4>\n<ul>\n  <li>Extremely efficient for low-power inference</li>\n  <li>On-chip memory reduces latency</li>\n</ul>\n<h4 id=\"watch-out-for-3\">Watch Out for</h4>\n<ol>\n  <li>\n    <p><strong>Operator Support Limitations:</strong></p>\n\n    <ul>\n      <li>Custom or unsupported operations must fall back to CPU, drastically hurting performance.</li>\n      <li>Stay within the vendor’s supported op-set (e.g., for Apple ANE, Qualcomm Hexagon, etc.).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Model Size Constraints:</strong></p>\n\n    <ul>\n      <li>Total model size must fit within NPU’s SRAM or limited DRAM window.</li>\n      <li>Quantization (<code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">int4</code>) is non-negotiable for many NPUs.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Toolchain Lock-in:</strong></p>\n\n    <ul>\n      <li>Conversion pipelines (e.g., PyTorch <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-364\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-365\"><span class=\"mo\" id=\"MathJax-Span-366\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">\\rightarrow</script> ONNX <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-367\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-368\"><span class=\"mo\" id=\"MathJax-Span-369\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">\\rightarrow</script> CoreML) must be strictly validated.</li>\n      <li>Vendor SDKs may lack transparency and debugging tools.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>No Runtime Reallocation:</strong></p>\n\n    <ul>\n      <li>Static allocation of KV cache, input tensors, and batch sizes is often required.</li>\n      <li>This can lead to fragmentation or wasted space.</li>\n    </ul>\n  </li>\n</ol>\n<p><strong>Operator Support Limitations:</strong></p>\n<ul>\n      <li>Custom or unsupported operations must fall back to CPU, drastically hurting performance.</li>\n      <li>Stay within the vendor’s supported op-set (e.g., for Apple ANE, Qualcomm Hexagon, etc.).</li>\n    </ul>\n<p><strong>Model Size Constraints:</strong></p>\n<ul>\n      <li>Total model size must fit within NPU’s SRAM or limited DRAM window.</li>\n      <li>Quantization (<code class=\"language-plaintext highlighter-rouge\">int8</code> or <code class=\"language-plaintext highlighter-rouge\">int4</code>) is non-negotiable for many NPUs.</li>\n    </ul>\n<p><strong>Toolchain Lock-in:</strong></p>\n<ul>\n      <li>Conversion pipelines (e.g., PyTorch <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-364\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-365\"><span class=\"mo\" id=\"MathJax-Span-366\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">\\rightarrow</script> ONNX <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-367\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-368\"><span class=\"mo\" id=\"MathJax-Span-369\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">\\rightarrow</script> CoreML) must be strictly validated.</li>\n      <li>Vendor SDKs may lack transparency and debugging tools.</li>\n    </ul>\n<p><strong>No Runtime Reallocation:</strong></p>\n<ul>\n      <li>Static allocation of KV cache, input tensors, and batch sizes is often required.</li>\n      <li>This can lead to fragmentation or wasted space.</li>\n    </ul>",
      "contentMarkdown": "#### Key Strengths\n\n*   Extremely efficient for low-power inference\n*   On-chip memory reduces latency\n\n#### Watch Out for\n\n1.  **Operator Support Limitations:**\n    \n    *   Custom or unsupported operations must fall back to CPU, drastically hurting performance.\n    *   Stay within the vendor’s supported op-set (e.g., for Apple ANE, Qualcomm Hexagon, etc.).\n2.  **Model Size Constraints:**\n    \n    *   Total model size must fit within NPU’s SRAM or limited DRAM window.\n    *   Quantization (`int8` or `int4`) is non-negotiable for many NPUs.\n3.  **Toolchain Lock-in:**\n    \n    *   Conversion pipelines (e.g., PyTorch →→\\\\rightarrow ONNX →→\\\\rightarrow CoreML) must be strictly validated.\n    *   Vendor SDKs may lack transparency and debugging tools.\n4.  **No Runtime Reallocation:**\n    \n    *   Static allocation of KV cache, input tensors, and batch sizes is often required.\n    *   This can lead to fragmentation or wasted space.\n\n**Operator Support Limitations:**\n\n*   Custom or unsupported operations must fall back to CPU, drastically hurting performance.\n*   Stay within the vendor’s supported op-set (e.g., for Apple ANE, Qualcomm Hexagon, etc.).\n\n**Model Size Constraints:**\n\n*   Total model size must fit within NPU’s SRAM or limited DRAM window.\n*   Quantization (`int8` or `int4`) is non-negotiable for many NPUs.\n\n**Toolchain Lock-in:**\n\n*   Conversion pipelines (e.g., PyTorch →→\\\\rightarrow ONNX →→\\\\rightarrow CoreML) must be strictly validated.\n*   Vendor SDKs may lack transparency and debugging tools.\n\n**No Runtime Reallocation:**\n\n*   Static allocation of KV cache, input tensors, and batch sizes is often required.\n*   This can lead to fragmentation or wasted space.",
      "order": 20,
      "orderInChapter": 4,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 234,
        "contentLength": 7398
      },
      "nextCards": [
        "ai-on-device-transformers-summary-table-key-pitfalls-by-hardware-21",
        "ai-on-device-transformers-tokenizer-and-vocabulary-size-22"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#npu-deployment-considerations-(edge/soc-devices)",
      "scrapedAt": "2025-12-28T11:56:11.408Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-summary-table-key-pitfalls-by-hardware-21",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Practical Considerations and Pitfalls When Deploying Transformers on CPU, GPU, and NPUs",
      "title": "Summary Table: Key Pitfalls by Hardware",
      "subtitle": "Practical Considerations and Pitfalls When Deploying Transformers on CPU, GPU, and NPUs",
      "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Platform</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Key Pitfall</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Best Practices</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">CPU</td>\n<td class=\"tg-tleft-valign-first\">Memory-bound decode + small caches</td>\n<td class=\"tg-tleft-valign-second\">Use quantization, cache-friendly layouts</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">GPU</td>\n<td class=\"tg-tleft-valign-first\">Underutilized during decoding</td>\n<td class=\"tg-tleft-valign-second\">Use batching, speculative decoding, fused kernels</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">TPU</td>\n<td class=\"tg-tleft-valign-first\">Poor step-by-step token generation</td>\n<td class=\"tg-tleft-valign-second\">Offload decode to CPU or use static batching</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">NPU</td>\n<td class=\"tg-tleft-valign-first\">Operator and memory constraints</td>\n<td class=\"tg-tleft-valign-second\">Quantize, simplify architecture, preallocate buffers</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Platform</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Key Pitfall</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Best Practices</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">CPU</td>\n<td class=\"tg-tleft-valign-first\">Memory-bound decode + small caches</td>\n<td class=\"tg-tleft-valign-second\">Use quantization, cache-friendly layouts</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">GPU</td>\n<td class=\"tg-tleft-valign-first\">Underutilized during decoding</td>\n<td class=\"tg-tleft-valign-second\">Use batching, speculative decoding, fused kernels</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">TPU</td>\n<td class=\"tg-tleft-valign-first\">Poor step-by-step token generation</td>\n<td class=\"tg-tleft-valign-second\">Offload decode to CPU or use static batching</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">NPU</td>\n<td class=\"tg-tleft-valign-first\">Operator and memory constraints</td>\n<td class=\"tg-tleft-valign-second\">Quantize, simplify architecture, preallocate buffers</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "**Platform**\n\n**Key Pitfall**\n\n**Best Practices**\n\nCPU\n\nMemory-bound decode + small caches\n\nUse quantization, cache-friendly layouts\n\nGPU\n\nUnderutilized during decoding\n\nUse batching, speculative decoding, fused kernels\n\nTPU\n\nPoor step-by-step token generation\n\nOffload decode to CPU or use static batching\n\nNPU\n\nOperator and memory constraints\n\nQuantize, simplify architecture, preallocate buffers\n\n**Platform**\n\n**Key Pitfall**\n\n**Best Practices**\n\nCPU\n\nMemory-bound decode + small caches\n\nUse quantization, cache-friendly layouts\n\nGPU\n\nUnderutilized during decoding\n\nUse batching, speculative decoding, fused kernels\n\nTPU\n\nPoor step-by-step token generation\n\nOffload decode to CPU or use static batching\n\nNPU\n\nOperator and memory constraints\n\nQuantize, simplify architecture, preallocate buffers",
      "order": 21,
      "orderInChapter": 5,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "ondevice ai"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 96,
        "contentLength": 2301
      },
      "nextCards": [
        "ai-on-device-transformers-tokenizer-and-vocabulary-size-22",
        "ai-on-device-transformers-modelembedding-dimension-23"
      ],
      "relatedCards": [
        "ai-model-compression-choosing-the-right-quantization-approach-16",
        "ai-model-compression-performance-results-17",
        "ai-model-compression-distillation-modes-24",
        "ai-model-compression-reverse-distillation-28",
        "ai-model-compression-limitations-and-challenges-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#summary-table:-key-pitfalls-by-hardware",
      "scrapedAt": "2025-12-28T11:56:11.408Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-tokenizer-and-vocabulary-size-22",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Parameter Choices and Their Runtime Implications in Transformer Architectures",
      "title": "Tokenizer and Vocabulary Size",
      "subtitle": "Parameter Choices and Their Runtime Implications in Transformer Architectures",
      "contentHtml": "<ul>\n  <li>A tokenizer converts raw text into discrete tokens that the model can process. The vocabulary size, denoted as <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-386\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-387\"><span class=\"mi\" id=\"MathJax-Span-388\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">V</script>, represents the number of unique tokens the model can recognize. This parameter has several direct and indirect effects on transformer runtime.</li>\n</ul>\n<h4 id=\"impact-on-embedding-and-output-layers\">Impact on Embedding and Output Layers</h4>\n<ul>\n  <li>\n    <p>The embedding layer maps each token ID to a vector of size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-389\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-390\"><span class=\"msubsup\" id=\"MathJax-Span-391\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-392\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-393\"><span class=\"mrow\" id=\"MathJax-Span-394\"><span class=\"mtext\" id=\"MathJax-Span-395\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">d_{\\text{model}}</script> (the embedding dimension). The computational and memory cost of this layer is:</p>\n\n    <ul>\n      <li><strong>Parameter count:</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>embed</mtext></mrow></msub><mo>=</mo><mi>V</mi><mo>&amp;#x00D7;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-396\" style=\"width: 9.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.971em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1007.97em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-397\"><span class=\"msubsup\" id=\"MathJax-Span-398\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-399\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-400\"><span class=\"mrow\" id=\"MathJax-Span-401\"><span class=\"mtext\" id=\"MathJax-Span-402\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">embed</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-403\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-404\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-405\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-406\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-407\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-408\"><span class=\"mrow\" id=\"MathJax-Span-409\"><span class=\"mtext\" id=\"MathJax-Span-410\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>embed</mtext></mrow></msub><mo>=</mo><mi>V</mi><mo>×</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">P_{\\text{embed}} = V \\times d_{\\text{model}}</script></li>\n      <li><strong>Memory footprint:</strong> Proportional to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>embed</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-411\" style=\"width: 3.076em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.55em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-412\"><span class=\"msubsup\" id=\"MathJax-Span-413\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-414\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-415\"><span class=\"mrow\" id=\"MathJax-Span-416\"><span class=\"mtext\" id=\"MathJax-Span-417\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">embed</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>embed</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-41\">P_{\\text{embed}}</script>, typically stored in float16 or int8.</li>\n      <li><strong>Runtime:</strong> At inference, the lookup cost is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-418\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.83em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-419\"><span class=\"mi\" id=\"MathJax-Span-420\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-421\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-422\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-423\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-42\">O(1)</script> per token, but the softmax in the output layer is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>V</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-424\" style=\"width: 2.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.14em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-425\"><span class=\"mi\" id=\"MathJax-Span-426\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-427\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-428\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-429\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-43\">O(V)</script> per token.</li>\n    </ul>\n  </li>\n  <li>\n    <p>The largest runtime penalty from a bigger vocabulary comes from the output projection and softmax. The final logits are computed via:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>z</mi><mi>t</mi></msub><mo>=</mo><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>out</mtext></mrow></msub><msub><mi>h</mi><mi>t</mi></msub><mo>+</mo><mi>b</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-430\" style=\"width: 7.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.04em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-431\"><span class=\"msubsup\" id=\"MathJax-Span-432\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.273em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-433\" style=\"font-family: STIXGeneral-Italic;\">z</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-434\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-435\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-436\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-437\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-438\"><span class=\"mrow\" id=\"MathJax-Span-439\"><span class=\"mtext\" id=\"MathJax-Span-440\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">out</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-441\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-442\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-443\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-444\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-445\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">b</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>z</mi><mi>t</mi></msub><mo>=</mo><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>out</mtext></mrow></msub><msub><mi>h</mi><mi>t</mi></msub><mo>+</mo><mi>b</mi></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-44\">z_t = W_{\\text{out}} h_t + b</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>out</mtext></mrow></msub><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>V</mi><mo>&amp;#x00D7;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-446\" style=\"width: 7.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.461em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1006.46em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-447\"><span class=\"msubsup\" id=\"MathJax-Span-448\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-449\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-450\"><span class=\"mrow\" id=\"MathJax-Span-451\"><span class=\"mtext\" id=\"MathJax-Span-452\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">out</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-453\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-454\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.44em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-455\"><span class=\"mrow\" id=\"MathJax-Span-456\"><span class=\"mi\" id=\"MathJax-Span-457\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-458\"><span class=\"mrow\" id=\"MathJax-Span-459\"><span class=\"mi\" id=\"MathJax-Span-460\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-461\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-462\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-463\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-464\"><span class=\"mrow\" id=\"MathJax-Span-465\"><span class=\"mtext\" id=\"MathJax-Span-466\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>out</mtext></mrow></msub><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>V</mi><mo>×</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-45\">W_{\\text{out}} \\in \\mathbb{R}^{V \\times d_{\\text{model}}}</script>.</li>\n    </ul>\n  </li>\n  <li>\n    <p>This operation costs <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>V</mi><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-467\" style=\"width: 6.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1005.42em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-468\"><span class=\"mi\" id=\"MathJax-Span-469\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-470\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-471\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-472\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-473\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-474\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-475\"><span class=\"mrow\" id=\"MathJax-Span-476\"><span class=\"mtext\" id=\"MathJax-Span-477\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-478\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>V</mi><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">O(V \\cdot d_{\\text{model}})</script> per decoding step. For large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-479\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-480\"><span class=\"mi\" id=\"MathJax-Span-481\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">V</script> (e.g., 50k+), this can dominate single-token decode latency on CPUs or NPUs, especially when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-482\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-483\"><span class=\"mi\" id=\"MathJax-Span-484\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">V</script> exceeds cache size.</p>\n  </li>\n</ul>\n<p>The embedding layer maps each token ID to a vector of size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-389\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-390\"><span class=\"msubsup\" id=\"MathJax-Span-391\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-392\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-393\"><span class=\"mrow\" id=\"MathJax-Span-394\"><span class=\"mtext\" id=\"MathJax-Span-395\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">d_{\\text{model}}</script> (the embedding dimension). The computational and memory cost of this layer is:</p>\n<ul>\n      <li><strong>Parameter count:</strong> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>embed</mtext></mrow></msub><mo>=</mo><mi>V</mi><mo>&amp;#x00D7;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-396\" style=\"width: 9.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.971em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1007.97em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-397\"><span class=\"msubsup\" id=\"MathJax-Span-398\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-399\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-400\"><span class=\"mrow\" id=\"MathJax-Span-401\"><span class=\"mtext\" id=\"MathJax-Span-402\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">embed</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-403\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-404\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-405\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-406\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-407\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-408\"><span class=\"mrow\" id=\"MathJax-Span-409\"><span class=\"mtext\" id=\"MathJax-Span-410\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>embed</mtext></mrow></msub><mo>=</mo><mi>V</mi><mo>×</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">P_{\\text{embed}} = V \\times d_{\\text{model}}</script></li>\n      <li><strong>Memory footprint:</strong> Proportional to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>embed</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-411\" style=\"width: 3.076em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.55em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-412\"><span class=\"msubsup\" id=\"MathJax-Span-413\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-414\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-415\"><span class=\"mrow\" id=\"MathJax-Span-416\"><span class=\"mtext\" id=\"MathJax-Span-417\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">embed</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>embed</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-41\">P_{\\text{embed}}</script>, typically stored in float16 or int8.</li>\n      <li><strong>Runtime:</strong> At inference, the lookup cost is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-418\" style=\"width: 2.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.83em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-419\"><span class=\"mi\" id=\"MathJax-Span-420\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-421\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mn\" id=\"MathJax-Span-422\" style=\"font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-423\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-42\">O(1)</script> per token, but the softmax in the output layer is <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>V</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-424\" style=\"width: 2.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.14em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-425\"><span class=\"mi\" id=\"MathJax-Span-426\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-427\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-428\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-429\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>V</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-43\">O(V)</script> per token.</li>\n    </ul>\n<p>The largest runtime penalty from a bigger vocabulary comes from the output projection and softmax. The final logits are computed via:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>out</mtext></mrow></msub><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>V</mi><mo>&amp;#x00D7;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-446\" style=\"width: 7.763em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.461em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1006.46em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-447\"><span class=\"msubsup\" id=\"MathJax-Span-448\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-449\" style=\"font-family: STIXGeneral-Italic;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-450\"><span class=\"mrow\" id=\"MathJax-Span-451\"><span class=\"mtext\" id=\"MathJax-Span-452\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">out</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-453\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-454\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.44em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-455\"><span class=\"mrow\" id=\"MathJax-Span-456\"><span class=\"mi\" id=\"MathJax-Span-457\" style=\"font-family: STIXGeneral-Regular;\">ℝ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-458\"><span class=\"mrow\" id=\"MathJax-Span-459\"><span class=\"mi\" id=\"MathJax-Span-460\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-461\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-462\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-463\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-464\"><span class=\"mrow\" id=\"MathJax-Span-465\"><span class=\"mtext\" id=\"MathJax-Span-466\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>W</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>out</mtext></mrow></msub><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>V</mi><mo>×</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-45\">W_{\\text{out}} \\in \\mathbb{R}^{V \\times d_{\\text{model}}}</script>.</li>\n    </ul>\n<p>This operation costs <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>V</mi><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-467\" style=\"width: 6.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1005.42em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-468\"><span class=\"mi\" id=\"MathJax-Span-469\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-470\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-471\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-472\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-473\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-474\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-475\"><span class=\"mrow\" id=\"MathJax-Span-476\"><span class=\"mtext\" id=\"MathJax-Span-477\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-478\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>V</mi><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">O(V \\cdot d_{\\text{model}})</script> per decoding step. For large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-479\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-480\"><span class=\"mi\" id=\"MathJax-Span-481\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-47\">V</script> (e.g., 50k+), this can dominate single-token decode latency on CPUs or NPUs, especially when <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-482\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-483\"><span class=\"mi\" id=\"MathJax-Span-484\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">V</script> exceeds cache size.</p>\n<h4 id=\"tradeoffs-of-vocabulary-size\">Tradeoffs of Vocabulary Size</h4>\n<ul>\n  <li>\n    <p><strong>Larger <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-485\" style=\"width: 0.932em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.78em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-486\"><span class=\"mi\" id=\"MathJax-Span-487\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-49\">V</script></strong></p>\n\n    <ul>\n      <li><strong>Pros:</strong> Fewer tokens per sentence (shorter sequences), potentially better language coverage.</li>\n      <li><strong>Cons:</strong> Larger embedding and output layers, higher per-step cost, more memory bandwidth pressure.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Smaller <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-488\" style=\"width: 0.932em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.78em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-489\"><span class=\"mi\" id=\"MathJax-Span-490\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">V</script></strong></p>\n\n    <ul>\n      <li><strong>Pros:</strong> Smaller parameter count, faster output projection, better fit for on-device constraints.</li>\n      <li><strong>Cons:</strong> Longer sequences (more decoding steps), higher total attention cost <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-491\" style=\"width: 4.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1003.8em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-492\"><span class=\"mi\" id=\"MathJax-Span-493\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-494\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-495\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-496\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-497\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-498\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-499\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-500\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-51\">O(n^2 \\cdot d)</script> in prefill.</li>\n    </ul>\n  </li>\n  <li>\n    <p>There is a non-trivial sweet spot where the cost of the output softmax balances with the cost of processing longer sequences.</p>\n  </li>\n</ul>\n<p><strong>Larger <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-485\" style=\"width: 0.932em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.78em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-486\"><span class=\"mi\" id=\"MathJax-Span-487\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-49\">V</script></strong></p>\n<ul>\n      <li><strong>Pros:</strong> Fewer tokens per sentence (shorter sequences), potentially better language coverage.</li>\n      <li><strong>Cons:</strong> Larger embedding and output layers, higher per-step cost, more memory bandwidth pressure.</li>\n    </ul>\n<p><strong>Smaller <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-488\" style=\"width: 0.932em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.78em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-489\"><span class=\"mi\" id=\"MathJax-Span-490\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">V</script></strong></p>\n<ul>\n      <li><strong>Pros:</strong> Smaller parameter count, faster output projection, better fit for on-device constraints.</li>\n      <li><strong>Cons:</strong> Longer sequences (more decoding steps), higher total attention cost <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>&amp;#x22C5;</mo><mi>d</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-491\" style=\"width: 4.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1003.8em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-492\"><span class=\"mi\" id=\"MathJax-Span-493\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-494\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-495\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-496\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-497\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-498\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-499\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-500\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>⋅</mo><mi>d</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-51\">O(n^2 \\cdot d)</script> in prefill.</li>\n    </ul>\n<p>There is a non-trivial sweet spot where the cost of the output softmax balances with the cost of processing longer sequences.</p>\n<h4 id=\"runtime-context-temperature-vs-vocabulary-size\">Runtime Context: Temperature vs. Vocabulary Size</h4>\n<ul>\n  <li>\n    <p>Note that temperature (as it relates to decoder sampling) affects randomness and diversity of outputs, but <strong>does not impact FLOPs, memory bandwidth, or parameter count</strong>. In contrast, vocabulary size directly changes the computational graph and memory access pattern at every generation step.</p>\n  </li>\n  <li>\n    <p>So in terms of runtime importance:</p>\n\n    <ul>\n      <li><strong>Vocabulary size is far more significant than temperature</strong> for performance.</li>\n      <li>Temperature can be tuned freely without runtime cost, but vocabulary size needs to be fixed at training time.</li>\n    </ul>\n  </li>\n</ul>\n<p>Note that temperature (as it relates to decoder sampling) affects randomness and diversity of outputs, but <strong>does not impact FLOPs, memory bandwidth, or parameter count</strong>. In contrast, vocabulary size directly changes the computational graph and memory access pattern at every generation step.</p>\n<p>So in terms of runtime importance:</p>\n<ul>\n      <li><strong>Vocabulary size is far more significant than temperature</strong> for performance.</li>\n      <li>Temperature can be tuned freely without runtime cost, but vocabulary size needs to be fixed at training time.</li>\n    </ul>\n<h4 id=\"server-vs-on-device-considerations\">Server vs. On-Device Considerations</h4>\n<ul>\n  <li>\n    <p><strong>Server-side (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference)</strong>: Large vocabularies are more tolerable since GPUs offer high-bandwidth memory (HBM) and Tensor Cores, which accelerate matrix-vector multiplies in the output layer. Softmax can be fused and batched.</p>\n  </li>\n  <li>\n    <p><strong>On-device (<code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, LidarTLM, GGML, MediaPipe + TFLite, ONNX Runtime Mobile)</strong>: Vocabulary size is a primary bottleneck, especially for NPUs or CPUs without fast GEMM acceleration for large matrices. LidarTLM, optimized for edge LiDAR transformers, often uses reduced vocabularies or subword schemes to minimize per-step cost.</p>\n  </li>\n</ul>\n<p><strong>Server-side (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference)</strong>: Large vocabularies are more tolerable since GPUs offer high-bandwidth memory (HBM) and Tensor Cores, which accelerate matrix-vector multiplies in the output layer. Softmax can be fused and batched.</p>\n<p><strong>On-device (<code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, LidarTLM, GGML, MediaPipe + TFLite, ONNX Runtime Mobile)</strong>: Vocabulary size is a primary bottleneck, especially for NPUs or CPUs without fast GEMM acceleration for large matrices. LidarTLM, optimized for edge LiDAR transformers, often uses reduced vocabularies or subword schemes to minimize per-step cost.</p>\n<h5 id=\"runtime-specific-notes\">Runtime-Specific Notes</h5>\n<ul>\n  <li>\n    <p><strong>NanoGPT</strong>: Tends toward larger vocab (10–50k), fine for servers but not optimal for low-latency mobile inference.</p>\n  </li>\n  <li>\n    <p><strong>TensorRT-LLM</strong>: Optimized for NVIDIA GPUs with fused kernels and mixed-precision GEMMs, allowing large vocab sizes with minimal per-token penalty, especially when batching is used.</p>\n  </li>\n  <li>\n    <p><strong>vLLM</strong>: Uses advanced caching and continuous batching strategies on servers, mitigating large-vocab output projection costs, but still sees latency increases at very high <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-501\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-502\"><span class=\"mi\" id=\"MathJax-Span-503\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">V</script>.</p>\n  </li>\n  <li>\n    <p><strong><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code></strong>: Often run quantized on CPUs; vocab size affects KV cache read/write less than it affects output projection latency. Implements vocabulary lookups and output projection via cache-aware tiled GEMM in GGML to minimize L3 cache misses.</p>\n  </li>\n  <li>\n    <p><strong>LidarTLM</strong>: Frequently aggressively pruned vocab for ultra-low-power environments.</p>\n  </li>\n  <li>\n    <p><strong>MediaPipe + TFLite LLMs</strong>: For mobile/embedded devices, vocab size is often cut down to 4k–8k to meet NPU SRAM limits, with aggressive quantization applied to the embedding and projection layers.</p>\n  </li>\n  <li>\n    <p><strong>ONNX Runtime (CPU/NPU)</strong>: On CPU, large vocabularies can cause cache misses in the output projection; on NPU, unsupported large matrix ops may be forced to fall back to CPU, severely impacting latency.</p>\n  </li>\n</ul>\n<p><strong>NanoGPT</strong>: Tends toward larger vocab (10–50k), fine for servers but not optimal for low-latency mobile inference.</p>\n<p><strong>TensorRT-LLM</strong>: Optimized for NVIDIA GPUs with fused kernels and mixed-precision GEMMs, allowing large vocab sizes with minimal per-token penalty, especially when batching is used.</p>\n<p><strong>vLLM</strong>: Uses advanced caching and continuous batching strategies on servers, mitigating large-vocab output projection costs, but still sees latency increases at very high <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-501\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-502\"><span class=\"mi\" id=\"MathJax-Span-503\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">V</script>.</p>\n<p><strong><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code></strong>: Often run quantized on CPUs; vocab size affects KV cache read/write less than it affects output projection latency. Implements vocabulary lookups and output projection via cache-aware tiled GEMM in GGML to minimize L3 cache misses.</p>\n<p><strong>LidarTLM</strong>: Frequently aggressively pruned vocab for ultra-low-power environments.</p>\n<p><strong>MediaPipe + TFLite LLMs</strong>: For mobile/embedded devices, vocab size is often cut down to 4k–8k to meet NPU SRAM limits, with aggressive quantization applied to the embedding and projection layers.</p>\n<p><strong>ONNX Runtime (CPU/NPU)</strong>: On CPU, large vocabularies can cause cache misses in the output projection; on NPU, unsupported large matrix ops may be forced to fall back to CPU, severely impacting latency.</p>",
      "contentMarkdown": "*   A tokenizer converts raw text into discrete tokens that the model can process. The vocabulary size, denoted as VVV, represents the number of unique tokens the model can recognize. This parameter has several direct and indirect effects on transformer runtime.\n\n#### Impact on Embedding and Output Layers\n\n*   The embedding layer maps each token ID to a vector of size dmodeldmodeld\\_{\\\\text{model}} (the embedding dimension). The computational and memory cost of this layer is:\n    \n    *   **Parameter count:** Pembed\\=V×dmodelPembed\\=V×dmodelP\\_{\\\\text{embed}} = V \\\\times d\\_{\\\\text{model}}\n    *   **Memory footprint:** Proportional to PembedPembedP\\_{\\\\text{embed}}, typically stored in float16 or int8.\n    *   **Runtime:** At inference, the lookup cost is O(1)O(1)O(1) per token, but the softmax in the output layer is O(V)O(V)O(V) per token.\n*   The largest runtime penalty from a bigger vocabulary comes from the output projection and softmax. The final logits are computed via:\n    \n    zt\\=Woutht+bzt\\=Woutht+b\n    \n    z\\_t = W\\_{\\\\text{out}} h\\_t + b\n    *   where Wout∈ℝV×dmodelWout∈RV×dmodelW\\_{\\\\text{out}} \\\\in \\\\mathbb{R}^{V \\\\times d\\_{\\\\text{model}}}.\n*   This operation costs O(V⋅dmodel)O(V⋅dmodel)O(V \\\\cdot d\\_{\\\\text{model}}) per decoding step. For large VVV (e.g., 50k+), this can dominate single-token decode latency on CPUs or NPUs, especially when VVV exceeds cache size.\n    \n\nThe embedding layer maps each token ID to a vector of size dmodeldmodeld\\_{\\\\text{model}} (the embedding dimension). The computational and memory cost of this layer is:\n\n*   **Parameter count:** Pembed\\=V×dmodelPembed\\=V×dmodelP\\_{\\\\text{embed}} = V \\\\times d\\_{\\\\text{model}}\n*   **Memory footprint:** Proportional to PembedPembedP\\_{\\\\text{embed}}, typically stored in float16 or int8.\n*   **Runtime:** At inference, the lookup cost is O(1)O(1)O(1) per token, but the softmax in the output layer is O(V)O(V)O(V) per token.\n\nThe largest runtime penalty from a bigger vocabulary comes from the output projection and softmax. The final logits are computed via:\n\n*   where Wout∈ℝV×dmodelWout∈RV×dmodelW\\_{\\\\text{out}} \\\\in \\\\mathbb{R}^{V \\\\times d\\_{\\\\text{model}}}.\n\nThis operation costs O(V⋅dmodel)O(V⋅dmodel)O(V \\\\cdot d\\_{\\\\text{model}}) per decoding step. For large VVV (e.g., 50k+), this can dominate single-token decode latency on CPUs or NPUs, especially when VVV exceeds cache size.\n\n#### Tradeoffs of Vocabulary Size\n\n*   **Larger VVV**\n    \n    *   **Pros:** Fewer tokens per sentence (shorter sequences), potentially better language coverage.\n    *   **Cons:** Larger embedding and output layers, higher per-step cost, more memory bandwidth pressure.\n*   **Smaller VVV**\n    \n    *   **Pros:** Smaller parameter count, faster output projection, better fit for on-device constraints.\n    *   **Cons:** Longer sequences (more decoding steps), higher total attention cost O(n2⋅d)O(n2⋅d)O(n^2 \\\\cdot d) in prefill.\n*   There is a non-trivial sweet spot where the cost of the output softmax balances with the cost of processing longer sequences.\n    \n\n**Larger VVV**\n\n*   **Pros:** Fewer tokens per sentence (shorter sequences), potentially better language coverage.\n*   **Cons:** Larger embedding and output layers, higher per-step cost, more memory bandwidth pressure.\n\n**Smaller VVV**\n\n*   **Pros:** Smaller parameter count, faster output projection, better fit for on-device constraints.\n*   **Cons:** Longer sequences (more decoding steps), higher total attention cost O(n2⋅d)O(n2⋅d)O(n^2 \\\\cdot d) in prefill.\n\nThere is a non-trivial sweet spot where the cost of the output softmax balances with the cost of processing longer sequences.\n\n#### Runtime Context: Temperature vs. Vocabulary Size\n\n*   Note that temperature (as it relates to decoder sampling) affects randomness and diversity of outputs, but **does not impact FLOPs, memory bandwidth, or parameter count**. In contrast, vocabulary size directly changes the computational graph and memory access pattern at every generation step.\n    \n*   So in terms of runtime importance:\n    \n    *   **Vocabulary size is far more significant than temperature** for performance.\n    *   Temperature can be tuned freely without runtime cost, but vocabulary size needs to be fixed at training time.\n\nNote that temperature (as it relates to decoder sampling) affects randomness and diversity of outputs, but **does not impact FLOPs, memory bandwidth, or parameter count**. In contrast, vocabulary size directly changes the computational graph and memory access pattern at every generation step.\n\nSo in terms of runtime importance:\n\n*   **Vocabulary size is far more significant than temperature** for performance.\n*   Temperature can be tuned freely without runtime cost, but vocabulary size needs to be fixed at training time.\n\n#### Server vs. On-Device Considerations\n\n*   **Server-side (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference)**: Large vocabularies are more tolerable since GPUs offer high-bandwidth memory (HBM) and Tensor Cores, which accelerate matrix-vector multiplies in the output layer. Softmax can be fused and batched.\n    \n*   **On-device (`llama.cpp`, LidarTLM, GGML, MediaPipe + TFLite, ONNX Runtime Mobile)**: Vocabulary size is a primary bottleneck, especially for NPUs or CPUs without fast GEMM acceleration for large matrices. LidarTLM, optimized for edge LiDAR transformers, often uses reduced vocabularies or subword schemes to minimize per-step cost.\n    \n\n**Server-side (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference)**: Large vocabularies are more tolerable since GPUs offer high-bandwidth memory (HBM) and Tensor Cores, which accelerate matrix-vector multiplies in the output layer. Softmax can be fused and batched.\n\n**On-device (`llama.cpp`, LidarTLM, GGML, MediaPipe + TFLite, ONNX Runtime Mobile)**: Vocabulary size is a primary bottleneck, especially for NPUs or CPUs without fast GEMM acceleration for large matrices. LidarTLM, optimized for edge LiDAR transformers, often uses reduced vocabularies or subword schemes to minimize per-step cost.\n\n##### Runtime-Specific Notes\n\n*   **NanoGPT**: Tends toward larger vocab (10–50k), fine for servers but not optimal for low-latency mobile inference.\n    \n*   **TensorRT-LLM**: Optimized for NVIDIA GPUs with fused kernels and mixed-precision GEMMs, allowing large vocab sizes with minimal per-token penalty, especially when batching is used.\n    \n*   **vLLM**: Uses advanced caching and continuous batching strategies on servers, mitigating large-vocab output projection costs, but still sees latency increases at very high VVV.\n    \n*   **`llama.cpp`**: Often run quantized on CPUs; vocab size affects KV cache read/write less than it affects output projection latency. Implements vocabulary lookups and output projection via cache-aware tiled GEMM in GGML to minimize L3 cache misses.\n    \n*   **LidarTLM**: Frequently aggressively pruned vocab for ultra-low-power environments.\n    \n*   **MediaPipe + TFLite LLMs**: For mobile/embedded devices, vocab size is often cut down to 4k–8k to meet NPU SRAM limits, with aggressive quantization applied to the embedding and projection layers.\n    \n*   **ONNX Runtime (CPU/NPU)**: On CPU, large vocabularies can cause cache misses in the output projection; on NPU, unsupported large matrix ops may be forced to fall back to CPU, severely impacting latency.\n    \n\n**NanoGPT**: Tends toward larger vocab (10–50k), fine for servers but not optimal for low-latency mobile inference.\n\n**TensorRT-LLM**: Optimized for NVIDIA GPUs with fused kernels and mixed-precision GEMMs, allowing large vocab sizes with minimal per-token penalty, especially when batching is used.\n\n**vLLM**: Uses advanced caching and continuous batching strategies on servers, mitigating large-vocab output projection costs, but still sees latency increases at very high VVV.\n\n**`llama.cpp`**: Often run quantized on CPUs; vocab size affects KV cache read/write less than it affects output projection latency. Implements vocabulary lookups and output projection via cache-aware tiled GEMM in GGML to minimize L3 cache misses.\n\n**LidarTLM**: Frequently aggressively pruned vocab for ultra-low-power environments.\n\n**MediaPipe + TFLite LLMs**: For mobile/embedded devices, vocab size is often cut down to 4k–8k to meet NPU SRAM limits, with aggressive quantization applied to the embedding and projection layers.\n\n**ONNX Runtime (CPU/NPU)**: On CPU, large vocabularies can cause cache misses in the output projection; on NPU, unsupported large matrix ops may be forced to fall back to CPU, severely impacting latency.",
      "order": 22,
      "orderInChapter": 1,
      "difficulty": 4,
      "estimatedMinutes": 6,
      "tags": [
        "ondevice ai",
        "transformer",
        "attention",
        "embedding",
        "gpt",
        "llm"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 1154,
        "contentLength": 70745
      },
      "nextCards": [
        "ai-on-device-transformers-modelembedding-dimension-23",
        "ai-on-device-transformers-sequence-length-and-kv-cache-size-24"
      ],
      "relatedCards": [
        "ai-model-compression-multimodal-quantization-14",
        "ai-model-compression-modern-quantization-techniques-13",
        "ai-model-compression-popular-quantization-libraries-19",
        "ai-RAG-response-generation-synthesis-5",
        "ai-model-compression-mitigation-strategies-8"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#tokenizer-and-vocabulary-size",
      "scrapedAt": "2025-12-28T11:56:11.408Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-modelembedding-dimension-23",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Parameter Choices and Their Runtime Implications in Transformer Architectures",
      "title": "Model/Embedding Dimension",
      "subtitle": "Parameter Choices and Their Runtime Implications in Transformer Architectures",
      "contentHtml": "<ul>\n  <li>The model dimension (also called the “embedding dimension”), <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-504\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-505\"><span class=\"msubsup\" id=\"MathJax-Span-506\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-507\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-508\"><span class=\"mrow\" id=\"MathJax-Span-509\"><span class=\"mtext\" id=\"MathJax-Span-510\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">d_{\\text{model}}</script>, determines the dimensionality of token embeddings and hidden states across the transformer.</li>\n  <li>The model dimension has a direct, predictable effect on runtime cost. Increasing the model dimension even slightly can push an on-device model from real-time to unusably slow since it appears in nearly every computational term in the architecture.</li>\n</ul>\n<h4 id=\"where-the-model-dimension-shows-up\">Where the Model Dimension Shows up</h4>\n<ul>\n  <li><strong>Embedding Layer:</strong>\n    <ul>\n      <li>Parameter count:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>embed</mtext></mrow></msub><mo>=</mo><mi>V</mi><mo>&amp;#x00D7;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-511\" style=\"width: 9.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.971em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1007.97em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-512\"><span class=\"msubsup\" id=\"MathJax-Span-513\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-514\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-515\"><span class=\"mrow\" id=\"MathJax-Span-516\"><span class=\"mtext\" id=\"MathJax-Span-517\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">embed</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-518\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-519\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-520\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-521\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-522\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-523\"><span class=\"mrow\" id=\"MathJax-Span-524\"><span class=\"mtext\" id=\"MathJax-Span-525\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>embed</mtext></mrow></msub><mo>=</mo><mi>V</mi><mo>×</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-54\">P_{\\text{embed}} = V \\times d_{\\text{model}}</script>\n\n    <ul>\n      <li>Memory bandwidth scales linearly with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-526\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-527\"><span class=\"msubsup\" id=\"MathJax-Span-528\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-529\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-530\"><span class=\"mrow\" id=\"MathJax-Span-531\"><span class=\"mtext\" id=\"MathJax-Span-532\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-55\">d_{\\text{model}}</script> when loading embeddings.</li>\n    </ul>\n  </li>\n  <li><strong>Multi-Head Attention (MHA):</strong>\n    <ul>\n      <li>Each attention projection (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo>,</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-533\" style=\"width: 4.169em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.44em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.39em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-534\"><span class=\"mi\" id=\"MathJax-Span-535\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-536\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-537\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-538\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-539\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-540\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo>,</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">Q, K, V,</script> output) is a matrix multiply of shape <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo>&amp;#x00D7;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x00D7;</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>&amp;#x00D7;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>proj</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-541\" style=\"width: 13.857em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.513em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1011.46em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-542\"><span class=\"mo\" id=\"MathJax-Span-543\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-544\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-545\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-546\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-547\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-548\"><span class=\"mrow\" id=\"MathJax-Span-549\"><span class=\"mtext\" id=\"MathJax-Span-550\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-551\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-552\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mo\" id=\"MathJax-Span-553\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-554\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-555\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-556\"><span class=\"mrow\" id=\"MathJax-Span-557\"><span class=\"mtext\" id=\"MathJax-Span-558\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-559\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-560\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-561\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-562\"><span class=\"mrow\" id=\"MathJax-Span-563\"><span class=\"mtext\" id=\"MathJax-Span-564\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">proj</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-565\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>n</mi><mo>×</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo stretchy=\"false\">)</mo><mo>×</mo><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>×</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>proj</mtext></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">(n \\times d_{\\text{model}}) \\times (d_{\\text{model}} \\times d_{\\text{proj}})</script>.</li>\n      <li>FLOPs per layer for MHA (excluding softmax):</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>head</mtext></mrow></msub><mo>&amp;#x22C5;</mo><mi>h</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-566\" style=\"width: 15.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.971em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1012.92em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-567\"><span class=\"mi\" id=\"MathJax-Span-568\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-569\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-570\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-571\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-572\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-573\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-574\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-575\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-576\"><span class=\"mrow\" id=\"MathJax-Span-577\"><span class=\"mtext\" id=\"MathJax-Span-578\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">head</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-579\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-580\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">h</span><span class=\"mo\" id=\"MathJax-Span-581\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-582\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-583\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">O</span><span class=\"mo\" id=\"MathJax-Span-584\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-585\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-586\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-587\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-588\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-589\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.83em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-590\"><span class=\"mrow\" id=\"MathJax-Span-591\"><span class=\"mtext\" id=\"MathJax-Span-592\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-593\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>head</mtext></mrow></msub><mo>⋅</mo><mi>h</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>O</mi><mo stretchy=\"false\">(</mo><mi>n</mi><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-58\">O(n^2 \\cdot d_{\\text{head}} \\cdot h) + O(n \\cdot d_{\\text{model}}^2)</script>\n\n    <ul>\n      <li>Since <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-59-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>head</mtext></mrow></msub><mo>&amp;#x22C5;</mo><mi>h</mi><mo>=</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-594\" style=\"width: 8.336em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.93em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-595\"><span class=\"msubsup\" id=\"MathJax-Span-596\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-597\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-598\"><span class=\"mrow\" id=\"MathJax-Span-599\"><span class=\"mtext\" id=\"MathJax-Span-600\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">head</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-601\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-602\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">h</span><span class=\"mo\" id=\"MathJax-Span-603\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-604\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-605\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-606\"><span class=\"mrow\" id=\"MathJax-Span-607\"><span class=\"mtext\" id=\"MathJax-Span-608\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>head</mtext></mrow></msub><mo>⋅</mo><mi>h</mi><mo>=</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-59\">d_{\\text{head}} \\cdot h = d_{\\text{model}}</script>, the cost scales as <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-60-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-609\" style=\"width: 6.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1005.58em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-610\"><span class=\"mi\" id=\"MathJax-Span-611\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-612\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-613\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-614\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-615\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-616\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-617\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-618\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-619\"><span class=\"mrow\" id=\"MathJax-Span-620\"><span class=\"mtext\" id=\"MathJax-Span-621\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-622\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-60\">O(n^2 \\cdot d_{\\text{model}})</script> for attention scores and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-623\" style=\"width: 6.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.211em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1005.16em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-624\"><span class=\"mi\" id=\"MathJax-Span-625\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-626\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-627\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-628\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-629\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-630\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-631\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.83em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-632\"><span class=\"mrow\" id=\"MathJax-Span-633\"><span class=\"mtext\" id=\"MathJax-Span-634\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-635\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>n</mi><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">O(n \\cdot d_{\\text{model}}^2)</script> for projections.</li>\n    </ul>\n  </li>\n  <li><strong>Feedforward Network (FFN):</strong>\n    <ul>\n      <li>Uses intermediate size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-62-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>ff</mtext></mrow></msub><mo>&amp;#x2248;</mo><mn>4</mn><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-636\" style=\"width: 7.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.04em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-637\"><span class=\"msubsup\" id=\"MathJax-Span-638\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-639\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-640\"><span class=\"mrow\" id=\"MathJax-Span-641\"><span class=\"mtext\" id=\"MathJax-Span-642\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">ff</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-643\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mn\" id=\"MathJax-Span-644\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span><span class=\"mo\" id=\"MathJax-Span-645\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-646\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-647\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-648\"><span class=\"mrow\" id=\"MathJax-Span-649\"><span class=\"mtext\" id=\"MathJax-Span-650\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>ff</mtext></mrow></msub><mo>≈</mo><mn>4</mn><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-62\">d_{\\text{ff}} \\approx 4 \\cdot d_{\\text{model}}</script>.</li>\n      <li>Per layer FLOPs:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-63-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>ff</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2248;</mo><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-651\" style=\"width: 16.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.596em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1013.54em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-652\"><span class=\"mi\" id=\"MathJax-Span-653\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-654\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-655\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-656\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-657\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-658\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-659\"><span class=\"mrow\" id=\"MathJax-Span-660\"><span class=\"mtext\" id=\"MathJax-Span-661\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-662\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-663\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-664\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-665\"><span class=\"mrow\" id=\"MathJax-Span-666\"><span class=\"mtext\" id=\"MathJax-Span-667\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">ff</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-668\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-669\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mi\" id=\"MathJax-Span-670\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">O</span><span class=\"mo\" id=\"MathJax-Span-671\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-672\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-673\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-674\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-675\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-676\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.83em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-677\"><span class=\"mrow\" id=\"MathJax-Span-678\"><span class=\"mtext\" id=\"MathJax-Span-679\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-680\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>n</mi><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>ff</mtext></mrow></msub><mo stretchy=\"false\">)</mo><mo>≈</mo><mi>O</mi><mo stretchy=\"false\">(</mo><mi>n</mi><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-63\">O(n \\cdot d_{\\text{model}} \\cdot d_{\\text{ff}}) \\approx O(n \\cdot d_{\\text{model}}^2)</script>\n  </li>\n  <li>\n    <p><strong>Output Projection:</strong></p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-64-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>V</mi><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-681\" style=\"width: 6.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1005.42em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-682\"><span class=\"mi\" id=\"MathJax-Span-683\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-684\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-685\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-686\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-687\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-688\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-689\"><span class=\"mrow\" id=\"MathJax-Span-690\"><span class=\"mtext\" id=\"MathJax-Span-691\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-692\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>V</mi><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-64\">O(V \\cdot d_{\\text{model}})</script>\n\n    <ul>\n      <li>This is where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-65-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-693\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-694\"><span class=\"msubsup\" id=\"MathJax-Span-695\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-696\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-697\"><span class=\"mrow\" id=\"MathJax-Span-698\"><span class=\"mtext\" id=\"MathJax-Span-699\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-65\">d_{\\text{model}}</script> multiplies with vocab size effects (cf. <a href=\"#tokenizer-and-vocabulary-size\">Tokenizer and Vocabulary Size</a> section).</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Parameter count:</li>\n    </ul>\n<ul>\n      <li>Memory bandwidth scales linearly with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-526\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-527\"><span class=\"msubsup\" id=\"MathJax-Span-528\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-529\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-530\"><span class=\"mrow\" id=\"MathJax-Span-531\"><span class=\"mtext\" id=\"MathJax-Span-532\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-55\">d_{\\text{model}}</script> when loading embeddings.</li>\n    </ul>\n<ul>\n      <li>Each attention projection (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo>,</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-533\" style=\"width: 4.169em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.44em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.39em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-534\"><span class=\"mi\" id=\"MathJax-Span-535\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-536\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-537\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-538\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-539\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-540\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi><mo>,</mo><mi>K</mi><mo>,</mo><mi>V</mi><mo>,</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">Q, K, V,</script> output) is a matrix multiply of shape <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo>&amp;#x00D7;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x00D7;</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>&amp;#x00D7;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>proj</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-541\" style=\"width: 13.857em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.513em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1011.46em, 2.659em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-542\"><span class=\"mo\" id=\"MathJax-Span-543\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-544\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-545\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-546\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-547\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-548\"><span class=\"mrow\" id=\"MathJax-Span-549\"><span class=\"mtext\" id=\"MathJax-Span-550\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-551\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-552\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mo\" id=\"MathJax-Span-553\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-554\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-555\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-556\"><span class=\"mrow\" id=\"MathJax-Span-557\"><span class=\"mtext\" id=\"MathJax-Span-558\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-559\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-560\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.721em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-561\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-562\"><span class=\"mrow\" id=\"MathJax-Span-563\"><span class=\"mtext\" id=\"MathJax-Span-564\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">proj</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-565\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>n</mi><mo>×</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo stretchy=\"false\">)</mo><mo>×</mo><mo stretchy=\"false\">(</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>×</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>proj</mtext></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">(n \\times d_{\\text{model}}) \\times (d_{\\text{model}} \\times d_{\\text{proj}})</script>.</li>\n      <li>FLOPs per layer for MHA (excluding softmax):</li>\n    </ul>\n<ul>\n      <li>Since <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-59-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>head</mtext></mrow></msub><mo>&amp;#x22C5;</mo><mi>h</mi><mo>=</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-594\" style=\"width: 8.336em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.93em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-595\"><span class=\"msubsup\" id=\"MathJax-Span-596\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-597\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-598\"><span class=\"mrow\" id=\"MathJax-Span-599\"><span class=\"mtext\" id=\"MathJax-Span-600\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">head</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-601\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-602\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">h</span><span class=\"mo\" id=\"MathJax-Span-603\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-604\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-605\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-606\"><span class=\"mrow\" id=\"MathJax-Span-607\"><span class=\"mtext\" id=\"MathJax-Span-608\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>head</mtext></mrow></msub><mo>⋅</mo><mi>h</mi><mo>=</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-59\">d_{\\text{head}} \\cdot h = d_{\\text{model}}</script>, the cost scales as <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-60-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-609\" style=\"width: 6.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1005.58em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-610\"><span class=\"mi\" id=\"MathJax-Span-611\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-612\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-613\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-614\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-615\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-616\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-617\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-618\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-619\"><span class=\"mrow\" id=\"MathJax-Span-620\"><span class=\"mtext\" id=\"MathJax-Span-621\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-622\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-60\">O(n^2 \\cdot d_{\\text{model}})</script> for attention scores and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-623\" style=\"width: 6.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.211em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1005.16em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-624\"><span class=\"mi\" id=\"MathJax-Span-625\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-626\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-627\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-628\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-629\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-630\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-631\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.83em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-632\"><span class=\"mrow\" id=\"MathJax-Span-633\"><span class=\"mtext\" id=\"MathJax-Span-634\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-635\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>n</mi><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">O(n \\cdot d_{\\text{model}}^2)</script> for projections.</li>\n    </ul>\n<ul>\n      <li>Uses intermediate size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-62-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>ff</mtext></mrow></msub><mo>&amp;#x2248;</mo><mn>4</mn><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-636\" style=\"width: 7.294em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.04em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-637\"><span class=\"msubsup\" id=\"MathJax-Span-638\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-639\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-640\"><span class=\"mrow\" id=\"MathJax-Span-641\"><span class=\"mtext\" id=\"MathJax-Span-642\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">ff</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-643\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mn\" id=\"MathJax-Span-644\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4</span><span class=\"mo\" id=\"MathJax-Span-645\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-646\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-647\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-648\"><span class=\"mrow\" id=\"MathJax-Span-649\"><span class=\"mtext\" id=\"MathJax-Span-650\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>ff</mtext></mrow></msub><mo>≈</mo><mn>4</mn><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-62\">d_{\\text{ff}} \\approx 4 \\cdot d_{\\text{model}}</script>.</li>\n      <li>Per layer FLOPs:</li>\n    </ul>\n<p><strong>Output Projection:</strong></p>\n<ul>\n      <li>This is where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-65-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-693\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-694\"><span class=\"msubsup\" id=\"MathJax-Span-695\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-696\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-697\"><span class=\"mrow\" id=\"MathJax-Span-698\"><span class=\"mtext\" id=\"MathJax-Span-699\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-65\">d_{\\text{model}}</script> multiplies with vocab size effects (cf. <a href=\"#tokenizer-and-vocabulary-size\">Tokenizer and Vocabulary Size</a> section).</li>\n    </ul>\n<h4 id=\"impact-on-the-kv-cache\">Impact on the KV Cache</h4>\n<ul>\n  <li>\n    <p>Increasing the model/embedding dimension <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-66-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-700\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-701\"><span class=\"msubsup\" id=\"MathJax-Span-702\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-703\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-704\"><span class=\"mrow\" id=\"MathJax-Span-705\"><span class=\"mtext\" id=\"MathJax-Span-706\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-66\">d_{\\text{model}}</script> linearly increases the size of the KV cache, which stores past attention states, and can quickly become a major driver of memory usage during inference.</p>\n  </li>\n  <li>\n    <p>During inference, each attention layer stores the <strong>Key</strong> (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-67-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-707\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-708\"><span class=\"mi\" id=\"MathJax-Span-709\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-67\">K</script>) and <strong>Value</strong> (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-68-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-710\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-711\"><span class=\"mi\" id=\"MathJax-Span-712\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-68\">V</script>) projections for every processed token in a <strong>KV cache</strong>, allowing the model to reuse past context without recomputing attention.</p>\n  </li>\n  <li>\n    <p>For a model with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-69-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>h</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-713\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-714\"><span class=\"mi\" id=\"MathJax-Span-715\" style=\"font-family: STIXGeneral-Italic;\">h</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>h</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-69\">h</script> heads, head dimension <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-70-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>head</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-716\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.93em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-717\"><span class=\"msubsup\" id=\"MathJax-Span-718\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-719\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-720\"><span class=\"mrow\" id=\"MathJax-Span-721\"><span class=\"mtext\" id=\"MathJax-Span-722\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">head</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>head</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-70\">d_{\\text{head}}</script>, and embedding dimension <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-71-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>=</mo><mi>h</mi><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>head</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-723\" style=\"width: 8.336em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.93em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-724\"><span class=\"msubsup\" id=\"MathJax-Span-725\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-726\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-727\"><span class=\"mrow\" id=\"MathJax-Span-728\"><span class=\"mtext\" id=\"MathJax-Span-729\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-730\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-731\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">h</span><span class=\"mo\" id=\"MathJax-Span-732\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-733\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-734\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-735\"><span class=\"mrow\" id=\"MathJax-Span-736\"><span class=\"mtext\" id=\"MathJax-Span-737\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">head</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>=</mo><mi>h</mi><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>head</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-71\">d_{\\text{model}} = h \\cdot d_{\\text{head}}</script>, the KV cache size per token for a single layer is:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-72-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>KV size per token</mtext><mo>=</mo><mn>2</mn><mo>&amp;#x00D7;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-738\" style=\"width: 14.846em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1012.35em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-739\"><span class=\"mtext\" id=\"MathJax-Span-740\" style=\"font-family: STIXGeneral-Regular;\">KV size per token</span><span class=\"mo\" id=\"MathJax-Span-741\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-742\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2</span><span class=\"mo\" id=\"MathJax-Span-743\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-744\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-745\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-746\"><span class=\"mrow\" id=\"MathJax-Span-747\"><span class=\"mtext\" id=\"MathJax-Span-748\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>KV size per token</mtext><mo>=</mo><mn>2</mn><mo>×</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-72\">\\text{KV size per token} = 2 \\times d_{\\text{model}}</script>\n\n    <ul>\n      <li>where, the factor of 2 accounts for storing both <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-73-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-749\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-750\"><span class=\"mi\" id=\"MathJax-Span-751\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-73\">K</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-74-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-752\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-753\"><span class=\"mi\" id=\"MathJax-Span-754\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-74\">V</script>.</li>\n    </ul>\n  </li>\n  <li>\n    <p>For a sequence of length <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-75-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-755\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-756\"><span class=\"mi\" id=\"MathJax-Span-757\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-75\">n</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-76-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-758\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-759\"><span class=\"mi\" id=\"MathJax-Span-760\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-76\">L</script> transformer layers, the total KV cache size is:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-77-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>Total KV memory</mtext><mo>=</mo><mi>L</mi><mo>&amp;#x00D7;</mo><mi>n</mi><mo>&amp;#x00D7;</mo><mn>2</mn><mo>&amp;#x00D7;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>&amp;#x00D7;</mo><mtext>sizeof(dtype)</mtext></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-761\" style=\"width: 26.513em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 22.086em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1022.03em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-762\"><span class=\"mtext\" id=\"MathJax-Span-763\" style=\"font-family: STIXGeneral-Regular;\">Total KV memory</span><span class=\"mo\" id=\"MathJax-Span-764\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-765\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-766\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-767\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">n</span><span class=\"mo\" id=\"MathJax-Span-768\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-769\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">2</span><span class=\"mo\" id=\"MathJax-Span-770\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-771\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-772\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-773\"><span class=\"mrow\" id=\"MathJax-Span-774\"><span class=\"mtext\" id=\"MathJax-Span-775\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-776\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mtext\" id=\"MathJax-Span-777\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">sizeof(dtype)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mtext>Total KV memory</mtext><mo>=</mo><mi>L</mi><mo>×</mo><mi>n</mi><mo>×</mo><mn>2</mn><mo>×</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>×</mo><mtext>sizeof(dtype)</mtext></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-77\">\\text{Total KV memory} = L \\times n \\times 2 \\times d_{\\text{model}} \\times \\text{sizeof(dtype)}</script>\n  </li>\n  <li>\n    <p><strong>Why this grows with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-78-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-778\" style=\"width: 2.843em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.327em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.294em, 1002.33em, 2.43em, -999.997em); top: -2.115em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-779\"><span class=\"msubsup\" id=\"MathJax-Span-780\"><span style=\"display: inline-block; position: relative; width: 2.327em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.52em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-781\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"texatom\" id=\"MathJax-Span-782\"><span class=\"mrow\" id=\"MathJax-Span-783\"><span class=\"mtext\" id=\"MathJax-Span-784\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.12em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-78\">d_{\\text{model}}</script>:</strong></p>\n\n    <ul>\n      <li>Both <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-79-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-785\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-786\"><span class=\"mi\" id=\"MathJax-Span-787\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-79\">K</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-80-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-788\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-789\"><span class=\"mi\" id=\"MathJax-Span-790\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-80\">V</script> vectors have dimensionality <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-81-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-791\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-792\"><span class=\"msubsup\" id=\"MathJax-Span-793\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-794\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-795\"><span class=\"mrow\" id=\"MathJax-Span-796\"><span class=\"mtext\" id=\"MathJax-Span-797\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-81\">d_{\\text{model}}</script>, so increasing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-82-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-798\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-799\"><span class=\"msubsup\" id=\"MathJax-Span-800\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-801\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-802\"><span class=\"mrow\" id=\"MathJax-Span-803\"><span class=\"mtext\" id=\"MathJax-Span-804\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-82\">d_{\\text{model}}</script> directly increases the number of stored values per token.</li>\n      <li>The growth is <strong>linear</strong> with respect to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-83-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-805\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-806\"><span class=\"msubsup\" id=\"MathJax-Span-807\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-808\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-809\"><span class=\"mrow\" id=\"MathJax-Span-810\"><span class=\"mtext\" id=\"MathJax-Span-811\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-83\">d_{\\text{model}}</script>, but multiplicative with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-84-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-812\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-813\"><span class=\"mi\" id=\"MathJax-Span-814\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-84\">n</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-85-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-815\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-816\"><span class=\"mi\" id=\"MathJax-Span-817\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-85\">L</script>.</li>\n      <li>This means that even modest increases in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-86-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-818\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-819\"><span class=\"msubsup\" id=\"MathJax-Span-820\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-821\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-822\"><span class=\"mrow\" id=\"MathJax-Span-823\"><span class=\"mtext\" id=\"MathJax-Span-824\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-86\">d_{\\text{model}}</script> can cause large jumps in memory usage for long sequences or deep networks.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Practical implications:</strong></p>\n\n    <ul>\n      <li>Larger <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-87-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-825\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-826\"><span class=\"msubsup\" id=\"MathJax-Span-827\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-828\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-829\"><span class=\"mrow\" id=\"MathJax-Span-830\"><span class=\"mtext\" id=\"MathJax-Span-831\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-87\">d_{\\text{model}}</script> means more memory bandwidth required to read/write the KV cache, which can bottleneck inference.</li>\n      <li>On memory-limited devices, KV cache capacity often becomes the limiting factor before compute throughput does.</li>\n    </ul>\n  </li>\n</ul>\n<p>Increasing the model/embedding dimension <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-66-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-700\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-701\"><span class=\"msubsup\" id=\"MathJax-Span-702\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-703\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-704\"><span class=\"mrow\" id=\"MathJax-Span-705\"><span class=\"mtext\" id=\"MathJax-Span-706\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-66\">d_{\\text{model}}</script> linearly increases the size of the KV cache, which stores past attention states, and can quickly become a major driver of memory usage during inference.</p>\n<p>During inference, each attention layer stores the <strong>Key</strong> (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-67-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-707\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-708\"><span class=\"mi\" id=\"MathJax-Span-709\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-67\">K</script>) and <strong>Value</strong> (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-68-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-710\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-711\"><span class=\"mi\" id=\"MathJax-Span-712\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-68\">V</script>) projections for every processed token in a <strong>KV cache</strong>, allowing the model to reuse past context without recomputing attention.</p>\n<p>For a model with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-69-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>h</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-713\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-714\"><span class=\"mi\" id=\"MathJax-Span-715\" style=\"font-family: STIXGeneral-Italic;\">h</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>h</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-69\">h</script> heads, head dimension <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-70-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>head</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-716\" style=\"width: 2.346em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.93em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-717\"><span class=\"msubsup\" id=\"MathJax-Span-718\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-719\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-720\"><span class=\"mrow\" id=\"MathJax-Span-721\"><span class=\"mtext\" id=\"MathJax-Span-722\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">head</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>head</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-70\">d_{\\text{head}}</script>, and embedding dimension <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-71-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>=</mo><mi>h</mi><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>head</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-723\" style=\"width: 8.336em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.93em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-724\"><span class=\"msubsup\" id=\"MathJax-Span-725\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-726\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-727\"><span class=\"mrow\" id=\"MathJax-Span-728\"><span class=\"mtext\" id=\"MathJax-Span-729\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-730\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-731\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">h</span><span class=\"mo\" id=\"MathJax-Span-732\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-733\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-734\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-735\"><span class=\"mrow\" id=\"MathJax-Span-736\"><span class=\"mtext\" id=\"MathJax-Span-737\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">head</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>=</mo><mi>h</mi><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>head</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-71\">d_{\\text{model}} = h \\cdot d_{\\text{head}}</script>, the KV cache size per token for a single layer is:</p>\n<ul>\n      <li>where, the factor of 2 accounts for storing both <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-73-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-749\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-750\"><span class=\"mi\" id=\"MathJax-Span-751\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-73\">K</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-74-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-752\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-753\"><span class=\"mi\" id=\"MathJax-Span-754\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-74\">V</script>.</li>\n    </ul>\n<p>For a sequence of length <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-75-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-755\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-756\"><span class=\"mi\" id=\"MathJax-Span-757\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-75\">n</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-76-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-758\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-759\"><span class=\"mi\" id=\"MathJax-Span-760\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-76\">L</script> transformer layers, the total KV cache size is:</p>\n<p><strong>Why this grows with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-78-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-778\" style=\"width: 2.843em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.327em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.294em, 1002.33em, 2.43em, -999.997em); top: -2.115em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-779\"><span class=\"msubsup\" id=\"MathJax-Span-780\"><span style=\"display: inline-block; position: relative; width: 2.327em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.52em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-781\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"texatom\" id=\"MathJax-Span-782\"><span class=\"mrow\" id=\"MathJax-Span-783\"><span class=\"mtext\" id=\"MathJax-Span-784\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.12em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-78\">d_{\\text{model}}</script>:</strong></p>\n<ul>\n      <li>Both <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-79-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-785\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-786\"><span class=\"mi\" id=\"MathJax-Span-787\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-79\">K</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-80-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-788\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-789\"><span class=\"mi\" id=\"MathJax-Span-790\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-80\">V</script> vectors have dimensionality <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-81-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-791\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-792\"><span class=\"msubsup\" id=\"MathJax-Span-793\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-794\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-795\"><span class=\"mrow\" id=\"MathJax-Span-796\"><span class=\"mtext\" id=\"MathJax-Span-797\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-81\">d_{\\text{model}}</script>, so increasing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-82-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-798\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-799\"><span class=\"msubsup\" id=\"MathJax-Span-800\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-801\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-802\"><span class=\"mrow\" id=\"MathJax-Span-803\"><span class=\"mtext\" id=\"MathJax-Span-804\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-82\">d_{\\text{model}}</script> directly increases the number of stored values per token.</li>\n      <li>The growth is <strong>linear</strong> with respect to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-83-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-805\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-806\"><span class=\"msubsup\" id=\"MathJax-Span-807\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-808\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-809\"><span class=\"mrow\" id=\"MathJax-Span-810\"><span class=\"mtext\" id=\"MathJax-Span-811\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-83\">d_{\\text{model}}</script>, but multiplicative with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-84-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-812\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-813\"><span class=\"mi\" id=\"MathJax-Span-814\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-84\">n</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-85-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-815\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-816\"><span class=\"mi\" id=\"MathJax-Span-817\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-85\">L</script>.</li>\n      <li>This means that even modest increases in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-86-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-818\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-819\"><span class=\"msubsup\" id=\"MathJax-Span-820\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-821\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-822\"><span class=\"mrow\" id=\"MathJax-Span-823\"><span class=\"mtext\" id=\"MathJax-Span-824\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-86\">d_{\\text{model}}</script> can cause large jumps in memory usage for long sequences or deep networks.</li>\n    </ul>\n<p><strong>Practical implications:</strong></p>\n<ul>\n      <li>Larger <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-87-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-825\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-826\"><span class=\"msubsup\" id=\"MathJax-Span-827\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-828\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-829\"><span class=\"mrow\" id=\"MathJax-Span-830\"><span class=\"mtext\" id=\"MathJax-Span-831\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-87\">d_{\\text{model}}</script> means more memory bandwidth required to read/write the KV cache, which can bottleneck inference.</li>\n      <li>On memory-limited devices, KV cache capacity often becomes the limiting factor before compute throughput does.</li>\n    </ul>\n<h4 id=\"runtime-and-memory-effects\">Runtime and Memory Effects</h4>\n<ul>\n  <li>\n    <p><strong>Larger Model Dimension:</strong></p>\n\n    <ul>\n      <li><strong>Pros:</strong> More capacity per token, better representation quality, higher perplexity performance.</li>\n      <li><strong>Cons:</strong> Quadratic or linear increase in FLOPs for most layers, large memory footprint, higher activation size in KV cache.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Smaller Model Dimension:</strong></p>\n\n    <ul>\n      <li><strong>Pros:</strong> Smaller models, faster inference, better suited for edge devices.</li>\n      <li><strong>Cons:</strong> Lower expressive power; can bottleneck performance before parameter count is reached.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Larger Model Dimension:</strong></p>\n<ul>\n      <li><strong>Pros:</strong> More capacity per token, better representation quality, higher perplexity performance.</li>\n      <li><strong>Cons:</strong> Quadratic or linear increase in FLOPs for most layers, large memory footprint, higher activation size in KV cache.</li>\n    </ul>\n<p><strong>Smaller Model Dimension:</strong></p>\n<ul>\n      <li><strong>Pros:</strong> Smaller models, faster inference, better suited for edge devices.</li>\n      <li><strong>Cons:</strong> Lower expressive power; can bottleneck performance before parameter count is reached.</li>\n    </ul>\n<h4 id=\"interaction-with-sequence-length\">Interaction with Sequence Length</h4>\n<ul>\n  <li>While increasing the model dimension linearly scales memory and compute in most layers, attention score calculation (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-88-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-832\" style=\"width: 6.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1005.58em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-833\"><span class=\"mi\" id=\"MathJax-Span-834\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-835\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-836\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-837\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-838\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-839\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-840\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-841\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-842\"><span class=\"mrow\" id=\"MathJax-Span-843\"><span class=\"mtext\" id=\"MathJax-Span-844\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-845\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-88\">O(n^2 \\cdot d_{\\text{model}})</script>) means that with long sequences, the impact of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-89-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-846\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-847\"><span class=\"msubsup\" id=\"MathJax-Span-848\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-849\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-850\"><span class=\"mrow\" id=\"MathJax-Span-851\"><span class=\"mtext\" id=\"MathJax-Span-852\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-89\">d_{\\text{model}}</script> is amplified.</li>\n</ul>\n<h4 id=\"server-vs-on-device\">Server vs. On-Device</h4>\n<ul>\n  <li><strong>Server (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference):</strong>\n    <ul>\n      <li>Can afford larger <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-90-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-853\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-854\"><span class=\"msubsup\" id=\"MathJax-Span-855\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-856\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-857\"><span class=\"mrow\" id=\"MathJax-Span-858\"><span class=\"mtext\" id=\"MathJax-Span-859\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-90\">d_{\\text{model}}</script> (e.g., 2048+) due to GPU tensor cores and high-bandwidth memory. The main tradeoff is training/inference cost, not latency.</li>\n      <li>TensorRT-LLM uses fused attention kernels and mixed-precision GEMMs to sustain high throughput at large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-91-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-860\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-861\"><span class=\"msubsup\" id=\"MathJax-Span-862\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-863\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-864\"><span class=\"mrow\" id=\"MathJax-Span-865\"><span class=\"mtext\" id=\"MathJax-Span-866\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-91\">d_{\\text{model}}</script>, while vLLM and DeepSpeed-Inference can overlap compute and memory ops to mask latency.</li>\n    </ul>\n  </li>\n  <li><strong>On-Device (llama.cpp, GGML, ONNX Runtime Mobile):</strong>\n    <ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-92-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-867\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-868\"><span class=\"msubsup\" id=\"MathJax-Span-869\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-870\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-871\"><span class=\"mrow\" id=\"MathJax-Span-872\"><span class=\"mtext\" id=\"MathJax-Span-873\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-92\">d_{\\text{model}}</script> needs to be small enough to fit activations and KV cache in available RAM/LLC. Even quantized models struggle if <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-93-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-874\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-875\"><span class=\"msubsup\" id=\"MathJax-Span-876\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-877\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-878\"><span class=\"mrow\" id=\"MathJax-Span-879\"><span class=\"mtext\" id=\"MathJax-Span-880\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-93\">d_{\\text{model}}</script> &gt; 1024 on low-end CPUs.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> and GGML use block-quantized matrix multiplies to reduce memory footprint, while ONNX Runtime Mobile leverages kernel fusion for small <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-94-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-881\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-882\"><span class=\"msubsup\" id=\"MathJax-Span-883\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-884\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-885\"><span class=\"mrow\" id=\"MathJax-Span-886\"><span class=\"mtext\" id=\"MathJax-Span-887\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-94\">d_{\\text{model}}</script> efficiency.</li>\n    </ul>\n  </li>\n  <li><strong>Edge-specialized (LidarTLM, MediaPipe + TFLite, Qualcomm AI Stack):</strong>\n    <ul>\n      <li>Often uses <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-95-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-888\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-889\"><span class=\"msubsup\" id=\"MathJax-Span-890\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-891\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-892\"><span class=\"mrow\" id=\"MathJax-Span-893\"><span class=\"mtext\" id=\"MathJax-Span-894\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-95\">d_{\\text{model}}</script> = 256–512 for real-time inference in sensor fusion pipelines, balancing compute with strict power envelopes.</li>\n      <li>MediaPipe + TFLite models targeting NPUs frequently compress <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-96-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-895\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-896\"><span class=\"msubsup\" id=\"MathJax-Span-897\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-898\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-899\"><span class=\"mrow\" id=\"MathJax-Span-900\"><span class=\"mtext\" id=\"MathJax-Span-901\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-96\">d_{\\text{model}}</script> to fit SRAM, while the Qualcomm AI Stack uses <code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">float16</code> hybrid kernels to keep latency under frame-time budgets.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Can afford larger <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-90-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-853\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-854\"><span class=\"msubsup\" id=\"MathJax-Span-855\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-856\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-857\"><span class=\"mrow\" id=\"MathJax-Span-858\"><span class=\"mtext\" id=\"MathJax-Span-859\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-90\">d_{\\text{model}}</script> (e.g., 2048+) due to GPU tensor cores and high-bandwidth memory. The main tradeoff is training/inference cost, not latency.</li>\n      <li>TensorRT-LLM uses fused attention kernels and mixed-precision GEMMs to sustain high throughput at large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-91-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-860\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-861\"><span class=\"msubsup\" id=\"MathJax-Span-862\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-863\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-864\"><span class=\"mrow\" id=\"MathJax-Span-865\"><span class=\"mtext\" id=\"MathJax-Span-866\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-91\">d_{\\text{model}}</script>, while vLLM and DeepSpeed-Inference can overlap compute and memory ops to mask latency.</li>\n    </ul>\n<ul>\n      <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-92-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-867\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-868\"><span class=\"msubsup\" id=\"MathJax-Span-869\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-870\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-871\"><span class=\"mrow\" id=\"MathJax-Span-872\"><span class=\"mtext\" id=\"MathJax-Span-873\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-92\">d_{\\text{model}}</script> needs to be small enough to fit activations and KV cache in available RAM/LLC. Even quantized models struggle if <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-93-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-874\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-875\"><span class=\"msubsup\" id=\"MathJax-Span-876\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-877\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-878\"><span class=\"mrow\" id=\"MathJax-Span-879\"><span class=\"mtext\" id=\"MathJax-Span-880\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-93\">d_{\\text{model}}</script> &gt; 1024 on low-end CPUs.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> and GGML use block-quantized matrix multiplies to reduce memory footprint, while ONNX Runtime Mobile leverages kernel fusion for small <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-94-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-881\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-882\"><span class=\"msubsup\" id=\"MathJax-Span-883\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-884\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-885\"><span class=\"mrow\" id=\"MathJax-Span-886\"><span class=\"mtext\" id=\"MathJax-Span-887\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-94\">d_{\\text{model}}</script> efficiency.</li>\n    </ul>\n<ul>\n      <li>Often uses <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-95-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-888\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-889\"><span class=\"msubsup\" id=\"MathJax-Span-890\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-891\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-892\"><span class=\"mrow\" id=\"MathJax-Span-893\"><span class=\"mtext\" id=\"MathJax-Span-894\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-95\">d_{\\text{model}}</script> = 256–512 for real-time inference in sensor fusion pipelines, balancing compute with strict power envelopes.</li>\n      <li>MediaPipe + TFLite models targeting NPUs frequently compress <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-96-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-895\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-896\"><span class=\"msubsup\" id=\"MathJax-Span-897\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-898\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-899\"><span class=\"mrow\" id=\"MathJax-Span-900\"><span class=\"mtext\" id=\"MathJax-Span-901\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-96\">d_{\\text{model}}</script> to fit SRAM, while the Qualcomm AI Stack uses <code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">float16</code> hybrid kernels to keep latency under frame-time budgets.</li>\n    </ul>",
      "contentMarkdown": "*   The model dimension (also called the “embedding dimension”), dmodeldmodeld\\_{\\\\text{model}}, determines the dimensionality of token embeddings and hidden states across the transformer.\n*   The model dimension has a direct, predictable effect on runtime cost. Increasing the model dimension even slightly can push an on-device model from real-time to unusably slow since it appears in nearly every computational term in the architecture.\n\n#### Where the Model Dimension Shows up\n\n*   **Embedding Layer:**\n    \n    *   Parameter count:\n    \n    Pembed\\=V×dmodelPembed\\=V×dmodel\n    \n    P\\_{\\\\text{embed}} = V \\\\times d\\_{\\\\text{model}}\n    *   Memory bandwidth scales linearly with dmodeldmodeld\\_{\\\\text{model}} when loading embeddings.\n*   **Multi-Head Attention (MHA):**\n    \n    *   Each attention projection (Q,K,V,Q,K,V,Q, K, V, output) is a matrix multiply of shape (n×dmodel)×(dmodel×dproj)(n×dmodel)×(dmodel×dproj)(n \\\\times d\\_{\\\\text{model}}) \\\\times (d\\_{\\\\text{model}} \\\\times d\\_{\\\\text{proj}}).\n    *   FLOPs per layer for MHA (excluding softmax):\n    \n    O(n2⋅dhead⋅h)+O(n⋅d2model)O(n2⋅dhead⋅h)+O(n⋅dmodel2)\n    \n    O(n^2 \\\\cdot d\\_{\\\\text{head}} \\\\cdot h) + O(n \\\\cdot d\\_{\\\\text{model}}^2)\n    *   Since dhead⋅h\\=dmodeldhead⋅h\\=dmodeld\\_{\\\\text{head}} \\\\cdot h = d\\_{\\\\text{model}}, the cost scales as O(n2⋅dmodel)O(n2⋅dmodel)O(n^2 \\\\cdot d\\_{\\\\text{model}}) for attention scores and O(n⋅d2model)O(n⋅dmodel2)O(n \\\\cdot d\\_{\\\\text{model}}^2) for projections.\n*   **Feedforward Network (FFN):**\n    \n    *   Uses intermediate size dff≈4⋅dmodeldff≈4⋅dmodeld\\_{\\\\text{ff}} \\\\approx 4 \\\\cdot d\\_{\\\\text{model}}.\n    *   Per layer FLOPs:\n    \n    O(n⋅dmodel⋅dff)≈O(n⋅d2model)O(n⋅dmodel⋅dff)≈O(n⋅dmodel2)\n    \n    O(n \\\\cdot d\\_{\\\\text{model}} \\\\cdot d\\_{\\\\text{ff}}) \\\\approx O(n \\\\cdot d\\_{\\\\text{model}}^2)\n*   **Output Projection:**\n    \n    O(V⋅dmodel)O(V⋅dmodel)\n    \n    O(V \\\\cdot d\\_{\\\\text{model}})\n    *   This is where dmodeldmodeld\\_{\\\\text{model}} multiplies with vocab size effects (cf. [Tokenizer and Vocabulary Size](#tokenizer-and-vocabulary-size) section).\n\n*   Parameter count:\n\n*   Memory bandwidth scales linearly with dmodeldmodeld\\_{\\\\text{model}} when loading embeddings.\n\n*   Each attention projection (Q,K,V,Q,K,V,Q, K, V, output) is a matrix multiply of shape (n×dmodel)×(dmodel×dproj)(n×dmodel)×(dmodel×dproj)(n \\\\times d\\_{\\\\text{model}}) \\\\times (d\\_{\\\\text{model}} \\\\times d\\_{\\\\text{proj}}).\n*   FLOPs per layer for MHA (excluding softmax):\n\n*   Since dhead⋅h\\=dmodeldhead⋅h\\=dmodeld\\_{\\\\text{head}} \\\\cdot h = d\\_{\\\\text{model}}, the cost scales as O(n2⋅dmodel)O(n2⋅dmodel)O(n^2 \\\\cdot d\\_{\\\\text{model}}) for attention scores and O(n⋅d2model)O(n⋅dmodel2)O(n \\\\cdot d\\_{\\\\text{model}}^2) for projections.\n\n*   Uses intermediate size dff≈4⋅dmodeldff≈4⋅dmodeld\\_{\\\\text{ff}} \\\\approx 4 \\\\cdot d\\_{\\\\text{model}}.\n*   Per layer FLOPs:\n\n**Output Projection:**\n\n*   This is where dmodeldmodeld\\_{\\\\text{model}} multiplies with vocab size effects (cf. [Tokenizer and Vocabulary Size](#tokenizer-and-vocabulary-size) section).\n\n#### Impact on the KV Cache\n\n*   Increasing the model/embedding dimension dmodeldmodeld\\_{\\\\text{model}} linearly increases the size of the KV cache, which stores past attention states, and can quickly become a major driver of memory usage during inference.\n    \n*   During inference, each attention layer stores the **Key** (KKK) and **Value** (VVV) projections for every processed token in a **KV cache**, allowing the model to reuse past context without recomputing attention.\n    \n*   For a model with hhh heads, head dimension dheaddheadd\\_{\\\\text{head}}, and embedding dimension dmodel\\=h⋅dheaddmodel\\=h⋅dheadd\\_{\\\\text{model}} = h \\\\cdot d\\_{\\\\text{head}}, the KV cache size per token for a single layer is:\n    \n    KV size per token\\=2×dmodelKV size per token\\=2×dmodel\n    \n    \\\\text{KV size per token} = 2 \\\\times d\\_{\\\\text{model}}\n    *   where, the factor of 2 accounts for storing both KKK and VVV.\n*   For a sequence of length nnn and LLL transformer layers, the total KV cache size is:\n    \n    Total KV memory\\=L×n×2×dmodel×sizeof(dtype)Total KV memory\\=L×n×2×dmodel×sizeof(dtype)\n    \n    \\\\text{Total KV memory} = L \\\\times n \\\\times 2 \\\\times d\\_{\\\\text{model}} \\\\times \\\\text{sizeof(dtype)}\n*   **Why this grows with dmodeldmodeld\\_{\\\\text{model}}:**\n    \n    *   Both KKK and VVV vectors have dimensionality dmodeldmodeld\\_{\\\\text{model}}, so increasing dmodeldmodeld\\_{\\\\text{model}} directly increases the number of stored values per token.\n    *   The growth is **linear** with respect to dmodeldmodeld\\_{\\\\text{model}}, but multiplicative with nnn and LLL.\n    *   This means that even modest increases in dmodeldmodeld\\_{\\\\text{model}} can cause large jumps in memory usage for long sequences or deep networks.\n*   **Practical implications:**\n    \n    *   Larger dmodeldmodeld\\_{\\\\text{model}} means more memory bandwidth required to read/write the KV cache, which can bottleneck inference.\n    *   On memory-limited devices, KV cache capacity often becomes the limiting factor before compute throughput does.\n\nIncreasing the model/embedding dimension dmodeldmodeld\\_{\\\\text{model}} linearly increases the size of the KV cache, which stores past attention states, and can quickly become a major driver of memory usage during inference.\n\nDuring inference, each attention layer stores the **Key** (KKK) and **Value** (VVV) projections for every processed token in a **KV cache**, allowing the model to reuse past context without recomputing attention.\n\nFor a model with hhh heads, head dimension dheaddheadd\\_{\\\\text{head}}, and embedding dimension dmodel\\=h⋅dheaddmodel\\=h⋅dheadd\\_{\\\\text{model}} = h \\\\cdot d\\_{\\\\text{head}}, the KV cache size per token for a single layer is:\n\n*   where, the factor of 2 accounts for storing both KKK and VVV.\n\nFor a sequence of length nnn and LLL transformer layers, the total KV cache size is:\n\n**Why this grows with dmodeldmodeld\\_{\\\\text{model}}:**\n\n*   Both KKK and VVV vectors have dimensionality dmodeldmodeld\\_{\\\\text{model}}, so increasing dmodeldmodeld\\_{\\\\text{model}} directly increases the number of stored values per token.\n*   The growth is **linear** with respect to dmodeldmodeld\\_{\\\\text{model}}, but multiplicative with nnn and LLL.\n*   This means that even modest increases in dmodeldmodeld\\_{\\\\text{model}} can cause large jumps in memory usage for long sequences or deep networks.\n\n**Practical implications:**\n\n*   Larger dmodeldmodeld\\_{\\\\text{model}} means more memory bandwidth required to read/write the KV cache, which can bottleneck inference.\n*   On memory-limited devices, KV cache capacity often becomes the limiting factor before compute throughput does.\n\n#### Runtime and Memory Effects\n\n*   **Larger Model Dimension:**\n    \n    *   **Pros:** More capacity per token, better representation quality, higher perplexity performance.\n    *   **Cons:** Quadratic or linear increase in FLOPs for most layers, large memory footprint, higher activation size in KV cache.\n*   **Smaller Model Dimension:**\n    \n    *   **Pros:** Smaller models, faster inference, better suited for edge devices.\n    *   **Cons:** Lower expressive power; can bottleneck performance before parameter count is reached.\n\n**Larger Model Dimension:**\n\n*   **Pros:** More capacity per token, better representation quality, higher perplexity performance.\n*   **Cons:** Quadratic or linear increase in FLOPs for most layers, large memory footprint, higher activation size in KV cache.\n\n**Smaller Model Dimension:**\n\n*   **Pros:** Smaller models, faster inference, better suited for edge devices.\n*   **Cons:** Lower expressive power; can bottleneck performance before parameter count is reached.\n\n#### Interaction with Sequence Length\n\n*   While increasing the model dimension linearly scales memory and compute in most layers, attention score calculation (O(n2⋅dmodel)O(n2⋅dmodel)O(n^2 \\\\cdot d\\_{\\\\text{model}})) means that with long sequences, the impact of dmodeldmodeld\\_{\\\\text{model}} is amplified.\n\n#### Server vs. On-Device\n\n*   **Server (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference):**\n    *   Can afford larger dmodeldmodeld\\_{\\\\text{model}} (e.g., 2048+) due to GPU tensor cores and high-bandwidth memory. The main tradeoff is training/inference cost, not latency.\n    *   TensorRT-LLM uses fused attention kernels and mixed-precision GEMMs to sustain high throughput at large dmodeldmodeld\\_{\\\\text{model}}, while vLLM and DeepSpeed-Inference can overlap compute and memory ops to mask latency.\n*   **On-Device (llama.cpp, GGML, ONNX Runtime Mobile):**\n    *   dmodeldmodeld\\_{\\\\text{model}} needs to be small enough to fit activations and KV cache in available RAM/LLC. Even quantized models struggle if dmodeldmodeld\\_{\\\\text{model}} > 1024 on low-end CPUs.\n    *   `llama.cpp` and GGML use block-quantized matrix multiplies to reduce memory footprint, while ONNX Runtime Mobile leverages kernel fusion for small dmodeldmodeld\\_{\\\\text{model}} efficiency.\n*   **Edge-specialized (LidarTLM, MediaPipe + TFLite, Qualcomm AI Stack):**\n    *   Often uses dmodeldmodeld\\_{\\\\text{model}} = 256–512 for real-time inference in sensor fusion pipelines, balancing compute with strict power envelopes.\n    *   MediaPipe + TFLite models targeting NPUs frequently compress dmodeldmodeld\\_{\\\\text{model}} to fit SRAM, while the Qualcomm AI Stack uses `int8`/`float16` hybrid kernels to keep latency under frame-time budgets.\n\n*   Can afford larger dmodeldmodeld\\_{\\\\text{model}} (e.g., 2048+) due to GPU tensor cores and high-bandwidth memory. The main tradeoff is training/inference cost, not latency.\n*   TensorRT-LLM uses fused attention kernels and mixed-precision GEMMs to sustain high throughput at large dmodeldmodeld\\_{\\\\text{model}}, while vLLM and DeepSpeed-Inference can overlap compute and memory ops to mask latency.\n\n*   dmodeldmodeld\\_{\\\\text{model}} needs to be small enough to fit activations and KV cache in available RAM/LLC. Even quantized models struggle if dmodeldmodeld\\_{\\\\text{model}} > 1024 on low-end CPUs.\n*   `llama.cpp` and GGML use block-quantized matrix multiplies to reduce memory footprint, while ONNX Runtime Mobile leverages kernel fusion for small dmodeldmodeld\\_{\\\\text{model}} efficiency.\n\n*   Often uses dmodeldmodeld\\_{\\\\text{model}} = 256–512 for real-time inference in sensor fusion pipelines, balancing compute with strict power envelopes.\n*   MediaPipe + TFLite models targeting NPUs frequently compress dmodeldmodeld\\_{\\\\text{model}} to fit SRAM, while the Qualcomm AI Stack uses `int8`/`float16` hybrid kernels to keep latency under frame-time budgets.",
      "order": 23,
      "orderInChapter": 2,
      "difficulty": 4,
      "estimatedMinutes": 7,
      "tags": [
        "ondevice ai",
        "transformer",
        "attention",
        "embedding",
        "gpt",
        "llm",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 1270,
        "contentLength": 193998
      },
      "nextCards": [
        "ai-on-device-transformers-sequence-length-and-kv-cache-size-24",
        "ai-on-device-transformers-parameter-count-and-model-depth-25"
      ],
      "relatedCards": [
        "ai-model-compression-multimodal-quantization-14",
        "ai-model-compression-popular-quantization-libraries-19",
        "ai-model-compression-mitigation-strategies-8",
        "ai-model-compression-compute-vs-memory-bottlenecks-12",
        "ai-model-compression-modern-quantization-techniques-13"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#model/embedding-dimension",
      "scrapedAt": "2025-12-28T11:56:11.408Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-sequence-length-and-kv-cache-size-24",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Parameter Choices and Their Runtime Implications in Transformer Architectures",
      "title": "Sequence Length and KV Cache Size",
      "subtitle": "Parameter Choices and Their Runtime Implications in Transformer Architectures",
      "contentHtml": "<ul>\n  <li>The sequence length <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-97-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-902\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-903\"><span class=\"mi\" id=\"MathJax-Span-904\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-97\">n</script> (number of tokens in context) directly controls both the <strong>prefill phase</strong> cost and the <strong>decode phase</strong> memory demands in transformer inference.</li>\n  <li>Sequence length has a <strong>relatively large runtime impact</strong> since it changes the size of the entire compute graph and KV cache footprint.</li>\n</ul>\n<h4 id=\"computational-scaling\">Computational Scaling</h4>\n<ul>\n  <li><strong>Prefill (parallelizable)</strong>:\n    <ul>\n      <li>Multi-head attention complexity for the encoder or decoder prefill step:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-98-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-905\" style=\"width: 6.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1005.58em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-906\"><span class=\"mi\" id=\"MathJax-Span-907\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-908\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-909\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-910\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-911\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-912\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-913\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-914\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-915\"><span class=\"mrow\" id=\"MathJax-Span-916\"><span class=\"mtext\" id=\"MathJax-Span-917\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-918\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-98\">O(n^2 \\cdot d_{\\text{model}})</script>\n\n    <ul>\n      <li>Because all <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-99-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-919\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-920\"><span class=\"mi\" id=\"MathJax-Span-921\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-99\">n</script> tokens attend to all others, cost grows quadratically. Increasing sequence length from 2k to 4k tokens quadruples the attention score computation.</li>\n    </ul>\n  </li>\n  <li><strong>Decode (autoregressive)</strong>:\n    <ul>\n      <li>With KV caching, each new token attends to all past tokens:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-100-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-922\" style=\"width: 6.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.211em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1005.16em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-923\"><span class=\"mi\" id=\"MathJax-Span-924\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-925\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-926\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-927\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-928\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-929\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-930\"><span class=\"mrow\" id=\"MathJax-Span-931\"><span class=\"mtext\" id=\"MathJax-Span-932\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-933\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>n</mi><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-100\">O(n \\cdot d_{\\text{model}})</script>\n\n    <ul>\n      <li>This is linear per token, but multiplied across <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-101-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>n</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>gen</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-934\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.62em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-935\"><span class=\"msubsup\" id=\"MathJax-Span-936\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-937\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-938\"><span class=\"mrow\" id=\"MathJax-Span-939\"><span class=\"mtext\" id=\"MathJax-Span-940\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">gen</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>n</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>gen</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-101\">n_{\\text{gen}}</script> output tokens.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Multi-head attention complexity for the encoder or decoder prefill step:</li>\n    </ul>\n<ul>\n      <li>Because all <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-99-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-919\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-920\"><span class=\"mi\" id=\"MathJax-Span-921\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-99\">n</script> tokens attend to all others, cost grows quadratically. Increasing sequence length from 2k to 4k tokens quadruples the attention score computation.</li>\n    </ul>\n<ul>\n      <li>With KV caching, each new token attends to all past tokens:</li>\n    </ul>\n<ul>\n      <li>This is linear per token, but multiplied across <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-101-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>n</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>gen</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-934\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.62em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-935\"><span class=\"msubsup\" id=\"MathJax-Span-936\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-937\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-938\"><span class=\"mrow\" id=\"MathJax-Span-939\"><span class=\"mtext\" id=\"MathJax-Span-940\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">gen</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>n</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>gen</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-101\">n_{\\text{gen}}</script> output tokens.</li>\n    </ul>\n<h4 id=\"kv-cache-size-and-memory-footprint\">KV Cache Size and Memory Footprint</h4>\n<ul>\n  <li>\n    <p>In autoregressive decoding, the KV cache stores keys and values for each token at each layer:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-102-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>K</mi><msub><mi>V</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow></msub><mo>&amp;#x2248;</mo><mi>n</mi><mo>&amp;#x00D7;</mo><mi>L</mi><mo>&amp;#x00D7;</mo><mn>2</mn><mo>&amp;#x00D7;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>&amp;#x00D7;</mo><mtext>precision</mtext></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-941\" style=\"width: 18.909em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 15.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1015.73em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-942\"><span class=\"mi\" id=\"MathJax-Span-943\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-944\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-945\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-946\"><span class=\"mrow\" id=\"MathJax-Span-947\"><span class=\"mi\" id=\"MathJax-Span-948\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">s</span><span class=\"mi\" id=\"MathJax-Span-949\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-950\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">z</span><span class=\"mi\" id=\"MathJax-Span-951\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-952\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mi\" id=\"MathJax-Span-953\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">n</span><span class=\"mo\" id=\"MathJax-Span-954\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mi\" id=\"MathJax-Span-955\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-956\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mn\" id=\"MathJax-Span-957\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">2</span><span class=\"mo\" id=\"MathJax-Span-958\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"msubsup\" id=\"MathJax-Span-959\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-960\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-961\"><span class=\"mrow\" id=\"MathJax-Span-962\"><span class=\"mtext\" id=\"MathJax-Span-963\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-964\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">×</span><span class=\"mtext\" id=\"MathJax-Span-965\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">precision</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>K</mi><msub><mi>V</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi></mrow></msub><mo>≈</mo><mi>n</mi><mo>×</mo><mi>L</mi><mo>×</mo><mn>2</mn><mo>×</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>×</mo><mtext>precision</mtext></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-102\">KV_{size} \\approx n \\times L \\times 2 \\times d_{\\text{model}} \\times \\text{precision}</script>\n\n    <ul>\n      <li>\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-103-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-966\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-967\"><span class=\"mi\" id=\"MathJax-Span-968\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-103\">n</script>: sequence length so far</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-104-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-969\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-970\"><span class=\"mi\" id=\"MathJax-Span-971\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-104\">L</script>: number of layers</li>\n          <li>Factor 2: separate <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-105-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-972\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-973\"><span class=\"mi\" id=\"MathJax-Span-974\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-105\">K</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-106-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-975\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-976\"><span class=\"mi\" id=\"MathJax-Span-977\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-106\">V</script> storage</li>\n          <li>precision: bytes per element (e.g., 2 for <code class=\"language-plaintext highlighter-rouge\">float16</code>, 1 for int8)</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>Example: For <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-107-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi><mo>=</mo><mn>2048</mn><mo>,</mo><mi>L</mi><mo>=</mo><mn>24</mn><mo>,</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>=</mo><mn>1024</mn><mo>,</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-978\" style=\"width: 15.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.232em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1013.18em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-979\"><span class=\"mi\" id=\"MathJax-Span-980\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-981\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-982\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2048</span><span class=\"mo\" id=\"MathJax-Span-983\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-984\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-985\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-986\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">24</span><span class=\"mo\" id=\"MathJax-Span-987\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-988\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-989\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-990\"><span class=\"mrow\" id=\"MathJax-Span-991\"><span class=\"mtext\" id=\"MathJax-Span-992\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-993\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-994\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1024</span><span class=\"mo\" id=\"MathJax-Span-995\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi><mo>=</mo><mn>2048</mn><mo>,</mo><mi>L</mi><mo>=</mo><mn>24</mn><mo>,</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>=</mo><mn>1024</mn><mo>,</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-107\">n = 2048, L = 24, d_{\\text{model}} = 1024,</script> <code class=\"language-plaintext highlighter-rouge\">float16</code>, KV cache ≈ 192 MB.</p>\n  </li>\n</ul>\n<p>In autoregressive decoding, the KV cache stores keys and values for each token at each layer:</p>\n<ul>\n      <li>\n        <p>where:</p>\n\n        <ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-103-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-966\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-967\"><span class=\"mi\" id=\"MathJax-Span-968\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-103\">n</script>: sequence length so far</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-104-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-969\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-970\"><span class=\"mi\" id=\"MathJax-Span-971\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-104\">L</script>: number of layers</li>\n          <li>Factor 2: separate <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-105-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-972\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-973\"><span class=\"mi\" id=\"MathJax-Span-974\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-105\">K</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-106-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-975\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-976\"><span class=\"mi\" id=\"MathJax-Span-977\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-106\">V</script> storage</li>\n          <li>precision: bytes per element (e.g., 2 for <code class=\"language-plaintext highlighter-rouge\">float16</code>, 1 for int8)</li>\n        </ul>\n      </li>\n    </ul>\n<p>where:</p>\n<ul>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-103-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-966\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-967\"><span class=\"mi\" id=\"MathJax-Span-968\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-103\">n</script>: sequence length so far</li>\n          <li><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-104-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-969\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-970\"><span class=\"mi\" id=\"MathJax-Span-971\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-104\">L</script>: number of layers</li>\n          <li>Factor 2: separate <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-105-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-972\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-973\"><span class=\"mi\" id=\"MathJax-Span-974\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-105\">K</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-106-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-975\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-976\"><span class=\"mi\" id=\"MathJax-Span-977\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-106\">V</script> storage</li>\n          <li>precision: bytes per element (e.g., 2 for <code class=\"language-plaintext highlighter-rouge\">float16</code>, 1 for int8)</li>\n        </ul>\n<p>Example: For <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-107-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi><mo>=</mo><mn>2048</mn><mo>,</mo><mi>L</mi><mo>=</mo><mn>24</mn><mo>,</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>=</mo><mn>1024</mn><mo>,</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-978\" style=\"width: 15.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.232em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1013.18em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-979\"><span class=\"mi\" id=\"MathJax-Span-980\" style=\"font-family: STIXGeneral-Italic;\">n</span><span class=\"mo\" id=\"MathJax-Span-981\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-982\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2048</span><span class=\"mo\" id=\"MathJax-Span-983\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-984\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-985\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-986\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">24</span><span class=\"mo\" id=\"MathJax-Span-987\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-988\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-989\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-990\"><span class=\"mrow\" id=\"MathJax-Span-991\"><span class=\"mtext\" id=\"MathJax-Span-992\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-993\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-994\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1024</span><span class=\"mo\" id=\"MathJax-Span-995\" style=\"font-family: STIXGeneral-Regular;\">,</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi><mo>=</mo><mn>2048</mn><mo>,</mo><mi>L</mi><mo>=</mo><mn>24</mn><mo>,</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>=</mo><mn>1024</mn><mo>,</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-107\">n = 2048, L = 24, d_{\\text{model}} = 1024,</script> <code class=\"language-plaintext highlighter-rouge\">float16</code>, KV cache ≈ 192 MB.</p>\n<h4 id=\"latency-effects\">Latency Effects</h4>\n<ul>\n  <li><strong>Prefill latency</strong> dominated by large GEMMs (matrix multiplications) <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-108-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-996\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-997\"><span class=\"mo\" id=\"MathJax-Span-998\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-108\">\\rightarrow</script> compute-bound.</li>\n  <li><strong>Decode latency</strong> dominated by KV cache reads and output projection <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-109-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-999\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1000\"><span class=\"mo\" id=\"MathJax-Span-1001\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-109\">\\rightarrow</script> memory-bound.</li>\n  <li>\n    <p>Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-110-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1002\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1003\"><span class=\"mi\" id=\"MathJax-Span-1004\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-110\">n</script> increases both phases:</p>\n\n    <ul>\n      <li><strong>In prefill:</strong> longer sequences multiply the quadratic term.</li>\n      <li><strong>In decode:</strong> KV cache memory footprint grows, possibly exceeding L2/L3 cache <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-111-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1005\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1006\"><span class=\"mo\" id=\"MathJax-Span-1007\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-111\">\\rightarrow</script> slower fetches.</li>\n    </ul>\n  </li>\n</ul>\n<p>Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-110-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1002\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1003\"><span class=\"mi\" id=\"MathJax-Span-1004\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-110\">n</script> increases both phases:</p>\n<ul>\n      <li><strong>In prefill:</strong> longer sequences multiply the quadratic term.</li>\n      <li><strong>In decode:</strong> KV cache memory footprint grows, possibly exceeding L2/L3 cache <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-111-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1005\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1006\"><span class=\"mo\" id=\"MathJax-Span-1007\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-111\">\\rightarrow</script> slower fetches.</li>\n    </ul>\n<h4 id=\"server-vs-on-device-1\">Server vs. On-Device</h4>\n<ul>\n  <li><strong>Server (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference)</strong>:\n    <ul>\n      <li>GPUs can handle long sequences for prefill, but decode can still be underutilized unless batching or speculative decoding is applied. KV cache often fits in GPU memory without issue.</li>\n      <li>TensorRT-LLM and DeepSpeed-Inference use fused attention kernels to reduce KV cache memory bandwidth costs, while vLLM leverages continuous batching to mask decode-phase stalls.</li>\n    </ul>\n  </li>\n  <li><strong>On-Device (<code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, GGML, ONNX Runtime Mobile)</strong>:\n    <ul>\n      <li>Sequence length is often capped (e.g., 512–1024 tokens) to avoid RAM exhaustion and to keep KV cache in CPU last-level cache. Even at <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization, longer contexts can stall inference.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> and GGML implement tiled KV cache layouts to keep hot data in L3 cache; ONNX Runtime Mobile sometimes offloads KV cache to DRAM when SRAM is insufficient, impacting latency.</li>\n    </ul>\n  </li>\n  <li><strong>Edge-specialized (LidarTLM, MediaPipe + TFLite LLMs)</strong>:\n    <ul>\n      <li>Sequences are kept short (e.g., 128–256 tokens) because LiDAR frame-based context naturally fits into smaller windows, allowing real-time inference.</li>\n      <li>On TFLite NPUs, static allocation of KV cache in SRAM ensures deterministic performance and avoids runtime allocation delays.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>GPUs can handle long sequences for prefill, but decode can still be underutilized unless batching or speculative decoding is applied. KV cache often fits in GPU memory without issue.</li>\n      <li>TensorRT-LLM and DeepSpeed-Inference use fused attention kernels to reduce KV cache memory bandwidth costs, while vLLM leverages continuous batching to mask decode-phase stalls.</li>\n    </ul>\n<ul>\n      <li>Sequence length is often capped (e.g., 512–1024 tokens) to avoid RAM exhaustion and to keep KV cache in CPU last-level cache. Even at <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization, longer contexts can stall inference.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> and GGML implement tiled KV cache layouts to keep hot data in L3 cache; ONNX Runtime Mobile sometimes offloads KV cache to DRAM when SRAM is insufficient, impacting latency.</li>\n    </ul>\n<ul>\n      <li>Sequences are kept short (e.g., 128–256 tokens) because LiDAR frame-based context naturally fits into smaller windows, allowing real-time inference.</li>\n      <li>On TFLite NPUs, static allocation of KV cache in SRAM ensures deterministic performance and avoids runtime allocation delays.</li>\n    </ul>",
      "contentMarkdown": "*   The sequence length nnn (number of tokens in context) directly controls both the **prefill phase** cost and the **decode phase** memory demands in transformer inference.\n*   Sequence length has a **relatively large runtime impact** since it changes the size of the entire compute graph and KV cache footprint.\n\n#### Computational Scaling\n\n*   **Prefill (parallelizable)**:\n    \n    *   Multi-head attention complexity for the encoder or decoder prefill step:\n    \n    O(n2⋅dmodel)O(n2⋅dmodel)\n    \n    O(n^2 \\\\cdot d\\_{\\\\text{model}})\n    *   Because all nnn tokens attend to all others, cost grows quadratically. Increasing sequence length from 2k to 4k tokens quadruples the attention score computation.\n*   **Decode (autoregressive)**:\n    \n    *   With KV caching, each new token attends to all past tokens:\n    \n    O(n⋅dmodel)O(n⋅dmodel)\n    \n    O(n \\\\cdot d\\_{\\\\text{model}})\n    *   This is linear per token, but multiplied across ngenngenn\\_{\\\\text{gen}} output tokens.\n\n*   Multi-head attention complexity for the encoder or decoder prefill step:\n\n*   Because all nnn tokens attend to all others, cost grows quadratically. Increasing sequence length from 2k to 4k tokens quadruples the attention score computation.\n\n*   With KV caching, each new token attends to all past tokens:\n\n*   This is linear per token, but multiplied across ngenngenn\\_{\\\\text{gen}} output tokens.\n\n#### KV Cache Size and Memory Footprint\n\n*   In autoregressive decoding, the KV cache stores keys and values for each token at each layer:\n    \n    KVsize≈n×L×2×dmodel×precisionKVsize≈n×L×2×dmodel×precision\n    \n    KV\\_{size} \\\\approx n \\\\times L \\\\times 2 \\\\times d\\_{\\\\text{model}} \\\\times \\\\text{precision}\n    *   where:\n        \n        *   nnn: sequence length so far\n        *   LLL: number of layers\n        *   Factor 2: separate KKK and VVV storage\n        *   precision: bytes per element (e.g., 2 for `float16`, 1 for int8)\n*   Example: For n\\=2048,L\\=24,dmodel\\=1024,n\\=2048,L\\=24,dmodel\\=1024,n = 2048, L = 24, d\\_{\\\\text{model}} = 1024, `float16`, KV cache ≈ 192 MB.\n    \n\nIn autoregressive decoding, the KV cache stores keys and values for each token at each layer:\n\n*   where:\n    \n    *   nnn: sequence length so far\n    *   LLL: number of layers\n    *   Factor 2: separate KKK and VVV storage\n    *   precision: bytes per element (e.g., 2 for `float16`, 1 for int8)\n\nwhere:\n\n*   nnn: sequence length so far\n*   LLL: number of layers\n*   Factor 2: separate KKK and VVV storage\n*   precision: bytes per element (e.g., 2 for `float16`, 1 for int8)\n\nExample: For n\\=2048,L\\=24,dmodel\\=1024,n\\=2048,L\\=24,dmodel\\=1024,n = 2048, L = 24, d\\_{\\\\text{model}} = 1024, `float16`, KV cache ≈ 192 MB.\n\n#### Latency Effects\n\n*   **Prefill latency** dominated by large GEMMs (matrix multiplications) →→\\\\rightarrow compute-bound.\n*   **Decode latency** dominated by KV cache reads and output projection →→\\\\rightarrow memory-bound.\n*   Large nnn increases both phases:\n    \n    *   **In prefill:** longer sequences multiply the quadratic term.\n    *   **In decode:** KV cache memory footprint grows, possibly exceeding L2/L3 cache →→\\\\rightarrow slower fetches.\n\nLarge nnn increases both phases:\n\n*   **In prefill:** longer sequences multiply the quadratic term.\n*   **In decode:** KV cache memory footprint grows, possibly exceeding L2/L3 cache →→\\\\rightarrow slower fetches.\n\n#### Server vs. On-Device\n\n*   **Server (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference)**:\n    *   GPUs can handle long sequences for prefill, but decode can still be underutilized unless batching or speculative decoding is applied. KV cache often fits in GPU memory without issue.\n    *   TensorRT-LLM and DeepSpeed-Inference use fused attention kernels to reduce KV cache memory bandwidth costs, while vLLM leverages continuous batching to mask decode-phase stalls.\n*   **On-Device (`llama.cpp`, GGML, ONNX Runtime Mobile)**:\n    *   Sequence length is often capped (e.g., 512–1024 tokens) to avoid RAM exhaustion and to keep KV cache in CPU last-level cache. Even at `int8` quantization, longer contexts can stall inference.\n    *   `llama.cpp` and GGML implement tiled KV cache layouts to keep hot data in L3 cache; ONNX Runtime Mobile sometimes offloads KV cache to DRAM when SRAM is insufficient, impacting latency.\n*   **Edge-specialized (LidarTLM, MediaPipe + TFLite LLMs)**:\n    *   Sequences are kept short (e.g., 128–256 tokens) because LiDAR frame-based context naturally fits into smaller windows, allowing real-time inference.\n    *   On TFLite NPUs, static allocation of KV cache in SRAM ensures deterministic performance and avoids runtime allocation delays.\n\n*   GPUs can handle long sequences for prefill, but decode can still be underutilized unless batching or speculative decoding is applied. KV cache often fits in GPU memory without issue.\n*   TensorRT-LLM and DeepSpeed-Inference use fused attention kernels to reduce KV cache memory bandwidth costs, while vLLM leverages continuous batching to mask decode-phase stalls.\n\n*   Sequence length is often capped (e.g., 512–1024 tokens) to avoid RAM exhaustion and to keep KV cache in CPU last-level cache. Even at `int8` quantization, longer contexts can stall inference.\n*   `llama.cpp` and GGML implement tiled KV cache layouts to keep hot data in L3 cache; ONNX Runtime Mobile sometimes offloads KV cache to DRAM when SRAM is insufficient, impacting latency.\n\n*   Sequences are kept short (e.g., 128–256 tokens) because LiDAR frame-based context naturally fits into smaller windows, allowing real-time inference.\n*   On TFLite NPUs, static allocation of KV cache in SRAM ensures deterministic performance and avoids runtime allocation delays.",
      "order": 24,
      "orderInChapter": 3,
      "difficulty": 4,
      "estimatedMinutes": 4,
      "tags": [
        "ondevice ai",
        "transformer",
        "attention",
        "gpt",
        "llm"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 791,
        "contentLength": 56130
      },
      "nextCards": [
        "ai-on-device-transformers-parameter-count-and-model-depth-25",
        "ai-on-device-transformers-embedding-size-times-vocabulary-size-times-depth-26"
      ],
      "relatedCards": [
        "ai-model-compression-modern-quantization-techniques-13",
        "ai-model-compression-multimodal-quantization-14",
        "ai-model-compression-mitigation-strategies-8",
        "ai-model-compression-further-reading-21",
        "ai-federated-learning-what-is-federated-lora-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#sequence-length-and-kv-cache-size",
      "scrapedAt": "2025-12-28T11:56:11.408Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-parameter-count-and-model-depth-25",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Parameter Choices and Their Runtime Implications in Transformer Architectures",
      "title": "Parameter Count and Model Depth",
      "subtitle": "Parameter Choices and Their Runtime Implications in Transformer Architectures",
      "contentHtml": "<ul>\n  <li>The total parameter count <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-112-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1008\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1009\"><span class=\"mi\" id=\"MathJax-Span-1010\" style=\"font-family: STIXGeneral-Italic;\">P</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-112\">P</script> and model depth <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-113-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1011\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1012\"><span class=\"mi\" id=\"MathJax-Span-1013\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-113\">L</script> (number of transformer blocks) determine the model’s representational capacity and directly affect runtime.</li>\n  <li>Doubling <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-114-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1014\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1015\"><span class=\"mi\" id=\"MathJax-Span-1016\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-114\">L</script> roughly doubles per-token latency.</li>\n</ul>\n<h4 id=\"parameter-count-decomposition\">Parameter Count Decomposition</h4>\n<ul>\n  <li>\n    <p>A rough parameter breakdown for a decoder-only transformer:</p>\n\n    <ol>\n      <li>\n        <p><strong>Embedding matrices:</strong></p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-115-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>embed</mtext></mrow></msub><mo>=</mo><mi>V</mi><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1017\" style=\"width: 9.378em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.815em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1007.82em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1018\"><span class=\"msubsup\" id=\"MathJax-Span-1019\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1020\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1021\"><span class=\"mrow\" id=\"MathJax-Span-1022\"><span class=\"mtext\" id=\"MathJax-Span-1023\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">embed</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1024\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1025\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1026\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1027\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1028\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1029\"><span class=\"mrow\" id=\"MathJax-Span-1030\"><span class=\"mtext\" id=\"MathJax-Span-1031\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>embed</mtext></mrow></msub><mo>=</mo><mi>V</mi><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-115\">P_{\\text{embed}} = V \\cdot d_{\\text{model}}</script>\n      </li>\n      <li>\n        <p><strong>Per-layer parameters:</strong></p>\n\n        <ul>\n          <li>\n            <p>Attention projections (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-116-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1032\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1033\"><span class=\"mi\" id=\"MathJax-Span-1034\" style=\"font-family: STIXGeneral-Italic;\">Q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-116\">Q</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-117-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1035\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1036\"><span class=\"mi\" id=\"MathJax-Span-1037\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-117\">K</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-118-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1038\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1039\"><span class=\"mi\" id=\"MathJax-Span-1040\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-118\">V</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-119-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1041\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1042\"><span class=\"mi\" id=\"MathJax-Span-1043\" style=\"font-family: STIXGeneral-Italic;\">O</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-119\">O</script>):</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-120-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mn>4</mn><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow><mn>2</mn></msubsup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1044\" style=\"width: 4.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1003.8em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1045\"><span class=\"mn\" id=\"MathJax-Span-1046\" style=\"font-family: STIXGeneral-Regular;\">4</span><span class=\"mo\" id=\"MathJax-Span-1047\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1048\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1049\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1050\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.83em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1051\"><span class=\"mrow\" id=\"MathJax-Span-1052\"><span class=\"mtext\" id=\"MathJax-Span-1053\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mn>4</mn><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow><mn>2</mn></msubsup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-120\">4 \\cdot d_{\\text{model}}^2</script>\n          </li>\n          <li>\n            <p>Feedforward networks:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-121-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mn>2</mn><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>ff</mtext></mrow></msub><mo>&amp;#x2248;</mo><mn>8</mn><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow><mn>2</mn></msubsup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1054\" style=\"width: 13.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1010.84em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1055\"><span class=\"mn\" id=\"MathJax-Span-1056\" style=\"font-family: STIXGeneral-Regular;\">2</span><span class=\"mo\" id=\"MathJax-Span-1057\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1058\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1059\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1060\"><span class=\"mrow\" id=\"MathJax-Span-1061\"><span class=\"mtext\" id=\"MathJax-Span-1062\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1063\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1064\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1065\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1066\"><span class=\"mrow\" id=\"MathJax-Span-1067\"><span class=\"mtext\" id=\"MathJax-Span-1068\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">ff</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1069\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mn\" id=\"MathJax-Span-1070\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span><span class=\"mo\" id=\"MathJax-Span-1071\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1072\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1073\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1074\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.83em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1075\"><span class=\"mrow\" id=\"MathJax-Span-1076\"><span class=\"mtext\" id=\"MathJax-Span-1077\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mn>2</mn><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>ff</mtext></mrow></msub><mo>≈</mo><mn>8</mn><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow><mn>2</mn></msubsup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-121\">2 \\cdot d_{\\text{model}} \\cdot d_{\\text{ff}} \\approx 8 \\cdot d_{\\text{model}}^2</script>\n          </li>\n          <li>\n            <p>LayerNorm &amp; biases: negligible in size</p>\n          </li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Total model parameters:</strong></p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-122-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>total</mtext></mrow></msub><mo>&amp;#x2248;</mo><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>embed</mtext></mrow></msub><mo>+</mo><mi>L</mi><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>(</mo><mn>12</mn><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1078\" style=\"width: 16.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.336em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1013.28em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1079\"><span class=\"msubsup\" id=\"MathJax-Span-1080\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1081\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1082\"><span class=\"mrow\" id=\"MathJax-Span-1083\"><span class=\"mtext\" id=\"MathJax-Span-1084\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">total</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1085\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"msubsup\" id=\"MathJax-Span-1086\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1087\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1088\"><span class=\"mrow\" id=\"MathJax-Span-1089\"><span class=\"mtext\" id=\"MathJax-Span-1090\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">embed</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1091\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-1092\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1093\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-1094\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">(</span><span class=\"mn\" id=\"MathJax-Span-1095\" style=\"font-family: STIXGeneral-Regular;\">12</span><span class=\"mo\" id=\"MathJax-Span-1096\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1097\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1098\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1099\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.83em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1100\"><span class=\"mrow\" id=\"MathJax-Span-1101\"><span class=\"mtext\" id=\"MathJax-Span-1102\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1103\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>total</mtext></mrow></msub><mo>≈</mo><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>embed</mtext></mrow></msub><mo>+</mo><mi>L</mi><mo>⋅</mo><mo stretchy=\"false\">(</mo><mn>12</mn><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-122\">P_{\\text{total}} \\approx P_{\\text{embed}} + L \\cdot (12 \\cdot d_{\\text{model}}^2)</script>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p>This shows that increasing <strong>depth <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-123-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1104\" style=\"width: 0.777em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.622em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.62em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1105\"><span class=\"mi\" id=\"MathJax-Span-1106\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-123\">L</script></strong> linearly scales parameter count and compute cost, whereas increasing <strong><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-124-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1107\" style=\"width: 2.843em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.327em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.294em, 1002.33em, 2.43em, -999.997em); top: -2.115em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1108\"><span class=\"msubsup\" id=\"MathJax-Span-1109\"><span style=\"display: inline-block; position: relative; width: 2.327em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.52em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1110\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"texatom\" id=\"MathJax-Span-1111\"><span class=\"mrow\" id=\"MathJax-Span-1112\"><span class=\"mtext\" id=\"MathJax-Span-1113\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.12em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-124\">d_{\\text{model}}</script></strong> scales cost quadratically.</p>\n  </li>\n</ul>\n<p>A rough parameter breakdown for a decoder-only transformer:</p>\n<ol>\n      <li>\n        <p><strong>Embedding matrices:</strong></p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-115-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>embed</mtext></mrow></msub><mo>=</mo><mi>V</mi><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1017\" style=\"width: 9.378em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.815em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1007.82em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1018\"><span class=\"msubsup\" id=\"MathJax-Span-1019\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1020\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1021\"><span class=\"mrow\" id=\"MathJax-Span-1022\"><span class=\"mtext\" id=\"MathJax-Span-1023\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">embed</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1024\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-1025\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1026\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1027\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1028\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1029\"><span class=\"mrow\" id=\"MathJax-Span-1030\"><span class=\"mtext\" id=\"MathJax-Span-1031\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>embed</mtext></mrow></msub><mo>=</mo><mi>V</mi><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-115\">P_{\\text{embed}} = V \\cdot d_{\\text{model}}</script>\n      </li>\n      <li>\n        <p><strong>Per-layer parameters:</strong></p>\n\n        <ul>\n          <li>\n            <p>Attention projections (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-116-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1032\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1033\"><span class=\"mi\" id=\"MathJax-Span-1034\" style=\"font-family: STIXGeneral-Italic;\">Q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-116\">Q</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-117-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1035\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1036\"><span class=\"mi\" id=\"MathJax-Span-1037\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-117\">K</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-118-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1038\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1039\"><span class=\"mi\" id=\"MathJax-Span-1040\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-118\">V</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-119-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1041\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1042\"><span class=\"mi\" id=\"MathJax-Span-1043\" style=\"font-family: STIXGeneral-Italic;\">O</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-119\">O</script>):</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-120-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mn>4</mn><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow><mn>2</mn></msubsup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1044\" style=\"width: 4.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1003.8em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1045\"><span class=\"mn\" id=\"MathJax-Span-1046\" style=\"font-family: STIXGeneral-Regular;\">4</span><span class=\"mo\" id=\"MathJax-Span-1047\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1048\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1049\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1050\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.83em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1051\"><span class=\"mrow\" id=\"MathJax-Span-1052\"><span class=\"mtext\" id=\"MathJax-Span-1053\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mn>4</mn><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow><mn>2</mn></msubsup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-120\">4 \\cdot d_{\\text{model}}^2</script>\n          </li>\n          <li>\n            <p>Feedforward networks:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-121-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mn>2</mn><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>ff</mtext></mrow></msub><mo>&amp;#x2248;</mo><mn>8</mn><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow><mn>2</mn></msubsup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1054\" style=\"width: 13.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1010.84em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1055\"><span class=\"mn\" id=\"MathJax-Span-1056\" style=\"font-family: STIXGeneral-Regular;\">2</span><span class=\"mo\" id=\"MathJax-Span-1057\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1058\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1059\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1060\"><span class=\"mrow\" id=\"MathJax-Span-1061\"><span class=\"mtext\" id=\"MathJax-Span-1062\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1063\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1064\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1065\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1066\"><span class=\"mrow\" id=\"MathJax-Span-1067\"><span class=\"mtext\" id=\"MathJax-Span-1068\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">ff</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1069\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mn\" id=\"MathJax-Span-1070\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span><span class=\"mo\" id=\"MathJax-Span-1071\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1072\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1073\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1074\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.83em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1075\"><span class=\"mrow\" id=\"MathJax-Span-1076\"><span class=\"mtext\" id=\"MathJax-Span-1077\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mn>2</mn><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>ff</mtext></mrow></msub><mo>≈</mo><mn>8</mn><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow><mn>2</mn></msubsup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-121\">2 \\cdot d_{\\text{model}} \\cdot d_{\\text{ff}} \\approx 8 \\cdot d_{\\text{model}}^2</script>\n          </li>\n          <li>\n            <p>LayerNorm &amp; biases: negligible in size</p>\n          </li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Total model parameters:</strong></p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-122-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>total</mtext></mrow></msub><mo>&amp;#x2248;</mo><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>embed</mtext></mrow></msub><mo>+</mo><mi>L</mi><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>(</mo><mn>12</mn><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1078\" style=\"width: 16.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.336em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1013.28em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1079\"><span class=\"msubsup\" id=\"MathJax-Span-1080\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1081\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1082\"><span class=\"mrow\" id=\"MathJax-Span-1083\"><span class=\"mtext\" id=\"MathJax-Span-1084\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">total</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1085\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"msubsup\" id=\"MathJax-Span-1086\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.555em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1087\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1088\"><span class=\"mrow\" id=\"MathJax-Span-1089\"><span class=\"mtext\" id=\"MathJax-Span-1090\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">embed</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1091\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-1092\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1093\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-1094\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">(</span><span class=\"mn\" id=\"MathJax-Span-1095\" style=\"font-family: STIXGeneral-Regular;\">12</span><span class=\"mo\" id=\"MathJax-Span-1096\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1097\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1098\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1099\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.83em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1100\"><span class=\"mrow\" id=\"MathJax-Span-1101\"><span class=\"mtext\" id=\"MathJax-Span-1102\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1103\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>total</mtext></mrow></msub><mo>≈</mo><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>embed</mtext></mrow></msub><mo>+</mo><mi>L</mi><mo>⋅</mo><mo stretchy=\"false\">(</mo><mn>12</mn><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-122\">P_{\\text{total}} \\approx P_{\\text{embed}} + L \\cdot (12 \\cdot d_{\\text{model}}^2)</script>\n      </li>\n    </ol>\n<p><strong>Embedding matrices:</strong></p>\n<p><strong>Per-layer parameters:</strong></p>\n<ul>\n          <li>\n            <p>Attention projections (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-116-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1032\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1033\"><span class=\"mi\" id=\"MathJax-Span-1034\" style=\"font-family: STIXGeneral-Italic;\">Q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-116\">Q</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-117-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1035\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1036\"><span class=\"mi\" id=\"MathJax-Span-1037\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-117\">K</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-118-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1038\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1039\"><span class=\"mi\" id=\"MathJax-Span-1040\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-118\">V</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-119-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1041\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1042\"><span class=\"mi\" id=\"MathJax-Span-1043\" style=\"font-family: STIXGeneral-Italic;\">O</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-119\">O</script>):</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-120-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mn>4</mn><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow><mn>2</mn></msubsup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1044\" style=\"width: 4.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.805em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1003.8em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1045\"><span class=\"mn\" id=\"MathJax-Span-1046\" style=\"font-family: STIXGeneral-Regular;\">4</span><span class=\"mo\" id=\"MathJax-Span-1047\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1048\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1049\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1050\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.83em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1051\"><span class=\"mrow\" id=\"MathJax-Span-1052\"><span class=\"mtext\" id=\"MathJax-Span-1053\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mn>4</mn><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow><mn>2</mn></msubsup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-120\">4 \\cdot d_{\\text{model}}^2</script>\n          </li>\n          <li>\n            <p>Feedforward networks:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-121-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mn>2</mn><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>ff</mtext></mrow></msub><mo>&amp;#x2248;</mo><mn>8</mn><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow><mn>2</mn></msubsup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1054\" style=\"width: 13.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1010.84em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1055\"><span class=\"mn\" id=\"MathJax-Span-1056\" style=\"font-family: STIXGeneral-Regular;\">2</span><span class=\"mo\" id=\"MathJax-Span-1057\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1058\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1059\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1060\"><span class=\"mrow\" id=\"MathJax-Span-1061\"><span class=\"mtext\" id=\"MathJax-Span-1062\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1063\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1064\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1065\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1066\"><span class=\"mrow\" id=\"MathJax-Span-1067\"><span class=\"mtext\" id=\"MathJax-Span-1068\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">ff</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1069\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mn\" id=\"MathJax-Span-1070\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8</span><span class=\"mo\" id=\"MathJax-Span-1071\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1072\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1073\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1074\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.83em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1075\"><span class=\"mrow\" id=\"MathJax-Span-1076\"><span class=\"mtext\" id=\"MathJax-Span-1077\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mn>2</mn><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>ff</mtext></mrow></msub><mo>≈</mo><mn>8</mn><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow><mn>2</mn></msubsup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-121\">2 \\cdot d_{\\text{model}} \\cdot d_{\\text{ff}} \\approx 8 \\cdot d_{\\text{model}}^2</script>\n          </li>\n          <li>\n            <p>LayerNorm &amp; biases: negligible in size</p>\n          </li>\n        </ul>\n<p>Attention projections (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-116-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Q</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1032\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1033\"><span class=\"mi\" id=\"MathJax-Span-1034\" style=\"font-family: STIXGeneral-Italic;\">Q</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Q</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-116\">Q</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-117-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>K</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1035\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1036\"><span class=\"mi\" id=\"MathJax-Span-1037\" style=\"font-family: STIXGeneral-Italic;\">K<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-117\">K</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-118-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1038\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1039\"><span class=\"mi\" id=\"MathJax-Span-1040\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-118\">V</script>, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-119-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1041\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1042\"><span class=\"mi\" id=\"MathJax-Span-1043\" style=\"font-family: STIXGeneral-Italic;\">O</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-119\">O</script>):</p>\n<p>Feedforward networks:</p>\n<p>LayerNorm &amp; biases: negligible in size</p>\n<p><strong>Total model parameters:</strong></p>\n<p>This shows that increasing <strong>depth <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-123-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1104\" style=\"width: 0.777em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.622em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.62em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1105\"><span class=\"mi\" id=\"MathJax-Span-1106\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-123\">L</script></strong> linearly scales parameter count and compute cost, whereas increasing <strong><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-124-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1107\" style=\"width: 2.843em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.327em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.294em, 1002.33em, 2.43em, -999.997em); top: -2.115em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1108\"><span class=\"msubsup\" id=\"MathJax-Span-1109\"><span style=\"display: inline-block; position: relative; width: 2.327em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.52em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1110\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"texatom\" id=\"MathJax-Span-1111\"><span class=\"mrow\" id=\"MathJax-Span-1112\"><span class=\"mtext\" id=\"MathJax-Span-1113\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.12em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-124\">d_{\\text{model}}</script></strong> scales cost quadratically.</p>\n<h4 id=\"runtime-scaling-with-depth\">Runtime Scaling with Depth</h4>\n<ul>\n  <li><strong>Prefill phase:</strong> Cost scales as <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-125-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>L</mi><mo>&amp;#x22C5;</mo><msup><mi>n</mi><mn>2</mn></msup><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mspace width=&quot;1em&quot; /><mtext>(attention)</mtext></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1114\" style=\"width: 15.107em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.555em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1012.5em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1115\"><span class=\"mi\" id=\"MathJax-Span-1116\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-1117\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1118\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1119\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1120\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1121\" style=\"font-family: STIXGeneral-Italic;\">n</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"mn\" id=\"MathJax-Span-1122\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1123\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1124\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1125\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1126\"><span class=\"mrow\" id=\"MathJax-Span-1127\"><span class=\"mtext\" id=\"MathJax-Span-1128\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1129\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mspace\" id=\"MathJax-Span-1130\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mtext\" id=\"MathJax-Span-1131\" style=\"font-family: STIXGeneral-Regular;\">(attention)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>L</mi><mo>⋅</mo><msup><mi>n</mi><mn>2</mn></msup><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo stretchy=\"false\">)</mo><mspace width=\"1em\"></mspace><mtext>(attention)</mtext></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-125\">O(L \\cdot n^2 \\cdot d_{\\text{model}}) \\quad \\text{(attention)}</script> plus <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-126-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>L</mi><mo>&amp;#x22C5;</mo><mi>n</mi><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=&quot;false&quot;>)</mo><mspace width=&quot;1em&quot; /><mtext>(FFN)</mtext></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1132\" style=\"width: 12.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.419em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1010.37em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1133\"><span class=\"mi\" id=\"MathJax-Span-1134\" style=\"font-family: STIXGeneral-Italic;\">O</span><span class=\"mo\" id=\"MathJax-Span-1135\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1136\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1137\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-1138\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">n</span><span class=\"mo\" id=\"MathJax-Span-1139\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1140\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1141\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1142\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.83em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1143\"><span class=\"mrow\" id=\"MathJax-Span-1144\"><span class=\"mtext\" id=\"MathJax-Span-1145\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1146\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mspace\" id=\"MathJax-Span-1147\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mtext\" id=\"MathJax-Span-1148\" style=\"font-family: STIXGeneral-Regular;\">(FFN)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>L</mi><mo>⋅</mo><mi>n</mi><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo><mspace width=\"1em\"></mspace><mtext>(FFN)</mtext></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-126\">O(L \\cdot n \\cdot d_{\\text{model}}^2) \\quad \\text{(FFN)}</script>.</li>\n  <li><strong>Decode phase:</strong> Each new token must pass through all <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-127-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1149\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1150\"><span class=\"mi\" id=\"MathJax-Span-1151\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-127\">L</script> layers sequentially, so latency per token scales linearly with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-128-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1152\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1153\"><span class=\"mi\" id=\"MathJax-Span-1154\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-128\">L</script>.</li>\n</ul>\n<h4 id=\"accuracy-vs-latency-tradeoff\">Accuracy vs. Latency Tradeoff</h4>\n<ul>\n  <li><strong>More layers</strong> generally yield better performance and longer-range reasoning, but each layer adds sequential compute steps during decoding.</li>\n  <li><strong>Shallow networks</strong> are much faster for autoregressive decoding but can struggle with complex reasoning or long-range dependencies.</li>\n</ul>\n<h4 id=\"server-vs-on-device-2\">Server vs. On-Device</h4>\n<ul>\n  <li>\n    <p><strong>Server (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference)</strong>: Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-129-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1155\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1156\"><span class=\"mi\" id=\"MathJax-Span-1157\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-129\">L</script> (24–48) is feasible. GPUs can hide some latency through parallelization across layers with pipeline or tensor parallelism. The prefill stage dominates for long contexts.</p>\n  </li>\n  <li>\n    <p><strong>On-Device (<code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, GGML, ONNX Runtime Mobile)</strong>: Depth often limited to 8–16 layers for real-time inference. Too many layers <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-130-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1158\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1159\"><span class=\"mo\" id=\"MathJax-Span-1160\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-130\">\\rightarrow</script> sequential decode bottleneck on CPU or NPU. Quantization helps memory but not sequential compute cost.</p>\n  </li>\n  <li>\n    <p><strong>Edge-specialized (LidarTLM, MediaPipe + TFLite LLMs)</strong>: Extremely shallow depths (4–8 layers) to maintain low latency for sensor-fusion loops. Accuracy loss offset by task-specific training and limited context size.</p>\n  </li>\n</ul>\n<p><strong>Server (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference)</strong>: Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-129-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1155\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1156\"><span class=\"mi\" id=\"MathJax-Span-1157\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-129\">L</script> (24–48) is feasible. GPUs can hide some latency through parallelization across layers with pipeline or tensor parallelism. The prefill stage dominates for long contexts.</p>\n<p><strong>On-Device (<code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, GGML, ONNX Runtime Mobile)</strong>: Depth often limited to 8–16 layers for real-time inference. Too many layers <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-130-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1158\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1159\"><span class=\"mo\" id=\"MathJax-Span-1160\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-130\">\\rightarrow</script> sequential decode bottleneck on CPU or NPU. Quantization helps memory but not sequential compute cost.</p>\n<p><strong>Edge-specialized (LidarTLM, MediaPipe + TFLite LLMs)</strong>: Extremely shallow depths (4–8 layers) to maintain low latency for sensor-fusion loops. Accuracy loss offset by task-specific training and limited context size.</p>",
      "contentMarkdown": "*   The total parameter count PPP and model depth LLL (number of transformer blocks) determine the model’s representational capacity and directly affect runtime.\n*   Doubling LLL roughly doubles per-token latency.\n\n#### Parameter Count Decomposition\n\n*   A rough parameter breakdown for a decoder-only transformer:\n    \n    1.  **Embedding matrices:**\n        \n        Pembed\\=V⋅dmodelPembed\\=V⋅dmodel\n        \n        P\\_{\\\\text{embed}} = V \\\\cdot d\\_{\\\\text{model}}\n    2.  **Per-layer parameters:**\n        \n        *   Attention projections (QQQ, KKK, VVV, OOO):\n            \n            4⋅d2model4⋅dmodel2\n            \n            4 \\\\cdot d\\_{\\\\text{model}}^2\n        *   Feedforward networks:\n            \n            2⋅dmodel⋅dff≈8⋅d2model2⋅dmodel⋅dff≈8⋅dmodel2\n            \n            2 \\\\cdot d\\_{\\\\text{model}} \\\\cdot d\\_{\\\\text{ff}} \\\\approx 8 \\\\cdot d\\_{\\\\text{model}}^2\n        *   LayerNorm & biases: negligible in size\n            \n    3.  **Total model parameters:**\n        \n        Ptotal≈Pembed+L⋅(12⋅d2model)Ptotal≈Pembed+L⋅(12⋅dmodel2)\n        \n        P\\_{\\\\text{total}} \\\\approx P\\_{\\\\text{embed}} + L \\\\cdot (12 \\\\cdot d\\_{\\\\text{model}}^2)\n*   This shows that increasing **depth LLL** linearly scales parameter count and compute cost, whereas increasing **dmodeldmodeld\\_{\\\\text{model}}** scales cost quadratically.\n    \n\nA rough parameter breakdown for a decoder-only transformer:\n\n1.  **Embedding matrices:**\n    \n    Pembed\\=V⋅dmodelPembed\\=V⋅dmodel\n    \n    P\\_{\\\\text{embed}} = V \\\\cdot d\\_{\\\\text{model}}\n2.  **Per-layer parameters:**\n    \n    *   Attention projections (QQQ, KKK, VVV, OOO):\n        \n        4⋅d2model4⋅dmodel2\n        \n        4 \\\\cdot d\\_{\\\\text{model}}^2\n    *   Feedforward networks:\n        \n        2⋅dmodel⋅dff≈8⋅d2model2⋅dmodel⋅dff≈8⋅dmodel2\n        \n        2 \\\\cdot d\\_{\\\\text{model}} \\\\cdot d\\_{\\\\text{ff}} \\\\approx 8 \\\\cdot d\\_{\\\\text{model}}^2\n    *   LayerNorm & biases: negligible in size\n        \n3.  **Total model parameters:**\n    \n    Ptotal≈Pembed+L⋅(12⋅d2model)Ptotal≈Pembed+L⋅(12⋅dmodel2)\n    \n    P\\_{\\\\text{total}} \\\\approx P\\_{\\\\text{embed}} + L \\\\cdot (12 \\\\cdot d\\_{\\\\text{model}}^2)\n\n**Embedding matrices:**\n\n**Per-layer parameters:**\n\n*   Attention projections (QQQ, KKK, VVV, OOO):\n    \n    4⋅d2model4⋅dmodel2\n    \n    4 \\\\cdot d\\_{\\\\text{model}}^2\n*   Feedforward networks:\n    \n    2⋅dmodel⋅dff≈8⋅d2model2⋅dmodel⋅dff≈8⋅dmodel2\n    \n    2 \\\\cdot d\\_{\\\\text{model}} \\\\cdot d\\_{\\\\text{ff}} \\\\approx 8 \\\\cdot d\\_{\\\\text{model}}^2\n*   LayerNorm & biases: negligible in size\n    \n\nAttention projections (QQQ, KKK, VVV, OOO):\n\nFeedforward networks:\n\nLayerNorm & biases: negligible in size\n\n**Total model parameters:**\n\nThis shows that increasing **depth LLL** linearly scales parameter count and compute cost, whereas increasing **dmodeldmodeld\\_{\\\\text{model}}** scales cost quadratically.\n\n#### Runtime Scaling with Depth\n\n*   **Prefill phase:** Cost scales as O(L⋅n2⋅dmodel)(attention)O(L⋅n2⋅dmodel)(attention)O(L \\\\cdot n^2 \\\\cdot d\\_{\\\\text{model}}) \\\\quad \\\\text{(attention)} plus O(L⋅n⋅d2model)(FFN)O(L⋅n⋅dmodel2)(FFN)O(L \\\\cdot n \\\\cdot d\\_{\\\\text{model}}^2) \\\\quad \\\\text{(FFN)}.\n*   **Decode phase:** Each new token must pass through all LLL layers sequentially, so latency per token scales linearly with LLL.\n\n#### Accuracy vs. Latency Tradeoff\n\n*   **More layers** generally yield better performance and longer-range reasoning, but each layer adds sequential compute steps during decoding.\n*   **Shallow networks** are much faster for autoregressive decoding but can struggle with complex reasoning or long-range dependencies.\n\n#### Server vs. On-Device\n\n*   **Server (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference)**: Large LLL (24–48) is feasible. GPUs can hide some latency through parallelization across layers with pipeline or tensor parallelism. The prefill stage dominates for long contexts.\n    \n*   **On-Device (`llama.cpp`, GGML, ONNX Runtime Mobile)**: Depth often limited to 8–16 layers for real-time inference. Too many layers →→\\\\rightarrow sequential decode bottleneck on CPU or NPU. Quantization helps memory but not sequential compute cost.\n    \n*   **Edge-specialized (LidarTLM, MediaPipe + TFLite LLMs)**: Extremely shallow depths (4–8 layers) to maintain low latency for sensor-fusion loops. Accuracy loss offset by task-specific training and limited context size.\n    \n\n**Server (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference)**: Large LLL (24–48) is feasible. GPUs can hide some latency through parallelization across layers with pipeline or tensor parallelism. The prefill stage dominates for long contexts.\n\n**On-Device (`llama.cpp`, GGML, ONNX Runtime Mobile)**: Depth often limited to 8–16 layers for real-time inference. Too many layers →→\\\\rightarrow sequential decode bottleneck on CPU or NPU. Quantization helps memory but not sequential compute cost.\n\n**Edge-specialized (LidarTLM, MediaPipe + TFLite LLMs)**: Extremely shallow depths (4–8 layers) to maintain low latency for sensor-fusion loops. Accuracy loss offset by task-specific training and limited context size.",
      "order": 25,
      "orderInChapter": 4,
      "difficulty": 4,
      "estimatedMinutes": 3,
      "tags": [
        "ondevice ai",
        "transformer",
        "attention",
        "embedding",
        "gpt",
        "llm"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 538,
        "contentLength": 96723
      },
      "nextCards": [
        "ai-on-device-transformers-embedding-size-times-vocabulary-size-times-depth-26",
        "ai-on-device-transformers-parameter-tuning-recipes-for-ml-runtimes-27"
      ],
      "relatedCards": [
        "ai-model-compression-multimodal-quantization-14",
        "ai-model-compression-modern-quantization-techniques-13",
        "ai-model-compression-popular-quantization-libraries-19",
        "ai-RAG-response-generation-synthesis-5",
        "ai-model-compression-mitigation-strategies-8"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#parameter-count-and-model-depth",
      "scrapedAt": "2025-12-28T11:56:11.408Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-embedding-size-times-vocabulary-size-times-depth-26",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Parameter Choices and Their Runtime Implications in Transformer Architectures",
      "title": "Embedding Size ××\\times Vocabulary Size ××\\times Depth",
      "subtitle": "Parameter Choices and Their Runtime Implications in Transformer Architectures",
      "contentHtml": "<ul>\n  <li>So far, we’ve treated vocabulary size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-133-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1167\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1168\"><span class=\"mi\" id=\"MathJax-Span-1169\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-133\">V</script>, embedding size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-134-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1170\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1171\"><span class=\"msubsup\" id=\"MathJax-Span-1172\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1173\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1174\"><span class=\"mrow\" id=\"MathJax-Span-1175\"><span class=\"mtext\" id=\"MathJax-Span-1176\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-134\">d_{\\text{model}}</script>, and depth <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-135-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1177\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1178\"><span class=\"mi\" id=\"MathJax-Span-1179\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-135\">L</script> mostly in isolation. In reality, these parameters interact in ways that compound runtime and memory demands.</li>\n</ul>\n<h4 id=\"compound-parameter-scaling\">Compound Parameter Scaling</h4>\n<ul>\n  <li>By examining the compute vs. memory analysis for <a href=\"#tokenizer-and-vocabulary-size\">tokenizer and vocabulary size</a>, <a href=\"#modelembedding-dimension\">model/embedding size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-136-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1180\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1181\"><span class=\"msubsup\" id=\"MathJax-Span-1182\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1183\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1184\"><span class=\"mrow\" id=\"MathJax-Span-1185\"><span class=\"mi\" id=\"MathJax-Span-1186\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span><span class=\"mi\" id=\"MathJax-Span-1187\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">o</span><span class=\"mi\" id=\"MathJax-Span-1188\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-1189\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">e</span><span class=\"mi\" id=\"MathJax-Span-1190\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-136\">d_{model}</script></a>, <a href=\"#sequence-length-and-kv-cache-size\">sequence length and KV cache size</a>, and <a href=\"#parameter-count-and-model-depth\">model parameter count and depth <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-137-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1191\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1192\"><span class=\"mi\" id=\"MathJax-Span-1193\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-137\">L</script></a>, a decoder-only transformer’s <strong>core parameter count</strong> is approximately:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-138-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>total</mtext></mrow></msub><mo>&amp;#x2248;</mo><mi>V</mi><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>+</mo><mi>L</mi><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>(</mo><mn>12</mn><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1194\" style=\"width: 17.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1014.79em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1195\"><span class=\"msubsup\" id=\"MathJax-Span-1196\"><span style=\"display: inline-block; position: relative; width: 1.93em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1197\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-1198\"><span class=\"mrow\" id=\"MathJax-Span-1199\"><span class=\"mtext\" id=\"MathJax-Span-1200\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">total</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1201\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mi\" id=\"MathJax-Span-1202\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1203\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1204\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1205\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1206\"><span class=\"mrow\" id=\"MathJax-Span-1207\"><span class=\"mtext\" id=\"MathJax-Span-1208\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1209\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-1210\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1211\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-1212\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">(</span><span class=\"mn\" id=\"MathJax-Span-1213\" style=\"font-family: STIXGeneral-Regular;\">12</span><span class=\"mo\" id=\"MathJax-Span-1214\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1215\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1216\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1217\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.83em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1218\"><span class=\"mrow\" id=\"MathJax-Span-1219\"><span class=\"mtext\" id=\"MathJax-Span-1220\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1221\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>total</mtext></mrow></msub><mo>≈</mo><mi>V</mi><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>+</mo><mi>L</mi><mo>⋅</mo><mo stretchy=\"false\">(</mo><mn>12</mn><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>The <strong>output projection cost</strong> per token is:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-139-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>out</mtext></mrow></msub><mo>&amp;#x2248;</mo><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>V</mi><mo>&amp;#x22C5;</mo><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1222\" style=\"width: 10.003em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.336em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1008.28em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1223\"><span class=\"msubsup\" id=\"MathJax-Span-1224\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1225\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1226\"><span class=\"mrow\" id=\"MathJax-Span-1227\"><span class=\"mtext\" id=\"MathJax-Span-1228\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">out</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1229\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mi\" id=\"MathJax-Span-1230\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">O</span><span class=\"mo\" id=\"MathJax-Span-1231\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1232\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1233\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1234\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1235\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1236\"><span class=\"mrow\" id=\"MathJax-Span-1237\"><span class=\"mtext\" id=\"MathJax-Span-1238\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1239\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>out</mtext></mrow></msub><mo>≈</mo><mi>O</mi><mo stretchy=\"false\">(</mo><mi>V</mi><mo>⋅</mo><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>\n    <p>The <strong>layer cost</strong> per token is:</p>\n  </li>\n  <li>\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-140-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>layer</mtext></mrow></msub><mo>&amp;#x2248;</mo><mi>O</mi><mo stretchy=&quot;false&quot;>(</mo><mi>L</mi><mo>&amp;#x22C5;</mo><msubsup><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1240\" style=\"width: 10.419em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.648em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1008.6em, 2.711em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1241\"><span class=\"msubsup\" id=\"MathJax-Span-1242\"><span style=\"display: inline-block; position: relative; width: 2.138em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1243\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1244\"><span class=\"mrow\" id=\"MathJax-Span-1245\"><span class=\"mtext\" id=\"MathJax-Span-1246\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">layer</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1247\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≈</span><span class=\"mi\" id=\"MathJax-Span-1248\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">O</span><span class=\"mo\" id=\"MathJax-Span-1249\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1250\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1251\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1252\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1253\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.42em, 4.169em, -999.997em); top: -4.372em; left: 0.576em;\"><span class=\"mn\" id=\"MathJax-Span-1254\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.83em, 4.169em, -999.997em); top: -3.695em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1255\"><span class=\"mrow\" id=\"MathJax-Span-1256\"><span class=\"mtext\" id=\"MathJax-Span-1257\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1258\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.503em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>layer</mtext></mrow></msub><mo>≈</mo><mi>O</mi><mo stretchy=\"false\">(</mo><mi>L</mi><mo>⋅</mo><msubsup><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow><mn>2</mn></msubsup><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-140\">C_{\\text{layer}} \\approx O(L \\cdot d_{\\text{model}}^2)</script>\n  </li>\n  <li>\n    <p>This implies:</p>\n\n    <ul>\n      <li>Increasing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-141-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1259\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1260\"><span class=\"msubsup\" id=\"MathJax-Span-1261\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1262\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1263\"><span class=\"mrow\" id=\"MathJax-Span-1264\"><span class=\"mtext\" id=\"MathJax-Span-1265\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-141\">d_{\\text{model}}</script> scales <strong>both</strong> terms quadratically (via layers) and linearly (via vocab projection).</li>\n      <li>Increasing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-142-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1266\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1267\"><span class=\"mi\" id=\"MathJax-Span-1268\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-142\">V</script> scales only the embedding/output projection term, but that term appears <strong>every decoding step</strong>.</li>\n      <li>Increasing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-143-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1269\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1270\"><span class=\"mi\" id=\"MathJax-Span-1271\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-143\">L</script> increases sequential operations per token, which is critical for autoregressive decoding latency.</li>\n    </ul>\n  </li>\n</ul>\n<p>The <strong>layer cost</strong> per token is:</p>\n<p>This implies:</p>\n<ul>\n      <li>Increasing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-141-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1259\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1260\"><span class=\"msubsup\" id=\"MathJax-Span-1261\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1262\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1263\"><span class=\"mrow\" id=\"MathJax-Span-1264\"><span class=\"mtext\" id=\"MathJax-Span-1265\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-141\">d_{\\text{model}}</script> scales <strong>both</strong> terms quadratically (via layers) and linearly (via vocab projection).</li>\n      <li>Increasing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-142-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1266\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1267\"><span class=\"mi\" id=\"MathJax-Span-1268\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-142\">V</script> scales only the embedding/output projection term, but that term appears <strong>every decoding step</strong>.</li>\n      <li>Increasing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-143-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1269\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1270\"><span class=\"mi\" id=\"MathJax-Span-1271\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-143\">L</script> increases sequential operations per token, which is critical for autoregressive decoding latency.</li>\n    </ul>\n<h4 id=\"interactions-in-practice\">Interactions in Practice</h4>\n<ul>\n  <li><strong>Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-144-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1272\" style=\"width: 0.932em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.78em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1273\"><span class=\"mi\" id=\"MathJax-Span-1274\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-144\">V</script> + Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-145-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1275\" style=\"width: 2.843em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.327em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.294em, 1002.33em, 2.43em, -999.997em); top: -2.115em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1276\"><span class=\"msubsup\" id=\"MathJax-Span-1277\"><span style=\"display: inline-block; position: relative; width: 2.327em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.52em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1278\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"texatom\" id=\"MathJax-Span-1279\"><span class=\"mrow\" id=\"MathJax-Span-1280\"><span class=\"mtext\" id=\"MathJax-Span-1281\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.12em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-145\">d_{\\text{model}}</script></strong>: Output projection dominates decode latency, especially on memory-bound devices.</li>\n  <li><strong>Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-146-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1282\" style=\"width: 0.777em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.622em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.62em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1283\"><span class=\"mi\" id=\"MathJax-Span-1284\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-146\">L</script> + Long Sequence <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-147-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1285\" style=\"width: 0.622em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.519em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.552em, 1000.47em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1286\"><span class=\"mi\" id=\"MathJax-Span-1287\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-147\">n</script></strong>: Prefill becomes a quadratic wall due to attention; decode slows due to sequential depth.</li>\n  <li><strong>Small <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-148-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1288\" style=\"width: 0.932em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.78em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1289\"><span class=\"mi\" id=\"MathJax-Span-1290\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-148\">V</script> + Small <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-149-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1291\" style=\"width: 2.843em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.327em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.294em, 1002.33em, 2.43em, -999.997em); top: -2.115em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1292\"><span class=\"msubsup\" id=\"MathJax-Span-1293\"><span style=\"display: inline-block; position: relative; width: 2.327em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.52em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1294\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"texatom\" id=\"MathJax-Span-1295\"><span class=\"mrow\" id=\"MathJax-Span-1296\"><span class=\"mtext\" id=\"MathJax-Span-1297\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.12em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-149\">d_{\\text{model}}</script> + Moderate <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-150-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1298\" style=\"width: 0.777em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.622em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.62em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1299\"><span class=\"mi\" id=\"MathJax-Span-1300\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-150\">L</script></strong>: Often the sweet spot for edge devices — balanced compute and memory, but can sacrifice accuracy.</li>\n</ul>\n<h4 id=\"optimization-strategies-by-deployment-context\">Optimization Strategies by Deployment Context</h4>\n<ul>\n  <li>\n    <p><strong>Server (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference)</strong>:</p>\n\n    <ul>\n      <li>High <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-151-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1301\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1302\"><span class=\"mi\" id=\"MathJax-Span-1303\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-151\">V</script>, high <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-152-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1304\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1305\"><span class=\"msubsup\" id=\"MathJax-Span-1306\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1307\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1308\"><span class=\"mrow\" id=\"MathJax-Span-1309\"><span class=\"mtext\" id=\"MathJax-Span-1310\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-152\">d_{\\text{model}}</script>, large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-153-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1311\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1312\"><span class=\"mi\" id=\"MathJax-Span-1313\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-153\">L</script> are fine if GPUs are saturated.</li>\n      <li>Use fused output softmax kernels and half-precision Tensor Core paths.</li>\n      <li>Speculative decoding helps mitigate large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-154-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1314\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1315\"><span class=\"mi\" id=\"MathJax-Span-1316\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-154\">L</script> in decode.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>On-Device CPU (<code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, GGML, ONNX Runtime Mobile)</strong>:</p>\n\n    <ul>\n      <li>Reduce <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-155-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1317\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1318\"><span class=\"mi\" id=\"MathJax-Span-1319\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-155\">V</script> aggressively to cut output projection cost.</li>\n      <li>Moderate <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-156-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1320\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1321\"><span class=\"msubsup\" id=\"MathJax-Span-1322\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1323\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1324\"><span class=\"mrow\" id=\"MathJax-Span-1325\"><span class=\"mtext\" id=\"MathJax-Span-1326\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-156\">d_{\\text{model}}</script> (512–1024) to keep KV cache and GEMMs manageable.</li>\n      <li>Lower <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-157-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1327\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1328\"><span class=\"mi\" id=\"MathJax-Span-1329\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-157\">L</script> to speed sequential decode; apply quantization (<code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">int4</code>) to fit in RAM.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Edge NPU (LidarTLM, MediaPipe + TFLite LLMs, Qualcomm AI Engine Direct)</strong>:</p>\n\n    <ul>\n      <li>Task-specific <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-158-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1330\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1331\"><span class=\"mi\" id=\"MathJax-Span-1332\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-158\">V</script> (often &lt;10k) to minimize projection latency.</li>\n      <li>Small <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-159-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1333\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1334\"><span class=\"msubsup\" id=\"MathJax-Span-1335\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1336\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1337\"><span class=\"mrow\" id=\"MathJax-Span-1338\"><span class=\"mtext\" id=\"MathJax-Span-1339\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-159\">d_{\\text{model}}</script> (256–512) for throughput under strict power budgets.</li>\n      <li>Shallow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-160-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1340\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1341\"><span class=\"mi\" id=\"MathJax-Span-1342\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-160\">L</script> (4–8) to ensure deterministic real-time latency for sensor fusion loops.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Server (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference)</strong>:</p>\n<ul>\n      <li>High <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-151-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1301\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1302\"><span class=\"mi\" id=\"MathJax-Span-1303\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-151\">V</script>, high <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-152-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1304\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1305\"><span class=\"msubsup\" id=\"MathJax-Span-1306\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1307\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1308\"><span class=\"mrow\" id=\"MathJax-Span-1309\"><span class=\"mtext\" id=\"MathJax-Span-1310\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-152\">d_{\\text{model}}</script>, large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-153-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1311\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1312\"><span class=\"mi\" id=\"MathJax-Span-1313\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-153\">L</script> are fine if GPUs are saturated.</li>\n      <li>Use fused output softmax kernels and half-precision Tensor Core paths.</li>\n      <li>Speculative decoding helps mitigate large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-154-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1314\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1315\"><span class=\"mi\" id=\"MathJax-Span-1316\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-154\">L</script> in decode.</li>\n    </ul>\n<p><strong>On-Device CPU (<code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, GGML, ONNX Runtime Mobile)</strong>:</p>\n<ul>\n      <li>Reduce <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-155-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1317\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1318\"><span class=\"mi\" id=\"MathJax-Span-1319\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-155\">V</script> aggressively to cut output projection cost.</li>\n      <li>Moderate <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-156-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1320\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1321\"><span class=\"msubsup\" id=\"MathJax-Span-1322\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1323\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1324\"><span class=\"mrow\" id=\"MathJax-Span-1325\"><span class=\"mtext\" id=\"MathJax-Span-1326\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-156\">d_{\\text{model}}</script> (512–1024) to keep KV cache and GEMMs manageable.</li>\n      <li>Lower <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-157-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1327\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1328\"><span class=\"mi\" id=\"MathJax-Span-1329\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-157\">L</script> to speed sequential decode; apply quantization (<code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">int4</code>) to fit in RAM.</li>\n    </ul>\n<p><strong>Edge NPU (LidarTLM, MediaPipe + TFLite LLMs, Qualcomm AI Engine Direct)</strong>:</p>\n<ul>\n      <li>Task-specific <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-158-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1330\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1331\"><span class=\"mi\" id=\"MathJax-Span-1332\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-158\">V</script> (often &lt;10k) to minimize projection latency.</li>\n      <li>Small <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-159-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1333\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1334\"><span class=\"msubsup\" id=\"MathJax-Span-1335\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1336\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1337\"><span class=\"mrow\" id=\"MathJax-Span-1338\"><span class=\"mtext\" id=\"MathJax-Span-1339\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-159\">d_{\\text{model}}</script> (256–512) for throughput under strict power budgets.</li>\n      <li>Shallow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-160-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1340\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1341\"><span class=\"mi\" id=\"MathJax-Span-1342\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-160\">L</script> (4–8) to ensure deterministic real-time latency for sensor fusion loops.</li>\n    </ul>\n<h4 id=\"tradeoffs\">Tradeoffs</h4>\n<ul>\n  <li>Reducing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-161-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1343\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1344\"><span class=\"mi\" id=\"MathJax-Span-1345\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-161\">V</script> trades fewer softmax FLOPs for longer token sequences (more steps).</li>\n  <li>Reducing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-162-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1346\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1347\"><span class=\"msubsup\" id=\"MathJax-Span-1348\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1349\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1350\"><span class=\"mrow\" id=\"MathJax-Span-1351\"><span class=\"mtext\" id=\"MathJax-Span-1352\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-162\">d_{\\text{model}}</script> cuts compute cost per layer but reduces representation power.</li>\n  <li>Reducing <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-163-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1353\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1354\"><span class=\"mi\" id=\"MathJax-Span-1355\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-163\">L</script> cuts sequential latency but lowers reasoning depth and long-context handling.</li>\n</ul>",
      "contentMarkdown": "*   So far, we’ve treated vocabulary size VVV, embedding size dmodeldmodeld\\_{\\\\text{model}}, and depth LLL mostly in isolation. In reality, these parameters interact in ways that compound runtime and memory demands.\n\n#### Compound Parameter Scaling\n\n*   By examining the compute vs. memory analysis for [tokenizer and vocabulary size](#tokenizer-and-vocabulary-size), [model/embedding size dmodeldmodeld\\_{model}](#modelembedding-dimension), [sequence length and KV cache size](#sequence-length-and-kv-cache-size), and [model parameter count and depth LLL](#parameter-count-and-model-depth), a decoder-only transformer’s **core parameter count** is approximately:\n\nPtotal≈V⋅dmodel+L⋅(12⋅d2model)Ptotal≈V⋅dmodel+L⋅(12⋅dmodel2)\n\n*   The **output projection cost** per token is:\n\nCout≈O(V⋅dmodel)Cout≈O(V⋅dmodel)\n\n*   The **layer cost** per token is:\n    \n*   Clayer≈O(L⋅d2model)Clayer≈O(L⋅dmodel2)\n    \n    C\\_{\\\\text{layer}} \\\\approx O(L \\\\cdot d\\_{\\\\text{model}}^2)\n*   This implies:\n    \n    *   Increasing dmodeldmodeld\\_{\\\\text{model}} scales **both** terms quadratically (via layers) and linearly (via vocab projection).\n    *   Increasing VVV scales only the embedding/output projection term, but that term appears **every decoding step**.\n    *   Increasing LLL increases sequential operations per token, which is critical for autoregressive decoding latency.\n\nThe **layer cost** per token is:\n\nThis implies:\n\n*   Increasing dmodeldmodeld\\_{\\\\text{model}} scales **both** terms quadratically (via layers) and linearly (via vocab projection).\n*   Increasing VVV scales only the embedding/output projection term, but that term appears **every decoding step**.\n*   Increasing LLL increases sequential operations per token, which is critical for autoregressive decoding latency.\n\n#### Interactions in Practice\n\n*   **Large VVV + Large dmodeldmodeld\\_{\\\\text{model}}**: Output projection dominates decode latency, especially on memory-bound devices.\n*   **Large LLL + Long Sequence nnn**: Prefill becomes a quadratic wall due to attention; decode slows due to sequential depth.\n*   **Small VVV + Small dmodeldmodeld\\_{\\\\text{model}} + Moderate LLL**: Often the sweet spot for edge devices — balanced compute and memory, but can sacrifice accuracy.\n\n#### Optimization Strategies by Deployment Context\n\n*   **Server (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference)**:\n    \n    *   High VVV, high dmodeldmodeld\\_{\\\\text{model}}, large LLL are fine if GPUs are saturated.\n    *   Use fused output softmax kernels and half-precision Tensor Core paths.\n    *   Speculative decoding helps mitigate large LLL in decode.\n*   **On-Device CPU (`llama.cpp`, GGML, ONNX Runtime Mobile)**:\n    \n    *   Reduce VVV aggressively to cut output projection cost.\n    *   Moderate dmodeldmodeld\\_{\\\\text{model}} (512–1024) to keep KV cache and GEMMs manageable.\n    *   Lower LLL to speed sequential decode; apply quantization (`int8`/`int4`) to fit in RAM.\n*   **Edge NPU (LidarTLM, MediaPipe + TFLite LLMs, Qualcomm AI Engine Direct)**:\n    \n    *   Task-specific VVV (often <10k) to minimize projection latency.\n    *   Small dmodeldmodeld\\_{\\\\text{model}} (256–512) for throughput under strict power budgets.\n    *   Shallow LLL (4–8) to ensure deterministic real-time latency for sensor fusion loops.\n\n**Server (NanoGPT, TensorRT-LLM, vLLM, DeepSpeed-Inference)**:\n\n*   High VVV, high dmodeldmodeld\\_{\\\\text{model}}, large LLL are fine if GPUs are saturated.\n*   Use fused output softmax kernels and half-precision Tensor Core paths.\n*   Speculative decoding helps mitigate large LLL in decode.\n\n**On-Device CPU (`llama.cpp`, GGML, ONNX Runtime Mobile)**:\n\n*   Reduce VVV aggressively to cut output projection cost.\n*   Moderate dmodeldmodeld\\_{\\\\text{model}} (512–1024) to keep KV cache and GEMMs manageable.\n*   Lower LLL to speed sequential decode; apply quantization (`int8`/`int4`) to fit in RAM.\n\n**Edge NPU (LidarTLM, MediaPipe + TFLite LLMs, Qualcomm AI Engine Direct)**:\n\n*   Task-specific VVV (often <10k) to minimize projection latency.\n*   Small dmodeldmodeld\\_{\\\\text{model}} (256–512) for throughput under strict power budgets.\n*   Shallow LLL (4–8) to ensure deterministic real-time latency for sensor fusion loops.\n\n#### Tradeoffs\n\n*   Reducing VVV trades fewer softmax FLOPs for longer token sequences (more steps).\n*   Reducing dmodeldmodeld\\_{\\\\text{model}} cuts compute cost per layer but reduces representation power.\n*   Reducing LLL cuts sequential latency but lowers reasoning depth and long-context handling.",
      "order": 26,
      "orderInChapter": 5,
      "difficulty": 4,
      "estimatedMinutes": 3,
      "tags": [
        "ondevice ai",
        "transformer",
        "attention",
        "embedding",
        "gpt",
        "llm",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 554,
        "contentLength": 84330
      },
      "nextCards": [
        "ai-on-device-transformers-parameter-tuning-recipes-for-ml-runtimes-27"
      ],
      "relatedCards": [
        "ai-model-compression-multimodal-quantization-14",
        "ai-model-compression-modern-quantization-techniques-13",
        "ai-model-compression-further-reading-21",
        "ai-LLM-challenges-with-context-scaling-8",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#embedding-size-××\\times-vocabulary-size-××\\times-depth",
      "scrapedAt": "2025-12-28T11:56:11.408Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    },
    {
      "id": "ai-on-device-transformers-parameter-tuning-recipes-for-ml-runtimes-27",
      "domain": "ai_primers",
      "category": "On-Device AI",
      "article": "On-device Transformers",
      "articleSlug": "on-device-transformers",
      "chapter": "Parameter Choices and Their Runtime Implications in Transformer Architectures",
      "title": "Parameter Tuning Recipes for ML Runtimes",
      "subtitle": "Parameter Choices and Their Runtime Implications in Transformer Architectures",
      "contentHtml": "<ul>\n  <li>This section distills the relationships between vocabulary size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-164-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1356\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1357\"><span class=\"mi\" id=\"MathJax-Span-1358\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-164\">V</script>, embedding size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-165-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1359\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1360\"><span class=\"msubsup\" id=\"MathJax-Span-1361\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1362\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1363\"><span class=\"mrow\" id=\"MathJax-Span-1364\"><span class=\"mtext\" id=\"MathJax-Span-1365\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-165\">d_{\\text{model}}</script>, and depth <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-166-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1366\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1367\"><span class=\"mi\" id=\"MathJax-Span-1368\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-166\">L</script> into actionable configurations, balancing accuracy, latency, and memory for three representative environments.</li>\n</ul>\n<h4 id=\"nanogpt-server-inference--gpu-centric\">NanoGPT (Server Inference / GPU-centric)</h4>\n<ul>\n  <li><strong>Hardware Context:</strong> High-bandwidth memory (HBM), tensor cores, strong batched throughput.</li>\n  <li>\n    <p><strong>Parameter Priorities:</strong></p>\n\n    <ul>\n      <li>Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-167-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1369\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1370\"><span class=\"mi\" id=\"MathJax-Span-1371\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-167\">V</script> (30k–50k) acceptable due to fused softmax and fast GEMM kernels.</li>\n      <li>High <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-168-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1372\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1373\"><span class=\"msubsup\" id=\"MathJax-Span-1374\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1375\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1376\"><span class=\"mrow\" id=\"MathJax-Span-1377\"><span class=\"mtext\" id=\"MathJax-Span-1378\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-168\">d_{\\text{model}}</script> (1536–2048) to maximize model capacity.</li>\n      <li>Deep <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-169-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1379\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1380\"><span class=\"mi\" id=\"MathJax-Span-1381\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-169\">L</script> (24–48) for reasoning and long-context tasks.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Typical Config:</strong></p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-170-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>V</mi><mo>=</mo><mn>50,000</mn><mo>,</mo><mspace width=&quot;1em&quot; /><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>=</mo><mn>2048</mn><mo>,</mo><mspace width=&quot;1em&quot; /><mi>L</mi><mo>=</mo><mn>36</mn><mo>,</mo><mspace width=&quot;1em&quot; /><mtext>seq len</mtext><mo>=</mo><mn>4096</mn></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1382\" style=\"width: 28.701em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 23.909em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1023.86em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1383\"><span class=\"mi\" id=\"MathJax-Span-1384\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1385\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1386\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">50,000</span><span class=\"mo\" id=\"MathJax-Span-1387\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1388\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"msubsup\" id=\"MathJax-Span-1389\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1390\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1391\"><span class=\"mrow\" id=\"MathJax-Span-1392\"><span class=\"mtext\" id=\"MathJax-Span-1393\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1394\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1395\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">2048</span><span class=\"mo\" id=\"MathJax-Span-1396\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1397\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-1398\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1399\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1400\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">36</span><span class=\"mo\" id=\"MathJax-Span-1401\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1402\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mtext\" id=\"MathJax-Span-1403\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">seq len</span><span class=\"mo\" id=\"MathJax-Span-1404\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1405\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">4096</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>V</mi><mo>=</mo><mn>50,000</mn><mo>,</mo><mspace width=\"1em\"></mspace><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>=</mo><mn>2048</mn><mo>,</mo><mspace width=\"1em\"></mspace><mi>L</mi><mo>=</mo><mn>36</mn><mo>,</mo><mspace width=\"1em\"></mspace><mtext>seq len</mtext><mo>=</mo><mn>4096</mn></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-170\">V = 50{,}000, \\quad d_{\\text{model}} = 2048, \\quad L = 36, \\quad \\text{seq len} = 4096</script>\n  </li>\n  <li>\n    <p><strong>Expected Behavior:</strong></p>\n\n    <ul>\n      <li>Prefill dominated by attention FLOPs, but GPUs handle with parallelization.</li>\n      <li>Decode latency per token low in absolute terms due to high raw throughput, but still benefits from speculative decoding.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Optimization Levers:</strong></p>\n\n    <ul>\n      <li>Mixed precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>/<code class=\"language-plaintext highlighter-rouge\">bfloat16</code>).</li>\n      <li>Layer fusion (e.g., FasterTransformer).</li>\n      <li>Batch multiple decode streams to utilize GPU fully.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Parameter Priorities:</strong></p>\n<ul>\n      <li>Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-167-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1369\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1370\"><span class=\"mi\" id=\"MathJax-Span-1371\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-167\">V</script> (30k–50k) acceptable due to fused softmax and fast GEMM kernels.</li>\n      <li>High <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-168-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1372\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1373\"><span class=\"msubsup\" id=\"MathJax-Span-1374\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1375\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1376\"><span class=\"mrow\" id=\"MathJax-Span-1377\"><span class=\"mtext\" id=\"MathJax-Span-1378\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-168\">d_{\\text{model}}</script> (1536–2048) to maximize model capacity.</li>\n      <li>Deep <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-169-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1379\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1380\"><span class=\"mi\" id=\"MathJax-Span-1381\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-169\">L</script> (24–48) for reasoning and long-context tasks.</li>\n    </ul>\n<p><strong>Typical Config:</strong></p>\n<p><strong>Expected Behavior:</strong></p>\n<ul>\n      <li>Prefill dominated by attention FLOPs, but GPUs handle with parallelization.</li>\n      <li>Decode latency per token low in absolute terms due to high raw throughput, but still benefits from speculative decoding.</li>\n    </ul>\n<p><strong>Optimization Levers:</strong></p>\n<ul>\n      <li>Mixed precision (<code class=\"language-plaintext highlighter-rouge\">float16</code>/<code class=\"language-plaintext highlighter-rouge\">bfloat16</code>).</li>\n      <li>Layer fusion (e.g., FasterTransformer).</li>\n      <li>Batch multiple decode streams to utilize GPU fully.</li>\n    </ul>\n<h4 id=\"vllm-server-inference--gpu-centric\">VLLM (Server Inference / GPU-centric)</h4>\n<ul>\n  <li><strong>Hardware Context:</strong> GPU servers optimized for dynamic batching.</li>\n  <li>\n    <p><strong>Parameter Priorities:</strong></p>\n\n    <ul>\n      <li>Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-171-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1406\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1407\"><span class=\"mi\" id=\"MathJax-Span-1408\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-171\">V</script> acceptable with efficient memory layouts.</li>\n      <li>High <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-172-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1409\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1410\"><span class=\"msubsup\" id=\"MathJax-Span-1411\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1412\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1413\"><span class=\"mrow\" id=\"MathJax-Span-1414\"><span class=\"mtext\" id=\"MathJax-Span-1415\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-172\">d_{\\text{model}}</script> paired with aggressive continuous batching.</li>\n      <li>Depth <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-173-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1416\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1417\"><span class=\"mi\" id=\"MathJax-Span-1418\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-173\">L</script> tuned for target latency vs throughput.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Optimization Levers:</strong></p>\n\n    <ul>\n      <li>Continuous batching to keep GPU fully utilized.</li>\n      <li>KV cache sharing across requests.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Parameter Priorities:</strong></p>\n<ul>\n      <li>Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-171-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1406\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1407\"><span class=\"mi\" id=\"MathJax-Span-1408\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-171\">V</script> acceptable with efficient memory layouts.</li>\n      <li>High <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-172-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1409\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1410\"><span class=\"msubsup\" id=\"MathJax-Span-1411\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1412\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1413\"><span class=\"mrow\" id=\"MathJax-Span-1414\"><span class=\"mtext\" id=\"MathJax-Span-1415\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-172\">d_{\\text{model}}</script> paired with aggressive continuous batching.</li>\n      <li>Depth <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-173-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1416\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1417\"><span class=\"mi\" id=\"MathJax-Span-1418\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-173\">L</script> tuned for target latency vs throughput.</li>\n    </ul>\n<p><strong>Optimization Levers:</strong></p>\n<ul>\n      <li>Continuous batching to keep GPU fully utilized.</li>\n      <li>KV cache sharing across requests.</li>\n    </ul>\n<h4 id=\"tensorrt-llm-server-inference--gpu-centric\">TensorRT-LLM (Server Inference / GPU-centric)</h4>\n<ul>\n  <li><strong>Hardware Context:</strong> NVIDIA GPUs with Tensor Cores and high-bandwidth memory.</li>\n  <li>\n    <p><strong>Parameter Priorities:</strong></p>\n\n    <ul>\n      <li>Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-174-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1419\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1420\"><span class=\"mi\" id=\"MathJax-Span-1421\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-174\">V</script> (up to 100k) supported with minimal softmax overhead via fused kernels.</li>\n      <li>High <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-175-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1422\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1423\"><span class=\"msubsup\" id=\"MathJax-Span-1424\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1425\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1426\"><span class=\"mrow\" id=\"MathJax-Span-1427\"><span class=\"mtext\" id=\"MathJax-Span-1428\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-175\">d_{\\text{model}}</script> (up to 4096) leveraged for accuracy without extreme latency.</li>\n      <li>Moderate-to-deep <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-176-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1429\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1430\"><span class=\"mi\" id=\"MathJax-Span-1431\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-176\">L</script> (20–48) manageable due to high parallelism.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Typical Config:</strong></p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-177-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>V</mi><mo>=</mo><mn>80,000</mn><mo>,</mo><mspace width=&quot;1em&quot; /><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>=</mo><mn>3072</mn><mo>,</mo><mspace width=&quot;1em&quot; /><mi>L</mi><mo>=</mo><mn>40</mn><mo>,</mo><mspace width=&quot;1em&quot; /><mtext>seq len</mtext><mo>=</mo><mn>8192</mn></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1432\" style=\"width: 28.701em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 23.909em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1023.91em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1433\"><span class=\"mi\" id=\"MathJax-Span-1434\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1435\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1436\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">80,000</span><span class=\"mo\" id=\"MathJax-Span-1437\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1438\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"msubsup\" id=\"MathJax-Span-1439\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1440\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1441\"><span class=\"mrow\" id=\"MathJax-Span-1442\"><span class=\"mtext\" id=\"MathJax-Span-1443\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1444\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1445\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">3072</span><span class=\"mo\" id=\"MathJax-Span-1446\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1447\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-1448\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1449\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1450\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">40</span><span class=\"mo\" id=\"MathJax-Span-1451\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1452\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mtext\" id=\"MathJax-Span-1453\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">seq len</span><span class=\"mo\" id=\"MathJax-Span-1454\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1455\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8192</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>V</mi><mo>=</mo><mn>80,000</mn><mo>,</mo><mspace width=\"1em\"></mspace><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>=</mo><mn>3072</mn><mo>,</mo><mspace width=\"1em\"></mspace><mi>L</mi><mo>=</mo><mn>40</mn><mo>,</mo><mspace width=\"1em\"></mspace><mtext>seq len</mtext><mo>=</mo><mn>8192</mn></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-177\">V = 80{,}000, \\quad d_{\\text{model}} = 3072, \\quad L = 40, \\quad \\text{seq len} = 8192</script>\n  </li>\n  <li>\n    <p><strong>Optimization Levers:</strong></p>\n\n    <ul>\n      <li>Tensor Core–optimized mixed-precision GEMM.</li>\n      <li>Fused attention and output projection.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Parameter Priorities:</strong></p>\n<ul>\n      <li>Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-174-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1419\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1420\"><span class=\"mi\" id=\"MathJax-Span-1421\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-174\">V</script> (up to 100k) supported with minimal softmax overhead via fused kernels.</li>\n      <li>High <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-175-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1422\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1423\"><span class=\"msubsup\" id=\"MathJax-Span-1424\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1425\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1426\"><span class=\"mrow\" id=\"MathJax-Span-1427\"><span class=\"mtext\" id=\"MathJax-Span-1428\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-175\">d_{\\text{model}}</script> (up to 4096) leveraged for accuracy without extreme latency.</li>\n      <li>Moderate-to-deep <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-176-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1429\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1430\"><span class=\"mi\" id=\"MathJax-Span-1431\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-176\">L</script> (20–48) manageable due to high parallelism.</li>\n    </ul>\n<p><strong>Typical Config:</strong></p>\n<p><strong>Optimization Levers:</strong></p>\n<ul>\n      <li>Tensor Core–optimized mixed-precision GEMM.</li>\n      <li>Fused attention and output projection.</li>\n    </ul>\n<h4 id=\"llamacpp-on-device-inference--cpu-centric\"><code class=\"language-plaintext Highlighter-rouge\">llama.cpp</code> (On-Device Inference / CPU-centric)</h4>\n<ul>\n  <li><strong>Hardware Context:</strong> Commodity x86/ARM CPU, limited parallelism, cache-sensitive.</li>\n  <li>\n    <p><strong>Parameter Priorities:</strong></p>\n\n    <ul>\n      <li>Reduce <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-178-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1456\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1457\"><span class=\"mi\" id=\"MathJax-Span-1458\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-178\">V</script> (16k–32k) to keep output projection matrix small and cache-friendly.</li>\n      <li>Moderate <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-179-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1459\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1460\"><span class=\"msubsup\" id=\"MathJax-Span-1461\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1462\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1463\"><span class=\"mrow\" id=\"MathJax-Span-1464\"><span class=\"mtext\" id=\"MathJax-Span-1465\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-179\">d_{\\text{model}}</script> (512–1024) to keep KV cache within LLC.</li>\n      <li>Lower <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-180-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1466\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1467\"><span class=\"mi\" id=\"MathJax-Span-1468\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-180\">L</script> (8–16) to reduce sequential decode steps.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Typical Config:</strong></p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-181-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>V</mi><mo>=</mo><mn>32,000</mn><mo>,</mo><mspace width=&quot;1em&quot; /><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>=</mo><mn>768</mn><mo>,</mo><mspace width=&quot;1em&quot; /><mi>L</mi><mo>=</mo><mn>12</mn><mo>,</mo><mspace width=&quot;1em&quot; /><mtext>seq len</mtext><mo>=</mo><mn>1024</mn></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1469\" style=\"width: 28.076em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 23.388em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1023.34em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1470\"><span class=\"mi\" id=\"MathJax-Span-1471\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1472\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1473\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">32,000</span><span class=\"mo\" id=\"MathJax-Span-1474\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1475\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"msubsup\" id=\"MathJax-Span-1476\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1477\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1478\"><span class=\"mrow\" id=\"MathJax-Span-1479\"><span class=\"mtext\" id=\"MathJax-Span-1480\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1481\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1482\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">768</span><span class=\"mo\" id=\"MathJax-Span-1483\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1484\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-1485\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1486\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1487\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">12</span><span class=\"mo\" id=\"MathJax-Span-1488\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1489\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mtext\" id=\"MathJax-Span-1490\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">seq len</span><span class=\"mo\" id=\"MathJax-Span-1491\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1492\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1024</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>V</mi><mo>=</mo><mn>32,000</mn><mo>,</mo><mspace width=\"1em\"></mspace><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>=</mo><mn>768</mn><mo>,</mo><mspace width=\"1em\"></mspace><mi>L</mi><mo>=</mo><mn>12</mn><mo>,</mo><mspace width=\"1em\"></mspace><mtext>seq len</mtext><mo>=</mo><mn>1024</mn></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-181\">V = 32{,}000, \\quad d_{\\text{model}} = 768, \\quad L = 12, \\quad \\text{seq len} = 1024</script>\n  </li>\n  <li>\n    <p><strong>Expected Behavior:</strong></p>\n\n    <ul>\n      <li>Prefill latency noticeable with large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-182-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1493\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1494\"><span class=\"mi\" id=\"MathJax-Span-1495\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-182\">n</script>, but acceptable for short prompts.</li>\n      <li>Decode speed highly sensitive to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-183-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1496\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1497\"><span class=\"mi\" id=\"MathJax-Span-1498\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-183\">L</script>% and KV cache fit.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Optimization Levers:</strong></p>\n\n    <ul>\n      <li>int8/int4 quantization.</li>\n      <li>Operator fusion (ONNX Runtime, GGML backend).</li>\n      <li>Cache-aware layout for KV storage.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Parameter Priorities:</strong></p>\n<ul>\n      <li>Reduce <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-178-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1456\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1457\"><span class=\"mi\" id=\"MathJax-Span-1458\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-178\">V</script> (16k–32k) to keep output projection matrix small and cache-friendly.</li>\n      <li>Moderate <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-179-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1459\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1460\"><span class=\"msubsup\" id=\"MathJax-Span-1461\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1462\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1463\"><span class=\"mrow\" id=\"MathJax-Span-1464\"><span class=\"mtext\" id=\"MathJax-Span-1465\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-179\">d_{\\text{model}}</script> (512–1024) to keep KV cache within LLC.</li>\n      <li>Lower <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-180-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1466\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1467\"><span class=\"mi\" id=\"MathJax-Span-1468\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-180\">L</script> (8–16) to reduce sequential decode steps.</li>\n    </ul>\n<p><strong>Typical Config:</strong></p>\n<p><strong>Expected Behavior:</strong></p>\n<ul>\n      <li>Prefill latency noticeable with large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-182-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1493\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1494\"><span class=\"mi\" id=\"MathJax-Span-1495\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-182\">n</script>, but acceptable for short prompts.</li>\n      <li>Decode speed highly sensitive to <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-183-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1496\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1497\"><span class=\"mi\" id=\"MathJax-Span-1498\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-183\">L</script>% and KV cache fit.</li>\n    </ul>\n<p><strong>Optimization Levers:</strong></p>\n<ul>\n      <li>int8/int4 quantization.</li>\n      <li>Operator fusion (ONNX Runtime, GGML backend).</li>\n      <li>Cache-aware layout for KV storage.</li>\n    </ul>\n<h4 id=\"lidartlm-edge-npu--sensor-fusion\">LidarTLM (Edge NPU / Sensor Fusion)</h4>\n<ul>\n  <li><strong>Hardware Context:</strong> Mobile/embedded NPU with strict SRAM and thermal limits.</li>\n  <li>\n    <p><strong>Parameter Priorities:</strong></p>\n\n    <ul>\n      <li>Task-specific <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-184-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1499\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1500\"><span class=\"mi\" id=\"MathJax-Span-1501\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-184\">V</script> (4k–10k) to minimize projection latency.</li>\n      <li>Small <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-185-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1502\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1503\"><span class=\"msubsup\" id=\"MathJax-Span-1504\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1505\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1506\"><span class=\"mrow\" id=\"MathJax-Span-1507\"><span class=\"mtext\" id=\"MathJax-Span-1508\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-185\">d_{\\text{model}}</script> (256–512) for real-time throughput.</li>\n      <li>Shallow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-186-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1509\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1510\"><span class=\"mi\" id=\"MathJax-Span-1511\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-186\">L</script> (4–8) for deterministic frame-to-frame latency.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Typical Config:</strong></p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-187-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>V</mi><mo>=</mo><mn>8,000</mn><mo>,</mo><mspace width=&quot;1em&quot; /><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub><mo>=</mo><mn>384</mn><mo>,</mo><mspace width=&quot;1em&quot; /><mi>L</mi><mo>=</mo><mn>6</mn><mo>,</mo><mspace width=&quot;1em&quot; /><mtext>seq len</mtext><mo>=</mo><mn>256</mn></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1512\" style=\"width: 26.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 21.878em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1021.83em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1513\"><span class=\"mi\" id=\"MathJax-Span-1514\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1515\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1516\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">8,000</span><span class=\"mo\" id=\"MathJax-Span-1517\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1518\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"msubsup\" id=\"MathJax-Span-1519\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1520\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1521\"><span class=\"mrow\" id=\"MathJax-Span-1522\"><span class=\"mtext\" id=\"MathJax-Span-1523\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1524\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1525\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">384</span><span class=\"mo\" id=\"MathJax-Span-1526\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1527\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mi\" id=\"MathJax-Span-1528\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1529\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1530\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">6</span><span class=\"mo\" id=\"MathJax-Span-1531\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mspace\" id=\"MathJax-Span-1532\" style=\"height: 0em; vertical-align: 0em; width: 1.148em; display: inline-block; overflow: hidden;\"></span><span class=\"mtext\" id=\"MathJax-Span-1533\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">seq len</span><span class=\"mo\" id=\"MathJax-Span-1534\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-1535\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">256</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>V</mi><mo>=</mo><mn>8,000</mn><mo>,</mo><mspace width=\"1em\"></mspace><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub><mo>=</mo><mn>384</mn><mo>,</mo><mspace width=\"1em\"></mspace><mi>L</mi><mo>=</mo><mn>6</mn><mo>,</mo><mspace width=\"1em\"></mspace><mtext>seq len</mtext><mo>=</mo><mn>256</mn></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-187\">V = 8{,}000, \\quad d_{\\text{model}} = 384, \\quad L = 6, \\quad \\text{seq len} = 256</script>\n  </li>\n  <li>\n    <p><strong>Expected Behavior:</strong></p>\n\n    <ul>\n      <li>Prefill and decode both complete well within a LiDAR frame budget (~33 ms).</li>\n      <li>KV cache fits entirely in on-chip SRAM, avoiding DRAM latency.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Optimization Levers:</strong></p>\n\n    <ul>\n      <li>Static <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization.</li>\n      <li>Operator fusion tailored to NPU’s op-set.</li>\n      <li>Preallocation of all buffers to avoid runtime allocation overhead.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Parameter Priorities:</strong></p>\n<ul>\n      <li>Task-specific <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-184-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1499\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1500\"><span class=\"mi\" id=\"MathJax-Span-1501\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-184\">V</script> (4k–10k) to minimize projection latency.</li>\n      <li>Small <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-185-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1502\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1503\"><span class=\"msubsup\" id=\"MathJax-Span-1504\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1505\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1506\"><span class=\"mrow\" id=\"MathJax-Span-1507\"><span class=\"mtext\" id=\"MathJax-Span-1508\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-185\">d_{\\text{model}}</script> (256–512) for real-time throughput.</li>\n      <li>Shallow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-186-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1509\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1510\"><span class=\"mi\" id=\"MathJax-Span-1511\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-186\">L</script> (4–8) for deterministic frame-to-frame latency.</li>\n    </ul>\n<p><strong>Typical Config:</strong></p>\n<p><strong>Expected Behavior:</strong></p>\n<ul>\n      <li>Prefill and decode both complete well within a LiDAR frame budget (~33 ms).</li>\n      <li>KV cache fits entirely in on-chip SRAM, avoiding DRAM latency.</li>\n    </ul>\n<p><strong>Optimization Levers:</strong></p>\n<ul>\n      <li>Static <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization.</li>\n      <li>Operator fusion tailored to NPU’s op-set.</li>\n      <li>Preallocation of all buffers to avoid runtime allocation overhead.</li>\n    </ul>\n<h4 id=\"mediapipe--tflite-llms-mobileedge-inference--specialized-ips-such-as-npusdsps\">MediaPipe + TFLite LLMs (Mobile/Edge Inference / Specialized IPs Such As NPUs/DSPs)</h4>\n<ul>\n  <li><strong>Hardware Context:</strong> Smartphone SoCs with NPUs/DSPs.</li>\n  <li>\n    <p><strong>Parameter Priorities:</strong></p>\n\n    <ul>\n      <li>Reduced <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-188-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1536\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1537\"><span class=\"mi\" id=\"MathJax-Span-1538\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-188\">V</script> (4k–8k) to fit projection in SRAM.</li>\n      <li>Small <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-189-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1539\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1540\"><span class=\"msubsup\" id=\"MathJax-Span-1541\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1542\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1543\"><span class=\"mrow\" id=\"MathJax-Span-1544\"><span class=\"mtext\" id=\"MathJax-Span-1545\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-189\">d_{\\text{model}}</script> (256–512) to fit memory and meet latency budgets.</li>\n      <li>Shallow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-190-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1546\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1547\"><span class=\"mi\" id=\"MathJax-Span-1548\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-190\">L</script> for real-time responses.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Optimization Levers:</strong></p>\n\n    <ul>\n      <li>Fully quantized static graphs.</li>\n      <li>Operator fusion for NPU op-sets.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Parameter Priorities:</strong></p>\n<ul>\n      <li>Reduced <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-188-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1536\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1537\"><span class=\"mi\" id=\"MathJax-Span-1538\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-188\">V</script> (4k–8k) to fit projection in SRAM.</li>\n      <li>Small <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-189-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1539\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1540\"><span class=\"msubsup\" id=\"MathJax-Span-1541\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1542\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1543\"><span class=\"mrow\" id=\"MathJax-Span-1544\"><span class=\"mtext\" id=\"MathJax-Span-1545\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-189\">d_{\\text{model}}</script> (256–512) to fit memory and meet latency budgets.</li>\n      <li>Shallow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-190-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1546\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1547\"><span class=\"mi\" id=\"MathJax-Span-1548\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-190\">L</script> for real-time responses.</li>\n    </ul>\n<p><strong>Optimization Levers:</strong></p>\n<ul>\n      <li>Fully quantized static graphs.</li>\n      <li>Operator fusion for NPU op-sets.</li>\n    </ul>\n<h4 id=\"cross-platform-insights\">Cross-Platform Insights</h4>\n<ul>\n  <li><strong>Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-191-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1549\" style=\"width: 0.932em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.777em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.78em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1550\"><span class=\"mi\" id=\"MathJax-Span-1551\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-191\">V</script></strong>: Fine on GPU servers, problematic for CPU/NPU without GEMM acceleration.</li>\n  <li><strong>Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-192-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>d</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>model</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1552\" style=\"width: 2.843em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.327em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.294em, 1002.33em, 2.43em, -999.997em); top: -2.115em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1553\"><span class=\"msubsup\" id=\"MathJax-Span-1554\"><span style=\"display: inline-block; position: relative; width: 2.327em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.153em, 1000.52em, 4.135em, -999.997em); top: -3.975em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1555\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.054em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span><span style=\"position: absolute; top: -3.82em; left: 0.519em;\"><span class=\"texatom\" id=\"MathJax-Span-1556\"><span class=\"mrow\" id=\"MathJax-Span-1557\"><span class=\"mtext\" id=\"MathJax-Span-1558\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">model</span></span></span><span style=\"display: inline-block; width: 0px; height: 3.98em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.12em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>d</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>model</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-192\">d_{\\text{model}}</script></strong>: Improves accuracy but hurts both memory and compute — only viable if hardware bandwidth is high.</li>\n  <li><strong>Large <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-193-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1559\" style=\"width: 0.777em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.622em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.62em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1560\"><span class=\"mi\" id=\"MathJax-Span-1561\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-193\">L</script></strong>: Boosts reasoning but increases decode latency linearly — worst-case for low-parallelism CPUs.</li>\n  <li><strong>Temperature</strong>: Has <strong>zero impact</strong> on runtime — safe to tune purely for output diversity.</li>\n</ul>",
      "contentMarkdown": "*   This section distills the relationships between vocabulary size VVV, embedding size dmodeldmodeld\\_{\\\\text{model}}, and depth LLL into actionable configurations, balancing accuracy, latency, and memory for three representative environments.\n\n#### NanoGPT (Server Inference / GPU-centric)\n\n*   **Hardware Context:** High-bandwidth memory (HBM), tensor cores, strong batched throughput.\n*   **Parameter Priorities:**\n    \n    *   Large VVV (30k–50k) acceptable due to fused softmax and fast GEMM kernels.\n    *   High dmodeldmodeld\\_{\\\\text{model}} (1536–2048) to maximize model capacity.\n    *   Deep LLL (24–48) for reasoning and long-context tasks.\n*   **Typical Config:**\n    \n    V\\=50,000,dmodel\\=2048,L\\=36,seq len\\=4096V\\=50,000,dmodel\\=2048,L\\=36,seq len\\=4096\n    \n    V = 50{,}000, \\\\quad d\\_{\\\\text{model}} = 2048, \\\\quad L = 36, \\\\quad \\\\text{seq len} = 4096\n*   **Expected Behavior:**\n    \n    *   Prefill dominated by attention FLOPs, but GPUs handle with parallelization.\n    *   Decode latency per token low in absolute terms due to high raw throughput, but still benefits from speculative decoding.\n*   **Optimization Levers:**\n    \n    *   Mixed precision (`float16`/`bfloat16`).\n    *   Layer fusion (e.g., FasterTransformer).\n    *   Batch multiple decode streams to utilize GPU fully.\n\n**Parameter Priorities:**\n\n*   Large VVV (30k–50k) acceptable due to fused softmax and fast GEMM kernels.\n*   High dmodeldmodeld\\_{\\\\text{model}} (1536–2048) to maximize model capacity.\n*   Deep LLL (24–48) for reasoning and long-context tasks.\n\n**Typical Config:**\n\n**Expected Behavior:**\n\n*   Prefill dominated by attention FLOPs, but GPUs handle with parallelization.\n*   Decode latency per token low in absolute terms due to high raw throughput, but still benefits from speculative decoding.\n\n**Optimization Levers:**\n\n*   Mixed precision (`float16`/`bfloat16`).\n*   Layer fusion (e.g., FasterTransformer).\n*   Batch multiple decode streams to utilize GPU fully.\n\n#### VLLM (Server Inference / GPU-centric)\n\n*   **Hardware Context:** GPU servers optimized for dynamic batching.\n*   **Parameter Priorities:**\n    \n    *   Large VVV acceptable with efficient memory layouts.\n    *   High dmodeldmodeld\\_{\\\\text{model}} paired with aggressive continuous batching.\n    *   Depth LLL tuned for target latency vs throughput.\n*   **Optimization Levers:**\n    \n    *   Continuous batching to keep GPU fully utilized.\n    *   KV cache sharing across requests.\n\n**Parameter Priorities:**\n\n*   Large VVV acceptable with efficient memory layouts.\n*   High dmodeldmodeld\\_{\\\\text{model}} paired with aggressive continuous batching.\n*   Depth LLL tuned for target latency vs throughput.\n\n**Optimization Levers:**\n\n*   Continuous batching to keep GPU fully utilized.\n*   KV cache sharing across requests.\n\n#### TensorRT-LLM (Server Inference / GPU-centric)\n\n*   **Hardware Context:** NVIDIA GPUs with Tensor Cores and high-bandwidth memory.\n*   **Parameter Priorities:**\n    \n    *   Large VVV (up to 100k) supported with minimal softmax overhead via fused kernels.\n    *   High dmodeldmodeld\\_{\\\\text{model}} (up to 4096) leveraged for accuracy without extreme latency.\n    *   Moderate-to-deep LLL (20–48) manageable due to high parallelism.\n*   **Typical Config:**\n    \n    V\\=80,000,dmodel\\=3072,L\\=40,seq len\\=8192V\\=80,000,dmodel\\=3072,L\\=40,seq len\\=8192\n    \n    V = 80{,}000, \\\\quad d\\_{\\\\text{model}} = 3072, \\\\quad L = 40, \\\\quad \\\\text{seq len} = 8192\n*   **Optimization Levers:**\n    \n    *   Tensor Core–optimized mixed-precision GEMM.\n    *   Fused attention and output projection.\n\n**Parameter Priorities:**\n\n*   Large VVV (up to 100k) supported with minimal softmax overhead via fused kernels.\n*   High dmodeldmodeld\\_{\\\\text{model}} (up to 4096) leveraged for accuracy without extreme latency.\n*   Moderate-to-deep LLL (20–48) manageable due to high parallelism.\n\n**Typical Config:**\n\n**Optimization Levers:**\n\n*   Tensor Core–optimized mixed-precision GEMM.\n*   Fused attention and output projection.\n\n#### `llama.cpp` (On-Device Inference / CPU-centric)\n\n*   **Hardware Context:** Commodity x86/ARM CPU, limited parallelism, cache-sensitive.\n*   **Parameter Priorities:**\n    \n    *   Reduce VVV (16k–32k) to keep output projection matrix small and cache-friendly.\n    *   Moderate dmodeldmodeld\\_{\\\\text{model}} (512–1024) to keep KV cache within LLC.\n    *   Lower LLL (8–16) to reduce sequential decode steps.\n*   **Typical Config:**\n    \n    V\\=32,000,dmodel\\=768,L\\=12,seq len\\=1024V\\=32,000,dmodel\\=768,L\\=12,seq len\\=1024\n    \n    V = 32{,}000, \\\\quad d\\_{\\\\text{model}} = 768, \\\\quad L = 12, \\\\quad \\\\text{seq len} = 1024\n*   **Expected Behavior:**\n    \n    *   Prefill latency noticeable with large nnn, but acceptable for short prompts.\n    *   Decode speed highly sensitive to LLL% and KV cache fit.\n*   **Optimization Levers:**\n    \n    *   int8/int4 quantization.\n    *   Operator fusion (ONNX Runtime, GGML backend).\n    *   Cache-aware layout for KV storage.\n\n**Parameter Priorities:**\n\n*   Reduce VVV (16k–32k) to keep output projection matrix small and cache-friendly.\n*   Moderate dmodeldmodeld\\_{\\\\text{model}} (512–1024) to keep KV cache within LLC.\n*   Lower LLL (8–16) to reduce sequential decode steps.\n\n**Typical Config:**\n\n**Expected Behavior:**\n\n*   Prefill latency noticeable with large nnn, but acceptable for short prompts.\n*   Decode speed highly sensitive to LLL% and KV cache fit.\n\n**Optimization Levers:**\n\n*   int8/int4 quantization.\n*   Operator fusion (ONNX Runtime, GGML backend).\n*   Cache-aware layout for KV storage.\n\n#### LidarTLM (Edge NPU / Sensor Fusion)\n\n*   **Hardware Context:** Mobile/embedded NPU with strict SRAM and thermal limits.\n*   **Parameter Priorities:**\n    \n    *   Task-specific VVV (4k–10k) to minimize projection latency.\n    *   Small dmodeldmodeld\\_{\\\\text{model}} (256–512) for real-time throughput.\n    *   Shallow LLL (4–8) for deterministic frame-to-frame latency.\n*   **Typical Config:**\n    \n    V\\=8,000,dmodel\\=384,L\\=6,seq len\\=256V\\=8,000,dmodel\\=384,L\\=6,seq len\\=256\n    \n    V = 8{,}000, \\\\quad d\\_{\\\\text{model}} = 384, \\\\quad L = 6, \\\\quad \\\\text{seq len} = 256\n*   **Expected Behavior:**\n    \n    *   Prefill and decode both complete well within a LiDAR frame budget (~33 ms).\n    *   KV cache fits entirely in on-chip SRAM, avoiding DRAM latency.\n*   **Optimization Levers:**\n    \n    *   Static `int8` quantization.\n    *   Operator fusion tailored to NPU’s op-set.\n    *   Preallocation of all buffers to avoid runtime allocation overhead.\n\n**Parameter Priorities:**\n\n*   Task-specific VVV (4k–10k) to minimize projection latency.\n*   Small dmodeldmodeld\\_{\\\\text{model}} (256–512) for real-time throughput.\n*   Shallow LLL (4–8) for deterministic frame-to-frame latency.\n\n**Typical Config:**\n\n**Expected Behavior:**\n\n*   Prefill and decode both complete well within a LiDAR frame budget (~33 ms).\n*   KV cache fits entirely in on-chip SRAM, avoiding DRAM latency.\n\n**Optimization Levers:**\n\n*   Static `int8` quantization.\n*   Operator fusion tailored to NPU’s op-set.\n*   Preallocation of all buffers to avoid runtime allocation overhead.\n\n#### MediaPipe + TFLite LLMs (Mobile/Edge Inference / Specialized IPs Such As NPUs/DSPs)\n\n*   **Hardware Context:** Smartphone SoCs with NPUs/DSPs.\n*   **Parameter Priorities:**\n    \n    *   Reduced VVV (4k–8k) to fit projection in SRAM.\n    *   Small dmodeldmodeld\\_{\\\\text{model}} (256–512) to fit memory and meet latency budgets.\n    *   Shallow LLL for real-time responses.\n*   **Optimization Levers:**\n    \n    *   Fully quantized static graphs.\n    *   Operator fusion for NPU op-sets.\n\n**Parameter Priorities:**\n\n*   Reduced VVV (4k–8k) to fit projection in SRAM.\n*   Small dmodeldmodeld\\_{\\\\text{model}} (256–512) to fit memory and meet latency budgets.\n*   Shallow LLL for real-time responses.\n\n**Optimization Levers:**\n\n*   Fully quantized static graphs.\n*   Operator fusion for NPU op-sets.\n\n#### Cross-Platform Insights\n\n*   **Large VVV**: Fine on GPU servers, problematic for CPU/NPU without GEMM acceleration.\n*   **Large dmodeldmodeld\\_{\\\\text{model}}**: Improves accuracy but hurts both memory and compute — only viable if hardware bandwidth is high.\n*   **Large LLL**: Boosts reasoning but increases decode latency linearly — worst-case for low-parallelism CPUs.\n*   **Temperature**: Has **zero impact** on runtime — safe to tune purely for output diversity.",
      "order": 27,
      "orderInChapter": 6,
      "difficulty": 4,
      "estimatedMinutes": 6,
      "tags": [
        "ondevice ai",
        "transformer",
        "attention",
        "embedding",
        "gpt",
        "llm",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 1046,
        "contentLength": 101095
      },
      "nextCards": [],
      "relatedCards": [
        "ai-model-compression-multimodal-quantization-14",
        "ai-model-compression-modern-quantization-techniques-13",
        "ai-model-compression-further-reading-21",
        "ai-LLM-challenges-with-context-scaling-8",
        "ai-model-compression-background-matrix-multiplication-in-gpus-2"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/on-device-transformers/#parameter-tuning-recipes-for-ml-runtimes",
      "scrapedAt": "2025-12-28T11:56:11.408Z",
      "siblings": [
        "ai-on-device-transformers-encoder-compute-bound-nature-1",
        "ai-on-device-transformers-decoder-memory-bound-nature-2",
        "ai-on-device-transformers-central-processing-unit-cpu-3",
        "ai-on-device-transformers-graphics-processing-unit-gpu-4",
        "ai-on-device-transformers-tensor-processing-unit-tpu-5"
      ]
    }
  ]
}