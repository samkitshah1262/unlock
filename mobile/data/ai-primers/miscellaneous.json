{
  "category": "Miscellaneous",
  "cards": [
    {
      "id": "ai-top-30-papers-the-first-law-of-complexodynamics-1",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "The First Law of Complexodynamics",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>Author: Scott Aaronson</li>\n  <li>The article “The First Law of Complexodynamics” discusses an intriguing question posed by Sean Carroll at the FQXi’s Setting Time Aright conference, which brought together experts from various fields to discuss the nature of time. Carroll’s question revolves around why the complexity of physical systems seems to increase, hit a maximum, and then decrease over time, unlike entropy, which consistently increases.</li>\n  <li>The article explains that entropy measures how disordered a system is and increases monotonically. However, complexity behaves differently, peaking at intermediate times before decreasing. To delve into this phenomenon, the author introduces concepts from Kolmogorov complexity. Kolmogorov complexity is defined as the length of the shortest computer program that can produce a given string. A related concept, sophistication, measures the complexity of a string as the shortest program describing a set of which the string is a typical member.</li>\n  <li>To address Carroll’s question, the author proposes the concept of “complextropy” as a measure of complexity that considers computational resource bounds. Complextropy should reflect the number of bits in the shortest efficient program that outputs a sample from a set such that the target string appears random with respect to that set. The conjecture is that complextropy will be small at the beginning and end of a system’s evolution but large at intermediate times, mirroring the observed pattern in complexity.</li>\n  <li>Proving this conjecture, either theoretically or empirically, presents challenges, particularly due to the difficulty of computing complextropy. One practical approach suggested is using the size of a gzip compressed file as an approximation for Kolmogorov complexity. The author mentions an ongoing research project aimed at empirically verifying the conjecture using this method.</li>\n  <li>The article also the idea that complexity, or complextropy, changes over time, peaking at intermediate stages. The author suggests using computational resource bounds to define this measure and discusses both theoretical and empirical approaches to validating the conjecture that complexity behaves in this manner. This exploration provides valuable insights into understanding the dynamic nature of complexity in physical systems.</li>\n</ul>",
      "contentMarkdown": "*   Author: Scott Aaronson\n*   The article “The First Law of Complexodynamics” discusses an intriguing question posed by Sean Carroll at the FQXi’s Setting Time Aright conference, which brought together experts from various fields to discuss the nature of time. Carroll’s question revolves around why the complexity of physical systems seems to increase, hit a maximum, and then decrease over time, unlike entropy, which consistently increases.\n*   The article explains that entropy measures how disordered a system is and increases monotonically. However, complexity behaves differently, peaking at intermediate times before decreasing. To delve into this phenomenon, the author introduces concepts from Kolmogorov complexity. Kolmogorov complexity is defined as the length of the shortest computer program that can produce a given string. A related concept, sophistication, measures the complexity of a string as the shortest program describing a set of which the string is a typical member.\n*   To address Carroll’s question, the author proposes the concept of “complextropy” as a measure of complexity that considers computational resource bounds. Complextropy should reflect the number of bits in the shortest efficient program that outputs a sample from a set such that the target string appears random with respect to that set. The conjecture is that complextropy will be small at the beginning and end of a system’s evolution but large at intermediate times, mirroring the observed pattern in complexity.\n*   Proving this conjecture, either theoretically or empirically, presents challenges, particularly due to the difficulty of computing complextropy. One practical approach suggested is using the size of a gzip compressed file as an approximation for Kolmogorov complexity. The author mentions an ongoing research project aimed at empirically verifying the conjecture using this method.\n*   The article also the idea that complexity, or complextropy, changes over time, peaking at intermediate stages. The author suggests using computational resource bounds to define this measure and discusses both theoretical and empirical approaches to validating the conjecture that complexity behaves in this manner. This exploration provides valuable insights into understanding the dynamic nature of complexity in physical systems.",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 340,
        "contentLength": 2387
      },
      "nextCards": [
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3"
      ],
      "relatedCards": [
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5",
        "ai-gpu-architecture-interconnects-6"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#the-first-law-of-complexodynamics",
      "scrapedAt": "2025-12-28T11:56:36.589Z",
      "siblings": [
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5",
        "ai-top-30-papers-pointer-networks-6"
      ]
    },
    {
      "id": "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "The Unreasonable Effectiveness of Recurrent Neural Networks",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>Author: Andrej Karpathy</li>\n  <li>The article “The Unreasonable Effectiveness of Recurrent Neural Networks” by Andrej Karpathy dives into the amazing abilities of Recurrent Neural Networks (RNNs). Karpathy talks about his first experience with training RNNs for image captioning, where even with random settings, the RNN started making believable image descriptions. This success was surprising because many people thought RNNs were hard to train, showing just how simple and powerful they can be.</li>\n  <li>RNNs are special because they can handle sequences of vectors, making them perfect for tasks that involve sequences as input and output. Unlike regular neural networks that deal with fixed-size inputs and outputs, RNNs can work with sequences of any length, making them very useful in many areas. Karpathy explains that RNNs work by keeping a hidden state that stores information from previous inputs, allowing them to “remember” past data.</li>\n  <li>Karpathy goes into detail about how RNNs work, including a simple interface where an input vector affects the output vector and considers all previous inputs. He shows how RNNs update their hidden state using matrix multiplications and non-linear functions. He also mentions Long Short-Term Memory (LSTM) networks, which are a more advanced type of RNN that solve some practical issues and are widely used.</li>\n  <li>To show how powerful RNNs can be, Karpathy describes training character-level language models. By feeding a large amount of text into an RNN, it learns to predict the next character in a sequence, allowing it to create text one character at a time. He gives examples of RNN-generated text from different sources, like Paul Graham’s essays, Shakespeare’s works, Wikipedia articles, algebraic geometry in LaTeX, Linux source code, and baby names. These examples show how RNNs can learn complex structures, grammar, and context from raw text.</li>\n  <li>Karpathy also talks about the training process and how the text generated by the RNN improves over time, showing how the model gradually gets better at understanding language. He visualizes the inner workings of the RNN, showing how different neurons react to specific patterns, like URLs or markdown syntax, which helps explain how the model learns.</li>\n  <li>Finally, Karpathy encourages readers to try out RNNs using the code he shared on GitHub, highlighting the fun and educational aspects of training character-level language models. He briefly touches on the bigger picture of RNN research and their growing importance in fields like natural language processing, computer vision, and machine learning. The article wraps up with a fun note, showing an RNN-generated sample from the article itself, proving how effective and versatile RNNs are.</li>\n</ul>",
      "contentMarkdown": "*   Author: Andrej Karpathy\n*   The article “The Unreasonable Effectiveness of Recurrent Neural Networks” by Andrej Karpathy dives into the amazing abilities of Recurrent Neural Networks (RNNs). Karpathy talks about his first experience with training RNNs for image captioning, where even with random settings, the RNN started making believable image descriptions. This success was surprising because many people thought RNNs were hard to train, showing just how simple and powerful they can be.\n*   RNNs are special because they can handle sequences of vectors, making them perfect for tasks that involve sequences as input and output. Unlike regular neural networks that deal with fixed-size inputs and outputs, RNNs can work with sequences of any length, making them very useful in many areas. Karpathy explains that RNNs work by keeping a hidden state that stores information from previous inputs, allowing them to “remember” past data.\n*   Karpathy goes into detail about how RNNs work, including a simple interface where an input vector affects the output vector and considers all previous inputs. He shows how RNNs update their hidden state using matrix multiplications and non-linear functions. He also mentions Long Short-Term Memory (LSTM) networks, which are a more advanced type of RNN that solve some practical issues and are widely used.\n*   To show how powerful RNNs can be, Karpathy describes training character-level language models. By feeding a large amount of text into an RNN, it learns to predict the next character in a sequence, allowing it to create text one character at a time. He gives examples of RNN-generated text from different sources, like Paul Graham’s essays, Shakespeare’s works, Wikipedia articles, algebraic geometry in LaTeX, Linux source code, and baby names. These examples show how RNNs can learn complex structures, grammar, and context from raw text.\n*   Karpathy also talks about the training process and how the text generated by the RNN improves over time, showing how the model gradually gets better at understanding language. He visualizes the inner workings of the RNN, showing how different neurons react to specific patterns, like URLs or markdown syntax, which helps explain how the model learns.\n*   Finally, Karpathy encourages readers to try out RNNs using the code he shared on GitHub, highlighting the fun and educational aspects of training character-level language models. He briefly touches on the bigger picture of RNN research and their growing importance in fields like natural language processing, computer vision, and machine learning. The article wraps up with a fun note, showing an RNN-generated sample from the article itself, proving how effective and versatile RNNs are.",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 2,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous",
        "neural network",
        "machine learning",
        "rnn",
        "lstm",
        "computer vision"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 427,
        "contentLength": 2803
      },
      "nextCards": [
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4"
      ],
      "relatedCards": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-pipeline-issues-6",
        "ai-ml-runtimes-architecture-10",
        "ai-ml-runtimes-supported-model-types-11",
        "ai-ann-similarity-search-real-world-applications-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#the-unreasonable-effectiveness-of-recurrent-neural-networks",
      "scrapedAt": "2025-12-28T11:56:36.589Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5",
        "ai-top-30-papers-pointer-networks-6"
      ]
    },
    {
      "id": "ai-top-30-papers-understanding-lstm-networks-3",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Understanding LSTM Networks",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>Author: Christopher Olah</li>\n  <li>The article “Understanding LSTM Networks” by Christopher Olah explains the structure and functioning of Long Short-Term Memory (LSTM) networks, a special kind of Recurrent Neural Network (RNN) that addresses the limitations of traditional RNNs in handling long-term dependencies.</li>\n  <li>Olah begins by highlighting the limitations of traditional neural networks and RNNs in maintaining persistent information, which is crucial for tasks involving sequences and lists, such as language modeling, translation, and speech recognition.</li>\n  <li>RNNs have loops that allow information to persist, making them suitable for sequential data. However, they struggle with long-term dependencies, where relevant information from earlier inputs is needed much later in the sequence.</li>\n  <li>The article introduces LSTMs, designed to overcome this limitation. LSTMs have a unique architecture that includes a cell state and three gates (input, forget, and output) that regulate the flow of information. These gates allow LSTMs to remember and forget information selectively, making them effective in learning long-term dependencies.</li>\n  <li>The forget gate decides what information to discard from the cell state, the input gate determines which new information to add, and the output gate controls what information is passed to the next step.</li>\n  <li>Olah explains the step-by-step functioning of LSTMs using diagrams and notations, making it easier to understand the complex interactions within the network. He also discusses variations of LSTMs, such as peephole connections and Gated Recurrent Units (GRUs), which offer different ways to handle long-term dependencies.</li>\n  <li>The article concludes by emphasizing the significance of LSTMs in achieving remarkable results in various applications and hints at future advancements in RNN research, such as attention mechanisms and Grid LSTMs, which further enhance the capabilities of neural networks.</li>\n</ul>",
      "contentMarkdown": "*   Author: Christopher Olah\n*   The article “Understanding LSTM Networks” by Christopher Olah explains the structure and functioning of Long Short-Term Memory (LSTM) networks, a special kind of Recurrent Neural Network (RNN) that addresses the limitations of traditional RNNs in handling long-term dependencies.\n*   Olah begins by highlighting the limitations of traditional neural networks and RNNs in maintaining persistent information, which is crucial for tasks involving sequences and lists, such as language modeling, translation, and speech recognition.\n*   RNNs have loops that allow information to persist, making them suitable for sequential data. However, they struggle with long-term dependencies, where relevant information from earlier inputs is needed much later in the sequence.\n*   The article introduces LSTMs, designed to overcome this limitation. LSTMs have a unique architecture that includes a cell state and three gates (input, forget, and output) that regulate the flow of information. These gates allow LSTMs to remember and forget information selectively, making them effective in learning long-term dependencies.\n*   The forget gate decides what information to discard from the cell state, the input gate determines which new information to add, and the output gate controls what information is passed to the next step.\n*   Olah explains the step-by-step functioning of LSTMs using diagrams and notations, making it easier to understand the complex interactions within the network. He also discusses variations of LSTMs, such as peephole connections and Gated Recurrent Units (GRUs), which offer different ways to handle long-term dependencies.\n*   The article concludes by emphasizing the significance of LSTMs in achieving remarkable results in various applications and hints at future advancements in RNN research, such as attention mechanisms and Grid LSTMs, which further enhance the capabilities of neural networks.",
      "order": 3,
      "orderInChapter": 3,
      "difficulty": 2,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "neural network",
        "attention",
        "rnn",
        "lstm",
        "gru"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 282,
        "contentLength": 2016
      },
      "nextCards": [
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ],
      "relatedCards": [
        "ai-ml-runtimes-architecture-10",
        "ai-ml-runtimes-supported-model-types-11",
        "ai-architectures-encoder-29",
        "ai-dl-comp-benefits-of-transformers-compared-to-rnnsgruslstms-7",
        "ai-padding-and-packing-motivation-the-problem-with-padding-6"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#understanding-lstm-networks",
      "scrapedAt": "2025-12-28T11:56:36.590Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5",
        "ai-top-30-papers-pointer-networks-6"
      ]
    },
    {
      "id": "ai-top-30-papers-recurrent-neural-network-regularization-4",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Recurrent Neural Network Regularization",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>Authors: Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals</li>\n  <li>The paper “Recurrent Neural Network Regularization” presents a novel method for applying dropout to Long Short-Term Memory (LSTM) networks to mitigate overfitting. Traditional dropout techniques are ineffective for Recurrent Neural Networks (RNNs) due to noise amplification in recurrent connections, which hampers learning. The authors propose a specialized dropout application that targets only non-recurrent connections in LSTMs, preserving the network’s ability to retain information over long sequences while reducing overfitting.</li>\n  <li>The study demonstrates significant performance improvements across various tasks, including language modeling, speech recognition, machine translation, and image caption generation. In language modeling, regularized LSTMs achieved better word-level perplexity on the Penn Tree Bank dataset compared to non-regularized models. The medium and large regularized LSTMs showed substantial reductions in perplexity, highlighting the efficacy of the proposed method.</li>\n  <li>For speech recognition, the authors tested their method on an internal Google Icelandic Speech dataset, showing that dropout improves frame accuracy, a critical metric correlating with Word Error Rate (WER). Regularized LSTMs achieved better generalization, indicating the potential of the proposed regularization technique for improving acoustic modeling.</li>\n  <li>In machine translation, the method was evaluated on the WMT’14 English to French dataset. The regularized LSTM outperformed non-regularized models, demonstrating higher BLEU scores, which measure translation quality. Although the regularized LSTM did not surpass the phrase-based LIUM SMT system, the results affirmed that dropout enhances translation performance.</li>\n  <li>The image caption generation task involved testing the dropout variant on an LSTM model that converts image vectors into captions. The authors used the MSCOCO dataset for this evaluation. The results showed that dropout helps improve caption quality, with regularized models performing comparably to model ensembles.</li>\n  <li>Overall, the paper establishes that correctly applying dropout to LSTMs effectively reduces overfitting and enhances performance across diverse applications. The authors suggest that this approach can be extended to other RNN architectures, potentially broadening the scope of improved regularization in neural networks.</li>\n</ul>",
      "contentMarkdown": "*   Authors: Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals\n*   The paper “Recurrent Neural Network Regularization” presents a novel method for applying dropout to Long Short-Term Memory (LSTM) networks to mitigate overfitting. Traditional dropout techniques are ineffective for Recurrent Neural Networks (RNNs) due to noise amplification in recurrent connections, which hampers learning. The authors propose a specialized dropout application that targets only non-recurrent connections in LSTMs, preserving the network’s ability to retain information over long sequences while reducing overfitting.\n*   The study demonstrates significant performance improvements across various tasks, including language modeling, speech recognition, machine translation, and image caption generation. In language modeling, regularized LSTMs achieved better word-level perplexity on the Penn Tree Bank dataset compared to non-regularized models. The medium and large regularized LSTMs showed substantial reductions in perplexity, highlighting the efficacy of the proposed method.\n*   For speech recognition, the authors tested their method on an internal Google Icelandic Speech dataset, showing that dropout improves frame accuracy, a critical metric correlating with Word Error Rate (WER). Regularized LSTMs achieved better generalization, indicating the potential of the proposed regularization technique for improving acoustic modeling.\n*   In machine translation, the method was evaluated on the WMT’14 English to French dataset. The regularized LSTM outperformed non-regularized models, demonstrating higher BLEU scores, which measure translation quality. Although the regularized LSTM did not surpass the phrase-based LIUM SMT system, the results affirmed that dropout enhances translation performance.\n*   The image caption generation task involved testing the dropout variant on an LSTM model that converts image vectors into captions. The authors used the MSCOCO dataset for this evaluation. The results showed that dropout helps improve caption quality, with regularized models performing comparably to model ensembles.\n*   Overall, the paper establishes that correctly applying dropout to LSTMs effectively reduces overfitting and enhances performance across diverse applications. The authors suggest that this approach can be extended to other RNN architectures, potentially broadening the scope of improved regularization in neural networks.",
      "order": 4,
      "orderInChapter": 4,
      "difficulty": 2,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "neural network",
        "rnn",
        "lstm",
        "regularization",
        "dropout"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 323,
        "contentLength": 2500
      },
      "nextCards": [
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5",
        "ai-top-30-papers-pointer-networks-6"
      ],
      "relatedCards": [
        "ai-model-debugging-regularization-14",
        "ai-ml-runtimes-architecture-10",
        "ai-ml-runtimes-supported-model-types-11",
        "ai-model-debugging-overfitting-8",
        "ai-tensorboard-plotting-losses-accuracies-and-weight-distribution-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#recurrent-neural-network-regularization",
      "scrapedAt": "2025-12-28T11:56:36.590Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5",
        "ai-top-30-papers-pointer-networks-6"
      ]
    },
    {
      "id": "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Keeping Neural Networks Simple by Minimizing the Description Length of the Weights",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>Authors: Geoffrey E. Hinton and Drew van Camp</li>\n  <li>The paper “Keeping Neural Networks Simple by Minimizing the Description Length of the Weights” by Hinton and van Camp introduces a method to regularize neural networks by penalizing the information content in the weights. The key idea is to add Gaussian noise to the weights and adapt the noise level during training to balance the trade-off between the network’s error and the complexity of the weights.</li>\n  <li>The Minimum Description Length (MDL) Principle underpins this method, suggesting that the best model minimizes the total cost of describing both the model and the errors it makes. For neural networks, this translates to minimizing the bits required to encode the weights and the discrepancies between the predicted and actual outputs.</li>\n  <li>By applying Gaussian noise to the weights, the authors effectively control the precision of weight values. This approach helps in reducing overfitting, especially in scenarios with limited training data. The noise level is adjusted to optimize the network’s performance while keeping the weights as simple as possible.</li>\n  <li>The method involves computing the derivatives of both the expected squared error and the information content in the weights. These derivatives are calculated efficiently without resorting to time-consuming Monte Carlo simulations, provided the output units are linear.</li>\n  <li>The authors introduce the concept of “noisy weights” where adding Gaussian noise allows for a more compact encoding of the weights. This noisy weight approach leverages the MDL principle to communicate weights more efficiently, balancing the trade-off between weight precision and the network’s error.</li>\n  <li>The study explores the application of this technique across different tasks, including language modeling, speech recognition, and image caption generation. The results show that the proposed regularization method significantly improves generalization by reducing overfitting.</li>\n  <li>Additionally, the paper discusses the benefits of using an adaptive mixture of Gaussians for encoding the weights. This mixture model adapts to the distribution of the weights during training, further enhancing the network’s ability to generalize from limited data.</li>\n  <li>Preliminary experiments on a high-dimensional task with scarce training data demonstrate that the new method allows for fitting complex non-linear models effectively. The results suggest that this approach is slightly better than traditional weight-decay methods, offering a new perspective on regularizing neural networks.</li>\n  <li>The authors conclude by acknowledging that while the new method shows promise, more experimental work is needed to determine its competitiveness with other statistical techniques for handling non-linear tasks with limited training data. They also highlight the potential for further refinements to enhance its performance.</li>\n</ul>",
      "contentMarkdown": "*   Authors: Geoffrey E. Hinton and Drew van Camp\n*   The paper “Keeping Neural Networks Simple by Minimizing the Description Length of the Weights” by Hinton and van Camp introduces a method to regularize neural networks by penalizing the information content in the weights. The key idea is to add Gaussian noise to the weights and adapt the noise level during training to balance the trade-off between the network’s error and the complexity of the weights.\n*   The Minimum Description Length (MDL) Principle underpins this method, suggesting that the best model minimizes the total cost of describing both the model and the errors it makes. For neural networks, this translates to minimizing the bits required to encode the weights and the discrepancies between the predicted and actual outputs.\n*   By applying Gaussian noise to the weights, the authors effectively control the precision of weight values. This approach helps in reducing overfitting, especially in scenarios with limited training data. The noise level is adjusted to optimize the network’s performance while keeping the weights as simple as possible.\n*   The method involves computing the derivatives of both the expected squared error and the information content in the weights. These derivatives are calculated efficiently without resorting to time-consuming Monte Carlo simulations, provided the output units are linear.\n*   The authors introduce the concept of “noisy weights” where adding Gaussian noise allows for a more compact encoding of the weights. This noisy weight approach leverages the MDL principle to communicate weights more efficiently, balancing the trade-off between weight precision and the network’s error.\n*   The study explores the application of this technique across different tasks, including language modeling, speech recognition, and image caption generation. The results show that the proposed regularization method significantly improves generalization by reducing overfitting.\n*   Additionally, the paper discusses the benefits of using an adaptive mixture of Gaussians for encoding the weights. This mixture model adapts to the distribution of the weights during training, further enhancing the network’s ability to generalize from limited data.\n*   Preliminary experiments on a high-dimensional task with scarce training data demonstrate that the new method allows for fitting complex non-linear models effectively. The results suggest that this approach is slightly better than traditional weight-decay methods, offering a new perspective on regularizing neural networks.\n*   The authors conclude by acknowledging that while the new method shows promise, more experimental work is needed to determine its competitiveness with other statistical techniques for handling non-linear tasks with limited training data. They also highlight the potential for further refinements to enhance its performance.",
      "order": 5,
      "orderInChapter": 5,
      "difficulty": 2,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous",
        "neural network",
        "regularization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 424,
        "contentLength": 2985
      },
      "nextCards": [
        "ai-top-30-papers-pointer-networks-6",
        "ai-top-30-papers-imagenet-classification-with-deep-convolutional-ne-7"
      ],
      "relatedCards": [
        "ai-ml-runtimes-architecture-10",
        "ai-ml-runtimes-architecture-16",
        "ai-ml-runtimes-overview-20",
        "ai-model-debugging-overfitting-8",
        "ai-model-debugging-regularization-14"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#keeping-neural-networks-simple-by-minimizing-the-description-length-of-the-weights",
      "scrapedAt": "2025-12-28T11:56:36.590Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-pointer-networks-6"
      ]
    },
    {
      "id": "ai-top-30-papers-pointer-networks-6",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Pointer Networks",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>Authors: Oriol Vinyals, Meire Fortunato, Navdeep Jaitly</li>\n  <li>\n    <p>The paper “Pointer Networks” introduces a novel neural architecture designed to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. This model, called Pointer Networks (Ptr-Nets), addresses the limitation of existing sequence-to-sequence models and Neural Turing Machines, which struggle with variable-sized output dictionaries. Ptr-Nets leverage a neural attention mechanism to select members of the input sequence as the output, making them particularly effective for problems such as sorting variable-sized sequences and various combinatorial optimization tasks.</p>\n  </li>\n  <li><strong>Key Contributions:</strong>\n    <ul>\n      <li>The Ptr-Net architecture is proposed to handle variable-length dictionaries using a softmax probability distribution as a pointer. This method is simple, effective, and enables the model to generalize to different input and output lengths.</li>\n      <li>Ptr-Nets are applied to three challenging geometric problems: computing planar convex hulls, Delaunay triangulations, and the planar Travelling Salesman Problem (TSP). The models learn to produce approximate solutions purely from training examples, demonstrating significant improvements over sequence-to-sequence models with input attention.</li>\n      <li>The learned models generalize beyond the maximum lengths they were trained on, showing the robustness and versatility of Ptr-Nets in handling variable-sized input and output sequences.</li>\n    </ul>\n  </li>\n  <li><strong>Models:</strong>\n    <ul>\n      <li><strong>Sequence-to-Sequence Model:</strong> This baseline model uses an encoder-decoder RNN framework to map an input sequence to an output sequence, but it requires a fixed output dictionary size. It uses Long Short Term Memory (LSTM) networks to estimate conditional probabilities, but struggles with tasks where the output size depends on the input length.</li>\n      <li><strong>Content Based Input Attention:</strong> An enhancement over the vanilla sequence-to-sequence model, this method introduces an attention mechanism that allows the decoder to focus on different parts of the input sequence. However, it still assumes a fixed output dictionary size.</li>\n      <li><strong>Pointer Networks (Ptr-Net):</strong> Ptr-Nets modify the attention mechanism to function as pointers, selecting elements from the input sequence as the output. This allows Ptr-Nets to handle variable-sized output dictionaries and solve combinatorial optimization problems effectively.</li>\n    </ul>\n  </li>\n  <li><strong>Empirical Results:</strong>\n    <ul>\n      <li><strong>Convex Hull:</strong> Ptr-Nets significantly outperform both the LSTM and LSTM with attention models on the convex hull problem. The Ptr-Net achieves high accuracy and nearly 100% area coverage, demonstrating its effectiveness in handling this combinatorial task.</li>\n      <li><strong>Delaunay Triangulation:</strong> Ptr-Nets achieve high triangle coverage and accuracy, showing their capability in solving the Delaunay triangulation problem. Although accuracy decreases for larger input sizes, the model still performs competitively.</li>\n      <li><strong>Travelling Salesman Problem (TSP):</strong> Ptr-Nets are tested on the planar symmetric TSP, demonstrating the ability to learn competitive solutions. The model performs well on small-scale TSP instances and generalizes to larger instances, though with some performance degradation.</li>\n    </ul>\n  </li>\n  <li><strong>Conclusion:</strong>\n    <ul>\n      <li>The Ptr-Net architecture successfully addresses the challenge of variable-length output dictionaries, outperforming traditional sequence-to-sequence models on fixed input size problems. By using attention mechanisms to solve combinatorial optimization problems, Ptr-Nets open up new possibilities for neural networks to tackle a broader class of problems without artificial constraints. Future work will explore the application of Ptr-Nets to other combinatorial problems such as sorting, aiming to further demonstrate their versatility and effectiveness.</li>\n    </ul>\n  </li>\n</ul>\n<p>The paper “Pointer Networks” introduces a novel neural architecture designed to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. This model, called Pointer Networks (Ptr-Nets), addresses the limitation of existing sequence-to-sequence models and Neural Turing Machines, which struggle with variable-sized output dictionaries. Ptr-Nets leverage a neural attention mechanism to select members of the input sequence as the output, making them particularly effective for problems such as sorting variable-sized sequences and various combinatorial optimization tasks.</p>\n<ul>\n      <li>The Ptr-Net architecture is proposed to handle variable-length dictionaries using a softmax probability distribution as a pointer. This method is simple, effective, and enables the model to generalize to different input and output lengths.</li>\n      <li>Ptr-Nets are applied to three challenging geometric problems: computing planar convex hulls, Delaunay triangulations, and the planar Travelling Salesman Problem (TSP). The models learn to produce approximate solutions purely from training examples, demonstrating significant improvements over sequence-to-sequence models with input attention.</li>\n      <li>The learned models generalize beyond the maximum lengths they were trained on, showing the robustness and versatility of Ptr-Nets in handling variable-sized input and output sequences.</li>\n    </ul>\n<ul>\n      <li><strong>Sequence-to-Sequence Model:</strong> This baseline model uses an encoder-decoder RNN framework to map an input sequence to an output sequence, but it requires a fixed output dictionary size. It uses Long Short Term Memory (LSTM) networks to estimate conditional probabilities, but struggles with tasks where the output size depends on the input length.</li>\n      <li><strong>Content Based Input Attention:</strong> An enhancement over the vanilla sequence-to-sequence model, this method introduces an attention mechanism that allows the decoder to focus on different parts of the input sequence. However, it still assumes a fixed output dictionary size.</li>\n      <li><strong>Pointer Networks (Ptr-Net):</strong> Ptr-Nets modify the attention mechanism to function as pointers, selecting elements from the input sequence as the output. This allows Ptr-Nets to handle variable-sized output dictionaries and solve combinatorial optimization problems effectively.</li>\n    </ul>\n<ul>\n      <li><strong>Convex Hull:</strong> Ptr-Nets significantly outperform both the LSTM and LSTM with attention models on the convex hull problem. The Ptr-Net achieves high accuracy and nearly 100% area coverage, demonstrating its effectiveness in handling this combinatorial task.</li>\n      <li><strong>Delaunay Triangulation:</strong> Ptr-Nets achieve high triangle coverage and accuracy, showing their capability in solving the Delaunay triangulation problem. Although accuracy decreases for larger input sizes, the model still performs competitively.</li>\n      <li><strong>Travelling Salesman Problem (TSP):</strong> Ptr-Nets are tested on the planar symmetric TSP, demonstrating the ability to learn competitive solutions. The model performs well on small-scale TSP instances and generalizes to larger instances, though with some performance degradation.</li>\n    </ul>\n<ul>\n      <li>The Ptr-Net architecture successfully addresses the challenge of variable-length output dictionaries, outperforming traditional sequence-to-sequence models on fixed input size problems. By using attention mechanisms to solve combinatorial optimization problems, Ptr-Nets open up new possibilities for neural networks to tackle a broader class of problems without artificial constraints. Future work will explore the application of Ptr-Nets to other combinatorial problems such as sorting, aiming to further demonstrate their versatility and effectiveness.</li>\n    </ul>",
      "contentMarkdown": "*   Authors: Oriol Vinyals, Meire Fortunato, Navdeep Jaitly\n*   The paper “Pointer Networks” introduces a novel neural architecture designed to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. This model, called Pointer Networks (Ptr-Nets), addresses the limitation of existing sequence-to-sequence models and Neural Turing Machines, which struggle with variable-sized output dictionaries. Ptr-Nets leverage a neural attention mechanism to select members of the input sequence as the output, making them particularly effective for problems such as sorting variable-sized sequences and various combinatorial optimization tasks.\n    \n*   **Key Contributions:**\n    *   The Ptr-Net architecture is proposed to handle variable-length dictionaries using a softmax probability distribution as a pointer. This method is simple, effective, and enables the model to generalize to different input and output lengths.\n    *   Ptr-Nets are applied to three challenging geometric problems: computing planar convex hulls, Delaunay triangulations, and the planar Travelling Salesman Problem (TSP). The models learn to produce approximate solutions purely from training examples, demonstrating significant improvements over sequence-to-sequence models with input attention.\n    *   The learned models generalize beyond the maximum lengths they were trained on, showing the robustness and versatility of Ptr-Nets in handling variable-sized input and output sequences.\n*   **Models:**\n    *   **Sequence-to-Sequence Model:** This baseline model uses an encoder-decoder RNN framework to map an input sequence to an output sequence, but it requires a fixed output dictionary size. It uses Long Short Term Memory (LSTM) networks to estimate conditional probabilities, but struggles with tasks where the output size depends on the input length.\n    *   **Content Based Input Attention:** An enhancement over the vanilla sequence-to-sequence model, this method introduces an attention mechanism that allows the decoder to focus on different parts of the input sequence. However, it still assumes a fixed output dictionary size.\n    *   **Pointer Networks (Ptr-Net):** Ptr-Nets modify the attention mechanism to function as pointers, selecting elements from the input sequence as the output. This allows Ptr-Nets to handle variable-sized output dictionaries and solve combinatorial optimization problems effectively.\n*   **Empirical Results:**\n    *   **Convex Hull:** Ptr-Nets significantly outperform both the LSTM and LSTM with attention models on the convex hull problem. The Ptr-Net achieves high accuracy and nearly 100% area coverage, demonstrating its effectiveness in handling this combinatorial task.\n    *   **Delaunay Triangulation:** Ptr-Nets achieve high triangle coverage and accuracy, showing their capability in solving the Delaunay triangulation problem. Although accuracy decreases for larger input sizes, the model still performs competitively.\n    *   **Travelling Salesman Problem (TSP):** Ptr-Nets are tested on the planar symmetric TSP, demonstrating the ability to learn competitive solutions. The model performs well on small-scale TSP instances and generalizes to larger instances, though with some performance degradation.\n*   **Conclusion:**\n    *   The Ptr-Net architecture successfully addresses the challenge of variable-length output dictionaries, outperforming traditional sequence-to-sequence models on fixed input size problems. By using attention mechanisms to solve combinatorial optimization problems, Ptr-Nets open up new possibilities for neural networks to tackle a broader class of problems without artificial constraints. Future work will explore the application of Ptr-Nets to other combinatorial problems such as sorting, aiming to further demonstrate their versatility and effectiveness.\n\nThe paper “Pointer Networks” introduces a novel neural architecture designed to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. This model, called Pointer Networks (Ptr-Nets), addresses the limitation of existing sequence-to-sequence models and Neural Turing Machines, which struggle with variable-sized output dictionaries. Ptr-Nets leverage a neural attention mechanism to select members of the input sequence as the output, making them particularly effective for problems such as sorting variable-sized sequences and various combinatorial optimization tasks.\n\n*   The Ptr-Net architecture is proposed to handle variable-length dictionaries using a softmax probability distribution as a pointer. This method is simple, effective, and enables the model to generalize to different input and output lengths.\n*   Ptr-Nets are applied to three challenging geometric problems: computing planar convex hulls, Delaunay triangulations, and the planar Travelling Salesman Problem (TSP). The models learn to produce approximate solutions purely from training examples, demonstrating significant improvements over sequence-to-sequence models with input attention.\n*   The learned models generalize beyond the maximum lengths they were trained on, showing the robustness and versatility of Ptr-Nets in handling variable-sized input and output sequences.\n\n*   **Sequence-to-Sequence Model:** This baseline model uses an encoder-decoder RNN framework to map an input sequence to an output sequence, but it requires a fixed output dictionary size. It uses Long Short Term Memory (LSTM) networks to estimate conditional probabilities, but struggles with tasks where the output size depends on the input length.\n*   **Content Based Input Attention:** An enhancement over the vanilla sequence-to-sequence model, this method introduces an attention mechanism that allows the decoder to focus on different parts of the input sequence. However, it still assumes a fixed output dictionary size.\n*   **Pointer Networks (Ptr-Net):** Ptr-Nets modify the attention mechanism to function as pointers, selecting elements from the input sequence as the output. This allows Ptr-Nets to handle variable-sized output dictionaries and solve combinatorial optimization problems effectively.\n\n*   **Convex Hull:** Ptr-Nets significantly outperform both the LSTM and LSTM with attention models on the convex hull problem. The Ptr-Net achieves high accuracy and nearly 100% area coverage, demonstrating its effectiveness in handling this combinatorial task.\n*   **Delaunay Triangulation:** Ptr-Nets achieve high triangle coverage and accuracy, showing their capability in solving the Delaunay triangulation problem. Although accuracy decreases for larger input sizes, the model still performs competitively.\n*   **Travelling Salesman Problem (TSP):** Ptr-Nets are tested on the planar symmetric TSP, demonstrating the ability to learn competitive solutions. The model performs well on small-scale TSP instances and generalizes to larger instances, though with some performance degradation.\n\n*   The Ptr-Net architecture successfully addresses the challenge of variable-length output dictionaries, outperforming traditional sequence-to-sequence models on fixed input size problems. By using attention mechanisms to solve combinatorial optimization problems, Ptr-Nets open up new possibilities for neural networks to tackle a broader class of problems without artificial constraints. Future work will explore the application of Ptr-Nets to other combinatorial problems such as sorting, aiming to further demonstrate their versatility and effectiveness.",
      "order": 6,
      "orderInChapter": 6,
      "difficulty": 3,
      "estimatedMinutes": 6,
      "tags": [
        "miscellaneous",
        "neural network",
        "attention",
        "rnn",
        "lstm",
        "optimization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 1013,
        "contentLength": 8218
      },
      "nextCards": [
        "ai-top-30-papers-imagenet-classification-with-deep-convolutional-ne-7",
        "ai-top-30-papers-order-matters-sequence-to-sequence-for-sets-8"
      ],
      "relatedCards": [
        "ai-model-debugging-weight-initialization-11",
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-ml-runtimes-architecture-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#pointer-networks",
      "scrapedAt": "2025-12-28T11:56:36.590Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-imagenet-classification-with-deep-convolutional-ne-7",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "ImageNet Classification with Deep Convolutional Neural Networks",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>Authors: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton</li>\n  <li>\n    <p>The paper “ImageNet Classification with Deep Convolutional Neural Networks” details the development and training of a large, deep convolutional neural network (CNN) designed to classify images from the ImageNet dataset. The network achieved significant improvements in classification accuracy, surpassing previous state-of-the-art results on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2010 and 2012 datasets.</p>\n  </li>\n  <li><strong>Key Contributions:</strong>\n    <ul>\n      <li>The CNN architecture consists of five convolutional layers followed by three fully-connected layers, culminating in a 1000-way softmax output layer. This design leverages the hierarchical nature of image data, with convolutional layers capturing local features and fully-connected layers integrating these features for final classification.</li>\n      <li>To accelerate training, the network uses Rectified Linear Units (ReLUs) instead of traditional tanh or sigmoid neurons. ReLUs help in reducing the likelihood of the vanishing gradient problem and enable faster convergence during training.</li>\n      <li>The network was trained on two GPUs using a model parallelism approach, where different layers of the network were distributed across the GPUs. This setup allowed the handling of large models that would not fit into the memory of a single GPU.</li>\n      <li>Local Response Normalization (LRN) was employed to improve generalization by normalizing the activities of neurons within the same layer, mimicking a form of lateral inhibition observed in real neurons.</li>\n      <li>Overlapping pooling was used to downsample the spatial dimensions of the feature maps. Unlike traditional non-overlapping pooling, overlapping pooling helps to retain more information and reduce overfitting.</li>\n      <li>To combat overfitting, the authors used data augmentation techniques, including image translations, horizontal reflections, and principal component analysis (PCA) jittering on the RGB values. These techniques increased the effective size of the training dataset and improved generalization.</li>\n      <li>Dropout was applied to the fully-connected layers, randomly setting a fraction of the neurons to zero during training. This regularization technique prevents complex co-adaptations of neurons and enhances the robustness of the learned features.</li>\n    </ul>\n  </li>\n  <li><strong>Empirical Results:</strong>\n    <ul>\n      <li>On the ILSVRC-2010 dataset, the CNN achieved a top-1 error rate of 37.5% and a top-5 error rate of 17.0%, which was significantly better than previous methods.</li>\n      <li>On the ILSVRC-2012 dataset, the network obtained a top-5 error rate of 18.2%. When combined with predictions from multiple models, this error rate was further reduced to 15.3%, substantially outperforming the second-best entry, which had a top-5 error rate of 26.2%.</li>\n      <li>Qualitative analysis of the learned features showed that the network captured various types of frequency- and orientation-selective kernels in the early layers and more abstract features in deeper layers.</li>\n    </ul>\n  </li>\n  <li><strong>Conclusion:</strong>\n    <ul>\n      <li>The paper demonstrates that large, deep CNNs can achieve state-of-the-art results on challenging image classification tasks using purely supervised learning. The depth and complexity of the network are crucial for its performance, as evidenced by the degradation in accuracy when any convolutional layer is removed.</li>\n      <li>The success of the network opens up possibilities for further advancements in computer vision by leveraging even larger datasets and more powerful computational resources. The methods and techniques developed in this work have since become foundational in the field of deep learning and computer vision.</li>\n    </ul>\n  </li>\n</ul>\n<p>The paper “ImageNet Classification with Deep Convolutional Neural Networks” details the development and training of a large, deep convolutional neural network (CNN) designed to classify images from the ImageNet dataset. The network achieved significant improvements in classification accuracy, surpassing previous state-of-the-art results on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2010 and 2012 datasets.</p>\n<ul>\n      <li>The CNN architecture consists of five convolutional layers followed by three fully-connected layers, culminating in a 1000-way softmax output layer. This design leverages the hierarchical nature of image data, with convolutional layers capturing local features and fully-connected layers integrating these features for final classification.</li>\n      <li>To accelerate training, the network uses Rectified Linear Units (ReLUs) instead of traditional tanh or sigmoid neurons. ReLUs help in reducing the likelihood of the vanishing gradient problem and enable faster convergence during training.</li>\n      <li>The network was trained on two GPUs using a model parallelism approach, where different layers of the network were distributed across the GPUs. This setup allowed the handling of large models that would not fit into the memory of a single GPU.</li>\n      <li>Local Response Normalization (LRN) was employed to improve generalization by normalizing the activities of neurons within the same layer, mimicking a form of lateral inhibition observed in real neurons.</li>\n      <li>Overlapping pooling was used to downsample the spatial dimensions of the feature maps. Unlike traditional non-overlapping pooling, overlapping pooling helps to retain more information and reduce overfitting.</li>\n      <li>To combat overfitting, the authors used data augmentation techniques, including image translations, horizontal reflections, and principal component analysis (PCA) jittering on the RGB values. These techniques increased the effective size of the training dataset and improved generalization.</li>\n      <li>Dropout was applied to the fully-connected layers, randomly setting a fraction of the neurons to zero during training. This regularization technique prevents complex co-adaptations of neurons and enhances the robustness of the learned features.</li>\n    </ul>\n<ul>\n      <li>On the ILSVRC-2010 dataset, the CNN achieved a top-1 error rate of 37.5% and a top-5 error rate of 17.0%, which was significantly better than previous methods.</li>\n      <li>On the ILSVRC-2012 dataset, the network obtained a top-5 error rate of 18.2%. When combined with predictions from multiple models, this error rate was further reduced to 15.3%, substantially outperforming the second-best entry, which had a top-5 error rate of 26.2%.</li>\n      <li>Qualitative analysis of the learned features showed that the network captured various types of frequency- and orientation-selective kernels in the early layers and more abstract features in deeper layers.</li>\n    </ul>\n<ul>\n      <li>The paper demonstrates that large, deep CNNs can achieve state-of-the-art results on challenging image classification tasks using purely supervised learning. The depth and complexity of the network are crucial for its performance, as evidenced by the degradation in accuracy when any convolutional layer is removed.</li>\n      <li>The success of the network opens up possibilities for further advancements in computer vision by leveraging even larger datasets and more powerful computational resources. The methods and techniques developed in this work have since become foundational in the field of deep learning and computer vision.</li>\n    </ul>",
      "contentMarkdown": "*   Authors: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n*   The paper “ImageNet Classification with Deep Convolutional Neural Networks” details the development and training of a large, deep convolutional neural network (CNN) designed to classify images from the ImageNet dataset. The network achieved significant improvements in classification accuracy, surpassing previous state-of-the-art results on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2010 and 2012 datasets.\n    \n*   **Key Contributions:**\n    *   The CNN architecture consists of five convolutional layers followed by three fully-connected layers, culminating in a 1000-way softmax output layer. This design leverages the hierarchical nature of image data, with convolutional layers capturing local features and fully-connected layers integrating these features for final classification.\n    *   To accelerate training, the network uses Rectified Linear Units (ReLUs) instead of traditional tanh or sigmoid neurons. ReLUs help in reducing the likelihood of the vanishing gradient problem and enable faster convergence during training.\n    *   The network was trained on two GPUs using a model parallelism approach, where different layers of the network were distributed across the GPUs. This setup allowed the handling of large models that would not fit into the memory of a single GPU.\n    *   Local Response Normalization (LRN) was employed to improve generalization by normalizing the activities of neurons within the same layer, mimicking a form of lateral inhibition observed in real neurons.\n    *   Overlapping pooling was used to downsample the spatial dimensions of the feature maps. Unlike traditional non-overlapping pooling, overlapping pooling helps to retain more information and reduce overfitting.\n    *   To combat overfitting, the authors used data augmentation techniques, including image translations, horizontal reflections, and principal component analysis (PCA) jittering on the RGB values. These techniques increased the effective size of the training dataset and improved generalization.\n    *   Dropout was applied to the fully-connected layers, randomly setting a fraction of the neurons to zero during training. This regularization technique prevents complex co-adaptations of neurons and enhances the robustness of the learned features.\n*   **Empirical Results:**\n    *   On the ILSVRC-2010 dataset, the CNN achieved a top-1 error rate of 37.5% and a top-5 error rate of 17.0%, which was significantly better than previous methods.\n    *   On the ILSVRC-2012 dataset, the network obtained a top-5 error rate of 18.2%. When combined with predictions from multiple models, this error rate was further reduced to 15.3%, substantially outperforming the second-best entry, which had a top-5 error rate of 26.2%.\n    *   Qualitative analysis of the learned features showed that the network captured various types of frequency- and orientation-selective kernels in the early layers and more abstract features in deeper layers.\n*   **Conclusion:**\n    *   The paper demonstrates that large, deep CNNs can achieve state-of-the-art results on challenging image classification tasks using purely supervised learning. The depth and complexity of the network are crucial for its performance, as evidenced by the degradation in accuracy when any convolutional layer is removed.\n    *   The success of the network opens up possibilities for further advancements in computer vision by leveraging even larger datasets and more powerful computational resources. The methods and techniques developed in this work have since become foundational in the field of deep learning and computer vision.\n\nThe paper “ImageNet Classification with Deep Convolutional Neural Networks” details the development and training of a large, deep convolutional neural network (CNN) designed to classify images from the ImageNet dataset. The network achieved significant improvements in classification accuracy, surpassing previous state-of-the-art results on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2010 and 2012 datasets.\n\n*   The CNN architecture consists of five convolutional layers followed by three fully-connected layers, culminating in a 1000-way softmax output layer. This design leverages the hierarchical nature of image data, with convolutional layers capturing local features and fully-connected layers integrating these features for final classification.\n*   To accelerate training, the network uses Rectified Linear Units (ReLUs) instead of traditional tanh or sigmoid neurons. ReLUs help in reducing the likelihood of the vanishing gradient problem and enable faster convergence during training.\n*   The network was trained on two GPUs using a model parallelism approach, where different layers of the network were distributed across the GPUs. This setup allowed the handling of large models that would not fit into the memory of a single GPU.\n*   Local Response Normalization (LRN) was employed to improve generalization by normalizing the activities of neurons within the same layer, mimicking a form of lateral inhibition observed in real neurons.\n*   Overlapping pooling was used to downsample the spatial dimensions of the feature maps. Unlike traditional non-overlapping pooling, overlapping pooling helps to retain more information and reduce overfitting.\n*   To combat overfitting, the authors used data augmentation techniques, including image translations, horizontal reflections, and principal component analysis (PCA) jittering on the RGB values. These techniques increased the effective size of the training dataset and improved generalization.\n*   Dropout was applied to the fully-connected layers, randomly setting a fraction of the neurons to zero during training. This regularization technique prevents complex co-adaptations of neurons and enhances the robustness of the learned features.\n\n*   On the ILSVRC-2010 dataset, the CNN achieved a top-1 error rate of 37.5% and a top-5 error rate of 17.0%, which was significantly better than previous methods.\n*   On the ILSVRC-2012 dataset, the network obtained a top-5 error rate of 18.2%. When combined with predictions from multiple models, this error rate was further reduced to 15.3%, substantially outperforming the second-best entry, which had a top-5 error rate of 26.2%.\n*   Qualitative analysis of the learned features showed that the network captured various types of frequency- and orientation-selective kernels in the early layers and more abstract features in deeper layers.\n\n*   The paper demonstrates that large, deep CNNs can achieve state-of-the-art results on challenging image classification tasks using purely supervised learning. The depth and complexity of the network are crucial for its performance, as evidenced by the degradation in accuracy when any convolutional layer is removed.\n*   The success of the network opens up possibilities for further advancements in computer vision by leveraging even larger datasets and more powerful computational resources. The methods and techniques developed in this work have since become foundational in the field of deep learning and computer vision.",
      "order": 7,
      "orderInChapter": 7,
      "difficulty": 3,
      "estimatedMinutes": 6,
      "tags": [
        "miscellaneous",
        "neural network",
        "deep learning",
        "convolution",
        "cnn",
        "computer vision",
        "supervised learning",
        "regularization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 1020,
        "contentLength": 7624
      },
      "nextCards": [
        "ai-top-30-papers-order-matters-sequence-to-sequence-for-sets-8",
        "ai-top-30-papers-gpipe-easy-scaling-with-micro-batch-pipeline-paral-9"
      ],
      "relatedCards": [
        "ai-cnns-for-text-classification-convolutions-1",
        "ai-cnns-for-text-classification-pooling-2",
        "ai-gpu-architecture-operator-dispatch-and-kernel-mapping-14",
        "ai-cnns-for-text-classification-fully-connected-3",
        "ai-cnns-for-text-classification-1-d-convolutions-over-text-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#imagenet-classification-with-deep-convolutional-neural-networks",
      "scrapedAt": "2025-12-28T11:56:36.590Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-order-matters-sequence-to-sequence-for-sets-8",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Order Matters: Sequence to Sequence for Sets",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>Authors: Oriol Vinyals, Samy Bengio, Manjunath Kudlur</li>\n  <li>\n    <p>The paper “Order Matters: Sequence to Sequence for Sets” explores the significance of input and output order in sequence-to-sequence (seq2seq) models, especially for tasks where the input or output is a set rather than a naturally ordered sequence. The authors propose methods to adapt seq2seq models for handling sets and demonstrate the impact of order on performance across various tasks.</p>\n  </li>\n  <li><strong>Key Contributions:</strong>\n    <ul>\n      <li>The authors highlight the limitations of traditional seq2seq models when dealing with sets, where the order of elements does not matter. They show that the order in which input and output data are presented significantly affects the learning and performance of these models.</li>\n      <li>They introduce an extension to the seq2seq framework to handle input sets in a principled way. This involves using an attention mechanism to process unordered sets, allowing the model to remain invariant to the input order.</li>\n      <li>For output sets, the authors propose a loss function that searches over possible orders during training to find the optimal arrangement, improving the model’s ability to generalize and perform accurately.</li>\n    </ul>\n  </li>\n  <li><strong>Experiments and Results:</strong>\n    <ul>\n      <li><strong>Language Modeling:</strong> The authors experiment with different orderings of input sentences and show that reversing the order of words in the source sentence can improve performance in machine translation tasks. They also find that for parsing tasks, the choice of traversal order (depth-first vs. breadth-first) significantly impacts the model’s accuracy.</li>\n      <li><strong>Combinatorial Problems:</strong> The paper demonstrates the importance of ordering in combinatorial problems such as sorting numbers and computing convex hulls. For example, sorting the input points by angle simplifies the convex hull computation, leading to faster training and higher accuracy.</li>\n      <li><strong>Graphical Models:</strong> The authors create artificial datasets with star-like graphical models and show that it is easier to learn the joint probability distribution when the head variable is presented first. This experiment highlights the significance of choosing the optimal order for modeling complex dependencies among random variables.</li>\n    </ul>\n  </li>\n  <li><strong>Model Architecture:</strong>\n    <ul>\n      <li><strong>Read, Process, Write Model:</strong> The proposed model consists of three components: a reading block that embeds each input element, a processing block that performs computation over the embeddings using an attention mechanism, and a writing block that produces the output sequence using a pointer network. This architecture ensures permutation invariance and effectively handles input sets.</li>\n      <li><strong>Attention Mechanisms:</strong> The authors leverage attention mechanisms to integrate information from variable-length input structures, maintaining the order invariance property crucial for handling sets.</li>\n      <li><strong>Finding Optimal Orderings:</strong> To address the challenge of determining the best output order, the authors propose an algorithm that explores different orderings during training. By sampling from the probability distribution over possible orders, the model can identify and reinforce the most suitable order for the task.</li>\n    </ul>\n  </li>\n  <li><strong>Conclusion:</strong>\n    <ul>\n      <li>The paper concludes that order significantly influences the performance of seq2seq models when dealing with sets. The proposed methods for handling input and output sets improve the generalization and accuracy of the models. The authors demonstrate the effectiveness of their approach through various experiments, including sorting, language modeling, parsing, and graphical model estimation. This work opens up new possibilities for extending seq2seq models to a broader range of tasks that involve unordered sets.</li>\n    </ul>\n  </li>\n</ul>\n<p>The paper “Order Matters: Sequence to Sequence for Sets” explores the significance of input and output order in sequence-to-sequence (seq2seq) models, especially for tasks where the input or output is a set rather than a naturally ordered sequence. The authors propose methods to adapt seq2seq models for handling sets and demonstrate the impact of order on performance across various tasks.</p>\n<ul>\n      <li>The authors highlight the limitations of traditional seq2seq models when dealing with sets, where the order of elements does not matter. They show that the order in which input and output data are presented significantly affects the learning and performance of these models.</li>\n      <li>They introduce an extension to the seq2seq framework to handle input sets in a principled way. This involves using an attention mechanism to process unordered sets, allowing the model to remain invariant to the input order.</li>\n      <li>For output sets, the authors propose a loss function that searches over possible orders during training to find the optimal arrangement, improving the model’s ability to generalize and perform accurately.</li>\n    </ul>\n<ul>\n      <li><strong>Language Modeling:</strong> The authors experiment with different orderings of input sentences and show that reversing the order of words in the source sentence can improve performance in machine translation tasks. They also find that for parsing tasks, the choice of traversal order (depth-first vs. breadth-first) significantly impacts the model’s accuracy.</li>\n      <li><strong>Combinatorial Problems:</strong> The paper demonstrates the importance of ordering in combinatorial problems such as sorting numbers and computing convex hulls. For example, sorting the input points by angle simplifies the convex hull computation, leading to faster training and higher accuracy.</li>\n      <li><strong>Graphical Models:</strong> The authors create artificial datasets with star-like graphical models and show that it is easier to learn the joint probability distribution when the head variable is presented first. This experiment highlights the significance of choosing the optimal order for modeling complex dependencies among random variables.</li>\n    </ul>\n<ul>\n      <li><strong>Read, Process, Write Model:</strong> The proposed model consists of three components: a reading block that embeds each input element, a processing block that performs computation over the embeddings using an attention mechanism, and a writing block that produces the output sequence using a pointer network. This architecture ensures permutation invariance and effectively handles input sets.</li>\n      <li><strong>Attention Mechanisms:</strong> The authors leverage attention mechanisms to integrate information from variable-length input structures, maintaining the order invariance property crucial for handling sets.</li>\n      <li><strong>Finding Optimal Orderings:</strong> To address the challenge of determining the best output order, the authors propose an algorithm that explores different orderings during training. By sampling from the probability distribution over possible orders, the model can identify and reinforce the most suitable order for the task.</li>\n    </ul>\n<ul>\n      <li>The paper concludes that order significantly influences the performance of seq2seq models when dealing with sets. The proposed methods for handling input and output sets improve the generalization and accuracy of the models. The authors demonstrate the effectiveness of their approach through various experiments, including sorting, language modeling, parsing, and graphical model estimation. This work opens up new possibilities for extending seq2seq models to a broader range of tasks that involve unordered sets.</li>\n    </ul>",
      "contentMarkdown": "*   Authors: Oriol Vinyals, Samy Bengio, Manjunath Kudlur\n*   The paper “Order Matters: Sequence to Sequence for Sets” explores the significance of input and output order in sequence-to-sequence (seq2seq) models, especially for tasks where the input or output is a set rather than a naturally ordered sequence. The authors propose methods to adapt seq2seq models for handling sets and demonstrate the impact of order on performance across various tasks.\n    \n*   **Key Contributions:**\n    *   The authors highlight the limitations of traditional seq2seq models when dealing with sets, where the order of elements does not matter. They show that the order in which input and output data are presented significantly affects the learning and performance of these models.\n    *   They introduce an extension to the seq2seq framework to handle input sets in a principled way. This involves using an attention mechanism to process unordered sets, allowing the model to remain invariant to the input order.\n    *   For output sets, the authors propose a loss function that searches over possible orders during training to find the optimal arrangement, improving the model’s ability to generalize and perform accurately.\n*   **Experiments and Results:**\n    *   **Language Modeling:** The authors experiment with different orderings of input sentences and show that reversing the order of words in the source sentence can improve performance in machine translation tasks. They also find that for parsing tasks, the choice of traversal order (depth-first vs. breadth-first) significantly impacts the model’s accuracy.\n    *   **Combinatorial Problems:** The paper demonstrates the importance of ordering in combinatorial problems such as sorting numbers and computing convex hulls. For example, sorting the input points by angle simplifies the convex hull computation, leading to faster training and higher accuracy.\n    *   **Graphical Models:** The authors create artificial datasets with star-like graphical models and show that it is easier to learn the joint probability distribution when the head variable is presented first. This experiment highlights the significance of choosing the optimal order for modeling complex dependencies among random variables.\n*   **Model Architecture:**\n    *   **Read, Process, Write Model:** The proposed model consists of three components: a reading block that embeds each input element, a processing block that performs computation over the embeddings using an attention mechanism, and a writing block that produces the output sequence using a pointer network. This architecture ensures permutation invariance and effectively handles input sets.\n    *   **Attention Mechanisms:** The authors leverage attention mechanisms to integrate information from variable-length input structures, maintaining the order invariance property crucial for handling sets.\n    *   **Finding Optimal Orderings:** To address the challenge of determining the best output order, the authors propose an algorithm that explores different orderings during training. By sampling from the probability distribution over possible orders, the model can identify and reinforce the most suitable order for the task.\n*   **Conclusion:**\n    *   The paper concludes that order significantly influences the performance of seq2seq models when dealing with sets. The proposed methods for handling input and output sets improve the generalization and accuracy of the models. The authors demonstrate the effectiveness of their approach through various experiments, including sorting, language modeling, parsing, and graphical model estimation. This work opens up new possibilities for extending seq2seq models to a broader range of tasks that involve unordered sets.\n\nThe paper “Order Matters: Sequence to Sequence for Sets” explores the significance of input and output order in sequence-to-sequence (seq2seq) models, especially for tasks where the input or output is a set rather than a naturally ordered sequence. The authors propose methods to adapt seq2seq models for handling sets and demonstrate the impact of order on performance across various tasks.\n\n*   The authors highlight the limitations of traditional seq2seq models when dealing with sets, where the order of elements does not matter. They show that the order in which input and output data are presented significantly affects the learning and performance of these models.\n*   They introduce an extension to the seq2seq framework to handle input sets in a principled way. This involves using an attention mechanism to process unordered sets, allowing the model to remain invariant to the input order.\n*   For output sets, the authors propose a loss function that searches over possible orders during training to find the optimal arrangement, improving the model’s ability to generalize and perform accurately.\n\n*   **Language Modeling:** The authors experiment with different orderings of input sentences and show that reversing the order of words in the source sentence can improve performance in machine translation tasks. They also find that for parsing tasks, the choice of traversal order (depth-first vs. breadth-first) significantly impacts the model’s accuracy.\n*   **Combinatorial Problems:** The paper demonstrates the importance of ordering in combinatorial problems such as sorting numbers and computing convex hulls. For example, sorting the input points by angle simplifies the convex hull computation, leading to faster training and higher accuracy.\n*   **Graphical Models:** The authors create artificial datasets with star-like graphical models and show that it is easier to learn the joint probability distribution when the head variable is presented first. This experiment highlights the significance of choosing the optimal order for modeling complex dependencies among random variables.\n\n*   **Read, Process, Write Model:** The proposed model consists of three components: a reading block that embeds each input element, a processing block that performs computation over the embeddings using an attention mechanism, and a writing block that produces the output sequence using a pointer network. This architecture ensures permutation invariance and effectively handles input sets.\n*   **Attention Mechanisms:** The authors leverage attention mechanisms to integrate information from variable-length input structures, maintaining the order invariance property crucial for handling sets.\n*   **Finding Optimal Orderings:** To address the challenge of determining the best output order, the authors propose an algorithm that explores different orderings during training. By sampling from the probability distribution over possible orders, the model can identify and reinforce the most suitable order for the task.\n\n*   The paper concludes that order significantly influences the performance of seq2seq models when dealing with sets. The proposed methods for handling input and output sets improve the generalization and accuracy of the models. The authors demonstrate the effectiveness of their approach through various experiments, including sorting, language modeling, parsing, and graphical model estimation. This work opens up new possibilities for extending seq2seq models to a broader range of tasks that involve unordered sets.",
      "order": 8,
      "orderInChapter": 8,
      "difficulty": 2,
      "estimatedMinutes": 6,
      "tags": [
        "miscellaneous",
        "attention",
        "embedding",
        "loss function"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 1047,
        "contentLength": 7914
      },
      "nextCards": [
        "ai-top-30-papers-gpipe-easy-scaling-with-micro-batch-pipeline-paral-9",
        "ai-top-30-papers-deep-residual-learning-for-image-recognition-10"
      ],
      "relatedCards": [
        "ai-model-debugging-learning-rate-12",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-choosing-the-right-ann-algorithm-family-9",
        "ai-ml-runtimes-architecture-40",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#order-matters:-sequence-to-sequence-for-sets",
      "scrapedAt": "2025-12-28T11:56:36.590Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-gpipe-easy-scaling-with-micro-batch-pipeline-paral-9",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>Authors: Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen</li>\n  <li>\n    <p>The paper “GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism” introduces GPipe, a scalable model-parallelism library designed to enable efficient training of large neural networks by partitioning models across multiple accelerators. GPipe overcomes memory limitations and achieves almost linear speedup by using a novel batch-splitting pipelining algorithm.</p>\n  </li>\n  <li><strong>Key Contributions:</strong>\n    <ul>\n      <li><strong>GPipe Architecture:</strong> The GPipe library partitions a neural network into smaller sub-sequences of layers, or “cells,” which are distributed across multiple accelerators. This setup allows the training of models that exceed the memory capacity of a single accelerator.</li>\n      <li><strong>Batch-Splitting Pipeline Parallelism:</strong> GPipe divides each mini-batch of training data into smaller micro-batches. These micro-batches are then processed in a pipelined manner across the different accelerators, ensuring high hardware utilization and minimizing idle time.</li>\n      <li><strong>Synchronous Gradient Descent:</strong> The library uses synchronous mini-batch gradient descent, where gradients are accumulated across all micro-batches before being applied to update the model parameters. This approach ensures consistent gradient updates regardless of the number of partitions.</li>\n    </ul>\n  </li>\n  <li><strong>Experiments and Results:</strong>\n    <ul>\n      <li><strong>Image Classification:</strong> GPipe was used to train a 557-million-parameter AmoebaNet model on the ImageNet-2012 dataset. The model achieved a top-1 accuracy of 84.4%, demonstrating the effectiveness of GPipe in scaling large convolutional networks.</li>\n      <li><strong>Multilingual Neural Machine Translation:</strong> GPipe enabled the training of a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages. This model outperformed individually trained bilingual models, highlighting GPipe’s ability to handle diverse and large-scale NLP tasks.</li>\n    </ul>\n  </li>\n  <li><strong>Performance Optimization:</strong>\n    <ul>\n      <li><strong>Re-materialization:</strong> To reduce activation memory requirements, GPipe supports re-materialization, where only output activations at partition boundaries are stored during the forward pass. The required activations are recomputed during the backward pass, reducing peak memory usage.</li>\n      <li><strong>Load Balancing:</strong> The partitioning algorithm aims to balance the computational load across accelerators by minimizing the variance in the estimated costs of all cells. This optimization ensures efficient pipeline execution.</li>\n    </ul>\n  </li>\n  <li><strong>Design Features and Trade-Offs:</strong>\n    <ul>\n      <li><strong>Flexibility:</strong> GPipe supports any neural network that can be expressed as a sequence of layers, providing a versatile solution for various architectures and tasks.</li>\n      <li><strong>Efficiency:</strong> By minimizing communication overhead and utilizing batch-splitting pipeline parallelism, GPipe achieves near-linear scaling with the number of accelerators, even in environments with limited inter-device communication bandwidth.</li>\n      <li><strong>Training Stability:</strong> The use of synchronous gradient updates ensures stable and consistent training across different partitioning configurations, making GPipe reliable for large-scale model training.</li>\n    </ul>\n  </li>\n  <li><strong>Conclusion:</strong>\n    <ul>\n      <li>The GPipe library offers an efficient and flexible approach to scaling deep neural networks beyond single-accelerator memory limits. Its batch-splitting pipelining algorithm allows for significant improvements in training throughput and model capacity. GPipe’s design principles ensure that it can be applied to a wide range of machine learning tasks, from image classification to multilingual machine translation, with strong empirical results. The library’s ability to handle large models and achieve near-linear speedup positions it as a valuable tool for advancing deep learning research and applications.</li>\n    </ul>\n  </li>\n</ul>\n<p>The paper “GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism” introduces GPipe, a scalable model-parallelism library designed to enable efficient training of large neural networks by partitioning models across multiple accelerators. GPipe overcomes memory limitations and achieves almost linear speedup by using a novel batch-splitting pipelining algorithm.</p>\n<ul>\n      <li><strong>GPipe Architecture:</strong> The GPipe library partitions a neural network into smaller sub-sequences of layers, or “cells,” which are distributed across multiple accelerators. This setup allows the training of models that exceed the memory capacity of a single accelerator.</li>\n      <li><strong>Batch-Splitting Pipeline Parallelism:</strong> GPipe divides each mini-batch of training data into smaller micro-batches. These micro-batches are then processed in a pipelined manner across the different accelerators, ensuring high hardware utilization and minimizing idle time.</li>\n      <li><strong>Synchronous Gradient Descent:</strong> The library uses synchronous mini-batch gradient descent, where gradients are accumulated across all micro-batches before being applied to update the model parameters. This approach ensures consistent gradient updates regardless of the number of partitions.</li>\n    </ul>\n<ul>\n      <li><strong>Image Classification:</strong> GPipe was used to train a 557-million-parameter AmoebaNet model on the ImageNet-2012 dataset. The model achieved a top-1 accuracy of 84.4%, demonstrating the effectiveness of GPipe in scaling large convolutional networks.</li>\n      <li><strong>Multilingual Neural Machine Translation:</strong> GPipe enabled the training of a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages. This model outperformed individually trained bilingual models, highlighting GPipe’s ability to handle diverse and large-scale NLP tasks.</li>\n    </ul>\n<ul>\n      <li><strong>Re-materialization:</strong> To reduce activation memory requirements, GPipe supports re-materialization, where only output activations at partition boundaries are stored during the forward pass. The required activations are recomputed during the backward pass, reducing peak memory usage.</li>\n      <li><strong>Load Balancing:</strong> The partitioning algorithm aims to balance the computational load across accelerators by minimizing the variance in the estimated costs of all cells. This optimization ensures efficient pipeline execution.</li>\n    </ul>\n<ul>\n      <li><strong>Flexibility:</strong> GPipe supports any neural network that can be expressed as a sequence of layers, providing a versatile solution for various architectures and tasks.</li>\n      <li><strong>Efficiency:</strong> By minimizing communication overhead and utilizing batch-splitting pipeline parallelism, GPipe achieves near-linear scaling with the number of accelerators, even in environments with limited inter-device communication bandwidth.</li>\n      <li><strong>Training Stability:</strong> The use of synchronous gradient updates ensures stable and consistent training across different partitioning configurations, making GPipe reliable for large-scale model training.</li>\n    </ul>\n<ul>\n      <li>The GPipe library offers an efficient and flexible approach to scaling deep neural networks beyond single-accelerator memory limits. Its batch-splitting pipelining algorithm allows for significant improvements in training throughput and model capacity. GPipe’s design principles ensure that it can be applied to a wide range of machine learning tasks, from image classification to multilingual machine translation, with strong empirical results. The library’s ability to handle large models and achieve near-linear speedup positions it as a valuable tool for advancing deep learning research and applications.</li>\n    </ul>",
      "contentMarkdown": "*   Authors: Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen\n*   The paper “GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism” introduces GPipe, a scalable model-parallelism library designed to enable efficient training of large neural networks by partitioning models across multiple accelerators. GPipe overcomes memory limitations and achieves almost linear speedup by using a novel batch-splitting pipelining algorithm.\n    \n*   **Key Contributions:**\n    *   **GPipe Architecture:** The GPipe library partitions a neural network into smaller sub-sequences of layers, or “cells,” which are distributed across multiple accelerators. This setup allows the training of models that exceed the memory capacity of a single accelerator.\n    *   **Batch-Splitting Pipeline Parallelism:** GPipe divides each mini-batch of training data into smaller micro-batches. These micro-batches are then processed in a pipelined manner across the different accelerators, ensuring high hardware utilization and minimizing idle time.\n    *   **Synchronous Gradient Descent:** The library uses synchronous mini-batch gradient descent, where gradients are accumulated across all micro-batches before being applied to update the model parameters. This approach ensures consistent gradient updates regardless of the number of partitions.\n*   **Experiments and Results:**\n    *   **Image Classification:** GPipe was used to train a 557-million-parameter AmoebaNet model on the ImageNet-2012 dataset. The model achieved a top-1 accuracy of 84.4%, demonstrating the effectiveness of GPipe in scaling large convolutional networks.\n    *   **Multilingual Neural Machine Translation:** GPipe enabled the training of a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages. This model outperformed individually trained bilingual models, highlighting GPipe’s ability to handle diverse and large-scale NLP tasks.\n*   **Performance Optimization:**\n    *   **Re-materialization:** To reduce activation memory requirements, GPipe supports re-materialization, where only output activations at partition boundaries are stored during the forward pass. The required activations are recomputed during the backward pass, reducing peak memory usage.\n    *   **Load Balancing:** The partitioning algorithm aims to balance the computational load across accelerators by minimizing the variance in the estimated costs of all cells. This optimization ensures efficient pipeline execution.\n*   **Design Features and Trade-Offs:**\n    *   **Flexibility:** GPipe supports any neural network that can be expressed as a sequence of layers, providing a versatile solution for various architectures and tasks.\n    *   **Efficiency:** By minimizing communication overhead and utilizing batch-splitting pipeline parallelism, GPipe achieves near-linear scaling with the number of accelerators, even in environments with limited inter-device communication bandwidth.\n    *   **Training Stability:** The use of synchronous gradient updates ensures stable and consistent training across different partitioning configurations, making GPipe reliable for large-scale model training.\n*   **Conclusion:**\n    *   The GPipe library offers an efficient and flexible approach to scaling deep neural networks beyond single-accelerator memory limits. Its batch-splitting pipelining algorithm allows for significant improvements in training throughput and model capacity. GPipe’s design principles ensure that it can be applied to a wide range of machine learning tasks, from image classification to multilingual machine translation, with strong empirical results. The library’s ability to handle large models and achieve near-linear speedup positions it as a valuable tool for advancing deep learning research and applications.\n\nThe paper “GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism” introduces GPipe, a scalable model-parallelism library designed to enable efficient training of large neural networks by partitioning models across multiple accelerators. GPipe overcomes memory limitations and achieves almost linear speedup by using a novel batch-splitting pipelining algorithm.\n\n*   **GPipe Architecture:** The GPipe library partitions a neural network into smaller sub-sequences of layers, or “cells,” which are distributed across multiple accelerators. This setup allows the training of models that exceed the memory capacity of a single accelerator.\n*   **Batch-Splitting Pipeline Parallelism:** GPipe divides each mini-batch of training data into smaller micro-batches. These micro-batches are then processed in a pipelined manner across the different accelerators, ensuring high hardware utilization and minimizing idle time.\n*   **Synchronous Gradient Descent:** The library uses synchronous mini-batch gradient descent, where gradients are accumulated across all micro-batches before being applied to update the model parameters. This approach ensures consistent gradient updates regardless of the number of partitions.\n\n*   **Image Classification:** GPipe was used to train a 557-million-parameter AmoebaNet model on the ImageNet-2012 dataset. The model achieved a top-1 accuracy of 84.4%, demonstrating the effectiveness of GPipe in scaling large convolutional networks.\n*   **Multilingual Neural Machine Translation:** GPipe enabled the training of a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages. This model outperformed individually trained bilingual models, highlighting GPipe’s ability to handle diverse and large-scale NLP tasks.\n\n*   **Re-materialization:** To reduce activation memory requirements, GPipe supports re-materialization, where only output activations at partition boundaries are stored during the forward pass. The required activations are recomputed during the backward pass, reducing peak memory usage.\n*   **Load Balancing:** The partitioning algorithm aims to balance the computational load across accelerators by minimizing the variance in the estimated costs of all cells. This optimization ensures efficient pipeline execution.\n\n*   **Flexibility:** GPipe supports any neural network that can be expressed as a sequence of layers, providing a versatile solution for various architectures and tasks.\n*   **Efficiency:** By minimizing communication overhead and utilizing batch-splitting pipeline parallelism, GPipe achieves near-linear scaling with the number of accelerators, even in environments with limited inter-device communication bandwidth.\n*   **Training Stability:** The use of synchronous gradient updates ensures stable and consistent training across different partitioning configurations, making GPipe reliable for large-scale model training.\n\n*   The GPipe library offers an efficient and flexible approach to scaling deep neural networks beyond single-accelerator memory limits. Its batch-splitting pipelining algorithm allows for significant improvements in training throughput and model capacity. GPipe’s design principles ensure that it can be applied to a wide range of machine learning tasks, from image classification to multilingual machine translation, with strong empirical results. The library’s ability to handle large models and achieve near-linear speedup positions it as a valuable tool for advancing deep learning research and applications.",
      "order": 9,
      "orderInChapter": 9,
      "difficulty": 3,
      "estimatedMinutes": 5,
      "tags": [
        "miscellaneous",
        "neural network",
        "deep learning",
        "machine learning",
        "transformer",
        "convolution",
        "nlp",
        "optimization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 968,
        "contentLength": 8232
      },
      "nextCards": [
        "ai-top-30-papers-deep-residual-learning-for-image-recognition-10",
        "ai-top-30-papers-multi-scale-context-aggregation-by-dilated-convolu-11"
      ],
      "relatedCards": [
        "ai-gpu-architecture-operator-dispatch-and-kernel-mapping-14",
        "ai-ann-similarity-search-tabular-comparison-13",
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-21",
        "ai-cnns-for-text-classification-1-d-convolutions-over-text-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#gpipe:-easy-scaling-with-micro-batch-pipeline-parallelism",
      "scrapedAt": "2025-12-28T11:56:36.590Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-deep-residual-learning-for-image-recognition-10",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Deep Residual Learning for Image Recognition",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li><strong>Authors</strong>: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</li>\n  <li>\n    <p><strong>Affiliation</strong>: Microsoft Research</p>\n  </li>\n  <li>This seminal paper introduces the concept of deep residual networks (ResNets), which significantly ease the training of networks that are substantially deeper than those used previously. By utilizing residual blocks that allow layers to fit a residual mapping instead of directly attempting to fit a desired underlying mapping, ResNets facilitate the training process and improve the accuracy from increased depth.</li>\n</ul>\n<p><strong>Affiliation</strong>: Microsoft Research</p>\n<p>Key innovations and findings from the paper include:</p>\n<ol>\n  <li><strong>Residual Learning Framework</strong>: The layers in ResNet learn residual functions with reference to the layer inputs, which simplifies the learning process because the network learns to modify the identity mapping rather than having to estimate the full output.</li>\n  <li><strong>Ease of Optimization</strong>: The residual blocks make deeper networks easier to optimize because they mitigate the problem of vanishing gradients by using shortcut connections that perform identity mapping.</li>\n  <li><strong>Superior Performance on Deep Networks</strong>: Extensive experiments demonstrate that ResNets, with their deeper architectures, outperform traditional networks on major datasets like ImageNet and CIFAR-10. For instance, ResNets with a depth of up to 152 layers show better performance and lower complexity compared to VGG nets.</li>\n  <li><strong>Broad Applicability</strong>: The paper also highlights the effectiveness of ResNets across various tasks beyond image classification, such as object detection and localization, through adaptations like bottleneck designs that enhance computational efficiency.</li>\n</ol>\n<ul>\n  <li>These contributions have had a profound impact on the field of deep learning, influencing a wide range of subsequent research and applications in both academia and industry.</li>\n</ul>",
      "contentMarkdown": "*   **Authors**: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n*   **Affiliation**: Microsoft Research\n    \n*   This seminal paper introduces the concept of deep residual networks (ResNets), which significantly ease the training of networks that are substantially deeper than those used previously. By utilizing residual blocks that allow layers to fit a residual mapping instead of directly attempting to fit a desired underlying mapping, ResNets facilitate the training process and improve the accuracy from increased depth.\n\n**Affiliation**: Microsoft Research\n\nKey innovations and findings from the paper include:\n\n1.  **Residual Learning Framework**: The layers in ResNet learn residual functions with reference to the layer inputs, which simplifies the learning process because the network learns to modify the identity mapping rather than having to estimate the full output.\n2.  **Ease of Optimization**: The residual blocks make deeper networks easier to optimize because they mitigate the problem of vanishing gradients by using shortcut connections that perform identity mapping.\n3.  **Superior Performance on Deep Networks**: Extensive experiments demonstrate that ResNets, with their deeper architectures, outperform traditional networks on major datasets like ImageNet and CIFAR-10. For instance, ResNets with a depth of up to 152 layers show better performance and lower complexity compared to VGG nets.\n4.  **Broad Applicability**: The paper also highlights the effectiveness of ResNets across various tasks beyond image classification, such as object detection and localization, through adaptations like bottleneck designs that enhance computational efficiency.\n\n*   These contributions have had a profound impact on the field of deep learning, influencing a wide range of subsequent research and applications in both academia and industry.",
      "order": 10,
      "orderInChapter": 10,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "deep learning",
        "optimization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 260,
        "contentLength": 2060
      },
      "nextCards": [
        "ai-top-30-papers-multi-scale-context-aggregation-by-dilated-convolu-11",
        "ai-top-30-papers-neural-message-passing-for-quantum-chemistry-12"
      ],
      "relatedCards": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-gpu-architecture-overview-of-precision-types-25",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#deep-residual-learning-for-image-recognition",
      "scrapedAt": "2025-12-28T11:56:36.590Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-multi-scale-context-aggregation-by-dilated-convolu-11",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Multi-Scale Context Aggregation by Dilated Convolutions",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li><strong>Authors</strong>: Fisher Yu, Vladlen Koltun</li>\n  <li>\n    <p><strong>Affiliations</strong>: Princeton University, Intel Labs</p>\n  </li>\n  <li>\n    <p>The paper “Multi-Scale Context Aggregation by Dilated Convolutions” presents a novel approach for improving semantic segmentation by leveraging dilated convolutions. This method allows convolutional neural networks to systematically aggregate multi-scale contextual information without losing resolution.</p>\n  </li>\n  <li>Key Contributions:\n    <ol>\n      <li><strong>Dilated Convolutions</strong>:</li>\n    </ol>\n    <ul>\n      <li>Introduces the concept of dilated convolutions, which enable exponential expansion of the receptive field without reducing resolution or coverage.</li>\n      <li>Dilated convolutions, also known as atrous convolutions, are crucial for dense prediction tasks as they support the aggregation of multi-scale context while preserving spatial resolution.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Affiliations</strong>: Princeton University, Intel Labs</p>\n<p>The paper “Multi-Scale Context Aggregation by Dilated Convolutions” presents a novel approach for improving semantic segmentation by leveraging dilated convolutions. This method allows convolutional neural networks to systematically aggregate multi-scale contextual information without losing resolution.</p>\n<ol>\n      <li><strong>Dilated Convolutions</strong>:</li>\n    </ol>\n<ul>\n      <li>Introduces the concept of dilated convolutions, which enable exponential expansion of the receptive field without reducing resolution or coverage.</li>\n      <li>Dilated convolutions, also known as atrous convolutions, are crucial for dense prediction tasks as they support the aggregation of multi-scale context while preserving spatial resolution.</li>\n    </ul>\n<ol>\n  <li><strong>Multi-Scale Context Aggregation</strong>:\n    <ul>\n      <li>Proposes a new convolutional network module that aggregates multi-scale contextual information, enhancing the performance of dense prediction architectures like semantic segmentation.</li>\n      <li>The network uses a rectangular prism of convolutional layers with varying dilation factors, eliminating the need for pooling or subsampling layers, thereby maintaining high resolution throughout the network.</li>\n    </ul>\n  </li>\n  <li><strong>Simplified Network Design</strong>:\n    <ul>\n      <li>Simplifies existing image classification networks adapted for dense prediction by removing unnecessary components and layers that do not contribute to performance.</li>\n      <li>Specifically, removes the last two pooling and striding layers in the VGG-16 network and uses dilated convolutions in subsequent layers to maintain high-resolution outputs.</li>\n    </ul>\n  </li>\n  <li><strong>Controlled Experiments</strong>:\n    <ul>\n      <li>Conducts experiments on the Pascal VOC 2012 dataset to evaluate the performance of the proposed context module.</li>\n      <li>Demonstrates that the context module reliably increases accuracy when integrated into existing semantic segmentation architectures, both with and without structured prediction methods like Conditional Random Fields (CRFs) and CRF-RNNs.</li>\n    </ul>\n  </li>\n  <li><strong>Performance Improvement</strong>:\n    <ul>\n      <li>The context module enhances the accuracy of semantic segmentation models, outperforming previous state-of-the-art models on the Pascal VOC 2012 test set.</li>\n      <li>The simplified front-end module alone achieves higher accuracy compared to prior models, indicating the effectiveness of removing vestigial components.</li>\n    </ul>\n  </li>\n</ol>\n<ul>\n      <li>Proposes a new convolutional network module that aggregates multi-scale contextual information, enhancing the performance of dense prediction architectures like semantic segmentation.</li>\n      <li>The network uses a rectangular prism of convolutional layers with varying dilation factors, eliminating the need for pooling or subsampling layers, thereby maintaining high resolution throughout the network.</li>\n    </ul>\n<ul>\n      <li>Simplifies existing image classification networks adapted for dense prediction by removing unnecessary components and layers that do not contribute to performance.</li>\n      <li>Specifically, removes the last two pooling and striding layers in the VGG-16 network and uses dilated convolutions in subsequent layers to maintain high-resolution outputs.</li>\n    </ul>\n<ul>\n      <li>Conducts experiments on the Pascal VOC 2012 dataset to evaluate the performance of the proposed context module.</li>\n      <li>Demonstrates that the context module reliably increases accuracy when integrated into existing semantic segmentation architectures, both with and without structured prediction methods like Conditional Random Fields (CRFs) and CRF-RNNs.</li>\n    </ul>\n<ul>\n      <li>The context module enhances the accuracy of semantic segmentation models, outperforming previous state-of-the-art models on the Pascal VOC 2012 test set.</li>\n      <li>The simplified front-end module alone achieves higher accuracy compared to prior models, indicating the effectiveness of removing vestigial components.</li>\n    </ul>\n<ul>\n  <li>Experiments:\n    <ul>\n      <li><strong>Dataset</strong>: Uses the Pascal VOC 2012 dataset augmented with additional annotations for training.</li>\n      <li><strong>Training Procedure</strong>: Employs stochastic gradient descent (SGD) with specific learning rates and momentum, and evaluates the performance on both validation and test sets.</li>\n      <li><strong>Evaluation</strong>: The context module and simplified front-end are tested against models like FCN-8s and DeepLab, showing significant improvements in mean Intersection over Union (IoU) scores.</li>\n    </ul>\n  </li>\n  <li>Conclusion:\n    <ul>\n      <li>The paper demonstrates that dilated convolutions are highly effective for dense prediction tasks, allowing for the integration of multi-scale context without loss of resolution.</li>\n      <li>The proposed context module and the simplified front-end module provide substantial performance gains in semantic segmentation.</li>\n      <li>The approach suggests a shift towards dedicated architectures for dense prediction, moving away from adaptations of image classification networks.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Dataset</strong>: Uses the Pascal VOC 2012 dataset augmented with additional annotations for training.</li>\n      <li><strong>Training Procedure</strong>: Employs stochastic gradient descent (SGD) with specific learning rates and momentum, and evaluates the performance on both validation and test sets.</li>\n      <li><strong>Evaluation</strong>: The context module and simplified front-end are tested against models like FCN-8s and DeepLab, showing significant improvements in mean Intersection over Union (IoU) scores.</li>\n    </ul>\n<ul>\n      <li>The paper demonstrates that dilated convolutions are highly effective for dense prediction tasks, allowing for the integration of multi-scale context without loss of resolution.</li>\n      <li>The proposed context module and the simplified front-end module provide substantial performance gains in semantic segmentation.</li>\n      <li>The approach suggests a shift towards dedicated architectures for dense prediction, moving away from adaptations of image classification networks.</li>\n    </ul>",
      "contentMarkdown": "*   **Authors**: Fisher Yu, Vladlen Koltun\n*   **Affiliations**: Princeton University, Intel Labs\n    \n*   The paper “Multi-Scale Context Aggregation by Dilated Convolutions” presents a novel approach for improving semantic segmentation by leveraging dilated convolutions. This method allows convolutional neural networks to systematically aggregate multi-scale contextual information without losing resolution.\n    \n*   Key Contributions:\n    \n    1.  **Dilated Convolutions**:\n    \n    *   Introduces the concept of dilated convolutions, which enable exponential expansion of the receptive field without reducing resolution or coverage.\n    *   Dilated convolutions, also known as atrous convolutions, are crucial for dense prediction tasks as they support the aggregation of multi-scale context while preserving spatial resolution.\n\n**Affiliations**: Princeton University, Intel Labs\n\nThe paper “Multi-Scale Context Aggregation by Dilated Convolutions” presents a novel approach for improving semantic segmentation by leveraging dilated convolutions. This method allows convolutional neural networks to systematically aggregate multi-scale contextual information without losing resolution.\n\n1.  **Dilated Convolutions**:\n\n*   Introduces the concept of dilated convolutions, which enable exponential expansion of the receptive field without reducing resolution or coverage.\n*   Dilated convolutions, also known as atrous convolutions, are crucial for dense prediction tasks as they support the aggregation of multi-scale context while preserving spatial resolution.\n\n1.  **Multi-Scale Context Aggregation**:\n    *   Proposes a new convolutional network module that aggregates multi-scale contextual information, enhancing the performance of dense prediction architectures like semantic segmentation.\n    *   The network uses a rectangular prism of convolutional layers with varying dilation factors, eliminating the need for pooling or subsampling layers, thereby maintaining high resolution throughout the network.\n2.  **Simplified Network Design**:\n    *   Simplifies existing image classification networks adapted for dense prediction by removing unnecessary components and layers that do not contribute to performance.\n    *   Specifically, removes the last two pooling and striding layers in the VGG-16 network and uses dilated convolutions in subsequent layers to maintain high-resolution outputs.\n3.  **Controlled Experiments**:\n    *   Conducts experiments on the Pascal VOC 2012 dataset to evaluate the performance of the proposed context module.\n    *   Demonstrates that the context module reliably increases accuracy when integrated into existing semantic segmentation architectures, both with and without structured prediction methods like Conditional Random Fields (CRFs) and CRF-RNNs.\n4.  **Performance Improvement**:\n    *   The context module enhances the accuracy of semantic segmentation models, outperforming previous state-of-the-art models on the Pascal VOC 2012 test set.\n    *   The simplified front-end module alone achieves higher accuracy compared to prior models, indicating the effectiveness of removing vestigial components.\n\n*   Proposes a new convolutional network module that aggregates multi-scale contextual information, enhancing the performance of dense prediction architectures like semantic segmentation.\n*   The network uses a rectangular prism of convolutional layers with varying dilation factors, eliminating the need for pooling or subsampling layers, thereby maintaining high resolution throughout the network.\n\n*   Simplifies existing image classification networks adapted for dense prediction by removing unnecessary components and layers that do not contribute to performance.\n*   Specifically, removes the last two pooling and striding layers in the VGG-16 network and uses dilated convolutions in subsequent layers to maintain high-resolution outputs.\n\n*   Conducts experiments on the Pascal VOC 2012 dataset to evaluate the performance of the proposed context module.\n*   Demonstrates that the context module reliably increases accuracy when integrated into existing semantic segmentation architectures, both with and without structured prediction methods like Conditional Random Fields (CRFs) and CRF-RNNs.\n\n*   The context module enhances the accuracy of semantic segmentation models, outperforming previous state-of-the-art models on the Pascal VOC 2012 test set.\n*   The simplified front-end module alone achieves higher accuracy compared to prior models, indicating the effectiveness of removing vestigial components.\n\n*   Experiments:\n    *   **Dataset**: Uses the Pascal VOC 2012 dataset augmented with additional annotations for training.\n    *   **Training Procedure**: Employs stochastic gradient descent (SGD) with specific learning rates and momentum, and evaluates the performance on both validation and test sets.\n    *   **Evaluation**: The context module and simplified front-end are tested against models like FCN-8s and DeepLab, showing significant improvements in mean Intersection over Union (IoU) scores.\n*   Conclusion:\n    *   The paper demonstrates that dilated convolutions are highly effective for dense prediction tasks, allowing for the integration of multi-scale context without loss of resolution.\n    *   The proposed context module and the simplified front-end module provide substantial performance gains in semantic segmentation.\n    *   The approach suggests a shift towards dedicated architectures for dense prediction, moving away from adaptations of image classification networks.\n\n*   **Dataset**: Uses the Pascal VOC 2012 dataset augmented with additional annotations for training.\n*   **Training Procedure**: Employs stochastic gradient descent (SGD) with specific learning rates and momentum, and evaluates the performance on both validation and test sets.\n*   **Evaluation**: The context module and simplified front-end are tested against models like FCN-8s and DeepLab, showing significant improvements in mean Intersection over Union (IoU) scores.\n\n*   The paper demonstrates that dilated convolutions are highly effective for dense prediction tasks, allowing for the integration of multi-scale context without loss of resolution.\n*   The proposed context module and the simplified front-end module provide substantial performance gains in semantic segmentation.\n*   The approach suggests a shift towards dedicated architectures for dense prediction, moving away from adaptations of image classification networks.",
      "order": 11,
      "orderInChapter": 11,
      "difficulty": 2,
      "estimatedMinutes": 5,
      "tags": [
        "miscellaneous",
        "neural network",
        "convolution",
        "rnn",
        "gradient descent"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 835,
        "contentLength": 7420
      },
      "nextCards": [
        "ai-top-30-papers-neural-message-passing-for-quantum-chemistry-12",
        "ai-top-30-papers-attention-is-all-you-need-13"
      ],
      "relatedCards": [
        "ai-model-debugging-weight-initialization-11",
        "ai-gpu-architecture-operator-dispatch-and-kernel-mapping-14",
        "ai-gpu-architecture-model-definition-and-training-cpugpu-13",
        "ai-gpu-architecture-kernel-launch-and-execution-on-sms-16",
        "ai-ml-runtimes-architecture-10"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#multi-scale-context-aggregation-by-dilated-convolutions",
      "scrapedAt": "2025-12-28T11:56:36.590Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-neural-message-passing-for-quantum-chemistry-12",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Neural Message Passing for Quantum Chemistry",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>Authors: Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl</li>\n  <li>The paper “Neural Message Passing for Quantum Chemistry” introduces Message Passing Neural Networks (MPNNs), a framework for supervised learning on molecular graphs that is invariant to molecular symmetries. The goal is to predict quantum mechanical properties of molecules, which is crucial in fields such as drug discovery and materials science.</li>\n  <li><strong>Introduction</strong>:\n    <ul>\n      <li>The paper emphasizes the need for machine learning models capable of predicting molecular properties directly from their structure without relying on handcrafted features. Previous methods relied heavily on feature engineering, which limits generalizability and performance.</li>\n      <li>MPNNs unify several existing neural network models that operate on graph-structured data and allow for learning molecular properties directly from raw molecular graphs.</li>\n    </ul>\n  </li>\n  <li><strong>Methodology</strong>:\n    <ul>\n      <li><strong>Message Passing Phase</strong>: In this phase, nodes (atoms) exchange information with their neighbors through message functions. Each node updates its state based on the messages received from its neighbors and its current state.\n        <ul>\n          <li>Formally, for a graph<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">G</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">G</script> with node features<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-5\"><span class=\"msubsup\" id=\"MathJax-Span-6\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-8\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">x_v</script> and edge features<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi><mi>w</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-9\" style=\"width: 1.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.3em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-10\"><span class=\"msubsup\" id=\"MathJax-Span-11\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-13\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mi\" id=\"MathJax-Span-16\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi><mi>w</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">e_{vw}</script>, the messages<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-17\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.83em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-18\"><span class=\"msubsup\" id=\"MathJax-Span-19\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-20\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-21\"><span class=\"mrow\" id=\"MathJax-Span-22\"><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-24\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-25\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">m^{t+1}_v</script> and node updates<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>h</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-27\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.62em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-28\"><span class=\"msubsup\" id=\"MathJax-Span-29\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-31\"><span class=\"mrow\" id=\"MathJax-Span-32\"><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-34\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-35\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>h</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">h^{t+1}_v</script> are given by:\n <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>w</mi><mo>&amp;#x2208;</mo><mi>N</mi><mo stretchy=&quot;false&quot;>(</mo><mi>v</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></munder><msub><mi>M</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup><mo>,</mo><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi><mi>w</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-37\" style=\"width: 14.898em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1012.35em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-38\"><span class=\"msubsup\" id=\"MathJax-Span-39\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-41\"><span class=\"mrow\" id=\"MathJax-Span-42\"><span class=\"mi\" id=\"MathJax-Span-43\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-44\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-45\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-48\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-50\"><span class=\"mrow\" id=\"MathJax-Span-51\"><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-55\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-56\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-57\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-58\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-62\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-67\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-68\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.52em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-70\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-72\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-74\"><span class=\"mrow\" id=\"MathJax-Span-75\"><span class=\"mi\" id=\"MathJax-Span-76\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-78\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>w</mi><mo>∈</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></munder><msub><mi>M</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup><mo>,</mo><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi><mi>w</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">m^{t+1}_v = \\sum_{w \\in N(v)} M_t(h^t_v, h^t_w, e_{vw})</script>\n <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>h</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-79\" style=\"width: 9.221em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1007.61em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-80\"><span class=\"msubsup\" id=\"MathJax-Span-81\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-83\"><span class=\"mrow\" id=\"MathJax-Span-84\"><span class=\"mi\" id=\"MathJax-Span-85\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-86\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-87\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-88\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-90\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-91\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-92\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-94\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-96\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-97\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-99\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-101\"><span class=\"mrow\" id=\"MathJax-Span-102\"><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-105\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>h</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">h^{t+1}_v = U_t(h^t_v, m^{t+1}_v)</script></li>\n          <li>The message function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>M</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-108\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-109\"><span class=\"msubsup\" id=\"MathJax-Span-110\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>M</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">M_t</script> and update function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>U</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-113\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-114\"><span class=\"msubsup\" id=\"MathJax-Span-115\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-117\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>U</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">U_t</script> are learned during training.</li>\n        </ul>\n      </li>\n      <li><strong>Readout Phase</strong>: After the message passing phase, a readout function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>R</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-118\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-119\"><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Italic;\">R</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>R</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">R</script> aggregates the node states to produce the final output. The readout function must be invariant to permutations of the nodes to ensure the model’s invariance to graph isomorphism.</li>\n    </ul>\n  </li>\n  <li><strong>Key Contributions</strong>:\n    <ul>\n      <li><strong>State of the Art Results</strong>: The authors demonstrate that MPNNs achieve state-of-the-art performance on the QM9 dataset, a benchmark for predicting quantum mechanical properties of small organic molecules. MPNNs predict properties such as atomization energies, fundamental vibrational frequencies, and electronic properties with high accuracy.</li>\n      <li><strong>Chemical Accuracy</strong>: The models achieve chemical accuracy (within the error margin acceptable in chemistry) for 11 out of 13 properties in the QM9 dataset.</li>\n      <li><strong>Scalability</strong>: The paper also explores methods to scale MPNNs to larger graphs, making them more computationally efficient without sacrificing performance. This includes the use of “virtual graph elements” and modifications like the “towers” structure.</li>\n    </ul>\n  </li>\n  <li><strong>Results</strong>:\n    <ul>\n      <li>The authors provide extensive empirical results showing the superiority of MPNNs over traditional methods that rely on feature engineering. They demonstrate that MPNNs can learn complex molecular interactions directly from the data.</li>\n      <li>They compare different variants of MPNNs and show that models using edge network message functions and set2set readout functions perform particularly well.</li>\n    </ul>\n  </li>\n  <li><strong>Conclusion</strong>:\n    <ul>\n      <li>The study establishes MPNNs as a powerful tool for molecular property prediction, highlighting their potential to replace feature engineering with end-to-end learning from raw molecular graphs.</li>\n      <li>Future work suggested includes improving the generalization to larger molecular graphs and further optimizing the computational efficiency of MPNNs.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>The paper emphasizes the need for machine learning models capable of predicting molecular properties directly from their structure without relying on handcrafted features. Previous methods relied heavily on feature engineering, which limits generalizability and performance.</li>\n      <li>MPNNs unify several existing neural network models that operate on graph-structured data and allow for learning molecular properties directly from raw molecular graphs.</li>\n    </ul>\n<ul>\n      <li><strong>Message Passing Phase</strong>: In this phase, nodes (atoms) exchange information with their neighbors through message functions. Each node updates its state based on the messages received from its neighbors and its current state.\n        <ul>\n          <li>Formally, for a graph<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">G</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">G</script> with node features<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-5\"><span class=\"msubsup\" id=\"MathJax-Span-6\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-8\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">x_v</script> and edge features<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi><mi>w</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-9\" style=\"width: 1.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.3em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-10\"><span class=\"msubsup\" id=\"MathJax-Span-11\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-13\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mi\" id=\"MathJax-Span-16\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi><mi>w</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">e_{vw}</script>, the messages<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-17\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.83em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-18\"><span class=\"msubsup\" id=\"MathJax-Span-19\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-20\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-21\"><span class=\"mrow\" id=\"MathJax-Span-22\"><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-24\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-25\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">m^{t+1}_v</script> and node updates<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>h</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-27\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.62em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-28\"><span class=\"msubsup\" id=\"MathJax-Span-29\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-31\"><span class=\"mrow\" id=\"MathJax-Span-32\"><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-34\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-35\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>h</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">h^{t+1}_v</script> are given by:\n <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>w</mi><mo>&amp;#x2208;</mo><mi>N</mi><mo stretchy=&quot;false&quot;>(</mo><mi>v</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></munder><msub><mi>M</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup><mo>,</mo><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi><mi>w</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-37\" style=\"width: 14.898em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1012.35em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-38\"><span class=\"msubsup\" id=\"MathJax-Span-39\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-41\"><span class=\"mrow\" id=\"MathJax-Span-42\"><span class=\"mi\" id=\"MathJax-Span-43\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-44\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-45\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-48\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-50\"><span class=\"mrow\" id=\"MathJax-Span-51\"><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-55\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-56\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-57\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-58\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-62\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-67\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-68\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.52em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-70\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-72\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-74\"><span class=\"mrow\" id=\"MathJax-Span-75\"><span class=\"mi\" id=\"MathJax-Span-76\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-78\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>w</mi><mo>∈</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></munder><msub><mi>M</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup><mo>,</mo><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi><mi>w</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">m^{t+1}_v = \\sum_{w \\in N(v)} M_t(h^t_v, h^t_w, e_{vw})</script>\n <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>h</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-79\" style=\"width: 9.221em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1007.61em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-80\"><span class=\"msubsup\" id=\"MathJax-Span-81\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-83\"><span class=\"mrow\" id=\"MathJax-Span-84\"><span class=\"mi\" id=\"MathJax-Span-85\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-86\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-87\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-88\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-90\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-91\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-92\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-94\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-96\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-97\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-99\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-101\"><span class=\"mrow\" id=\"MathJax-Span-102\"><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-105\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>h</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">h^{t+1}_v = U_t(h^t_v, m^{t+1}_v)</script></li>\n          <li>The message function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>M</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-108\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-109\"><span class=\"msubsup\" id=\"MathJax-Span-110\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>M</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">M_t</script> and update function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>U</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-113\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-114\"><span class=\"msubsup\" id=\"MathJax-Span-115\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-117\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>U</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">U_t</script> are learned during training.</li>\n        </ul>\n      </li>\n      <li><strong>Readout Phase</strong>: After the message passing phase, a readout function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>R</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-118\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-119\"><span class=\"mi\" id=\"MathJax-Span-120\" style=\"font-family: STIXGeneral-Italic;\">R</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>R</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">R</script> aggregates the node states to produce the final output. The readout function must be invariant to permutations of the nodes to ensure the model’s invariance to graph isomorphism.</li>\n    </ul>\n<ul>\n          <li>Formally, for a graph<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">G</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">G</script> with node features<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>v</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4\" style=\"width: 1.044em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.84em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-5\"><span class=\"msubsup\" id=\"MathJax-Span-6\"><span style=\"display: inline-block; position: relative; width: 0.836em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-8\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mi>v</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">x_v</script> and edge features<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi><mi>w</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-9\" style=\"width: 1.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.3em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-10\"><span class=\"msubsup\" id=\"MathJax-Span-11\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-13\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mi\" id=\"MathJax-Span-16\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi><mi>w</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">e_{vw}</script>, the messages<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-17\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.83em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-18\"><span class=\"msubsup\" id=\"MathJax-Span-19\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-20\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-21\"><span class=\"mrow\" id=\"MathJax-Span-22\"><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-24\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-25\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">m^{t+1}_v</script> and node updates<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>h</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-27\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1001.62em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-28\"><span class=\"msubsup\" id=\"MathJax-Span-29\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-31\"><span class=\"mrow\" id=\"MathJax-Span-32\"><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-34\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-35\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-36\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>h</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">h^{t+1}_v</script> are given by:\n <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>w</mi><mo>&amp;#x2208;</mo><mi>N</mi><mo stretchy=&quot;false&quot;>(</mo><mi>v</mi><mo stretchy=&quot;false&quot;>)</mo></mrow></munder><msub><mi>M</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup><mo>,</mo><msub><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi><mi>w</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-37\" style=\"width: 14.898em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1012.35em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-38\"><span class=\"msubsup\" id=\"MathJax-Span-39\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-40\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-41\"><span class=\"mrow\" id=\"MathJax-Span-42\"><span class=\"mi\" id=\"MathJax-Span-43\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-44\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-45\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-48\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-49\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.695em; left: 0.94em;\"><span class=\"texatom\" id=\"MathJax-Span-50\"><span class=\"mrow\" id=\"MathJax-Span-51\"><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-53\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-55\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-56\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-57\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-58\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-62\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-63\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-66\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-67\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-68\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-69\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.52em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-70\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-72\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-73\" style=\"font-family: STIXGeneral-Italic;\">e</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-74\"><span class=\"mrow\" id=\"MathJax-Span-75\"><span class=\"mi\" id=\"MathJax-Span-76\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-78\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>w</mi><mo>∈</mo><mi>N</mi><mo stretchy=\"false\">(</mo><mi>v</mi><mo stretchy=\"false\">)</mo></mrow></munder><msub><mi>M</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>w</mi><mi>t</mi></msubsup><mo>,</mo><msub><mi>e</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi><mi>w</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">m^{t+1}_v = \\sum_{w \\in N(v)} M_t(h^t_v, h^t_w, e_{vw})</script>\n <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msubsup><mi>h</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy=&quot;false&quot;>(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>v</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-79\" style=\"width: 9.221em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1007.61em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-80\"><span class=\"msubsup\" id=\"MathJax-Span-81\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-83\"><span class=\"mrow\" id=\"MathJax-Span-84\"><span class=\"mi\" id=\"MathJax-Span-85\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-86\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-87\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-88\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-90\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-91\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-92\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-94\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Italic;\">h</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.492em, 1000.26em, 4.169em, -999.997em); top: -4.372em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-96\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-97\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-99\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Italic;\">m</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.15em, 4.221em, -999.997em); top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-101\"><span class=\"mrow\" id=\"MathJax-Span-102\"><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-105\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-107\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msubsup><mi>h</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>=</mo><msub><mi>U</mi><mi>t</mi></msub><mo stretchy=\"false\">(</mo><msubsup><mi>h</mi><mi>v</mi><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>v</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">h^{t+1}_v = U_t(h^t_v, m^{t+1}_v)</script></li>\n          <li>The message function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>M</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-108\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-109\"><span class=\"msubsup\" id=\"MathJax-Span-110\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.89em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.836em;\"><span class=\"mi\" id=\"MathJax-Span-112\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>M</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">M_t</script> and update function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>U</mi><mi>t</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-113\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.99em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-114\"><span class=\"msubsup\" id=\"MathJax-Span-115\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-116\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-117\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>U</mi><mi>t</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">U_t</script> are learned during training.</li>\n        </ul>\n<ul>\n      <li><strong>State of the Art Results</strong>: The authors demonstrate that MPNNs achieve state-of-the-art performance on the QM9 dataset, a benchmark for predicting quantum mechanical properties of small organic molecules. MPNNs predict properties such as atomization energies, fundamental vibrational frequencies, and electronic properties with high accuracy.</li>\n      <li><strong>Chemical Accuracy</strong>: The models achieve chemical accuracy (within the error margin acceptable in chemistry) for 11 out of 13 properties in the QM9 dataset.</li>\n      <li><strong>Scalability</strong>: The paper also explores methods to scale MPNNs to larger graphs, making them more computationally efficient without sacrificing performance. This includes the use of “virtual graph elements” and modifications like the “towers” structure.</li>\n    </ul>\n<ul>\n      <li>The authors provide extensive empirical results showing the superiority of MPNNs over traditional methods that rely on feature engineering. They demonstrate that MPNNs can learn complex molecular interactions directly from the data.</li>\n      <li>They compare different variants of MPNNs and show that models using edge network message functions and set2set readout functions perform particularly well.</li>\n    </ul>\n<ul>\n      <li>The study establishes MPNNs as a powerful tool for molecular property prediction, highlighting their potential to replace feature engineering with end-to-end learning from raw molecular graphs.</li>\n      <li>Future work suggested includes improving the generalization to larger molecular graphs and further optimizing the computational efficiency of MPNNs.</li>\n    </ul>",
      "contentMarkdown": "*   Authors: Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl\n*   The paper “Neural Message Passing for Quantum Chemistry” introduces Message Passing Neural Networks (MPNNs), a framework for supervised learning on molecular graphs that is invariant to molecular symmetries. The goal is to predict quantum mechanical properties of molecules, which is crucial in fields such as drug discovery and materials science.\n*   **Introduction**:\n    *   The paper emphasizes the need for machine learning models capable of predicting molecular properties directly from their structure without relying on handcrafted features. Previous methods relied heavily on feature engineering, which limits generalizability and performance.\n    *   MPNNs unify several existing neural network models that operate on graph-structured data and allow for learning molecular properties directly from raw molecular graphs.\n*   **Methodology**:\n    *   **Message Passing Phase**: In this phase, nodes (atoms) exchange information with their neighbors through message functions. Each node updates its state based on the messages received from its neighbors and its current state.\n        *   Formally, for a graphGGG with node featuresxvxvx\\_v and edge featuresevwevwe\\_{vw}, the messagesmt+1vmvt+1m^{t+1}\\_v and node updatesht+1vhvt+1h^{t+1}\\_v are given by: mt+1v\\=∑w∈N(v)Mt(htv,htw,evw)mvt+1\\=∑w∈N(v)Mt(hvt,hwt,evw)m^{t+1}\\_v = \\\\sum\\_{w \\\\in N(v)} M\\_t(h^t\\_v, h^t\\_w, e\\_{vw}) ht+1v\\=Ut(htv,mt+1v)hvt+1\\=Ut(hvt,mvt+1)h^{t+1}\\_v = U\\_t(h^t\\_v, m^{t+1}\\_v)\n        *   The message functionMtMtM\\_t and update functionUtUtU\\_t are learned during training.\n    *   **Readout Phase**: After the message passing phase, a readout functionRRR aggregates the node states to produce the final output. The readout function must be invariant to permutations of the nodes to ensure the model’s invariance to graph isomorphism.\n*   **Key Contributions**:\n    *   **State of the Art Results**: The authors demonstrate that MPNNs achieve state-of-the-art performance on the QM9 dataset, a benchmark for predicting quantum mechanical properties of small organic molecules. MPNNs predict properties such as atomization energies, fundamental vibrational frequencies, and electronic properties with high accuracy.\n    *   **Chemical Accuracy**: The models achieve chemical accuracy (within the error margin acceptable in chemistry) for 11 out of 13 properties in the QM9 dataset.\n    *   **Scalability**: The paper also explores methods to scale MPNNs to larger graphs, making them more computationally efficient without sacrificing performance. This includes the use of “virtual graph elements” and modifications like the “towers” structure.\n*   **Results**:\n    *   The authors provide extensive empirical results showing the superiority of MPNNs over traditional methods that rely on feature engineering. They demonstrate that MPNNs can learn complex molecular interactions directly from the data.\n    *   They compare different variants of MPNNs and show that models using edge network message functions and set2set readout functions perform particularly well.\n*   **Conclusion**:\n    *   The study establishes MPNNs as a powerful tool for molecular property prediction, highlighting their potential to replace feature engineering with end-to-end learning from raw molecular graphs.\n    *   Future work suggested includes improving the generalization to larger molecular graphs and further optimizing the computational efficiency of MPNNs.\n\n*   The paper emphasizes the need for machine learning models capable of predicting molecular properties directly from their structure without relying on handcrafted features. Previous methods relied heavily on feature engineering, which limits generalizability and performance.\n*   MPNNs unify several existing neural network models that operate on graph-structured data and allow for learning molecular properties directly from raw molecular graphs.\n\n*   **Message Passing Phase**: In this phase, nodes (atoms) exchange information with their neighbors through message functions. Each node updates its state based on the messages received from its neighbors and its current state.\n    *   Formally, for a graphGGG with node featuresxvxvx\\_v and edge featuresevwevwe\\_{vw}, the messagesmt+1vmvt+1m^{t+1}\\_v and node updatesht+1vhvt+1h^{t+1}\\_v are given by: mt+1v\\=∑w∈N(v)Mt(htv,htw,evw)mvt+1\\=∑w∈N(v)Mt(hvt,hwt,evw)m^{t+1}\\_v = \\\\sum\\_{w \\\\in N(v)} M\\_t(h^t\\_v, h^t\\_w, e\\_{vw}) ht+1v\\=Ut(htv,mt+1v)hvt+1\\=Ut(hvt,mvt+1)h^{t+1}\\_v = U\\_t(h^t\\_v, m^{t+1}\\_v)\n    *   The message functionMtMtM\\_t and update functionUtUtU\\_t are learned during training.\n*   **Readout Phase**: After the message passing phase, a readout functionRRR aggregates the node states to produce the final output. The readout function must be invariant to permutations of the nodes to ensure the model’s invariance to graph isomorphism.\n\n*   Formally, for a graphGGG with node featuresxvxvx\\_v and edge featuresevwevwe\\_{vw}, the messagesmt+1vmvt+1m^{t+1}\\_v and node updatesht+1vhvt+1h^{t+1}\\_v are given by: mt+1v\\=∑w∈N(v)Mt(htv,htw,evw)mvt+1\\=∑w∈N(v)Mt(hvt,hwt,evw)m^{t+1}\\_v = \\\\sum\\_{w \\\\in N(v)} M\\_t(h^t\\_v, h^t\\_w, e\\_{vw}) ht+1v\\=Ut(htv,mt+1v)hvt+1\\=Ut(hvt,mvt+1)h^{t+1}\\_v = U\\_t(h^t\\_v, m^{t+1}\\_v)\n*   The message functionMtMtM\\_t and update functionUtUtU\\_t are learned during training.\n\n*   **State of the Art Results**: The authors demonstrate that MPNNs achieve state-of-the-art performance on the QM9 dataset, a benchmark for predicting quantum mechanical properties of small organic molecules. MPNNs predict properties such as atomization energies, fundamental vibrational frequencies, and electronic properties with high accuracy.\n*   **Chemical Accuracy**: The models achieve chemical accuracy (within the error margin acceptable in chemistry) for 11 out of 13 properties in the QM9 dataset.\n*   **Scalability**: The paper also explores methods to scale MPNNs to larger graphs, making them more computationally efficient without sacrificing performance. This includes the use of “virtual graph elements” and modifications like the “towers” structure.\n\n*   The authors provide extensive empirical results showing the superiority of MPNNs over traditional methods that rely on feature engineering. They demonstrate that MPNNs can learn complex molecular interactions directly from the data.\n*   They compare different variants of MPNNs and show that models using edge network message functions and set2set readout functions perform particularly well.\n\n*   The study establishes MPNNs as a powerful tool for molecular property prediction, highlighting their potential to replace feature engineering with end-to-end learning from raw molecular graphs.\n*   Future work suggested includes improving the generalization to larger molecular graphs and further optimizing the computational efficiency of MPNNs.",
      "order": 12,
      "orderInChapter": 12,
      "difficulty": 3,
      "estimatedMinutes": 5,
      "tags": [
        "miscellaneous",
        "neural network",
        "machine learning",
        "supervised learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 885,
        "contentLength": 103662
      },
      "nextCards": [
        "ai-top-30-papers-attention-is-all-you-need-13",
        "ai-top-30-papers-neural-machine-translation-by-jointly-learning-to--14"
      ],
      "relatedCards": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-pipeline-issues-6",
        "ai-ml-runtimes-architecture-10",
        "ai-ml-runtimes-architecture-16",
        "ai-ml-runtimes-overview-20"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#neural-message-passing-for-quantum-chemistry",
      "scrapedAt": "2025-12-28T11:56:36.590Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-attention-is-all-you-need-13",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Attention is All You Need",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li><strong>Authors</strong>: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin</li>\n  <li>\n    <p><strong>Affiliations</strong>: Google Brain, Google Research, University of Toronto</p>\n  </li>\n  <li>\n    <p>The paper “Attention Is All You Need” introduces the Transformer, a novel neural network architecture that relies entirely on self-attention mechanisms, dispensing with recurrence and convolutions entirely. This model architecture significantly improves computational efficiency and parallelization, leading to state-of-the-art performance in various sequence transduction tasks such as machine translation.</p>\n  </li>\n  <li>Key Contributions:\n    <ol>\n      <li><strong>Transformer Architecture</strong>:\n        <ul>\n          <li>The Transformer uses a novel architecture based solely on attention mechanisms, enabling the model to draw global dependencies between input and output without using sequence-aligned RNNs or convolutions.</li>\n          <li>The architecture comprises an encoder-decoder structure where both the encoder and decoder are composed of multiple identical layers, each consisting of a multi-head self-attention mechanism followed by a position-wise fully connected feed-forward network.</li>\n        </ul>\n      </li>\n      <li><strong>Self-Attention Mechanism</strong>:\n        <ul>\n          <li><strong>Scaled Dot-Product Attention</strong>: This is the core component of the self-attention mechanism, where the dot products of the query with all keys are computed, scaled, and passed through a softmax function to obtain the weights on the values.</li>\n          <li><strong>Multi-Head Attention</strong>: Allows the model to jointly attend to information from different representation subspaces at different positions by performing multiple attention operations in parallel, each with different learned linear projections.</li>\n        </ul>\n      </li>\n      <li><strong>Positional Encoding</strong>:\n        <ul>\n          <li>Since the Transformer model does not use recurrence to handle sequence order, positional encodings are added to the input embeddings to inject information about the position of each token in the sequence. The authors use sine and cosine functions of different frequencies for these encodings.</li>\n        </ul>\n      </li>\n      <li><strong>Training Efficiency and Performance</strong>:\n        <ul>\n          <li>The Transformer model achieves superior performance on machine translation tasks while being more parallelizable and requiring significantly less time to train compared to RNN-based models.</li>\n          <li>For the WMT 2014 English-to-German translation task, the Transformer achieves a BLEU score of 28.4, outperforming previous state-of-the-art models by over 2 BLEU points. Similarly, it achieves a BLEU score of 41.8 on the WMT 2014 English-to-French translation task with much less training time.</li>\n        </ul>\n      </li>\n      <li><strong>Generalization to Other Tasks</strong>:\n        <ul>\n          <li>The Transformer model generalizes well to other tasks beyond machine translation. The paper demonstrates its effectiveness in English constituency parsing, achieving competitive results with less task-specific tuning.</li>\n        </ul>\n      </li>\n      <li><strong>Advantages Over Previous Models</strong>:\n        <ul>\n          <li>The Transformer reduces the path length between long-range dependencies to a constant number of operations, unlike RNNs and convolutional models, which grow linearly or logarithmically with the sequence length.</li>\n          <li>This reduction in path length improves the model’s ability to learn dependencies between distant positions, leading to better performance in sequence transduction tasks.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>Experimental Results:\n    <ul>\n      <li><strong>Machine Translation</strong>: The Transformer sets new benchmarks in BLEU scores for both English-to-German and English-to-French translation tasks, showcasing its superior translation quality and training efficiency.</li>\n      <li><strong>Model Variations</strong>: The paper explores various modifications to the Transformer architecture, including the number of attention heads and the size of attention key/value dimensions, demonstrating the robustness and flexibility of the model.</li>\n      <li><strong>English Constituency Parsing</strong>: The model achieves high F1 scores on the Penn Treebank dataset, indicating its capability to generalize to different natural language processing tasks.</li>\n    </ul>\n  </li>\n  <li>Conclusion:\n    <ul>\n      <li>The Transformer represents a significant advancement in sequence transduction models, providing a highly efficient and effective alternative to traditional RNN and convolution-based architectures.</li>\n      <li>Its reliance on self-attention mechanisms not only improves performance but also allows for greater parallelization, making it suitable for a wide range of applications in natural language processing and beyond.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Affiliations</strong>: Google Brain, Google Research, University of Toronto</p>\n<p>The paper “Attention Is All You Need” introduces the Transformer, a novel neural network architecture that relies entirely on self-attention mechanisms, dispensing with recurrence and convolutions entirely. This model architecture significantly improves computational efficiency and parallelization, leading to state-of-the-art performance in various sequence transduction tasks such as machine translation.</p>\n<ol>\n      <li><strong>Transformer Architecture</strong>:\n        <ul>\n          <li>The Transformer uses a novel architecture based solely on attention mechanisms, enabling the model to draw global dependencies between input and output without using sequence-aligned RNNs or convolutions.</li>\n          <li>The architecture comprises an encoder-decoder structure where both the encoder and decoder are composed of multiple identical layers, each consisting of a multi-head self-attention mechanism followed by a position-wise fully connected feed-forward network.</li>\n        </ul>\n      </li>\n      <li><strong>Self-Attention Mechanism</strong>:\n        <ul>\n          <li><strong>Scaled Dot-Product Attention</strong>: This is the core component of the self-attention mechanism, where the dot products of the query with all keys are computed, scaled, and passed through a softmax function to obtain the weights on the values.</li>\n          <li><strong>Multi-Head Attention</strong>: Allows the model to jointly attend to information from different representation subspaces at different positions by performing multiple attention operations in parallel, each with different learned linear projections.</li>\n        </ul>\n      </li>\n      <li><strong>Positional Encoding</strong>:\n        <ul>\n          <li>Since the Transformer model does not use recurrence to handle sequence order, positional encodings are added to the input embeddings to inject information about the position of each token in the sequence. The authors use sine and cosine functions of different frequencies for these encodings.</li>\n        </ul>\n      </li>\n      <li><strong>Training Efficiency and Performance</strong>:\n        <ul>\n          <li>The Transformer model achieves superior performance on machine translation tasks while being more parallelizable and requiring significantly less time to train compared to RNN-based models.</li>\n          <li>For the WMT 2014 English-to-German translation task, the Transformer achieves a BLEU score of 28.4, outperforming previous state-of-the-art models by over 2 BLEU points. Similarly, it achieves a BLEU score of 41.8 on the WMT 2014 English-to-French translation task with much less training time.</li>\n        </ul>\n      </li>\n      <li><strong>Generalization to Other Tasks</strong>:\n        <ul>\n          <li>The Transformer model generalizes well to other tasks beyond machine translation. The paper demonstrates its effectiveness in English constituency parsing, achieving competitive results with less task-specific tuning.</li>\n        </ul>\n      </li>\n      <li><strong>Advantages Over Previous Models</strong>:\n        <ul>\n          <li>The Transformer reduces the path length between long-range dependencies to a constant number of operations, unlike RNNs and convolutional models, which grow linearly or logarithmically with the sequence length.</li>\n          <li>This reduction in path length improves the model’s ability to learn dependencies between distant positions, leading to better performance in sequence transduction tasks.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>The Transformer uses a novel architecture based solely on attention mechanisms, enabling the model to draw global dependencies between input and output without using sequence-aligned RNNs or convolutions.</li>\n          <li>The architecture comprises an encoder-decoder structure where both the encoder and decoder are composed of multiple identical layers, each consisting of a multi-head self-attention mechanism followed by a position-wise fully connected feed-forward network.</li>\n        </ul>\n<ul>\n          <li><strong>Scaled Dot-Product Attention</strong>: This is the core component of the self-attention mechanism, where the dot products of the query with all keys are computed, scaled, and passed through a softmax function to obtain the weights on the values.</li>\n          <li><strong>Multi-Head Attention</strong>: Allows the model to jointly attend to information from different representation subspaces at different positions by performing multiple attention operations in parallel, each with different learned linear projections.</li>\n        </ul>\n<ul>\n          <li>Since the Transformer model does not use recurrence to handle sequence order, positional encodings are added to the input embeddings to inject information about the position of each token in the sequence. The authors use sine and cosine functions of different frequencies for these encodings.</li>\n        </ul>\n<ul>\n          <li>The Transformer model achieves superior performance on machine translation tasks while being more parallelizable and requiring significantly less time to train compared to RNN-based models.</li>\n          <li>For the WMT 2014 English-to-German translation task, the Transformer achieves a BLEU score of 28.4, outperforming previous state-of-the-art models by over 2 BLEU points. Similarly, it achieves a BLEU score of 41.8 on the WMT 2014 English-to-French translation task with much less training time.</li>\n        </ul>\n<ul>\n          <li>The Transformer model generalizes well to other tasks beyond machine translation. The paper demonstrates its effectiveness in English constituency parsing, achieving competitive results with less task-specific tuning.</li>\n        </ul>\n<ul>\n          <li>The Transformer reduces the path length between long-range dependencies to a constant number of operations, unlike RNNs and convolutional models, which grow linearly or logarithmically with the sequence length.</li>\n          <li>This reduction in path length improves the model’s ability to learn dependencies between distant positions, leading to better performance in sequence transduction tasks.</li>\n        </ul>\n<ul>\n      <li><strong>Machine Translation</strong>: The Transformer sets new benchmarks in BLEU scores for both English-to-German and English-to-French translation tasks, showcasing its superior translation quality and training efficiency.</li>\n      <li><strong>Model Variations</strong>: The paper explores various modifications to the Transformer architecture, including the number of attention heads and the size of attention key/value dimensions, demonstrating the robustness and flexibility of the model.</li>\n      <li><strong>English Constituency Parsing</strong>: The model achieves high F1 scores on the Penn Treebank dataset, indicating its capability to generalize to different natural language processing tasks.</li>\n    </ul>\n<ul>\n      <li>The Transformer represents a significant advancement in sequence transduction models, providing a highly efficient and effective alternative to traditional RNN and convolution-based architectures.</li>\n      <li>Its reliance on self-attention mechanisms not only improves performance but also allows for greater parallelization, making it suitable for a wide range of applications in natural language processing and beyond.</li>\n    </ul>",
      "contentMarkdown": "*   **Authors**: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n*   **Affiliations**: Google Brain, Google Research, University of Toronto\n    \n*   The paper “Attention Is All You Need” introduces the Transformer, a novel neural network architecture that relies entirely on self-attention mechanisms, dispensing with recurrence and convolutions entirely. This model architecture significantly improves computational efficiency and parallelization, leading to state-of-the-art performance in various sequence transduction tasks such as machine translation.\n    \n*   Key Contributions:\n    1.  **Transformer Architecture**:\n        *   The Transformer uses a novel architecture based solely on attention mechanisms, enabling the model to draw global dependencies between input and output without using sequence-aligned RNNs or convolutions.\n        *   The architecture comprises an encoder-decoder structure where both the encoder and decoder are composed of multiple identical layers, each consisting of a multi-head self-attention mechanism followed by a position-wise fully connected feed-forward network.\n    2.  **Self-Attention Mechanism**:\n        *   **Scaled Dot-Product Attention**: This is the core component of the self-attention mechanism, where the dot products of the query with all keys are computed, scaled, and passed through a softmax function to obtain the weights on the values.\n        *   **Multi-Head Attention**: Allows the model to jointly attend to information from different representation subspaces at different positions by performing multiple attention operations in parallel, each with different learned linear projections.\n    3.  **Positional Encoding**:\n        *   Since the Transformer model does not use recurrence to handle sequence order, positional encodings are added to the input embeddings to inject information about the position of each token in the sequence. The authors use sine and cosine functions of different frequencies for these encodings.\n    4.  **Training Efficiency and Performance**:\n        *   The Transformer model achieves superior performance on machine translation tasks while being more parallelizable and requiring significantly less time to train compared to RNN-based models.\n        *   For the WMT 2014 English-to-German translation task, the Transformer achieves a BLEU score of 28.4, outperforming previous state-of-the-art models by over 2 BLEU points. Similarly, it achieves a BLEU score of 41.8 on the WMT 2014 English-to-French translation task with much less training time.\n    5.  **Generalization to Other Tasks**:\n        *   The Transformer model generalizes well to other tasks beyond machine translation. The paper demonstrates its effectiveness in English constituency parsing, achieving competitive results with less task-specific tuning.\n    6.  **Advantages Over Previous Models**:\n        *   The Transformer reduces the path length between long-range dependencies to a constant number of operations, unlike RNNs and convolutional models, which grow linearly or logarithmically with the sequence length.\n        *   This reduction in path length improves the model’s ability to learn dependencies between distant positions, leading to better performance in sequence transduction tasks.\n*   Experimental Results:\n    *   **Machine Translation**: The Transformer sets new benchmarks in BLEU scores for both English-to-German and English-to-French translation tasks, showcasing its superior translation quality and training efficiency.\n    *   **Model Variations**: The paper explores various modifications to the Transformer architecture, including the number of attention heads and the size of attention key/value dimensions, demonstrating the robustness and flexibility of the model.\n    *   **English Constituency Parsing**: The model achieves high F1 scores on the Penn Treebank dataset, indicating its capability to generalize to different natural language processing tasks.\n*   Conclusion:\n    *   The Transformer represents a significant advancement in sequence transduction models, providing a highly efficient and effective alternative to traditional RNN and convolution-based architectures.\n    *   Its reliance on self-attention mechanisms not only improves performance but also allows for greater parallelization, making it suitable for a wide range of applications in natural language processing and beyond.\n\n**Affiliations**: Google Brain, Google Research, University of Toronto\n\nThe paper “Attention Is All You Need” introduces the Transformer, a novel neural network architecture that relies entirely on self-attention mechanisms, dispensing with recurrence and convolutions entirely. This model architecture significantly improves computational efficiency and parallelization, leading to state-of-the-art performance in various sequence transduction tasks such as machine translation.\n\n1.  **Transformer Architecture**:\n    *   The Transformer uses a novel architecture based solely on attention mechanisms, enabling the model to draw global dependencies between input and output without using sequence-aligned RNNs or convolutions.\n    *   The architecture comprises an encoder-decoder structure where both the encoder and decoder are composed of multiple identical layers, each consisting of a multi-head self-attention mechanism followed by a position-wise fully connected feed-forward network.\n2.  **Self-Attention Mechanism**:\n    *   **Scaled Dot-Product Attention**: This is the core component of the self-attention mechanism, where the dot products of the query with all keys are computed, scaled, and passed through a softmax function to obtain the weights on the values.\n    *   **Multi-Head Attention**: Allows the model to jointly attend to information from different representation subspaces at different positions by performing multiple attention operations in parallel, each with different learned linear projections.\n3.  **Positional Encoding**:\n    *   Since the Transformer model does not use recurrence to handle sequence order, positional encodings are added to the input embeddings to inject information about the position of each token in the sequence. The authors use sine and cosine functions of different frequencies for these encodings.\n4.  **Training Efficiency and Performance**:\n    *   The Transformer model achieves superior performance on machine translation tasks while being more parallelizable and requiring significantly less time to train compared to RNN-based models.\n    *   For the WMT 2014 English-to-German translation task, the Transformer achieves a BLEU score of 28.4, outperforming previous state-of-the-art models by over 2 BLEU points. Similarly, it achieves a BLEU score of 41.8 on the WMT 2014 English-to-French translation task with much less training time.\n5.  **Generalization to Other Tasks**:\n    *   The Transformer model generalizes well to other tasks beyond machine translation. The paper demonstrates its effectiveness in English constituency parsing, achieving competitive results with less task-specific tuning.\n6.  **Advantages Over Previous Models**:\n    *   The Transformer reduces the path length between long-range dependencies to a constant number of operations, unlike RNNs and convolutional models, which grow linearly or logarithmically with the sequence length.\n    *   This reduction in path length improves the model’s ability to learn dependencies between distant positions, leading to better performance in sequence transduction tasks.\n\n*   The Transformer uses a novel architecture based solely on attention mechanisms, enabling the model to draw global dependencies between input and output without using sequence-aligned RNNs or convolutions.\n*   The architecture comprises an encoder-decoder structure where both the encoder and decoder are composed of multiple identical layers, each consisting of a multi-head self-attention mechanism followed by a position-wise fully connected feed-forward network.\n\n*   **Scaled Dot-Product Attention**: This is the core component of the self-attention mechanism, where the dot products of the query with all keys are computed, scaled, and passed through a softmax function to obtain the weights on the values.\n*   **Multi-Head Attention**: Allows the model to jointly attend to information from different representation subspaces at different positions by performing multiple attention operations in parallel, each with different learned linear projections.\n\n*   Since the Transformer model does not use recurrence to handle sequence order, positional encodings are added to the input embeddings to inject information about the position of each token in the sequence. The authors use sine and cosine functions of different frequencies for these encodings.\n\n*   The Transformer model achieves superior performance on machine translation tasks while being more parallelizable and requiring significantly less time to train compared to RNN-based models.\n*   For the WMT 2014 English-to-German translation task, the Transformer achieves a BLEU score of 28.4, outperforming previous state-of-the-art models by over 2 BLEU points. Similarly, it achieves a BLEU score of 41.8 on the WMT 2014 English-to-French translation task with much less training time.\n\n*   The Transformer model generalizes well to other tasks beyond machine translation. The paper demonstrates its effectiveness in English constituency parsing, achieving competitive results with less task-specific tuning.\n\n*   The Transformer reduces the path length between long-range dependencies to a constant number of operations, unlike RNNs and convolutional models, which grow linearly or logarithmically with the sequence length.\n*   This reduction in path length improves the model’s ability to learn dependencies between distant positions, leading to better performance in sequence transduction tasks.\n\n*   **Machine Translation**: The Transformer sets new benchmarks in BLEU scores for both English-to-German and English-to-French translation tasks, showcasing its superior translation quality and training efficiency.\n*   **Model Variations**: The paper explores various modifications to the Transformer architecture, including the number of attention heads and the size of attention key/value dimensions, demonstrating the robustness and flexibility of the model.\n*   **English Constituency Parsing**: The model achieves high F1 scores on the Penn Treebank dataset, indicating its capability to generalize to different natural language processing tasks.\n\n*   The Transformer represents a significant advancement in sequence transduction models, providing a highly efficient and effective alternative to traditional RNN and convolution-based architectures.\n*   Its reliance on self-attention mechanisms not only improves performance but also allows for greater parallelization, making it suitable for a wide range of applications in natural language processing and beyond.",
      "order": 13,
      "orderInChapter": 13,
      "difficulty": 3,
      "estimatedMinutes": 8,
      "tags": [
        "miscellaneous",
        "neural network",
        "transformer",
        "attention",
        "embedding",
        "convolution",
        "rnn"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 1467,
        "contentLength": 12619
      },
      "nextCards": [
        "ai-top-30-papers-neural-machine-translation-by-jointly-learning-to--14",
        "ai-top-30-papers-identity-mappings-in-deep-residual-networks-15"
      ],
      "relatedCards": [
        "ai-ml-runtimes-supported-model-types-11",
        "ai-ml-runtimes-architecture-33",
        "ai-ml-runtimes-runtime-specific-execution-lifecycles-62",
        "ai-ml-runtimes-architecture-40",
        "ai-cnns-for-text-classification-1-d-convolutions-over-text-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#attention-is-all-you-need",
      "scrapedAt": "2025-12-28T11:56:36.591Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-neural-machine-translation-by-jointly-learning-to--14",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li><strong>Author</strong>: Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio</li>\n  <li><strong>Abstract</strong>: Neural machine translation (NMT) is an emerging approach that builds a single neural network to maximize translation performance. Unlike traditional methods, NMT uses encoder-decoder architectures to translate sentences. This paper introduces a method allowing the model to search for relevant parts of a source sentence during translation, enhancing performance.</li>\n  <li><strong>Key Concepts</strong>:\n    <ul>\n      <li><strong>Encoder-Decoder Model</strong>: The basic architecture for NMT, where the encoder converts a source sentence into a fixed-length vector, and the decoder generates the translation.</li>\n      <li><strong>Fixed-Length Vector Bottleneck</strong>: A significant limitation of traditional encoder-decoder models is the fixed-length vector, which hampers performance, especially for long sentences.</li>\n      <li><strong>Attention Mechanism</strong>: This model introduces an attention mechanism that enables the decoder to focus on relevant parts of the source sentence dynamically. This improves translation quality by addressing the fixed-length vector bottleneck.</li>\n    </ul>\n  </li>\n  <li><strong>Proposed Model</strong>:\n    <ul>\n      <li><strong>Bidirectional RNN Encoder</strong>: Encodes the input sentence into a sequence of vectors rather than a single vector, capturing more context.</li>\n      <li><strong>Attention-Based Decoder</strong>: Computes a weighted sum of these vectors for each target word, allowing the model to focus on different parts of the source sentence for each target word.</li>\n    </ul>\n  </li>\n  <li><strong>Performance</strong>:\n    <ul>\n      <li>The proposed model outperforms traditional RNN encoder-decoder models, especially with longer sentences.</li>\n      <li>Achieves comparable results to state-of-the-art phrase-based systems on English-to-French translation tasks.</li>\n      <li>Qualitative analysis shows that the alignments produced by the model are linguistically plausible.</li>\n    </ul>\n  </li>\n  <li><strong>Experiment</strong>:\n    <ul>\n      <li>The models were tested on the WMT ’14 English-to-French translation task.</li>\n      <li>The proposed model demonstrates significant improvements over the basic encoder-decoder model in BLEU scores.</li>\n    </ul>\n  </li>\n  <li><strong>Conclusion</strong>:\n    <ul>\n      <li>The attention mechanism significantly enhances the NMT model’s ability to handle long sentences and complex linguistic structures.</li>\n      <li>Future work should address handling unknown or rare words to further improve translation performance.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Encoder-Decoder Model</strong>: The basic architecture for NMT, where the encoder converts a source sentence into a fixed-length vector, and the decoder generates the translation.</li>\n      <li><strong>Fixed-Length Vector Bottleneck</strong>: A significant limitation of traditional encoder-decoder models is the fixed-length vector, which hampers performance, especially for long sentences.</li>\n      <li><strong>Attention Mechanism</strong>: This model introduces an attention mechanism that enables the decoder to focus on relevant parts of the source sentence dynamically. This improves translation quality by addressing the fixed-length vector bottleneck.</li>\n    </ul>\n<ul>\n      <li><strong>Bidirectional RNN Encoder</strong>: Encodes the input sentence into a sequence of vectors rather than a single vector, capturing more context.</li>\n      <li><strong>Attention-Based Decoder</strong>: Computes a weighted sum of these vectors for each target word, allowing the model to focus on different parts of the source sentence for each target word.</li>\n    </ul>\n<ul>\n      <li>The proposed model outperforms traditional RNN encoder-decoder models, especially with longer sentences.</li>\n      <li>Achieves comparable results to state-of-the-art phrase-based systems on English-to-French translation tasks.</li>\n      <li>Qualitative analysis shows that the alignments produced by the model are linguistically plausible.</li>\n    </ul>\n<ul>\n      <li>The models were tested on the WMT ’14 English-to-French translation task.</li>\n      <li>The proposed model demonstrates significant improvements over the basic encoder-decoder model in BLEU scores.</li>\n    </ul>\n<ul>\n      <li>The attention mechanism significantly enhances the NMT model’s ability to handle long sentences and complex linguistic structures.</li>\n      <li>Future work should address handling unknown or rare words to further improve translation performance.</li>\n    </ul>",
      "contentMarkdown": "*   **Author**: Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio\n*   **Abstract**: Neural machine translation (NMT) is an emerging approach that builds a single neural network to maximize translation performance. Unlike traditional methods, NMT uses encoder-decoder architectures to translate sentences. This paper introduces a method allowing the model to search for relevant parts of a source sentence during translation, enhancing performance.\n*   **Key Concepts**:\n    *   **Encoder-Decoder Model**: The basic architecture for NMT, where the encoder converts a source sentence into a fixed-length vector, and the decoder generates the translation.\n    *   **Fixed-Length Vector Bottleneck**: A significant limitation of traditional encoder-decoder models is the fixed-length vector, which hampers performance, especially for long sentences.\n    *   **Attention Mechanism**: This model introduces an attention mechanism that enables the decoder to focus on relevant parts of the source sentence dynamically. This improves translation quality by addressing the fixed-length vector bottleneck.\n*   **Proposed Model**:\n    *   **Bidirectional RNN Encoder**: Encodes the input sentence into a sequence of vectors rather than a single vector, capturing more context.\n    *   **Attention-Based Decoder**: Computes a weighted sum of these vectors for each target word, allowing the model to focus on different parts of the source sentence for each target word.\n*   **Performance**:\n    *   The proposed model outperforms traditional RNN encoder-decoder models, especially with longer sentences.\n    *   Achieves comparable results to state-of-the-art phrase-based systems on English-to-French translation tasks.\n    *   Qualitative analysis shows that the alignments produced by the model are linguistically plausible.\n*   **Experiment**:\n    *   The models were tested on the WMT ’14 English-to-French translation task.\n    *   The proposed model demonstrates significant improvements over the basic encoder-decoder model in BLEU scores.\n*   **Conclusion**:\n    *   The attention mechanism significantly enhances the NMT model’s ability to handle long sentences and complex linguistic structures.\n    *   Future work should address handling unknown or rare words to further improve translation performance.\n\n*   **Encoder-Decoder Model**: The basic architecture for NMT, where the encoder converts a source sentence into a fixed-length vector, and the decoder generates the translation.\n*   **Fixed-Length Vector Bottleneck**: A significant limitation of traditional encoder-decoder models is the fixed-length vector, which hampers performance, especially for long sentences.\n*   **Attention Mechanism**: This model introduces an attention mechanism that enables the decoder to focus on relevant parts of the source sentence dynamically. This improves translation quality by addressing the fixed-length vector bottleneck.\n\n*   **Bidirectional RNN Encoder**: Encodes the input sentence into a sequence of vectors rather than a single vector, capturing more context.\n*   **Attention-Based Decoder**: Computes a weighted sum of these vectors for each target word, allowing the model to focus on different parts of the source sentence for each target word.\n\n*   The proposed model outperforms traditional RNN encoder-decoder models, especially with longer sentences.\n*   Achieves comparable results to state-of-the-art phrase-based systems on English-to-French translation tasks.\n*   Qualitative analysis shows that the alignments produced by the model are linguistically plausible.\n\n*   The models were tested on the WMT ’14 English-to-French translation task.\n*   The proposed model demonstrates significant improvements over the basic encoder-decoder model in BLEU scores.\n\n*   The attention mechanism significantly enhances the NMT model’s ability to handle long sentences and complex linguistic structures.\n*   Future work should address handling unknown or rare words to further improve translation performance.",
      "order": 14,
      "orderInChapter": 14,
      "difficulty": 2,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous",
        "neural network",
        "attention",
        "rnn"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 531,
        "contentLength": 4685
      },
      "nextCards": [
        "ai-top-30-papers-identity-mappings-in-deep-residual-networks-15",
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16"
      ],
      "relatedCards": [
        "ai-ml-runtimes-architecture-10",
        "ai-ml-runtimes-supported-model-types-11",
        "ai-ml-runtimes-architecture-16",
        "ai-ml-runtimes-overview-20",
        "ai-model-debugging-framework-for-debugging-2"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#neural-machine-translation-by-jointly-learning-to-align-and-translate",
      "scrapedAt": "2025-12-28T11:56:36.591Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-identity-mappings-in-deep-residual-networks-15",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Identity Mappings in Deep Residual Networks",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li><strong>Authors</strong>: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</li>\n  <li>\n    <p><strong>Affiliations</strong>: Microsoft Research</p>\n  </li>\n  <li>\n    <p>The paper “Identity Mappings in Deep Residual Networks” explores the role of identity mappings in the architecture of deep residual networks (ResNets), which are used extensively in computer vision tasks. The authors analyze the propagation of forward and backward signals in ResNets and propose modifications to improve training and generalization.</p>\n  </li>\n  <li>Key Contributions:\n    <ol>\n      <li><strong>Analysis of Identity Mappings</strong>:\n        <ul>\n          <li>The authors focus on the importance of identity mappings in ResNets, which allow the forward and backward signals to propagate directly from one residual block to any other block.</li>\n          <li>They demonstrate that when using identity mappings as skip connections and after-addition activation functions, the training process becomes easier and the network’s generalization improves.</li>\n        </ul>\n      </li>\n      <li><strong>Proposed Residual Unit</strong>:\n        <ul>\n          <li>A new residual unit design is proposed, incorporating identity mappings both as skip connections and after-addition activations.</li>\n          <li>This design ensures that the signal can be directly propagated between blocks, simplifying the training process and improving the network’s ability to generalize.</li>\n        </ul>\n      </li>\n      <li><strong>Empirical Validation</strong>:\n        <ul>\n          <li>The authors conduct a series of ablation experiments to support the importance of identity mappings.</li>\n          <li>Results show that their proposed modifications lead to lower training errors and improved test accuracy on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet.</li>\n        </ul>\n      </li>\n      <li><strong>Deep Residual Networks</strong>:\n        <ul>\n          <li>They train extremely deep networks, including a 1001-layer ResNet on CIFAR-10 and CIFAR-100, and a 200-layer ResNet on ImageNet.</li>\n          <li>These deep networks achieve state-of-the-art performance, demonstrating the effectiveness of the proposed modifications.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>Experimental Results:\n    <ul>\n      <li><strong>CIFAR-10 and CIFAR-100</strong>:\n        <ul>\n          <li>A 1001-layer ResNet achieves 4.62% error on CIFAR-10 and demonstrates superior performance on CIFAR-100 as well.</li>\n          <li>The proposed identity mapping improves training convergence and generalization compared to the original ResNet design.</li>\n        </ul>\n      </li>\n      <li><strong>ImageNet</strong>:\n        <ul>\n          <li>A 200-layer ResNet trained on ImageNet achieves better accuracy than the original 152-layer ResNet, showing the scalability of the proposed identity mapping approach.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>Conclusion:\n    <ul>\n      <li>The study reveals that identity mappings play a crucial role in the efficiency of deep residual networks.</li>\n      <li>By incorporating identity mappings both in skip connections and after-addition activation, the proposed design simplifies training and enhances generalization.</li>\n      <li>The findings suggest significant potential for further exploiting network depth in modern deep learning architectures.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Affiliations</strong>: Microsoft Research</p>\n<p>The paper “Identity Mappings in Deep Residual Networks” explores the role of identity mappings in the architecture of deep residual networks (ResNets), which are used extensively in computer vision tasks. The authors analyze the propagation of forward and backward signals in ResNets and propose modifications to improve training and generalization.</p>\n<ol>\n      <li><strong>Analysis of Identity Mappings</strong>:\n        <ul>\n          <li>The authors focus on the importance of identity mappings in ResNets, which allow the forward and backward signals to propagate directly from one residual block to any other block.</li>\n          <li>They demonstrate that when using identity mappings as skip connections and after-addition activation functions, the training process becomes easier and the network’s generalization improves.</li>\n        </ul>\n      </li>\n      <li><strong>Proposed Residual Unit</strong>:\n        <ul>\n          <li>A new residual unit design is proposed, incorporating identity mappings both as skip connections and after-addition activations.</li>\n          <li>This design ensures that the signal can be directly propagated between blocks, simplifying the training process and improving the network’s ability to generalize.</li>\n        </ul>\n      </li>\n      <li><strong>Empirical Validation</strong>:\n        <ul>\n          <li>The authors conduct a series of ablation experiments to support the importance of identity mappings.</li>\n          <li>Results show that their proposed modifications lead to lower training errors and improved test accuracy on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet.</li>\n        </ul>\n      </li>\n      <li><strong>Deep Residual Networks</strong>:\n        <ul>\n          <li>They train extremely deep networks, including a 1001-layer ResNet on CIFAR-10 and CIFAR-100, and a 200-layer ResNet on ImageNet.</li>\n          <li>These deep networks achieve state-of-the-art performance, demonstrating the effectiveness of the proposed modifications.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>The authors focus on the importance of identity mappings in ResNets, which allow the forward and backward signals to propagate directly from one residual block to any other block.</li>\n          <li>They demonstrate that when using identity mappings as skip connections and after-addition activation functions, the training process becomes easier and the network’s generalization improves.</li>\n        </ul>\n<ul>\n          <li>A new residual unit design is proposed, incorporating identity mappings both as skip connections and after-addition activations.</li>\n          <li>This design ensures that the signal can be directly propagated between blocks, simplifying the training process and improving the network’s ability to generalize.</li>\n        </ul>\n<ul>\n          <li>The authors conduct a series of ablation experiments to support the importance of identity mappings.</li>\n          <li>Results show that their proposed modifications lead to lower training errors and improved test accuracy on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet.</li>\n        </ul>\n<ul>\n          <li>They train extremely deep networks, including a 1001-layer ResNet on CIFAR-10 and CIFAR-100, and a 200-layer ResNet on ImageNet.</li>\n          <li>These deep networks achieve state-of-the-art performance, demonstrating the effectiveness of the proposed modifications.</li>\n        </ul>\n<ul>\n      <li><strong>CIFAR-10 and CIFAR-100</strong>:\n        <ul>\n          <li>A 1001-layer ResNet achieves 4.62% error on CIFAR-10 and demonstrates superior performance on CIFAR-100 as well.</li>\n          <li>The proposed identity mapping improves training convergence and generalization compared to the original ResNet design.</li>\n        </ul>\n      </li>\n      <li><strong>ImageNet</strong>:\n        <ul>\n          <li>A 200-layer ResNet trained on ImageNet achieves better accuracy than the original 152-layer ResNet, showing the scalability of the proposed identity mapping approach.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>A 1001-layer ResNet achieves 4.62% error on CIFAR-10 and demonstrates superior performance on CIFAR-100 as well.</li>\n          <li>The proposed identity mapping improves training convergence and generalization compared to the original ResNet design.</li>\n        </ul>\n<ul>\n          <li>A 200-layer ResNet trained on ImageNet achieves better accuracy than the original 152-layer ResNet, showing the scalability of the proposed identity mapping approach.</li>\n        </ul>\n<ul>\n      <li>The study reveals that identity mappings play a crucial role in the efficiency of deep residual networks.</li>\n      <li>By incorporating identity mappings both in skip connections and after-addition activation, the proposed design simplifies training and enhances generalization.</li>\n      <li>The findings suggest significant potential for further exploiting network depth in modern deep learning architectures.</li>\n    </ul>",
      "contentMarkdown": "*   **Authors**: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n*   **Affiliations**: Microsoft Research\n    \n*   The paper “Identity Mappings in Deep Residual Networks” explores the role of identity mappings in the architecture of deep residual networks (ResNets), which are used extensively in computer vision tasks. The authors analyze the propagation of forward and backward signals in ResNets and propose modifications to improve training and generalization.\n    \n*   Key Contributions:\n    1.  **Analysis of Identity Mappings**:\n        *   The authors focus on the importance of identity mappings in ResNets, which allow the forward and backward signals to propagate directly from one residual block to any other block.\n        *   They demonstrate that when using identity mappings as skip connections and after-addition activation functions, the training process becomes easier and the network’s generalization improves.\n    2.  **Proposed Residual Unit**:\n        *   A new residual unit design is proposed, incorporating identity mappings both as skip connections and after-addition activations.\n        *   This design ensures that the signal can be directly propagated between blocks, simplifying the training process and improving the network’s ability to generalize.\n    3.  **Empirical Validation**:\n        *   The authors conduct a series of ablation experiments to support the importance of identity mappings.\n        *   Results show that their proposed modifications lead to lower training errors and improved test accuracy on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet.\n    4.  **Deep Residual Networks**:\n        *   They train extremely deep networks, including a 1001-layer ResNet on CIFAR-10 and CIFAR-100, and a 200-layer ResNet on ImageNet.\n        *   These deep networks achieve state-of-the-art performance, demonstrating the effectiveness of the proposed modifications.\n*   Experimental Results:\n    *   **CIFAR-10 and CIFAR-100**:\n        *   A 1001-layer ResNet achieves 4.62% error on CIFAR-10 and demonstrates superior performance on CIFAR-100 as well.\n        *   The proposed identity mapping improves training convergence and generalization compared to the original ResNet design.\n    *   **ImageNet**:\n        *   A 200-layer ResNet trained on ImageNet achieves better accuracy than the original 152-layer ResNet, showing the scalability of the proposed identity mapping approach.\n*   Conclusion:\n    *   The study reveals that identity mappings play a crucial role in the efficiency of deep residual networks.\n    *   By incorporating identity mappings both in skip connections and after-addition activation, the proposed design simplifies training and enhances generalization.\n    *   The findings suggest significant potential for further exploiting network depth in modern deep learning architectures.\n\n**Affiliations**: Microsoft Research\n\nThe paper “Identity Mappings in Deep Residual Networks” explores the role of identity mappings in the architecture of deep residual networks (ResNets), which are used extensively in computer vision tasks. The authors analyze the propagation of forward and backward signals in ResNets and propose modifications to improve training and generalization.\n\n1.  **Analysis of Identity Mappings**:\n    *   The authors focus on the importance of identity mappings in ResNets, which allow the forward and backward signals to propagate directly from one residual block to any other block.\n    *   They demonstrate that when using identity mappings as skip connections and after-addition activation functions, the training process becomes easier and the network’s generalization improves.\n2.  **Proposed Residual Unit**:\n    *   A new residual unit design is proposed, incorporating identity mappings both as skip connections and after-addition activations.\n    *   This design ensures that the signal can be directly propagated between blocks, simplifying the training process and improving the network’s ability to generalize.\n3.  **Empirical Validation**:\n    *   The authors conduct a series of ablation experiments to support the importance of identity mappings.\n    *   Results show that their proposed modifications lead to lower training errors and improved test accuracy on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet.\n4.  **Deep Residual Networks**:\n    *   They train extremely deep networks, including a 1001-layer ResNet on CIFAR-10 and CIFAR-100, and a 200-layer ResNet on ImageNet.\n    *   These deep networks achieve state-of-the-art performance, demonstrating the effectiveness of the proposed modifications.\n\n*   The authors focus on the importance of identity mappings in ResNets, which allow the forward and backward signals to propagate directly from one residual block to any other block.\n*   They demonstrate that when using identity mappings as skip connections and after-addition activation functions, the training process becomes easier and the network’s generalization improves.\n\n*   A new residual unit design is proposed, incorporating identity mappings both as skip connections and after-addition activations.\n*   This design ensures that the signal can be directly propagated between blocks, simplifying the training process and improving the network’s ability to generalize.\n\n*   The authors conduct a series of ablation experiments to support the importance of identity mappings.\n*   Results show that their proposed modifications lead to lower training errors and improved test accuracy on benchmark datasets such as CIFAR-10, CIFAR-100, and ImageNet.\n\n*   They train extremely deep networks, including a 1001-layer ResNet on CIFAR-10 and CIFAR-100, and a 200-layer ResNet on ImageNet.\n*   These deep networks achieve state-of-the-art performance, demonstrating the effectiveness of the proposed modifications.\n\n*   **CIFAR-10 and CIFAR-100**:\n    *   A 1001-layer ResNet achieves 4.62% error on CIFAR-10 and demonstrates superior performance on CIFAR-100 as well.\n    *   The proposed identity mapping improves training convergence and generalization compared to the original ResNet design.\n*   **ImageNet**:\n    *   A 200-layer ResNet trained on ImageNet achieves better accuracy than the original 152-layer ResNet, showing the scalability of the proposed identity mapping approach.\n\n*   A 1001-layer ResNet achieves 4.62% error on CIFAR-10 and demonstrates superior performance on CIFAR-100 as well.\n*   The proposed identity mapping improves training convergence and generalization compared to the original ResNet design.\n\n*   A 200-layer ResNet trained on ImageNet achieves better accuracy than the original 152-layer ResNet, showing the scalability of the proposed identity mapping approach.\n\n*   The study reveals that identity mappings play a crucial role in the efficiency of deep residual networks.\n*   By incorporating identity mappings both in skip connections and after-addition activation, the proposed design simplifies training and enhances generalization.\n*   The findings suggest significant potential for further exploiting network depth in modern deep learning architectures.",
      "order": 15,
      "orderInChapter": 15,
      "difficulty": 2,
      "estimatedMinutes": 5,
      "tags": [
        "miscellaneous",
        "deep learning",
        "computer vision",
        "activation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 965,
        "contentLength": 8569
      },
      "nextCards": [
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16",
        "ai-top-30-papers-variational-lossy-autoencoder-17"
      ],
      "relatedCards": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-operator-dispatch-and-kernel-mapping-14",
        "ai-gpu-architecture-forward-and-backward-passes-training-17",
        "ai-gpu-architecture-overview-of-precision-types-25",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#identity-mappings-in-deep-residual-networks",
      "scrapedAt": "2025-12-28T11:56:36.591Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "A Simple Neural Network Module for Relational Reasoning",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li><strong>Authors</strong>: Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap</li>\n  <li>\n    <p><strong>Affiliations</strong>: DeepMind, London, United Kingdom</p>\n  </li>\n  <li>\n    <p>The paper “A Simple Neural Network Module for Relational Reasoning” introduces the concept of Relation Networks (RNs) as a module for neural networks to solve tasks that require relational reasoning. The paper demonstrates the effectiveness of RNs across multiple domains, including visual question answering, text-based question answering, and reasoning about dynamic physical systems.</p>\n  </li>\n  <li>Key Contributions:\n    <ol>\n      <li><strong>Introduction of Relation Networks (RNs)</strong>:\n        <ul>\n          <li>RNs are designed to explicitly compute relations between pairs of objects, making them suitable for tasks that involve relational reasoning.</li>\n          <li>The RN is a plug-and-play module that can be added to existing neural network architectures, enhancing their ability to reason about relationships.</li>\n        </ul>\n      </li>\n      <li><strong>Application to Visual Question Answering (CLEVR)</strong>:\n        <ul>\n          <li>The authors tested RNs on the CLEVR dataset, which requires complex relational reasoning about visual scenes.</li>\n          <li>The RN-augmented model achieved state-of-the-art performance, surpassing human accuracy on the CLEVR benchmark.</li>\n        </ul>\n      </li>\n      <li><strong>Sort-of-CLEVR Dataset</strong>:\n        <ul>\n          <li>The paper introduces the Sort-of-CLEVR dataset, designed to separate relational and non-relational questions explicitly.</li>\n          <li>Experiments on Sort-of-CLEVR show that RNs significantly outperform standard neural network architectures on relational questions, highlighting the importance of explicit relational reasoning.</li>\n        </ul>\n      </li>\n      <li><strong>Text-Based Question Answering (bAbI)</strong>:\n        <ul>\n          <li>RNs were also applied to the bAbI suite of tasks, which involve various types of reasoning such as deduction and induction.</li>\n          <li>The RN-augmented model successfully solved 18 out of 20 bAbI tasks, demonstrating its versatility and effectiveness in text-based relational reasoning.</li>\n        </ul>\n      </li>\n      <li><strong>Dynamic Physical Systems</strong>:\n        <ul>\n          <li>The paper explores the use of RNs for reasoning about dynamic physical systems, such as inferring connections between moving objects and counting the number of connected systems.</li>\n          <li>RNs achieved high accuracy in these tasks, showcasing their ability to handle complex relational inferences in physical simulations.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>Model Details:</li>\n  <li><strong>Architecture</strong>:\n    <ul>\n      <li>RNs operate on sets of objects, where each object is represented by a feature vector.</li>\n      <li>\n        <p>The RN computes pairwise relations using a function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B8;</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-121\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-122\"><span class=\"msubsup\" id=\"MathJax-Span-123\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-125\"><span class=\"mrow\" id=\"MathJax-Span-126\"><span class=\"mi\" id=\"MathJax-Span-127\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>g</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>θ</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">g_{\\theta}</script> and aggregates these relations using a function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03D5;</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-128\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.78em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-129\"><span class=\"msubsup\" id=\"MathJax-Span-130\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-132\"><span class=\"mrow\" id=\"MathJax-Span-133\"><span class=\"mi\" id=\"MathJax-Span-134\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">ϕ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>ϕ</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">f_{\\phi}</script>, allowing the network to infer and reason about the relationships between objects.</p>\n      </li>\n      <li><strong>Training</strong>:\n        <ul>\n          <li>The models were trained using standard optimization techniques, such as the Adam optimizer, and were evaluated on various benchmarks to validate their performance.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>Results:</li>\n  <li><strong>CLEVR</strong>:\n    <ul>\n      <li>\n        <p>The RN-augmented model achieved 95.5% accuracy on the CLEVR dataset, significantly outperforming previous models that lacked explicit relational reasoning components.</p>\n      </li>\n      <li><strong>Sort-of-CLEVR</strong>:\n        <ul>\n          <li>On the Sort-of-CLEVR dataset, the RN-augmented model achieved over 94% accuracy on both relational and non-relational questions, while standard models struggled with relational questions.</li>\n        </ul>\n      </li>\n      <li><strong>bAbI</strong>:\n        <ul>\n          <li>The RN model passed 18 out of 20 tasks, demonstrating its capability to handle different types of reasoning required by the bAbI tasks.</li>\n        </ul>\n      </li>\n      <li><strong>Dynamic Physical Systems</strong>:\n        <ul>\n          <li>RNs accurately inferred connections and counted connected systems, showing their effectiveness in reasoning about physical interactions.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>Conclusion:\n    <ul>\n      <li>The introduction of Relation Networks provides a powerful tool for enhancing neural networks with relational reasoning capabilities.</li>\n      <li>RNs are versatile and can be applied to a wide range of tasks, including visual and text-based question answering and reasoning about physical systems.</li>\n      <li>The success of RNs across diverse domains highlights their potential as a general solution for tasks requiring relational reasoning.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Affiliations</strong>: DeepMind, London, United Kingdom</p>\n<p>The paper “A Simple Neural Network Module for Relational Reasoning” introduces the concept of Relation Networks (RNs) as a module for neural networks to solve tasks that require relational reasoning. The paper demonstrates the effectiveness of RNs across multiple domains, including visual question answering, text-based question answering, and reasoning about dynamic physical systems.</p>\n<ol>\n      <li><strong>Introduction of Relation Networks (RNs)</strong>:\n        <ul>\n          <li>RNs are designed to explicitly compute relations between pairs of objects, making them suitable for tasks that involve relational reasoning.</li>\n          <li>The RN is a plug-and-play module that can be added to existing neural network architectures, enhancing their ability to reason about relationships.</li>\n        </ul>\n      </li>\n      <li><strong>Application to Visual Question Answering (CLEVR)</strong>:\n        <ul>\n          <li>The authors tested RNs on the CLEVR dataset, which requires complex relational reasoning about visual scenes.</li>\n          <li>The RN-augmented model achieved state-of-the-art performance, surpassing human accuracy on the CLEVR benchmark.</li>\n        </ul>\n      </li>\n      <li><strong>Sort-of-CLEVR Dataset</strong>:\n        <ul>\n          <li>The paper introduces the Sort-of-CLEVR dataset, designed to separate relational and non-relational questions explicitly.</li>\n          <li>Experiments on Sort-of-CLEVR show that RNs significantly outperform standard neural network architectures on relational questions, highlighting the importance of explicit relational reasoning.</li>\n        </ul>\n      </li>\n      <li><strong>Text-Based Question Answering (bAbI)</strong>:\n        <ul>\n          <li>RNs were also applied to the bAbI suite of tasks, which involve various types of reasoning such as deduction and induction.</li>\n          <li>The RN-augmented model successfully solved 18 out of 20 bAbI tasks, demonstrating its versatility and effectiveness in text-based relational reasoning.</li>\n        </ul>\n      </li>\n      <li><strong>Dynamic Physical Systems</strong>:\n        <ul>\n          <li>The paper explores the use of RNs for reasoning about dynamic physical systems, such as inferring connections between moving objects and counting the number of connected systems.</li>\n          <li>RNs achieved high accuracy in these tasks, showcasing their ability to handle complex relational inferences in physical simulations.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>RNs are designed to explicitly compute relations between pairs of objects, making them suitable for tasks that involve relational reasoning.</li>\n          <li>The RN is a plug-and-play module that can be added to existing neural network architectures, enhancing their ability to reason about relationships.</li>\n        </ul>\n<ul>\n          <li>The authors tested RNs on the CLEVR dataset, which requires complex relational reasoning about visual scenes.</li>\n          <li>The RN-augmented model achieved state-of-the-art performance, surpassing human accuracy on the CLEVR benchmark.</li>\n        </ul>\n<ul>\n          <li>The paper introduces the Sort-of-CLEVR dataset, designed to separate relational and non-relational questions explicitly.</li>\n          <li>Experiments on Sort-of-CLEVR show that RNs significantly outperform standard neural network architectures on relational questions, highlighting the importance of explicit relational reasoning.</li>\n        </ul>\n<ul>\n          <li>RNs were also applied to the bAbI suite of tasks, which involve various types of reasoning such as deduction and induction.</li>\n          <li>The RN-augmented model successfully solved 18 out of 20 bAbI tasks, demonstrating its versatility and effectiveness in text-based relational reasoning.</li>\n        </ul>\n<ul>\n          <li>The paper explores the use of RNs for reasoning about dynamic physical systems, such as inferring connections between moving objects and counting the number of connected systems.</li>\n          <li>RNs achieved high accuracy in these tasks, showcasing their ability to handle complex relational inferences in physical simulations.</li>\n        </ul>\n<ul>\n      <li>RNs operate on sets of objects, where each object is represented by a feature vector.</li>\n      <li>\n        <p>The RN computes pairwise relations using a function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B8;</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-121\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-122\"><span class=\"msubsup\" id=\"MathJax-Span-123\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-125\"><span class=\"mrow\" id=\"MathJax-Span-126\"><span class=\"mi\" id=\"MathJax-Span-127\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>g</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>θ</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">g_{\\theta}</script> and aggregates these relations using a function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03D5;</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-128\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.78em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-129\"><span class=\"msubsup\" id=\"MathJax-Span-130\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-132\"><span class=\"mrow\" id=\"MathJax-Span-133\"><span class=\"mi\" id=\"MathJax-Span-134\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">ϕ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>ϕ</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">f_{\\phi}</script>, allowing the network to infer and reason about the relationships between objects.</p>\n      </li>\n      <li><strong>Training</strong>:\n        <ul>\n          <li>The models were trained using standard optimization techniques, such as the Adam optimizer, and were evaluated on various benchmarks to validate their performance.</li>\n        </ul>\n      </li>\n    </ul>\n<p>The RN computes pairwise relations using a function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03B8;</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-121\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-122\"><span class=\"msubsup\" id=\"MathJax-Span-123\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-124\" style=\"font-family: STIXGeneral-Italic;\">g</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-125\"><span class=\"mrow\" id=\"MathJax-Span-126\"><span class=\"mi\" id=\"MathJax-Span-127\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>g</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>θ</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">g_{\\theta}</script> and aggregates these relations using a function<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>&amp;#x03D5;</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-128\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.78em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-129\"><span class=\"msubsup\" id=\"MathJax-Span-130\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-132\"><span class=\"mrow\" id=\"MathJax-Span-133\"><span class=\"mi\" id=\"MathJax-Span-134\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">ϕ</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>ϕ</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">f_{\\phi}</script>, allowing the network to infer and reason about the relationships between objects.</p>\n<ul>\n          <li>The models were trained using standard optimization techniques, such as the Adam optimizer, and were evaluated on various benchmarks to validate their performance.</li>\n        </ul>\n<ul>\n      <li>\n        <p>The RN-augmented model achieved 95.5% accuracy on the CLEVR dataset, significantly outperforming previous models that lacked explicit relational reasoning components.</p>\n      </li>\n      <li><strong>Sort-of-CLEVR</strong>:\n        <ul>\n          <li>On the Sort-of-CLEVR dataset, the RN-augmented model achieved over 94% accuracy on both relational and non-relational questions, while standard models struggled with relational questions.</li>\n        </ul>\n      </li>\n      <li><strong>bAbI</strong>:\n        <ul>\n          <li>The RN model passed 18 out of 20 tasks, demonstrating its capability to handle different types of reasoning required by the bAbI tasks.</li>\n        </ul>\n      </li>\n      <li><strong>Dynamic Physical Systems</strong>:\n        <ul>\n          <li>RNs accurately inferred connections and counted connected systems, showing their effectiveness in reasoning about physical interactions.</li>\n        </ul>\n      </li>\n    </ul>\n<p>The RN-augmented model achieved 95.5% accuracy on the CLEVR dataset, significantly outperforming previous models that lacked explicit relational reasoning components.</p>\n<ul>\n          <li>On the Sort-of-CLEVR dataset, the RN-augmented model achieved over 94% accuracy on both relational and non-relational questions, while standard models struggled with relational questions.</li>\n        </ul>\n<ul>\n          <li>The RN model passed 18 out of 20 tasks, demonstrating its capability to handle different types of reasoning required by the bAbI tasks.</li>\n        </ul>\n<ul>\n          <li>RNs accurately inferred connections and counted connected systems, showing their effectiveness in reasoning about physical interactions.</li>\n        </ul>\n<ul>\n      <li>The introduction of Relation Networks provides a powerful tool for enhancing neural networks with relational reasoning capabilities.</li>\n      <li>RNs are versatile and can be applied to a wide range of tasks, including visual and text-based question answering and reasoning about physical systems.</li>\n      <li>The success of RNs across diverse domains highlights their potential as a general solution for tasks requiring relational reasoning.</li>\n    </ul>",
      "contentMarkdown": "*   **Authors**: Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap\n*   **Affiliations**: DeepMind, London, United Kingdom\n    \n*   The paper “A Simple Neural Network Module for Relational Reasoning” introduces the concept of Relation Networks (RNs) as a module for neural networks to solve tasks that require relational reasoning. The paper demonstrates the effectiveness of RNs across multiple domains, including visual question answering, text-based question answering, and reasoning about dynamic physical systems.\n    \n*   Key Contributions:\n    1.  **Introduction of Relation Networks (RNs)**:\n        *   RNs are designed to explicitly compute relations between pairs of objects, making them suitable for tasks that involve relational reasoning.\n        *   The RN is a plug-and-play module that can be added to existing neural network architectures, enhancing their ability to reason about relationships.\n    2.  **Application to Visual Question Answering (CLEVR)**:\n        *   The authors tested RNs on the CLEVR dataset, which requires complex relational reasoning about visual scenes.\n        *   The RN-augmented model achieved state-of-the-art performance, surpassing human accuracy on the CLEVR benchmark.\n    3.  **Sort-of-CLEVR Dataset**:\n        *   The paper introduces the Sort-of-CLEVR dataset, designed to separate relational and non-relational questions explicitly.\n        *   Experiments on Sort-of-CLEVR show that RNs significantly outperform standard neural network architectures on relational questions, highlighting the importance of explicit relational reasoning.\n    4.  **Text-Based Question Answering (bAbI)**:\n        *   RNs were also applied to the bAbI suite of tasks, which involve various types of reasoning such as deduction and induction.\n        *   The RN-augmented model successfully solved 18 out of 20 bAbI tasks, demonstrating its versatility and effectiveness in text-based relational reasoning.\n    5.  **Dynamic Physical Systems**:\n        *   The paper explores the use of RNs for reasoning about dynamic physical systems, such as inferring connections between moving objects and counting the number of connected systems.\n        *   RNs achieved high accuracy in these tasks, showcasing their ability to handle complex relational inferences in physical simulations.\n*   Model Details:\n*   **Architecture**:\n    *   RNs operate on sets of objects, where each object is represented by a feature vector.\n    *   The RN computes pairwise relations using a functiongθgθg\\_{\\\\theta} and aggregates these relations using a functionfϕfϕf\\_{\\\\phi}, allowing the network to infer and reason about the relationships between objects.\n        \n    *   **Training**:\n        *   The models were trained using standard optimization techniques, such as the Adam optimizer, and were evaluated on various benchmarks to validate their performance.\n*   Results:\n*   **CLEVR**:\n    *   The RN-augmented model achieved 95.5% accuracy on the CLEVR dataset, significantly outperforming previous models that lacked explicit relational reasoning components.\n        \n    *   **Sort-of-CLEVR**:\n        *   On the Sort-of-CLEVR dataset, the RN-augmented model achieved over 94% accuracy on both relational and non-relational questions, while standard models struggled with relational questions.\n    *   **bAbI**:\n        *   The RN model passed 18 out of 20 tasks, demonstrating its capability to handle different types of reasoning required by the bAbI tasks.\n    *   **Dynamic Physical Systems**:\n        *   RNs accurately inferred connections and counted connected systems, showing their effectiveness in reasoning about physical interactions.\n*   Conclusion:\n    *   The introduction of Relation Networks provides a powerful tool for enhancing neural networks with relational reasoning capabilities.\n    *   RNs are versatile and can be applied to a wide range of tasks, including visual and text-based question answering and reasoning about physical systems.\n    *   The success of RNs across diverse domains highlights their potential as a general solution for tasks requiring relational reasoning.\n\n**Affiliations**: DeepMind, London, United Kingdom\n\nThe paper “A Simple Neural Network Module for Relational Reasoning” introduces the concept of Relation Networks (RNs) as a module for neural networks to solve tasks that require relational reasoning. The paper demonstrates the effectiveness of RNs across multiple domains, including visual question answering, text-based question answering, and reasoning about dynamic physical systems.\n\n1.  **Introduction of Relation Networks (RNs)**:\n    *   RNs are designed to explicitly compute relations between pairs of objects, making them suitable for tasks that involve relational reasoning.\n    *   The RN is a plug-and-play module that can be added to existing neural network architectures, enhancing their ability to reason about relationships.\n2.  **Application to Visual Question Answering (CLEVR)**:\n    *   The authors tested RNs on the CLEVR dataset, which requires complex relational reasoning about visual scenes.\n    *   The RN-augmented model achieved state-of-the-art performance, surpassing human accuracy on the CLEVR benchmark.\n3.  **Sort-of-CLEVR Dataset**:\n    *   The paper introduces the Sort-of-CLEVR dataset, designed to separate relational and non-relational questions explicitly.\n    *   Experiments on Sort-of-CLEVR show that RNs significantly outperform standard neural network architectures on relational questions, highlighting the importance of explicit relational reasoning.\n4.  **Text-Based Question Answering (bAbI)**:\n    *   RNs were also applied to the bAbI suite of tasks, which involve various types of reasoning such as deduction and induction.\n    *   The RN-augmented model successfully solved 18 out of 20 bAbI tasks, demonstrating its versatility and effectiveness in text-based relational reasoning.\n5.  **Dynamic Physical Systems**:\n    *   The paper explores the use of RNs for reasoning about dynamic physical systems, such as inferring connections between moving objects and counting the number of connected systems.\n    *   RNs achieved high accuracy in these tasks, showcasing their ability to handle complex relational inferences in physical simulations.\n\n*   RNs are designed to explicitly compute relations between pairs of objects, making them suitable for tasks that involve relational reasoning.\n*   The RN is a plug-and-play module that can be added to existing neural network architectures, enhancing their ability to reason about relationships.\n\n*   The authors tested RNs on the CLEVR dataset, which requires complex relational reasoning about visual scenes.\n*   The RN-augmented model achieved state-of-the-art performance, surpassing human accuracy on the CLEVR benchmark.\n\n*   The paper introduces the Sort-of-CLEVR dataset, designed to separate relational and non-relational questions explicitly.\n*   Experiments on Sort-of-CLEVR show that RNs significantly outperform standard neural network architectures on relational questions, highlighting the importance of explicit relational reasoning.\n\n*   RNs were also applied to the bAbI suite of tasks, which involve various types of reasoning such as deduction and induction.\n*   The RN-augmented model successfully solved 18 out of 20 bAbI tasks, demonstrating its versatility and effectiveness in text-based relational reasoning.\n\n*   The paper explores the use of RNs for reasoning about dynamic physical systems, such as inferring connections between moving objects and counting the number of connected systems.\n*   RNs achieved high accuracy in these tasks, showcasing their ability to handle complex relational inferences in physical simulations.\n\n*   RNs operate on sets of objects, where each object is represented by a feature vector.\n*   The RN computes pairwise relations using a functiongθgθg\\_{\\\\theta} and aggregates these relations using a functionfϕfϕf\\_{\\\\phi}, allowing the network to infer and reason about the relationships between objects.\n    \n*   **Training**:\n    *   The models were trained using standard optimization techniques, such as the Adam optimizer, and were evaluated on various benchmarks to validate their performance.\n\nThe RN computes pairwise relations using a functiongθgθg\\_{\\\\theta} and aggregates these relations using a functionfϕfϕf\\_{\\\\phi}, allowing the network to infer and reason about the relationships between objects.\n\n*   The models were trained using standard optimization techniques, such as the Adam optimizer, and were evaluated on various benchmarks to validate their performance.\n\n*   The RN-augmented model achieved 95.5% accuracy on the CLEVR dataset, significantly outperforming previous models that lacked explicit relational reasoning components.\n    \n*   **Sort-of-CLEVR**:\n    *   On the Sort-of-CLEVR dataset, the RN-augmented model achieved over 94% accuracy on both relational and non-relational questions, while standard models struggled with relational questions.\n*   **bAbI**:\n    *   The RN model passed 18 out of 20 tasks, demonstrating its capability to handle different types of reasoning required by the bAbI tasks.\n*   **Dynamic Physical Systems**:\n    *   RNs accurately inferred connections and counted connected systems, showing their effectiveness in reasoning about physical interactions.\n\nThe RN-augmented model achieved 95.5% accuracy on the CLEVR dataset, significantly outperforming previous models that lacked explicit relational reasoning components.\n\n*   On the Sort-of-CLEVR dataset, the RN-augmented model achieved over 94% accuracy on both relational and non-relational questions, while standard models struggled with relational questions.\n\n*   The RN model passed 18 out of 20 tasks, demonstrating its capability to handle different types of reasoning required by the bAbI tasks.\n\n*   RNs accurately inferred connections and counted connected systems, showing their effectiveness in reasoning about physical interactions.\n\n*   The introduction of Relation Networks provides a powerful tool for enhancing neural networks with relational reasoning capabilities.\n*   RNs are versatile and can be applied to a wide range of tasks, including visual and text-based question answering and reasoning about physical systems.\n*   The success of RNs across diverse domains highlights their potential as a general solution for tasks requiring relational reasoning.",
      "order": 16,
      "orderInChapter": 16,
      "difficulty": 4,
      "estimatedMinutes": 8,
      "tags": [
        "miscellaneous",
        "neural network",
        "optimization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 1401,
        "contentLength": 25164
      },
      "nextCards": [
        "ai-top-30-papers-variational-lossy-autoencoder-17",
        "ai-top-30-papers-relational-recurrent-neural-networks-18"
      ],
      "relatedCards": [
        "ai-model-debugging-weight-initialization-11",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-pros-and-cons-6",
        "ai-ml-runtimes-architecture-10"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#a-simple-neural-network-module-for-relational-reasoning",
      "scrapedAt": "2025-12-28T11:56:36.591Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-variational-lossy-autoencoder-17",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Variational Lossy Autoencoder",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>Authors: Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, Pieter Abbeel</li>\n  <li>Published: ICLR 2017</li>\n  <li>\n    <p>Institutions: UC Berkeley, OpenAI</p>\n  </li>\n  <li>\n    <table>\n      <tbody>\n        <tr>\n          <td>The paper introduces a method to learn global representations by combining Variational Autoencoders (VAE) with neural autoregressive models (e.g., RNN, MADE, PixelRNN/CNN). This model, the Variational Lossy Autoencoder (VLAE), can control the learned global latent code to discard irrelevant information such as textures in 2D images, hence “autoencoding” data in a lossy manner. Using autoregressive models as both the prior distribution<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-135\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.57em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-136\"><span class=\"mi\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-140\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">p(z)</script> and the decoding distribution$$ p(x</td>\n          <td>z)$$ enhances generative modeling performance, achieving state-of-the-art results on several datasets.</td>\n        </tr>\n      </tbody>\n    </table>\n  </li>\n  <li><strong>Key Concepts:</strong>\n    <ol>\n      <li><strong>Representation Learning:</strong> Aims to expose certain aspects of observed data to make it suitable for downstream tasks like classification. VLAE focuses on capturing global structures and discarding detailed textures.</li>\n      <li><strong>Variational Autoencoder (VAE):</strong> VAEs typically combine a probabilistic generative model with an inference model to optimize a lower bound on the data’s log-likelihood.</li>\n      <li><strong>Autoregressive Models:</strong> These models, like RNNs, MADE, and PixelCNN, handle data dependencies in sequences, allowing for robust density estimation.</li>\n    </ol>\n  </li>\n  <li><strong>Technical Highlights:</strong>\n    <ol>\n      <li><strong>Combination of VAE and Autoregressive Models:</strong>\n        <ul>\n          <li>Traditional VAEs may not use the latent code effectively when powerful decoders like RNNs are employed.</li>\n          <li>The authors propose using a local receptive field in the decoder to ensure the latent code captures global structures.</li>\n        </ul>\n      </li>\n      <li><strong>Bits-Back Coding and Information Preference:</strong>\n        <ul>\n          <li>Bits-Back Coding is an information-theoretic view of Variational Inference.</li>\n          <li>The model minimizes the expected code length by subtracting the extra information transmitted through the approximate posterior.</li>\n        </ul>\n      </li>\n      <li><strong>Lossy Code via Explicit Information Placement:</strong>\n        <ul>\n          <li>By designing the decoder to model only local dependencies, the VLAE forces the latent code to capture global information.</li>\n          <li>This results in a lossy compression that retains essential global structures while discarding local details.</li>\n        </ul>\n      </li>\n      <li><strong>Learned Prior with Autoregressive Flow:</strong>\n        <ul>\n          <li>The prior distribution<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo>;</mo><mi>&amp;#x03B8;</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-141\" style=\"width: 3.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.61em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-142\"><span class=\"mi\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"mi\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo>;</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">p(z; \\theta)</script> is parameterized with an autoregressive model, improving the efficiency of Bits-Back Coding.</li>\n          <li>Autoregressive flow (AF) transforms a simple noise source into a complex latent code, enhancing the model’s expressive power.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li><strong>Experiments and Results:</strong>\n    <ol>\n      <li><strong>Datasets:</strong>\n        <ul>\n          <li>The model is evaluated on binary image datasets (MNIST, OMNIGLOT, Caltech-101 Silhouettes) and CIFAR10.</li>\n        </ul>\n      </li>\n      <li><strong>Performance:</strong>\n        <ul>\n          <li><strong>MNIST:</strong> The VLAE achieves new state-of-the-art results, outperforming models like PixelRNN and IAF VAE.</li>\n          <li><strong>OMNIGLOT and Caltech-101:</strong> Significant improvements in log-likelihood compared to previous models.</li>\n          <li><strong>CIFAR10:</strong> VLAE demonstrates competitive performance, achieving state-of-the-art results among variational latent-variable models.</li>\n        </ul>\n      </li>\n      <li><strong>Visualization:</strong>\n        <ul>\n          <li>The authors provide visualizations of original and decompressed images from VLAE, showing that the model captures global structures while regenerating plausible local details.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li><strong>Conclusion:</strong></li>\n  <li>The Variational Lossy Autoencoder (VLAE) effectively combines the strengths of VAEs and autoregressive models, enabling controllable representation learning and improved density estimation. The model’s design ensures that the latent code captures essential global information, making it suitable for various generative tasks. Future work includes extending VLAE to other data types, such as audio and video, and designing task-specific representations to enhance semi-supervised learning.</li>\n</ul>\n<p>Institutions: UC Berkeley, OpenAI</p>\n<table>\n      <tbody>\n        <tr>\n          <td>The paper introduces a method to learn global representations by combining Variational Autoencoders (VAE) with neural autoregressive models (e.g., RNN, MADE, PixelRNN/CNN). This model, the Variational Lossy Autoencoder (VLAE), can control the learned global latent code to discard irrelevant information such as textures in 2D images, hence “autoencoding” data in a lossy manner. Using autoregressive models as both the prior distribution<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-135\" style=\"width: 1.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.617em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1001.57em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-136\"><span class=\"mi\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-140\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">p(z)</script> and the decoding distribution$$ p(x</td>\n          <td>z)$$ enhances generative modeling performance, achieving state-of-the-art results on several datasets.</td>\n        </tr>\n      </tbody>\n    </table>\n<ol>\n      <li><strong>Representation Learning:</strong> Aims to expose certain aspects of observed data to make it suitable for downstream tasks like classification. VLAE focuses on capturing global structures and discarding detailed textures.</li>\n      <li><strong>Variational Autoencoder (VAE):</strong> VAEs typically combine a probabilistic generative model with an inference model to optimize a lower bound on the data’s log-likelihood.</li>\n      <li><strong>Autoregressive Models:</strong> These models, like RNNs, MADE, and PixelCNN, handle data dependencies in sequences, allowing for robust density estimation.</li>\n    </ol>\n<ol>\n      <li><strong>Combination of VAE and Autoregressive Models:</strong>\n        <ul>\n          <li>Traditional VAEs may not use the latent code effectively when powerful decoders like RNNs are employed.</li>\n          <li>The authors propose using a local receptive field in the decoder to ensure the latent code captures global structures.</li>\n        </ul>\n      </li>\n      <li><strong>Bits-Back Coding and Information Preference:</strong>\n        <ul>\n          <li>Bits-Back Coding is an information-theoretic view of Variational Inference.</li>\n          <li>The model minimizes the expected code length by subtracting the extra information transmitted through the approximate posterior.</li>\n        </ul>\n      </li>\n      <li><strong>Lossy Code via Explicit Information Placement:</strong>\n        <ul>\n          <li>By designing the decoder to model only local dependencies, the VLAE forces the latent code to capture global information.</li>\n          <li>This results in a lossy compression that retains essential global structures while discarding local details.</li>\n        </ul>\n      </li>\n      <li><strong>Learned Prior with Autoregressive Flow:</strong>\n        <ul>\n          <li>The prior distribution<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo>;</mo><mi>&amp;#x03B8;</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-141\" style=\"width: 3.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.61em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-142\"><span class=\"mi\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"mi\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo>;</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">p(z; \\theta)</script> is parameterized with an autoregressive model, improving the efficiency of Bits-Back Coding.</li>\n          <li>Autoregressive flow (AF) transforms a simple noise source into a complex latent code, enhancing the model’s expressive power.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>Traditional VAEs may not use the latent code effectively when powerful decoders like RNNs are employed.</li>\n          <li>The authors propose using a local receptive field in the decoder to ensure the latent code captures global structures.</li>\n        </ul>\n<ul>\n          <li>Bits-Back Coding is an information-theoretic view of Variational Inference.</li>\n          <li>The model minimizes the expected code length by subtracting the extra information transmitted through the approximate posterior.</li>\n        </ul>\n<ul>\n          <li>By designing the decoder to model only local dependencies, the VLAE forces the latent code to capture global information.</li>\n          <li>This results in a lossy compression that retains essential global structures while discarding local details.</li>\n        </ul>\n<ul>\n          <li>The prior distribution<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>z</mi><mo>;</mo><mi>&amp;#x03B8;</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-141\" style=\"width: 3.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.61em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-142\"><span class=\"mi\" id=\"MathJax-Span-143\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Italic;\">z</span><span class=\"mo\" id=\"MathJax-Span-146\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"mi\" id=\"MathJax-Span-147\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo>;</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">p(z; \\theta)</script> is parameterized with an autoregressive model, improving the efficiency of Bits-Back Coding.</li>\n          <li>Autoregressive flow (AF) transforms a simple noise source into a complex latent code, enhancing the model’s expressive power.</li>\n        </ul>\n<ol>\n      <li><strong>Datasets:</strong>\n        <ul>\n          <li>The model is evaluated on binary image datasets (MNIST, OMNIGLOT, Caltech-101 Silhouettes) and CIFAR10.</li>\n        </ul>\n      </li>\n      <li><strong>Performance:</strong>\n        <ul>\n          <li><strong>MNIST:</strong> The VLAE achieves new state-of-the-art results, outperforming models like PixelRNN and IAF VAE.</li>\n          <li><strong>OMNIGLOT and Caltech-101:</strong> Significant improvements in log-likelihood compared to previous models.</li>\n          <li><strong>CIFAR10:</strong> VLAE demonstrates competitive performance, achieving state-of-the-art results among variational latent-variable models.</li>\n        </ul>\n      </li>\n      <li><strong>Visualization:</strong>\n        <ul>\n          <li>The authors provide visualizations of original and decompressed images from VLAE, showing that the model captures global structures while regenerating plausible local details.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>The model is evaluated on binary image datasets (MNIST, OMNIGLOT, Caltech-101 Silhouettes) and CIFAR10.</li>\n        </ul>\n<ul>\n          <li><strong>MNIST:</strong> The VLAE achieves new state-of-the-art results, outperforming models like PixelRNN and IAF VAE.</li>\n          <li><strong>OMNIGLOT and Caltech-101:</strong> Significant improvements in log-likelihood compared to previous models.</li>\n          <li><strong>CIFAR10:</strong> VLAE demonstrates competitive performance, achieving state-of-the-art results among variational latent-variable models.</li>\n        </ul>\n<ul>\n          <li>The authors provide visualizations of original and decompressed images from VLAE, showing that the model captures global structures while regenerating plausible local details.</li>\n        </ul>",
      "contentMarkdown": "*   Authors: Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, Pieter Abbeel\n*   Published: ICLR 2017\n*   Institutions: UC Berkeley, OpenAI\n    \n*   The paper introduces a method to learn global representations by combining Variational Autoencoders (VAE) with neural autoregressive models (e.g., RNN, MADE, PixelRNN/CNN). This model, the Variational Lossy Autoencoder (VLAE), can control the learned global latent code to discard irrelevant information such as textures in 2D images, hence “autoencoding” data in a lossy manner. Using autoregressive models as both the prior distributionp(z)p(z)p(z) and the decoding distribution$$ p(x\n    \n    z)$$ enhances generative modeling performance, achieving state-of-the-art results on several datasets.\n    \n*   **Key Concepts:**\n    1.  **Representation Learning:** Aims to expose certain aspects of observed data to make it suitable for downstream tasks like classification. VLAE focuses on capturing global structures and discarding detailed textures.\n    2.  **Variational Autoencoder (VAE):** VAEs typically combine a probabilistic generative model with an inference model to optimize a lower bound on the data’s log-likelihood.\n    3.  **Autoregressive Models:** These models, like RNNs, MADE, and PixelCNN, handle data dependencies in sequences, allowing for robust density estimation.\n*   **Technical Highlights:**\n    1.  **Combination of VAE and Autoregressive Models:**\n        *   Traditional VAEs may not use the latent code effectively when powerful decoders like RNNs are employed.\n        *   The authors propose using a local receptive field in the decoder to ensure the latent code captures global structures.\n    2.  **Bits-Back Coding and Information Preference:**\n        *   Bits-Back Coding is an information-theoretic view of Variational Inference.\n        *   The model minimizes the expected code length by subtracting the extra information transmitted through the approximate posterior.\n    3.  **Lossy Code via Explicit Information Placement:**\n        *   By designing the decoder to model only local dependencies, the VLAE forces the latent code to capture global information.\n        *   This results in a lossy compression that retains essential global structures while discarding local details.\n    4.  **Learned Prior with Autoregressive Flow:**\n        *   The prior distributionp(z;θ)p(z;θ)p(z; \\\\theta) is parameterized with an autoregressive model, improving the efficiency of Bits-Back Coding.\n        *   Autoregressive flow (AF) transforms a simple noise source into a complex latent code, enhancing the model’s expressive power.\n*   **Experiments and Results:**\n    1.  **Datasets:**\n        *   The model is evaluated on binary image datasets (MNIST, OMNIGLOT, Caltech-101 Silhouettes) and CIFAR10.\n    2.  **Performance:**\n        *   **MNIST:** The VLAE achieves new state-of-the-art results, outperforming models like PixelRNN and IAF VAE.\n        *   **OMNIGLOT and Caltech-101:** Significant improvements in log-likelihood compared to previous models.\n        *   **CIFAR10:** VLAE demonstrates competitive performance, achieving state-of-the-art results among variational latent-variable models.\n    3.  **Visualization:**\n        *   The authors provide visualizations of original and decompressed images from VLAE, showing that the model captures global structures while regenerating plausible local details.\n*   **Conclusion:**\n*   The Variational Lossy Autoencoder (VLAE) effectively combines the strengths of VAEs and autoregressive models, enabling controllable representation learning and improved density estimation. The model’s design ensures that the latent code captures essential global information, making it suitable for various generative tasks. Future work includes extending VLAE to other data types, such as audio and video, and designing task-specific representations to enhance semi-supervised learning.\n\nInstitutions: UC Berkeley, OpenAI\n\nThe paper introduces a method to learn global representations by combining Variational Autoencoders (VAE) with neural autoregressive models (e.g., RNN, MADE, PixelRNN/CNN). This model, the Variational Lossy Autoencoder (VLAE), can control the learned global latent code to discard irrelevant information such as textures in 2D images, hence “autoencoding” data in a lossy manner. Using autoregressive models as both the prior distributionp(z)p(z)p(z) and the decoding distribution$$ p(x\n\nz)$$ enhances generative modeling performance, achieving state-of-the-art results on several datasets.\n\n1.  **Representation Learning:** Aims to expose certain aspects of observed data to make it suitable for downstream tasks like classification. VLAE focuses on capturing global structures and discarding detailed textures.\n2.  **Variational Autoencoder (VAE):** VAEs typically combine a probabilistic generative model with an inference model to optimize a lower bound on the data’s log-likelihood.\n3.  **Autoregressive Models:** These models, like RNNs, MADE, and PixelCNN, handle data dependencies in sequences, allowing for robust density estimation.\n\n1.  **Combination of VAE and Autoregressive Models:**\n    *   Traditional VAEs may not use the latent code effectively when powerful decoders like RNNs are employed.\n    *   The authors propose using a local receptive field in the decoder to ensure the latent code captures global structures.\n2.  **Bits-Back Coding and Information Preference:**\n    *   Bits-Back Coding is an information-theoretic view of Variational Inference.\n    *   The model minimizes the expected code length by subtracting the extra information transmitted through the approximate posterior.\n3.  **Lossy Code via Explicit Information Placement:**\n    *   By designing the decoder to model only local dependencies, the VLAE forces the latent code to capture global information.\n    *   This results in a lossy compression that retains essential global structures while discarding local details.\n4.  **Learned Prior with Autoregressive Flow:**\n    *   The prior distributionp(z;θ)p(z;θ)p(z; \\\\theta) is parameterized with an autoregressive model, improving the efficiency of Bits-Back Coding.\n    *   Autoregressive flow (AF) transforms a simple noise source into a complex latent code, enhancing the model’s expressive power.\n\n*   Traditional VAEs may not use the latent code effectively when powerful decoders like RNNs are employed.\n*   The authors propose using a local receptive field in the decoder to ensure the latent code captures global structures.\n\n*   Bits-Back Coding is an information-theoretic view of Variational Inference.\n*   The model minimizes the expected code length by subtracting the extra information transmitted through the approximate posterior.\n\n*   By designing the decoder to model only local dependencies, the VLAE forces the latent code to capture global information.\n*   This results in a lossy compression that retains essential global structures while discarding local details.\n\n*   The prior distributionp(z;θ)p(z;θ)p(z; \\\\theta) is parameterized with an autoregressive model, improving the efficiency of Bits-Back Coding.\n*   Autoregressive flow (AF) transforms a simple noise source into a complex latent code, enhancing the model’s expressive power.\n\n1.  **Datasets:**\n    *   The model is evaluated on binary image datasets (MNIST, OMNIGLOT, Caltech-101 Silhouettes) and CIFAR10.\n2.  **Performance:**\n    *   **MNIST:** The VLAE achieves new state-of-the-art results, outperforming models like PixelRNN and IAF VAE.\n    *   **OMNIGLOT and Caltech-101:** Significant improvements in log-likelihood compared to previous models.\n    *   **CIFAR10:** VLAE demonstrates competitive performance, achieving state-of-the-art results among variational latent-variable models.\n3.  **Visualization:**\n    *   The authors provide visualizations of original and decompressed images from VLAE, showing that the model captures global structures while regenerating plausible local details.\n\n*   The model is evaluated on binary image datasets (MNIST, OMNIGLOT, Caltech-101 Silhouettes) and CIFAR10.\n\n*   **MNIST:** The VLAE achieves new state-of-the-art results, outperforming models like PixelRNN and IAF VAE.\n*   **OMNIGLOT and Caltech-101:** Significant improvements in log-likelihood compared to previous models.\n*   **CIFAR10:** VLAE demonstrates competitive performance, achieving state-of-the-art results among variational latent-variable models.\n\n*   The authors provide visualizations of original and decompressed images from VLAE, showing that the model captures global structures while regenerating plausible local details.",
      "order": 17,
      "orderInChapter": 17,
      "difficulty": 4,
      "estimatedMinutes": 6,
      "tags": [
        "miscellaneous",
        "cnn",
        "rnn",
        "supervised learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 1123,
        "contentLength": 19673
      },
      "nextCards": [
        "ai-top-30-papers-relational-recurrent-neural-networks-18",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19"
      ],
      "relatedCards": [
        "ai-ml-runtimes-example-pseudocode-flow-36",
        "ai-ml-runtimes-final-thoughts-55",
        "ai-ml-runtimes-architecture-33",
        "ai-ml-runtimes-runtime-specific-execution-lifecycles-62",
        "ai-ann-similarity-search-clustering-based-methods-6"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#variational-lossy-autoencoder",
      "scrapedAt": "2025-12-28T11:56:36.591Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-relational-recurrent-neural-networks-18",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Relational Recurrent Neural Networks",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li><strong>Authors:</strong> Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Théophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap</li>\n  <li><strong>Institution:</strong> DeepMind, University College London</li>\n  <li>\n    <p><strong>Abstract:</strong> The paper “Relational Recurrent Neural Networks” investigates the limitations of standard memory-based neural network architectures, such as LSTMs, in handling tasks that require complex relational reasoning. The authors introduce a new memory module, the Relational Memory Core (RMC), which employs multi-head dot product attention to allow memories to interact. The RMC shows improved performance on tasks requiring relational reasoning across sequential information, including reinforcement learning, program evaluation, and language modeling.</p>\n  </li>\n  <li><strong>Key Points:</strong>\n    <ul>\n      <li><strong>Relational Reasoning Deficits in Standard Architectures:</strong> Standard memory architectures like LSTMs often struggle with tasks that involve understanding complex relational reasoning between entities.</li>\n      <li><strong>Introduction of Relational Memory Core (RMC):</strong> The RMC employs multi-head dot product attention, allowing for interactions between memories, thus improving the model’s ability to perform relational reasoning.</li>\n      <li><strong>Application and Results:</strong>\n        <ul>\n          <li><strong>Toy Task for Relational Reasoning:</strong> A toy task was developed to stress test relational reasoning of sequential information, demonstrating the superior performance of RMC over standard architectures.</li>\n          <li><strong>Reinforcement Learning:</strong> In the Mini PacMan task, the RMC significantly outperformed LSTM, particularly when trained with full observation, nearly doubling the performance.</li>\n          <li><strong>Language Modeling:</strong> The RMC achieved lower perplexity scores across language modeling tasks, demonstrating improved data efficiency and better modeling of frequent words.</li>\n        </ul>\n      </li>\n      <li><strong>Model Design and Functionality:</strong>\n        <ul>\n          <li><strong>Memory Interactions:</strong> The RMC allows for interactions between memory slots using multi-head dot product attention, which improves the model’s capacity for relational reasoning over time.</li>\n          <li><strong>Task Performance:</strong> The RMC outperformed standard architectures in tasks such as partially observed reinforcement learning, program evaluation, and language modeling.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Conclusion:</strong> The introduction of the RMC shows that explicit modeling of memory interactions can enhance the performance of neural networks on tasks that require complex relational reasoning across sequential information. The study emphasizes the importance of enabling interactions between memory vectors to improve relational reasoning capabilities in recurrent neural networks.</li>\n</ul>\n<p><strong>Abstract:</strong> The paper “Relational Recurrent Neural Networks” investigates the limitations of standard memory-based neural network architectures, such as LSTMs, in handling tasks that require complex relational reasoning. The authors introduce a new memory module, the Relational Memory Core (RMC), which employs multi-head dot product attention to allow memories to interact. The RMC shows improved performance on tasks requiring relational reasoning across sequential information, including reinforcement learning, program evaluation, and language modeling.</p>\n<ul>\n      <li><strong>Relational Reasoning Deficits in Standard Architectures:</strong> Standard memory architectures like LSTMs often struggle with tasks that involve understanding complex relational reasoning between entities.</li>\n      <li><strong>Introduction of Relational Memory Core (RMC):</strong> The RMC employs multi-head dot product attention, allowing for interactions between memories, thus improving the model’s ability to perform relational reasoning.</li>\n      <li><strong>Application and Results:</strong>\n        <ul>\n          <li><strong>Toy Task for Relational Reasoning:</strong> A toy task was developed to stress test relational reasoning of sequential information, demonstrating the superior performance of RMC over standard architectures.</li>\n          <li><strong>Reinforcement Learning:</strong> In the Mini PacMan task, the RMC significantly outperformed LSTM, particularly when trained with full observation, nearly doubling the performance.</li>\n          <li><strong>Language Modeling:</strong> The RMC achieved lower perplexity scores across language modeling tasks, demonstrating improved data efficiency and better modeling of frequent words.</li>\n        </ul>\n      </li>\n      <li><strong>Model Design and Functionality:</strong>\n        <ul>\n          <li><strong>Memory Interactions:</strong> The RMC allows for interactions between memory slots using multi-head dot product attention, which improves the model’s capacity for relational reasoning over time.</li>\n          <li><strong>Task Performance:</strong> The RMC outperformed standard architectures in tasks such as partially observed reinforcement learning, program evaluation, and language modeling.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li><strong>Toy Task for Relational Reasoning:</strong> A toy task was developed to stress test relational reasoning of sequential information, demonstrating the superior performance of RMC over standard architectures.</li>\n          <li><strong>Reinforcement Learning:</strong> In the Mini PacMan task, the RMC significantly outperformed LSTM, particularly when trained with full observation, nearly doubling the performance.</li>\n          <li><strong>Language Modeling:</strong> The RMC achieved lower perplexity scores across language modeling tasks, demonstrating improved data efficiency and better modeling of frequent words.</li>\n        </ul>\n<ul>\n          <li><strong>Memory Interactions:</strong> The RMC allows for interactions between memory slots using multi-head dot product attention, which improves the model’s capacity for relational reasoning over time.</li>\n          <li><strong>Task Performance:</strong> The RMC outperformed standard architectures in tasks such as partially observed reinforcement learning, program evaluation, and language modeling.</li>\n        </ul>",
      "contentMarkdown": "*   **Authors:** Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Théophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap\n*   **Institution:** DeepMind, University College London\n*   **Abstract:** The paper “Relational Recurrent Neural Networks” investigates the limitations of standard memory-based neural network architectures, such as LSTMs, in handling tasks that require complex relational reasoning. The authors introduce a new memory module, the Relational Memory Core (RMC), which employs multi-head dot product attention to allow memories to interact. The RMC shows improved performance on tasks requiring relational reasoning across sequential information, including reinforcement learning, program evaluation, and language modeling.\n    \n*   **Key Points:**\n    *   **Relational Reasoning Deficits in Standard Architectures:** Standard memory architectures like LSTMs often struggle with tasks that involve understanding complex relational reasoning between entities.\n    *   **Introduction of Relational Memory Core (RMC):** The RMC employs multi-head dot product attention, allowing for interactions between memories, thus improving the model’s ability to perform relational reasoning.\n    *   **Application and Results:**\n        *   **Toy Task for Relational Reasoning:** A toy task was developed to stress test relational reasoning of sequential information, demonstrating the superior performance of RMC over standard architectures.\n        *   **Reinforcement Learning:** In the Mini PacMan task, the RMC significantly outperformed LSTM, particularly when trained with full observation, nearly doubling the performance.\n        *   **Language Modeling:** The RMC achieved lower perplexity scores across language modeling tasks, demonstrating improved data efficiency and better modeling of frequent words.\n    *   **Model Design and Functionality:**\n        *   **Memory Interactions:** The RMC allows for interactions between memory slots using multi-head dot product attention, which improves the model’s capacity for relational reasoning over time.\n        *   **Task Performance:** The RMC outperformed standard architectures in tasks such as partially observed reinforcement learning, program evaluation, and language modeling.\n*   **Conclusion:** The introduction of the RMC shows that explicit modeling of memory interactions can enhance the performance of neural networks on tasks that require complex relational reasoning across sequential information. The study emphasizes the importance of enabling interactions between memory vectors to improve relational reasoning capabilities in recurrent neural networks.\n\n**Abstract:** The paper “Relational Recurrent Neural Networks” investigates the limitations of standard memory-based neural network architectures, such as LSTMs, in handling tasks that require complex relational reasoning. The authors introduce a new memory module, the Relational Memory Core (RMC), which employs multi-head dot product attention to allow memories to interact. The RMC shows improved performance on tasks requiring relational reasoning across sequential information, including reinforcement learning, program evaluation, and language modeling.\n\n*   **Relational Reasoning Deficits in Standard Architectures:** Standard memory architectures like LSTMs often struggle with tasks that involve understanding complex relational reasoning between entities.\n*   **Introduction of Relational Memory Core (RMC):** The RMC employs multi-head dot product attention, allowing for interactions between memories, thus improving the model’s ability to perform relational reasoning.\n*   **Application and Results:**\n    *   **Toy Task for Relational Reasoning:** A toy task was developed to stress test relational reasoning of sequential information, demonstrating the superior performance of RMC over standard architectures.\n    *   **Reinforcement Learning:** In the Mini PacMan task, the RMC significantly outperformed LSTM, particularly when trained with full observation, nearly doubling the performance.\n    *   **Language Modeling:** The RMC achieved lower perplexity scores across language modeling tasks, demonstrating improved data efficiency and better modeling of frequent words.\n*   **Model Design and Functionality:**\n    *   **Memory Interactions:** The RMC allows for interactions between memory slots using multi-head dot product attention, which improves the model’s capacity for relational reasoning over time.\n    *   **Task Performance:** The RMC outperformed standard architectures in tasks such as partially observed reinforcement learning, program evaluation, and language modeling.\n\n*   **Toy Task for Relational Reasoning:** A toy task was developed to stress test relational reasoning of sequential information, demonstrating the superior performance of RMC over standard architectures.\n*   **Reinforcement Learning:** In the Mini PacMan task, the RMC significantly outperformed LSTM, particularly when trained with full observation, nearly doubling the performance.\n*   **Language Modeling:** The RMC achieved lower perplexity scores across language modeling tasks, demonstrating improved data efficiency and better modeling of frequent words.\n\n*   **Memory Interactions:** The RMC allows for interactions between memory slots using multi-head dot product attention, which improves the model’s capacity for relational reasoning over time.\n*   **Task Performance:** The RMC outperformed standard architectures in tasks such as partially observed reinforcement learning, program evaluation, and language modeling.",
      "order": 18,
      "orderInChapter": 18,
      "difficulty": 3,
      "estimatedMinutes": 4,
      "tags": [
        "miscellaneous",
        "neural network",
        "attention",
        "lstm",
        "reinforcement learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 710,
        "contentLength": 6511
      },
      "nextCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-neural-turing-machines-20"
      ],
      "relatedCards": [
        "ai-ml-runtimes-architecture-10",
        "ai-ml-runtimes-architecture-16",
        "ai-ml-runtimes-overview-20",
        "ai-ml-runtimes-design-notes-67",
        "ai-model-debugging-framework-for-debugging-2"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#relational-recurrent-neural-networks",
      "scrapedAt": "2025-12-28T11:56:36.591Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Quantifying the Rise and Fall of Complexity in Closed Systems: the Coffee Automaton",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>Authors: Scott Aaronson, Sean M. Carroll, Lauren Ouellette</li>\n  <li>\n    <p>The paper explores the behavior of complexity in closed systems, comparing it to entropy which increases monotonically. The authors use a two-dimensional cellular automaton, simulating the mixing of “coffee” and “cream,” to model and measure complexity, referred to as “apparent complexity,” defined as the Kolmogorov complexity of a coarse-grained state.</p>\n  </li>\n  <li>\n    <p><strong>Introduction</strong>: The paper begins by contrasting entropy with complexity. While entropy increases over time, complexity appears to rise, reach a maximum, and then fall. The authors aim to quantify this pattern using a simple automaton model.</p>\n  </li>\n  <li><strong>Background</strong>: Several concepts of entropy and complexity are discussed:\n    <ul>\n      <li><strong>Entropy</strong>: Boltzmann entropy, Gibbs entropy, Shannon entropy, and Kolmogorov complexity.</li>\n      <li><strong>Complexity</strong>: Different measures of complexity are introduced, including apparent complexity, sophistication, logical depth, and light-cone complexity.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Apparent Complexity</strong>: Defined as the Kolmogorov complexity of a denoised or smoothed version of a state. This measure aims to capture the “interesting” non-random information in a system.</p>\n  </li>\n  <li>\n    <p><strong>Sophistication</strong>: A measure based on Kolmogorov complexity, aiming to capture the amount of non-random information in a system. It involves finding a set S such that a string x is a generic element of S.</p>\n  </li>\n  <li>\n    <p><strong>Logical Depth</strong>: Introduced by Bennett, it measures the time taken by the shortest program to output a string, capturing the “computational effort” to produce a state.</p>\n  </li>\n  <li>\n    <p><strong>Light-Cone Complexity</strong>: Proposed by Shalizi et al., it measures the mutual information between the past and future light-cones of a point in a spacetime history, reflecting the predictive information content.</p>\n  </li>\n  <li><strong>Coffee Automaton Models</strong>:\n    <ul>\n      <li><strong>Interacting Model</strong>: Particles interact, swapping positions if they are adjacent and different.</li>\n      <li><strong>Non-Interacting Model</strong>: Particles move independently in random walks.</li>\n    </ul>\n  </li>\n  <li><strong>Experiment and Results</strong>:\n    <ul>\n      <li>The automaton begins with separated coffee and cream, mixing over time.</li>\n      <li><strong>Coarse-Graining</strong>: The state is averaged over local regions to produce a coarse-grained version.</li>\n      <li><strong>Measurements</strong>: Complexity and entropy are estimated using file compression (e.g., gzip) of the fine-grained and coarse-grained states.</li>\n      <li>Results show complexity increasing, peaking, and then decreasing, while entropy steadily increases.</li>\n    </ul>\n  </li>\n  <li><strong>Adjusted Coarse-Graining</strong>:\n    <ul>\n      <li>To reduce artifacts from thresholding, an adjustment method is introduced, enhancing the robustness of complexity measurements.</li>\n    </ul>\n  </li>\n  <li><strong>Conclusions and Further Work</strong>:\n    <ul>\n      <li>The coarse-graining approach effectively mirrors human intuition of complexity.</li>\n      <li>Future work could explore other metrics like light-cone complexity and improve theoretical foundations for complexity measures.</li>\n    </ul>\n  </li>\n</ul>\n<p>The paper explores the behavior of complexity in closed systems, comparing it to entropy which increases monotonically. The authors use a two-dimensional cellular automaton, simulating the mixing of “coffee” and “cream,” to model and measure complexity, referred to as “apparent complexity,” defined as the Kolmogorov complexity of a coarse-grained state.</p>\n<p><strong>Introduction</strong>: The paper begins by contrasting entropy with complexity. While entropy increases over time, complexity appears to rise, reach a maximum, and then fall. The authors aim to quantify this pattern using a simple automaton model.</p>\n<ul>\n      <li><strong>Entropy</strong>: Boltzmann entropy, Gibbs entropy, Shannon entropy, and Kolmogorov complexity.</li>\n      <li><strong>Complexity</strong>: Different measures of complexity are introduced, including apparent complexity, sophistication, logical depth, and light-cone complexity.</li>\n    </ul>\n<p><strong>Apparent Complexity</strong>: Defined as the Kolmogorov complexity of a denoised or smoothed version of a state. This measure aims to capture the “interesting” non-random information in a system.</p>\n<p><strong>Sophistication</strong>: A measure based on Kolmogorov complexity, aiming to capture the amount of non-random information in a system. It involves finding a set S such that a string x is a generic element of S.</p>\n<p><strong>Logical Depth</strong>: Introduced by Bennett, it measures the time taken by the shortest program to output a string, capturing the “computational effort” to produce a state.</p>\n<p><strong>Light-Cone Complexity</strong>: Proposed by Shalizi et al., it measures the mutual information between the past and future light-cones of a point in a spacetime history, reflecting the predictive information content.</p>\n<ul>\n      <li><strong>Interacting Model</strong>: Particles interact, swapping positions if they are adjacent and different.</li>\n      <li><strong>Non-Interacting Model</strong>: Particles move independently in random walks.</li>\n    </ul>\n<ul>\n      <li>The automaton begins with separated coffee and cream, mixing over time.</li>\n      <li><strong>Coarse-Graining</strong>: The state is averaged over local regions to produce a coarse-grained version.</li>\n      <li><strong>Measurements</strong>: Complexity and entropy are estimated using file compression (e.g., gzip) of the fine-grained and coarse-grained states.</li>\n      <li>Results show complexity increasing, peaking, and then decreasing, while entropy steadily increases.</li>\n    </ul>\n<ul>\n      <li>To reduce artifacts from thresholding, an adjustment method is introduced, enhancing the robustness of complexity measurements.</li>\n    </ul>\n<ul>\n      <li>The coarse-graining approach effectively mirrors human intuition of complexity.</li>\n      <li>Future work could explore other metrics like light-cone complexity and improve theoretical foundations for complexity measures.</li>\n    </ul>",
      "contentMarkdown": "*   Authors: Scott Aaronson, Sean M. Carroll, Lauren Ouellette\n*   The paper explores the behavior of complexity in closed systems, comparing it to entropy which increases monotonically. The authors use a two-dimensional cellular automaton, simulating the mixing of “coffee” and “cream,” to model and measure complexity, referred to as “apparent complexity,” defined as the Kolmogorov complexity of a coarse-grained state.\n    \n*   **Introduction**: The paper begins by contrasting entropy with complexity. While entropy increases over time, complexity appears to rise, reach a maximum, and then fall. The authors aim to quantify this pattern using a simple automaton model.\n    \n*   **Background**: Several concepts of entropy and complexity are discussed:\n    *   **Entropy**: Boltzmann entropy, Gibbs entropy, Shannon entropy, and Kolmogorov complexity.\n    *   **Complexity**: Different measures of complexity are introduced, including apparent complexity, sophistication, logical depth, and light-cone complexity.\n*   **Apparent Complexity**: Defined as the Kolmogorov complexity of a denoised or smoothed version of a state. This measure aims to capture the “interesting” non-random information in a system.\n    \n*   **Sophistication**: A measure based on Kolmogorov complexity, aiming to capture the amount of non-random information in a system. It involves finding a set S such that a string x is a generic element of S.\n    \n*   **Logical Depth**: Introduced by Bennett, it measures the time taken by the shortest program to output a string, capturing the “computational effort” to produce a state.\n    \n*   **Light-Cone Complexity**: Proposed by Shalizi et al., it measures the mutual information between the past and future light-cones of a point in a spacetime history, reflecting the predictive information content.\n    \n*   **Coffee Automaton Models**:\n    *   **Interacting Model**: Particles interact, swapping positions if they are adjacent and different.\n    *   **Non-Interacting Model**: Particles move independently in random walks.\n*   **Experiment and Results**:\n    *   The automaton begins with separated coffee and cream, mixing over time.\n    *   **Coarse-Graining**: The state is averaged over local regions to produce a coarse-grained version.\n    *   **Measurements**: Complexity and entropy are estimated using file compression (e.g., gzip) of the fine-grained and coarse-grained states.\n    *   Results show complexity increasing, peaking, and then decreasing, while entropy steadily increases.\n*   **Adjusted Coarse-Graining**:\n    *   To reduce artifacts from thresholding, an adjustment method is introduced, enhancing the robustness of complexity measurements.\n*   **Conclusions and Further Work**:\n    *   The coarse-graining approach effectively mirrors human intuition of complexity.\n    *   Future work could explore other metrics like light-cone complexity and improve theoretical foundations for complexity measures.\n\nThe paper explores the behavior of complexity in closed systems, comparing it to entropy which increases monotonically. The authors use a two-dimensional cellular automaton, simulating the mixing of “coffee” and “cream,” to model and measure complexity, referred to as “apparent complexity,” defined as the Kolmogorov complexity of a coarse-grained state.\n\n**Introduction**: The paper begins by contrasting entropy with complexity. While entropy increases over time, complexity appears to rise, reach a maximum, and then fall. The authors aim to quantify this pattern using a simple automaton model.\n\n*   **Entropy**: Boltzmann entropy, Gibbs entropy, Shannon entropy, and Kolmogorov complexity.\n*   **Complexity**: Different measures of complexity are introduced, including apparent complexity, sophistication, logical depth, and light-cone complexity.\n\n**Apparent Complexity**: Defined as the Kolmogorov complexity of a denoised or smoothed version of a state. This measure aims to capture the “interesting” non-random information in a system.\n\n**Sophistication**: A measure based on Kolmogorov complexity, aiming to capture the amount of non-random information in a system. It involves finding a set S such that a string x is a generic element of S.\n\n**Logical Depth**: Introduced by Bennett, it measures the time taken by the shortest program to output a string, capturing the “computational effort” to produce a state.\n\n**Light-Cone Complexity**: Proposed by Shalizi et al., it measures the mutual information between the past and future light-cones of a point in a spacetime history, reflecting the predictive information content.\n\n*   **Interacting Model**: Particles interact, swapping positions if they are adjacent and different.\n*   **Non-Interacting Model**: Particles move independently in random walks.\n\n*   The automaton begins with separated coffee and cream, mixing over time.\n*   **Coarse-Graining**: The state is averaged over local regions to produce a coarse-grained version.\n*   **Measurements**: Complexity and entropy are estimated using file compression (e.g., gzip) of the fine-grained and coarse-grained states.\n*   Results show complexity increasing, peaking, and then decreasing, while entropy steadily increases.\n\n*   To reduce artifacts from thresholding, an adjustment method is introduced, enhancing the robustness of complexity measurements.\n\n*   The coarse-graining approach effectively mirrors human intuition of complexity.\n*   Future work could explore other metrics like light-cone complexity and improve theoretical foundations for complexity measures.",
      "order": 19,
      "orderInChapter": 19,
      "difficulty": 3,
      "estimatedMinutes": 4,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 753,
        "contentLength": 6472
      },
      "nextCards": [
        "ai-top-30-papers-neural-turing-machines-20",
        "ai-top-30-papers-deep-speech-2-end-to-end-speech-recognition-in-eng-21"
      ],
      "relatedCards": [
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5",
        "ai-gpu-architecture-interconnects-6"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#quantifying-the-rise-and-fall-of-complexity-in-closed-systems:-the-coffee-automaton",
      "scrapedAt": "2025-12-28T11:56:36.591Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-neural-turing-machines-20",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Neural Turing Machines",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li><strong>Author</strong>: Alex Graves, Greg Wayne, Ivo Danihelka</li>\n  <li><strong>Summary</strong>:\n    <ul>\n      <li><strong>Introduction</strong>:\n        <ul>\n          <li>The paper introduces Neural Turing Machines (NTMs), a novel architecture that combines neural networks with external memory resources. This setup is inspired by the structure of a Turing Machine but is differentiable end-to-end, allowing it to be trained using gradient descent.</li>\n        </ul>\n      </li>\n      <li><strong>Foundational Research</strong>:\n        <ul>\n          <li><strong>Psychology and Neuroscience</strong>: Discusses working memory as a system involving short-term storage and manipulation of information, typically associated with the prefrontal cortex and basal ganglia.</li>\n          <li><strong>Cognitive Science and Linguistics</strong>: Highlights the evolution of cognitive science and the debates around connectionist theories, variable-binding, and recursive processing, which are critical for human cognition and language processing.</li>\n          <li><strong>Recurrent Neural Networks</strong>: Describes RNNs and Long Short-Term Memory (LSTM) networks, emphasizing their ability to handle sequences and their Turing-completeness, which allows them to simulate any algorithm given sufficient resources.</li>\n        </ul>\n      </li>\n      <li><strong>Neural Turing Machines</strong>:\n        <ul>\n          <li>NTMs combine a neural network controller with a memory matrix. This memory can be read from and written to using differentiable operations, making the entire system trainable via gradient descent.</li>\n          <li><strong>Reading and Writing</strong>: NTMs perform read and write operations using a weighting mechanism over the memory locations, which allows both fine-grained control and robust data storage.</li>\n          <li><strong>Addressing Mechanisms</strong>: NTMs employ both content-based and location-based addressing to efficiently manage memory operations. Content-based addressing focuses on the similarity of stored values, while location-based addressing facilitates iteration and random access.</li>\n          <li><strong>Controller Network</strong>: The architecture can use either a recurrent (LSTM) or feedforward neural network as the controller, with each choice offering different advantages.</li>\n        </ul>\n      </li>\n      <li><strong>Experiments</strong>:\n        <ul>\n          <li>The paper presents experiments on various tasks, such as copying, repeat copy, associative recall, dynamic N-grams, and priority sorting. NTMs demonstrated superior performance and generalization capabilities compared to standard LSTMs.</li>\n          <li><strong>Copy Task</strong>: NTMs learned to store and recall sequences more effectively than LSTMs, showing better generalization to longer sequences.</li>\n          <li><strong>Repeat Copy Task</strong>: NTMs excelled at repeating sequences a specified number of times, leveraging their memory and addressing mechanisms.</li>\n          <li><strong>Associative Recall</strong>: NTMs performed well in recalling items based on associative queries, using their ability to manage complex data structures.</li>\n          <li><strong>Dynamic N-Grams</strong>: NTMs adapted quickly to changing predictive distributions, outperforming LSTMs.</li>\n          <li><strong>Priority Sort</strong>: NTMs were capable of sorting data based on priorities, showcasing their algorithmic learning capabilities.</li>\n        </ul>\n      </li>\n      <li><strong>Conclusion</strong>:\n        <ul>\n          <li>NTMs represent a significant step towards more general and powerful neural network architectures. Their ability to learn and generalize simple algorithms opens up new possibilities for applications in machine learning and artificial intelligence.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>This paper introduces the Neural Turing Machine architecture, highlighting its foundation, structure, and performance in various algorithmic tasks, demonstrating its potential to revolutionize neural network capabilities by integrating external memory and addressing mechanisms.</li>\n</ul>\n<ul>\n      <li><strong>Introduction</strong>:\n        <ul>\n          <li>The paper introduces Neural Turing Machines (NTMs), a novel architecture that combines neural networks with external memory resources. This setup is inspired by the structure of a Turing Machine but is differentiable end-to-end, allowing it to be trained using gradient descent.</li>\n        </ul>\n      </li>\n      <li><strong>Foundational Research</strong>:\n        <ul>\n          <li><strong>Psychology and Neuroscience</strong>: Discusses working memory as a system involving short-term storage and manipulation of information, typically associated with the prefrontal cortex and basal ganglia.</li>\n          <li><strong>Cognitive Science and Linguistics</strong>: Highlights the evolution of cognitive science and the debates around connectionist theories, variable-binding, and recursive processing, which are critical for human cognition and language processing.</li>\n          <li><strong>Recurrent Neural Networks</strong>: Describes RNNs and Long Short-Term Memory (LSTM) networks, emphasizing their ability to handle sequences and their Turing-completeness, which allows them to simulate any algorithm given sufficient resources.</li>\n        </ul>\n      </li>\n      <li><strong>Neural Turing Machines</strong>:\n        <ul>\n          <li>NTMs combine a neural network controller with a memory matrix. This memory can be read from and written to using differentiable operations, making the entire system trainable via gradient descent.</li>\n          <li><strong>Reading and Writing</strong>: NTMs perform read and write operations using a weighting mechanism over the memory locations, which allows both fine-grained control and robust data storage.</li>\n          <li><strong>Addressing Mechanisms</strong>: NTMs employ both content-based and location-based addressing to efficiently manage memory operations. Content-based addressing focuses on the similarity of stored values, while location-based addressing facilitates iteration and random access.</li>\n          <li><strong>Controller Network</strong>: The architecture can use either a recurrent (LSTM) or feedforward neural network as the controller, with each choice offering different advantages.</li>\n        </ul>\n      </li>\n      <li><strong>Experiments</strong>:\n        <ul>\n          <li>The paper presents experiments on various tasks, such as copying, repeat copy, associative recall, dynamic N-grams, and priority sorting. NTMs demonstrated superior performance and generalization capabilities compared to standard LSTMs.</li>\n          <li><strong>Copy Task</strong>: NTMs learned to store and recall sequences more effectively than LSTMs, showing better generalization to longer sequences.</li>\n          <li><strong>Repeat Copy Task</strong>: NTMs excelled at repeating sequences a specified number of times, leveraging their memory and addressing mechanisms.</li>\n          <li><strong>Associative Recall</strong>: NTMs performed well in recalling items based on associative queries, using their ability to manage complex data structures.</li>\n          <li><strong>Dynamic N-Grams</strong>: NTMs adapted quickly to changing predictive distributions, outperforming LSTMs.</li>\n          <li><strong>Priority Sort</strong>: NTMs were capable of sorting data based on priorities, showcasing their algorithmic learning capabilities.</li>\n        </ul>\n      </li>\n      <li><strong>Conclusion</strong>:\n        <ul>\n          <li>NTMs represent a significant step towards more general and powerful neural network architectures. Their ability to learn and generalize simple algorithms opens up new possibilities for applications in machine learning and artificial intelligence.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>The paper introduces Neural Turing Machines (NTMs), a novel architecture that combines neural networks with external memory resources. This setup is inspired by the structure of a Turing Machine but is differentiable end-to-end, allowing it to be trained using gradient descent.</li>\n        </ul>\n<ul>\n          <li><strong>Psychology and Neuroscience</strong>: Discusses working memory as a system involving short-term storage and manipulation of information, typically associated with the prefrontal cortex and basal ganglia.</li>\n          <li><strong>Cognitive Science and Linguistics</strong>: Highlights the evolution of cognitive science and the debates around connectionist theories, variable-binding, and recursive processing, which are critical for human cognition and language processing.</li>\n          <li><strong>Recurrent Neural Networks</strong>: Describes RNNs and Long Short-Term Memory (LSTM) networks, emphasizing their ability to handle sequences and their Turing-completeness, which allows them to simulate any algorithm given sufficient resources.</li>\n        </ul>\n<ul>\n          <li>NTMs combine a neural network controller with a memory matrix. This memory can be read from and written to using differentiable operations, making the entire system trainable via gradient descent.</li>\n          <li><strong>Reading and Writing</strong>: NTMs perform read and write operations using a weighting mechanism over the memory locations, which allows both fine-grained control and robust data storage.</li>\n          <li><strong>Addressing Mechanisms</strong>: NTMs employ both content-based and location-based addressing to efficiently manage memory operations. Content-based addressing focuses on the similarity of stored values, while location-based addressing facilitates iteration and random access.</li>\n          <li><strong>Controller Network</strong>: The architecture can use either a recurrent (LSTM) or feedforward neural network as the controller, with each choice offering different advantages.</li>\n        </ul>\n<ul>\n          <li>The paper presents experiments on various tasks, such as copying, repeat copy, associative recall, dynamic N-grams, and priority sorting. NTMs demonstrated superior performance and generalization capabilities compared to standard LSTMs.</li>\n          <li><strong>Copy Task</strong>: NTMs learned to store and recall sequences more effectively than LSTMs, showing better generalization to longer sequences.</li>\n          <li><strong>Repeat Copy Task</strong>: NTMs excelled at repeating sequences a specified number of times, leveraging their memory and addressing mechanisms.</li>\n          <li><strong>Associative Recall</strong>: NTMs performed well in recalling items based on associative queries, using their ability to manage complex data structures.</li>\n          <li><strong>Dynamic N-Grams</strong>: NTMs adapted quickly to changing predictive distributions, outperforming LSTMs.</li>\n          <li><strong>Priority Sort</strong>: NTMs were capable of sorting data based on priorities, showcasing their algorithmic learning capabilities.</li>\n        </ul>\n<ul>\n          <li>NTMs represent a significant step towards more general and powerful neural network architectures. Their ability to learn and generalize simple algorithms opens up new possibilities for applications in machine learning and artificial intelligence.</li>\n        </ul>",
      "contentMarkdown": "*   **Author**: Alex Graves, Greg Wayne, Ivo Danihelka\n*   **Summary**:\n    *   **Introduction**:\n        *   The paper introduces Neural Turing Machines (NTMs), a novel architecture that combines neural networks with external memory resources. This setup is inspired by the structure of a Turing Machine but is differentiable end-to-end, allowing it to be trained using gradient descent.\n    *   **Foundational Research**:\n        *   **Psychology and Neuroscience**: Discusses working memory as a system involving short-term storage and manipulation of information, typically associated with the prefrontal cortex and basal ganglia.\n        *   **Cognitive Science and Linguistics**: Highlights the evolution of cognitive science and the debates around connectionist theories, variable-binding, and recursive processing, which are critical for human cognition and language processing.\n        *   **Recurrent Neural Networks**: Describes RNNs and Long Short-Term Memory (LSTM) networks, emphasizing their ability to handle sequences and their Turing-completeness, which allows them to simulate any algorithm given sufficient resources.\n    *   **Neural Turing Machines**:\n        *   NTMs combine a neural network controller with a memory matrix. This memory can be read from and written to using differentiable operations, making the entire system trainable via gradient descent.\n        *   **Reading and Writing**: NTMs perform read and write operations using a weighting mechanism over the memory locations, which allows both fine-grained control and robust data storage.\n        *   **Addressing Mechanisms**: NTMs employ both content-based and location-based addressing to efficiently manage memory operations. Content-based addressing focuses on the similarity of stored values, while location-based addressing facilitates iteration and random access.\n        *   **Controller Network**: The architecture can use either a recurrent (LSTM) or feedforward neural network as the controller, with each choice offering different advantages.\n    *   **Experiments**:\n        *   The paper presents experiments on various tasks, such as copying, repeat copy, associative recall, dynamic N-grams, and priority sorting. NTMs demonstrated superior performance and generalization capabilities compared to standard LSTMs.\n        *   **Copy Task**: NTMs learned to store and recall sequences more effectively than LSTMs, showing better generalization to longer sequences.\n        *   **Repeat Copy Task**: NTMs excelled at repeating sequences a specified number of times, leveraging their memory and addressing mechanisms.\n        *   **Associative Recall**: NTMs performed well in recalling items based on associative queries, using their ability to manage complex data structures.\n        *   **Dynamic N-Grams**: NTMs adapted quickly to changing predictive distributions, outperforming LSTMs.\n        *   **Priority Sort**: NTMs were capable of sorting data based on priorities, showcasing their algorithmic learning capabilities.\n    *   **Conclusion**:\n        *   NTMs represent a significant step towards more general and powerful neural network architectures. Their ability to learn and generalize simple algorithms opens up new possibilities for applications in machine learning and artificial intelligence.\n*   This paper introduces the Neural Turing Machine architecture, highlighting its foundation, structure, and performance in various algorithmic tasks, demonstrating its potential to revolutionize neural network capabilities by integrating external memory and addressing mechanisms.\n\n*   **Introduction**:\n    *   The paper introduces Neural Turing Machines (NTMs), a novel architecture that combines neural networks with external memory resources. This setup is inspired by the structure of a Turing Machine but is differentiable end-to-end, allowing it to be trained using gradient descent.\n*   **Foundational Research**:\n    *   **Psychology and Neuroscience**: Discusses working memory as a system involving short-term storage and manipulation of information, typically associated with the prefrontal cortex and basal ganglia.\n    *   **Cognitive Science and Linguistics**: Highlights the evolution of cognitive science and the debates around connectionist theories, variable-binding, and recursive processing, which are critical for human cognition and language processing.\n    *   **Recurrent Neural Networks**: Describes RNNs and Long Short-Term Memory (LSTM) networks, emphasizing their ability to handle sequences and their Turing-completeness, which allows them to simulate any algorithm given sufficient resources.\n*   **Neural Turing Machines**:\n    *   NTMs combine a neural network controller with a memory matrix. This memory can be read from and written to using differentiable operations, making the entire system trainable via gradient descent.\n    *   **Reading and Writing**: NTMs perform read and write operations using a weighting mechanism over the memory locations, which allows both fine-grained control and robust data storage.\n    *   **Addressing Mechanisms**: NTMs employ both content-based and location-based addressing to efficiently manage memory operations. Content-based addressing focuses on the similarity of stored values, while location-based addressing facilitates iteration and random access.\n    *   **Controller Network**: The architecture can use either a recurrent (LSTM) or feedforward neural network as the controller, with each choice offering different advantages.\n*   **Experiments**:\n    *   The paper presents experiments on various tasks, such as copying, repeat copy, associative recall, dynamic N-grams, and priority sorting. NTMs demonstrated superior performance and generalization capabilities compared to standard LSTMs.\n    *   **Copy Task**: NTMs learned to store and recall sequences more effectively than LSTMs, showing better generalization to longer sequences.\n    *   **Repeat Copy Task**: NTMs excelled at repeating sequences a specified number of times, leveraging their memory and addressing mechanisms.\n    *   **Associative Recall**: NTMs performed well in recalling items based on associative queries, using their ability to manage complex data structures.\n    *   **Dynamic N-Grams**: NTMs adapted quickly to changing predictive distributions, outperforming LSTMs.\n    *   **Priority Sort**: NTMs were capable of sorting data based on priorities, showcasing their algorithmic learning capabilities.\n*   **Conclusion**:\n    *   NTMs represent a significant step towards more general and powerful neural network architectures. Their ability to learn and generalize simple algorithms opens up new possibilities for applications in machine learning and artificial intelligence.\n\n*   The paper introduces Neural Turing Machines (NTMs), a novel architecture that combines neural networks with external memory resources. This setup is inspired by the structure of a Turing Machine but is differentiable end-to-end, allowing it to be trained using gradient descent.\n\n*   **Psychology and Neuroscience**: Discusses working memory as a system involving short-term storage and manipulation of information, typically associated with the prefrontal cortex and basal ganglia.\n*   **Cognitive Science and Linguistics**: Highlights the evolution of cognitive science and the debates around connectionist theories, variable-binding, and recursive processing, which are critical for human cognition and language processing.\n*   **Recurrent Neural Networks**: Describes RNNs and Long Short-Term Memory (LSTM) networks, emphasizing their ability to handle sequences and their Turing-completeness, which allows them to simulate any algorithm given sufficient resources.\n\n*   NTMs combine a neural network controller with a memory matrix. This memory can be read from and written to using differentiable operations, making the entire system trainable via gradient descent.\n*   **Reading and Writing**: NTMs perform read and write operations using a weighting mechanism over the memory locations, which allows both fine-grained control and robust data storage.\n*   **Addressing Mechanisms**: NTMs employ both content-based and location-based addressing to efficiently manage memory operations. Content-based addressing focuses on the similarity of stored values, while location-based addressing facilitates iteration and random access.\n*   **Controller Network**: The architecture can use either a recurrent (LSTM) or feedforward neural network as the controller, with each choice offering different advantages.\n\n*   The paper presents experiments on various tasks, such as copying, repeat copy, associative recall, dynamic N-grams, and priority sorting. NTMs demonstrated superior performance and generalization capabilities compared to standard LSTMs.\n*   **Copy Task**: NTMs learned to store and recall sequences more effectively than LSTMs, showing better generalization to longer sequences.\n*   **Repeat Copy Task**: NTMs excelled at repeating sequences a specified number of times, leveraging their memory and addressing mechanisms.\n*   **Associative Recall**: NTMs performed well in recalling items based on associative queries, using their ability to manage complex data structures.\n*   **Dynamic N-Grams**: NTMs adapted quickly to changing predictive distributions, outperforming LSTMs.\n*   **Priority Sort**: NTMs were capable of sorting data based on priorities, showcasing their algorithmic learning capabilities.\n\n*   NTMs represent a significant step towards more general and powerful neural network architectures. Their ability to learn and generalize simple algorithms opens up new possibilities for applications in machine learning and artificial intelligence.",
      "order": 20,
      "orderInChapter": 20,
      "difficulty": 3,
      "estimatedMinutes": 7,
      "tags": [
        "miscellaneous",
        "neural network",
        "machine learning",
        "rnn",
        "lstm",
        "gradient descent"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 1264,
        "contentLength": 11408
      },
      "nextCards": [
        "ai-top-30-papers-deep-speech-2-end-to-end-speech-recognition-in-eng-21",
        "ai-top-30-papers-scaling-laws-for-neural-language-models-22"
      ],
      "relatedCards": [
        "ai-model-debugging-weight-initialization-11",
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-pipeline-issues-6",
        "ai-ml-runtimes-architecture-10",
        "ai-ml-runtimes-supported-model-types-11"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#neural-turing-machines",
      "scrapedAt": "2025-12-28T11:56:36.591Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-deep-speech-2-end-to-end-speech-recognition-in-eng-21",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Authors:</strong> Baidu Research – Silicon Valley AI Lab</p>\n  </li>\n  <li>\n    <p><strong>Abstract:</strong>\nThe paper presents Deep Speech 2, an end-to-end deep learning model for speech recognition that can handle both English and Mandarin Chinese. The approach replaces traditional ASR pipelines with neural networks, enabling robustness to noisy environments, accents, and different languages. Leveraging high-performance computing techniques, the model achieves a significant speedup, allowing for rapid experimentation and model improvements. The system demonstrates competitive performance with human transcribers on several benchmarks and can be efficiently deployed in online settings with low latency.</p>\n  </li>\n  <li>\n    <p><strong>Introduction:</strong>\nTraditional ASR systems rely on multiple hand-engineered components, making them complex and hard to adapt to new languages or environments. Deep Speech 2 simplifies this by using deep learning to train a single model end-to-end. The system achieves high accuracy in both English and Mandarin, and can be quickly iterated upon thanks to efficient high-performance computing techniques.</p>\n  </li>\n  <li>\n    <p><strong>Model Architecture:</strong>\nThe model architecture includes multiple layers, such as convolutional layers for feature extraction and recurrent layers for temporal modeling. Key improvements over previous models include the use of Batch Normalization for faster convergence and SortaGrad for efficient training on varying-length sequences. The system also explores different recurrent unit types, like GRUs, and employs striding and row convolution for better performance and deployability.</p>\n  </li>\n  <li>\n    <p><strong>Training Data:</strong>\nTraining leverages extensive datasets, with 11,940 hours of English speech and 9,400 hours of Mandarin speech. Data augmentation techniques, such as adding noise, enhance robustness to different environments. The training process involves using large minibatches distributed over multiple GPUs, with synchronous SGD to maintain reproducibility.</p>\n  </li>\n  <li><strong>Results:</strong>\n    <ul>\n      <li><strong>English:</strong> Deep Speech 2 outperforms human transcribers on several read speech benchmarks, such as WSJ and LibriSpeech. It also shows significant improvements in handling accented and noisy speech, though it still lags behind human performance in very noisy conditions.</li>\n      <li><strong>Mandarin:</strong> The system achieves competitive results with human transcribers on short voice-query utterances. Architectural improvements, such as deeper networks and Batch Normalization, significantly enhance performance.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Deployment:</strong>\nThe system is designed for efficient deployment in production environments, using techniques like Batch Dispatch to ensure low latency when handling multiple user streams. This makes it suitable for real-time applications.</p>\n  </li>\n  <li>\n    <p><strong>Conclusion:</strong>\nDeep Speech 2 represents a significant advancement in end-to-end speech recognition, demonstrating high accuracy across different languages and conditions. Its ability to leverage large datasets and high-performance computing techniques allows for rapid development and deployment of robust ASR systems.</p>\n  </li>\n  <li>This summary covers the main findings and contributions of the Deep Speech 2 paper, highlighting its end-to-end deep learning approach, architectural innovations, and significant performance improvements in both English and Mandarin speech recognition.</li>\n</ul>\n<p><strong>Authors:</strong> Baidu Research – Silicon Valley AI Lab</p>\n<p><strong>Abstract:</strong>\nThe paper presents Deep Speech 2, an end-to-end deep learning model for speech recognition that can handle both English and Mandarin Chinese. The approach replaces traditional ASR pipelines with neural networks, enabling robustness to noisy environments, accents, and different languages. Leveraging high-performance computing techniques, the model achieves a significant speedup, allowing for rapid experimentation and model improvements. The system demonstrates competitive performance with human transcribers on several benchmarks and can be efficiently deployed in online settings with low latency.</p>\n<p><strong>Introduction:</strong>\nTraditional ASR systems rely on multiple hand-engineered components, making them complex and hard to adapt to new languages or environments. Deep Speech 2 simplifies this by using deep learning to train a single model end-to-end. The system achieves high accuracy in both English and Mandarin, and can be quickly iterated upon thanks to efficient high-performance computing techniques.</p>\n<p><strong>Model Architecture:</strong>\nThe model architecture includes multiple layers, such as convolutional layers for feature extraction and recurrent layers for temporal modeling. Key improvements over previous models include the use of Batch Normalization for faster convergence and SortaGrad for efficient training on varying-length sequences. The system also explores different recurrent unit types, like GRUs, and employs striding and row convolution for better performance and deployability.</p>\n<p><strong>Training Data:</strong>\nTraining leverages extensive datasets, with 11,940 hours of English speech and 9,400 hours of Mandarin speech. Data augmentation techniques, such as adding noise, enhance robustness to different environments. The training process involves using large minibatches distributed over multiple GPUs, with synchronous SGD to maintain reproducibility.</p>\n<ul>\n      <li><strong>English:</strong> Deep Speech 2 outperforms human transcribers on several read speech benchmarks, such as WSJ and LibriSpeech. It also shows significant improvements in handling accented and noisy speech, though it still lags behind human performance in very noisy conditions.</li>\n      <li><strong>Mandarin:</strong> The system achieves competitive results with human transcribers on short voice-query utterances. Architectural improvements, such as deeper networks and Batch Normalization, significantly enhance performance.</li>\n    </ul>\n<p><strong>Deployment:</strong>\nThe system is designed for efficient deployment in production environments, using techniques like Batch Dispatch to ensure low latency when handling multiple user streams. This makes it suitable for real-time applications.</p>\n<p><strong>Conclusion:</strong>\nDeep Speech 2 represents a significant advancement in end-to-end speech recognition, demonstrating high accuracy across different languages and conditions. Its ability to leverage large datasets and high-performance computing techniques allows for rapid development and deployment of robust ASR systems.</p>",
      "contentMarkdown": "*   **Authors:** Baidu Research – Silicon Valley AI Lab\n    \n*   **Abstract:** The paper presents Deep Speech 2, an end-to-end deep learning model for speech recognition that can handle both English and Mandarin Chinese. The approach replaces traditional ASR pipelines with neural networks, enabling robustness to noisy environments, accents, and different languages. Leveraging high-performance computing techniques, the model achieves a significant speedup, allowing for rapid experimentation and model improvements. The system demonstrates competitive performance with human transcribers on several benchmarks and can be efficiently deployed in online settings with low latency.\n    \n*   **Introduction:** Traditional ASR systems rely on multiple hand-engineered components, making them complex and hard to adapt to new languages or environments. Deep Speech 2 simplifies this by using deep learning to train a single model end-to-end. The system achieves high accuracy in both English and Mandarin, and can be quickly iterated upon thanks to efficient high-performance computing techniques.\n    \n*   **Model Architecture:** The model architecture includes multiple layers, such as convolutional layers for feature extraction and recurrent layers for temporal modeling. Key improvements over previous models include the use of Batch Normalization for faster convergence and SortaGrad for efficient training on varying-length sequences. The system also explores different recurrent unit types, like GRUs, and employs striding and row convolution for better performance and deployability.\n    \n*   **Training Data:** Training leverages extensive datasets, with 11,940 hours of English speech and 9,400 hours of Mandarin speech. Data augmentation techniques, such as adding noise, enhance robustness to different environments. The training process involves using large minibatches distributed over multiple GPUs, with synchronous SGD to maintain reproducibility.\n    \n*   **Results:**\n    *   **English:** Deep Speech 2 outperforms human transcribers on several read speech benchmarks, such as WSJ and LibriSpeech. It also shows significant improvements in handling accented and noisy speech, though it still lags behind human performance in very noisy conditions.\n    *   **Mandarin:** The system achieves competitive results with human transcribers on short voice-query utterances. Architectural improvements, such as deeper networks and Batch Normalization, significantly enhance performance.\n*   **Deployment:** The system is designed for efficient deployment in production environments, using techniques like Batch Dispatch to ensure low latency when handling multiple user streams. This makes it suitable for real-time applications.\n    \n*   **Conclusion:** Deep Speech 2 represents a significant advancement in end-to-end speech recognition, demonstrating high accuracy across different languages and conditions. Its ability to leverage large datasets and high-performance computing techniques allows for rapid development and deployment of robust ASR systems.\n    \n*   This summary covers the main findings and contributions of the Deep Speech 2 paper, highlighting its end-to-end deep learning approach, architectural innovations, and significant performance improvements in both English and Mandarin speech recognition.\n\n**Authors:** Baidu Research – Silicon Valley AI Lab\n\n**Abstract:** The paper presents Deep Speech 2, an end-to-end deep learning model for speech recognition that can handle both English and Mandarin Chinese. The approach replaces traditional ASR pipelines with neural networks, enabling robustness to noisy environments, accents, and different languages. Leveraging high-performance computing techniques, the model achieves a significant speedup, allowing for rapid experimentation and model improvements. The system demonstrates competitive performance with human transcribers on several benchmarks and can be efficiently deployed in online settings with low latency.\n\n**Introduction:** Traditional ASR systems rely on multiple hand-engineered components, making them complex and hard to adapt to new languages or environments. Deep Speech 2 simplifies this by using deep learning to train a single model end-to-end. The system achieves high accuracy in both English and Mandarin, and can be quickly iterated upon thanks to efficient high-performance computing techniques.\n\n**Model Architecture:** The model architecture includes multiple layers, such as convolutional layers for feature extraction and recurrent layers for temporal modeling. Key improvements over previous models include the use of Batch Normalization for faster convergence and SortaGrad for efficient training on varying-length sequences. The system also explores different recurrent unit types, like GRUs, and employs striding and row convolution for better performance and deployability.\n\n**Training Data:** Training leverages extensive datasets, with 11,940 hours of English speech and 9,400 hours of Mandarin speech. Data augmentation techniques, such as adding noise, enhance robustness to different environments. The training process involves using large minibatches distributed over multiple GPUs, with synchronous SGD to maintain reproducibility.\n\n*   **English:** Deep Speech 2 outperforms human transcribers on several read speech benchmarks, such as WSJ and LibriSpeech. It also shows significant improvements in handling accented and noisy speech, though it still lags behind human performance in very noisy conditions.\n*   **Mandarin:** The system achieves competitive results with human transcribers on short voice-query utterances. Architectural improvements, such as deeper networks and Batch Normalization, significantly enhance performance.\n\n**Deployment:** The system is designed for efficient deployment in production environments, using techniques like Batch Dispatch to ensure low latency when handling multiple user streams. This makes it suitable for real-time applications.\n\n**Conclusion:** Deep Speech 2 represents a significant advancement in end-to-end speech recognition, demonstrating high accuracy across different languages and conditions. Its ability to leverage large datasets and high-performance computing techniques allows for rapid development and deployment of robust ASR systems.",
      "order": 21,
      "orderInChapter": 21,
      "difficulty": 3,
      "estimatedMinutes": 5,
      "tags": [
        "miscellaneous",
        "neural network",
        "deep learning",
        "convolution",
        "gru",
        "batch normalization",
        "data augmentation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 833,
        "contentLength": 6827
      },
      "nextCards": [
        "ai-top-30-papers-scaling-laws-for-neural-language-models-22",
        "ai-top-30-papers-a-tutorial-introduction-to-the-minimum-description-23"
      ],
      "relatedCards": [
        "ai-gpu-architecture-operator-dispatch-and-kernel-mapping-14",
        "ai-cnns-for-text-classification-convolutions-1",
        "ai-cnns-for-text-classification-pooling-2",
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-model-definition-and-training-cpugpu-13"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#deep-speech-2:-end-to-end-speech-recognition-in-english-and-mandarin",
      "scrapedAt": "2025-12-28T11:56:36.591Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-scaling-laws-for-neural-language-models-22",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Scaling Laws for Neural Language Models",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li><strong>Authors</strong>: Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei</li>\n  <li>\n    <p><strong>Institution</strong>: OpenAI, Johns Hopkins University</p>\n  </li>\n  <li>The paper “Scaling Laws for Neural Language Models” explores empirical scaling laws that describe the relationship between language model performance and factors such as model size, dataset size, and computational resources used for training. The study finds that performance scales predictably according to power laws over several orders of magnitude. Key findings include:</li>\n</ul>\n<p><strong>Institution</strong>: OpenAI, Johns Hopkins University</p>\n<ol>\n  <li><strong>Power-law relationships</strong>: Language model performance improves predictably with increases in model size (number of parameters), dataset size (number of tokens), and compute (floating point operations). These improvements follow simple power-law relationships.</li>\n  <li><strong>Model size and data efficiency</strong>: Larger models are significantly more sample-efficient, meaning they require fewer data points to achieve the same level of performance compared to smaller models.</li>\n  <li><strong>Optimal compute allocation</strong>: For a fixed compute budget, it is most efficient to train very large models on a relatively modest amount of data and to stop training before full convergence.</li>\n  <li><strong>Minimal architectural effects</strong>: Performance depends strongly on scale (size, data, compute) and weakly on specific architectural hyperparameters such as network width or depth.</li>\n</ol>\n<ul>\n  <li>Key Equations</li>\n  <li><strong>Model performance as a function of parameters</strong>:\n-<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi><mo stretchy=&quot;false&quot;>(</mo><mi>N</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>N</mi><mi>c</mi></msub><mi>N</mi></mfrac><mo>)</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>&amp;#x03B1;</mi><mi>N</mi></msub></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-149\" style=\"width: 7.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.992em, 1006.25em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-150\"><span class=\"mi\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-154\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-155\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-156\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.023em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.98em, 4.586em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-157\"><span class=\"mo\" id=\"MathJax-Span-158\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">(</span></span></span><span class=\"mfrac\" id=\"MathJax-Span-159\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.73em, 4.273em, -999.997em); top: -4.529em; left: 50%; margin-left: -0.362em;\"><span class=\"msubsup\" id=\"MathJax-Span-160\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-161\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-162\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-163\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.89em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.888em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-164\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.737em; left: 2.138em;\"><span class=\"texatom\" id=\"MathJax-Span-165\"><span class=\"mrow\" id=\"MathJax-Span-166\"><span class=\"msubsup\" id=\"MathJax-Span-167\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-168\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-169\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>N</mi><mi>c</mi></msub><mi>N</mi></mfrac><mo>)</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>α</mi><mi>N</mi></msub></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">L(N) = \\left( \\frac{N_c}{N} \\right)^{\\alpha_N}</script>\n    <ul>\n      <li>Where<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-170\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-171\"><span class=\"mi\" id=\"MathJax-Span-172\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">L</script> is the loss,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-173\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-174\"><span class=\"mi\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">N</script> is the number of non-embedding parameters,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>N</mi><mi>c</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-176\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.04em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-177\"><span class=\"msubsup\" id=\"MathJax-Span-178\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-180\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>N</mi><mi>c</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">N_c</script> is a constant, and<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B1;</mi><mi>N</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-181\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-182\"><span class=\"msubsup\" id=\"MathJax-Span-183\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-184\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-185\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>α</mi><mi>N</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">\\alpha_N</script> is the scaling exponent.</li>\n    </ul>\n  </li>\n  <li><strong>Dataset size relationship</strong>:\n-<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi><mo stretchy=&quot;false&quot;>(</mo><mi>D</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>D</mi><mi>c</mi></msub><mi>D</mi></mfrac><mo>)</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>&amp;#x03B1;</mi><mi>D</mi></msub></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-186\" style=\"width: 7.503em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.253em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.992em, 1006.25em, 2.763em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-187\"><span class=\"mi\" id=\"MathJax-Span-188\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-189\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-190\" style=\"font-family: STIXGeneral-Italic;\">D</span><span class=\"mo\" id=\"MathJax-Span-191\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-192\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-193\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.076em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1002.03em, 4.586em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-194\"><span class=\"mo\" id=\"MathJax-Span-195\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">(</span></span></span><span class=\"mfrac\" id=\"MathJax-Span-196\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.78em, 4.273em, -999.997em); top: -4.529em; left: 50%; margin-left: -0.414em;\"><span class=\"msubsup\" id=\"MathJax-Span-197\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-198\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-199\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.52em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-200\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.89em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.888em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-201\" style=\"vertical-align: -0.258em;\"><span><span style=\"font-size: 110%; font-family: STIXSizeOneSym;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.737em; left: 2.19em;\"><span class=\"texatom\" id=\"MathJax-Span-202\"><span class=\"mrow\" id=\"MathJax-Span-203\"><span class=\"msubsup\" id=\"MathJax-Span-204\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-205\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"mi\" id=\"MathJax-Span-206\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.559em; border-left: 0px solid; width: 0px; height: 1.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi><mo stretchy=\"false\">(</mo><mi>D</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>D</mi><mi>c</mi></msub><mi>D</mi></mfrac><mo>)</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>α</mi><mi>D</mi></msub></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">L(D) = \\left( \\frac{D_c}{D} \\right)^{\\alpha_D}</script>\n    <ul>\n      <li>Where<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-207\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-208\"><span class=\"mi\" id=\"MathJax-Span-209\" style=\"font-family: STIXGeneral-Italic;\">D</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">D</script> is the dataset size in tokens,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>D</mi><mi>c</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-210\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-211\"><span class=\"msubsup\" id=\"MathJax-Span-212\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-213\" style=\"font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-214\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>D</mi><mi>c</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">D_c</script> is a constant, and<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B1;</mi><mi>D</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-215\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-216\"><span class=\"msubsup\" id=\"MathJax-Span-217\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-219\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>α</mi><mi>D</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">\\alpha_D</script> is the scaling exponent.</li>\n    </ul>\n  </li>\n  <li><strong>Compute efficiency</strong>:\n-<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext><mo>,</mo><mi>c</mi></mrow></msub><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext></mrow></msub></mfrac><mo>)</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>&amp;#x03B1;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext><mo>,</mo><mi>C</mi></mrow></msub></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-220\" style=\"width: 11.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.273em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.836em, 1009.27em, 3.023em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-221\"><span class=\"mi\" id=\"MathJax-Span-222\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-223\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-224\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-225\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-226\"><span class=\"mrow\" id=\"MathJax-Span-227\"><span class=\"mtext\" id=\"MathJax-Span-228\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">min</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-229\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-230\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-231\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 4.951em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.659em, 1003.08em, 4.846em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-232\"><span class=\"mo\" id=\"MathJax-Span-233\" style=\"vertical-align: -0.414em;\"><span style=\"font-family: STIXSizeTwoSym;\">(</span></span><span class=\"mfrac\" id=\"MathJax-Span-234\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1001.67em, 4.326em, -999.997em); top: -4.581em; left: 50%; margin-left: -0.831em;\"><span class=\"msubsup\" id=\"MathJax-Span-235\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-236\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-237\"><span class=\"mrow\" id=\"MathJax-Span-238\"><span class=\"mtext\" id=\"MathJax-Span-239\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">min</span><span class=\"mo\" id=\"MathJax-Span-240\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-241\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">c</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.3em, 4.273em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.674em;\"><span class=\"msubsup\" id=\"MathJax-Span-242\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-243\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-244\"><span class=\"mrow\" id=\"MathJax-Span-245\"><span class=\"mtext\" id=\"MathJax-Span-246\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">min</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1001.77em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.773em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-247\" style=\"vertical-align: -0.414em;\"><span style=\"font-family: STIXSizeTwoSym;\">)</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.893em; left: 3.232em;\"><span class=\"texatom\" id=\"MathJax-Span-248\"><span class=\"mrow\" id=\"MathJax-Span-249\"><span class=\"msubsup\" id=\"MathJax-Span-250\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-251\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-252\"><span class=\"mrow\" id=\"MathJax-Span-253\"><span class=\"mtext\" id=\"MathJax-Span-254\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">min</span><span class=\"mo\" id=\"MathJax-Span-255\" style=\"font-size: 50%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-256\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi><mo stretchy=\"false\">(</mo><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext></mrow></msub><mo stretchy=\"false\">)</mo><mo>=</mo><msup><mrow><mo>(</mo><mfrac><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext><mo>,</mo><mi>c</mi></mrow></msub><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext></mrow></msub></mfrac><mo>)</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>α</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext><mo>,</mo><mi>C</mi></mrow></msub></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">L(C_{\\text{min}}) = \\left( \\frac{C_{\\text{min}, c}}{C_{\\text{min}}} \\right)^{\\alpha_{\\text{min}, C}}</script>\n    <ul>\n      <li>Where<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-257\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.83em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-258\"><span class=\"msubsup\" id=\"MathJax-Span-259\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-260\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-261\"><span class=\"mrow\" id=\"MathJax-Span-262\"><span class=\"mtext\" id=\"MathJax-Span-263\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">min</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">C_{\\text{min}}</script> is the minimum compute required,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext><mo>,</mo><mi>c</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-264\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.555em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-265\"><span class=\"msubsup\" id=\"MathJax-Span-266\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-267\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-268\"><span class=\"mrow\" id=\"MathJax-Span-269\"><span class=\"mtext\" id=\"MathJax-Span-270\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">min</span><span class=\"mo\" id=\"MathJax-Span-271\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-272\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext><mo>,</mo><mi>c</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">C_{\\text{min}, c}</script> is a constant, and<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B1;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext><mo>,</mo><mi>C</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-273\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1002.4em, 2.555em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-274\"><span class=\"msubsup\" id=\"MathJax-Span-275\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-276\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-277\"><span class=\"mrow\" id=\"MathJax-Span-278\"><span class=\"mtext\" id=\"MathJax-Span-279\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">min</span><span class=\"mo\" id=\"MathJax-Span-280\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-281\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>α</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext><mo>,</mo><mi>C</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">\\alpha_{\\text{min}, C}</script> is the scaling exponent.</li>\n    </ul>\n  </li>\n  <li><strong>Sample efficiency</strong>: Larger models trained with the same amount of data achieve better performance due to their improved ability to utilize the data.</li>\n  <li><strong>Training dynamics</strong>: Training curves follow predictable power-laws, allowing early extrapolation to predict the final performance of the model.</li>\n  <li><strong>Generalization</strong>: Performance on different datasets improves consistently with the performance on the training dataset, suggesting that better in-distribution performance translates to better out-of-distribution performance.</li>\n  <li><strong>Model size vs. dataset size</strong>: As model size increases, the dataset size should be scaled sublinearly to avoid overfitting, implying that moderately increasing data is sufficient for much larger models.</li>\n  <li>\n    <p><strong>Compute-efficient training</strong>: Optimal performance is achieved by training very large models for fewer steps, using relatively small datasets compared to the model size.</p>\n  </li>\n  <li>These findings provide a framework for understanding and predicting the performance of large-scale neural language models, guiding future research and practical applications in optimizing model training and deployment.</li>\n</ul>\n<ul>\n      <li>Where<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>L</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-170\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-171\"><span class=\"mi\" id=\"MathJax-Span-172\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>L</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">L</script> is the loss,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-173\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-174\"><span class=\"mi\" id=\"MathJax-Span-175\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">N</script> is the number of non-embedding parameters,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>N</mi><mi>c</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-176\" style=\"width: 1.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.04em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-177\"><span class=\"msubsup\" id=\"MathJax-Span-178\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"mi\" id=\"MathJax-Span-180\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>N</mi><mi>c</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">N_c</script> is a constant, and<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B1;</mi><mi>N</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-181\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-182\"><span class=\"msubsup\" id=\"MathJax-Span-183\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-184\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-185\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>α</mi><mi>N</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">\\alpha_N</script> is the scaling exponent.</li>\n    </ul>\n<ul>\n      <li>Where<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>D</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-207\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-208\"><span class=\"mi\" id=\"MathJax-Span-209\" style=\"font-family: STIXGeneral-Italic;\">D</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>D</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">D</script> is the dataset size in tokens,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>D</mi><mi>c</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-210\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.1em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-211\"><span class=\"msubsup\" id=\"MathJax-Span-212\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-213\" style=\"font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.732em;\"><span class=\"mi\" id=\"MathJax-Span-214\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>D</mi><mi>c</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">D_c</script> is a constant, and<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B1;</mi><mi>D</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-215\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.15em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-216\"><span class=\"msubsup\" id=\"MathJax-Span-217\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"mi\" id=\"MathJax-Span-219\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">D</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>α</mi><mi>D</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">\\alpha_D</script> is the scaling exponent.</li>\n    </ul>\n<ul>\n      <li>Where<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-257\" style=\"width: 2.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.83em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-258\"><span class=\"msubsup\" id=\"MathJax-Span-259\"><span style=\"display: inline-block; position: relative; width: 1.826em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-260\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-261\"><span class=\"mrow\" id=\"MathJax-Span-262\"><span class=\"mtext\" id=\"MathJax-Span-263\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">min</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">C_{\\text{min}}</script> is the minimum compute required,<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>C</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext><mo>,</mo><mi>c</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-264\" style=\"width: 2.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1002.35em, 2.555em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-265\"><span class=\"msubsup\" id=\"MathJax-Span-266\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-267\" style=\"font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-268\"><span class=\"mrow\" id=\"MathJax-Span-269\"><span class=\"mtext\" id=\"MathJax-Span-270\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">min</span><span class=\"mo\" id=\"MathJax-Span-271\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-272\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">c</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>C</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext><mo>,</mo><mi>c</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">C_{\\text{min}, c}</script> is a constant, and<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B1;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>min</mtext><mo>,</mo><mi>C</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-273\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1002.4em, 2.555em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-274\"><span class=\"msubsup\" id=\"MathJax-Span-275\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-276\" style=\"font-family: STIXGeneral-Italic;\">α</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-277\"><span class=\"mrow\" id=\"MathJax-Span-278\"><span class=\"mtext\" id=\"MathJax-Span-279\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">min</span><span class=\"mo\" id=\"MathJax-Span-280\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-281\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>α</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>min</mtext><mo>,</mo><mi>C</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">\\alpha_{\\text{min}, C}</script> is the scaling exponent.</li>\n    </ul>\n<p><strong>Compute-efficient training</strong>: Optimal performance is achieved by training very large models for fewer steps, using relatively small datasets compared to the model size.</p>",
      "contentMarkdown": "*   **Authors**: Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei\n*   **Institution**: OpenAI, Johns Hopkins University\n    \n*   The paper “Scaling Laws for Neural Language Models” explores empirical scaling laws that describe the relationship between language model performance and factors such as model size, dataset size, and computational resources used for training. The study finds that performance scales predictably according to power laws over several orders of magnitude. Key findings include:\n\n**Institution**: OpenAI, Johns Hopkins University\n\n1.  **Power-law relationships**: Language model performance improves predictably with increases in model size (number of parameters), dataset size (number of tokens), and compute (floating point operations). These improvements follow simple power-law relationships.\n2.  **Model size and data efficiency**: Larger models are significantly more sample-efficient, meaning they require fewer data points to achieve the same level of performance compared to smaller models.\n3.  **Optimal compute allocation**: For a fixed compute budget, it is most efficient to train very large models on a relatively modest amount of data and to stop training before full convergence.\n4.  **Minimal architectural effects**: Performance depends strongly on scale (size, data, compute) and weakly on specific architectural hyperparameters such as network width or depth.\n\n*   Key Equations\n*   **Model performance as a function of parameters**: -L(N)\\=(NcN)αNL(N)\\=(NcN)αNL(N) = \\\\left( \\\\frac{N\\_c}{N} \\\\right)^{\\\\alpha\\_N}\n    *   WhereLLL is the loss,NNN is the number of non-embedding parameters,NcNcN\\_c is a constant, andαNαN\\\\alpha\\_N is the scaling exponent.\n*   **Dataset size relationship**: -L(D)\\=(DcD)αDL(D)\\=(DcD)αDL(D) = \\\\left( \\\\frac{D\\_c}{D} \\\\right)^{\\\\alpha\\_D}\n    *   WhereDDD is the dataset size in tokens,DcDcD\\_c is a constant, andαDαD\\\\alpha\\_D is the scaling exponent.\n*   **Compute efficiency**: -L(Cmin)\\=(Cmin,cCmin)αmin,CL(Cmin)\\=(Cmin,cCmin)αmin,CL(C\\_{\\\\text{min}}) = \\\\left( \\\\frac{C\\_{\\\\text{min}, c}}{C\\_{\\\\text{min}}} \\\\right)^{\\\\alpha\\_{\\\\text{min}, C}}\n    *   WhereCminCminC\\_{\\\\text{min}} is the minimum compute required,Cmin,cCmin,cC\\_{\\\\text{min}, c} is a constant, andαmin,Cαmin,C\\\\alpha\\_{\\\\text{min}, C} is the scaling exponent.\n*   **Sample efficiency**: Larger models trained with the same amount of data achieve better performance due to their improved ability to utilize the data.\n*   **Training dynamics**: Training curves follow predictable power-laws, allowing early extrapolation to predict the final performance of the model.\n*   **Generalization**: Performance on different datasets improves consistently with the performance on the training dataset, suggesting that better in-distribution performance translates to better out-of-distribution performance.\n*   **Model size vs. dataset size**: As model size increases, the dataset size should be scaled sublinearly to avoid overfitting, implying that moderately increasing data is sufficient for much larger models.\n*   **Compute-efficient training**: Optimal performance is achieved by training very large models for fewer steps, using relatively small datasets compared to the model size.\n    \n*   These findings provide a framework for understanding and predicting the performance of large-scale neural language models, guiding future research and practical applications in optimizing model training and deployment.\n\n*   WhereLLL is the loss,NNN is the number of non-embedding parameters,NcNcN\\_c is a constant, andαNαN\\\\alpha\\_N is the scaling exponent.\n\n*   WhereDDD is the dataset size in tokens,DcDcD\\_c is a constant, andαDαD\\\\alpha\\_D is the scaling exponent.\n\n*   WhereCminCminC\\_{\\\\text{min}} is the minimum compute required,Cmin,cCmin,cC\\_{\\\\text{min}, c} is a constant, andαmin,Cαmin,C\\\\alpha\\_{\\\\text{min}, C} is the scaling exponent.\n\n**Compute-efficient training**: Optimal performance is achieved by training very large models for fewer steps, using relatively small datasets compared to the model size.",
      "order": 22,
      "orderInChapter": 22,
      "difficulty": 4,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous",
        "embedding"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 519,
        "contentLength": 60479
      },
      "nextCards": [
        "ai-top-30-papers-a-tutorial-introduction-to-the-minimum-description-23",
        "ai-top-30-papers-machine-super-intelligence-24"
      ],
      "relatedCards": [
        "ai-ml-runtimes-architecture-27",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-choosing-the-right-ann-algorithm-family-9",
        "ai-ann-similarity-search-graph-based-methods-7",
        "ai-ann-similarity-search-annoy-approximate-nearest-neighbors-oh-yeah-12"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#scaling-laws-for-neural-language-models",
      "scrapedAt": "2025-12-28T11:56:36.592Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-a-tutorial-introduction-to-the-minimum-description-23",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "A Tutorial Introduction to the Minimum Description Length Principle",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>Authors: Peter Grünwald</li>\n  <li>\n    <p>This paper provides an extensive introduction and technical exposition on Rissanen’s Minimum Description Length (MDL) Principle. The tutorial is structured to offer both a conceptual and a technically precise exploration of MDL, making the ideas accessible first at a conceptual level and then delving into mathematical specifics.</p>\n  </li>\n  <li>Key Technical Details:\n    <ol>\n      <li>\n        <p><strong>MDL and Data Compression</strong>: The MDL Principle is introduced as a method of statistical modeling and inference that views learning and model selection through the lens of data compression. It encapsulates the idea that the best model of a dataset is the one that compresses the data most effectively, balancing model complexity and goodness of fit.</p>\n      </li>\n      <li>\n        <p><strong>Kolmogorov Complexity and MDL</strong>: The tutorial discusses Kolmogorov Complexity as a theoretical foundation of MDL, describing it as the length of the shortest possible description of a string in some fixed universal language.</p>\n      </li>\n      <li>\n        <p><strong>Practical MDL</strong>: This involves approximations of ideal MDL to make it applicable in real-world scenarios, where exact computation of Kolmogorov Complexity is not feasible. Practical implementations often use statistical models and coding schemes that approximate the Kolmogorov Complexity.</p>\n      </li>\n      <li>\n        <p><strong>Refined and Crude MDL</strong>: The distinction between crude MDL, which approximates the model cost without considering the exact fit, and refined MDL, which provides a more precise model by considering both the cost of the model and the cost of fitting the model to the data, is elaborated.</p>\n      </li>\n      <li>\n        <p><strong>MDL for Model Selection</strong>: MDL is particularly highlighted for its utility in model selection, where it serves as a criterion to choose between competing models by evaluating which model provides the best compression of the data.</p>\n      </li>\n      <li>\n        <p><strong>Statistical and Information Theoretic Underpinnings</strong>: The tutorial introduces the basic concepts of information theory relevant to MDL, such as entropy, mutual information, and the relationship between probability and codelength, primarily through the Kraft Inequality and the Information Inequality.</p>\n      </li>\n      <li>\n        <p><strong>Applications and Extensions</strong>: The document discusses various applications of MDL in areas like coding, machine learning, and statistical inference, showing how MDL can be a unifying approach in understanding and applying concepts across these domains.</p>\n      </li>\n    </ol>\n  </li>\n  <li>The document serves as a comprehensive introduction to MDL, providing essential insights into both the theoretical and practical aspects of the principle. It emphasizes the importance of MDL in selecting models that are not just good at fitting the data, but also in providing meaningful insights in a parsimonious way .</li>\n</ul>\n<p>This paper provides an extensive introduction and technical exposition on Rissanen’s Minimum Description Length (MDL) Principle. The tutorial is structured to offer both a conceptual and a technically precise exploration of MDL, making the ideas accessible first at a conceptual level and then delving into mathematical specifics.</p>\n<ol>\n      <li>\n        <p><strong>MDL and Data Compression</strong>: The MDL Principle is introduced as a method of statistical modeling and inference that views learning and model selection through the lens of data compression. It encapsulates the idea that the best model of a dataset is the one that compresses the data most effectively, balancing model complexity and goodness of fit.</p>\n      </li>\n      <li>\n        <p><strong>Kolmogorov Complexity and MDL</strong>: The tutorial discusses Kolmogorov Complexity as a theoretical foundation of MDL, describing it as the length of the shortest possible description of a string in some fixed universal language.</p>\n      </li>\n      <li>\n        <p><strong>Practical MDL</strong>: This involves approximations of ideal MDL to make it applicable in real-world scenarios, where exact computation of Kolmogorov Complexity is not feasible. Practical implementations often use statistical models and coding schemes that approximate the Kolmogorov Complexity.</p>\n      </li>\n      <li>\n        <p><strong>Refined and Crude MDL</strong>: The distinction between crude MDL, which approximates the model cost without considering the exact fit, and refined MDL, which provides a more precise model by considering both the cost of the model and the cost of fitting the model to the data, is elaborated.</p>\n      </li>\n      <li>\n        <p><strong>MDL for Model Selection</strong>: MDL is particularly highlighted for its utility in model selection, where it serves as a criterion to choose between competing models by evaluating which model provides the best compression of the data.</p>\n      </li>\n      <li>\n        <p><strong>Statistical and Information Theoretic Underpinnings</strong>: The tutorial introduces the basic concepts of information theory relevant to MDL, such as entropy, mutual information, and the relationship between probability and codelength, primarily through the Kraft Inequality and the Information Inequality.</p>\n      </li>\n      <li>\n        <p><strong>Applications and Extensions</strong>: The document discusses various applications of MDL in areas like coding, machine learning, and statistical inference, showing how MDL can be a unifying approach in understanding and applying concepts across these domains.</p>\n      </li>\n    </ol>\n<p><strong>MDL and Data Compression</strong>: The MDL Principle is introduced as a method of statistical modeling and inference that views learning and model selection through the lens of data compression. It encapsulates the idea that the best model of a dataset is the one that compresses the data most effectively, balancing model complexity and goodness of fit.</p>\n<p><strong>Kolmogorov Complexity and MDL</strong>: The tutorial discusses Kolmogorov Complexity as a theoretical foundation of MDL, describing it as the length of the shortest possible description of a string in some fixed universal language.</p>\n<p><strong>Practical MDL</strong>: This involves approximations of ideal MDL to make it applicable in real-world scenarios, where exact computation of Kolmogorov Complexity is not feasible. Practical implementations often use statistical models and coding schemes that approximate the Kolmogorov Complexity.</p>\n<p><strong>Refined and Crude MDL</strong>: The distinction between crude MDL, which approximates the model cost without considering the exact fit, and refined MDL, which provides a more precise model by considering both the cost of the model and the cost of fitting the model to the data, is elaborated.</p>\n<p><strong>MDL for Model Selection</strong>: MDL is particularly highlighted for its utility in model selection, where it serves as a criterion to choose between competing models by evaluating which model provides the best compression of the data.</p>\n<p><strong>Statistical and Information Theoretic Underpinnings</strong>: The tutorial introduces the basic concepts of information theory relevant to MDL, such as entropy, mutual information, and the relationship between probability and codelength, primarily through the Kraft Inequality and the Information Inequality.</p>\n<p><strong>Applications and Extensions</strong>: The document discusses various applications of MDL in areas like coding, machine learning, and statistical inference, showing how MDL can be a unifying approach in understanding and applying concepts across these domains.</p>",
      "contentMarkdown": "*   Authors: Peter Grünwald\n*   This paper provides an extensive introduction and technical exposition on Rissanen’s Minimum Description Length (MDL) Principle. The tutorial is structured to offer both a conceptual and a technically precise exploration of MDL, making the ideas accessible first at a conceptual level and then delving into mathematical specifics.\n    \n*   Key Technical Details:\n    1.  **MDL and Data Compression**: The MDL Principle is introduced as a method of statistical modeling and inference that views learning and model selection through the lens of data compression. It encapsulates the idea that the best model of a dataset is the one that compresses the data most effectively, balancing model complexity and goodness of fit.\n        \n    2.  **Kolmogorov Complexity and MDL**: The tutorial discusses Kolmogorov Complexity as a theoretical foundation of MDL, describing it as the length of the shortest possible description of a string in some fixed universal language.\n        \n    3.  **Practical MDL**: This involves approximations of ideal MDL to make it applicable in real-world scenarios, where exact computation of Kolmogorov Complexity is not feasible. Practical implementations often use statistical models and coding schemes that approximate the Kolmogorov Complexity.\n        \n    4.  **Refined and Crude MDL**: The distinction between crude MDL, which approximates the model cost without considering the exact fit, and refined MDL, which provides a more precise model by considering both the cost of the model and the cost of fitting the model to the data, is elaborated.\n        \n    5.  **MDL for Model Selection**: MDL is particularly highlighted for its utility in model selection, where it serves as a criterion to choose between competing models by evaluating which model provides the best compression of the data.\n        \n    6.  **Statistical and Information Theoretic Underpinnings**: The tutorial introduces the basic concepts of information theory relevant to MDL, such as entropy, mutual information, and the relationship between probability and codelength, primarily through the Kraft Inequality and the Information Inequality.\n        \n    7.  **Applications and Extensions**: The document discusses various applications of MDL in areas like coding, machine learning, and statistical inference, showing how MDL can be a unifying approach in understanding and applying concepts across these domains.\n        \n*   The document serves as a comprehensive introduction to MDL, providing essential insights into both the theoretical and practical aspects of the principle. It emphasizes the importance of MDL in selecting models that are not just good at fitting the data, but also in providing meaningful insights in a parsimonious way .\n\nThis paper provides an extensive introduction and technical exposition on Rissanen’s Minimum Description Length (MDL) Principle. The tutorial is structured to offer both a conceptual and a technically precise exploration of MDL, making the ideas accessible first at a conceptual level and then delving into mathematical specifics.\n\n1.  **MDL and Data Compression**: The MDL Principle is introduced as a method of statistical modeling and inference that views learning and model selection through the lens of data compression. It encapsulates the idea that the best model of a dataset is the one that compresses the data most effectively, balancing model complexity and goodness of fit.\n    \n2.  **Kolmogorov Complexity and MDL**: The tutorial discusses Kolmogorov Complexity as a theoretical foundation of MDL, describing it as the length of the shortest possible description of a string in some fixed universal language.\n    \n3.  **Practical MDL**: This involves approximations of ideal MDL to make it applicable in real-world scenarios, where exact computation of Kolmogorov Complexity is not feasible. Practical implementations often use statistical models and coding schemes that approximate the Kolmogorov Complexity.\n    \n4.  **Refined and Crude MDL**: The distinction between crude MDL, which approximates the model cost without considering the exact fit, and refined MDL, which provides a more precise model by considering both the cost of the model and the cost of fitting the model to the data, is elaborated.\n    \n5.  **MDL for Model Selection**: MDL is particularly highlighted for its utility in model selection, where it serves as a criterion to choose between competing models by evaluating which model provides the best compression of the data.\n    \n6.  **Statistical and Information Theoretic Underpinnings**: The tutorial introduces the basic concepts of information theory relevant to MDL, such as entropy, mutual information, and the relationship between probability and codelength, primarily through the Kraft Inequality and the Information Inequality.\n    \n7.  **Applications and Extensions**: The document discusses various applications of MDL in areas like coding, machine learning, and statistical inference, showing how MDL can be a unifying approach in understanding and applying concepts across these domains.\n    \n\n**MDL and Data Compression**: The MDL Principle is introduced as a method of statistical modeling and inference that views learning and model selection through the lens of data compression. It encapsulates the idea that the best model of a dataset is the one that compresses the data most effectively, balancing model complexity and goodness of fit.\n\n**Kolmogorov Complexity and MDL**: The tutorial discusses Kolmogorov Complexity as a theoretical foundation of MDL, describing it as the length of the shortest possible description of a string in some fixed universal language.\n\n**Practical MDL**: This involves approximations of ideal MDL to make it applicable in real-world scenarios, where exact computation of Kolmogorov Complexity is not feasible. Practical implementations often use statistical models and coding schemes that approximate the Kolmogorov Complexity.\n\n**Refined and Crude MDL**: The distinction between crude MDL, which approximates the model cost without considering the exact fit, and refined MDL, which provides a more precise model by considering both the cost of the model and the cost of fitting the model to the data, is elaborated.\n\n**MDL for Model Selection**: MDL is particularly highlighted for its utility in model selection, where it serves as a criterion to choose between competing models by evaluating which model provides the best compression of the data.\n\n**Statistical and Information Theoretic Underpinnings**: The tutorial introduces the basic concepts of information theory relevant to MDL, such as entropy, mutual information, and the relationship between probability and codelength, primarily through the Kraft Inequality and the Information Inequality.\n\n**Applications and Extensions**: The document discusses various applications of MDL in areas like coding, machine learning, and statistical inference, showing how MDL can be a unifying approach in understanding and applying concepts across these domains.",
      "order": 23,
      "orderInChapter": 23,
      "difficulty": 3,
      "estimatedMinutes": 6,
      "tags": [
        "miscellaneous",
        "machine learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 1021,
        "contentLength": 7841
      },
      "nextCards": [
        "ai-top-30-papers-machine-super-intelligence-24",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25"
      ],
      "relatedCards": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-pipeline-issues-6",
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-model-debugging-data-issues-4",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#a-tutorial-introduction-to-the-minimum-description-length-principle",
      "scrapedAt": "2025-12-28T11:56:36.592Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-machine-super-intelligence-24",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Machine Super Intelligence",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>Shane Legg’s dissertation, “Machine Super Intelligence,” presents an extensive analysis of the challenges and theoretical foundations underlying the development of superintelligent machines. Key technical discussions in the thesis include:</li>\n</ul>\n<ol>\n  <li>\n    <p><strong>Framework for Intelligence Measures:</strong> Legg introduces a formal measure of machine intelligence that encompasses both theoretical and practical aspects. This measure is designed to evaluate the ability of a system to achieve a variety of goals in different environments, which is fundamental to the concept of superintelligence.</p>\n  </li>\n  <li>\n    <p><strong>Superintelligence Pathways:</strong> The dissertation explores various pathways that could potentially lead to superintelligence, including enhancement of human intelligence via biological means, machine learning algorithms, brain-computer interfaces, and self-improving AI systems. Legg evaluates the feasibility of each pathway and their potential impacts on developing a superintelligent system.</p>\n  </li>\n  <li>\n    <p><strong>Algorithmic Insights into Intelligence:</strong> Detailed discussions are provided on the role of algorithms in simulating or replicating human-like intelligence. This includes analyses of existing machine learning techniques and their limitations, and how they might evolve to handle more complex, abstract tasks associated with higher intelligence.</p>\n  </li>\n  <li>\n    <p><strong>Theoretical Models of Machine Learning:</strong> Legg delves into theoretical models that could underpin superintelligent AI, discussing concepts like the Bayesian framework for machine learning, the role of reinforcement learning in decision-making processes, and the potential of recursive self-improvement algorithms that could lead AI to reach or surpass human intelligence levels.</p>\n  </li>\n  <li>\n    <p><strong>Safety and Control:</strong> A significant portion of the thesis is dedicated to the implications of AI superintelligence, particularly the problems of control and safety. Legg discusses strategies to ensure that superintelligent systems operate within human-intended boundaries, which is crucial to prevent undesirable or catastrophic scenarios.</p>\n  </li>\n</ol>\n<p><strong>Framework for Intelligence Measures:</strong> Legg introduces a formal measure of machine intelligence that encompasses both theoretical and practical aspects. This measure is designed to evaluate the ability of a system to achieve a variety of goals in different environments, which is fundamental to the concept of superintelligence.</p>\n<p><strong>Superintelligence Pathways:</strong> The dissertation explores various pathways that could potentially lead to superintelligence, including enhancement of human intelligence via biological means, machine learning algorithms, brain-computer interfaces, and self-improving AI systems. Legg evaluates the feasibility of each pathway and their potential impacts on developing a superintelligent system.</p>\n<p><strong>Algorithmic Insights into Intelligence:</strong> Detailed discussions are provided on the role of algorithms in simulating or replicating human-like intelligence. This includes analyses of existing machine learning techniques and their limitations, and how they might evolve to handle more complex, abstract tasks associated with higher intelligence.</p>\n<p><strong>Theoretical Models of Machine Learning:</strong> Legg delves into theoretical models that could underpin superintelligent AI, discussing concepts like the Bayesian framework for machine learning, the role of reinforcement learning in decision-making processes, and the potential of recursive self-improvement algorithms that could lead AI to reach or surpass human intelligence levels.</p>\n<p><strong>Safety and Control:</strong> A significant portion of the thesis is dedicated to the implications of AI superintelligence, particularly the problems of control and safety. Legg discusses strategies to ensure that superintelligent systems operate within human-intended boundaries, which is crucial to prevent undesirable or catastrophic scenarios.</p>\n<ul>\n  <li>These components of Legg’s dissertation provide a deep theoretical foundation for understanding and advancing toward the development of superintelligent AI systems, while also addressing the critical issues of control and safety in such developments.</li>\n</ul>",
      "contentMarkdown": "*   Shane Legg’s dissertation, “Machine Super Intelligence,” presents an extensive analysis of the challenges and theoretical foundations underlying the development of superintelligent machines. Key technical discussions in the thesis include:\n\n1.  **Framework for Intelligence Measures:** Legg introduces a formal measure of machine intelligence that encompasses both theoretical and practical aspects. This measure is designed to evaluate the ability of a system to achieve a variety of goals in different environments, which is fundamental to the concept of superintelligence.\n    \n2.  **Superintelligence Pathways:** The dissertation explores various pathways that could potentially lead to superintelligence, including enhancement of human intelligence via biological means, machine learning algorithms, brain-computer interfaces, and self-improving AI systems. Legg evaluates the feasibility of each pathway and their potential impacts on developing a superintelligent system.\n    \n3.  **Algorithmic Insights into Intelligence:** Detailed discussions are provided on the role of algorithms in simulating or replicating human-like intelligence. This includes analyses of existing machine learning techniques and their limitations, and how they might evolve to handle more complex, abstract tasks associated with higher intelligence.\n    \n4.  **Theoretical Models of Machine Learning:** Legg delves into theoretical models that could underpin superintelligent AI, discussing concepts like the Bayesian framework for machine learning, the role of reinforcement learning in decision-making processes, and the potential of recursive self-improvement algorithms that could lead AI to reach or surpass human intelligence levels.\n    \n5.  **Safety and Control:** A significant portion of the thesis is dedicated to the implications of AI superintelligence, particularly the problems of control and safety. Legg discusses strategies to ensure that superintelligent systems operate within human-intended boundaries, which is crucial to prevent undesirable or catastrophic scenarios.\n    \n\n**Framework for Intelligence Measures:** Legg introduces a formal measure of machine intelligence that encompasses both theoretical and practical aspects. This measure is designed to evaluate the ability of a system to achieve a variety of goals in different environments, which is fundamental to the concept of superintelligence.\n\n**Superintelligence Pathways:** The dissertation explores various pathways that could potentially lead to superintelligence, including enhancement of human intelligence via biological means, machine learning algorithms, brain-computer interfaces, and self-improving AI systems. Legg evaluates the feasibility of each pathway and their potential impacts on developing a superintelligent system.\n\n**Algorithmic Insights into Intelligence:** Detailed discussions are provided on the role of algorithms in simulating or replicating human-like intelligence. This includes analyses of existing machine learning techniques and their limitations, and how they might evolve to handle more complex, abstract tasks associated with higher intelligence.\n\n**Theoretical Models of Machine Learning:** Legg delves into theoretical models that could underpin superintelligent AI, discussing concepts like the Bayesian framework for machine learning, the role of reinforcement learning in decision-making processes, and the potential of recursive self-improvement algorithms that could lead AI to reach or surpass human intelligence levels.\n\n**Safety and Control:** A significant portion of the thesis is dedicated to the implications of AI superintelligence, particularly the problems of control and safety. Legg discusses strategies to ensure that superintelligent systems operate within human-intended boundaries, which is crucial to prevent undesirable or catastrophic scenarios.\n\n*   These components of Legg’s dissertation provide a deep theoretical foundation for understanding and advancing toward the development of superintelligent AI systems, while also addressing the critical issues of control and safety in such developments.",
      "order": 24,
      "orderInChapter": 24,
      "difficulty": 3,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous",
        "machine learning",
        "reinforcement learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 536,
        "contentLength": 4428
      },
      "nextCards": [
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-top-30-papers-stanfords-cs231n-convolutional-neural-networks-for-26"
      ],
      "relatedCards": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-pipeline-issues-6",
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-model-debugging-data-issues-4",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#machine-super-intelligence",
      "scrapedAt": "2025-12-28T11:56:36.592Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Kolmogorov Complexity and Algorithmic Randomness",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li>\n    <p>The book “Kolmogorov Complexity and Algorithmic Randomness” by A. Shen, V. A. Uspensky, and N. Vereshchagin offers a comprehensive overview of the fundamental concepts of Kolmogorov complexity and algorithmic randomness. Here are the detailed technical insights and frameworks discussed in the book:</p>\n  </li>\n  <li><strong>Definition and Significance</strong>: Kolmogorov complexity is defined as the shortest binary program (in the sense of Turing machine code) that can generate a given string and then halt. The complexity measures the amount of information contained in the string, essentially quantifying its randomness.</li>\n  <li>\n    <p><strong>Unpredictability and Random Sequences</strong>: Algorithmic randomness enhances the understanding of what makes a sequence random. This is crucial for fields like cryptography and theories of computation, where randomness ensures security and efficiency.</p>\n  </li>\n  <li>Theoretical Foundations\n    <ul>\n      <li><strong>Formalisms and Proofs</strong>: The authors delve into formal definitions, providing rigorous proofs to support the theoretical underpinnings of algorithmic information theory.</li>\n      <li><strong>Incompressibility Method</strong>: A significant portion of the book is dedicated to explaining the incompressibility method, which uses Kolmogorov complexity to prove lower bounds on the resources needed for solving computational problems.</li>\n    </ul>\n  </li>\n  <li>Practical Applications\n    <ul>\n      <li><strong>Data Compression</strong>: The principles of Kolmogorov complexity are directly applicable to data compression, where the objective is to encode data in the shortest form possible.</li>\n      <li><strong>Psychological Models</strong>: The book explores how human perceptions of randomness and complexity can be modeled using algorithmic information theory.</li>\n    </ul>\n  </li>\n  <li>Advanced Topics\n    <ul>\n      <li><strong>Mutual Information</strong>: Detailed discussions on mutual information in the context of Kolmogorov complexity, exploring how information can be shared or transferred between different parts of a string or between different strings.</li>\n      <li><strong>Conditional Complexity</strong>: The concept of conditional complexity, or the complexity of one string given another, is thoroughly explained, which helps in understanding the dependencies and relationships in data.</li>\n    </ul>\n  </li>\n  <li>Mathematical Rigor\n    <ul>\n      <li><strong>Deep Mathematical Analysis</strong>: The book is rich with mathematical discussions that provide a deep understanding of the concepts. It includes complex proofs and theoretical explorations that are essential for advanced studies in computer science and mathematics.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Future Directions</strong>: The concluding sections discuss the limitations of current theories and potential areas for further research. The authors speculate on the future applications of algorithmic information theory in emerging technologies and sciences.</p>\n  </li>\n  <li>This book is a valuable resource for researchers, scholars, and students interested in the deep mathematical structures that underlie information theory, computer science, and related disciplines. It not only provides a rigorous introduction to Kolmogorov complexity and algorithmic randomness but also explores their implications in practical and theoretical domains.</li>\n</ul>\n<p>The book “Kolmogorov Complexity and Algorithmic Randomness” by A. Shen, V. A. Uspensky, and N. Vereshchagin offers a comprehensive overview of the fundamental concepts of Kolmogorov complexity and algorithmic randomness. Here are the detailed technical insights and frameworks discussed in the book:</p>\n<p><strong>Unpredictability and Random Sequences</strong>: Algorithmic randomness enhances the understanding of what makes a sequence random. This is crucial for fields like cryptography and theories of computation, where randomness ensures security and efficiency.</p>\n<ul>\n      <li><strong>Formalisms and Proofs</strong>: The authors delve into formal definitions, providing rigorous proofs to support the theoretical underpinnings of algorithmic information theory.</li>\n      <li><strong>Incompressibility Method</strong>: A significant portion of the book is dedicated to explaining the incompressibility method, which uses Kolmogorov complexity to prove lower bounds on the resources needed for solving computational problems.</li>\n    </ul>\n<ul>\n      <li><strong>Data Compression</strong>: The principles of Kolmogorov complexity are directly applicable to data compression, where the objective is to encode data in the shortest form possible.</li>\n      <li><strong>Psychological Models</strong>: The book explores how human perceptions of randomness and complexity can be modeled using algorithmic information theory.</li>\n    </ul>\n<ul>\n      <li><strong>Mutual Information</strong>: Detailed discussions on mutual information in the context of Kolmogorov complexity, exploring how information can be shared or transferred between different parts of a string or between different strings.</li>\n      <li><strong>Conditional Complexity</strong>: The concept of conditional complexity, or the complexity of one string given another, is thoroughly explained, which helps in understanding the dependencies and relationships in data.</li>\n    </ul>\n<ul>\n      <li><strong>Deep Mathematical Analysis</strong>: The book is rich with mathematical discussions that provide a deep understanding of the concepts. It includes complex proofs and theoretical explorations that are essential for advanced studies in computer science and mathematics.</li>\n    </ul>\n<p><strong>Future Directions</strong>: The concluding sections discuss the limitations of current theories and potential areas for further research. The authors speculate on the future applications of algorithmic information theory in emerging technologies and sciences.</p>",
      "contentMarkdown": "*   The book “Kolmogorov Complexity and Algorithmic Randomness” by A. Shen, V. A. Uspensky, and N. Vereshchagin offers a comprehensive overview of the fundamental concepts of Kolmogorov complexity and algorithmic randomness. Here are the detailed technical insights and frameworks discussed in the book:\n    \n*   **Definition and Significance**: Kolmogorov complexity is defined as the shortest binary program (in the sense of Turing machine code) that can generate a given string and then halt. The complexity measures the amount of information contained in the string, essentially quantifying its randomness.\n*   **Unpredictability and Random Sequences**: Algorithmic randomness enhances the understanding of what makes a sequence random. This is crucial for fields like cryptography and theories of computation, where randomness ensures security and efficiency.\n    \n*   Theoretical Foundations\n    *   **Formalisms and Proofs**: The authors delve into formal definitions, providing rigorous proofs to support the theoretical underpinnings of algorithmic information theory.\n    *   **Incompressibility Method**: A significant portion of the book is dedicated to explaining the incompressibility method, which uses Kolmogorov complexity to prove lower bounds on the resources needed for solving computational problems.\n*   Practical Applications\n    *   **Data Compression**: The principles of Kolmogorov complexity are directly applicable to data compression, where the objective is to encode data in the shortest form possible.\n    *   **Psychological Models**: The book explores how human perceptions of randomness and complexity can be modeled using algorithmic information theory.\n*   Advanced Topics\n    *   **Mutual Information**: Detailed discussions on mutual information in the context of Kolmogorov complexity, exploring how information can be shared or transferred between different parts of a string or between different strings.\n    *   **Conditional Complexity**: The concept of conditional complexity, or the complexity of one string given another, is thoroughly explained, which helps in understanding the dependencies and relationships in data.\n*   Mathematical Rigor\n    *   **Deep Mathematical Analysis**: The book is rich with mathematical discussions that provide a deep understanding of the concepts. It includes complex proofs and theoretical explorations that are essential for advanced studies in computer science and mathematics.\n*   **Future Directions**: The concluding sections discuss the limitations of current theories and potential areas for further research. The authors speculate on the future applications of algorithmic information theory in emerging technologies and sciences.\n    \n*   This book is a valuable resource for researchers, scholars, and students interested in the deep mathematical structures that underlie information theory, computer science, and related disciplines. It not only provides a rigorous introduction to Kolmogorov complexity and algorithmic randomness but also explores their implications in practical and theoretical domains.\n\nThe book “Kolmogorov Complexity and Algorithmic Randomness” by A. Shen, V. A. Uspensky, and N. Vereshchagin offers a comprehensive overview of the fundamental concepts of Kolmogorov complexity and algorithmic randomness. Here are the detailed technical insights and frameworks discussed in the book:\n\n**Unpredictability and Random Sequences**: Algorithmic randomness enhances the understanding of what makes a sequence random. This is crucial for fields like cryptography and theories of computation, where randomness ensures security and efficiency.\n\n*   **Formalisms and Proofs**: The authors delve into formal definitions, providing rigorous proofs to support the theoretical underpinnings of algorithmic information theory.\n*   **Incompressibility Method**: A significant portion of the book is dedicated to explaining the incompressibility method, which uses Kolmogorov complexity to prove lower bounds on the resources needed for solving computational problems.\n\n*   **Data Compression**: The principles of Kolmogorov complexity are directly applicable to data compression, where the objective is to encode data in the shortest form possible.\n*   **Psychological Models**: The book explores how human perceptions of randomness and complexity can be modeled using algorithmic information theory.\n\n*   **Mutual Information**: Detailed discussions on mutual information in the context of Kolmogorov complexity, exploring how information can be shared or transferred between different parts of a string or between different strings.\n*   **Conditional Complexity**: The concept of conditional complexity, or the complexity of one string given another, is thoroughly explained, which helps in understanding the dependencies and relationships in data.\n\n*   **Deep Mathematical Analysis**: The book is rich with mathematical discussions that provide a deep understanding of the concepts. It includes complex proofs and theoretical explorations that are essential for advanced studies in computer science and mathematics.\n\n**Future Directions**: The concluding sections discuss the limitations of current theories and potential areas for further research. The authors speculate on the future applications of algorithmic information theory in emerging technologies and sciences.",
      "order": 25,
      "orderInChapter": 25,
      "difficulty": 3,
      "estimatedMinutes": 4,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 719,
        "contentLength": 6011
      },
      "nextCards": [
        "ai-top-30-papers-stanfords-cs231n-convolutional-neural-networks-for-26",
        "ai-top-30-papers-better-faster-large-language-models-via-multi-toke-27"
      ],
      "relatedCards": [
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5",
        "ai-gpu-architecture-interconnects-6"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#kolmogorov-complexity-and-algorithmic-randomness",
      "scrapedAt": "2025-12-28T11:56:36.592Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-stanfords-cs231n-convolutional-neural-networks-for-26",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Ilya Sutskever’s Top 30 Reading List",
      "title": "Stanford’s CS231n Convolutional Neural Networks for Visual Recognition",
      "subtitle": "Ilya Sutskever’s Top 30 Reading List",
      "contentHtml": "<ul>\n  <li><strong>Purpose</strong>: The course introduces students to the fundamental concepts in convolutional neural networks (ConvNets) and their application in image recognition and processing tasks. ConvNets are a category of Neural Networks that have proven very effective in areas such as image recognition and classification.</li>\n  <li>\n    <p><strong>Architectural Advantage</strong>: ConvNets inherently take advantage of the 2D structure of input data, which makes them particularly well-suited for image processing. Unlike regular dense neural networks, ConvNets preserve the spatial hierarchy between pixels to manage the computational complexity involved in processing large images.</p>\n  </li>\n  <li>Core Components of ConvNets\n    <ul>\n      <li><strong>Layers</strong>: The primary layers used in ConvNets include Convolutional Layer, Pooling Layer, and Fully Connected Layer (Dense Layer).\n        <ul>\n          <li><strong>Convolutional Layer</strong>: Applies a convolution operation to the input, passing the result to the next layer. This layer’s parameters consist of a set of learnable filters that are spatially small but extend through the full depth of the input volume.</li>\n          <li><strong>Pooling (Subsampling or Downsampling) Layer</strong>: Commonly used to reduce the spatial dimensions (width and height) of the input volume for the next convolutional layer. It helps to reduce the number of parameters and computation in the network.</li>\n          <li><strong>Fully Connected Layer</strong>: Neurons in a fully connected layer have full connections to all activations in the previous layer. This layer typically computes the class scores, resulting in the volume size of [1x1xN] where N is the number of classes.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li>Training ConvNets\n    <ul>\n      <li><strong>Loss Functions</strong>: Training involves defining a loss function (like cross-entropy loss), which measures how good the network’s predictions are compared to the actual labels.</li>\n      <li><strong>Backpropagation</strong>: Uses the chain rule of calculus to iteratively compute gradients for each weight in the network, effectively training the model by minimizing the loss function using techniques like stochastic gradient descent.</li>\n    </ul>\n  </li>\n  <li>Practical Challenges\n    <ul>\n      <li><strong>Overfitting</strong>: A major challenge when training ConvNets, particularly when the number of parameters is large compared to the number of training samples. Techniques like Dropout, Data Augmentation, and L2 Regularization are used to mitigate this issue.</li>\n      <li><strong>Hyperparameter Tuning</strong>: Includes selecting learning rates, learning rate decay, regularization constants, and more.</li>\n    </ul>\n  </li>\n  <li>Advanced Topics\n    <ul>\n      <li><strong>Batch Normalization</strong>: A technique to improve the training speed and stability of artificial neural networks. It normalizes the inputs for each mini-batch, maintaining the mean output close to 0 and the output standard deviation close to 1.</li>\n      <li><strong>Transfer Learning and Fine-tuning</strong>: Techniques where a network developed for a specific task is reused as the starting point for a model on a second task. Particularly effective when modeling datasets that do not have a large number of labeled training samples.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Architectural Advantage</strong>: ConvNets inherently take advantage of the 2D structure of input data, which makes them particularly well-suited for image processing. Unlike regular dense neural networks, ConvNets preserve the spatial hierarchy between pixels to manage the computational complexity involved in processing large images.</p>\n<ul>\n      <li><strong>Layers</strong>: The primary layers used in ConvNets include Convolutional Layer, Pooling Layer, and Fully Connected Layer (Dense Layer).\n        <ul>\n          <li><strong>Convolutional Layer</strong>: Applies a convolution operation to the input, passing the result to the next layer. This layer’s parameters consist of a set of learnable filters that are spatially small but extend through the full depth of the input volume.</li>\n          <li><strong>Pooling (Subsampling or Downsampling) Layer</strong>: Commonly used to reduce the spatial dimensions (width and height) of the input volume for the next convolutional layer. It helps to reduce the number of parameters and computation in the network.</li>\n          <li><strong>Fully Connected Layer</strong>: Neurons in a fully connected layer have full connections to all activations in the previous layer. This layer typically computes the class scores, resulting in the volume size of [1x1xN] where N is the number of classes.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li><strong>Convolutional Layer</strong>: Applies a convolution operation to the input, passing the result to the next layer. This layer’s parameters consist of a set of learnable filters that are spatially small but extend through the full depth of the input volume.</li>\n          <li><strong>Pooling (Subsampling or Downsampling) Layer</strong>: Commonly used to reduce the spatial dimensions (width and height) of the input volume for the next convolutional layer. It helps to reduce the number of parameters and computation in the network.</li>\n          <li><strong>Fully Connected Layer</strong>: Neurons in a fully connected layer have full connections to all activations in the previous layer. This layer typically computes the class scores, resulting in the volume size of [1x1xN] where N is the number of classes.</li>\n        </ul>\n<ul>\n      <li><strong>Loss Functions</strong>: Training involves defining a loss function (like cross-entropy loss), which measures how good the network’s predictions are compared to the actual labels.</li>\n      <li><strong>Backpropagation</strong>: Uses the chain rule of calculus to iteratively compute gradients for each weight in the network, effectively training the model by minimizing the loss function using techniques like stochastic gradient descent.</li>\n    </ul>\n<ul>\n      <li><strong>Overfitting</strong>: A major challenge when training ConvNets, particularly when the number of parameters is large compared to the number of training samples. Techniques like Dropout, Data Augmentation, and L2 Regularization are used to mitigate this issue.</li>\n      <li><strong>Hyperparameter Tuning</strong>: Includes selecting learning rates, learning rate decay, regularization constants, and more.</li>\n    </ul>\n<ul>\n      <li><strong>Batch Normalization</strong>: A technique to improve the training speed and stability of artificial neural networks. It normalizes the inputs for each mini-batch, maintaining the mean output close to 0 and the output standard deviation close to 1.</li>\n      <li><strong>Transfer Learning and Fine-tuning</strong>: Techniques where a network developed for a specific task is reused as the starting point for a model on a second task. Particularly effective when modeling datasets that do not have a large number of labeled training samples.</li>\n    </ul>",
      "contentMarkdown": "*   **Purpose**: The course introduces students to the fundamental concepts in convolutional neural networks (ConvNets) and their application in image recognition and processing tasks. ConvNets are a category of Neural Networks that have proven very effective in areas such as image recognition and classification.\n*   **Architectural Advantage**: ConvNets inherently take advantage of the 2D structure of input data, which makes them particularly well-suited for image processing. Unlike regular dense neural networks, ConvNets preserve the spatial hierarchy between pixels to manage the computational complexity involved in processing large images.\n    \n*   Core Components of ConvNets\n    *   **Layers**: The primary layers used in ConvNets include Convolutional Layer, Pooling Layer, and Fully Connected Layer (Dense Layer).\n        *   **Convolutional Layer**: Applies a convolution operation to the input, passing the result to the next layer. This layer’s parameters consist of a set of learnable filters that are spatially small but extend through the full depth of the input volume.\n        *   **Pooling (Subsampling or Downsampling) Layer**: Commonly used to reduce the spatial dimensions (width and height) of the input volume for the next convolutional layer. It helps to reduce the number of parameters and computation in the network.\n        *   **Fully Connected Layer**: Neurons in a fully connected layer have full connections to all activations in the previous layer. This layer typically computes the class scores, resulting in the volume size of \\[1x1xN\\] where N is the number of classes.\n*   Training ConvNets\n    *   **Loss Functions**: Training involves defining a loss function (like cross-entropy loss), which measures how good the network’s predictions are compared to the actual labels.\n    *   **Backpropagation**: Uses the chain rule of calculus to iteratively compute gradients for each weight in the network, effectively training the model by minimizing the loss function using techniques like stochastic gradient descent.\n*   Practical Challenges\n    *   **Overfitting**: A major challenge when training ConvNets, particularly when the number of parameters is large compared to the number of training samples. Techniques like Dropout, Data Augmentation, and L2 Regularization are used to mitigate this issue.\n    *   **Hyperparameter Tuning**: Includes selecting learning rates, learning rate decay, regularization constants, and more.\n*   Advanced Topics\n    *   **Batch Normalization**: A technique to improve the training speed and stability of artificial neural networks. It normalizes the inputs for each mini-batch, maintaining the mean output close to 0 and the output standard deviation close to 1.\n    *   **Transfer Learning and Fine-tuning**: Techniques where a network developed for a specific task is reused as the starting point for a model on a second task. Particularly effective when modeling datasets that do not have a large number of labeled training samples.\n\n**Architectural Advantage**: ConvNets inherently take advantage of the 2D structure of input data, which makes them particularly well-suited for image processing. Unlike regular dense neural networks, ConvNets preserve the spatial hierarchy between pixels to manage the computational complexity involved in processing large images.\n\n*   **Layers**: The primary layers used in ConvNets include Convolutional Layer, Pooling Layer, and Fully Connected Layer (Dense Layer).\n    *   **Convolutional Layer**: Applies a convolution operation to the input, passing the result to the next layer. This layer’s parameters consist of a set of learnable filters that are spatially small but extend through the full depth of the input volume.\n    *   **Pooling (Subsampling or Downsampling) Layer**: Commonly used to reduce the spatial dimensions (width and height) of the input volume for the next convolutional layer. It helps to reduce the number of parameters and computation in the network.\n    *   **Fully Connected Layer**: Neurons in a fully connected layer have full connections to all activations in the previous layer. This layer typically computes the class scores, resulting in the volume size of \\[1x1xN\\] where N is the number of classes.\n\n*   **Convolutional Layer**: Applies a convolution operation to the input, passing the result to the next layer. This layer’s parameters consist of a set of learnable filters that are spatially small but extend through the full depth of the input volume.\n*   **Pooling (Subsampling or Downsampling) Layer**: Commonly used to reduce the spatial dimensions (width and height) of the input volume for the next convolutional layer. It helps to reduce the number of parameters and computation in the network.\n*   **Fully Connected Layer**: Neurons in a fully connected layer have full connections to all activations in the previous layer. This layer typically computes the class scores, resulting in the volume size of \\[1x1xN\\] where N is the number of classes.\n\n*   **Loss Functions**: Training involves defining a loss function (like cross-entropy loss), which measures how good the network’s predictions are compared to the actual labels.\n*   **Backpropagation**: Uses the chain rule of calculus to iteratively compute gradients for each weight in the network, effectively training the model by minimizing the loss function using techniques like stochastic gradient descent.\n\n*   **Overfitting**: A major challenge when training ConvNets, particularly when the number of parameters is large compared to the number of training samples. Techniques like Dropout, Data Augmentation, and L2 Regularization are used to mitigate this issue.\n*   **Hyperparameter Tuning**: Includes selecting learning rates, learning rate decay, regularization constants, and more.\n\n*   **Batch Normalization**: A technique to improve the training speed and stability of artificial neural networks. It normalizes the inputs for each mini-batch, maintaining the mean output close to 0 and the output standard deviation close to 1.\n*   **Transfer Learning and Fine-tuning**: Techniques where a network developed for a specific task is reused as the starting point for a model on a second task. Particularly effective when modeling datasets that do not have a large number of labeled training samples.",
      "order": 26,
      "orderInChapter": 26,
      "difficulty": 4,
      "estimatedMinutes": 5,
      "tags": [
        "miscellaneous",
        "neural network",
        "convolution",
        "gradient descent",
        "backpropagation",
        "loss function",
        "activation",
        "regularization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 923,
        "contentLength": 7184
      },
      "nextCards": [
        "ai-top-30-papers-better-faster-large-language-models-via-multi-toke-27",
        "ai-top-30-papers-dense-passage-retrieval-for-open-domain-question-a-28"
      ],
      "relatedCards": [
        "ai-model-debugging-knowledge-distillation-student-teacher-approach-16",
        "ai-gpu-architecture-operator-dispatch-and-kernel-mapping-14",
        "ai-model-debugging-optimizer-13",
        "ai-cnns-for-text-classification-1-d-convolutions-over-text-4",
        "ai-model-debugging-weight-initialization-11"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#stanford’s-cs231n-convolutional-neural-networks-for-visual-recognition",
      "scrapedAt": "2025-12-28T11:56:36.592Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-better-faster-large-language-models-via-multi-toke-27",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Meta",
      "title": "Better & Faster Large Language Models Via Multi-token Prediction",
      "subtitle": "Meta",
      "contentHtml": "<ul>\n  <li>\n    <p>Authors: Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve</p>\n  </li>\n  <li>The recent advancements in large language models (LLMs) have primarily revolved around the next-token prediction methodology. However, a novel approach introduced in the paper titled “Better &amp; Faster Large Language Models via Multi-token Prediction” suggests a significant shift towards predicting multiple tokens simultaneously. This method not only enhances the efficiency and speed of LLMs but also demonstrates considerable improvements in model performance across various tasks, especially in coding benchmarks.</li>\n  <li>The multi-token prediction architecture redefines how LLMs process and generate text by allowing the model to predict several future tokens at once. Unlike traditional architectures that predict the next single token sequentially, this approach utilizes multiple independent output heads that work in parallel, significantly speeding up the training and inference processes.</li>\n  <li>At the core of the multi-token prediction architecture is the shared trunk, a common feature extractor that processes the input data. This trunk is responsible for producing a rich, contextualized representation of the input, which is then fed into multiple output heads. Each head is tasked with predicting a different future token based on the shared representation, ensuring that all predicted tokens are contextually coherent and relevant.</li>\n  <li>The introduction of multi-token prediction architecture has several profound implications. Firstly, it enhances sample efficiency, meaning the model requires fewer data iterations to achieve high performance. Secondly, it significantly speeds up the inference process, as multiple tokens can be generated in parallel, reducing the time needed to produce outputs. This architecture also shows great scalability with increased model size, making it particularly effective for larger models that traditionally face bottlenecks in speed and efficiency.</li>\n  <li>Empirical results from the study highlight the effectiveness of the multi-token prediction model. On coding benchmarks like HumanEval and MBPP, models equipped with this new architecture outperform traditional next-token prediction models by a considerable margin. For instance, models trained with multi-token prediction solve up to 17% more problems on MBPP and demonstrate similar improvements on HumanEval.</li>\n  <li>Moreover, these models are up to three times faster at inference compared to their traditional counterparts. This speed increase is crucial for real-time applications and services that rely on quick responses from LLMs. The architecture’s benefits are also more pronounced as the model size increases, which confirms its suitability for large-scale implementations where efficiency and speed are critical.</li>\n  <li>Thus, the multi-token prediction architecture presents a viable and promising alternative to the conventional methodologies used in training large language models, pushing the boundaries of what is possible in natural language processing and machine learning.</li>\n</ul>\n<p>Authors: Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve</p>\n<h4 id=\"key-takeaways\">Key Takeaways:</h4>\n<ul>\n  <li>🔹 The model consists of a shared trunk and several independent output heads. It processes incoming data to generate a contextualized representation, which is then utilized simultaneously by all output heads for predicting multiple future tokens.</li>\n  <li>🔹 Departing from traditional single-token prediction, this model enables simultaneous prediction of multiple tokens, significantly accelerating both training and inference processes.</li>\n  <li>🔹 The shared trunk, built on transformer technology, extracts a latent representation from the input data. This unified representation is shared across all output heads, ensuring consistent and coherent predictions.</li>\n  <li>🔹 Each output head functions independently to predict a distinct future token. This design reduces the sequential dependencies typical in conventional language models, enhancing the model’s efficiency.</li>\n  <li>🔹 The model’s ability to make multiple predictions concurrently not only speeds up learning but also improves sample efficiency. This results in quicker model convergence and less data required for effective training.</li>\n  <li>🔹 At the inference stage, the model can leverage all output heads simultaneously, leading to swift generation of text sequences. This is particularly advantageous for real-time application scenarios.</li>\n</ul>",
      "contentMarkdown": "*   Authors: Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve\n    \n*   The recent advancements in large language models (LLMs) have primarily revolved around the next-token prediction methodology. However, a novel approach introduced in the paper titled “Better & Faster Large Language Models via Multi-token Prediction” suggests a significant shift towards predicting multiple tokens simultaneously. This method not only enhances the efficiency and speed of LLMs but also demonstrates considerable improvements in model performance across various tasks, especially in coding benchmarks.\n*   The multi-token prediction architecture redefines how LLMs process and generate text by allowing the model to predict several future tokens at once. Unlike traditional architectures that predict the next single token sequentially, this approach utilizes multiple independent output heads that work in parallel, significantly speeding up the training and inference processes.\n*   At the core of the multi-token prediction architecture is the shared trunk, a common feature extractor that processes the input data. This trunk is responsible for producing a rich, contextualized representation of the input, which is then fed into multiple output heads. Each head is tasked with predicting a different future token based on the shared representation, ensuring that all predicted tokens are contextually coherent and relevant.\n*   The introduction of multi-token prediction architecture has several profound implications. Firstly, it enhances sample efficiency, meaning the model requires fewer data iterations to achieve high performance. Secondly, it significantly speeds up the inference process, as multiple tokens can be generated in parallel, reducing the time needed to produce outputs. This architecture also shows great scalability with increased model size, making it particularly effective for larger models that traditionally face bottlenecks in speed and efficiency.\n*   Empirical results from the study highlight the effectiveness of the multi-token prediction model. On coding benchmarks like HumanEval and MBPP, models equipped with this new architecture outperform traditional next-token prediction models by a considerable margin. For instance, models trained with multi-token prediction solve up to 17% more problems on MBPP and demonstrate similar improvements on HumanEval.\n*   Moreover, these models are up to three times faster at inference compared to their traditional counterparts. This speed increase is crucial for real-time applications and services that rely on quick responses from LLMs. The architecture’s benefits are also more pronounced as the model size increases, which confirms its suitability for large-scale implementations where efficiency and speed are critical.\n*   Thus, the multi-token prediction architecture presents a viable and promising alternative to the conventional methodologies used in training large language models, pushing the boundaries of what is possible in natural language processing and machine learning.\n\nAuthors: Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve\n\n#### Key Takeaways:\n\n*   🔹 The model consists of a shared trunk and several independent output heads. It processes incoming data to generate a contextualized representation, which is then utilized simultaneously by all output heads for predicting multiple future tokens.\n*   🔹 Departing from traditional single-token prediction, this model enables simultaneous prediction of multiple tokens, significantly accelerating both training and inference processes.\n*   🔹 The shared trunk, built on transformer technology, extracts a latent representation from the input data. This unified representation is shared across all output heads, ensuring consistent and coherent predictions.\n*   🔹 Each output head functions independently to predict a distinct future token. This design reduces the sequential dependencies typical in conventional language models, enhancing the model’s efficiency.\n*   🔹 The model’s ability to make multiple predictions concurrently not only speeds up learning but also improves sample efficiency. This results in quicker model convergence and less data required for effective training.\n*   🔹 At the inference stage, the model can leverage all output heads simultaneously, leading to swift generation of text sequences. This is particularly advantageous for real-time application scenarios.",
      "order": 27,
      "orderInChapter": 1,
      "difficulty": 4,
      "estimatedMinutes": 4,
      "tags": [
        "miscellaneous",
        "machine learning",
        "transformer",
        "llm"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 627,
        "contentLength": 4681
      },
      "nextCards": [
        "ai-top-30-papers-dense-passage-retrieval-for-open-domain-question-a-28",
        "ai-top-30-papers-retrieval-augmented-generation-for-knowledge-inten-29"
      ],
      "relatedCards": [
        "ai-gpu-architecture-tensor-core-evolution-23",
        "ai-gpu-architecture-per-generation-precision-support-and-performance-26",
        "ai-gpu-architecture-key-takeaways-38",
        "ai-ml-runtimes-llamacpp-deep-dive-38",
        "ai-ml-runtimes-architecture-by-runtime-2"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#better-&-faster-large-language-models-via-multi-token-prediction",
      "scrapedAt": "2025-12-28T11:56:36.592Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-dense-passage-retrieval-for-open-domain-question-a-28",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Meta",
      "title": "Dense Passage Retrieval for Open-Domain Question Answering",
      "subtitle": "Meta",
      "contentHtml": "<ul>\n  <li>Authors: Vladimir Karpukhin, Barlas Oguz,Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih</li>\n  <li>In open-domain question answering (system’s capability to answer questions on any topic rather than being restricted on a specific domain), it’s vital to efficiently identify the right passages from vast information sources (retrieval). Traditional methods, like TF-IDF and BM25, utilize sparse vector models to pick these passages. However, Karpukhin and colleagues in their 2020 EMNLP paper demonstrate a novel approach: using dense vector representations. They employ a dual-encoder framework to generate embeddings from a select set of questions and passages.</li>\n  <li>Their objective is metric learning: crafting a vector space where relevant question-passage pairs are closer together than unrelated ones. They optimize this by focusing on the likelihood of selecting the correct (positive) passage amidst a sea of irrelevant (negative) ones.</li>\n  <li>Collecting negative examples for training from such a vast pool is challenging. Their solution? Utilizing random passages, ones that match the most question tokens without the actual answer (via BM25), and relevant passages paired with other questions. The most effective model they produced uses these “gold” passages from the same training batch as negative instances, combined with one BM25 negative passage.</li>\n  <li>Results were promising. When tested on diverse open-domain QA datasets, their model greatly outperformed the established Lucene-BM25 system, enhancing top-20 passage retrieval accuracy by 9%-19%. This led to their model setting new performance benchmarks in open-domain QA.</li>\n</ul>\n<h4 id=\"dense-passage-retriever-dpr\">Dense Passage Retriever (DPR):</h4>\n<ol>\n  <li><strong>Purpose</strong>: The goal of the DPR is to improve the retrieval component in open-domain QA. This involves efficiently retrieving relevant text passages from a vast collection when given a question.</li>\n  <li><strong>Key Task</strong>: Given a large number <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-282\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-283\"><span class=\"mi\" id=\"MathJax-Span-284\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">M</script> of text passages, the DPR aims to index all of these passages in a low-dimensional continuous space, making it efficient to retrieve the top <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-285\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-286\"><span class=\"mi\" id=\"MathJax-Span-287\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">k</script> most relevant passages for a given input question. <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-288\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-289\"><span class=\"mi\" id=\"MathJax-Span-290\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">M</script> can be very large, like 21 million passages, but <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-291\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-292\"><span class=\"mi\" id=\"MathJax-Span-293\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">k</script> (the number of passages we want to retrieve for a given question) is relatively small, often between 20 and 100.</li>\n  <li><strong>DPR’s Mechanism</strong>:\n    <ul>\n      <li><strong>Dense Encoder for Passages <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-294\" style=\"width: 2.792em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.275em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.397em, 1002.22em, 2.534em, -999.997em); top: -2.218em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-295\"><span class=\"mi\" id=\"MathJax-Span-296\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-297\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-298\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mo\" id=\"MathJax-Span-299\" style=\"font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-300\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.224em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi><mi>P</mi><mo stretchy=\"false\">(</mo><mo>⋅</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">EP(\\cdot)</script></strong>: It converts any text passage to a <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-301\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-302\"><span class=\"mi\" id=\"MathJax-Span-303\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">d</script>-dimensional real-valued vector. This encoder processes and indexes all <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-304\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-305\"><span class=\"mi\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">M</script> passages for retrieval.</li>\n      <li><strong>Encoder for Questions <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi><mi>Q</mi><mo stretchy=&quot;false&quot;>(</mo><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-307\" style=\"width: 2.947em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.43em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.397em, 1002.38em, 2.585em, -999.997em); top: -2.218em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-308\"><span class=\"mi\" id=\"MathJax-Span-309\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-310\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-311\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mo\" id=\"MathJax-Span-312\" style=\"font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-313\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.224em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi><mi>Q</mi><mo stretchy=\"false\">(</mo><mo>⋅</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">EQ(\\cdot)</script></strong>: At runtime, when a question is posed, this encoder turns the question into a <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-314\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-315\"><span class=\"mi\" id=\"MathJax-Span-316\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">d</script>-dimensional vector.</li>\n      <li><strong>Similarity Measurement</strong>: The similarity between a question and a passage is calculated using the dot product of their respective vectors: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi><mi>i</mi><mi>m</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><mi>p</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>E</mi><mi>Q</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x22C5;</mo><mi>E</mi><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>p</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-317\" style=\"width: 12.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1010.52em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-318\"><span class=\"mi\" id=\"MathJax-Span-319\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mi\" id=\"MathJax-Span-320\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-321\" style=\"font-family: STIXGeneral-Italic;\">m</span><span class=\"mo\" id=\"MathJax-Span-322\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-323\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-324\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-325\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">p</span><span class=\"mo\" id=\"MathJax-Span-326\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-328\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-329\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-330\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-331\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-332\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-333\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-334\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-335\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-336\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-337\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-338\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi><mi>i</mi><mi>m</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>E</mi><mi>Q</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi>E</mi><mi>P</mi><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">sim(q, p) = EQ(q) \\cdot EP(p)</script>.</li>\n    </ul>\n  </li>\n  <li><strong>Passage Size and Boundaries</strong>: The passage’s size and the decision of where a passage begins and ends affect the retriever and reader. Fixed-length passages have been found to be more effective in retrieval and QA accuracy.</li>\n  <li><strong>Encoders Implementation</strong>: The encoders for both questions and passages are based on BERT networks, a popular deep learning model for NLP. They use the representation at the [CLS] token as the output, meaning the output vector has 768 dimensions.</li>\n  <li><strong>Inference</strong>: During the process of answering a question, the system uses the passage encoder to process all passages and then indexes them using FAISS, an efficient library for similarity search. For any given question, its embedding is computed, and the top <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-339\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-340\"><span class=\"mi\" id=\"MathJax-Span-341\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">k</script> passages with the closest embeddings are retrieved.</li>\n  <li><strong>Training</strong>:\n    <ul>\n      <li>The main goal during training is to optimize the encoders such that relevant questions and passages have a high similarity (close in vector space) and irrelevant ones have a low similarity.</li>\n      <li>The training data consists of question-passage pairs with both positive (relevant) and negative (irrelevant) passages. The system is trained to increase the similarity for relevant pairs and decrease it for irrelevant ones.</li>\n      <li>For training, they have explicit positive examples (relevant passages) but need to choose negatives from a vast collection. They experimented with different types of negative passages: random, those ranked high by BM25 but not containing the answer, and relevant passages for other questions.</li>\n    </ul>\n  </li>\n  <li><strong>In-batch Negatives</strong>: A training optimization method is discussed where they use relevant passages from the same batch of questions as negatives, which makes computation more efficient. This technique leverages the similarities between passages in the same batch to boost the number of training examples, effectively reusing computation.</li>\n</ol>\n<ul>\n      <li><strong>Dense Encoder for Passages <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-294\" style=\"width: 2.792em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.275em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.397em, 1002.22em, 2.534em, -999.997em); top: -2.218em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-295\"><span class=\"mi\" id=\"MathJax-Span-296\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-297\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-298\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mo\" id=\"MathJax-Span-299\" style=\"font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-300\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.224em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi><mi>P</mi><mo stretchy=\"false\">(</mo><mo>⋅</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">EP(\\cdot)</script></strong>: It converts any text passage to a <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-301\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-302\"><span class=\"mi\" id=\"MathJax-Span-303\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">d</script>-dimensional real-valued vector. This encoder processes and indexes all <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-304\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.94em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-305\"><span class=\"mi\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>M</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">M</script> passages for retrieval.</li>\n      <li><strong>Encoder for Questions <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>E</mi><mi>Q</mi><mo stretchy=&quot;false&quot;>(</mo><mo>&amp;#x22C5;</mo><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-307\" style=\"width: 2.947em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.43em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.397em, 1002.38em, 2.585em, -999.997em); top: -2.218em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-308\"><span class=\"mi\" id=\"MathJax-Span-309\" style=\"font-family: STIXGeneral-Italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-310\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-311\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mo\" id=\"MathJax-Span-312\" style=\"font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mo\" id=\"MathJax-Span-313\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.224em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>E</mi><mi>Q</mi><mo stretchy=\"false\">(</mo><mo>⋅</mo><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">EQ(\\cdot)</script></strong>: At runtime, when a question is posed, this encoder turns the question into a <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-314\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-315\"><span class=\"mi\" id=\"MathJax-Span-316\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-36\">d</script>-dimensional vector.</li>\n      <li><strong>Similarity Measurement</strong>: The similarity between a question and a passage is calculated using the dot product of their respective vectors: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>s</mi><mi>i</mi><mi>m</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo>,</mo><mi>p</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>E</mi><mi>Q</mi><mo stretchy=&quot;false&quot;>(</mo><mi>q</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x22C5;</mo><mi>E</mi><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>p</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-317\" style=\"width: 12.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1010.52em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-318\"><span class=\"mi\" id=\"MathJax-Span-319\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mi\" id=\"MathJax-Span-320\" style=\"font-family: STIXGeneral-Italic;\">i</span><span class=\"mi\" id=\"MathJax-Span-321\" style=\"font-family: STIXGeneral-Italic;\">m</span><span class=\"mo\" id=\"MathJax-Span-322\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-323\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-324\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-325\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">p</span><span class=\"mo\" id=\"MathJax-Span-326\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-328\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-329\" style=\"font-family: STIXGeneral-Italic;\">Q</span><span class=\"mo\" id=\"MathJax-Span-330\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-331\" style=\"font-family: STIXGeneral-Italic;\">q</span><span class=\"mo\" id=\"MathJax-Span-332\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-333\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-334\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-335\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-336\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-337\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-338\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>s</mi><mi>i</mi><mi>m</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><mi>p</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>E</mi><mi>Q</mi><mo stretchy=\"false\">(</mo><mi>q</mi><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi>E</mi><mi>P</mi><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">sim(q, p) = EQ(q) \\cdot EP(p)</script>.</li>\n    </ul>\n<ul>\n      <li>The main goal during training is to optimize the encoders such that relevant questions and passages have a high similarity (close in vector space) and irrelevant ones have a low similarity.</li>\n      <li>The training data consists of question-passage pairs with both positive (relevant) and negative (irrelevant) passages. The system is trained to increase the similarity for relevant pairs and decrease it for irrelevant ones.</li>\n      <li>For training, they have explicit positive examples (relevant passages) but need to choose negatives from a vast collection. They experimented with different types of negative passages: random, those ranked high by BM25 but not containing the answer, and relevant passages for other questions.</li>\n    </ul>",
      "contentMarkdown": "*   Authors: Vladimir Karpukhin, Barlas Oguz,Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih\n*   In open-domain question answering (system’s capability to answer questions on any topic rather than being restricted on a specific domain), it’s vital to efficiently identify the right passages from vast information sources (retrieval). Traditional methods, like TF-IDF and BM25, utilize sparse vector models to pick these passages. However, Karpukhin and colleagues in their 2020 EMNLP paper demonstrate a novel approach: using dense vector representations. They employ a dual-encoder framework to generate embeddings from a select set of questions and passages.\n*   Their objective is metric learning: crafting a vector space where relevant question-passage pairs are closer together than unrelated ones. They optimize this by focusing on the likelihood of selecting the correct (positive) passage amidst a sea of irrelevant (negative) ones.\n*   Collecting negative examples for training from such a vast pool is challenging. Their solution? Utilizing random passages, ones that match the most question tokens without the actual answer (via BM25), and relevant passages paired with other questions. The most effective model they produced uses these “gold” passages from the same training batch as negative instances, combined with one BM25 negative passage.\n*   Results were promising. When tested on diverse open-domain QA datasets, their model greatly outperformed the established Lucene-BM25 system, enhancing top-20 passage retrieval accuracy by 9%-19%. This led to their model setting new performance benchmarks in open-domain QA.\n\n#### Dense Passage Retriever (DPR):\n\n1.  **Purpose**: The goal of the DPR is to improve the retrieval component in open-domain QA. This involves efficiently retrieving relevant text passages from a vast collection when given a question.\n2.  **Key Task**: Given a large number MMM of text passages, the DPR aims to index all of these passages in a low-dimensional continuous space, making it efficient to retrieve the top kkk most relevant passages for a given input question. MMM can be very large, like 21 million passages, but kkk (the number of passages we want to retrieve for a given question) is relatively small, often between 20 and 100.\n3.  **DPR’s Mechanism**:\n    *   **Dense Encoder for Passages EP(⋅)EP(⋅)EP(\\\\cdot)**: It converts any text passage to a ddd\\-dimensional real-valued vector. This encoder processes and indexes all MMM passages for retrieval.\n    *   **Encoder for Questions EQ(⋅)EQ(⋅)EQ(\\\\cdot)**: At runtime, when a question is posed, this encoder turns the question into a ddd\\-dimensional vector.\n    *   **Similarity Measurement**: The similarity between a question and a passage is calculated using the dot product of their respective vectors: sim(q,p)\\=EQ(q)⋅EP(p)sim(q,p)\\=EQ(q)⋅EP(p)sim(q, p) = EQ(q) \\\\cdot EP(p).\n4.  **Passage Size and Boundaries**: The passage’s size and the decision of where a passage begins and ends affect the retriever and reader. Fixed-length passages have been found to be more effective in retrieval and QA accuracy.\n5.  **Encoders Implementation**: The encoders for both questions and passages are based on BERT networks, a popular deep learning model for NLP. They use the representation at the \\[CLS\\] token as the output, meaning the output vector has 768 dimensions.\n6.  **Inference**: During the process of answering a question, the system uses the passage encoder to process all passages and then indexes them using FAISS, an efficient library for similarity search. For any given question, its embedding is computed, and the top kkk passages with the closest embeddings are retrieved.\n7.  **Training**:\n    *   The main goal during training is to optimize the encoders such that relevant questions and passages have a high similarity (close in vector space) and irrelevant ones have a low similarity.\n    *   The training data consists of question-passage pairs with both positive (relevant) and negative (irrelevant) passages. The system is trained to increase the similarity for relevant pairs and decrease it for irrelevant ones.\n    *   For training, they have explicit positive examples (relevant passages) but need to choose negatives from a vast collection. They experimented with different types of negative passages: random, those ranked high by BM25 but not containing the answer, and relevant passages for other questions.\n8.  **In-batch Negatives**: A training optimization method is discussed where they use relevant passages from the same batch of questions as negatives, which makes computation more efficient. This technique leverages the similarities between passages in the same batch to boost the number of training examples, effectively reusing computation.\n\n*   **Dense Encoder for Passages EP(⋅)EP(⋅)EP(\\\\cdot)**: It converts any text passage to a ddd\\-dimensional real-valued vector. This encoder processes and indexes all MMM passages for retrieval.\n*   **Encoder for Questions EQ(⋅)EQ(⋅)EQ(\\\\cdot)**: At runtime, when a question is posed, this encoder turns the question into a ddd\\-dimensional vector.\n*   **Similarity Measurement**: The similarity between a question and a passage is calculated using the dot product of their respective vectors: sim(q,p)\\=EQ(q)⋅EP(p)sim(q,p)\\=EQ(q)⋅EP(p)sim(q, p) = EQ(q) \\\\cdot EP(p).\n\n*   The main goal during training is to optimize the encoders such that relevant questions and passages have a high similarity (close in vector space) and irrelevant ones have a low similarity.\n*   The training data consists of question-passage pairs with both positive (relevant) and negative (irrelevant) passages. The system is trained to increase the similarity for relevant pairs and decrease it for irrelevant ones.\n*   For training, they have explicit positive examples (relevant passages) but need to choose negatives from a vast collection. They experimented with different types of negative passages: random, those ranked high by BM25 but not containing the answer, and relevant passages for other questions.",
      "order": 28,
      "orderInChapter": 2,
      "difficulty": 4,
      "estimatedMinutes": 5,
      "tags": [
        "miscellaneous",
        "deep learning",
        "embedding",
        "bert",
        "nlp",
        "optimization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 893,
        "contentLength": 35251
      },
      "nextCards": [
        "ai-top-30-papers-retrieval-augmented-generation-for-knowledge-inten-29",
        "ai-top-30-papers-zephyr-direct-distillation-of-lm-alignment-30"
      ],
      "relatedCards": [
        "ai-ann-similarity-search-tabular-comparison-13",
        "ai-ann-similarity-search-quantization-based-methods-5",
        "ai-ann-similarity-search-graph-based-methods-7",
        "ai-ann-similarity-search-annoy-approximate-nearest-neighbors-oh-yeah-12",
        "ai-ml-runtimes-comparative-analysis-52"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#dense-passage-retrieval-for-open-domain-question-answering",
      "scrapedAt": "2025-12-28T11:56:36.592Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-retrieval-augmented-generation-for-knowledge-inten-29",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Meta",
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
      "subtitle": "Meta",
      "contentHtml": "<ul>\n  <li>The paper by Lewis et al. from Facebook AI Research, University College London, and New York University, introduces Retrieval-Augmented Generation (RAG) models combining pre-trained parametric and non-parametric memory for language generation tasks.</li>\n  <li>Addressing limitations of large pre-trained language models, such as difficulty in accessing and precisely manipulating knowledge, RAG models merge a pre-trained sequence-to-sequence (seq2seq) model with a dense vector index of Wikipedia, accessed by a neural retriever.</li>\n  <li>The RAG framework encompasses two models: RAG-Sequence, using the same retrieved document for the entire sequence, and RAG-Token, allowing different passages for each token.</li>\n  <li>The retrieval component, Dense Passage Retriever (DPR), uses a bi-encoder architecture with BERT-based document and query encoders. The generator component utilizes BART-large, a pre-trained seq2seq transformer with 400M parameters.</li>\n  <li>RAG models were trained jointly on the retriever and generator components without direct supervision on which documents to retrieve, using stochastic gradient descent with Adam. The training used a Wikipedia dump as the non-parametric knowledge source, split into 21M 100-word chunks.</li>\n  <li>In open-domain QA tasks, RAG established new state-of-the-art results, outperforming both parametric seq2seq models and task-specific retrieve-and-extract architectures. RAG models showed the ability to generate correct answers even when the right answer wasn’t in any retrieved document.</li>\n  <li>RAG-Sequence surpassed BART in Open MS-MARCO NLG, indicating less hallucination and more factually correct text generation. RAG-Token outperformed RAG-Sequence in Jeopardy question generation, demonstrating higher factuality and specificity.</li>\n  <li>On the FEVER fact verification task, RAG models achieved results close to state-of-the-art models that require more complex architectures and intermediate retrieval supervision.</li>\n  <li>This study showcases the effectiveness of hybrid generation models, combining parametric and non-parametric memories, offering new directions in combining these components for a range of NLP tasks.</li>\n</ul>",
      "contentMarkdown": "*   The paper by Lewis et al. from Facebook AI Research, University College London, and New York University, introduces Retrieval-Augmented Generation (RAG) models combining pre-trained parametric and non-parametric memory for language generation tasks.\n*   Addressing limitations of large pre-trained language models, such as difficulty in accessing and precisely manipulating knowledge, RAG models merge a pre-trained sequence-to-sequence (seq2seq) model with a dense vector index of Wikipedia, accessed by a neural retriever.\n*   The RAG framework encompasses two models: RAG-Sequence, using the same retrieved document for the entire sequence, and RAG-Token, allowing different passages for each token.\n*   The retrieval component, Dense Passage Retriever (DPR), uses a bi-encoder architecture with BERT-based document and query encoders. The generator component utilizes BART-large, a pre-trained seq2seq transformer with 400M parameters.\n*   RAG models were trained jointly on the retriever and generator components without direct supervision on which documents to retrieve, using stochastic gradient descent with Adam. The training used a Wikipedia dump as the non-parametric knowledge source, split into 21M 100-word chunks.\n*   In open-domain QA tasks, RAG established new state-of-the-art results, outperforming both parametric seq2seq models and task-specific retrieve-and-extract architectures. RAG models showed the ability to generate correct answers even when the right answer wasn’t in any retrieved document.\n*   RAG-Sequence surpassed BART in Open MS-MARCO NLG, indicating less hallucination and more factually correct text generation. RAG-Token outperformed RAG-Sequence in Jeopardy question generation, demonstrating higher factuality and specificity.\n*   On the FEVER fact verification task, RAG models achieved results close to state-of-the-art models that require more complex architectures and intermediate retrieval supervision.\n*   This study showcases the effectiveness of hybrid generation models, combining parametric and non-parametric memories, offering new directions in combining these components for a range of NLP tasks.",
      "order": 29,
      "orderInChapter": 3,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "transformer",
        "bert",
        "nlp",
        "gradient descent"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 286,
        "contentLength": 2229
      },
      "nextCards": [
        "ai-top-30-papers-zephyr-direct-distillation-of-lm-alignment-30",
        "ai-top-30-papers-lost-in-the-middle-how-language-models-use-long-co-31"
      ],
      "relatedCards": [
        "ai-ml-runtimes-suitable-applications-44",
        "ai-ml-runtimes-comparative-analysis-52",
        "ai-gpu-architecture-performance-analysis-27",
        "ai-gpu-architecture-cache-hierarchy-changes-29",
        "ai-gpu-architecture-memory-latency-and-efficiency-improvements-30"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks",
      "scrapedAt": "2025-12-28T11:56:36.592Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-zephyr-direct-distillation-of-lm-alignment-30",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "HuggingFace",
      "title": "Zephyr: Direct Distillation of LM Alignment",
      "subtitle": "HuggingFace",
      "contentHtml": "<ul>\n  <li>Authors: Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf</li>\n  <li>The paper introduces a technique termed “distilled direct preference optimization” (dDPO), designed to align a small language model (LM) to user intent via distillation, eliminating the need for human feedback. Furthermore, the study presents a 7B parameter language model named Zephyr, which is specifically tailored to align with user intent. Their approach has 3 main steps:\n    <ol>\n      <li>Distilled Supervised Fine-Tuning (dSFT): They first fine-tune the base 7B Mistral model using the UltraChat dataset, which contains 1.4M dialogues generated by having a large proprietary teacher model like GPT-3.5 Turbo converse with itself. This provides a strong initialization for the student model.</li>\n      <li>AI Feedback (AIF) Collection: An ensemble of diverse open chat models (e.g. Claude, Falcon) are used to generate responses to prompts from the UltraFeedback dataset. These responses are then scored by a powerful teacher model like GPT-4. The top scoring response is taken as the “chosen” response and one random lower scoring response as the “rejected” response. This provides training pairs of good vs bad responses.</li>\n      <li>Distilled Direct Preference Optimization (dDPO): The dSFT model is further optimized by training it to rank the “chosen” responses higher than “rejected” responses from the AIF collection step. This is done by directly optimizing a preference likelihood objective on the static AIF data without needing to sample from the model during training.</li>\n    </ol>\n  </li>\n  <li>They apply this approach to train Zephyr-7B, starting from Mistral-7B. First dSFT using UltraChat (1.4M examples from GPT-3.5), then AIF from UltraFeedback (64K prompts ranked by GPT-4), then dDPO.</li>\n  <li>Results:\n    <ul>\n      <li>Zephyr-7B sets a new SOTA for 7B models on MT-Bench (7.34 score) and AlpacaEval (90.6% win rate), surpassing prior best dSFT and PPO distillation methods.</li>\n      <li>It matches performance of 70B RLHF models like LLaMA2 on MT-Bench.</li>\n      <li>Ablations show dSFT is necessary before dDPO, and overfitting dDPO can still improve performance.</li>\n    </ul>\n  </li>\n  <li>The key technical innovation is direct distillation of preferences without human involvement, through dSFT then dDPO, achieving strong alignment for small 7B models.</li>\n  <li>The resulting 7B Zephyr model sets a new SOTA for alignment and conversational ability compared to other 7B models. It even outperforms the 70B LLaMA2 model on the MT-Bench benchmark.</li>\n  <li>Key advantages are that it requires no human labeling or feedback, scales easily to larger models, and can be trained in just a few hours on commercially available hardware. Limitations are potential biases inherited from the teacher models and lack of safety considerations. Overall, it demonstrates the surprising efficacy of distillation and preference learning for aligning smaller open models.</li>\n  <li>The image below <a href=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\">(source)</a> gives a graphical sense of Zephyr’s performance on tasks as compared with our LLMs.</li>\n</ul>\n<ol>\n      <li>Distilled Supervised Fine-Tuning (dSFT): They first fine-tune the base 7B Mistral model using the UltraChat dataset, which contains 1.4M dialogues generated by having a large proprietary teacher model like GPT-3.5 Turbo converse with itself. This provides a strong initialization for the student model.</li>\n      <li>AI Feedback (AIF) Collection: An ensemble of diverse open chat models (e.g. Claude, Falcon) are used to generate responses to prompts from the UltraFeedback dataset. These responses are then scored by a powerful teacher model like GPT-4. The top scoring response is taken as the “chosen” response and one random lower scoring response as the “rejected” response. This provides training pairs of good vs bad responses.</li>\n      <li>Distilled Direct Preference Optimization (dDPO): The dSFT model is further optimized by training it to rank the “chosen” responses higher than “rejected” responses from the AIF collection step. This is done by directly optimizing a preference likelihood objective on the static AIF data without needing to sample from the model during training.</li>\n    </ol>\n<ul>\n      <li>Zephyr-7B sets a new SOTA for 7B models on MT-Bench (7.34 score) and AlpacaEval (90.6% win rate), surpassing prior best dSFT and PPO distillation methods.</li>\n      <li>It matches performance of 70B RLHF models like LLaMA2 on MT-Bench.</li>\n      <li>Ablations show dSFT is necessary before dDPO, and overfitting dDPO can still improve performance.</li>\n    </ul>\n<p><img src=\"assets/paper/1.png\" alt=\"\"></p>",
      "contentMarkdown": "*   Authors: Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf\n*   The paper introduces a technique termed “distilled direct preference optimization” (dDPO), designed to align a small language model (LM) to user intent via distillation, eliminating the need for human feedback. Furthermore, the study presents a 7B parameter language model named Zephyr, which is specifically tailored to align with user intent. Their approach has 3 main steps:\n    1.  Distilled Supervised Fine-Tuning (dSFT): They first fine-tune the base 7B Mistral model using the UltraChat dataset, which contains 1.4M dialogues generated by having a large proprietary teacher model like GPT-3.5 Turbo converse with itself. This provides a strong initialization for the student model.\n    2.  AI Feedback (AIF) Collection: An ensemble of diverse open chat models (e.g. Claude, Falcon) are used to generate responses to prompts from the UltraFeedback dataset. These responses are then scored by a powerful teacher model like GPT-4. The top scoring response is taken as the “chosen” response and one random lower scoring response as the “rejected” response. This provides training pairs of good vs bad responses.\n    3.  Distilled Direct Preference Optimization (dDPO): The dSFT model is further optimized by training it to rank the “chosen” responses higher than “rejected” responses from the AIF collection step. This is done by directly optimizing a preference likelihood objective on the static AIF data without needing to sample from the model during training.\n*   They apply this approach to train Zephyr-7B, starting from Mistral-7B. First dSFT using UltraChat (1.4M examples from GPT-3.5), then AIF from UltraFeedback (64K prompts ranked by GPT-4), then dDPO.\n*   Results:\n    *   Zephyr-7B sets a new SOTA for 7B models on MT-Bench (7.34 score) and AlpacaEval (90.6% win rate), surpassing prior best dSFT and PPO distillation methods.\n    *   It matches performance of 70B RLHF models like LLaMA2 on MT-Bench.\n    *   Ablations show dSFT is necessary before dDPO, and overfitting dDPO can still improve performance.\n*   The key technical innovation is direct distillation of preferences without human involvement, through dSFT then dDPO, achieving strong alignment for small 7B models.\n*   The resulting 7B Zephyr model sets a new SOTA for alignment and conversational ability compared to other 7B models. It even outperforms the 70B LLaMA2 model on the MT-Bench benchmark.\n*   Key advantages are that it requires no human labeling or feedback, scales easily to larger models, and can be trained in just a few hours on commercially available hardware. Limitations are potential biases inherited from the teacher models and lack of safety considerations. Overall, it demonstrates the surprising efficacy of distillation and preference learning for aligning smaller open models.\n*   The image below [(source)](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) gives a graphical sense of Zephyr’s performance on tasks as compared with our LLMs.\n\n1.  Distilled Supervised Fine-Tuning (dSFT): They first fine-tune the base 7B Mistral model using the UltraChat dataset, which contains 1.4M dialogues generated by having a large proprietary teacher model like GPT-3.5 Turbo converse with itself. This provides a strong initialization for the student model.\n2.  AI Feedback (AIF) Collection: An ensemble of diverse open chat models (e.g. Claude, Falcon) are used to generate responses to prompts from the UltraFeedback dataset. These responses are then scored by a powerful teacher model like GPT-4. The top scoring response is taken as the “chosen” response and one random lower scoring response as the “rejected” response. This provides training pairs of good vs bad responses.\n3.  Distilled Direct Preference Optimization (dDPO): The dSFT model is further optimized by training it to rank the “chosen” responses higher than “rejected” responses from the AIF collection step. This is done by directly optimizing a preference likelihood objective on the static AIF data without needing to sample from the model during training.\n\n*   Zephyr-7B sets a new SOTA for 7B models on MT-Bench (7.34 score) and AlpacaEval (90.6% win rate), surpassing prior best dSFT and PPO distillation methods.\n*   It matches performance of 70B RLHF models like LLaMA2 on MT-Bench.\n*   Ablations show dSFT is necessary before dDPO, and overfitting dDPO can still improve performance.\n\n![](assets/paper/1.png)",
      "order": 30,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 4,
      "tags": [
        "miscellaneous",
        "bert",
        "gpt",
        "llm",
        "optimization",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 691,
        "contentLength": 4920
      },
      "nextCards": [
        "ai-top-30-papers-lost-in-the-middle-how-language-models-use-long-co-31",
        "ai-top-30-papers-precise-zero-shot-dense-retrieval-without-relevanc-32"
      ],
      "relatedCards": [
        "ai-ml-runtimes-overview-15",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-39",
        "ai-ml-runtimes-implementation-details-41",
        "ai-gpu-architecture-key-architectural-design-goals-7"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#zephyr:-direct-distillation-of-lm-alignment",
      "scrapedAt": "2025-12-28T11:56:36.592Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-lost-in-the-middle-how-language-models-use-long-co-31",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Stanford",
      "title": "Lost in the Middle: How Language Models Use Long Contexts",
      "subtitle": "Stanford",
      "contentHtml": "<ul>\n  <li>This paper by Liu et al. from Stanford University, University of California Berkeley, and Samaya AI, focuses on analyzing language models’ performance in tasks that require identifying relevant information in long input contexts. The research particularly highlights issues in multi-document question answering and key-value retrieval tasks, revealing a significant degradation in performance when relevant information is situated in the middle of lengthy contexts.</li>\n  <li>The study involved an experimental setup for multi-document question answering. Models were tasked with identifying relevant information from a set of documents to answer questions. The researchers manipulated both the length of the input context and the position of the relevant information to observe changes in task performance.</li>\n  <li>Several state-of-the-art open and closed language models were evaluated. Among the open models were MPT-30B-Instruct, capable of handling up to 8192 tokens, and LongChat-13B (16K), which extends the context window to 16384 tokens. Closed models included GPT-3.5-Turbo and its variant with an expanded context length of 16K tokens, as well as Claude-1.3 and Claude-1.3 (100K).</li>\n  <li>The results revealed a distinct U-shaped performance curve across these models. They performed best when relevant information appeared at the beginning or end of the input context. However, the performance significantly declined when accessing information in the middle of long contexts, challenging the efficacy of extended-context models in utilizing their input effectively.</li>\n  <li>A synthetic key-value retrieval task was also used to assess models’ ability to retrieve exact matches from an input context. The task’s simplicity varied across models, with some achieving near-perfect performance, while others struggled with larger contexts.</li>\n  <li>The study also explored the impact of model architecture on context usage, comparing decoder-only and encoder-decoder models. Encoder-decoder models like Flan-T5-XXL and Flan-UL2 exhibited more stable performance across various contexts. However, they also began to show performance degradation with sequences longer than their training-time context windows.</li>\n  <li>The impact of query-aware contextualization was examined. While this dramatically improved performance in the key-value retrieval task, it had only a minimal effect on the multi-document question answering task.</li>\n  <li>Instruction fine-tuning’s effect was analyzed by comparing models like MPT-30B and MPT-30B-Instruct, both fine-tuned for instructions. Both models showed similar U-shaped performance curves, indicating that instruction fine-tuning alone is not responsible for these trends.</li>\n  <li>In a case study on open-domain question answering, the research found that model performance does not always improve with an increase in the amount of context provided. The study observed that performance saturates before retriever recall, suggesting that providing too much context may not be beneficial and could potentially reduce accuracy.</li>\n</ul>",
      "contentMarkdown": "*   This paper by Liu et al. from Stanford University, University of California Berkeley, and Samaya AI, focuses on analyzing language models’ performance in tasks that require identifying relevant information in long input contexts. The research particularly highlights issues in multi-document question answering and key-value retrieval tasks, revealing a significant degradation in performance when relevant information is situated in the middle of lengthy contexts.\n*   The study involved an experimental setup for multi-document question answering. Models were tasked with identifying relevant information from a set of documents to answer questions. The researchers manipulated both the length of the input context and the position of the relevant information to observe changes in task performance.\n*   Several state-of-the-art open and closed language models were evaluated. Among the open models were MPT-30B-Instruct, capable of handling up to 8192 tokens, and LongChat-13B (16K), which extends the context window to 16384 tokens. Closed models included GPT-3.5-Turbo and its variant with an expanded context length of 16K tokens, as well as Claude-1.3 and Claude-1.3 (100K).\n*   The results revealed a distinct U-shaped performance curve across these models. They performed best when relevant information appeared at the beginning or end of the input context. However, the performance significantly declined when accessing information in the middle of long contexts, challenging the efficacy of extended-context models in utilizing their input effectively.\n*   A synthetic key-value retrieval task was also used to assess models’ ability to retrieve exact matches from an input context. The task’s simplicity varied across models, with some achieving near-perfect performance, while others struggled with larger contexts.\n*   The study also explored the impact of model architecture on context usage, comparing decoder-only and encoder-decoder models. Encoder-decoder models like Flan-T5-XXL and Flan-UL2 exhibited more stable performance across various contexts. However, they also began to show performance degradation with sequences longer than their training-time context windows.\n*   The impact of query-aware contextualization was examined. While this dramatically improved performance in the key-value retrieval task, it had only a minimal effect on the multi-document question answering task.\n*   Instruction fine-tuning’s effect was analyzed by comparing models like MPT-30B and MPT-30B-Instruct, both fine-tuned for instructions. Both models showed similar U-shaped performance curves, indicating that instruction fine-tuning alone is not responsible for these trends.\n*   In a case study on open-domain question answering, the research found that model performance does not always improve with an increase in the amount of context provided. The study observed that performance saturates before retriever recall, suggesting that providing too much context may not be beneficial and could potentially reduce accuracy.",
      "order": 31,
      "orderInChapter": 1,
      "difficulty": 4,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous",
        "gpt",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 424,
        "contentLength": 3111
      },
      "nextCards": [
        "ai-top-30-papers-precise-zero-shot-dense-retrieval-without-relevanc-32",
        "ai-top-30-papers-alcuna-large-language-models-meet-new-knowledge-33"
      ],
      "relatedCards": [
        "ai-ml-runtimes-overview-15",
        "ai-ml-runtimes-overview-39",
        "ai-gpu-architecture-synchronization-and-communication-12",
        "ai-gpu-architecture-memory-management-and-transfer-15",
        "ai-gpu-architecture-inference-deployment-18"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#lost-in-the-middle:-how-language-models-use-long-contexts",
      "scrapedAt": "2025-12-28T11:56:36.592Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-precise-zero-shot-dense-retrieval-without-relevanc-32",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Misc",
      "title": "Precise Zero-Shot Dense Retrieval Without Relevance Labels",
      "subtitle": "Misc",
      "contentHtml": "<ul>\n  <li>The paper by Gao, Ma, Lin, and Callan from Carnegie Mellon University and University of Waterloo introduces Hypothetical Document Embeddings (HyDE), a novel approach for fully zero-shot dense retrieval in the absence of relevance labels. HyDE utilizes instruction-following language models (like InstructGPT) to generate a hypothetical document capturing relevance patterns, although these documents may contain inaccuracies or fictional details.</li>\n  <li>Dense retrieval has been effective across various tasks and languages but creating an effective fully zero-shot dense retrieval system without relevance labels remains challenging. Traditional methods like negative mining, distillation, and task-specific pre-training have been proposed to enhance supervised dense retrieval models, yet zero-shot dense retrieval still presents difficulties.</li>\n  <li>HyDE’s methodology involves two main steps: generating a hypothetical document that answers the query, and then encoding this document into an embedding vector using an unsupervised contrastively learned encoder like Contriever. This process pivots away from traditional dense retrieval’s reliance on relevance judgments, instead utilizing a language model’s ability to generate relevant content.</li>\n  <li>Experiments conducted with HyDE used InstructGPT and Contriever models, along with datasets such as TREC DL19, DL20 (based on MS-MARCO), and a collection from the BEIR dataset for web search, question answering, fact verification, and non-English retrieval tasks. The results showed that HyDE outperforms the state-of-the-art unsupervised dense retriever Contriever and is comparable to fine-tuned retrievers across these tasks and languages.</li>\n  <li>The paper concludes by reflecting on HyDE’s novel approach to relevance modeling, which shifts from traditional numerical relevance scores to leveraging natural language generation models. This paradigm suggests a future where the need for relevance labels might be eliminated, and relevance modeling and instruction understanding can be delegated to more powerful and flexible language models. HyDE is practical in the initial stages of a search system’s life, providing performance comparable to fine-tuned models without reliance on relevance labels.</li>\n</ul>",
      "contentMarkdown": "*   The paper by Gao, Ma, Lin, and Callan from Carnegie Mellon University and University of Waterloo introduces Hypothetical Document Embeddings (HyDE), a novel approach for fully zero-shot dense retrieval in the absence of relevance labels. HyDE utilizes instruction-following language models (like InstructGPT) to generate a hypothetical document capturing relevance patterns, although these documents may contain inaccuracies or fictional details.\n*   Dense retrieval has been effective across various tasks and languages but creating an effective fully zero-shot dense retrieval system without relevance labels remains challenging. Traditional methods like negative mining, distillation, and task-specific pre-training have been proposed to enhance supervised dense retrieval models, yet zero-shot dense retrieval still presents difficulties.\n*   HyDE’s methodology involves two main steps: generating a hypothetical document that answers the query, and then encoding this document into an embedding vector using an unsupervised contrastively learned encoder like Contriever. This process pivots away from traditional dense retrieval’s reliance on relevance judgments, instead utilizing a language model’s ability to generate relevant content.\n*   Experiments conducted with HyDE used InstructGPT and Contriever models, along with datasets such as TREC DL19, DL20 (based on MS-MARCO), and a collection from the BEIR dataset for web search, question answering, fact verification, and non-English retrieval tasks. The results showed that HyDE outperforms the state-of-the-art unsupervised dense retriever Contriever and is comparable to fine-tuned retrievers across these tasks and languages.\n*   The paper concludes by reflecting on HyDE’s novel approach to relevance modeling, which shifts from traditional numerical relevance scores to leveraging natural language generation models. This paradigm suggests a future where the need for relevance labels might be eliminated, and relevance modeling and instruction understanding can be delegated to more powerful and flexible language models. HyDE is practical in the initial stages of a search system’s life, providing performance comparable to fine-tuned models without reliance on relevance labels.",
      "order": 32,
      "orderInChapter": 1,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "embedding",
        "gpt"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 305,
        "contentLength": 2298
      },
      "nextCards": [
        "ai-top-30-papers-alcuna-large-language-models-meet-new-knowledge-33",
        "ai-top-30-papers-the-perils-promises-of-fact-checking-with-large-la-34"
      ],
      "relatedCards": [
        "ai-ml-runtimes-architecture-27",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-choosing-the-right-ann-algorithm-family-9",
        "ai-ann-similarity-search-graph-based-methods-7",
        "ai-ann-similarity-search-annoy-approximate-nearest-neighbors-oh-yeah-12"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#precise-zero-shot-dense-retrieval-without-relevance-labels",
      "scrapedAt": "2025-12-28T11:56:36.592Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-alcuna-large-language-models-meet-new-knowledge-33",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Misc",
      "title": "ALCUNA: Large Language Models Meet New Knowledge",
      "subtitle": "Misc",
      "contentHtml": "<ul>\n  <li>Authors: Xunjian Yin, Baizhou Huang, and Xiaojun Wan</li>\n  <li>The paper proposes a new method called KnowGen to generate artificial entities with new knowledge by making changes to the attributes and relationships of existing entities. This simulates the natural process of new knowledge emerging in the real world.</li>\n  <li>KnowGen is applied to structured biological taxonomic data from the EOL database to create artificial organisms. This results in a benchmark dataset called ALCUNA for evaluating large language models (LLMs) on their ability to handle new knowledge.</li>\n  <li>ALCUNA contains questions testing the model’s knowledge understanding, differentiation, and association abilities when faced with new entities.</li>\n  <li>Several popular LLMs like ChatGPT, Alpaca, Vicuna, and ChatGLM are evaluated on ALCUNA in zero-shot and few-shot settings. The results show these models still struggle with reasoning between new and existing knowledge.</li>\n  <li>Analysis reveals factors impacting model performance on new knowledge like entity similarity, contextual knowledge, and input representation format.</li>\n  <li>The paper argues benchmarks with truly new knowledge like ALCUNA are important to drive progress in LLMs’ ability to understand and reason with new information, as opposed to existing knowledge already seen during training.</li>\n  <li>The artificial nature of the knowledge in ALCUNA makes it reusable as a standard benchmark to assess different models on new knowledge without having to collect new data repeatedly.</li>\n  <li>This paper proposes a novel method to automatically generate new structured knowledge for evaluating LLMs’ capabilities in more realistic and challenging settings involving unfamiliar information. The ALCUNA benchmark constructed using this approach provides insights into current model limitations and opportunities for improvement.</li>\n</ul>",
      "contentMarkdown": "*   Authors: Xunjian Yin, Baizhou Huang, and Xiaojun Wan\n*   The paper proposes a new method called KnowGen to generate artificial entities with new knowledge by making changes to the attributes and relationships of existing entities. This simulates the natural process of new knowledge emerging in the real world.\n*   KnowGen is applied to structured biological taxonomic data from the EOL database to create artificial organisms. This results in a benchmark dataset called ALCUNA for evaluating large language models (LLMs) on their ability to handle new knowledge.\n*   ALCUNA contains questions testing the model’s knowledge understanding, differentiation, and association abilities when faced with new entities.\n*   Several popular LLMs like ChatGPT, Alpaca, Vicuna, and ChatGLM are evaluated on ALCUNA in zero-shot and few-shot settings. The results show these models still struggle with reasoning between new and existing knowledge.\n*   Analysis reveals factors impacting model performance on new knowledge like entity similarity, contextual knowledge, and input representation format.\n*   The paper argues benchmarks with truly new knowledge like ALCUNA are important to drive progress in LLMs’ ability to understand and reason with new information, as opposed to existing knowledge already seen during training.\n*   The artificial nature of the knowledge in ALCUNA makes it reusable as a standard benchmark to assess different models on new knowledge without having to collect new data repeatedly.\n*   This paper proposes a novel method to automatically generate new structured knowledge for evaluating LLMs’ capabilities in more realistic and challenging settings involving unfamiliar information. The ALCUNA benchmark constructed using this approach provides insights into current model limitations and opportunities for improvement.",
      "order": 33,
      "orderInChapter": 2,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "gpt",
        "llm"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 265,
        "contentLength": 1917
      },
      "nextCards": [
        "ai-top-30-papers-the-perils-promises-of-fact-checking-with-large-la-34"
      ],
      "relatedCards": [
        "ai-ml-runtimes-overview-39",
        "ai-ml-runtimes-example-cli-inference-43",
        "ai-ml-runtimes-runtime-selection-guidance-54",
        "ai-ml-runtimes-comparative-analysis-60",
        "ai-gpu-architecture-tensor-core-evolution-23"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#alcuna:-large-language-models-meet-new-knowledge",
      "scrapedAt": "2025-12-28T11:56:36.592Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-top-30-papers-the-perils-promises-of-fact-checking-with-large-la-34",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Ilya Sutskever’s Top 30",
      "articleSlug": "top-30-papers",
      "chapter": "Misc",
      "title": "The Perils & Promises of Fact-checking with Large Language Models",
      "subtitle": "Misc",
      "contentHtml": "<ul>\n  <li>Authors: Dorian Quelle &amp; Alexandre Bovet</li>\n  <li>The paper evaluates using large language models (LLMs) like GPT-3.5 and GPT-4 for automated fact-checking of claims. This is important as LLMs are being used more in high stakes domains like research and journalism.</li>\n  <li>They test the models on two datasets: PolitFact (US political claims) and a multilingual dataset from Data Commons. The models are evaluated with and without providing contextual information from web searches.</li>\n  <li>\n    <p>Motivation: Fact-checking is important to combat misinformation, but manual fact-checking has limited capacity. Large language models (LLMs) like GPT-3.5 and GPT-4 are increasingly used for writing and information gathering, so understanding their fact-checking abilities is critical.</p>\n  </li>\n  <li>\n    <p>Methods: Evaluated GPT-3.5 and GPT-4 on fact-checking claims from PolitiFact and a multilingual dataset. Tested models with and without retrieving context from Google. Compared performance across languages.</p>\n  </li>\n  <li>Key Results:\n    <ul>\n      <li>GPT-4 outperformed GPT-3.5 overall.</li>\n      <li>Providing context significantly improved accuracy, highlighting the importance of evidence gathering.</li>\n      <li>Models struggled with ambiguous “half-true” type verdicts.</li>\n      <li>Performance varied across languages - non-English claims saw a boost when translated to English first.</li>\n      <li>No sharp drop in accuracy after GPT-3.5/4 training cutoff dates, suggesting continued learning from human feedback.</li>\n    </ul>\n  </li>\n  <li>Limitations:\n    <ul>\n      <li>Biased evaluation due to use of GPT-4 as a scorer.</li>\n      <li>Did not explore model scaling or curating better training data.</li>\n      <li>Safety/ethics of potential misinformation not addressed.</li>\n    </ul>\n  </li>\n  <li>Implications:\n    <ul>\n      <li>LLMs show promise for assisting human fact-checkers but cannot fully automate the process yet.</li>\n      <li>Critical examination of LLM reasoning is important before deployment.</li>\n      <li>Understanding model limitations and language-specific differences is key.</li>\n      <li>Continued learning after initial training needs more investigation.</li>\n    </ul>\n  </li>\n  <li>The paper provides a comprehensive evaluation of GPT-3.5 and GPT-4 on fact-checking, using novel context retrieval and multilingual data. Key findings highlight the models’ strengths as well as areas needing improvement before responsible LLM-assisted fact-checking.</li>\n</ul>\n<p>Motivation: Fact-checking is important to combat misinformation, but manual fact-checking has limited capacity. Large language models (LLMs) like GPT-3.5 and GPT-4 are increasingly used for writing and information gathering, so understanding their fact-checking abilities is critical.</p>\n<p>Methods: Evaluated GPT-3.5 and GPT-4 on fact-checking claims from PolitiFact and a multilingual dataset. Tested models with and without retrieving context from Google. Compared performance across languages.</p>\n<ul>\n      <li>GPT-4 outperformed GPT-3.5 overall.</li>\n      <li>Providing context significantly improved accuracy, highlighting the importance of evidence gathering.</li>\n      <li>Models struggled with ambiguous “half-true” type verdicts.</li>\n      <li>Performance varied across languages - non-English claims saw a boost when translated to English first.</li>\n      <li>No sharp drop in accuracy after GPT-3.5/4 training cutoff dates, suggesting continued learning from human feedback.</li>\n    </ul>\n<ul>\n      <li>Biased evaluation due to use of GPT-4 as a scorer.</li>\n      <li>Did not explore model scaling or curating better training data.</li>\n      <li>Safety/ethics of potential misinformation not addressed.</li>\n    </ul>\n<ul>\n      <li>LLMs show promise for assisting human fact-checkers but cannot fully automate the process yet.</li>\n      <li>Critical examination of LLM reasoning is important before deployment.</li>\n      <li>Understanding model limitations and language-specific differences is key.</li>\n      <li>Continued learning after initial training needs more investigation.</li>\n    </ul>",
      "contentMarkdown": "*   Authors: Dorian Quelle & Alexandre Bovet\n*   The paper evaluates using large language models (LLMs) like GPT-3.5 and GPT-4 for automated fact-checking of claims. This is important as LLMs are being used more in high stakes domains like research and journalism.\n*   They test the models on two datasets: PolitFact (US political claims) and a multilingual dataset from Data Commons. The models are evaluated with and without providing contextual information from web searches.\n*   Motivation: Fact-checking is important to combat misinformation, but manual fact-checking has limited capacity. Large language models (LLMs) like GPT-3.5 and GPT-4 are increasingly used for writing and information gathering, so understanding their fact-checking abilities is critical.\n    \n*   Methods: Evaluated GPT-3.5 and GPT-4 on fact-checking claims from PolitiFact and a multilingual dataset. Tested models with and without retrieving context from Google. Compared performance across languages.\n    \n*   Key Results:\n    *   GPT-4 outperformed GPT-3.5 overall.\n    *   Providing context significantly improved accuracy, highlighting the importance of evidence gathering.\n    *   Models struggled with ambiguous “half-true” type verdicts.\n    *   Performance varied across languages - non-English claims saw a boost when translated to English first.\n    *   No sharp drop in accuracy after GPT-3.5/4 training cutoff dates, suggesting continued learning from human feedback.\n*   Limitations:\n    *   Biased evaluation due to use of GPT-4 as a scorer.\n    *   Did not explore model scaling or curating better training data.\n    *   Safety/ethics of potential misinformation not addressed.\n*   Implications:\n    *   LLMs show promise for assisting human fact-checkers but cannot fully automate the process yet.\n    *   Critical examination of LLM reasoning is important before deployment.\n    *   Understanding model limitations and language-specific differences is key.\n    *   Continued learning after initial training needs more investigation.\n*   The paper provides a comprehensive evaluation of GPT-3.5 and GPT-4 on fact-checking, using novel context retrieval and multilingual data. Key findings highlight the models’ strengths as well as areas needing improvement before responsible LLM-assisted fact-checking.\n\nMotivation: Fact-checking is important to combat misinformation, but manual fact-checking has limited capacity. Large language models (LLMs) like GPT-3.5 and GPT-4 are increasingly used for writing and information gathering, so understanding their fact-checking abilities is critical.\n\nMethods: Evaluated GPT-3.5 and GPT-4 on fact-checking claims from PolitiFact and a multilingual dataset. Tested models with and without retrieving context from Google. Compared performance across languages.\n\n*   GPT-4 outperformed GPT-3.5 overall.\n*   Providing context significantly improved accuracy, highlighting the importance of evidence gathering.\n*   Models struggled with ambiguous “half-true” type verdicts.\n*   Performance varied across languages - non-English claims saw a boost when translated to English first.\n*   No sharp drop in accuracy after GPT-3.5/4 training cutoff dates, suggesting continued learning from human feedback.\n\n*   Biased evaluation due to use of GPT-4 as a scorer.\n*   Did not explore model scaling or curating better training data.\n*   Safety/ethics of potential misinformation not addressed.\n\n*   LLMs show promise for assisting human fact-checkers but cannot fully automate the process yet.\n*   Critical examination of LLM reasoning is important before deployment.\n*   Understanding model limitations and language-specific differences is key.\n*   Continued learning after initial training needs more investigation.",
      "order": 34,
      "orderInChapter": 3,
      "difficulty": 4,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous",
        "gpt",
        "llm"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 505,
        "contentLength": 4176
      },
      "nextCards": [],
      "relatedCards": [
        "ai-ml-runtimes-overview-39",
        "ai-ml-runtimes-example-cli-inference-43",
        "ai-ml-runtimes-runtime-selection-guidance-54",
        "ai-ml-runtimes-comparative-analysis-60",
        "ai-gpu-architecture-tensor-core-evolution-23"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/top-30-papers/#the-perils-&-promises-of-fact-checking-with-large-language-models",
      "scrapedAt": "2025-12-28T11:56:36.592Z",
      "siblings": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-the-unreasonable-effectiveness-of-recurrent-neural-2",
        "ai-top-30-papers-understanding-lstm-networks-3",
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-streaming-multiprocessors-sms-1",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Fundamental Architectural Components",
      "title": "Streaming Multiprocessors (SMs)",
      "subtitle": "Fundamental Architectural Components",
      "contentHtml": "<ul>\n  <li>\n    <p>The SM is the fundamental execution unit in an NVIDIA GPU. Each SM contains:</p>\n\n    <ul>\n      <li>Multiple CUDA cores (integer and floating-point ALUs)</li>\n      <li>Special Function Units (SFUs) for transcendental math</li>\n      <li>Tensor Cores for matrix-heavy operations (deep learning, HPC)</li>\n      <li>Load/store units for memory access</li>\n      <li>Warp schedulers for issuing instructions to warps</li>\n    </ul>\n  </li>\n</ul>\n<p>The SM is the fundamental execution unit in an NVIDIA GPU. Each SM contains:</p>\n<ul>\n      <li>Multiple CUDA cores (integer and floating-point ALUs)</li>\n      <li>Special Function Units (SFUs) for transcendental math</li>\n      <li>Tensor Cores for matrix-heavy operations (deep learning, HPC)</li>\n      <li>Load/store units for memory access</li>\n      <li>Warp schedulers for issuing instructions to warps</li>\n    </ul>",
      "contentMarkdown": "*   The SM is the fundamental execution unit in an NVIDIA GPU. Each SM contains:\n    \n    *   Multiple CUDA cores (integer and floating-point ALUs)\n    *   Special Function Units (SFUs) for transcendental math\n    *   Tensor Cores for matrix-heavy operations (deep learning, HPC)\n    *   Load/store units for memory access\n    *   Warp schedulers for issuing instructions to warps\n\nThe SM is the fundamental execution unit in an NVIDIA GPU. Each SM contains:\n\n*   Multiple CUDA cores (integer and floating-point ALUs)\n*   Special Function Units (SFUs) for transcendental math\n*   Tensor Cores for matrix-heavy operations (deep learning, HPC)\n*   Load/store units for memory access\n*   Warp schedulers for issuing instructions to warps",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "deep learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 107,
        "contentLength": 892
      },
      "nextCards": [
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "relatedCards": [
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-ml-runtimes-overview-3",
        "ai-top-30-papers-deep-residual-learning-for-image-recognition-10",
        "ai-ann-similarity-search-scann-scalable-nearest-neighbors-11",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#streaming-multiprocessors-(sms)",
      "scrapedAt": "2025-12-28T11:56:41.454Z",
      "siblings": [
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5",
        "ai-gpu-architecture-interconnects-6"
      ]
    },
    {
      "id": "ai-gpu-architecture-cuda-cores-2",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Fundamental Architectural Components",
      "title": "CUDA Cores",
      "subtitle": "Fundamental Architectural Components",
      "contentHtml": "<ul>\n  <li>Handle general-purpose floating-point and integer operations.</li>\n  <li>Organized into warps of 32 threads.</li>\n  <li>Designed for SIMD-like execution, where all threads in a warp execute the same instruction.</li>\n</ul>",
      "contentMarkdown": "*   Handle general-purpose floating-point and integer operations.\n*   Organized into warps of 32 threads.\n*   Designed for SIMD-like execution, where all threads in a warp execute the same instruction.",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 29,
        "contentLength": 233
      },
      "nextCards": [
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#cuda-cores",
      "scrapedAt": "2025-12-28T11:56:41.454Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5",
        "ai-gpu-architecture-interconnects-6"
      ]
    },
    {
      "id": "ai-gpu-architecture-tensor-cores-3",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Fundamental Architectural Components",
      "title": "Tensor Cores",
      "subtitle": "Fundamental Architectural Components",
      "contentHtml": "<ul>\n  <li>Specialized hardware for matrix multiply-and-accumulate (MMA) operations.</li>\n  <li>Key for AI/ML workloads (e.g., FP16, BF16, INT8 precision).</li>\n  <li>Introduced in Volta, improved in each subsequent architecture.</li>\n</ul>",
      "contentMarkdown": "*   Specialized hardware for matrix multiply-and-accumulate (MMA) operations.\n*   Key for AI/ML workloads (e.g., FP16, BF16, INT8 precision).\n*   Introduced in Volta, improved in each subsequent architecture.",
      "order": 3,
      "orderInChapter": 3,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 27,
        "contentLength": 240
      },
      "nextCards": [
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#tensor-cores",
      "scrapedAt": "2025-12-28T11:56:41.454Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5",
        "ai-gpu-architecture-interconnects-6"
      ]
    },
    {
      "id": "ai-gpu-architecture-ray-tracing-rt-cores-4",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Fundamental Architectural Components",
      "title": "Ray Tracing (RT) Cores",
      "subtitle": "Fundamental Architectural Components",
      "contentHtml": "<ul>\n  <li>Accelerate BVH traversal and ray-triangle intersection tests for real-time ray tracing.</li>\n  <li>First introduced in Turing architecture.</li>\n</ul>",
      "contentMarkdown": "*   Accelerate BVH traversal and ray-triangle intersection tests for real-time ray tracing.\n*   First introduced in Turing architecture.",
      "order": 4,
      "orderInChapter": 4,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 18,
        "contentLength": 161
      },
      "nextCards": [
        "ai-gpu-architecture-memory-hierarchy-5",
        "ai-gpu-architecture-interconnects-6"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#ray-tracing-(rt)-cores",
      "scrapedAt": "2025-12-28T11:56:41.454Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-memory-hierarchy-5",
        "ai-gpu-architecture-interconnects-6"
      ]
    },
    {
      "id": "ai-gpu-architecture-memory-hierarchy-5",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Fundamental Architectural Components",
      "title": "Memory Hierarchy",
      "subtitle": "Fundamental Architectural Components",
      "contentHtml": "<ul>\n  <li>\n    <p>NVIDIA GPUs employ a multi-tier memory system:</p>\n\n    <ul>\n      <li><strong>Registers</strong> (per thread, lowest latency)</li>\n      <li><strong>Shared Memory / L1 Cache</strong> (per SM, low latency, user-controlled or cache mode)</li>\n      <li><strong>L2 Cache</strong> (shared across all SMs, higher capacity)</li>\n      <li><strong>Global Memory (VRAM)</strong> (off-chip, high latency, GDDR6 or HBM)</li>\n      <li><strong>Texture and constant caches</strong> (specialized caching units)</li>\n    </ul>\n  </li>\n</ul>\n<p>NVIDIA GPUs employ a multi-tier memory system:</p>\n<ul>\n      <li><strong>Registers</strong> (per thread, lowest latency)</li>\n      <li><strong>Shared Memory / L1 Cache</strong> (per SM, low latency, user-controlled or cache mode)</li>\n      <li><strong>L2 Cache</strong> (shared across all SMs, higher capacity)</li>\n      <li><strong>Global Memory (VRAM)</strong> (off-chip, high latency, GDDR6 or HBM)</li>\n      <li><strong>Texture and constant caches</strong> (specialized caching units)</li>\n    </ul>",
      "contentMarkdown": "*   NVIDIA GPUs employ a multi-tier memory system:\n    \n    *   **Registers** (per thread, lowest latency)\n    *   **Shared Memory / L1 Cache** (per SM, low latency, user-controlled or cache mode)\n    *   **L2 Cache** (shared across all SMs, higher capacity)\n    *   **Global Memory (VRAM)** (off-chip, high latency, GDDR6 or HBM)\n    *   **Texture and constant caches** (specialized caching units)\n\nNVIDIA GPUs employ a multi-tier memory system:\n\n*   **Registers** (per thread, lowest latency)\n*   **Shared Memory / L1 Cache** (per SM, low latency, user-controlled or cache mode)\n*   **L2 Cache** (shared across all SMs, higher capacity)\n*   **Global Memory (VRAM)** (off-chip, high latency, GDDR6 or HBM)\n*   **Texture and constant caches** (specialized caching units)",
      "order": 5,
      "orderInChapter": 5,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 109,
        "contentLength": 1058
      },
      "nextCards": [
        "ai-gpu-architecture-interconnects-6",
        "ai-gpu-architecture-key-architectural-design-goals-7"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#memory-hierarchy",
      "scrapedAt": "2025-12-28T11:56:41.454Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-interconnects-6"
      ]
    },
    {
      "id": "ai-gpu-architecture-interconnects-6",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Fundamental Architectural Components",
      "title": "Interconnects",
      "subtitle": "Fundamental Architectural Components",
      "contentHtml": "<ul>\n  <li><strong>NVLink</strong>: High-bandwidth, low-latency GPU-to-GPU and GPU-to-CPU interconnect.</li>\n  <li><strong>PCIe</strong>: Standard system interconnect, slower than NVLink.</li>\n</ul>",
      "contentMarkdown": "*   **NVLink**: High-bandwidth, low-latency GPU-to-GPU and GPU-to-CPU interconnect.\n*   **PCIe**: Standard system interconnect, slower than NVLink.",
      "order": 6,
      "orderInChapter": 6,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 16,
        "contentLength": 198
      },
      "nextCards": [
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-thread-and-warp-model-8"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#interconnects",
      "scrapedAt": "2025-12-28T11:56:41.454Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-key-architectural-design-goals-7",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Fundamental Architectural Components",
      "title": "Key Architectural Design Goals",
      "subtitle": "Fundamental Architectural Components",
      "contentHtml": "<ol>\n  <li><strong>High Parallelism</strong> – Scaling SM count and CUDA cores.</li>\n  <li><strong>Specialization</strong> – Adding RT and Tensor Cores for dedicated workloads.</li>\n  <li><strong>Memory Bandwidth</strong> – Using faster VRAM and wider buses.</li>\n  <li><strong>Energy Efficiency</strong> – Improved performance per watt through better fabrication processes and architectural optimizations.</li>\n  <li><strong>Scalability</strong> – Supporting multi-GPU setups with coherent memory models.</li>\n</ol>",
      "contentMarkdown": "1.  **High Parallelism** – Scaling SM count and CUDA cores.\n2.  **Specialization** – Adding RT and Tensor Cores for dedicated workloads.\n3.  **Memory Bandwidth** – Using faster VRAM and wider buses.\n4.  **Energy Efficiency** – Improved performance per watt through better fabrication processes and architectural optimizations.\n5.  **Scalability** – Supporting multi-GPU setups with coherent memory models.",
      "order": 7,
      "orderInChapter": 7,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "optimization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 56,
        "contentLength": 516
      },
      "nextCards": [
        "ai-gpu-architecture-thread-and-warp-model-8",
        "ai-gpu-architecture-thread-blocks-and-grids-9"
      ],
      "relatedCards": [
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-pros-and-cons-6",
        "ai-ml-runtimes-pros-and-cons-23",
        "ai-ml-runtimes-implementation-details-28"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#key-architectural-design-goals",
      "scrapedAt": "2025-12-28T11:56:41.454Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-thread-and-warp-model-8",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Execution Paradigms in NVIDIA GPUs",
      "title": "Thread and Warp Model",
      "subtitle": "Execution Paradigms in NVIDIA GPUs",
      "contentHtml": "<ul>\n  <li><strong>Thread</strong>:\n    <ul>\n      <li>The smallest unit of execution. Each thread has its own registers, program counter, and can execute independently, but in practice, threads are grouped for efficiency.</li>\n    </ul>\n  </li>\n  <li><strong>Warp</strong>:\n    <ul>\n      <li>A warp is a group of <strong>32 threads</strong> that execute the same instruction in lockstep on a single SM.\n        <ul>\n          <li>If threads within a warp diverge (take different branches), execution is serialized for each path until they reconverge (warp divergence penalty).</li>\n          <li>Warps are the main scheduling unit in NVIDIA GPUs.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>The smallest unit of execution. Each thread has its own registers, program counter, and can execute independently, but in practice, threads are grouped for efficiency.</li>\n    </ul>\n<ul>\n      <li>A warp is a group of <strong>32 threads</strong> that execute the same instruction in lockstep on a single SM.\n        <ul>\n          <li>If threads within a warp diverge (take different branches), execution is serialized for each path until they reconverge (warp divergence penalty).</li>\n          <li>Warps are the main scheduling unit in NVIDIA GPUs.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>If threads within a warp diverge (take different branches), execution is serialized for each path until they reconverge (warp divergence penalty).</li>\n          <li>Warps are the main scheduling unit in NVIDIA GPUs.</li>\n        </ul>",
      "contentMarkdown": "*   **Thread**:\n    *   The smallest unit of execution. Each thread has its own registers, program counter, and can execute independently, but in practice, threads are grouped for efficiency.\n*   **Warp**:\n    *   A warp is a group of **32 threads** that execute the same instruction in lockstep on a single SM.\n        *   If threads within a warp diverge (take different branches), execution is serialized for each path until they reconverge (warp divergence penalty).\n        *   Warps are the main scheduling unit in NVIDIA GPUs.\n\n*   The smallest unit of execution. Each thread has its own registers, program counter, and can execute independently, but in practice, threads are grouped for efficiency.\n\n*   A warp is a group of **32 threads** that execute the same instruction in lockstep on a single SM.\n    *   If threads within a warp diverge (take different branches), execution is serialized for each path until they reconverge (warp divergence penalty).\n    *   Warps are the main scheduling unit in NVIDIA GPUs.\n\n*   If threads within a warp diverge (take different branches), execution is serialized for each path until they reconverge (warp divergence penalty).\n*   Warps are the main scheduling unit in NVIDIA GPUs.",
      "order": 8,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 192,
        "contentLength": 1567
      },
      "nextCards": [
        "ai-gpu-architecture-thread-blocks-and-grids-9",
        "ai-gpu-architecture-simt-execution-model-10"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#thread-and-warp-model",
      "scrapedAt": "2025-12-28T11:56:41.454Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-thread-blocks-and-grids-9",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Execution Paradigms in NVIDIA GPUs",
      "title": "Thread Blocks and Grids",
      "subtitle": "Execution Paradigms in NVIDIA GPUs",
      "contentHtml": "<ul>\n  <li><strong>Thread Block (also known as Cooperative Thread Array)</strong>:\n    <ul>\n      <li>A thread block, also called a Cooperative Thread Array (CTA), is a group of threads (typically up to 1024) organized in 1D, 2D, or 3D layouts that work together, sharing data via shared memory and synchronizing their execution.\n        <ul>\n          <li>Threads in a block share <strong>SM-local resources</strong> such as shared memory and L1 cache.</li>\n          <li>A block is always executed on a single SM (cannot span multiple SMs).</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n  <li><strong>Grid</strong>:\n    <ul>\n      <li>A grid is a set of thread blocks launched for a kernel execution.\n        <ul>\n          <li>Can have 1D, 2D, or 3D configuration.</li>\n          <li>Enables scaling up to millions of threads.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>A thread block, also called a Cooperative Thread Array (CTA), is a group of threads (typically up to 1024) organized in 1D, 2D, or 3D layouts that work together, sharing data via shared memory and synchronizing their execution.\n        <ul>\n          <li>Threads in a block share <strong>SM-local resources</strong> such as shared memory and L1 cache.</li>\n          <li>A block is always executed on a single SM (cannot span multiple SMs).</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Threads in a block share <strong>SM-local resources</strong> such as shared memory and L1 cache.</li>\n          <li>A block is always executed on a single SM (cannot span multiple SMs).</li>\n        </ul>\n<ul>\n      <li>A grid is a set of thread blocks launched for a kernel execution.\n        <ul>\n          <li>Can have 1D, 2D, or 3D configuration.</li>\n          <li>Enables scaling up to millions of threads.</li>\n        </ul>\n      </li>\n    </ul>\n<ul>\n          <li>Can have 1D, 2D, or 3D configuration.</li>\n          <li>Enables scaling up to millions of threads.</li>\n        </ul>",
      "contentMarkdown": "*   **Thread Block (also known as Cooperative Thread Array)**:\n    *   A thread block, also called a Cooperative Thread Array (CTA), is a group of threads (typically up to 1024) organized in 1D, 2D, or 3D layouts that work together, sharing data via shared memory and synchronizing their execution.\n        *   Threads in a block share **SM-local resources** such as shared memory and L1 cache.\n        *   A block is always executed on a single SM (cannot span multiple SMs).\n*   **Grid**:\n    *   A grid is a set of thread blocks launched for a kernel execution.\n        *   Can have 1D, 2D, or 3D configuration.\n        *   Enables scaling up to millions of threads.\n\n*   A thread block, also called a Cooperative Thread Array (CTA), is a group of threads (typically up to 1024) organized in 1D, 2D, or 3D layouts that work together, sharing data via shared memory and synchronizing their execution.\n    *   Threads in a block share **SM-local resources** such as shared memory and L1 cache.\n    *   A block is always executed on a single SM (cannot span multiple SMs).\n\n*   Threads in a block share **SM-local resources** such as shared memory and L1 cache.\n*   A block is always executed on a single SM (cannot span multiple SMs).\n\n*   A grid is a set of thread blocks launched for a kernel execution.\n    *   Can have 1D, 2D, or 3D configuration.\n    *   Enables scaling up to millions of threads.\n\n*   Can have 1D, 2D, or 3D configuration.\n*   Enables scaling up to millions of threads.",
      "order": 9,
      "orderInChapter": 2,
      "difficulty": 2,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 252,
        "contentLength": 1995
      },
      "nextCards": [
        "ai-gpu-architecture-simt-execution-model-10",
        "ai-gpu-architecture-warp-scheduling-11"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#thread-blocks-and-grids",
      "scrapedAt": "2025-12-28T11:56:41.454Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-simt-execution-model-10",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Execution Paradigms in NVIDIA GPUs",
      "title": "SIMT Execution Model",
      "subtitle": "Execution Paradigms in NVIDIA GPUs",
      "contentHtml": "<ul>\n  <li>SIMT is similar to SIMD but with more flexibility—each thread has its own instruction state, yet warps execute in a vector-like fashion.</li>\n  <li>Hardware warp schedulers select ready warps each cycle to hide memory latency.</li>\n  <li>The SIMT model allows massive parallelism while tolerating high memory access latencies.</li>\n</ul>",
      "contentMarkdown": "*   SIMT is similar to SIMD but with more flexibility—each thread has its own instruction state, yet warps execute in a vector-like fashion.\n*   Hardware warp schedulers select ready warps each cycle to hide memory latency.\n*   The SIMT model allows massive parallelism while tolerating high memory access latencies.",
      "order": 10,
      "orderInChapter": 3,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 49,
        "contentLength": 348
      },
      "nextCards": [
        "ai-gpu-architecture-warp-scheduling-11",
        "ai-gpu-architecture-synchronization-and-communication-12"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#simt-execution-model",
      "scrapedAt": "2025-12-28T11:56:41.454Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-warp-scheduling-11",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Execution Paradigms in NVIDIA GPUs",
      "title": "Warp Scheduling",
      "subtitle": "Execution Paradigms in NVIDIA GPUs",
      "contentHtml": "<ul>\n  <li>Each SM contains <strong>warp schedulers</strong> (number varies by architecture).</li>\n  <li>The scheduler chooses one or more ready warps per cycle, issuing instructions to execution units.</li>\n  <li>\n    <p>Techniques to maximize utilization:</p>\n\n    <ul>\n      <li><strong>Latency hiding</strong>: Switching to another ready warp when one stalls.</li>\n      <li><strong>Dual-issue</strong>: Issuing two independent instructions in the same cycle.</li>\n      <li><strong>Specialized pipelines</strong>: Routing instructions to CUDA cores, Tensor Cores, or SFUs.</li>\n    </ul>\n  </li>\n</ul>\n<p>Techniques to maximize utilization:</p>\n<ul>\n      <li><strong>Latency hiding</strong>: Switching to another ready warp when one stalls.</li>\n      <li><strong>Dual-issue</strong>: Issuing two independent instructions in the same cycle.</li>\n      <li><strong>Specialized pipelines</strong>: Routing instructions to CUDA cores, Tensor Cores, or SFUs.</li>\n    </ul>",
      "contentMarkdown": "*   Each SM contains **warp schedulers** (number varies by architecture).\n*   The scheduler chooses one or more ready warps per cycle, issuing instructions to execution units.\n*   Techniques to maximize utilization:\n    \n    *   **Latency hiding**: Switching to another ready warp when one stalls.\n    *   **Dual-issue**: Issuing two independent instructions in the same cycle.\n    *   **Specialized pipelines**: Routing instructions to CUDA cores, Tensor Cores, or SFUs.\n\nTechniques to maximize utilization:\n\n*   **Latency hiding**: Switching to another ready warp when one stalls.\n*   **Dual-issue**: Issuing two independent instructions in the same cycle.\n*   **Specialized pipelines**: Routing instructions to CUDA cores, Tensor Cores, or SFUs.",
      "order": 11,
      "orderInChapter": 4,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 101,
        "contentLength": 975
      },
      "nextCards": [
        "ai-gpu-architecture-synchronization-and-communication-12",
        "ai-gpu-architecture-model-definition-and-training-cpugpu-13"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#warp-scheduling",
      "scrapedAt": "2025-12-28T11:56:41.454Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-synchronization-and-communication-12",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Execution Paradigms in NVIDIA GPUs",
      "title": "Synchronization and Communication",
      "subtitle": "Execution Paradigms in NVIDIA GPUs",
      "contentHtml": "<ul>\n  <li><strong>Intra-warp</strong>: Implicit, as all threads execute in lockstep.</li>\n  <li><strong>Intra-block</strong>: Achieved via <code class=\"language-plaintext highlighter-rouge\">__syncthreads()</code> barrier and shared memory.</li>\n  <li><strong>Inter-block</strong>: No built-in sync; requires multiple kernel launches or cooperative groups.</li>\n</ul>",
      "contentMarkdown": "*   **Intra-warp**: Implicit, as all threads execute in lockstep.\n*   **Intra-block**: Achieved via `__syncthreads()` barrier and shared memory.\n*   **Inter-block**: No built-in sync; requires multiple kernel launches or cooperative groups.",
      "order": 12,
      "orderInChapter": 5,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 30,
        "contentLength": 367
      },
      "nextCards": [
        "ai-gpu-architecture-model-definition-and-training-cpugpu-13",
        "ai-gpu-architecture-operator-dispatch-and-kernel-mapping-14"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#synchronization-and-communication",
      "scrapedAt": "2025-12-28T11:56:41.454Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-model-definition-and-training-cpugpu-13",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
      "title": "Model Definition and Training (CPU/GPU)",
      "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
      "contentHtml": "<ul>\n  <li>Models are typically defined in Python using high-level APIs in PyTorch (<code class=\"language-plaintext highlighter-rouge\">nn.Module</code>) or TensorFlow (<code class=\"language-plaintext highlighter-rouge\">tf.keras.Model</code>). During training, operations like matrix multiplications, convolutions, and non-linearities are recorded dynamically (in PyTorch’s eager mode) or statically (in TensorFlow’s graph mode).</li>\n  <li>If a GPU is available and selected (<code class=\"language-plaintext highlighter-rouge\">device=\"cuda\"</code> or <code class=\"language-plaintext highlighter-rouge\">tf.device('/GPU:0')</code>), the framework schedules these computations for GPU execution.</li>\n</ul>",
      "contentMarkdown": "*   Models are typically defined in Python using high-level APIs in PyTorch (`nn.Module`) or TensorFlow (`tf.keras.Model`). During training, operations like matrix multiplications, convolutions, and non-linearities are recorded dynamically (in PyTorch’s eager mode) or statically (in TensorFlow’s graph mode).\n*   If a GPU is available and selected (`device=\"cuda\"` or `tf.device('/GPU:0')`), the framework schedules these computations for GPU execution.",
      "order": 13,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "convolution"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 57,
        "contentLength": 703
      },
      "nextCards": [
        "ai-gpu-architecture-operator-dispatch-and-kernel-mapping-14",
        "ai-gpu-architecture-memory-management-and-transfer-15"
      ],
      "relatedCards": [
        "ai-cnns-for-text-classification-pooling-5",
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-cnns-for-text-classification-fully-connected-3",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#model-definition-and-training-(cpu/gpu)",
      "scrapedAt": "2025-12-28T11:56:41.454Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-operator-dispatch-and-kernel-mapping-14",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
      "title": "Operator Dispatch and Kernel Mapping",
      "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
      "contentHtml": "<ul>\n  <li>\n    <p>Each tensor operation (e.g., <code class=\"language-plaintext highlighter-rouge\">matmul</code>, <code class=\"language-plaintext highlighter-rouge\">ReLU</code>, <code class=\"language-plaintext highlighter-rouge\">conv2d</code>) corresponds to one or more GPU kernels. The deep learning framework uses backend libraries like:</p>\n\n    <ul>\n      <li><strong>cuDNN</strong> (for deep neural networks: convolutions, activation, normalization)</li>\n      <li><strong>cuBLAS</strong> (for matrix algebra)</li>\n      <li><strong>cuFFT</strong>, <strong>cuSPARSE</strong>, etc. (for specialized ops)</li>\n    </ul>\n  </li>\n  <li>\n    <p>These libraries provide pre-optimized CUDA kernels tailored for various tensor shapes and precisions (e.g., FP32, FP16, BF16, INT8). When a model executes, the framework dispatches operator calls to these GPU-optimized kernels via these libraries.</p>\n  </li>\n</ul>\n<p>Each tensor operation (e.g., <code class=\"language-plaintext highlighter-rouge\">matmul</code>, <code class=\"language-plaintext highlighter-rouge\">ReLU</code>, <code class=\"language-plaintext highlighter-rouge\">conv2d</code>) corresponds to one or more GPU kernels. The deep learning framework uses backend libraries like:</p>\n<ul>\n      <li><strong>cuDNN</strong> (for deep neural networks: convolutions, activation, normalization)</li>\n      <li><strong>cuBLAS</strong> (for matrix algebra)</li>\n      <li><strong>cuFFT</strong>, <strong>cuSPARSE</strong>, etc. (for specialized ops)</li>\n    </ul>\n<p>These libraries provide pre-optimized CUDA kernels tailored for various tensor shapes and precisions (e.g., FP32, FP16, BF16, INT8). When a model executes, the framework dispatches operator calls to these GPU-optimized kernels via these libraries.</p>",
      "contentMarkdown": "*   Each tensor operation (e.g., `matmul`, `ReLU`, `conv2d`) corresponds to one or more GPU kernels. The deep learning framework uses backend libraries like:\n    \n    *   **cuDNN** (for deep neural networks: convolutions, activation, normalization)\n    *   **cuBLAS** (for matrix algebra)\n    *   **cuFFT**, **cuSPARSE**, etc. (for specialized ops)\n*   These libraries provide pre-optimized CUDA kernels tailored for various tensor shapes and precisions (e.g., FP32, FP16, BF16, INT8). When a model executes, the framework dispatches operator calls to these GPU-optimized kernels via these libraries.\n    \n\nEach tensor operation (e.g., `matmul`, `ReLU`, `conv2d`) corresponds to one or more GPU kernels. The deep learning framework uses backend libraries like:\n\n*   **cuDNN** (for deep neural networks: convolutions, activation, normalization)\n*   **cuBLAS** (for matrix algebra)\n*   **cuFFT**, **cuSPARSE**, etc. (for specialized ops)\n\nThese libraries provide pre-optimized CUDA kernels tailored for various tensor shapes and precisions (e.g., FP32, FP16, BF16, INT8). When a model executes, the framework dispatches operator calls to these GPU-optimized kernels via these libraries.",
      "order": 14,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "neural network",
        "deep learning",
        "convolution",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 156,
        "contentLength": 1769
      },
      "nextCards": [
        "ai-gpu-architecture-memory-management-and-transfer-15",
        "ai-gpu-architecture-kernel-launch-and-execution-on-sms-16"
      ],
      "relatedCards": [
        "ai-top-30-papers-identity-mappings-in-deep-residual-networks-15",
        "ai-top-30-papers-deep-speech-2-end-to-end-speech-recognition-in-eng-21",
        "ai-cnns-for-text-classification-convolutions-1",
        "ai-cnns-for-text-classification-pooling-2",
        "ai-cnns-for-text-classification-1-d-convolutions-over-text-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#operator-dispatch-and-kernel-mapping",
      "scrapedAt": "2025-12-28T11:56:41.454Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-memory-management-and-transfer-15",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
      "title": "Memory Management and Transfer",
      "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
      "contentHtml": "<ul>\n  <li>\n    <p>Before execution, model parameters and inputs are moved to GPU memory (VRAM). PyTorch and TensorFlow both manage memory allocation, deallocation, and device transfers:</p>\n\n    <ul>\n      <li>CPU to GPU: <code class=\"language-plaintext highlighter-rouge\">tensor.to(\"cuda\")</code> (PyTorch) or <code class=\"language-plaintext highlighter-rouge\">.gpu()</code> context (TensorFlow)</li>\n      <li>GPU memory is allocated dynamically and reused to reduce fragmentation</li>\n      <li>Intermediate results during forward/backward passes are cached in GPU RAM to avoid excessive recomputation</li>\n    </ul>\n  </li>\n  <li>\n    <p>Efficient memory use is critical, especially for large models and high-resolution inputs.</p>\n  </li>\n</ul>\n<p>Before execution, model parameters and inputs are moved to GPU memory (VRAM). PyTorch and TensorFlow both manage memory allocation, deallocation, and device transfers:</p>\n<ul>\n      <li>CPU to GPU: <code class=\"language-plaintext highlighter-rouge\">tensor.to(\"cuda\")</code> (PyTorch) or <code class=\"language-plaintext highlighter-rouge\">.gpu()</code> context (TensorFlow)</li>\n      <li>GPU memory is allocated dynamically and reused to reduce fragmentation</li>\n      <li>Intermediate results during forward/backward passes are cached in GPU RAM to avoid excessive recomputation</li>\n    </ul>\n<p>Efficient memory use is critical, especially for large models and high-resolution inputs.</p>",
      "contentMarkdown": "*   Before execution, model parameters and inputs are moved to GPU memory (VRAM). PyTorch and TensorFlow both manage memory allocation, deallocation, and device transfers:\n    \n    *   CPU to GPU: `tensor.to(\"cuda\")` (PyTorch) or `.gpu()` context (TensorFlow)\n    *   GPU memory is allocated dynamically and reused to reduce fragmentation\n    *   Intermediate results during forward/backward passes are cached in GPU RAM to avoid excessive recomputation\n*   Efficient memory use is critical, especially for large models and high-resolution inputs.\n    \n\nBefore execution, model parameters and inputs are moved to GPU memory (VRAM). PyTorch and TensorFlow both manage memory allocation, deallocation, and device transfers:\n\n*   CPU to GPU: `tensor.to(\"cuda\")` (PyTorch) or `.gpu()` context (TensorFlow)\n*   GPU memory is allocated dynamically and reused to reduce fragmentation\n*   Intermediate results during forward/backward passes are cached in GPU RAM to avoid excessive recomputation\n\nEfficient memory use is critical, especially for large models and high-resolution inputs.",
      "order": 15,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 144,
        "contentLength": 1447
      },
      "nextCards": [
        "ai-gpu-architecture-kernel-launch-and-execution-on-sms-16",
        "ai-gpu-architecture-forward-and-backward-passes-training-17"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#memory-management-and-transfer",
      "scrapedAt": "2025-12-28T11:56:41.454Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-kernel-launch-and-execution-on-sms-16",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
      "title": "Kernel Launch and Execution on SMs",
      "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
      "contentHtml": "<ul>\n  <li>\n    <p>Once data and operations are prepared, each operator triggers a <strong>CUDA kernel launch</strong>. This is where NVIDIA’s execution model comes into play:</p>\n\n    <ul>\n      <li>Kernels are executed by <strong>Streaming Multiprocessors (SMs)</strong>, leveraging thousands of <strong>CUDA cores</strong></li>\n      <li>Work is parallelized into <strong>warps</strong> (32 threads), grouped into <strong>thread blocks</strong></li>\n      <li>Tensor Cores accelerate operations like GEMMs and convolutions in mixed precision (FP16, BF16, TF32)</li>\n    </ul>\n  </li>\n  <li>\n    <p>The GPU’s warp schedulers manage kernel instruction dispatch, hide memory latency, and maximize throughput by overlapping compute and memory-bound operations.</p>\n  </li>\n</ul>\n<p>Once data and operations are prepared, each operator triggers a <strong>CUDA kernel launch</strong>. This is where NVIDIA’s execution model comes into play:</p>\n<ul>\n      <li>Kernels are executed by <strong>Streaming Multiprocessors (SMs)</strong>, leveraging thousands of <strong>CUDA cores</strong></li>\n      <li>Work is parallelized into <strong>warps</strong> (32 threads), grouped into <strong>thread blocks</strong></li>\n      <li>Tensor Cores accelerate operations like GEMMs and convolutions in mixed precision (FP16, BF16, TF32)</li>\n    </ul>\n<p>The GPU’s warp schedulers manage kernel instruction dispatch, hide memory latency, and maximize throughput by overlapping compute and memory-bound operations.</p>",
      "contentMarkdown": "*   Once data and operations are prepared, each operator triggers a **CUDA kernel launch**. This is where NVIDIA’s execution model comes into play:\n    \n    *   Kernels are executed by **Streaming Multiprocessors (SMs)**, leveraging thousands of **CUDA cores**\n    *   Work is parallelized into **warps** (32 threads), grouped into **thread blocks**\n    *   Tensor Cores accelerate operations like GEMMs and convolutions in mixed precision (FP16, BF16, TF32)\n*   The GPU’s warp schedulers manage kernel instruction dispatch, hide memory latency, and maximize throughput by overlapping compute and memory-bound operations.\n    \n\nOnce data and operations are prepared, each operator triggers a **CUDA kernel launch**. This is where NVIDIA’s execution model comes into play:\n\n*   Kernels are executed by **Streaming Multiprocessors (SMs)**, leveraging thousands of **CUDA cores**\n*   Work is parallelized into **warps** (32 threads), grouped into **thread blocks**\n*   Tensor Cores accelerate operations like GEMMs and convolutions in mixed precision (FP16, BF16, TF32)\n\nThe GPU’s warp schedulers manage kernel instruction dispatch, hide memory latency, and maximize throughput by overlapping compute and memory-bound operations.",
      "order": 16,
      "orderInChapter": 4,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "convolution"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 166,
        "contentLength": 1501
      },
      "nextCards": [
        "ai-gpu-architecture-forward-and-backward-passes-training-17",
        "ai-gpu-architecture-inference-deployment-18"
      ],
      "relatedCards": [
        "ai-cnns-for-text-classification-pooling-5",
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-cnns-for-text-classification-fully-connected-3",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#kernel-launch-and-execution-on-sms",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-forward-and-backward-passes-training-17",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
      "title": "Forward and Backward Passes (Training)",
      "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
      "contentHtml": "<ul>\n  <li><strong>Forward pass</strong>: Model computations are executed layer by layer, and activations are stored.</li>\n  <li><strong>Backward pass</strong>: Gradients are computed using automatic differentiation, with additional kernels launched for each operation’s derivative.</li>\n  <li>\n    <p><strong>Gradient updates</strong>: Optimizer steps (e.g., Adam, SGD) are also dispatched to GPU kernels when tensors reside in GPU memory.</p>\n  </li>\n  <li>All of this happens in the context of a training loop, with heavy reliance on GPU compute resources and memory bandwidth.</li>\n</ul>\n<p><strong>Gradient updates</strong>: Optimizer steps (e.g., Adam, SGD) are also dispatched to GPU kernels when tensors reside in GPU memory.</p>",
      "contentMarkdown": "*   **Forward pass**: Model computations are executed layer by layer, and activations are stored.\n*   **Backward pass**: Gradients are computed using automatic differentiation, with additional kernels launched for each operation’s derivative.\n*   **Gradient updates**: Optimizer steps (e.g., Adam, SGD) are also dispatched to GPU kernels when tensors reside in GPU memory.\n    \n*   All of this happens in the context of a training loop, with heavy reliance on GPU compute resources and memory bandwidth.\n\n**Gradient updates**: Optimizer steps (e.g., Adam, SGD) are also dispatched to GPU kernels when tensors reside in GPU memory.",
      "order": 17,
      "orderInChapter": 5,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "activation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 92,
        "contentLength": 737
      },
      "nextCards": [
        "ai-gpu-architecture-inference-deployment-18",
        "ai-gpu-architecture-precision-optimization-19"
      ],
      "relatedCards": [
        "ai-model-debugging-grad-cam-15",
        "ai-model-debugging-gradient-checking-10",
        "ai-ml-runtimes-implementation-details-5",
        "ai-ml-runtimes-design-notes-67",
        "ai-tensorboard-plotting-losses-accuracies-and-weight-distribution-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#forward-and-backward-passes-(training)",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-inference-deployment-18",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
      "title": "Inference Deployment",
      "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
      "contentHtml": "<ul>\n  <li>\n    <p>Once trained, the model is often exported for inference:</p>\n\n    <ul>\n      <li><strong>TensorFlow:</strong> SavedModel, TF Lite, or ONNX format</li>\n      <li><strong>PyTorch:</strong> TorchScript, ONNX, or native PyTorch weights</li>\n    </ul>\n  </li>\n  <li>\n    <p>During inference, only the forward pass is executed. Frameworks and inference engines (like TensorRT, TorchServe, or TF Serving) strip unnecessary training components, optimize kernel ordering, and preload weights into GPU memory to reduce latency and maximize throughput.</p>\n  </li>\n</ul>\n<p>Once trained, the model is often exported for inference:</p>\n<ul>\n      <li><strong>TensorFlow:</strong> SavedModel, TF Lite, or ONNX format</li>\n      <li><strong>PyTorch:</strong> TorchScript, ONNX, or native PyTorch weights</li>\n    </ul>\n<p>During inference, only the forward pass is executed. Frameworks and inference engines (like TensorRT, TorchServe, or TF Serving) strip unnecessary training components, optimize kernel ordering, and preload weights into GPU memory to reduce latency and maximize throughput.</p>",
      "contentMarkdown": "*   Once trained, the model is often exported for inference:\n    \n    *   **TensorFlow:** SavedModel, TF Lite, or ONNX format\n    *   **PyTorch:** TorchScript, ONNX, or native PyTorch weights\n*   During inference, only the forward pass is executed. Frameworks and inference engines (like TensorRT, TorchServe, or TF Serving) strip unnecessary training components, optimize kernel ordering, and preload weights into GPU memory to reduce latency and maximize throughput.\n    \n\nOnce trained, the model is often exported for inference:\n\n*   **TensorFlow:** SavedModel, TF Lite, or ONNX format\n*   **PyTorch:** TorchScript, ONNX, or native PyTorch weights\n\nDuring inference, only the forward pass is executed. Frameworks and inference engines (like TensorRT, TorchServe, or TF Serving) strip unnecessary training components, optimize kernel ordering, and preload weights into GPU memory to reduce latency and maximize throughput.",
      "order": 18,
      "orderInChapter": 6,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 126,
        "contentLength": 1103
      },
      "nextCards": [
        "ai-gpu-architecture-precision-optimization-19",
        "ai-gpu-architecture-runtime-tools-and-profiling-20"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#inference-deployment",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-precision-optimization-19",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
      "title": "Precision Optimization",
      "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
      "contentHtml": "<ul>\n  <li>\n    <p>To reduce GPU memory usage and increase performance, models often use:</p>\n\n    <ul>\n      <li><strong>Mixed precision training</strong> (e.g., <code class=\"language-plaintext highlighter-rouge\">float16</code> for activations, <code class=\"language-plaintext highlighter-rouge\">float32</code> for loss and gradients)</li>\n      <li><strong>Quantized inference</strong> (e.g., <code class=\"language-plaintext highlighter-rouge\">int8</code> for weights and activations)\nThese techniques rely on the GPU’s Tensor Cores and require calibration or loss-scaling strategies for training stability.</li>\n    </ul>\n  </li>\n</ul>\n<p>To reduce GPU memory usage and increase performance, models often use:</p>\n<ul>\n      <li><strong>Mixed precision training</strong> (e.g., <code class=\"language-plaintext highlighter-rouge\">float16</code> for activations, <code class=\"language-plaintext highlighter-rouge\">float32</code> for loss and gradients)</li>\n      <li><strong>Quantized inference</strong> (e.g., <code class=\"language-plaintext highlighter-rouge\">int8</code> for weights and activations)\nThese techniques rely on the GPU’s Tensor Cores and require calibration or loss-scaling strategies for training stability.</li>\n    </ul>",
      "contentMarkdown": "*   To reduce GPU memory usage and increase performance, models often use:\n    \n    *   **Mixed precision training** (e.g., `float16` for activations, `float32` for loss and gradients)\n    *   **Quantized inference** (e.g., `int8` for weights and activations) These techniques rely on the GPU’s Tensor Cores and require calibration or loss-scaling strategies for training stability.\n\nTo reduce GPU memory usage and increase performance, models often use:\n\n*   **Mixed precision training** (e.g., `float16` for activations, `float32` for loss and gradients)\n*   **Quantized inference** (e.g., `int8` for weights and activations) These techniques rely on the GPU’s Tensor Cores and require calibration or loss-scaling strategies for training stability.",
      "order": 19,
      "orderInChapter": 7,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 101,
        "contentLength": 1242
      },
      "nextCards": [
        "ai-gpu-architecture-runtime-tools-and-profiling-20",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21"
      ],
      "relatedCards": [
        "ai-model-debugging-grad-cam-15",
        "ai-ml-runtimes-implementation-details-5",
        "ai-ml-runtimes-design-notes-67",
        "ai-tensorboard-plotting-losses-accuracies-and-weight-distribution-3",
        "ai-tensorboard-logging-images-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#precision-optimization",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-runtime-tools-and-profiling-20",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Workflow: Running AI Models on NVIDIA GPUs",
      "title": "Runtime Tools and Profiling",
      "subtitle": "Workflow: Running AI Models on NVIDIA GPUs",
      "contentHtml": "<ul>\n  <li>\n    <p>Execution can be monitored and optimized using:</p>\n\n    <ul>\n      <li><strong>Nsight Compute/Systems</strong>: Low-level GPU profiling</li>\n      <li><strong>TensorBoard</strong>: Graph inspection and timing (TensorFlow)</li>\n      <li><strong>PyTorch Profiler</strong>: Operator-level breakdown and bottleneck tracing</li>\n      <li><strong>NVIDIA’s CUPTI</strong>: Provides hooks for collecting runtime metrics</li>\n    </ul>\n  </li>\n</ul>\n<p>Execution can be monitored and optimized using:</p>\n<ul>\n      <li><strong>Nsight Compute/Systems</strong>: Low-level GPU profiling</li>\n      <li><strong>TensorBoard</strong>: Graph inspection and timing (TensorFlow)</li>\n      <li><strong>PyTorch Profiler</strong>: Operator-level breakdown and bottleneck tracing</li>\n      <li><strong>NVIDIA’s CUPTI</strong>: Provides hooks for collecting runtime metrics</li>\n    </ul>",
      "contentMarkdown": "*   Execution can be monitored and optimized using:\n    \n    *   **Nsight Compute/Systems**: Low-level GPU profiling\n    *   **TensorBoard**: Graph inspection and timing (TensorFlow)\n    *   **PyTorch Profiler**: Operator-level breakdown and bottleneck tracing\n    *   **NVIDIA’s CUPTI**: Provides hooks for collecting runtime metrics\n\nExecution can be monitored and optimized using:\n\n*   **Nsight Compute/Systems**: Low-level GPU profiling\n*   **TensorBoard**: Graph inspection and timing (TensorFlow)\n*   **PyTorch Profiler**: Operator-level breakdown and bottleneck tracing\n*   **NVIDIA’s CUPTI**: Provides hooks for collecting runtime metrics",
      "order": 20,
      "orderInChapter": 8,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 75,
        "contentLength": 890
      },
      "nextCards": [
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-gpu-architecture-cuda-core-advancements-22"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#runtime-tools-and-profiling",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Compute Architecture Evolution",
      "title": "Streaming Multiprocessors (SM) Evolution",
      "subtitle": "Compute Architecture Evolution",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Ampere (2020)</strong>:</p>\n\n    <ul>\n      <li>SMs housed 128 CUDA cores (vs. 64 in Turing).</li>\n      <li>Warp schedulers improved to issue instructions to mixed precision units simultaneously.</li>\n      <li>Dual datapath per CUDA core allowed FP32 + INT32 execution concurrently.</li>\n      <li>FP64 throughput doubled in data center models (A100).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hopper (2022)</strong>:</p>\n\n    <ul>\n      <li>Redesigned SMs for higher clock speeds and more instruction-level parallelism.</li>\n      <li>Added <strong>DPX instructions</strong> for dynamic programming acceleration (bioinformatics, optimization).</li>\n      <li>Warp specializations for matrix workloads to better feed Tensor Cores.</li>\n      <li>Increased register file size for HPC workloads.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Ada Lovelace (2022)</strong>:</p>\n\n    <ul>\n      <li>Focused on gaming and creative workloads.</li>\n      <li>Higher boost frequencies per SM.</li>\n      <li>Improved power gating for efficiency.</li>\n      <li>Enhanced scheduling for real-time ray tracing workloads.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Blackwell (2024)</strong>:</p>\n\n    <ul>\n      <li>Next-generation SMs with <em>thread block clusters</em> for improved multi-SM cooperation.</li>\n      <li>Improved simultaneous multi-kernel execution.</li>\n      <li>Expanded warp schedulers to reduce instruction stalls in large AI inference.</li>\n      <li>Further increased FP8, BF16, and mixed-precision throughput in SM datapaths.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Ampere (2020)</strong>:</p>\n<ul>\n      <li>SMs housed 128 CUDA cores (vs. 64 in Turing).</li>\n      <li>Warp schedulers improved to issue instructions to mixed precision units simultaneously.</li>\n      <li>Dual datapath per CUDA core allowed FP32 + INT32 execution concurrently.</li>\n      <li>FP64 throughput doubled in data center models (A100).</li>\n    </ul>\n<p><strong>Hopper (2022)</strong>:</p>\n<ul>\n      <li>Redesigned SMs for higher clock speeds and more instruction-level parallelism.</li>\n      <li>Added <strong>DPX instructions</strong> for dynamic programming acceleration (bioinformatics, optimization).</li>\n      <li>Warp specializations for matrix workloads to better feed Tensor Cores.</li>\n      <li>Increased register file size for HPC workloads.</li>\n    </ul>\n<p><strong>Ada Lovelace (2022)</strong>:</p>\n<ul>\n      <li>Focused on gaming and creative workloads.</li>\n      <li>Higher boost frequencies per SM.</li>\n      <li>Improved power gating for efficiency.</li>\n      <li>Enhanced scheduling for real-time ray tracing workloads.</li>\n    </ul>\n<p><strong>Blackwell (2024)</strong>:</p>\n<ul>\n      <li>Next-generation SMs with <em>thread block clusters</em> for improved multi-SM cooperation.</li>\n      <li>Improved simultaneous multi-kernel execution.</li>\n      <li>Expanded warp schedulers to reduce instruction stalls in large AI inference.</li>\n      <li>Further increased FP8, BF16, and mixed-precision throughput in SM datapaths.</li>\n    </ul>",
      "contentMarkdown": "*   **Ampere (2020)**:\n    \n    *   SMs housed 128 CUDA cores (vs. 64 in Turing).\n    *   Warp schedulers improved to issue instructions to mixed precision units simultaneously.\n    *   Dual datapath per CUDA core allowed FP32 + INT32 execution concurrently.\n    *   FP64 throughput doubled in data center models (A100).\n*   **Hopper (2022)**:\n    \n    *   Redesigned SMs for higher clock speeds and more instruction-level parallelism.\n    *   Added **DPX instructions** for dynamic programming acceleration (bioinformatics, optimization).\n    *   Warp specializations for matrix workloads to better feed Tensor Cores.\n    *   Increased register file size for HPC workloads.\n*   **Ada Lovelace (2022)**:\n    \n    *   Focused on gaming and creative workloads.\n    *   Higher boost frequencies per SM.\n    *   Improved power gating for efficiency.\n    *   Enhanced scheduling for real-time ray tracing workloads.\n*   **Blackwell (2024)**:\n    \n    *   Next-generation SMs with _thread block clusters_ for improved multi-SM cooperation.\n    *   Improved simultaneous multi-kernel execution.\n    *   Expanded warp schedulers to reduce instruction stalls in large AI inference.\n    *   Further increased FP8, BF16, and mixed-precision throughput in SM datapaths.\n\n**Ampere (2020)**:\n\n*   SMs housed 128 CUDA cores (vs. 64 in Turing).\n*   Warp schedulers improved to issue instructions to mixed precision units simultaneously.\n*   Dual datapath per CUDA core allowed FP32 + INT32 execution concurrently.\n*   FP64 throughput doubled in data center models (A100).\n\n**Hopper (2022)**:\n\n*   Redesigned SMs for higher clock speeds and more instruction-level parallelism.\n*   Added **DPX instructions** for dynamic programming acceleration (bioinformatics, optimization).\n*   Warp specializations for matrix workloads to better feed Tensor Cores.\n*   Increased register file size for HPC workloads.\n\n**Ada Lovelace (2022)**:\n\n*   Focused on gaming and creative workloads.\n*   Higher boost frequencies per SM.\n*   Improved power gating for efficiency.\n*   Enhanced scheduling for real-time ray tracing workloads.\n\n**Blackwell (2024)**:\n\n*   Next-generation SMs with _thread block clusters_ for improved multi-SM cooperation.\n*   Improved simultaneous multi-kernel execution.\n*   Expanded warp schedulers to reduce instruction stalls in large AI inference.\n*   Further increased FP8, BF16, and mixed-precision throughput in SM datapaths.",
      "order": 21,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "optimization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 320,
        "contentLength": 3094
      },
      "nextCards": [
        "ai-gpu-architecture-cuda-core-advancements-22",
        "ai-gpu-architecture-tensor-core-evolution-23"
      ],
      "relatedCards": [
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-pros-and-cons-6",
        "ai-ml-runtimes-implementation-details-22",
        "ai-ml-runtimes-pros-and-cons-23"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#streaming-multiprocessors-(sm)-evolution",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-cuda-core-advancements-22",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Compute Architecture Evolution",
      "title": "CUDA Core Advancements",
      "subtitle": "Compute Architecture Evolution",
      "contentHtml": "<ul>\n  <li><strong>Ampere:</strong> 128 CUDA cores/SM, concurrent FP32 and INT32 execution per clock.</li>\n  <li><strong>Hopper:</strong> Improved dual-issue scheduling, better cache locality for CUDA workloads.</li>\n  <li><strong>Ada:</strong> Boosted per-core clock frequency; targeted gaming rasterization and shading throughput.</li>\n  <li><strong>Blackwell:</strong> Higher per-core IPC, deeper pipelines for AI workloads.</li>\n</ul>",
      "contentMarkdown": "*   **Ampere:** 128 CUDA cores/SM, concurrent FP32 and INT32 execution per clock.\n*   **Hopper:** Improved dual-issue scheduling, better cache locality for CUDA workloads.\n*   **Ada:** Boosted per-core clock frequency; targeted gaming rasterization and shading throughput.\n*   **Blackwell:** Higher per-core IPC, deeper pipelines for AI workloads.",
      "order": 22,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 45,
        "contentLength": 438
      },
      "nextCards": [
        "ai-gpu-architecture-tensor-core-evolution-23",
        "ai-gpu-architecture-ray-tracing-core-evolution-24"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#cuda-core-advancements",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-tensor-core-evolution-23",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Compute Architecture Evolution",
      "title": "Tensor Core Evolution",
      "subtitle": "Compute Architecture Evolution",
      "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Generation</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Precision Support</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Notable Features</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ampere</td>\n<td class=\"tg-tleft-valign-first\"><code>float16</code>, <code>bfloat16</code>, <code>TensorFloat-32\n</code>, <code>int8</code>, <code>int4</code></td>\n<td class=\"tg-tleft-valign-second\">TF32 introduced for AI training; structured sparsity (2:4 pattern).</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Hopper</td>\n<td class=\"tg-tleft-valign-first\">Adds <code>float8</code>, improved <code>bfloat16</code>/<code>float16</code></td>\n<td class=\"tg-tleft-valign-second\">Transformer Engine for mixed-precision AI acceleration.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ada</td>\n<td class=\"tg-tleft-valign-first\">Similar to Hopper for consumer SKUs</td>\n<td class=\"tg-tleft-valign-second\">AI super resolution for DLSS 3.x.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Blackwell</td>\n<td class=\"tg-tleft-valign-first\"><code>float4</code>, expanded <code>float8</code></td>\n<td class=\"tg-tleft-valign-second\">Enhanced Transformer Engine for LLMs, better sparsity handling.</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Generation</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Precision Support</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Notable Features</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ampere</td>\n<td class=\"tg-tleft-valign-first\"><code>float16</code>, <code>bfloat16</code>, <code>TensorFloat-32\n</code>, <code>int8</code>, <code>int4</code></td>\n<td class=\"tg-tleft-valign-second\">TF32 introduced for AI training; structured sparsity (2:4 pattern).</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Hopper</td>\n<td class=\"tg-tleft-valign-first\">Adds <code>float8</code>, improved <code>bfloat16</code>/<code>float16</code></td>\n<td class=\"tg-tleft-valign-second\">Transformer Engine for mixed-precision AI acceleration.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ada</td>\n<td class=\"tg-tleft-valign-first\">Similar to Hopper for consumer SKUs</td>\n<td class=\"tg-tleft-valign-second\">AI super resolution for DLSS 3.x.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Blackwell</td>\n<td class=\"tg-tleft-valign-first\"><code>float4</code>, expanded <code>float8</code></td>\n<td class=\"tg-tleft-valign-second\">Enhanced Transformer Engine for LLMs, better sparsity handling.</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "**Generation**\n\n**Precision Support**\n\n**Notable Features**\n\nAmpere\n\n`float16`, `bfloat16`, `TensorFloat-32` , `int8`, `int4`\n\nTF32 introduced for AI training; structured sparsity (2:4 pattern).\n\nHopper\n\nAdds `float8`, improved `bfloat16`/`float16`\n\nTransformer Engine for mixed-precision AI acceleration.\n\nAda\n\nSimilar to Hopper for consumer SKUs\n\nAI super resolution for DLSS 3.x.\n\nBlackwell\n\n`float4`, expanded `float8`\n\nEnhanced Transformer Engine for LLMs, better sparsity handling.\n\n**Generation**\n\n**Precision Support**\n\n**Notable Features**\n\nAmpere\n\n`float16`, `bfloat16`, `TensorFloat-32` , `int8`, `int4`\n\nTF32 introduced for AI training; structured sparsity (2:4 pattern).\n\nHopper\n\nAdds `float8`, improved `bfloat16`/`float16`\n\nTransformer Engine for mixed-precision AI acceleration.\n\nAda\n\nSimilar to Hopper for consumer SKUs\n\nAI super resolution for DLSS 3.x.\n\nBlackwell\n\n`float4`, expanded `float8`\n\nEnhanced Transformer Engine for LLMs, better sparsity handling.",
      "order": 23,
      "orderInChapter": 3,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "transformer",
        "llm"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 114,
        "contentLength": 2699
      },
      "nextCards": [
        "ai-gpu-architecture-ray-tracing-core-evolution-24",
        "ai-gpu-architecture-overview-of-precision-types-25"
      ],
      "relatedCards": [
        "ai-ml-runtimes-llamacpp-deep-dive-38",
        "ai-top-30-papers-better-faster-large-language-models-via-multi-toke-27",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-comparative-summary-and-guidance-53",
        "ai-ml-runtimes-gguf-gpt-generated-ggml-unified-format-58"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#tensor-core-evolution",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-ray-tracing-core-evolution-24",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Compute Architecture Evolution",
      "title": "Ray Tracing Core Evolution",
      "subtitle": "Compute Architecture Evolution",
      "contentHtml": "<ul>\n  <li><strong>Ampere:</strong> 2nd-gen RT Cores, hardware triangle intersection, motion blur support.</li>\n  <li><strong>Hopper:</strong> Primarily HPC focus; RT improvements not the priority.</li>\n  <li><strong>Ada:</strong> 3rd-gen RT Cores, Opacity Micromaps, Displaced Micro-Meshes.</li>\n  <li><strong>Blackwell:</strong> AI-assisted ray traversal prediction, further pipeline efficiency gains.</li>\n</ul>",
      "contentMarkdown": "*   **Ampere:** 2nd-gen RT Cores, hardware triangle intersection, motion blur support.\n*   **Hopper:** Primarily HPC focus; RT improvements not the priority.\n*   **Ada:** 3rd-gen RT Cores, Opacity Micromaps, Displaced Micro-Meshes.\n*   **Blackwell:** AI-assisted ray traversal prediction, further pipeline efficiency gains.",
      "order": 24,
      "orderInChapter": 4,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 40,
        "contentLength": 414
      },
      "nextCards": [
        "ai-gpu-architecture-overview-of-precision-types-25",
        "ai-gpu-architecture-per-generation-precision-support-and-performance-26"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#ray-tracing-core-evolution",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-overview-of-precision-types-25",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Floating-Point Precision Performance Evolution",
      "title": "Overview of Precision Types",
      "subtitle": "Floating-Point Precision Performance Evolution",
      "contentHtml": "<ul>\n  <li>\n    <p>NVIDIA GPUs handle multiple floating-point formats for different workloads:</p>\n\n    <ul>\n      <li><strong>FP64 (Double Precision):</strong> 64-bit IEEE format; essential for scientific and HPC workloads.</li>\n      <li><strong>FP32 (Single Precision):</strong> 32-bit IEEE format; common for graphics, simulations, and ML training.</li>\n      <li><strong>TF32 (Tensor Float 32):</strong> 19-bit precision (8-bit exponent, 10-bit mantissa) introduced in Ampere for AI workloads—maintains FP32 range with reduced precision for faster computation.</li>\n      <li><strong>FP16 (Half Precision):</strong> 16-bit IEEE format; used for mixed-precision deep learning training.</li>\n      <li><strong>BF16 (Brain Floating Point 16):</strong> 16-bit format with FP32’s exponent range but reduced mantissa precision; popular in AI training for stability.</li>\n      <li><strong>FP8:</strong> 8-bit floating point (two formats E4M3 and E5M2); optimized for AI inference.</li>\n      <li><strong>FP4:</strong> 4-bit floating point; introduced in Blackwell for ultra-low precision inference.</li>\n    </ul>\n  </li>\n</ul>\n<p>NVIDIA GPUs handle multiple floating-point formats for different workloads:</p>\n<ul>\n      <li><strong>FP64 (Double Precision):</strong> 64-bit IEEE format; essential for scientific and HPC workloads.</li>\n      <li><strong>FP32 (Single Precision):</strong> 32-bit IEEE format; common for graphics, simulations, and ML training.</li>\n      <li><strong>TF32 (Tensor Float 32):</strong> 19-bit precision (8-bit exponent, 10-bit mantissa) introduced in Ampere for AI workloads—maintains FP32 range with reduced precision for faster computation.</li>\n      <li><strong>FP16 (Half Precision):</strong> 16-bit IEEE format; used for mixed-precision deep learning training.</li>\n      <li><strong>BF16 (Brain Floating Point 16):</strong> 16-bit format with FP32’s exponent range but reduced mantissa precision; popular in AI training for stability.</li>\n      <li><strong>FP8:</strong> 8-bit floating point (two formats E4M3 and E5M2); optimized for AI inference.</li>\n      <li><strong>FP4:</strong> 4-bit floating point; introduced in Blackwell for ultra-low precision inference.</li>\n    </ul>",
      "contentMarkdown": "*   NVIDIA GPUs handle multiple floating-point formats for different workloads:\n    \n    *   **FP64 (Double Precision):** 64-bit IEEE format; essential for scientific and HPC workloads.\n    *   **FP32 (Single Precision):** 32-bit IEEE format; common for graphics, simulations, and ML training.\n    *   **TF32 (Tensor Float 32):** 19-bit precision (8-bit exponent, 10-bit mantissa) introduced in Ampere for AI workloads—maintains FP32 range with reduced precision for faster computation.\n    *   **FP16 (Half Precision):** 16-bit IEEE format; used for mixed-precision deep learning training.\n    *   **BF16 (Brain Floating Point 16):** 16-bit format with FP32’s exponent range but reduced mantissa precision; popular in AI training for stability.\n    *   **FP8:** 8-bit floating point (two formats E4M3 and E5M2); optimized for AI inference.\n    *   **FP4:** 4-bit floating point; introduced in Blackwell for ultra-low precision inference.\n\nNVIDIA GPUs handle multiple floating-point formats for different workloads:\n\n*   **FP64 (Double Precision):** 64-bit IEEE format; essential for scientific and HPC workloads.\n*   **FP32 (Single Precision):** 32-bit IEEE format; common for graphics, simulations, and ML training.\n*   **TF32 (Tensor Float 32):** 19-bit precision (8-bit exponent, 10-bit mantissa) introduced in Ampere for AI workloads—maintains FP32 range with reduced precision for faster computation.\n*   **FP16 (Half Precision):** 16-bit IEEE format; used for mixed-precision deep learning training.\n*   **BF16 (Brain Floating Point 16):** 16-bit format with FP32’s exponent range but reduced mantissa precision; popular in AI training for stability.\n*   **FP8:** 8-bit floating point (two formats E4M3 and E5M2); optimized for AI inference.\n*   **FP4:** 4-bit floating point; introduced in Blackwell for ultra-low precision inference.",
      "order": 25,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "deep learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 245,
        "contentLength": 2218
      },
      "nextCards": [
        "ai-gpu-architecture-per-generation-precision-support-and-performance-26",
        "ai-gpu-architecture-performance-analysis-27"
      ],
      "relatedCards": [
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-ml-runtimes-overview-3",
        "ai-top-30-papers-deep-residual-learning-for-image-recognition-10",
        "ai-ann-similarity-search-scann-scalable-nearest-neighbors-11",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#overview-of-precision-types",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-per-generation-precision-support-and-performance-26",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Floating-Point Precision Performance Evolution",
      "title": "Per-Generation Precision Support and Performance",
      "subtitle": "Floating-Point Precision Performance Evolution",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Ampere (2020)</strong>:</p>\n\n    <ul>\n      <li>FP64: 1/2 rate of FP32 in A100 (HPC models), 1/64 in consumer cards.</li>\n      <li>FP32: 2× throughput vs Turing; dual datapath allowed FP32 + INT32 execution.</li>\n      <li>TF32: Enabled on Tensor Cores; up to 20× faster AI training vs pure FP32.</li>\n      <li>FP16/BF16: Tensor Core acceleration, ~312 TFLOPS on A100.</li>\n      <li>INT8/INT4: Supported with sparsity acceleration.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hopper (2022)</strong>:</p>\n\n    <ul>\n      <li>FP64: Maintained 1/2 rate in HPC variants (H100).</li>\n      <li>FP32: Slight IPC and clock improvements.</li>\n      <li>FP16/BF16: Transformer Engine dynamically chose precision (FP16, BF16, FP8) per layer to optimize throughput.</li>\n      <li>FP8: Introduced with up to 4× throughput improvement over FP16 for inference.</li>\n      <li>TF32: Same as Ampere but with efficiency gains via scheduling.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Ada Lovelace (2022)</strong>:</p>\n\n    <ul>\n      <li>FP64: Mostly absent in consumer models (used in some pro variants at reduced rate).</li>\n      <li>FP32: Highest per-core clock speeds to date for rasterization.</li>\n      <li>FP16/BF16: Similar to Hopper in supported formats but tuned for DLSS and AI upscaling.</li>\n      <li>FP8: Present in professional Ada GPUs for AI workloads.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Blackwell (2024)</strong>:</p>\n\n    <ul>\n      <li>FP64: Maintains HPC capability at 1/2 FP32 rate in B100/B200.</li>\n      <li>FP32: Further IPC gains; optimized for large-scale AI as well as HPC.</li>\n      <li>FP16/BF16: Higher throughput than Hopper with enhanced Transformer Engine.</li>\n      <li>FP8: Doubled throughput over Hopper for inference.</li>\n      <li>FP4: New ultra-low precision mode; allows extremely high throughput for LLM inference with minimal accuracy loss using quantization-aware training.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Ampere (2020)</strong>:</p>\n<ul>\n      <li>FP64: 1/2 rate of FP32 in A100 (HPC models), 1/64 in consumer cards.</li>\n      <li>FP32: 2× throughput vs Turing; dual datapath allowed FP32 + INT32 execution.</li>\n      <li>TF32: Enabled on Tensor Cores; up to 20× faster AI training vs pure FP32.</li>\n      <li>FP16/BF16: Tensor Core acceleration, ~312 TFLOPS on A100.</li>\n      <li>INT8/INT4: Supported with sparsity acceleration.</li>\n    </ul>\n<p><strong>Hopper (2022)</strong>:</p>\n<ul>\n      <li>FP64: Maintained 1/2 rate in HPC variants (H100).</li>\n      <li>FP32: Slight IPC and clock improvements.</li>\n      <li>FP16/BF16: Transformer Engine dynamically chose precision (FP16, BF16, FP8) per layer to optimize throughput.</li>\n      <li>FP8: Introduced with up to 4× throughput improvement over FP16 for inference.</li>\n      <li>TF32: Same as Ampere but with efficiency gains via scheduling.</li>\n    </ul>\n<p><strong>Ada Lovelace (2022)</strong>:</p>\n<ul>\n      <li>FP64: Mostly absent in consumer models (used in some pro variants at reduced rate).</li>\n      <li>FP32: Highest per-core clock speeds to date for rasterization.</li>\n      <li>FP16/BF16: Similar to Hopper in supported formats but tuned for DLSS and AI upscaling.</li>\n      <li>FP8: Present in professional Ada GPUs for AI workloads.</li>\n    </ul>\n<p><strong>Blackwell (2024)</strong>:</p>\n<ul>\n      <li>FP64: Maintains HPC capability at 1/2 FP32 rate in B100/B200.</li>\n      <li>FP32: Further IPC gains; optimized for large-scale AI as well as HPC.</li>\n      <li>FP16/BF16: Higher throughput than Hopper with enhanced Transformer Engine.</li>\n      <li>FP8: Doubled throughput over Hopper for inference.</li>\n      <li>FP4: New ultra-low precision mode; allows extremely high throughput for LLM inference with minimal accuracy loss using quantization-aware training.</li>\n    </ul>",
      "contentMarkdown": "*   **Ampere (2020)**:\n    \n    *   FP64: 1/2 rate of FP32 in A100 (HPC models), 1/64 in consumer cards.\n    *   FP32: 2× throughput vs Turing; dual datapath allowed FP32 + INT32 execution.\n    *   TF32: Enabled on Tensor Cores; up to 20× faster AI training vs pure FP32.\n    *   FP16/BF16: Tensor Core acceleration, ~312 TFLOPS on A100.\n    *   INT8/INT4: Supported with sparsity acceleration.\n*   **Hopper (2022)**:\n    \n    *   FP64: Maintained 1/2 rate in HPC variants (H100).\n    *   FP32: Slight IPC and clock improvements.\n    *   FP16/BF16: Transformer Engine dynamically chose precision (FP16, BF16, FP8) per layer to optimize throughput.\n    *   FP8: Introduced with up to 4× throughput improvement over FP16 for inference.\n    *   TF32: Same as Ampere but with efficiency gains via scheduling.\n*   **Ada Lovelace (2022)**:\n    \n    *   FP64: Mostly absent in consumer models (used in some pro variants at reduced rate).\n    *   FP32: Highest per-core clock speeds to date for rasterization.\n    *   FP16/BF16: Similar to Hopper in supported formats but tuned for DLSS and AI upscaling.\n    *   FP8: Present in professional Ada GPUs for AI workloads.\n*   **Blackwell (2024)**:\n    \n    *   FP64: Maintains HPC capability at 1/2 FP32 rate in B100/B200.\n    *   FP32: Further IPC gains; optimized for large-scale AI as well as HPC.\n    *   FP16/BF16: Higher throughput than Hopper with enhanced Transformer Engine.\n    *   FP8: Doubled throughput over Hopper for inference.\n    *   FP4: New ultra-low precision mode; allows extremely high throughput for LLM inference with minimal accuracy loss using quantization-aware training.\n\n**Ampere (2020)**:\n\n*   FP64: 1/2 rate of FP32 in A100 (HPC models), 1/64 in consumer cards.\n*   FP32: 2× throughput vs Turing; dual datapath allowed FP32 + INT32 execution.\n*   TF32: Enabled on Tensor Cores; up to 20× faster AI training vs pure FP32.\n*   FP16/BF16: Tensor Core acceleration, ~312 TFLOPS on A100.\n*   INT8/INT4: Supported with sparsity acceleration.\n\n**Hopper (2022)**:\n\n*   FP64: Maintained 1/2 rate in HPC variants (H100).\n*   FP32: Slight IPC and clock improvements.\n*   FP16/BF16: Transformer Engine dynamically chose precision (FP16, BF16, FP8) per layer to optimize throughput.\n*   FP8: Introduced with up to 4× throughput improvement over FP16 for inference.\n*   TF32: Same as Ampere but with efficiency gains via scheduling.\n\n**Ada Lovelace (2022)**:\n\n*   FP64: Mostly absent in consumer models (used in some pro variants at reduced rate).\n*   FP32: Highest per-core clock speeds to date for rasterization.\n*   FP16/BF16: Similar to Hopper in supported formats but tuned for DLSS and AI upscaling.\n*   FP8: Present in professional Ada GPUs for AI workloads.\n\n**Blackwell (2024)**:\n\n*   FP64: Maintains HPC capability at 1/2 FP32 rate in B100/B200.\n*   FP32: Further IPC gains; optimized for large-scale AI as well as HPC.\n*   FP16/BF16: Higher throughput than Hopper with enhanced Transformer Engine.\n*   FP8: Doubled throughput over Hopper for inference.\n*   FP4: New ultra-low precision mode; allows extremely high throughput for LLM inference with minimal accuracy loss using quantization-aware training.",
      "order": 26,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous",
        "transformer",
        "llm"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 470,
        "contentLength": 3856
      },
      "nextCards": [
        "ai-gpu-architecture-performance-analysis-27",
        "ai-gpu-architecture-vram-technologies-and-bandwidth-28"
      ],
      "relatedCards": [
        "ai-ml-runtimes-llamacpp-deep-dive-38",
        "ai-top-30-papers-better-faster-large-language-models-via-multi-toke-27",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-pros-and-cons-42",
        "ai-ml-runtimes-comparative-summary-and-guidance-53"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#per-generation-precision-support-and-performance",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-performance-analysis-27",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Floating-Point Precision Performance Evolution",
      "title": "Performance Analysis",
      "subtitle": "Floating-Point Precision Performance Evolution",
      "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Generation</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP64 (HPC)</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP32</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TF32</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP16/BF16</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP8</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>FP4</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ampere</td>\n<td class=\"tg-tleft-valign-first\">1/2 FP32 rate (A100)</td>\n<td class=\"tg-tleft-valign-first\">Dual datapath, +2× Turing</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Tensor Core, ~312 TFLOPS</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Hopper</td>\n<td class=\"tg-tleft-valign-first\">1/2 FP32 rate (H100)</td>\n<td class=\"tg-tleft-valign-first\">IPC + clock boost</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Transformer Engine</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ada</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-first\">High clocks</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Consumer AI workloads</td>\n<td class=\"tg-tleft-valign-first\">Some pro models</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Blackwell</td>\n<td class=\"tg-tleft-valign-first\">1/2 FP32 rate (B100/B200)</td>\n<td class=\"tg-tleft-valign-first\">Higher IPC</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Enhanced Transformer Engine</td>\n<td class=\"tg-tleft-valign-first\">Yes, 2× Hopper</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Generation</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP64 (HPC)</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP32</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TF32</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP16/BF16</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FP8</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>FP4</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ampere</td>\n<td class=\"tg-tleft-valign-first\">1/2 FP32 rate (A100)</td>\n<td class=\"tg-tleft-valign-first\">Dual datapath, +2× Turing</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Tensor Core, ~312 TFLOPS</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Hopper</td>\n<td class=\"tg-tleft-valign-first\">1/2 FP32 rate (H100)</td>\n<td class=\"tg-tleft-valign-first\">IPC + clock boost</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Transformer Engine</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ada</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-first\">High clocks</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Consumer AI workloads</td>\n<td class=\"tg-tleft-valign-first\">Some pro models</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Blackwell</td>\n<td class=\"tg-tleft-valign-first\">1/2 FP32 rate (B100/B200)</td>\n<td class=\"tg-tleft-valign-first\">Higher IPC</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Enhanced Transformer Engine</td>\n<td class=\"tg-tleft-valign-first\">Yes, 2× Hopper</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "**Generation**\n\n**FP64 (HPC)**\n\n**FP32**\n\n**TF32**\n\n**FP16/BF16**\n\n**FP8**\n\n**FP4**\n\nAmpere\n\n1/2 FP32 rate (A100)\n\nDual datapath, +2× Turing\n\nYes\n\nTensor Core, ~312 TFLOPS\n\nNo\n\nNo\n\nHopper\n\n1/2 FP32 rate (H100)\n\nIPC + clock boost\n\nYes\n\nTransformer Engine\n\nYes\n\nNo\n\nAda\n\nLimited\n\nHigh clocks\n\nYes\n\nConsumer AI workloads\n\nSome pro models\n\nNo\n\nBlackwell\n\n1/2 FP32 rate (B100/B200)\n\nHigher IPC\n\nYes\n\nEnhanced Transformer Engine\n\nYes, 2× Hopper\n\nYes\n\n**Generation**\n\n**FP64 (HPC)**\n\n**FP32**\n\n**TF32**\n\n**FP16/BF16**\n\n**FP8**\n\n**FP4**\n\nAmpere\n\n1/2 FP32 rate (A100)\n\nDual datapath, +2× Turing\n\nYes\n\nTensor Core, ~312 TFLOPS\n\nNo\n\nNo\n\nHopper\n\n1/2 FP32 rate (H100)\n\nIPC + clock boost\n\nYes\n\nTransformer Engine\n\nYes\n\nNo\n\nAda\n\nLimited\n\nHigh clocks\n\nYes\n\nConsumer AI workloads\n\nSome pro models\n\nNo\n\nBlackwell\n\n1/2 FP32 rate (B100/B200)\n\nHigher IPC\n\nYes\n\nEnhanced Transformer Engine\n\nYes, 2× Hopper\n\nYes",
      "order": 27,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "transformer"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 130,
        "contentLength": 4031
      },
      "nextCards": [
        "ai-gpu-architecture-vram-technologies-and-bandwidth-28",
        "ai-gpu-architecture-cache-hierarchy-changes-29"
      ],
      "relatedCards": [
        "ai-ml-runtimes-pros-and-cons-42",
        "ai-ml-runtimes-architecture-21",
        "ai-ml-runtimes-llamacpp-deep-dive-38",
        "ai-ml-runtimes-suitable-applications-44",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#performance-analysis",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-vram-technologies-and-bandwidth-28",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Memory Architecture Evolution",
      "title": "VRAM Technologies and Bandwidth",
      "subtitle": "Memory Architecture Evolution",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Ampere (2020)</strong>:</p>\n\n    <ul>\n      <li><strong>Data center (A100):</strong> Used <strong>HBM2e</strong> with up to 1.6 TB/s bandwidth.</li>\n      <li><strong>Consumer (RTX 30-series):</strong> Used <strong>GDDR6X</strong> (developed with Micron) on higher-end cards for &gt;900 GB/s bandwidth (RTX 3090).</li>\n      <li><strong>Bus widths:</strong> Up to 384-bit on flagship consumer GPUs.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hopper (2022)</strong>:</p>\n\n    <ul>\n      <li><strong>H100:</strong> Used <strong>HBM3</strong> in some SKUs, offering up to 3 TB/s bandwidth.</li>\n      <li>Support for larger memory capacities per GPU package (up to 80 GB HBM3).</li>\n      <li>Designed for massive AI model parameter storage in-memory.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Ada Lovelace (2022)</strong>:</p>\n\n    <ul>\n      <li><strong>Consumer focus:</strong> GDDR6X on high-end cards, GDDR6 on midrange.</li>\n      <li>Bandwidth efficiency improved with <strong>L2 cache enlargement</strong>, reducing VRAM fetch pressure.</li>\n      <li>Top models (RTX 4090) reached 1 TB/s effective bandwidth.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Blackwell (2024)</strong>:</p>\n\n    <ul>\n      <li><strong>Data center (B100/B200):</strong> HBM3e with &gt;4 TB/s bandwidth in top configurations.</li>\n      <li><strong>Consumer:</strong> Higher-speed GDDR7 for &gt;1.2 TB/s bandwidth on enthusiast cards.</li>\n      <li>Improved memory controllers for lower latency in AI workloads.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Ampere (2020)</strong>:</p>\n<ul>\n      <li><strong>Data center (A100):</strong> Used <strong>HBM2e</strong> with up to 1.6 TB/s bandwidth.</li>\n      <li><strong>Consumer (RTX 30-series):</strong> Used <strong>GDDR6X</strong> (developed with Micron) on higher-end cards for &gt;900 GB/s bandwidth (RTX 3090).</li>\n      <li><strong>Bus widths:</strong> Up to 384-bit on flagship consumer GPUs.</li>\n    </ul>\n<p><strong>Hopper (2022)</strong>:</p>\n<ul>\n      <li><strong>H100:</strong> Used <strong>HBM3</strong> in some SKUs, offering up to 3 TB/s bandwidth.</li>\n      <li>Support for larger memory capacities per GPU package (up to 80 GB HBM3).</li>\n      <li>Designed for massive AI model parameter storage in-memory.</li>\n    </ul>\n<p><strong>Ada Lovelace (2022)</strong>:</p>\n<ul>\n      <li><strong>Consumer focus:</strong> GDDR6X on high-end cards, GDDR6 on midrange.</li>\n      <li>Bandwidth efficiency improved with <strong>L2 cache enlargement</strong>, reducing VRAM fetch pressure.</li>\n      <li>Top models (RTX 4090) reached 1 TB/s effective bandwidth.</li>\n    </ul>\n<p><strong>Blackwell (2024)</strong>:</p>\n<ul>\n      <li><strong>Data center (B100/B200):</strong> HBM3e with &gt;4 TB/s bandwidth in top configurations.</li>\n      <li><strong>Consumer:</strong> Higher-speed GDDR7 for &gt;1.2 TB/s bandwidth on enthusiast cards.</li>\n      <li>Improved memory controllers for lower latency in AI workloads.</li>\n    </ul>",
      "contentMarkdown": "*   **Ampere (2020)**:\n    \n    *   **Data center (A100):** Used **HBM2e** with up to 1.6 TB/s bandwidth.\n    *   **Consumer (RTX 30-series):** Used **GDDR6X** (developed with Micron) on higher-end cards for >900 GB/s bandwidth (RTX 3090).\n    *   **Bus widths:** Up to 384-bit on flagship consumer GPUs.\n*   **Hopper (2022)**:\n    \n    *   **H100:** Used **HBM3** in some SKUs, offering up to 3 TB/s bandwidth.\n    *   Support for larger memory capacities per GPU package (up to 80 GB HBM3).\n    *   Designed for massive AI model parameter storage in-memory.\n*   **Ada Lovelace (2022)**:\n    \n    *   **Consumer focus:** GDDR6X on high-end cards, GDDR6 on midrange.\n    *   Bandwidth efficiency improved with **L2 cache enlargement**, reducing VRAM fetch pressure.\n    *   Top models (RTX 4090) reached 1 TB/s effective bandwidth.\n*   **Blackwell (2024)**:\n    \n    *   **Data center (B100/B200):** HBM3e with >4 TB/s bandwidth in top configurations.\n    *   **Consumer:** Higher-speed GDDR7 for >1.2 TB/s bandwidth on enthusiast cards.\n    *   Improved memory controllers for lower latency in AI workloads.\n\n**Ampere (2020)**:\n\n*   **Data center (A100):** Used **HBM2e** with up to 1.6 TB/s bandwidth.\n*   **Consumer (RTX 30-series):** Used **GDDR6X** (developed with Micron) on higher-end cards for >900 GB/s bandwidth (RTX 3090).\n*   **Bus widths:** Up to 384-bit on flagship consumer GPUs.\n\n**Hopper (2022)**:\n\n*   **H100:** Used **HBM3** in some SKUs, offering up to 3 TB/s bandwidth.\n*   Support for larger memory capacities per GPU package (up to 80 GB HBM3).\n*   Designed for massive AI model parameter storage in-memory.\n\n**Ada Lovelace (2022)**:\n\n*   **Consumer focus:** GDDR6X on high-end cards, GDDR6 on midrange.\n*   Bandwidth efficiency improved with **L2 cache enlargement**, reducing VRAM fetch pressure.\n*   Top models (RTX 4090) reached 1 TB/s effective bandwidth.\n\n**Blackwell (2024)**:\n\n*   **Data center (B100/B200):** HBM3e with >4 TB/s bandwidth in top configurations.\n*   **Consumer:** Higher-speed GDDR7 for >1.2 TB/s bandwidth on enthusiast cards.\n*   Improved memory controllers for lower latency in AI workloads.",
      "order": 28,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 304,
        "contentLength": 3004
      },
      "nextCards": [
        "ai-gpu-architecture-cache-hierarchy-changes-29",
        "ai-gpu-architecture-memory-latency-and-efficiency-improvements-30"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#vram-technologies-and-bandwidth",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-cache-hierarchy-changes-29",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Memory Architecture Evolution",
      "title": "Cache Hierarchy Changes",
      "subtitle": "Memory Architecture Evolution",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Ampere</strong>:</p>\n\n    <ul>\n      <li>L2 cache sizes: up to 40 MB on A100, smaller on consumer (~6 MB).</li>\n      <li>Configurable L1/shared memory up to 192 KB per SM.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hopper</strong>:</p>\n\n    <ul>\n      <li>L2 cache: 50 MB on H100.</li>\n      <li>L1/shared memory bandwidth doubled compared to Ampere.</li>\n      <li>Improved cache coherence across NVLink-connected GPUs.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Ada</strong>:</p>\n\n    <ul>\n      <li>Significantly larger L2 cache for consumer GPUs (72 MB on RTX 4090).</li>\n      <li>Reduced VRAM dependence for gaming workloads.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Blackwell</strong>:</p>\n\n    <ul>\n      <li>Unified large L2 (100+ MB) for data center GPUs.</li>\n      <li>Faster shared memory with AI-aware prefetching.</li>\n      <li>AI-managed cache policies to keep transformer weights resident.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Ampere</strong>:</p>\n<ul>\n      <li>L2 cache sizes: up to 40 MB on A100, smaller on consumer (~6 MB).</li>\n      <li>Configurable L1/shared memory up to 192 KB per SM.</li>\n    </ul>\n<p><strong>Hopper</strong>:</p>\n<ul>\n      <li>L2 cache: 50 MB on H100.</li>\n      <li>L1/shared memory bandwidth doubled compared to Ampere.</li>\n      <li>Improved cache coherence across NVLink-connected GPUs.</li>\n    </ul>\n<p><strong>Ada</strong>:</p>\n<ul>\n      <li>Significantly larger L2 cache for consumer GPUs (72 MB on RTX 4090).</li>\n      <li>Reduced VRAM dependence for gaming workloads.</li>\n    </ul>\n<p><strong>Blackwell</strong>:</p>\n<ul>\n      <li>Unified large L2 (100+ MB) for data center GPUs.</li>\n      <li>Faster shared memory with AI-aware prefetching.</li>\n      <li>AI-managed cache policies to keep transformer weights resident.</li>\n    </ul>",
      "contentMarkdown": "*   **Ampere**:\n    \n    *   L2 cache sizes: up to 40 MB on A100, smaller on consumer (~6 MB).\n    *   Configurable L1/shared memory up to 192 KB per SM.\n*   **Hopper**:\n    \n    *   L2 cache: 50 MB on H100.\n    *   L1/shared memory bandwidth doubled compared to Ampere.\n    *   Improved cache coherence across NVLink-connected GPUs.\n*   **Ada**:\n    \n    *   Significantly larger L2 cache for consumer GPUs (72 MB on RTX 4090).\n    *   Reduced VRAM dependence for gaming workloads.\n*   **Blackwell**:\n    \n    *   Unified large L2 (100+ MB) for data center GPUs.\n    *   Faster shared memory with AI-aware prefetching.\n    *   AI-managed cache policies to keep transformer weights resident.\n\n**Ampere**:\n\n*   L2 cache sizes: up to 40 MB on A100, smaller on consumer (~6 MB).\n*   Configurable L1/shared memory up to 192 KB per SM.\n\n**Hopper**:\n\n*   L2 cache: 50 MB on H100.\n*   L1/shared memory bandwidth doubled compared to Ampere.\n*   Improved cache coherence across NVLink-connected GPUs.\n\n**Ada**:\n\n*   Significantly larger L2 cache for consumer GPUs (72 MB on RTX 4090).\n*   Reduced VRAM dependence for gaming workloads.\n\n**Blackwell**:\n\n*   Unified large L2 (100+ MB) for data center GPUs.\n*   Faster shared memory with AI-aware prefetching.\n*   AI-managed cache policies to keep transformer weights resident.",
      "order": 29,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "transformer"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 198,
        "contentLength": 1838
      },
      "nextCards": [
        "ai-gpu-architecture-memory-latency-and-efficiency-improvements-30",
        "ai-gpu-architecture-multi-gpu-memory-coherency-31"
      ],
      "relatedCards": [
        "ai-ml-runtimes-pros-and-cons-42",
        "ai-ml-runtimes-architecture-21",
        "ai-ml-runtimes-llamacpp-deep-dive-38",
        "ai-ml-runtimes-suitable-applications-44",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#cache-hierarchy-changes",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-memory-latency-and-efficiency-improvements-30",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Memory Architecture Evolution",
      "title": "Memory Latency and Efficiency Improvements",
      "subtitle": "Memory Architecture Evolution",
      "contentHtml": "<ul>\n  <li><strong>Ampere:</strong> Introduced <strong>structured sparsity</strong> support in Tensor Cores, reducing VRAM traffic.</li>\n  <li><strong>Hopper:</strong> Added <strong>Transformer Engine</strong>, dynamically choosing precision to reduce memory footprint.</li>\n  <li><strong>Ada:</strong> Focused on L2 cache expansion to mask VRAM latency.</li>\n  <li><strong>Blackwell:</strong> Integrated <strong>streaming memory partitioning</strong> for large model inference; data preloaded into SM-local caches before execution.</li>\n</ul>",
      "contentMarkdown": "*   **Ampere:** Introduced **structured sparsity** support in Tensor Cores, reducing VRAM traffic.\n*   **Hopper:** Added **Transformer Engine**, dynamically choosing precision to reduce memory footprint.\n*   **Ada:** Focused on L2 cache expansion to mask VRAM latency.\n*   **Blackwell:** Integrated **streaming memory partitioning** for large model inference; data preloaded into SM-local caches before execution.",
      "order": 30,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "transformer"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 52,
        "contentLength": 543
      },
      "nextCards": [
        "ai-gpu-architecture-multi-gpu-memory-coherency-31",
        "ai-gpu-architecture-fundamental-architectural-components-amd-32"
      ],
      "relatedCards": [
        "ai-ml-runtimes-pros-and-cons-42",
        "ai-ml-runtimes-architecture-21",
        "ai-ml-runtimes-llamacpp-deep-dive-38",
        "ai-ml-runtimes-suitable-applications-44",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#memory-latency-and-efficiency-improvements",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-multi-gpu-memory-coherency-31",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "Memory Architecture Evolution",
      "title": "Multi-GPU Memory Coherency",
      "subtitle": "Memory Architecture Evolution",
      "contentHtml": "<ul>\n  <li><strong>Ampere:</strong> NVLink 3.0 with 600 GB/s bidirectional bandwidth, partial memory coherency.</li>\n  <li><strong>Hopper:</strong> NVLink 4.0 with 900 GB/s bandwidth, full memory coherency across up to 256 GPUs in NVSwitch topologies.</li>\n  <li><strong>Ada:</strong> NVLink limited or absent in consumer cards.</li>\n  <li><strong>Blackwell:</strong> NVLink 5.0, &gt;1 TB/s, improved for multi-node AI training with direct GPU-to-GPU streaming.</li>\n</ul>",
      "contentMarkdown": "*   **Ampere:** NVLink 3.0 with 600 GB/s bidirectional bandwidth, partial memory coherency.\n*   **Hopper:** NVLink 4.0 with 900 GB/s bandwidth, full memory coherency across up to 256 GPUs in NVSwitch topologies.\n*   **Ada:** NVLink limited or absent in consumer cards.\n*   **Blackwell:** NVLink 5.0, >1 TB/s, improved for multi-node AI training with direct GPU-to-GPU streaming.",
      "order": 31,
      "orderInChapter": 4,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 55,
        "contentLength": 472
      },
      "nextCards": [
        "ai-gpu-architecture-fundamental-architectural-components-amd-32",
        "ai-gpu-architecture-execution-paradigms-amd-33"
      ],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-ml-runtimes-example-workflow-7",
        "ai-ml-runtimes-overview-9",
        "ai-ml-runtimes-implementation-details-12"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#multi-gpu-memory-coherency",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-fundamental-architectural-components-amd-32",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "AMD GPU Architecture Overview and Comparison with NVIDIA",
      "title": "Fundamental Architectural Components (AMD)",
      "subtitle": "AMD GPU Architecture Overview and Comparison with NVIDIA",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Compute Units (CUs)</strong>:</p>\n\n    <ul>\n      <li>AMD’s equivalent of NVIDIA’s Streaming Multiprocessors (SMs).</li>\n      <li>\n        <p>Each CU contains:</p>\n\n        <ul>\n          <li><strong>64 Stream Processors</strong> (SPs, roughly equivalent to CUDA cores)</li>\n          <li><strong>Vector ALUs</strong> for FP32/INT32</li>\n          <li><strong>Scalar Units</strong> for FP32/INT32 operations shared across the CU</li>\n          <li><strong>Matrix Cores (AI Matrix Accelerators)</strong> in newer RDNA/CDNA architectures for mixed-precision AI workloads.</li>\n        </ul>\n      </li>\n      <li>CUs are grouped into Shader Arrays and Shader Engines.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Specialized Hardware</strong>:</p>\n\n    <ul>\n      <li><strong>Ray Accelerators</strong> (since RDNA 2) — analogous to NVIDIA’s RT Cores.</li>\n      <li><strong>Matrix Cores</strong> (CDNA and RDNA 3) — compete with NVIDIA’s Tensor Cores for AI/HPC workloads.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Memory Hierarchy</strong>:</p>\n\n    <ul>\n      <li>Registers per wavefront (equivalent to warp)</li>\n      <li>Local Data Share (LDS) — similar to NVIDIA’s shared memory, software-managed.</li>\n      <li>L1 Cache and scalar caches.</li>\n      <li>L2 Cache shared across CUs.</li>\n      <li>VRAM (GDDR6, GDDR6X, or HBM depending on SKU).</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Compute Units (CUs)</strong>:</p>\n<ul>\n      <li>AMD’s equivalent of NVIDIA’s Streaming Multiprocessors (SMs).</li>\n      <li>\n        <p>Each CU contains:</p>\n\n        <ul>\n          <li><strong>64 Stream Processors</strong> (SPs, roughly equivalent to CUDA cores)</li>\n          <li><strong>Vector ALUs</strong> for FP32/INT32</li>\n          <li><strong>Scalar Units</strong> for FP32/INT32 operations shared across the CU</li>\n          <li><strong>Matrix Cores (AI Matrix Accelerators)</strong> in newer RDNA/CDNA architectures for mixed-precision AI workloads.</li>\n        </ul>\n      </li>\n      <li>CUs are grouped into Shader Arrays and Shader Engines.</li>\n    </ul>\n<p>Each CU contains:</p>\n<ul>\n          <li><strong>64 Stream Processors</strong> (SPs, roughly equivalent to CUDA cores)</li>\n          <li><strong>Vector ALUs</strong> for FP32/INT32</li>\n          <li><strong>Scalar Units</strong> for FP32/INT32 operations shared across the CU</li>\n          <li><strong>Matrix Cores (AI Matrix Accelerators)</strong> in newer RDNA/CDNA architectures for mixed-precision AI workloads.</li>\n        </ul>\n<p><strong>Specialized Hardware</strong>:</p>\n<ul>\n      <li><strong>Ray Accelerators</strong> (since RDNA 2) — analogous to NVIDIA’s RT Cores.</li>\n      <li><strong>Matrix Cores</strong> (CDNA and RDNA 3) — compete with NVIDIA’s Tensor Cores for AI/HPC workloads.</li>\n    </ul>\n<p><strong>Memory Hierarchy</strong>:</p>\n<ul>\n      <li>Registers per wavefront (equivalent to warp)</li>\n      <li>Local Data Share (LDS) — similar to NVIDIA’s shared memory, software-managed.</li>\n      <li>L1 Cache and scalar caches.</li>\n      <li>L2 Cache shared across CUs.</li>\n      <li>VRAM (GDDR6, GDDR6X, or HBM depending on SKU).</li>\n    </ul>",
      "contentMarkdown": "*   **Compute Units (CUs)**:\n    \n    *   AMD’s equivalent of NVIDIA’s Streaming Multiprocessors (SMs).\n    *   Each CU contains:\n        \n        *   **64 Stream Processors** (SPs, roughly equivalent to CUDA cores)\n        *   **Vector ALUs** for FP32/INT32\n        *   **Scalar Units** for FP32/INT32 operations shared across the CU\n        *   **Matrix Cores (AI Matrix Accelerators)** in newer RDNA/CDNA architectures for mixed-precision AI workloads.\n    *   CUs are grouped into Shader Arrays and Shader Engines.\n*   **Specialized Hardware**:\n    \n    *   **Ray Accelerators** (since RDNA 2) — analogous to NVIDIA’s RT Cores.\n    *   **Matrix Cores** (CDNA and RDNA 3) — compete with NVIDIA’s Tensor Cores for AI/HPC workloads.\n*   **Memory Hierarchy**:\n    \n    *   Registers per wavefront (equivalent to warp)\n    *   Local Data Share (LDS) — similar to NVIDIA’s shared memory, software-managed.\n    *   L1 Cache and scalar caches.\n    *   L2 Cache shared across CUs.\n    *   VRAM (GDDR6, GDDR6X, or HBM depending on SKU).\n\n**Compute Units (CUs)**:\n\n*   AMD’s equivalent of NVIDIA’s Streaming Multiprocessors (SMs).\n*   Each CU contains:\n    \n    *   **64 Stream Processors** (SPs, roughly equivalent to CUDA cores)\n    *   **Vector ALUs** for FP32/INT32\n    *   **Scalar Units** for FP32/INT32 operations shared across the CU\n    *   **Matrix Cores (AI Matrix Accelerators)** in newer RDNA/CDNA architectures for mixed-precision AI workloads.\n*   CUs are grouped into Shader Arrays and Shader Engines.\n\nEach CU contains:\n\n*   **64 Stream Processors** (SPs, roughly equivalent to CUDA cores)\n*   **Vector ALUs** for FP32/INT32\n*   **Scalar Units** for FP32/INT32 operations shared across the CU\n*   **Matrix Cores (AI Matrix Accelerators)** in newer RDNA/CDNA architectures for mixed-precision AI workloads.\n\n**Specialized Hardware**:\n\n*   **Ray Accelerators** (since RDNA 2) — analogous to NVIDIA’s RT Cores.\n*   **Matrix Cores** (CDNA and RDNA 3) — compete with NVIDIA’s Tensor Cores for AI/HPC workloads.\n\n**Memory Hierarchy**:\n\n*   Registers per wavefront (equivalent to warp)\n*   Local Data Share (LDS) — similar to NVIDIA’s shared memory, software-managed.\n*   L1 Cache and scalar caches.\n*   L2 Cache shared across CUs.\n*   VRAM (GDDR6, GDDR6X, or HBM depending on SKU).",
      "order": 32,
      "orderInChapter": 1,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 317,
        "contentLength": 3177
      },
      "nextCards": [
        "ai-gpu-architecture-execution-paradigms-amd-33",
        "ai-gpu-architecture-compute-architecture-evolution-amd-cdna-rdna-34"
      ],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-ml-runtimes-example-workflow-7",
        "ai-ml-runtimes-overview-9",
        "ai-ml-runtimes-implementation-details-12"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#fundamental-architectural-components-(amd)",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-execution-paradigms-amd-33",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "AMD GPU Architecture Overview and Comparison with NVIDIA",
      "title": "Execution Paradigms (AMD)",
      "subtitle": "AMD GPU Architecture Overview and Comparison with NVIDIA",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Wavefronts</strong>:</p>\n\n    <ul>\n      <li>AMD’s equivalent to NVIDIA’s warps — <strong>fixed at 64 threads</strong> (vs. NVIDIA’s 32).</li>\n      <li>Like warps, wavefronts execute in lockstep using SIMD lanes.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Workgroups</strong>:</p>\n\n    <ul>\n      <li>Equivalent to NVIDIA’s CTAs (Cooperative Thread Arrays).</li>\n      <li>Workgroups contain one or more wavefronts and share LDS memory.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>SIMD Execution</strong>:</p>\n\n    <ul>\n      <li>Each CU contains multiple SIMD units (typically 4×16-wide for wavefront64).</li>\n      <li>Supports predication and divergence handling similar to NVIDIA’s warp divergence model.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Scheduling</strong>:</p>\n\n    <ul>\n      <li>Hardware schedulers issue wavefront instructions to SIMD units.</li>\n      <li>AMD uses <strong>asynchronous compute</strong> extensively — multiple compute queues can be scheduled across the GPU concurrently.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Wavefronts</strong>:</p>\n<ul>\n      <li>AMD’s equivalent to NVIDIA’s warps — <strong>fixed at 64 threads</strong> (vs. NVIDIA’s 32).</li>\n      <li>Like warps, wavefronts execute in lockstep using SIMD lanes.</li>\n    </ul>\n<p><strong>Workgroups</strong>:</p>\n<ul>\n      <li>Equivalent to NVIDIA’s CTAs (Cooperative Thread Arrays).</li>\n      <li>Workgroups contain one or more wavefronts and share LDS memory.</li>\n    </ul>\n<p><strong>SIMD Execution</strong>:</p>\n<ul>\n      <li>Each CU contains multiple SIMD units (typically 4×16-wide for wavefront64).</li>\n      <li>Supports predication and divergence handling similar to NVIDIA’s warp divergence model.</li>\n    </ul>\n<p><strong>Scheduling</strong>:</p>\n<ul>\n      <li>Hardware schedulers issue wavefront instructions to SIMD units.</li>\n      <li>AMD uses <strong>asynchronous compute</strong> extensively — multiple compute queues can be scheduled across the GPU concurrently.</li>\n    </ul>",
      "contentMarkdown": "*   **Wavefronts**:\n    \n    *   AMD’s equivalent to NVIDIA’s warps — **fixed at 64 threads** (vs. NVIDIA’s 32).\n    *   Like warps, wavefronts execute in lockstep using SIMD lanes.\n*   **Workgroups**:\n    \n    *   Equivalent to NVIDIA’s CTAs (Cooperative Thread Arrays).\n    *   Workgroups contain one or more wavefronts and share LDS memory.\n*   **SIMD Execution**:\n    \n    *   Each CU contains multiple SIMD units (typically 4×16-wide for wavefront64).\n    *   Supports predication and divergence handling similar to NVIDIA’s warp divergence model.\n*   **Scheduling**:\n    \n    *   Hardware schedulers issue wavefront instructions to SIMD units.\n    *   AMD uses **asynchronous compute** extensively — multiple compute queues can be scheduled across the GPU concurrently.\n\n**Wavefronts**:\n\n*   AMD’s equivalent to NVIDIA’s warps — **fixed at 64 threads** (vs. NVIDIA’s 32).\n*   Like warps, wavefronts execute in lockstep using SIMD lanes.\n\n**Workgroups**:\n\n*   Equivalent to NVIDIA’s CTAs (Cooperative Thread Arrays).\n*   Workgroups contain one or more wavefronts and share LDS memory.\n\n**SIMD Execution**:\n\n*   Each CU contains multiple SIMD units (typically 4×16-wide for wavefront64).\n*   Supports predication and divergence handling similar to NVIDIA’s warp divergence model.\n\n**Scheduling**:\n\n*   Hardware schedulers issue wavefront instructions to SIMD units.\n*   AMD uses **asynchronous compute** extensively — multiple compute queues can be scheduled across the GPU concurrently.",
      "order": 33,
      "orderInChapter": 2,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 198,
        "contentLength": 2030
      },
      "nextCards": [
        "ai-gpu-architecture-compute-architecture-evolution-amd-cdna-rdna-34",
        "ai-gpu-architecture-floating-point-precision-performance-evolution-35"
      ],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-ml-runtimes-example-workflow-7",
        "ai-ml-runtimes-overview-9",
        "ai-ml-runtimes-implementation-details-12"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#execution-paradigms-(amd)",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-compute-architecture-evolution-amd-cdna-rdna-34",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "AMD GPU Architecture Overview and Comparison with NVIDIA",
      "title": "Compute Architecture Evolution (AMD CDNA & RDNA)",
      "subtitle": "AMD GPU Architecture Overview and Comparison with NVIDIA",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>CDNA (Data Center, HPC)</strong>:</p>\n\n    <ul>\n      <li>Focused on FP64, large HBM bandwidth, Infinity Fabric interconnect.</li>\n      <li>MI100, MI200, MI300 accelerators target AI training/inference and HPC workloads.</li>\n      <li>Incorporates AI Matrix Cores for FP16/BF16/FP8 acceleration.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>RDNA (Gaming &amp; Consumer)</strong>:</p>\n\n    <ul>\n      <li>RDNA 2 (2020) — introduced hardware Ray Accelerators.</li>\n      <li>RDNA 3 (2022) — chiplet-based design, higher clock speeds, improved efficiency, larger L0/L1 caches.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Trends</strong>:</p>\n\n    <ul>\n      <li>Gradual convergence with NVIDIA’s philosophy on specialized cores for AI and ray tracing.</li>\n      <li>CDNA increasingly mirrors NVIDIA’s HPC-first approach, while RDNA focuses on gaming + mixed AI features.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>CDNA (Data Center, HPC)</strong>:</p>\n<ul>\n      <li>Focused on FP64, large HBM bandwidth, Infinity Fabric interconnect.</li>\n      <li>MI100, MI200, MI300 accelerators target AI training/inference and HPC workloads.</li>\n      <li>Incorporates AI Matrix Cores for FP16/BF16/FP8 acceleration.</li>\n    </ul>\n<p><strong>RDNA (Gaming &amp; Consumer)</strong>:</p>\n<ul>\n      <li>RDNA 2 (2020) — introduced hardware Ray Accelerators.</li>\n      <li>RDNA 3 (2022) — chiplet-based design, higher clock speeds, improved efficiency, larger L0/L1 caches.</li>\n    </ul>\n<p><strong>Trends</strong>:</p>\n<ul>\n      <li>Gradual convergence with NVIDIA’s philosophy on specialized cores for AI and ray tracing.</li>\n      <li>CDNA increasingly mirrors NVIDIA’s HPC-first approach, while RDNA focuses on gaming + mixed AI features.</li>\n    </ul>",
      "contentMarkdown": "*   **CDNA (Data Center, HPC)**:\n    \n    *   Focused on FP64, large HBM bandwidth, Infinity Fabric interconnect.\n    *   MI100, MI200, MI300 accelerators target AI training/inference and HPC workloads.\n    *   Incorporates AI Matrix Cores for FP16/BF16/FP8 acceleration.\n*   **RDNA (Gaming & Consumer)**:\n    \n    *   RDNA 2 (2020) — introduced hardware Ray Accelerators.\n    *   RDNA 3 (2022) — chiplet-based design, higher clock speeds, improved efficiency, larger L0/L1 caches.\n*   **Trends**:\n    \n    *   Gradual convergence with NVIDIA’s philosophy on specialized cores for AI and ray tracing.\n    *   CDNA increasingly mirrors NVIDIA’s HPC-first approach, while RDNA focuses on gaming + mixed AI features.\n\n**CDNA (Data Center, HPC)**:\n\n*   Focused on FP64, large HBM bandwidth, Infinity Fabric interconnect.\n*   MI100, MI200, MI300 accelerators target AI training/inference and HPC workloads.\n*   Incorporates AI Matrix Cores for FP16/BF16/FP8 acceleration.\n\n**RDNA (Gaming & Consumer)**:\n\n*   RDNA 2 (2020) — introduced hardware Ray Accelerators.\n*   RDNA 3 (2022) — chiplet-based design, higher clock speeds, improved efficiency, larger L0/L1 caches.\n\n**Trends**:\n\n*   Gradual convergence with NVIDIA’s philosophy on specialized cores for AI and ray tracing.\n*   CDNA increasingly mirrors NVIDIA’s HPC-first approach, while RDNA focuses on gaming + mixed AI features.",
      "order": 34,
      "orderInChapter": 3,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 187,
        "contentLength": 1772
      },
      "nextCards": [
        "ai-gpu-architecture-floating-point-precision-performance-evolution-35",
        "ai-gpu-architecture-memory-architecture-evolution-36"
      ],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-ml-runtimes-example-workflow-7",
        "ai-ml-runtimes-overview-9",
        "ai-ml-runtimes-implementation-details-12"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#compute-architecture-evolution-(amd-cdna-&-rdna)",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-floating-point-precision-performance-evolution-35",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "AMD GPU Architecture Overview and Comparison with NVIDIA",
      "title": "Floating-Point Precision Performance Evolution",
      "subtitle": "AMD GPU Architecture Overview and Comparison with NVIDIA",
      "contentHtml": "<ul>\n  <li>\n    <p>AMD’s HPC GPUs (CDNA) support a similar spread of precisions:</p>\n\n    <ul>\n      <li><strong>FP64:</strong> Full-rate or half-rate in CDNA for HPC.</li>\n      <li><strong>FP32:</strong> High throughput in all architectures; RDNA targets gaming efficiency.</li>\n      <li><strong>FP16/BF16:</strong> Supported in Matrix Cores for AI workloads.</li>\n      <li><strong>FP8:</strong> Introduced in MI300 for AI inference acceleration.</li>\n      <li><strong>No FP4 yet</strong> in shipping AMD hardware (as of 2025).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>NVIDIA Comparison:</strong></p>\n\n    <ul>\n      <li>AMD lagged behind in adopting sub-16-bit formats but has now matched FP8 capabilities in data center parts.</li>\n      <li>NVIDIA maintains an edge in software-optimized mixed-precision scheduling (Transformer Engine, FP4).</li>\n    </ul>\n  </li>\n</ul>\n<p>AMD’s HPC GPUs (CDNA) support a similar spread of precisions:</p>\n<ul>\n      <li><strong>FP64:</strong> Full-rate or half-rate in CDNA for HPC.</li>\n      <li><strong>FP32:</strong> High throughput in all architectures; RDNA targets gaming efficiency.</li>\n      <li><strong>FP16/BF16:</strong> Supported in Matrix Cores for AI workloads.</li>\n      <li><strong>FP8:</strong> Introduced in MI300 for AI inference acceleration.</li>\n      <li><strong>No FP4 yet</strong> in shipping AMD hardware (as of 2025).</li>\n    </ul>\n<p><strong>NVIDIA Comparison:</strong></p>\n<ul>\n      <li>AMD lagged behind in adopting sub-16-bit formats but has now matched FP8 capabilities in data center parts.</li>\n      <li>NVIDIA maintains an edge in software-optimized mixed-precision scheduling (Transformer Engine, FP4).</li>\n    </ul>",
      "contentMarkdown": "*   AMD’s HPC GPUs (CDNA) support a similar spread of precisions:\n    \n    *   **FP64:** Full-rate or half-rate in CDNA for HPC.\n    *   **FP32:** High throughput in all architectures; RDNA targets gaming efficiency.\n    *   **FP16/BF16:** Supported in Matrix Cores for AI workloads.\n    *   **FP8:** Introduced in MI300 for AI inference acceleration.\n    *   **No FP4 yet** in shipping AMD hardware (as of 2025).\n*   **NVIDIA Comparison:**\n    \n    *   AMD lagged behind in adopting sub-16-bit formats but has now matched FP8 capabilities in data center parts.\n    *   NVIDIA maintains an edge in software-optimized mixed-precision scheduling (Transformer Engine, FP4).\n\nAMD’s HPC GPUs (CDNA) support a similar spread of precisions:\n\n*   **FP64:** Full-rate or half-rate in CDNA for HPC.\n*   **FP32:** High throughput in all architectures; RDNA targets gaming efficiency.\n*   **FP16/BF16:** Supported in Matrix Cores for AI workloads.\n*   **FP8:** Introduced in MI300 for AI inference acceleration.\n*   **No FP4 yet** in shipping AMD hardware (as of 2025).\n\n**NVIDIA Comparison:**\n\n*   AMD lagged behind in adopting sub-16-bit formats but has now matched FP8 capabilities in data center parts.\n*   NVIDIA maintains an edge in software-optimized mixed-precision scheduling (Transformer Engine, FP4).",
      "order": 35,
      "orderInChapter": 4,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "transformer"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 184,
        "contentLength": 1706
      },
      "nextCards": [
        "ai-gpu-architecture-memory-architecture-evolution-36",
        "ai-gpu-architecture-comparative-analysis-amd-vs-nvidia-architectures-37"
      ],
      "relatedCards": [
        "ai-ml-runtimes-pros-and-cons-42",
        "ai-ml-runtimes-architecture-21",
        "ai-ml-runtimes-llamacpp-deep-dive-38",
        "ai-ml-runtimes-suitable-applications-44",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#floating-point-precision-performance-evolution",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-memory-architecture-evolution-36",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "AMD GPU Architecture Overview and Comparison with NVIDIA",
      "title": "Memory Architecture Evolution",
      "subtitle": "AMD GPU Architecture Overview and Comparison with NVIDIA",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>VRAM Types</strong></p>\n\n    <ul>\n      <li><strong>RDNA:</strong> Primarily GDDR6 (with Infinity Cache to offset bandwidth needs).</li>\n      <li><strong>CDNA:</strong> HBM2/HBM2e (MI100), HBM3 (MI250), HBM3e (MI300).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cache Strategies</strong></p>\n\n    <ul>\n      <li><strong>Infinity Cache</strong> (RDNA 2 &amp; 3) — very large L3 cache (up to 128 MB) to minimize VRAM accesses in gaming workloads.</li>\n      <li>L1/L2 sizes generally smaller than NVIDIA’s in absolute terms, but Infinity Cache changes bandwidth behavior.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Interconnects</strong></p>\n\n    <ul>\n      <li><strong>Infinity Fabric</strong> — scalable, high-bandwidth interconnect across dies or GPUs.</li>\n      <li>Competes with NVIDIA NVLink; excels in multi-GPU HPC configurations.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>NVIDIA Comparison:</strong></p>\n\n    <ul>\n      <li>NVIDIA tends to rely on increasing L2 cache size and raw VRAM bandwidth;</li>\n      <li>AMD focuses on large on-die caches (Infinity Cache) to improve effective bandwidth, especially in gaming SKUs.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>VRAM Types</strong></p>\n<ul>\n      <li><strong>RDNA:</strong> Primarily GDDR6 (with Infinity Cache to offset bandwidth needs).</li>\n      <li><strong>CDNA:</strong> HBM2/HBM2e (MI100), HBM3 (MI250), HBM3e (MI300).</li>\n    </ul>\n<p><strong>Cache Strategies</strong></p>\n<ul>\n      <li><strong>Infinity Cache</strong> (RDNA 2 &amp; 3) — very large L3 cache (up to 128 MB) to minimize VRAM accesses in gaming workloads.</li>\n      <li>L1/L2 sizes generally smaller than NVIDIA’s in absolute terms, but Infinity Cache changes bandwidth behavior.</li>\n    </ul>\n<p><strong>Interconnects</strong></p>\n<ul>\n      <li><strong>Infinity Fabric</strong> — scalable, high-bandwidth interconnect across dies or GPUs.</li>\n      <li>Competes with NVIDIA NVLink; excels in multi-GPU HPC configurations.</li>\n    </ul>\n<p><strong>NVIDIA Comparison:</strong></p>\n<ul>\n      <li>NVIDIA tends to rely on increasing L2 cache size and raw VRAM bandwidth;</li>\n      <li>AMD focuses on large on-die caches (Infinity Cache) to improve effective bandwidth, especially in gaming SKUs.</li>\n    </ul>",
      "contentMarkdown": "*   **VRAM Types**\n    \n    *   **RDNA:** Primarily GDDR6 (with Infinity Cache to offset bandwidth needs).\n    *   **CDNA:** HBM2/HBM2e (MI100), HBM3 (MI250), HBM3e (MI300).\n*   **Cache Strategies**\n    \n    *   **Infinity Cache** (RDNA 2 & 3) — very large L3 cache (up to 128 MB) to minimize VRAM accesses in gaming workloads.\n    *   L1/L2 sizes generally smaller than NVIDIA’s in absolute terms, but Infinity Cache changes bandwidth behavior.\n*   **Interconnects**\n    \n    *   **Infinity Fabric** — scalable, high-bandwidth interconnect across dies or GPUs.\n    *   Competes with NVIDIA NVLink; excels in multi-GPU HPC configurations.\n*   **NVIDIA Comparison:**\n    \n    *   NVIDIA tends to rely on increasing L2 cache size and raw VRAM bandwidth;\n    *   AMD focuses on large on-die caches (Infinity Cache) to improve effective bandwidth, especially in gaming SKUs.\n\n**VRAM Types**\n\n*   **RDNA:** Primarily GDDR6 (with Infinity Cache to offset bandwidth needs).\n*   **CDNA:** HBM2/HBM2e (MI100), HBM3 (MI250), HBM3e (MI300).\n\n**Cache Strategies**\n\n*   **Infinity Cache** (RDNA 2 & 3) — very large L3 cache (up to 128 MB) to minimize VRAM accesses in gaming workloads.\n*   L1/L2 sizes generally smaller than NVIDIA’s in absolute terms, but Infinity Cache changes bandwidth behavior.\n\n**Interconnects**\n\n*   **Infinity Fabric** — scalable, high-bandwidth interconnect across dies or GPUs.\n*   Competes with NVIDIA NVLink; excels in multi-GPU HPC configurations.\n\n**NVIDIA Comparison:**\n\n*   NVIDIA tends to rely on increasing L2 cache size and raw VRAM bandwidth;\n*   AMD focuses on large on-die caches (Infinity Cache) to improve effective bandwidth, especially in gaming SKUs.",
      "order": 36,
      "orderInChapter": 5,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 238,
        "contentLength": 2280
      },
      "nextCards": [
        "ai-gpu-architecture-comparative-analysis-amd-vs-nvidia-architectures-37",
        "ai-gpu-architecture-key-takeaways-38"
      ],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-ml-runtimes-example-workflow-7",
        "ai-ml-runtimes-overview-9",
        "ai-ml-runtimes-implementation-details-12"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#memory-architecture-evolution",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-comparative-analysis-amd-vs-nvidia-architectures-37",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "AMD GPU Architecture Overview and Comparison with NVIDIA",
      "title": "Comparative Analysis: AMD vs. NVIDIA Architectures",
      "subtitle": "AMD GPU Architecture Overview and Comparison with NVIDIA",
      "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Aspect</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>AMD Approach</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>NVIDIA Approach</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Execution Unit</td>\n<td class=\"tg-tleft-valign-first\">Compute Units (64 SPs)</td>\n<td class=\"tg-tleft-valign-second\">Streaming Multiprocessors (128 CUDA cores)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Warp/Wavefront</td>\n<td class=\"tg-tleft-valign-first\">Wavefront64</td>\n<td class=\"tg-tleft-valign-second\">Warp32</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Specialized Cores</td>\n<td class=\"tg-tleft-valign-first\">Ray Accelerators, Matrix Cores</td>\n<td class=\"tg-tleft-valign-second\">RT Cores, Tensor Cores</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">AI Acceleration</td>\n<td class=\"tg-tleft-valign-first\">Matrix Cores (FP16/BF16/FP8)</td>\n<td class=\"tg-tleft-valign-second\">Tensor Cores (FP16/BF16/FP8/FP4, Transformer Engine)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">VRAM Strategy</td>\n<td class=\"tg-tleft-valign-first\">Infinity Cache + VRAM</td>\n<td class=\"tg-tleft-valign-second\">Large L2 + VRAM</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">HPC Interconnect</td>\n<td class=\"tg-tleft-valign-first\">Infinity Fabric</td>\n<td class=\"tg-tleft-valign-second\">NVLink/NVSwitch</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Sub-16-bit Formats</td>\n<td class=\"tg-tleft-valign-first\">FP8 (CDNA MI300)</td>\n<td class=\"tg-tleft-valign-second\">FP8, FP4</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Software Ecosystem</td>\n<td class=\"tg-tleft-valign-first\">ROCm, HIP</td>\n<td class=\"tg-tleft-valign-second\">CUDA, cuDNN, TensorRT</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Consumer Focus</td>\n<td class=\"tg-tleft-valign-first\">RDNA for gaming</td>\n<td class=\"tg-tleft-valign-second\">Ada for gaming</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">HPC Focus</td>\n<td class=\"tg-tleft-valign-first\">CDNA for data center</td>\n<td class=\"tg-tleft-valign-second\">Hopper/Blackwell for data center</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Aspect</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>AMD Approach</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>NVIDIA Approach</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Execution Unit</td>\n<td class=\"tg-tleft-valign-first\">Compute Units (64 SPs)</td>\n<td class=\"tg-tleft-valign-second\">Streaming Multiprocessors (128 CUDA cores)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Warp/Wavefront</td>\n<td class=\"tg-tleft-valign-first\">Wavefront64</td>\n<td class=\"tg-tleft-valign-second\">Warp32</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Specialized Cores</td>\n<td class=\"tg-tleft-valign-first\">Ray Accelerators, Matrix Cores</td>\n<td class=\"tg-tleft-valign-second\">RT Cores, Tensor Cores</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">AI Acceleration</td>\n<td class=\"tg-tleft-valign-first\">Matrix Cores (FP16/BF16/FP8)</td>\n<td class=\"tg-tleft-valign-second\">Tensor Cores (FP16/BF16/FP8/FP4, Transformer Engine)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">VRAM Strategy</td>\n<td class=\"tg-tleft-valign-first\">Infinity Cache + VRAM</td>\n<td class=\"tg-tleft-valign-second\">Large L2 + VRAM</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">HPC Interconnect</td>\n<td class=\"tg-tleft-valign-first\">Infinity Fabric</td>\n<td class=\"tg-tleft-valign-second\">NVLink/NVSwitch</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Sub-16-bit Formats</td>\n<td class=\"tg-tleft-valign-first\">FP8 (CDNA MI300)</td>\n<td class=\"tg-tleft-valign-second\">FP8, FP4</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Software Ecosystem</td>\n<td class=\"tg-tleft-valign-first\">ROCm, HIP</td>\n<td class=\"tg-tleft-valign-second\">CUDA, cuDNN, TensorRT</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Consumer Focus</td>\n<td class=\"tg-tleft-valign-first\">RDNA for gaming</td>\n<td class=\"tg-tleft-valign-second\">Ada for gaming</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">HPC Focus</td>\n<td class=\"tg-tleft-valign-first\">CDNA for data center</td>\n<td class=\"tg-tleft-valign-second\">Hopper/Blackwell for data center</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "**Aspect**\n\n**AMD Approach**\n\n**NVIDIA Approach**\n\nExecution Unit\n\nCompute Units (64 SPs)\n\nStreaming Multiprocessors (128 CUDA cores)\n\nWarp/Wavefront\n\nWavefront64\n\nWarp32\n\nSpecialized Cores\n\nRay Accelerators, Matrix Cores\n\nRT Cores, Tensor Cores\n\nAI Acceleration\n\nMatrix Cores (FP16/BF16/FP8)\n\nTensor Cores (FP16/BF16/FP8/FP4, Transformer Engine)\n\nVRAM Strategy\n\nInfinity Cache + VRAM\n\nLarge L2 + VRAM\n\nHPC Interconnect\n\nInfinity Fabric\n\nNVLink/NVSwitch\n\nSub-16-bit Formats\n\nFP8 (CDNA MI300)\n\nFP8, FP4\n\nSoftware Ecosystem\n\nROCm, HIP\n\nCUDA, cuDNN, TensorRT\n\nConsumer Focus\n\nRDNA for gaming\n\nAda for gaming\n\nHPC Focus\n\nCDNA for data center\n\nHopper/Blackwell for data center\n\n**Aspect**\n\n**AMD Approach**\n\n**NVIDIA Approach**\n\nExecution Unit\n\nCompute Units (64 SPs)\n\nStreaming Multiprocessors (128 CUDA cores)\n\nWarp/Wavefront\n\nWavefront64\n\nWarp32\n\nSpecialized Cores\n\nRay Accelerators, Matrix Cores\n\nRT Cores, Tensor Cores\n\nAI Acceleration\n\nMatrix Cores (FP16/BF16/FP8)\n\nTensor Cores (FP16/BF16/FP8/FP4, Transformer Engine)\n\nVRAM Strategy\n\nInfinity Cache + VRAM\n\nLarge L2 + VRAM\n\nHPC Interconnect\n\nInfinity Fabric\n\nNVLink/NVSwitch\n\nSub-16-bit Formats\n\nFP8 (CDNA MI300)\n\nFP8, FP4\n\nSoftware Ecosystem\n\nROCm, HIP\n\nCUDA, cuDNN, TensorRT\n\nConsumer Focus\n\nRDNA for gaming\n\nAda for gaming\n\nHPC Focus\n\nCDNA for data center\n\nHopper/Blackwell for data center",
      "order": 37,
      "orderInChapter": 6,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "transformer"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 172,
        "contentLength": 4359
      },
      "nextCards": [
        "ai-gpu-architecture-key-takeaways-38"
      ],
      "relatedCards": [
        "ai-ml-runtimes-pros-and-cons-42",
        "ai-ml-runtimes-architecture-21",
        "ai-ml-runtimes-llamacpp-deep-dive-38",
        "ai-ml-runtimes-suitable-applications-44",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#comparative-analysis:-amd-vs.-nvidia-architectures",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-gpu-architecture-key-takeaways-38",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "GPU Architecture",
      "articleSlug": "gpu-architecture",
      "chapter": "AMD GPU Architecture Overview and Comparison with NVIDIA",
      "title": "Key Takeaways",
      "subtitle": "AMD GPU Architecture Overview and Comparison with NVIDIA",
      "contentHtml": "<ol>\n  <li><strong>Warp/Wavefront Size Difference:</strong> NVIDIA’s 32-thread warps offer finer granularity, while AMD’s 64-thread wavefronts can be more efficient in highly parallel workloads but risk more idle lanes under divergence.</li>\n  <li><strong>Memory Philosophy:</strong> NVIDIA favors enlarging L2 cache and maximizing bandwidth; AMD offsets narrower VRAM interfaces with large on-die Infinity Cache.</li>\n  <li><strong>AI Focus:</strong> NVIDIA’s Transformer Engine and <code class=\"language-plaintext highlighter-rouge\">float4</code> give it a precision flexibility edge for LLM workloads, while AMD is catching up rapidly in FP8 inference.</li>\n  <li><strong>Interconnect Strategy:</strong> Both offer high-bandwidth interconnects (Infinity Fabric vs. NVLink), but NVIDIA currently scales to larger GPU counts in a single coherent memory space.</li>\n  <li><strong>Software Ecosystem:</strong> NVIDIA’s CUDA ecosystem remains more mature, but AMD’s ROCm stack has made major strides in HPC adoption.</li>\n</ol>",
      "contentMarkdown": "1.  **Warp/Wavefront Size Difference:** NVIDIA’s 32-thread warps offer finer granularity, while AMD’s 64-thread wavefronts can be more efficient in highly parallel workloads but risk more idle lanes under divergence.\n2.  **Memory Philosophy:** NVIDIA favors enlarging L2 cache and maximizing bandwidth; AMD offsets narrower VRAM interfaces with large on-die Infinity Cache.\n3.  **AI Focus:** NVIDIA’s Transformer Engine and `float4` give it a precision flexibility edge for LLM workloads, while AMD is catching up rapidly in FP8 inference.\n4.  **Interconnect Strategy:** Both offer high-bandwidth interconnects (Infinity Fabric vs. NVLink), but NVIDIA currently scales to larger GPU counts in a single coherent memory space.\n5.  **Software Ecosystem:** NVIDIA’s CUDA ecosystem remains more mature, but AMD’s ROCm stack has made major strides in HPC adoption.",
      "order": 38,
      "orderInChapter": 7,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "transformer",
        "llm"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 121,
        "contentLength": 1025
      },
      "nextCards": [],
      "relatedCards": [
        "ai-ml-runtimes-llamacpp-deep-dive-38",
        "ai-top-30-papers-better-faster-large-language-models-via-multi-toke-27",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-comparative-summary-and-guidance-53",
        "ai-ml-runtimes-gguf-gpt-generated-ggml-unified-format-58"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/gpu-architecture/#key-takeaways",
      "scrapedAt": "2025-12-28T11:56:41.455Z",
      "siblings": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3",
        "ai-gpu-architecture-ray-tracing-rt-cores-4",
        "ai-gpu-architecture-memory-hierarchy-5"
      ]
    },
    {
      "id": "ai-model-debugging-debugging-model-training-1",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Debugging Model Training",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>Debugging machine learning models is a critical process for identifying and resolving issues that affect the models’ performance and reliability.</li>\n  <li>Debugging involves a systematic approach to looking at each component of the ML development pipeline, from the data to the model architecture and its hyperparameters, to the predicted output.</li>\n  <li>By performing sanity checks, examining performance metrics, and scrutinizing error patterns, practitioners can uncover common issues like overfitting, underfitting, data leakage, or feature selection problems. Debugging also involves leveraging visualizations, interpreting model outputs, and seeking domain expertise to gain deeper insights into the model’s behavior. Debugging ML models is important for avoiding costly mistakes, improving performance, and enhancing interpretability and explainability.</li>\n</ul>",
      "contentMarkdown": "*   Debugging machine learning models is a critical process for identifying and resolving issues that affect the models’ performance and reliability.\n*   Debugging involves a systematic approach to looking at each component of the ML development pipeline, from the data to the model architecture and its hyperparameters, to the predicted output.\n*   By performing sanity checks, examining performance metrics, and scrutinizing error patterns, practitioners can uncover common issues like overfitting, underfitting, data leakage, or feature selection problems. Debugging also involves leveraging visualizations, interpreting model outputs, and seeking domain expertise to gain deeper insights into the model’s behavior. Debugging ML models is important for avoiding costly mistakes, improving performance, and enhancing interpretability and explainability.",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "machine learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 113,
        "contentLength": 887
      },
      "nextCards": [
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3"
      ],
      "relatedCards": [
        "ai-top-30-papers-a-tutorial-introduction-to-the-minimum-description-23",
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-top-30-papers-machine-super-intelligence-24",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#debugging-model-training",
      "scrapedAt": "2025-12-28T11:56:46.429Z",
      "siblings": [
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-model-debugging-pipeline-issues-6"
      ]
    },
    {
      "id": "ai-model-debugging-framework-for-debugging-2",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Framework for Debugging",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li><a href=\"https://truera.com/how-to-debug-ml-model-performance-a-framework/\">Truera</a> detailed a general method of debugging ML models:\n    <ol>\n      <li><strong>Evaluate the model performance on relevant data:</strong>\n        <ul>\n          <li>To effectively debug performance issues in a machine learning model, it is crucial to evaluate the model’s performance on relevant data and understand the context of the problem. Here are key considerations for debugging based on different scenarios:\n            <ul>\n              <li><strong>Low training accuracy:</strong> If the model’s accuracy on the training data is low, it may indicate that the model is underfitting or that the relationship between the inputs and outputs is inherently noisy. Insufficient data may also be a factor.</li>\n              <li><strong>Low test accuracy:</strong> If the model’s accuracy on the test data is low, check the accuracy on the training set as well. If the training accuracy is also low, it suggests issues like the model underfitting or inherent noise in the data. However, high training accuracy with low test accuracy is indicative of overfitting, which can be addressed through regularization techniques.</li>\n              <li><strong>Performance drop across datasets:</strong> If the model’s performance drops when evaluated on different datasets, it could be a sign of overfitting or a change in the data or label distribution. If there are shifts in the input-output relationship or concept drift, retraining the model on new data may be necessary.</li>\n              <li><strong>Poor performance on a specific segment:</strong> Evaluate the significance of the problem segment. If it represents a large portion of the population, investigate if the model performed well on this segment during training or testing. For smaller segments, consider if there was enough data to effectively model them.</li>\n              <li><strong>Debugging single-point errors:</strong> To debug misclassifications or errors for individual data points, employ local explanation techniques. Examine the features that influenced the model’s prediction and assess their reasonableness. Analyzing the model’s performance in the neighborhood of the point of interest can provide additional insights.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n      <li><strong>Narrow the problem scope:</strong>\n        <ul>\n          <li>Once you have identified the datasets where the model performs poorly, it is important to narrow down the problem scope and investigate specific regions or instances of poor performance. Treating the dataset as a single unit may not provide actionable insights, as the model’s performance may vary across different regions or data points.</li>\n          <li>By breaking down the problem, you can look for patterns or anomalous signals that are associated with the occurrence of poor performance. This could include examining data quality issues such as missing values, changes in the ground truth rate, shifts in data values, bugs in the data processing pipeline, or even external events that might have influenced the data.</li>\n          <li>By identifying these specific factors or signals that correlate with the problem, you can gain a better understanding of the underlying causes and develop a targeted action plan to address them. This approach allows for more focused debugging and troubleshooting, increasing the chances of effectively resolving the performance issues in the model.</li>\n        </ul>\n      </li>\n      <li><strong>Analyze the root cause of the problem:</strong>\n        <ul>\n          <li>To further investigate the performance issues, it is important to identify the root cause of the problem. Start by examining whether the regions with high errors are well-represented in the training data. If these regions are underrepresented, the model may not have enough examples to learn the patterns effectively. In such cases, increasing the coverage of training data in those regions may help improve performance.</li>\n          <li>If the problematic regions are well-covered in the training set, other factors may be contributing to the low model performance, such as noisy labels. It is also crucial to analyze whether there is any data drift, which can be done by studying the distribution of each feature and assessing if the model’s performance is disproportionately worse on “newer” data points. Additionally, investigate whether there are changes in the label distribution alongside data shifts, as this may indicate concept drift.</li>\n          <li>Furthermore, utilize explanation frameworks to gain insights into the model’s decision-making process. Identify the features that are driving the model predictions and assess whether these features align with the expected conceptual understanding of the problem. Monitor if these features are changing over time, as it can indicate the need for model adaptation to evolving patterns. Pay attention to specific feature values that correlate with lower model performance, as this might indicate overfitting or the model relying excessively on certain ranges of feature values.</li>\n          <li>By understanding the root cause of the performance issues and investigating factors like data representation, data drift, label distribution, and feature importance, you can develop targeted strategies to improve the model’s performance and address any underlying issues.</li>\n        </ul>\n      </li>\n      <li><strong>Mitigate the issue:</strong>\n        <ul>\n          <li>Once the root cause of the performance issues has been identified, it is essential to select appropriate mitigation strategies based on the analysis. Here are some insights from the root cause analysis along with corresponding amelioration strategies:</li>\n          <li>If specific features are strongly contributing to the low performance, consider removing or regularizing those features to mitigate overfitting on noisy values. However, carefully evaluate the overall influence of the feature on model predictions, as important features may naturally contribute to errors even when the model is performing well.</li>\n          <li>In cases of data or concept drift, retraining the model is necessary. Add representative data or labels that capture the shifted patterns to the original training data. Alternatively, incorporate alternative data sources that better align with the factors causing the drift.</li>\n          <li>Noisy labels that do not match similar points indicate potential mislabelling errors in the dataset. Identify these instances and involve human experts to verify and correct the labels.</li>\n          <li>By aligning the chosen mitigation strategies with the insights gained from the root cause analysis, model developers can effectively debug and improve their models in a more targeted manner. This approach enables more precise and tailored actions to address the underlying issues and enhance the overall performance of the model.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>The image below <a href=\"https://truera.com/how-to-debug-ml-model-performance-a-framework/\">(source)</a> displays the overall general debugging framework that can be leveraged for finding bugs in ML models.</li>\n</ul>\n<ol>\n      <li><strong>Evaluate the model performance on relevant data:</strong>\n        <ul>\n          <li>To effectively debug performance issues in a machine learning model, it is crucial to evaluate the model’s performance on relevant data and understand the context of the problem. Here are key considerations for debugging based on different scenarios:\n            <ul>\n              <li><strong>Low training accuracy:</strong> If the model’s accuracy on the training data is low, it may indicate that the model is underfitting or that the relationship between the inputs and outputs is inherently noisy. Insufficient data may also be a factor.</li>\n              <li><strong>Low test accuracy:</strong> If the model’s accuracy on the test data is low, check the accuracy on the training set as well. If the training accuracy is also low, it suggests issues like the model underfitting or inherent noise in the data. However, high training accuracy with low test accuracy is indicative of overfitting, which can be addressed through regularization techniques.</li>\n              <li><strong>Performance drop across datasets:</strong> If the model’s performance drops when evaluated on different datasets, it could be a sign of overfitting or a change in the data or label distribution. If there are shifts in the input-output relationship or concept drift, retraining the model on new data may be necessary.</li>\n              <li><strong>Poor performance on a specific segment:</strong> Evaluate the significance of the problem segment. If it represents a large portion of the population, investigate if the model performed well on this segment during training or testing. For smaller segments, consider if there was enough data to effectively model them.</li>\n              <li><strong>Debugging single-point errors:</strong> To debug misclassifications or errors for individual data points, employ local explanation techniques. Examine the features that influenced the model’s prediction and assess their reasonableness. Analyzing the model’s performance in the neighborhood of the point of interest can provide additional insights.</li>\n            </ul>\n          </li>\n        </ul>\n      </li>\n      <li><strong>Narrow the problem scope:</strong>\n        <ul>\n          <li>Once you have identified the datasets where the model performs poorly, it is important to narrow down the problem scope and investigate specific regions or instances of poor performance. Treating the dataset as a single unit may not provide actionable insights, as the model’s performance may vary across different regions or data points.</li>\n          <li>By breaking down the problem, you can look for patterns or anomalous signals that are associated with the occurrence of poor performance. This could include examining data quality issues such as missing values, changes in the ground truth rate, shifts in data values, bugs in the data processing pipeline, or even external events that might have influenced the data.</li>\n          <li>By identifying these specific factors or signals that correlate with the problem, you can gain a better understanding of the underlying causes and develop a targeted action plan to address them. This approach allows for more focused debugging and troubleshooting, increasing the chances of effectively resolving the performance issues in the model.</li>\n        </ul>\n      </li>\n      <li><strong>Analyze the root cause of the problem:</strong>\n        <ul>\n          <li>To further investigate the performance issues, it is important to identify the root cause of the problem. Start by examining whether the regions with high errors are well-represented in the training data. If these regions are underrepresented, the model may not have enough examples to learn the patterns effectively. In such cases, increasing the coverage of training data in those regions may help improve performance.</li>\n          <li>If the problematic regions are well-covered in the training set, other factors may be contributing to the low model performance, such as noisy labels. It is also crucial to analyze whether there is any data drift, which can be done by studying the distribution of each feature and assessing if the model’s performance is disproportionately worse on “newer” data points. Additionally, investigate whether there are changes in the label distribution alongside data shifts, as this may indicate concept drift.</li>\n          <li>Furthermore, utilize explanation frameworks to gain insights into the model’s decision-making process. Identify the features that are driving the model predictions and assess whether these features align with the expected conceptual understanding of the problem. Monitor if these features are changing over time, as it can indicate the need for model adaptation to evolving patterns. Pay attention to specific feature values that correlate with lower model performance, as this might indicate overfitting or the model relying excessively on certain ranges of feature values.</li>\n          <li>By understanding the root cause of the performance issues and investigating factors like data representation, data drift, label distribution, and feature importance, you can develop targeted strategies to improve the model’s performance and address any underlying issues.</li>\n        </ul>\n      </li>\n      <li><strong>Mitigate the issue:</strong>\n        <ul>\n          <li>Once the root cause of the performance issues has been identified, it is essential to select appropriate mitigation strategies based on the analysis. Here are some insights from the root cause analysis along with corresponding amelioration strategies:</li>\n          <li>If specific features are strongly contributing to the low performance, consider removing or regularizing those features to mitigate overfitting on noisy values. However, carefully evaluate the overall influence of the feature on model predictions, as important features may naturally contribute to errors even when the model is performing well.</li>\n          <li>In cases of data or concept drift, retraining the model is necessary. Add representative data or labels that capture the shifted patterns to the original training data. Alternatively, incorporate alternative data sources that better align with the factors causing the drift.</li>\n          <li>Noisy labels that do not match similar points indicate potential mislabelling errors in the dataset. Identify these instances and involve human experts to verify and correct the labels.</li>\n          <li>By aligning the chosen mitigation strategies with the insights gained from the root cause analysis, model developers can effectively debug and improve their models in a more targeted manner. This approach enables more precise and tailored actions to address the underlying issues and enhance the overall performance of the model.</li>\n        </ul>\n      </li>\n    </ol>\n<ul>\n          <li>To effectively debug performance issues in a machine learning model, it is crucial to evaluate the model’s performance on relevant data and understand the context of the problem. Here are key considerations for debugging based on different scenarios:\n            <ul>\n              <li><strong>Low training accuracy:</strong> If the model’s accuracy on the training data is low, it may indicate that the model is underfitting or that the relationship between the inputs and outputs is inherently noisy. Insufficient data may also be a factor.</li>\n              <li><strong>Low test accuracy:</strong> If the model’s accuracy on the test data is low, check the accuracy on the training set as well. If the training accuracy is also low, it suggests issues like the model underfitting or inherent noise in the data. However, high training accuracy with low test accuracy is indicative of overfitting, which can be addressed through regularization techniques.</li>\n              <li><strong>Performance drop across datasets:</strong> If the model’s performance drops when evaluated on different datasets, it could be a sign of overfitting or a change in the data or label distribution. If there are shifts in the input-output relationship or concept drift, retraining the model on new data may be necessary.</li>\n              <li><strong>Poor performance on a specific segment:</strong> Evaluate the significance of the problem segment. If it represents a large portion of the population, investigate if the model performed well on this segment during training or testing. For smaller segments, consider if there was enough data to effectively model them.</li>\n              <li><strong>Debugging single-point errors:</strong> To debug misclassifications or errors for individual data points, employ local explanation techniques. Examine the features that influenced the model’s prediction and assess their reasonableness. Analyzing the model’s performance in the neighborhood of the point of interest can provide additional insights.</li>\n            </ul>\n          </li>\n        </ul>\n<ul>\n              <li><strong>Low training accuracy:</strong> If the model’s accuracy on the training data is low, it may indicate that the model is underfitting or that the relationship between the inputs and outputs is inherently noisy. Insufficient data may also be a factor.</li>\n              <li><strong>Low test accuracy:</strong> If the model’s accuracy on the test data is low, check the accuracy on the training set as well. If the training accuracy is also low, it suggests issues like the model underfitting or inherent noise in the data. However, high training accuracy with low test accuracy is indicative of overfitting, which can be addressed through regularization techniques.</li>\n              <li><strong>Performance drop across datasets:</strong> If the model’s performance drops when evaluated on different datasets, it could be a sign of overfitting or a change in the data or label distribution. If there are shifts in the input-output relationship or concept drift, retraining the model on new data may be necessary.</li>\n              <li><strong>Poor performance on a specific segment:</strong> Evaluate the significance of the problem segment. If it represents a large portion of the population, investigate if the model performed well on this segment during training or testing. For smaller segments, consider if there was enough data to effectively model them.</li>\n              <li><strong>Debugging single-point errors:</strong> To debug misclassifications or errors for individual data points, employ local explanation techniques. Examine the features that influenced the model’s prediction and assess their reasonableness. Analyzing the model’s performance in the neighborhood of the point of interest can provide additional insights.</li>\n            </ul>\n<ul>\n          <li>Once you have identified the datasets where the model performs poorly, it is important to narrow down the problem scope and investigate specific regions or instances of poor performance. Treating the dataset as a single unit may not provide actionable insights, as the model’s performance may vary across different regions or data points.</li>\n          <li>By breaking down the problem, you can look for patterns or anomalous signals that are associated with the occurrence of poor performance. This could include examining data quality issues such as missing values, changes in the ground truth rate, shifts in data values, bugs in the data processing pipeline, or even external events that might have influenced the data.</li>\n          <li>By identifying these specific factors or signals that correlate with the problem, you can gain a better understanding of the underlying causes and develop a targeted action plan to address them. This approach allows for more focused debugging and troubleshooting, increasing the chances of effectively resolving the performance issues in the model.</li>\n        </ul>\n<ul>\n          <li>To further investigate the performance issues, it is important to identify the root cause of the problem. Start by examining whether the regions with high errors are well-represented in the training data. If these regions are underrepresented, the model may not have enough examples to learn the patterns effectively. In such cases, increasing the coverage of training data in those regions may help improve performance.</li>\n          <li>If the problematic regions are well-covered in the training set, other factors may be contributing to the low model performance, such as noisy labels. It is also crucial to analyze whether there is any data drift, which can be done by studying the distribution of each feature and assessing if the model’s performance is disproportionately worse on “newer” data points. Additionally, investigate whether there are changes in the label distribution alongside data shifts, as this may indicate concept drift.</li>\n          <li>Furthermore, utilize explanation frameworks to gain insights into the model’s decision-making process. Identify the features that are driving the model predictions and assess whether these features align with the expected conceptual understanding of the problem. Monitor if these features are changing over time, as it can indicate the need for model adaptation to evolving patterns. Pay attention to specific feature values that correlate with lower model performance, as this might indicate overfitting or the model relying excessively on certain ranges of feature values.</li>\n          <li>By understanding the root cause of the performance issues and investigating factors like data representation, data drift, label distribution, and feature importance, you can develop targeted strategies to improve the model’s performance and address any underlying issues.</li>\n        </ul>\n<ul>\n          <li>Once the root cause of the performance issues has been identified, it is essential to select appropriate mitigation strategies based on the analysis. Here are some insights from the root cause analysis along with corresponding amelioration strategies:</li>\n          <li>If specific features are strongly contributing to the low performance, consider removing or regularizing those features to mitigate overfitting on noisy values. However, carefully evaluate the overall influence of the feature on model predictions, as important features may naturally contribute to errors even when the model is performing well.</li>\n          <li>In cases of data or concept drift, retraining the model is necessary. Add representative data or labels that capture the shifted patterns to the original training data. Alternatively, incorporate alternative data sources that better align with the factors causing the drift.</li>\n          <li>Noisy labels that do not match similar points indicate potential mislabelling errors in the dataset. Identify these instances and involve human experts to verify and correct the labels.</li>\n          <li>By aligning the chosen mitigation strategies with the insights gained from the root cause analysis, model developers can effectively debug and improve their models in a more targeted manner. This approach enables more precise and tailored actions to address the underlying issues and enhance the overall performance of the model.</li>\n        </ul>\n<p><img src=\"/primers/ai/assets/learning/3.webp\" alt=\"\"></p>\n<ul>\n  <li>Below, we will look into more specific root cause issues and different ways we can handle them.</li>\n</ul>\n<blockquote>\n  <p>The ideal end-result being sought here is a learning strategy that would enable us to understand if the model is learning while minimizing wall-clock time and GPU time (to optimize for user efficiency and cost) associated with the training process. In other words, discover bugs sooner and fail fast so the turn-around time for the next training iteration is minimal.</p>\n</blockquote>\n<p>The ideal end-result being sought here is a learning strategy that would enable us to understand if the model is learning while minimizing wall-clock time and GPU time (to optimize for user efficiency and cost) associated with the training process. In other words, discover bugs sooner and fail fast so the turn-around time for the next training iteration is minimal.</p>",
      "contentMarkdown": "*   [Truera](https://truera.com/how-to-debug-ml-model-performance-a-framework/) detailed a general method of debugging ML models:\n    1.  **Evaluate the model performance on relevant data:**\n        *   To effectively debug performance issues in a machine learning model, it is crucial to evaluate the model’s performance on relevant data and understand the context of the problem. Here are key considerations for debugging based on different scenarios:\n            *   **Low training accuracy:** If the model’s accuracy on the training data is low, it may indicate that the model is underfitting or that the relationship between the inputs and outputs is inherently noisy. Insufficient data may also be a factor.\n            *   **Low test accuracy:** If the model’s accuracy on the test data is low, check the accuracy on the training set as well. If the training accuracy is also low, it suggests issues like the model underfitting or inherent noise in the data. However, high training accuracy with low test accuracy is indicative of overfitting, which can be addressed through regularization techniques.\n            *   **Performance drop across datasets:** If the model’s performance drops when evaluated on different datasets, it could be a sign of overfitting or a change in the data or label distribution. If there are shifts in the input-output relationship or concept drift, retraining the model on new data may be necessary.\n            *   **Poor performance on a specific segment:** Evaluate the significance of the problem segment. If it represents a large portion of the population, investigate if the model performed well on this segment during training or testing. For smaller segments, consider if there was enough data to effectively model them.\n            *   **Debugging single-point errors:** To debug misclassifications or errors for individual data points, employ local explanation techniques. Examine the features that influenced the model’s prediction and assess their reasonableness. Analyzing the model’s performance in the neighborhood of the point of interest can provide additional insights.\n    2.  **Narrow the problem scope:**\n        *   Once you have identified the datasets where the model performs poorly, it is important to narrow down the problem scope and investigate specific regions or instances of poor performance. Treating the dataset as a single unit may not provide actionable insights, as the model’s performance may vary across different regions or data points.\n        *   By breaking down the problem, you can look for patterns or anomalous signals that are associated with the occurrence of poor performance. This could include examining data quality issues such as missing values, changes in the ground truth rate, shifts in data values, bugs in the data processing pipeline, or even external events that might have influenced the data.\n        *   By identifying these specific factors or signals that correlate with the problem, you can gain a better understanding of the underlying causes and develop a targeted action plan to address them. This approach allows for more focused debugging and troubleshooting, increasing the chances of effectively resolving the performance issues in the model.\n    3.  **Analyze the root cause of the problem:**\n        *   To further investigate the performance issues, it is important to identify the root cause of the problem. Start by examining whether the regions with high errors are well-represented in the training data. If these regions are underrepresented, the model may not have enough examples to learn the patterns effectively. In such cases, increasing the coverage of training data in those regions may help improve performance.\n        *   If the problematic regions are well-covered in the training set, other factors may be contributing to the low model performance, such as noisy labels. It is also crucial to analyze whether there is any data drift, which can be done by studying the distribution of each feature and assessing if the model’s performance is disproportionately worse on “newer” data points. Additionally, investigate whether there are changes in the label distribution alongside data shifts, as this may indicate concept drift.\n        *   Furthermore, utilize explanation frameworks to gain insights into the model’s decision-making process. Identify the features that are driving the model predictions and assess whether these features align with the expected conceptual understanding of the problem. Monitor if these features are changing over time, as it can indicate the need for model adaptation to evolving patterns. Pay attention to specific feature values that correlate with lower model performance, as this might indicate overfitting or the model relying excessively on certain ranges of feature values.\n        *   By understanding the root cause of the performance issues and investigating factors like data representation, data drift, label distribution, and feature importance, you can develop targeted strategies to improve the model’s performance and address any underlying issues.\n    4.  **Mitigate the issue:**\n        *   Once the root cause of the performance issues has been identified, it is essential to select appropriate mitigation strategies based on the analysis. Here are some insights from the root cause analysis along with corresponding amelioration strategies:\n        *   If specific features are strongly contributing to the low performance, consider removing or regularizing those features to mitigate overfitting on noisy values. However, carefully evaluate the overall influence of the feature on model predictions, as important features may naturally contribute to errors even when the model is performing well.\n        *   In cases of data or concept drift, retraining the model is necessary. Add representative data or labels that capture the shifted patterns to the original training data. Alternatively, incorporate alternative data sources that better align with the factors causing the drift.\n        *   Noisy labels that do not match similar points indicate potential mislabelling errors in the dataset. Identify these instances and involve human experts to verify and correct the labels.\n        *   By aligning the chosen mitigation strategies with the insights gained from the root cause analysis, model developers can effectively debug and improve their models in a more targeted manner. This approach enables more precise and tailored actions to address the underlying issues and enhance the overall performance of the model.\n*   The image below [(source)](https://truera.com/how-to-debug-ml-model-performance-a-framework/) displays the overall general debugging framework that can be leveraged for finding bugs in ML models.\n\n1.  **Evaluate the model performance on relevant data:**\n    *   To effectively debug performance issues in a machine learning model, it is crucial to evaluate the model’s performance on relevant data and understand the context of the problem. Here are key considerations for debugging based on different scenarios:\n        *   **Low training accuracy:** If the model’s accuracy on the training data is low, it may indicate that the model is underfitting or that the relationship between the inputs and outputs is inherently noisy. Insufficient data may also be a factor.\n        *   **Low test accuracy:** If the model’s accuracy on the test data is low, check the accuracy on the training set as well. If the training accuracy is also low, it suggests issues like the model underfitting or inherent noise in the data. However, high training accuracy with low test accuracy is indicative of overfitting, which can be addressed through regularization techniques.\n        *   **Performance drop across datasets:** If the model’s performance drops when evaluated on different datasets, it could be a sign of overfitting or a change in the data or label distribution. If there are shifts in the input-output relationship or concept drift, retraining the model on new data may be necessary.\n        *   **Poor performance on a specific segment:** Evaluate the significance of the problem segment. If it represents a large portion of the population, investigate if the model performed well on this segment during training or testing. For smaller segments, consider if there was enough data to effectively model them.\n        *   **Debugging single-point errors:** To debug misclassifications or errors for individual data points, employ local explanation techniques. Examine the features that influenced the model’s prediction and assess their reasonableness. Analyzing the model’s performance in the neighborhood of the point of interest can provide additional insights.\n2.  **Narrow the problem scope:**\n    *   Once you have identified the datasets where the model performs poorly, it is important to narrow down the problem scope and investigate specific regions or instances of poor performance. Treating the dataset as a single unit may not provide actionable insights, as the model’s performance may vary across different regions or data points.\n    *   By breaking down the problem, you can look for patterns or anomalous signals that are associated with the occurrence of poor performance. This could include examining data quality issues such as missing values, changes in the ground truth rate, shifts in data values, bugs in the data processing pipeline, or even external events that might have influenced the data.\n    *   By identifying these specific factors or signals that correlate with the problem, you can gain a better understanding of the underlying causes and develop a targeted action plan to address them. This approach allows for more focused debugging and troubleshooting, increasing the chances of effectively resolving the performance issues in the model.\n3.  **Analyze the root cause of the problem:**\n    *   To further investigate the performance issues, it is important to identify the root cause of the problem. Start by examining whether the regions with high errors are well-represented in the training data. If these regions are underrepresented, the model may not have enough examples to learn the patterns effectively. In such cases, increasing the coverage of training data in those regions may help improve performance.\n    *   If the problematic regions are well-covered in the training set, other factors may be contributing to the low model performance, such as noisy labels. It is also crucial to analyze whether there is any data drift, which can be done by studying the distribution of each feature and assessing if the model’s performance is disproportionately worse on “newer” data points. Additionally, investigate whether there are changes in the label distribution alongside data shifts, as this may indicate concept drift.\n    *   Furthermore, utilize explanation frameworks to gain insights into the model’s decision-making process. Identify the features that are driving the model predictions and assess whether these features align with the expected conceptual understanding of the problem. Monitor if these features are changing over time, as it can indicate the need for model adaptation to evolving patterns. Pay attention to specific feature values that correlate with lower model performance, as this might indicate overfitting or the model relying excessively on certain ranges of feature values.\n    *   By understanding the root cause of the performance issues and investigating factors like data representation, data drift, label distribution, and feature importance, you can develop targeted strategies to improve the model’s performance and address any underlying issues.\n4.  **Mitigate the issue:**\n    *   Once the root cause of the performance issues has been identified, it is essential to select appropriate mitigation strategies based on the analysis. Here are some insights from the root cause analysis along with corresponding amelioration strategies:\n    *   If specific features are strongly contributing to the low performance, consider removing or regularizing those features to mitigate overfitting on noisy values. However, carefully evaluate the overall influence of the feature on model predictions, as important features may naturally contribute to errors even when the model is performing well.\n    *   In cases of data or concept drift, retraining the model is necessary. Add representative data or labels that capture the shifted patterns to the original training data. Alternatively, incorporate alternative data sources that better align with the factors causing the drift.\n    *   Noisy labels that do not match similar points indicate potential mislabelling errors in the dataset. Identify these instances and involve human experts to verify and correct the labels.\n    *   By aligning the chosen mitigation strategies with the insights gained from the root cause analysis, model developers can effectively debug and improve their models in a more targeted manner. This approach enables more precise and tailored actions to address the underlying issues and enhance the overall performance of the model.\n\n*   To effectively debug performance issues in a machine learning model, it is crucial to evaluate the model’s performance on relevant data and understand the context of the problem. Here are key considerations for debugging based on different scenarios:\n    *   **Low training accuracy:** If the model’s accuracy on the training data is low, it may indicate that the model is underfitting or that the relationship between the inputs and outputs is inherently noisy. Insufficient data may also be a factor.\n    *   **Low test accuracy:** If the model’s accuracy on the test data is low, check the accuracy on the training set as well. If the training accuracy is also low, it suggests issues like the model underfitting or inherent noise in the data. However, high training accuracy with low test accuracy is indicative of overfitting, which can be addressed through regularization techniques.\n    *   **Performance drop across datasets:** If the model’s performance drops when evaluated on different datasets, it could be a sign of overfitting or a change in the data or label distribution. If there are shifts in the input-output relationship or concept drift, retraining the model on new data may be necessary.\n    *   **Poor performance on a specific segment:** Evaluate the significance of the problem segment. If it represents a large portion of the population, investigate if the model performed well on this segment during training or testing. For smaller segments, consider if there was enough data to effectively model them.\n    *   **Debugging single-point errors:** To debug misclassifications or errors for individual data points, employ local explanation techniques. Examine the features that influenced the model’s prediction and assess their reasonableness. Analyzing the model’s performance in the neighborhood of the point of interest can provide additional insights.\n\n*   **Low training accuracy:** If the model’s accuracy on the training data is low, it may indicate that the model is underfitting or that the relationship between the inputs and outputs is inherently noisy. Insufficient data may also be a factor.\n*   **Low test accuracy:** If the model’s accuracy on the test data is low, check the accuracy on the training set as well. If the training accuracy is also low, it suggests issues like the model underfitting or inherent noise in the data. However, high training accuracy with low test accuracy is indicative of overfitting, which can be addressed through regularization techniques.\n*   **Performance drop across datasets:** If the model’s performance drops when evaluated on different datasets, it could be a sign of overfitting or a change in the data or label distribution. If there are shifts in the input-output relationship or concept drift, retraining the model on new data may be necessary.\n*   **Poor performance on a specific segment:** Evaluate the significance of the problem segment. If it represents a large portion of the population, investigate if the model performed well on this segment during training or testing. For smaller segments, consider if there was enough data to effectively model them.\n*   **Debugging single-point errors:** To debug misclassifications or errors for individual data points, employ local explanation techniques. Examine the features that influenced the model’s prediction and assess their reasonableness. Analyzing the model’s performance in the neighborhood of the point of interest can provide additional insights.\n\n*   Once you have identified the datasets where the model performs poorly, it is important to narrow down the problem scope and investigate specific regions or instances of poor performance. Treating the dataset as a single unit may not provide actionable insights, as the model’s performance may vary across different regions or data points.\n*   By breaking down the problem, you can look for patterns or anomalous signals that are associated with the occurrence of poor performance. This could include examining data quality issues such as missing values, changes in the ground truth rate, shifts in data values, bugs in the data processing pipeline, or even external events that might have influenced the data.\n*   By identifying these specific factors or signals that correlate with the problem, you can gain a better understanding of the underlying causes and develop a targeted action plan to address them. This approach allows for more focused debugging and troubleshooting, increasing the chances of effectively resolving the performance issues in the model.\n\n*   To further investigate the performance issues, it is important to identify the root cause of the problem. Start by examining whether the regions with high errors are well-represented in the training data. If these regions are underrepresented, the model may not have enough examples to learn the patterns effectively. In such cases, increasing the coverage of training data in those regions may help improve performance.\n*   If the problematic regions are well-covered in the training set, other factors may be contributing to the low model performance, such as noisy labels. It is also crucial to analyze whether there is any data drift, which can be done by studying the distribution of each feature and assessing if the model’s performance is disproportionately worse on “newer” data points. Additionally, investigate whether there are changes in the label distribution alongside data shifts, as this may indicate concept drift.\n*   Furthermore, utilize explanation frameworks to gain insights into the model’s decision-making process. Identify the features that are driving the model predictions and assess whether these features align with the expected conceptual understanding of the problem. Monitor if these features are changing over time, as it can indicate the need for model adaptation to evolving patterns. Pay attention to specific feature values that correlate with lower model performance, as this might indicate overfitting or the model relying excessively on certain ranges of feature values.\n*   By understanding the root cause of the performance issues and investigating factors like data representation, data drift, label distribution, and feature importance, you can develop targeted strategies to improve the model’s performance and address any underlying issues.\n\n*   Once the root cause of the performance issues has been identified, it is essential to select appropriate mitigation strategies based on the analysis. Here are some insights from the root cause analysis along with corresponding amelioration strategies:\n*   If specific features are strongly contributing to the low performance, consider removing or regularizing those features to mitigate overfitting on noisy values. However, carefully evaluate the overall influence of the feature on model predictions, as important features may naturally contribute to errors even when the model is performing well.\n*   In cases of data or concept drift, retraining the model is necessary. Add representative data or labels that capture the shifted patterns to the original training data. Alternatively, incorporate alternative data sources that better align with the factors causing the drift.\n*   Noisy labels that do not match similar points indicate potential mislabelling errors in the dataset. Identify these instances and involve human experts to verify and correct the labels.\n*   By aligning the chosen mitigation strategies with the insights gained from the root cause analysis, model developers can effectively debug and improve their models in a more targeted manner. This approach enables more precise and tailored actions to address the underlying issues and enhance the overall performance of the model.\n\n![](/primers/ai/assets/learning/3.webp)\n\n*   Below, we will look into more specific root cause issues and different ways we can handle them.\n\n> The ideal end-result being sought here is a learning strategy that would enable us to understand if the model is learning while minimizing wall-clock time and GPU time (to optimize for user efficiency and cost) associated with the training process. In other words, discover bugs sooner and fail fast so the turn-around time for the next training iteration is minimal.\n\nThe ideal end-result being sought here is a learning strategy that would enable us to understand if the model is learning while minimizing wall-clock time and GPU time (to optimize for user efficiency and cost) associated with the training process. In other words, discover bugs sooner and fail fast so the turn-around time for the next training iteration is minimal.",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 2,
      "estimatedMinutes": 17,
      "tags": [
        "miscellaneous",
        "machine learning",
        "attention",
        "regularization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 3265,
        "contentLength": 23636
      },
      "nextCards": [
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4"
      ],
      "relatedCards": [
        "ai-top-30-papers-a-tutorial-introduction-to-the-minimum-description-23",
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5",
        "ai-top-30-papers-machine-super-intelligence-24",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#framework-for-debugging",
      "scrapedAt": "2025-12-28T11:56:46.430Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-model-debugging-pipeline-issues-6"
      ]
    },
    {
      "id": "ai-model-debugging-sanity-check-model-architecture-and-data-3",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Sanity Check: Model Architecture and Data",
      "subtitle": null,
      "contentHtml": "<blockquote>\n  <p>It may initially sound counterintuitive, but one method for running a sanity check on our model is to intentionally overfit it on a minibatch (or a small dataset) to ensure that there are no bugs in the model’s architecture code (for example, misplaced connections, features/data lacking patterns that can be learned, etc.) and the expected performance on the training set can be achieved.</p>\n</blockquote>\n<p>It may initially sound counterintuitive, but one method for running a sanity check on our model is to intentionally overfit it on a minibatch (or a small dataset) to ensure that there are no bugs in the model’s architecture code (for example, misplaced connections, features/data lacking patterns that can be learned, etc.) and the expected performance on the training set can be achieved.</p>\n<ul>\n  <li>In other words, overfitting a model on a small mini-batch of data is sometimes a useful technique for debugging a deep learning model. Overfitting on a mini-batch means training the model to fit the mini-batch perfectly, even if it results in poor generalization performance.</li>\n  <li>Once we establish the above, we can then set the batch size to what fits in the GPU memory for maximum vectorization/parallelization.</li>\n  <li>The reason why this can be useful for debugging is that it allows you to quickly identify issues with the model architecture or training process, such as high bias (underfitting) or issues with the data (lack of learnable features, i.e., data being too noisy to derive meaningful patterns from it). For example, if the model is not able to overfit a small mini-batch over a sufficiently large number of epochs, it may indicate that the model is too shallow.</li>\n</ul>",
      "contentMarkdown": "> It may initially sound counterintuitive, but one method for running a sanity check on our model is to intentionally overfit it on a minibatch (or a small dataset) to ensure that there are no bugs in the model’s architecture code (for example, misplaced connections, features/data lacking patterns that can be learned, etc.) and the expected performance on the training set can be achieved.\n\nIt may initially sound counterintuitive, but one method for running a sanity check on our model is to intentionally overfit it on a minibatch (or a small dataset) to ensure that there are no bugs in the model’s architecture code (for example, misplaced connections, features/data lacking patterns that can be learned, etc.) and the expected performance on the training set can be achieved.\n\n*   In other words, overfitting a model on a small mini-batch of data is sometimes a useful technique for debugging a deep learning model. Overfitting on a mini-batch means training the model to fit the mini-batch perfectly, even if it results in poor generalization performance.\n*   Once we establish the above, we can then set the batch size to what fits in the GPU memory for maximum vectorization/parallelization.\n*   The reason why this can be useful for debugging is that it allows you to quickly identify issues with the model architecture or training process, such as high bias (underfitting) or issues with the data (lack of learnable features, i.e., data being too noisy to derive meaningful patterns from it). For example, if the model is not able to overfit a small mini-batch over a sufficiently large number of epochs, it may indicate that the model is too shallow.",
      "order": 3,
      "orderInChapter": 3,
      "difficulty": 2,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "deep learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 275,
        "contentLength": 1734
      },
      "nextCards": [
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5"
      ],
      "relatedCards": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-overview-of-precision-types-25",
        "ai-ml-runtimes-overview-3",
        "ai-top-30-papers-deep-residual-learning-for-image-recognition-10",
        "ai-ann-similarity-search-scann-scalable-nearest-neighbors-11"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#sanity-check:-model-architecture-and-data",
      "scrapedAt": "2025-12-28T11:56:46.430Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-model-debugging-pipeline-issues-6"
      ]
    },
    {
      "id": "ai-model-debugging-data-issues-4",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Data Issues",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>As the saying goes, “you put garbage in, you get garbage out” of the model so making sure the data is clean is a very important step.</li>\n  <li>The first thing is to make sure the data is labelled correctly and their is no noise present in the data collection pipeline as it will lead to an inaccurate model.</li>\n  <li><strong>Insufficient data and/or labels:</strong> Inadequate training data or missing relevant features can lead to performance degradation in machine learning models. This can be addressed by adding more helpful features or expanding the precision of existing features. Additionally, if certain segments of the population are not well represented in the training data, it can result in poor predictions for those subgroups. Increasing the number of training examples for underrepresented regions can help alleviate this issue.</li>\n  <li><strong>Shifting data and/or labels:</strong> Data drift and concept drift are two common issues related to shifting data. Data drift occurs when the distribution of input data changes over time, which can cause a model trained on previous data to not generalize well to the new data. Concept drift refers to a shift in the underlying relationship between inputs and outputs, requiring the model to be retrained to capture the new relationship. Monitoring data and labels can help identify potential data or concept drift, allowing for timely actions to be taken.\n    <ul>\n      <li>By understanding these issues and employing appropriate strategies, such as data augmentation, feature engineering, adding more training examples, and monitoring data and labels, you can address and mitigate the impact of insufficient data, shifting data, and shifting labels on your machine learning models.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>By understanding these issues and employing appropriate strategies, such as data augmentation, feature engineering, adding more training examples, and monitoring data and labels, you can address and mitigate the impact of insufficient data, shifting data, and shifting labels on your machine learning models.</li>\n    </ul>",
      "contentMarkdown": "*   As the saying goes, “you put garbage in, you get garbage out” of the model so making sure the data is clean is a very important step.\n*   The first thing is to make sure the data is labelled correctly and their is no noise present in the data collection pipeline as it will lead to an inaccurate model.\n*   **Insufficient data and/or labels:** Inadequate training data or missing relevant features can lead to performance degradation in machine learning models. This can be addressed by adding more helpful features or expanding the precision of existing features. Additionally, if certain segments of the population are not well represented in the training data, it can result in poor predictions for those subgroups. Increasing the number of training examples for underrepresented regions can help alleviate this issue.\n*   **Shifting data and/or labels:** Data drift and concept drift are two common issues related to shifting data. Data drift occurs when the distribution of input data changes over time, which can cause a model trained on previous data to not generalize well to the new data. Concept drift refers to a shift in the underlying relationship between inputs and outputs, requiring the model to be retrained to capture the new relationship. Monitoring data and labels can help identify potential data or concept drift, allowing for timely actions to be taken.\n    *   By understanding these issues and employing appropriate strategies, such as data augmentation, feature engineering, adding more training examples, and monitoring data and labels, you can address and mitigate the impact of insufficient data, shifting data, and shifting labels on your machine learning models.\n\n*   By understanding these issues and employing appropriate strategies, such as data augmentation, feature engineering, adding more training examples, and monitoring data and labels, you can address and mitigate the impact of insufficient data, shifting data, and shifting labels on your machine learning models.",
      "order": 4,
      "orderInChapter": 4,
      "difficulty": 2,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "machine learning",
        "data augmentation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 314,
        "contentLength": 2130
      },
      "nextCards": [
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-model-debugging-pipeline-issues-6"
      ],
      "relatedCards": [
        "ai-top-30-papers-a-tutorial-introduction-to-the-minimum-description-23",
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-top-30-papers-machine-super-intelligence-24",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-top-30-papers-neural-message-passing-for-quantum-chemistry-12"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#data-issues",
      "scrapedAt": "2025-12-28T11:56:46.430Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-model-debugging-pipeline-issues-6"
      ]
    },
    {
      "id": "ai-model-debugging-misaligned-optimization-and-evaluation-5",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Misaligned Optimization and Evaluation",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>One common issue is when the chosen training procedure optimizes a different metric than the one used for evaluating the model’s final performance. For example, optimizing for log-loss during training but evaluating precision or recall can lead to suboptimal results, especially in cases of imbalanced datasets. It’s important to ensure that the training and evaluation metrics align with the desired model performance.</li>\n  <li>To address misaligned optimization and evaluation, it’s necessary to choose appropriate evaluation metrics that align with the desired outcome and incorporate them into the training process. Understanding the business requirements and domain-specific considerations can help select the right metrics.</li>\n</ul>",
      "contentMarkdown": "*   One common issue is when the chosen training procedure optimizes a different metric than the one used for evaluating the model’s final performance. For example, optimizing for log-loss during training but evaluating precision or recall can lead to suboptimal results, especially in cases of imbalanced datasets. It’s important to ensure that the training and evaluation metrics align with the desired model performance.\n*   To address misaligned optimization and evaluation, it’s necessary to choose appropriate evaluation metrics that align with the desired outcome and incorporate them into the training process. Understanding the business requirements and domain-specific considerations can help select the right metrics.",
      "order": 5,
      "orderInChapter": 5,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "optimization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 103,
        "contentLength": 753
      },
      "nextCards": [
        "ai-model-debugging-pipeline-issues-6",
        "ai-model-debugging-underfitting-7"
      ],
      "relatedCards": [
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-pros-and-cons-6",
        "ai-ml-runtimes-pros-and-cons-23"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#misaligned-optimization-and-evaluation",
      "scrapedAt": "2025-12-28T11:56:46.430Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-pipeline-issues-6"
      ]
    },
    {
      "id": "ai-model-debugging-pipeline-issues-6",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Pipeline Issues",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>Machine learning models often rely on pipelines that involve feature engineering and data preprocessing. Any failures or changes in the pipeline, whether upstream or downstream of the model training or serving process, can significantly impact the model’s performance. It’s crucial to monitor and maintain the entire pipeline to ensure consistent and reliable model performance.</li>\n  <li>To mitigate pipeline issues, regular monitoring, testing, and version control of the pipeline components are essential to identify and resolve any issues that may impact the model’s performance.</li>\n</ul>",
      "contentMarkdown": "*   Machine learning models often rely on pipelines that involve feature engineering and data preprocessing. Any failures or changes in the pipeline, whether upstream or downstream of the model training or serving process, can significantly impact the model’s performance. It’s crucial to monitor and maintain the entire pipeline to ensure consistent and reliable model performance.\n*   To mitigate pipeline issues, regular monitoring, testing, and version control of the pipeline components are essential to identify and resolve any issues that may impact the model’s performance.",
      "order": 6,
      "orderInChapter": 6,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "machine learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 84,
        "contentLength": 606
      },
      "nextCards": [
        "ai-model-debugging-underfitting-7",
        "ai-model-debugging-overfitting-8"
      ],
      "relatedCards": [
        "ai-top-30-papers-a-tutorial-introduction-to-the-minimum-description-23",
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-top-30-papers-machine-super-intelligence-24",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#pipeline-issues",
      "scrapedAt": "2025-12-28T11:56:46.430Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5"
      ]
    },
    {
      "id": "ai-model-debugging-underfitting-7",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Underfitting",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>Underfitting occurs when a model fails to accurately capture the relationship between the inputs and outputs/labels of the training data, even if such a relationship exists. This can happen when the model is not complex enough (under-parameterized) or hasn’t been trained sufficiently to learn the relationship. Underfit models tend to have high bias and low variance.</li>\n  <li>To address underfitting, you can consider increasing the model’s complexity, adding more features or layers, or training for a longer duration.</li>\n</ul>",
      "contentMarkdown": "*   Underfitting occurs when a model fails to accurately capture the relationship between the inputs and outputs/labels of the training data, even if such a relationship exists. This can happen when the model is not complex enough (under-parameterized) or hasn’t been trained sufficiently to learn the relationship. Underfit models tend to have high bias and low variance.\n*   To address underfitting, you can consider increasing the model’s complexity, adding more features or layers, or training for a longer duration.",
      "order": 7,
      "orderInChapter": 7,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 79,
        "contentLength": 545
      },
      "nextCards": [
        "ai-model-debugging-overfitting-8",
        "ai-model-debugging-gradual-complexity-increase-9"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#underfitting",
      "scrapedAt": "2025-12-28T11:56:46.430Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5"
      ]
    },
    {
      "id": "ai-model-debugging-overfitting-8",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Overfitting",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>Overfitting happens when a model becomes too focused on capturing the nuances and noise of the training data, resulting in poor generalization to unseen data. It occurs when the model is overly complex (over-parameterized) and fits itself too closely to the training data, including the noise. Overfit models tend to have low bias and high variance.</li>\n  <li>Techniques like regularization can also help prevent overfitting by adding constraints to the model’s parameters. Evaluating the model’s performance on unseen data and using techniques like cross-validation can help identify and mitigate both underfitting and overfitting issues.</li>\n</ul>",
      "contentMarkdown": "*   Overfitting happens when a model becomes too focused on capturing the nuances and noise of the training data, resulting in poor generalization to unseen data. It occurs when the model is overly complex (over-parameterized) and fits itself too closely to the training data, including the noise. Overfit models tend to have low bias and high variance.\n*   Techniques like regularization can also help prevent overfitting by adding constraints to the model’s parameters. Evaluating the model’s performance on unseen data and using techniques like cross-validation can help identify and mitigate both underfitting and overfitting issues.",
      "order": 8,
      "orderInChapter": 8,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "regularization",
        "cross-validation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 95,
        "contentLength": 662
      },
      "nextCards": [
        "ai-model-debugging-gradual-complexity-increase-9",
        "ai-model-debugging-gradient-checking-10"
      ],
      "relatedCards": [
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5",
        "ai-conditional-random-fields-parameter-estimation-5",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#overfitting",
      "scrapedAt": "2025-12-28T11:56:46.430Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5"
      ]
    },
    {
      "id": "ai-model-debugging-gradual-complexity-increase-9",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Gradual Complexity Increase",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>Instead of starting with the most complex model architecture, it’s better to begin with a simpler version and gradually increase it’s complexity.</li>\n  <li>By incrementally adding layers, neurons, or features, you can pinpoint when the model’s performance takes a downturn and this helps assess the issues related to model complexity, capacity, or architecture choice.</li>\n</ul>",
      "contentMarkdown": "*   Instead of starting with the most complex model architecture, it’s better to begin with a simpler version and gradually increase it’s complexity.\n*   By incrementally adding layers, neurons, or features, you can pinpoint when the model’s performance takes a downturn and this helps assess the issues related to model complexity, capacity, or architecture choice.",
      "order": 9,
      "orderInChapter": 9,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 55,
        "contentLength": 391
      },
      "nextCards": [
        "ai-model-debugging-gradient-checking-10",
        "ai-model-debugging-weight-initialization-11"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#gradual-complexity-increase",
      "scrapedAt": "2025-12-28T11:56:46.430Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5"
      ]
    },
    {
      "id": "ai-model-debugging-gradient-checking-10",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Gradient Checking",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>This involves comparing the gradients calculated analytically using backpropagation with the gradients calculated numerically using finite differences. If the gradients differ significantly, it may indicate an issue with the model implementation or the gradient computation.</li>\n</ul>\n<h4 id=\"visualization-activations-and-gradients\">Visualization Activations and Gradients</h4>\n<ul>\n  <li>This involves visualizing the activations and gradients of individual neurons in the network during training. This can help identify issues such as vanishing or exploding gradients, and can also provide insights into the model’s decision-making process.</li>\n</ul>\n<blockquote>\n  <p>Specifically, taking the L2 norm of the gradients gives a summary value of the efficacy of the model’s learning process. If the values are small in magnitude, it implies vanishing gradients where large values (that could be orders-of-magnitude more than the current weight values) can imply exploding gradients.</p>\n</blockquote>\n<p>Specifically, taking the L2 norm of the gradients gives a summary value of the efficacy of the model’s learning process. If the values are small in magnitude, it implies vanishing gradients where large values (that could be orders-of-magnitude more than the current weight values) can imply exploding gradients.</p>",
      "contentMarkdown": "*   This involves comparing the gradients calculated analytically using backpropagation with the gradients calculated numerically using finite differences. If the gradients differ significantly, it may indicate an issue with the model implementation or the gradient computation.\n\n#### Visualization Activations and Gradients\n\n*   This involves visualizing the activations and gradients of individual neurons in the network during training. This can help identify issues such as vanishing or exploding gradients, and can also provide insights into the model’s decision-making process.\n\n> Specifically, taking the L2 norm of the gradients gives a summary value of the efficacy of the model’s learning process. If the values are small in magnitude, it implies vanishing gradients where large values (that could be orders-of-magnitude more than the current weight values) can imply exploding gradients.\n\nSpecifically, taking the L2 norm of the gradients gives a summary value of the efficacy of the model’s learning process. If the values are small in magnitude, it implies vanishing gradients where large values (that could be orders-of-magnitude more than the current weight values) can imply exploding gradients.",
      "order": 10,
      "orderInChapter": 10,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "backpropagation",
        "activation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 175,
        "contentLength": 1333
      },
      "nextCards": [
        "ai-model-debugging-weight-initialization-11",
        "ai-model-debugging-learning-rate-12"
      ],
      "relatedCards": [
        "ai-gpu-architecture-forward-and-backward-passes-training-17",
        "ai-gpu-architecture-precision-optimization-19",
        "ai-ml-runtimes-implementation-details-5",
        "ai-tensorboard-plotting-losses-accuracies-and-weight-distribution-3",
        "ai-tensorboard-logging-images-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#gradient-checking",
      "scrapedAt": "2025-12-28T11:56:46.430Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5"
      ]
    },
    {
      "id": "ai-model-debugging-weight-initialization-11",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Weight Initialization",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>Initializing the model weights in a certain way can impact the model’s performance. It is recommended to use appropriate weight initialization schemes such as Xavier or He initialization and evaluate the impact of the initialization on the model’s performance.</li>\n</ul>\n<h4 id=\"why-do-we-initialize-weights-randomly--what-if-we-initialize-the-weights-with-the-same-values\">Why Do We Initialize Weights Randomly? / What If We Initialize the Weights with the Same Values?</h4>\n<ul>\n  <li>If all weights are initialized with the same values, all neurons in each layer give you the same outputs (and thus redundantly learn the same features) which implies the model will never learn. This is the reason that the weights are initialized with random numbers.</li>\n  <li>Detailed explanation:\n    <ul>\n      <li>The optimization algorithms we usually use for training neural networks are deterministic. Gradient descent, the most basic algorithm, that is a base for the more complicated ones, is defined in terms of partial derivatives</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>&amp;#x03B8;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msub><mo>:=</mo><msub><mi>&amp;#x03B8;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msub><mo>&amp;#x2212;</mo><mi>&amp;#x03B1;</mi><mfrac><mi mathvariant=&quot;normal&quot;>&amp;#x2202;</mi><mrow><mi mathvariant=&quot;normal&quot;>&amp;#x2202;</mi><msub><mi>&amp;#x03B8;</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msub></mrow></mfrac><mi>J</mi><mo stretchy=&quot;false&quot;>(</mo><mi>&amp;#x03B8;</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 9.482em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.867em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.68em, 1007.82em, 3.336em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"msubsup\" id=\"MathJax-Span-3\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-5\"><span class=\"mrow\" id=\"MathJax-Span-6\"><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-8\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">:<span style=\"font-family: STIXGeneral-Regular; font-style: normal; font-weight: normal;\">=</span></span><span class=\"msubsup\" id=\"MathJax-Span-9\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-10\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-11\"><span class=\"mrow\" id=\"MathJax-Span-12\"><span class=\"mi\" id=\"MathJax-Span-13\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-14\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">α</span><span class=\"mfrac\" id=\"MathJax-Span-16\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mi\" id=\"MathJax-Span-17\" style=\"font-family: STIXGeneral-Regular;\">∂</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1001.2em, 4.482em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.622em;\"><span class=\"mrow\" id=\"MathJax-Span-18\"><span class=\"mi\" id=\"MathJax-Span-19\" style=\"font-family: STIXGeneral-Regular;\">∂</span><span class=\"msubsup\" id=\"MathJax-Span-20\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-21\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-22\"><span class=\"mrow\" id=\"MathJax-Span-23\"><span class=\"mi\" id=\"MathJax-Span-24\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1001.36em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.357em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-25\" style=\"font-family: STIXGeneral-Italic;\">J<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-28\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.247em; border-left: 0px solid; width: 0px; height: 2.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>θ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msub><mo>:=</mo><msub><mi>θ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msub><mo>−</mo><mi>α</mi><mfrac><mi mathvariant=\"normal\">∂</mi><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>θ</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msub></mrow></mfrac><mi>J</mi><mo stretchy=\"false\">(</mo><mi>θ</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-1\">\\theta_{j}:=\\theta_{j}-\\alpha \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)</script>\n\n    <ul>\n      <li>\n        <p>A <a href=\"https://en.wikipedia.org/wiki/Partial_derivative\">partial derivative</a> tells you how does the change of the optimized function is affected by the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B8;</mi><mi>j</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-29\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.73em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-30\"><span class=\"msubsup\" id=\"MathJax-Span-31\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-32\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>θ</mi><mi>j</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">\\theta_j</script> parameter. If all the parameters are the same, they all have the same impact on the result, so will change by the same quantity. If you change all the parameters by the same value, they will keep being the same. In such a case, each neuron will be doing the same thing, they will be redundant and there would be no point in having multiple neurons. There is no point in wasting your compute repeating exactly the same operations multiple times. In other words, the model does not learn because error is propagated back through the weights in proportion to the values of the weights. This means that all hidden units connected directly to the output units will get identical error signals, and, since the weight changes depend on the error signals, the weights from those units to the output units will be the same.</p>\n      </li>\n      <li>\n        <p>When you initialize the neurons randomly, each of them will hopefully be evolving during the optimization in a different “direction”, they will be learning to detect different features from the data. You can think of early layers as of doing automatic feature engineering for you, by transforming the data, that are used by the final layer of the network. If all the learned features are the same, it would be a wasted effort.</p>\n      </li>\n      <li>\n        <p><a href=\"https://arxiv.org/abs/1803.03635v1\">The Lottery Ticket Hypothesis: Training Pruned Neural Networks by Frankle and Carbin</a> explores the hypothesis that the big neural networks are so effective because randomly initializing multiple parameters helps our luck by drawing the lucky “lottery ticket” parameters that work well for the problem.</p>\n      </li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>The optimization algorithms we usually use for training neural networks are deterministic. Gradient descent, the most basic algorithm, that is a base for the more complicated ones, is defined in terms of partial derivatives</li>\n    </ul>\n<ul>\n      <li>\n        <p>A <a href=\"https://en.wikipedia.org/wiki/Partial_derivative\">partial derivative</a> tells you how does the change of the optimized function is affected by the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B8;</mi><mi>j</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-29\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.73em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-30\"><span class=\"msubsup\" id=\"MathJax-Span-31\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-32\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>θ</mi><mi>j</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">\\theta_j</script> parameter. If all the parameters are the same, they all have the same impact on the result, so will change by the same quantity. If you change all the parameters by the same value, they will keep being the same. In such a case, each neuron will be doing the same thing, they will be redundant and there would be no point in having multiple neurons. There is no point in wasting your compute repeating exactly the same operations multiple times. In other words, the model does not learn because error is propagated back through the weights in proportion to the values of the weights. This means that all hidden units connected directly to the output units will get identical error signals, and, since the weight changes depend on the error signals, the weights from those units to the output units will be the same.</p>\n      </li>\n      <li>\n        <p>When you initialize the neurons randomly, each of them will hopefully be evolving during the optimization in a different “direction”, they will be learning to detect different features from the data. You can think of early layers as of doing automatic feature engineering for you, by transforming the data, that are used by the final layer of the network. If all the learned features are the same, it would be a wasted effort.</p>\n      </li>\n      <li>\n        <p><a href=\"https://arxiv.org/abs/1803.03635v1\">The Lottery Ticket Hypothesis: Training Pruned Neural Networks by Frankle and Carbin</a> explores the hypothesis that the big neural networks are so effective because randomly initializing multiple parameters helps our luck by drawing the lucky “lottery ticket” parameters that work well for the problem.</p>\n      </li>\n    </ul>\n<p>A <a href=\"https://en.wikipedia.org/wiki/Partial_derivative\">partial derivative</a> tells you how does the change of the optimized function is affected by the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03B8;</mi><mi>j</mi></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-29\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.73em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-30\"><span class=\"msubsup\" id=\"MathJax-Span-31\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-32\" style=\"font-family: STIXGeneral-Italic;\">θ<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"mi\" id=\"MathJax-Span-33\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>θ</mi><mi>j</mi></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">\\theta_j</script> parameter. If all the parameters are the same, they all have the same impact on the result, so will change by the same quantity. If you change all the parameters by the same value, they will keep being the same. In such a case, each neuron will be doing the same thing, they will be redundant and there would be no point in having multiple neurons. There is no point in wasting your compute repeating exactly the same operations multiple times. In other words, the model does not learn because error is propagated back through the weights in proportion to the values of the weights. This means that all hidden units connected directly to the output units will get identical error signals, and, since the weight changes depend on the error signals, the weights from those units to the output units will be the same.</p>\n<p>When you initialize the neurons randomly, each of them will hopefully be evolving during the optimization in a different “direction”, they will be learning to detect different features from the data. You can think of early layers as of doing automatic feature engineering for you, by transforming the data, that are used by the final layer of the network. If all the learned features are the same, it would be a wasted effort.</p>\n<p><a href=\"https://arxiv.org/abs/1803.03635v1\">The Lottery Ticket Hypothesis: Training Pruned Neural Networks by Frankle and Carbin</a> explores the hypothesis that the big neural networks are so effective because randomly initializing multiple parameters helps our luck by drawing the lucky “lottery ticket” parameters that work well for the problem.</p>",
      "contentMarkdown": "*   Initializing the model weights in a certain way can impact the model’s performance. It is recommended to use appropriate weight initialization schemes such as Xavier or He initialization and evaluate the impact of the initialization on the model’s performance.\n\n#### Why Do We Initialize Weights Randomly? / What If We Initialize the Weights with the Same Values?\n\n*   If all weights are initialized with the same values, all neurons in each layer give you the same outputs (and thus redundantly learn the same features) which implies the model will never learn. This is the reason that the weights are initialized with random numbers.\n*   Detailed explanation:\n    \n    *   The optimization algorithms we usually use for training neural networks are deterministic. Gradient descent, the most basic algorithm, that is a base for the more complicated ones, is defined in terms of partial derivatives\n    \n    θj:\\=θj−α∂∂θjJ(θ)θj:=θj−α∂∂θjJ(θ)\n    \n    \\\\theta\\_{j}:=\\\\theta\\_{j}-\\\\alpha \\\\frac{\\\\partial}{\\\\partial \\\\theta\\_{j}} J(\\\\theta)\n    *   A [partial derivative](https://en.wikipedia.org/wiki/Partial_derivative) tells you how does the change of the optimized function is affected by the θjθj\\\\theta\\_j parameter. If all the parameters are the same, they all have the same impact on the result, so will change by the same quantity. If you change all the parameters by the same value, they will keep being the same. In such a case, each neuron will be doing the same thing, they will be redundant and there would be no point in having multiple neurons. There is no point in wasting your compute repeating exactly the same operations multiple times. In other words, the model does not learn because error is propagated back through the weights in proportion to the values of the weights. This means that all hidden units connected directly to the output units will get identical error signals, and, since the weight changes depend on the error signals, the weights from those units to the output units will be the same.\n        \n    *   When you initialize the neurons randomly, each of them will hopefully be evolving during the optimization in a different “direction”, they will be learning to detect different features from the data. You can think of early layers as of doing automatic feature engineering for you, by transforming the data, that are used by the final layer of the network. If all the learned features are the same, it would be a wasted effort.\n        \n    *   [The Lottery Ticket Hypothesis: Training Pruned Neural Networks by Frankle and Carbin](https://arxiv.org/abs/1803.03635v1) explores the hypothesis that the big neural networks are so effective because randomly initializing multiple parameters helps our luck by drawing the lucky “lottery ticket” parameters that work well for the problem.\n        \n\n*   The optimization algorithms we usually use for training neural networks are deterministic. Gradient descent, the most basic algorithm, that is a base for the more complicated ones, is defined in terms of partial derivatives\n\n*   A [partial derivative](https://en.wikipedia.org/wiki/Partial_derivative) tells you how does the change of the optimized function is affected by the θjθj\\\\theta\\_j parameter. If all the parameters are the same, they all have the same impact on the result, so will change by the same quantity. If you change all the parameters by the same value, they will keep being the same. In such a case, each neuron will be doing the same thing, they will be redundant and there would be no point in having multiple neurons. There is no point in wasting your compute repeating exactly the same operations multiple times. In other words, the model does not learn because error is propagated back through the weights in proportion to the values of the weights. This means that all hidden units connected directly to the output units will get identical error signals, and, since the weight changes depend on the error signals, the weights from those units to the output units will be the same.\n    \n*   When you initialize the neurons randomly, each of them will hopefully be evolving during the optimization in a different “direction”, they will be learning to detect different features from the data. You can think of early layers as of doing automatic feature engineering for you, by transforming the data, that are used by the final layer of the network. If all the learned features are the same, it would be a wasted effort.\n    \n*   [The Lottery Ticket Hypothesis: Training Pruned Neural Networks by Frankle and Carbin](https://arxiv.org/abs/1803.03635v1) explores the hypothesis that the big neural networks are so effective because randomly initializing multiple parameters helps our luck by drawing the lucky “lottery ticket” parameters that work well for the problem.\n    \n\nA [partial derivative](https://en.wikipedia.org/wiki/Partial_derivative) tells you how does the change of the optimized function is affected by the θjθj\\\\theta\\_j parameter. If all the parameters are the same, they all have the same impact on the result, so will change by the same quantity. If you change all the parameters by the same value, they will keep being the same. In such a case, each neuron will be doing the same thing, they will be redundant and there would be no point in having multiple neurons. There is no point in wasting your compute repeating exactly the same operations multiple times. In other words, the model does not learn because error is propagated back through the weights in proportion to the values of the weights. This means that all hidden units connected directly to the output units will get identical error signals, and, since the weight changes depend on the error signals, the weights from those units to the output units will be the same.\n\nWhen you initialize the neurons randomly, each of them will hopefully be evolving during the optimization in a different “direction”, they will be learning to detect different features from the data. You can think of early layers as of doing automatic feature engineering for you, by transforming the data, that are used by the final layer of the network. If all the learned features are the same, it would be a wasted effort.\n\n[The Lottery Ticket Hypothesis: Training Pruned Neural Networks by Frankle and Carbin](https://arxiv.org/abs/1803.03635v1) explores the hypothesis that the big neural networks are so effective because randomly initializing multiple parameters helps our luck by drawing the lucky “lottery ticket” parameters that work well for the problem.",
      "order": 11,
      "orderInChapter": 11,
      "difficulty": 3,
      "estimatedMinutes": 6,
      "tags": [
        "miscellaneous",
        "neural network",
        "optimization",
        "gradient descent"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 1028,
        "contentLength": 20157
      },
      "nextCards": [
        "ai-model-debugging-learning-rate-12",
        "ai-model-debugging-optimizer-13"
      ],
      "relatedCards": [
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16",
        "ai-top-30-papers-multi-scale-context-aggregation-by-dilated-convolu-11",
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-ml-runtimes-architecture-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#weight-initialization",
      "scrapedAt": "2025-12-28T11:56:46.430Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5"
      ]
    },
    {
      "id": "ai-model-debugging-learning-rate-12",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Learning Rate",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>The learning rate is a hyperparameter and it determines the step size at which the model parameters are updated during training. It controls the magnitude of the adjustments made to the parameters based on the computed gradients. Selecting an appropriate learning rate is vital for achieving efficient and effective model training.</li>\n  <li>It needs to be tuned along with other hyperparameters to find the optimal configuration for the given task and dataset. The choice of an appropriate learning rate is crucial as it directly affects the model’s training dynamics, convergence speed, and final performance. Different learning rates can lead to significantly different outcomes in terms of training stability and the quality of learned representations.rom a debugging perspective, here are the common themes that are usually observed:\n    <ul>\n      <li><strong>Learning Rate Too High:</strong> If the learning rate is set too high, the model may experience unstable training. This can result in large fluctuations in the loss function or the model failing to converge to an optimal solution. In such cases, it’s necessary to reduce the learning rate gradually to stabilize the training process.</li>\n      <li><strong>Learning Rate Too Low:</strong> Conversely, if the learning rate is too low, the model’s training progress may be slow, and it may struggle to find the optimal solution. In this case, increasing the learning rate can help speed up the training process and improve convergence.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Learning Rate Too High:</strong> If the learning rate is set too high, the model may experience unstable training. This can result in large fluctuations in the loss function or the model failing to converge to an optimal solution. In such cases, it’s necessary to reduce the learning rate gradually to stabilize the training process.</li>\n      <li><strong>Learning Rate Too Low:</strong> Conversely, if the learning rate is too low, the model’s training progress may be slow, and it may struggle to find the optimal solution. In this case, increasing the learning rate can help speed up the training process and improve convergence.</li>\n    </ul>",
      "contentMarkdown": "*   The learning rate is a hyperparameter and it determines the step size at which the model parameters are updated during training. It controls the magnitude of the adjustments made to the parameters based on the computed gradients. Selecting an appropriate learning rate is vital for achieving efficient and effective model training.\n*   It needs to be tuned along with other hyperparameters to find the optimal configuration for the given task and dataset. The choice of an appropriate learning rate is crucial as it directly affects the model’s training dynamics, convergence speed, and final performance. Different learning rates can lead to significantly different outcomes in terms of training stability and the quality of learned representations.rom a debugging perspective, here are the common themes that are usually observed:\n    *   **Learning Rate Too High:** If the learning rate is set too high, the model may experience unstable training. This can result in large fluctuations in the loss function or the model failing to converge to an optimal solution. In such cases, it’s necessary to reduce the learning rate gradually to stabilize the training process.\n    *   **Learning Rate Too Low:** Conversely, if the learning rate is too low, the model’s training progress may be slow, and it may struggle to find the optimal solution. In this case, increasing the learning rate can help speed up the training process and improve convergence.\n\n*   **Learning Rate Too High:** If the learning rate is set too high, the model may experience unstable training. This can result in large fluctuations in the loss function or the model failing to converge to an optimal solution. In such cases, it’s necessary to reduce the learning rate gradually to stabilize the training process.\n*   **Learning Rate Too Low:** Conversely, if the learning rate is too low, the model’s training progress may be slow, and it may struggle to find the optimal solution. In this case, increasing the learning rate can help speed up the training process and improve convergence.",
      "order": 12,
      "orderInChapter": 12,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "loss function"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 329,
        "contentLength": 2212
      },
      "nextCards": [
        "ai-model-debugging-optimizer-13",
        "ai-model-debugging-regularization-14"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-order-matters-sequence-to-sequence-for-sets-8",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#learning-rate",
      "scrapedAt": "2025-12-28T11:56:46.430Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5"
      ]
    },
    {
      "id": "ai-model-debugging-optimizer-13",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Optimizer",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>The learning rate and optimizer can impact the model’s convergence and performance. It’s important to use an appropriate learning rate schedule and optimizer, and to experiment with different values to find the optimal combination.</li>\n  <li>The optimizer is responsible for updating the model’s parameters based on the computed gradients during the backpropagation process. It determines the specific algorithm used to perform the parameter updates. Different optimizers have different characteristics and performance in terms of convergence speed and robustness to different types of data. From a debugging perspective, here are the common themes that are usually observed:\n    <ul>\n      <li><strong>Optimizer Selection:</strong> Different optimizers, such as Stochastic Gradient Descent (SGD), Adam, RMSprop, or AdaGrad, have different properties and are suitable for different scenarios. It’s important to choose an optimizer that is well-suited to the problem at hand. If the chosen optimizer is not performing well (which usually manifests as a relatively long time-to-convergence), it might be necessary to experiment with the hyperparameters of the optimizer and even empirically try out different optimizers to find the one that yields better results.</li>\n      <li><strong>Learning Rate Schedule:</strong> Some optimizers, such as Adam, automatically adapt the learning rate based on the gradient history. However, for optimizers like SGD, the learning rate needs to be manually adjusted. The learning rate schedule determines how the learning rate changes over time during training. Debugging the learning rate schedule involves experimenting with different schedules, such as fixed learning rate, exponential decay, step decay, or adaptive schedules, to find the one that leads to faster convergence and better performance.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>Optimizer Selection:</strong> Different optimizers, such as Stochastic Gradient Descent (SGD), Adam, RMSprop, or AdaGrad, have different properties and are suitable for different scenarios. It’s important to choose an optimizer that is well-suited to the problem at hand. If the chosen optimizer is not performing well (which usually manifests as a relatively long time-to-convergence), it might be necessary to experiment with the hyperparameters of the optimizer and even empirically try out different optimizers to find the one that yields better results.</li>\n      <li><strong>Learning Rate Schedule:</strong> Some optimizers, such as Adam, automatically adapt the learning rate based on the gradient history. However, for optimizers like SGD, the learning rate needs to be manually adjusted. The learning rate schedule determines how the learning rate changes over time during training. Debugging the learning rate schedule involves experimenting with different schedules, such as fixed learning rate, exponential decay, step decay, or adaptive schedules, to find the one that leads to faster convergence and better performance.</li>\n    </ul>",
      "contentMarkdown": "*   The learning rate and optimizer can impact the model’s convergence and performance. It’s important to use an appropriate learning rate schedule and optimizer, and to experiment with different values to find the optimal combination.\n*   The optimizer is responsible for updating the model’s parameters based on the computed gradients during the backpropagation process. It determines the specific algorithm used to perform the parameter updates. Different optimizers have different characteristics and performance in terms of convergence speed and robustness to different types of data. From a debugging perspective, here are the common themes that are usually observed:\n    *   **Optimizer Selection:** Different optimizers, such as Stochastic Gradient Descent (SGD), Adam, RMSprop, or AdaGrad, have different properties and are suitable for different scenarios. It’s important to choose an optimizer that is well-suited to the problem at hand. If the chosen optimizer is not performing well (which usually manifests as a relatively long time-to-convergence), it might be necessary to experiment with the hyperparameters of the optimizer and even empirically try out different optimizers to find the one that yields better results.\n    *   **Learning Rate Schedule:** Some optimizers, such as Adam, automatically adapt the learning rate based on the gradient history. However, for optimizers like SGD, the learning rate needs to be manually adjusted. The learning rate schedule determines how the learning rate changes over time during training. Debugging the learning rate schedule involves experimenting with different schedules, such as fixed learning rate, exponential decay, step decay, or adaptive schedules, to find the one that leads to faster convergence and better performance.\n\n*   **Optimizer Selection:** Different optimizers, such as Stochastic Gradient Descent (SGD), Adam, RMSprop, or AdaGrad, have different properties and are suitable for different scenarios. It’s important to choose an optimizer that is well-suited to the problem at hand. If the chosen optimizer is not performing well (which usually manifests as a relatively long time-to-convergence), it might be necessary to experiment with the hyperparameters of the optimizer and even empirically try out different optimizers to find the one that yields better results.\n*   **Learning Rate Schedule:** Some optimizers, such as Adam, automatically adapt the learning rate based on the gradient history. However, for optimizers like SGD, the learning rate needs to be manually adjusted. The learning rate schedule determines how the learning rate changes over time during training. Debugging the learning rate schedule involves experimenting with different schedules, such as fixed learning rate, exponential decay, step decay, or adaptive schedules, to find the one that leads to faster convergence and better performance.",
      "order": 13,
      "orderInChapter": 13,
      "difficulty": 3,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous",
        "gradient descent",
        "backpropagation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 419,
        "contentLength": 3051
      },
      "nextCards": [
        "ai-model-debugging-regularization-14",
        "ai-model-debugging-grad-cam-15"
      ],
      "relatedCards": [
        "ai-top-30-papers-stanfords-cs231n-convolutional-neural-networks-for-26",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-multi-scale-context-aggregation-by-dilated-convolu-11",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#optimizer",
      "scrapedAt": "2025-12-28T11:56:46.430Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5"
      ]
    },
    {
      "id": "ai-model-debugging-regularization-14",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Regularization",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>Regularization techniques such as L1/L2 regularization, dropout, and early stopping can help prevent overfitting and improve the model’s generalization performance.</li>\n</ul>",
      "contentMarkdown": "*   Regularization techniques such as L1/L2 regularization, dropout, and early stopping can help prevent overfitting and improve the model’s generalization performance.",
      "order": 14,
      "orderInChapter": 14,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "regularization",
        "dropout"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 21,
        "contentLength": 186
      },
      "nextCards": [
        "ai-model-debugging-grad-cam-15",
        "ai-model-debugging-knowledge-distillation-student-teacher-approach-16"
      ],
      "relatedCards": [
        "ai-top-30-papers-recurrent-neural-network-regularization-4",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5",
        "ai-tensorboard-plotting-losses-accuracies-and-weight-distribution-3",
        "ai-tensorboard-logging-images-4",
        "ai-tensorboard-custom-logging-callbacks-5"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#regularization",
      "scrapedAt": "2025-12-28T11:56:46.430Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5"
      ]
    },
    {
      "id": "ai-model-debugging-grad-cam-15",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Grad-CAM",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>Gradient-weighted Class Activation Mapping (Grad-CAM) is a visualization technique that can help identify which parts of an input image the model is focusing on when making a prediction. This can provide insights into how the model is making decisions and can help identify issues with the model architecture or training process.</li>\n</ul>",
      "contentMarkdown": "*   Gradient-weighted Class Activation Mapping (Grad-CAM) is a visualization technique that can help identify which parts of an input image the model is focusing on when making a prediction. This can provide insights into how the model is making decisions and can help identify issues with the model architecture or training process.",
      "order": 15,
      "orderInChapter": 15,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "activation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 52,
        "contentLength": 351
      },
      "nextCards": [
        "ai-model-debugging-knowledge-distillation-student-teacher-approach-16",
        "ai-model-debugging-hyper-parameter-optimization-hpo-17"
      ],
      "relatedCards": [
        "ai-gpu-architecture-forward-and-backward-passes-training-17",
        "ai-gpu-architecture-precision-optimization-19",
        "ai-ml-runtimes-implementation-details-5",
        "ai-tensorboard-plotting-losses-accuracies-and-weight-distribution-3",
        "ai-tensorboard-logging-images-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#grad-cam",
      "scrapedAt": "2025-12-28T11:56:46.430Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5"
      ]
    },
    {
      "id": "ai-model-debugging-knowledge-distillation-student-teacher-approach-16",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Knowledge Distillation / Student-Teacher Approach",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>The student-teacher approach, also known as model or knowledge distillation, is a technique used to improve the performance of a deep learning model by using a larger, more complex model (the teacher) to train a smaller, simpler model (the student). This approach can also be used for debugging deep learning models.</li>\n  <li>The goal of student-teacher approach is to transfer the knowledge and generalization capabilities of the teacher model to the student model, making it more compact, computationally efficient, and sometimes more interpretable while maintaining a competitive level of performance. The student model is trained using a combination of the original training data and the soft targets provided by the teacher model.</li>\n  <li>Here’s how the student-teacher approach can be used for debugging:\n    <ul>\n      <li>Train a larger, more complex model (the teacher) on the full training dataset.</li>\n      <li>Use the trained teacher model to generate predictions (soft targets) for the training dataset.</li>\n      <li>Train a smaller, simpler model (the student) on the same training dataset, using the soft targets generated by the teacher model as a guide.</li>\n      <li>Evaluate the performance of the student model on a validation dataset. If the performance of the student model is not satisfactory, the soft targets generated by the teacher model can be used to identify and correct the areas where the student model is struggling.</li>\n    </ul>\n  </li>\n  <li>\n    <p>This approach can be particularly useful for identifying and correcting issues with the student model’s generalization performance, as the teacher model has already learned to generalize well on the training dataset.</p>\n  </li>\n  <li>If the student model performs better than the teacher model, it may indicate that the teacher model is too complex or has overfit the training data. In this case, it may be necessary to re-evaluate the teacher model and make adjustments to improve its performance.</li>\n  <li>There are a few potential reasons why the student model may outperform the teacher model:\n    <ul>\n      <li><strong>Regularization:</strong> The student model may have been regularized more effectively than the teacher model, preventing overfitting.</li>\n      <li><strong>Model architecture:</strong> The student model may have a simpler, more efficient architecture that is better suited to the task at hand.</li>\n      <li><strong>Learning rate and optimizer:</strong> The student model may be using a more appropriate learning rate and optimizer than the teacher model, leading to better convergence and performance.</li>\n    </ul>\n  </li>\n  <li>If the student model is performing better than the teacher model, it may be worth investigating these potential reasons to identify areas for improvement in the teacher model. Additionally, the student model may still benefit from further optimization, such as hyperparameter tuning or ensemble learning.</li>\n  <li>During the training process, the student model tries to mimic the soft targets generated by the teacher model. This is typically done by adding a regularization term to the loss function that encourages the student model to match the soft targets. The regularization term is often implemented using the Kullback-Leibler (KL) divergence between the soft targets and the student model’s predictions.</li>\n  <li>The training process for the student model typically involves the following steps:\n    <ul>\n      <li><strong>Forward pass:</strong> The input data is passed through the student model to generate a prediction.</li>\n      <li><strong>Compute loss:</strong> The student model’s prediction is compared to the soft targets generated by the teacher model, and the KL divergence between the two distributions is computed.</li>\n      <li><strong>Backward pass:</strong> The gradients of the loss function with respect to the student model’s parameters are computed using backpropagation.</li>\n      <li><strong>pdate parameters:</strong> The student model’s parameters are updated using an optimizer such as stochastic gradient descent (SGD) or Adam.</li>\n    </ul>\n  </li>\n  <li>This process is repeated for multiple epochs until the student model’s performance on a validation set converges.</li>\n</ul>\n<ul>\n      <li>Train a larger, more complex model (the teacher) on the full training dataset.</li>\n      <li>Use the trained teacher model to generate predictions (soft targets) for the training dataset.</li>\n      <li>Train a smaller, simpler model (the student) on the same training dataset, using the soft targets generated by the teacher model as a guide.</li>\n      <li>Evaluate the performance of the student model on a validation dataset. If the performance of the student model is not satisfactory, the soft targets generated by the teacher model can be used to identify and correct the areas where the student model is struggling.</li>\n    </ul>\n<p>This approach can be particularly useful for identifying and correcting issues with the student model’s generalization performance, as the teacher model has already learned to generalize well on the training dataset.</p>\n<ul>\n      <li><strong>Regularization:</strong> The student model may have been regularized more effectively than the teacher model, preventing overfitting.</li>\n      <li><strong>Model architecture:</strong> The student model may have a simpler, more efficient architecture that is better suited to the task at hand.</li>\n      <li><strong>Learning rate and optimizer:</strong> The student model may be using a more appropriate learning rate and optimizer than the teacher model, leading to better convergence and performance.</li>\n    </ul>\n<ul>\n      <li><strong>Forward pass:</strong> The input data is passed through the student model to generate a prediction.</li>\n      <li><strong>Compute loss:</strong> The student model’s prediction is compared to the soft targets generated by the teacher model, and the KL divergence between the two distributions is computed.</li>\n      <li><strong>Backward pass:</strong> The gradients of the loss function with respect to the student model’s parameters are computed using backpropagation.</li>\n      <li><strong>pdate parameters:</strong> The student model’s parameters are updated using an optimizer such as stochastic gradient descent (SGD) or Adam.</li>\n    </ul>",
      "contentMarkdown": "*   The student-teacher approach, also known as model or knowledge distillation, is a technique used to improve the performance of a deep learning model by using a larger, more complex model (the teacher) to train a smaller, simpler model (the student). This approach can also be used for debugging deep learning models.\n*   The goal of student-teacher approach is to transfer the knowledge and generalization capabilities of the teacher model to the student model, making it more compact, computationally efficient, and sometimes more interpretable while maintaining a competitive level of performance. The student model is trained using a combination of the original training data and the soft targets provided by the teacher model.\n*   Here’s how the student-teacher approach can be used for debugging:\n    *   Train a larger, more complex model (the teacher) on the full training dataset.\n    *   Use the trained teacher model to generate predictions (soft targets) for the training dataset.\n    *   Train a smaller, simpler model (the student) on the same training dataset, using the soft targets generated by the teacher model as a guide.\n    *   Evaluate the performance of the student model on a validation dataset. If the performance of the student model is not satisfactory, the soft targets generated by the teacher model can be used to identify and correct the areas where the student model is struggling.\n*   This approach can be particularly useful for identifying and correcting issues with the student model’s generalization performance, as the teacher model has already learned to generalize well on the training dataset.\n    \n*   If the student model performs better than the teacher model, it may indicate that the teacher model is too complex or has overfit the training data. In this case, it may be necessary to re-evaluate the teacher model and make adjustments to improve its performance.\n*   There are a few potential reasons why the student model may outperform the teacher model:\n    *   **Regularization:** The student model may have been regularized more effectively than the teacher model, preventing overfitting.\n    *   **Model architecture:** The student model may have a simpler, more efficient architecture that is better suited to the task at hand.\n    *   **Learning rate and optimizer:** The student model may be using a more appropriate learning rate and optimizer than the teacher model, leading to better convergence and performance.\n*   If the student model is performing better than the teacher model, it may be worth investigating these potential reasons to identify areas for improvement in the teacher model. Additionally, the student model may still benefit from further optimization, such as hyperparameter tuning or ensemble learning.\n*   During the training process, the student model tries to mimic the soft targets generated by the teacher model. This is typically done by adding a regularization term to the loss function that encourages the student model to match the soft targets. The regularization term is often implemented using the Kullback-Leibler (KL) divergence between the soft targets and the student model’s predictions.\n*   The training process for the student model typically involves the following steps:\n    *   **Forward pass:** The input data is passed through the student model to generate a prediction.\n    *   **Compute loss:** The student model’s prediction is compared to the soft targets generated by the teacher model, and the KL divergence between the two distributions is computed.\n    *   **Backward pass:** The gradients of the loss function with respect to the student model’s parameters are computed using backpropagation.\n    *   **pdate parameters:** The student model’s parameters are updated using an optimizer such as stochastic gradient descent (SGD) or Adam.\n*   This process is repeated for multiple epochs until the student model’s performance on a validation set converges.\n\n*   Train a larger, more complex model (the teacher) on the full training dataset.\n*   Use the trained teacher model to generate predictions (soft targets) for the training dataset.\n*   Train a smaller, simpler model (the student) on the same training dataset, using the soft targets generated by the teacher model as a guide.\n*   Evaluate the performance of the student model on a validation dataset. If the performance of the student model is not satisfactory, the soft targets generated by the teacher model can be used to identify and correct the areas where the student model is struggling.\n\nThis approach can be particularly useful for identifying and correcting issues with the student model’s generalization performance, as the teacher model has already learned to generalize well on the training dataset.\n\n*   **Regularization:** The student model may have been regularized more effectively than the teacher model, preventing overfitting.\n*   **Model architecture:** The student model may have a simpler, more efficient architecture that is better suited to the task at hand.\n*   **Learning rate and optimizer:** The student model may be using a more appropriate learning rate and optimizer than the teacher model, leading to better convergence and performance.\n\n*   **Forward pass:** The input data is passed through the student model to generate a prediction.\n*   **Compute loss:** The student model’s prediction is compared to the soft targets generated by the teacher model, and the KL divergence between the two distributions is computed.\n*   **Backward pass:** The gradients of the loss function with respect to the student model’s parameters are computed using backpropagation.\n*   **pdate parameters:** The student model’s parameters are updated using an optimizer such as stochastic gradient descent (SGD) or Adam.",
      "order": 16,
      "orderInChapter": 16,
      "difficulty": 5,
      "estimatedMinutes": 5,
      "tags": [
        "miscellaneous",
        "deep learning",
        "optimization",
        "gradient descent",
        "backpropagation",
        "loss function",
        "regularization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 888,
        "contentLength": 6386
      },
      "nextCards": [
        "ai-model-debugging-hyper-parameter-optimization-hpo-17"
      ],
      "relatedCards": [
        "ai-top-30-papers-stanfords-cs231n-convolutional-neural-networks-for-26",
        "ai-conditional-random-fields-parameter-estimation-5",
        "ai-top-30-papers-dense-passage-retrieval-for-open-domain-question-a-28",
        "ai-ml-runtimes-implementation-details-22",
        "ai-ml-runtimes-implementation-details-34"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#knowledge-distillation-/-student-teacher-approach",
      "scrapedAt": "2025-12-28T11:56:46.431Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5"
      ]
    },
    {
      "id": "ai-model-debugging-hyper-parameter-optimization-hpo-17",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Debugging Model Training",
      "articleSlug": "model-debugging",
      "chapter": "Debugging Model Training",
      "title": "Hyper-Parameter Optimization (HPO)",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>The learning strategy here should be to fork parallel trials/experiments using various sets of hyperparameter values and run the training jobs over a fixed max number of steps.</li>\n  <li>Next, figure out the best performing trial on the validation set, continue training further on the entire training dataset until the training loss no longer improves.</li>\n  <li>Below are a few methods for HPO and which to use when:</li>\n</ul>\n<h4 id=\"grid-search\">Grid Search</h4>\n<ul>\n  <li>As a recap, grid search is arguably the most basic hyperparameter tuning method. With this technique, we simply build a model for each possible combination of all of the hyperparameter values provided, evaluating each model, and selecting the architecture which produces the best results. The image below <a href=\"https://medium.com/@senapati.dipak97/grid-search-vs-random-search-d34c92946318\">(source)</a> illustrates this:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/learning/1.jpeg\" alt=\"\"></p>\n<ul>\n  <li>Grid search then trains an SVM with each pair (C, γ) in the cartesian product of these two sets and evaluates their performance on a held-out validation set (or by internal cross-validation on the training set, in which case multiple SVMs are trained per pair). Finally, the grid search algorithm outputs the settings that achieved the highest score in the validation procedure.</li>\n</ul>\n<h4 id=\"random-search\">Random Search</h4>\n<ul>\n  <li>As a recap, random Search sets up a grid of hyperparameter values and selects random combinations to train the model and score. This allows you to explicitly control the number of parameter combinations that are attempted. The number of search iterations is set based on time or resources. Scikit Learn offers the RandomizedSearchCV function for this process. The image below <a href=\"https://medium.com/@senapati.dipak97/grid-search-vs-random-search-d34c92946318\">(source)</a> illustrates this:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/learning/2.jpeg\" alt=\"\"></p>\n<blockquote>\n  <p>The chances of finding the optimal parameter are comparatively higher in random search because of the random search pattern where the model might end up being trained on the optimized parameters without any aliasing. Random search works best for lower dimensional data since the time taken to find the right set is less with less number of iterations. Random search is the best parameter search technique when there are less number of dimensions.</p>\n</blockquote>\n<p>The chances of finding the optimal parameter are comparatively higher in random search because of the random search pattern where the model might end up being trained on the optimized parameters without any aliasing. Random search works best for lower dimensional data since the time taken to find the right set is less with less number of iterations. Random search is the best parameter search technique when there are less number of dimensions.</p>\n<h4 id=\"bayesian-optimization\">Bayesian Optimization</h4>\n<ul>\n  <li>Bayesian optimization provides a more efficient alternative, where a probabilistic model is used to model the objective function, which is typically the validation accuracy of the model. The probabilistic model is updated after each evaluation, and the next hyperparameters to evaluate are chosen based on a trade-off between exploitation (exploiting areas where the model is likely to perform well based on the surrogate model) and exploration (exploring areas where the surrogate model is uncertain or has high variance).</li>\n</ul>",
      "contentMarkdown": "*   The learning strategy here should be to fork parallel trials/experiments using various sets of hyperparameter values and run the training jobs over a fixed max number of steps.\n*   Next, figure out the best performing trial on the validation set, continue training further on the entire training dataset until the training loss no longer improves.\n*   Below are a few methods for HPO and which to use when:\n\n#### Grid Search\n\n*   As a recap, grid search is arguably the most basic hyperparameter tuning method. With this technique, we simply build a model for each possible combination of all of the hyperparameter values provided, evaluating each model, and selecting the architecture which produces the best results. The image below [(source)](https://medium.com/@senapati.dipak97/grid-search-vs-random-search-d34c92946318) illustrates this:\n\n![](/primers/ai/assets/learning/1.jpeg)\n\n*   Grid search then trains an SVM with each pair (C, γ) in the cartesian product of these two sets and evaluates their performance on a held-out validation set (or by internal cross-validation on the training set, in which case multiple SVMs are trained per pair). Finally, the grid search algorithm outputs the settings that achieved the highest score in the validation procedure.\n\n#### Random Search\n\n*   As a recap, random Search sets up a grid of hyperparameter values and selects random combinations to train the model and score. This allows you to explicitly control the number of parameter combinations that are attempted. The number of search iterations is set based on time or resources. Scikit Learn offers the RandomizedSearchCV function for this process. The image below [(source)](https://medium.com/@senapati.dipak97/grid-search-vs-random-search-d34c92946318) illustrates this:\n\n![](/primers/ai/assets/learning/2.jpeg)\n\n> The chances of finding the optimal parameter are comparatively higher in random search because of the random search pattern where the model might end up being trained on the optimized parameters without any aliasing. Random search works best for lower dimensional data since the time taken to find the right set is less with less number of iterations. Random search is the best parameter search technique when there are less number of dimensions.\n\nThe chances of finding the optimal parameter are comparatively higher in random search because of the random search pattern where the model might end up being trained on the optimized parameters without any aliasing. Random search works best for lower dimensional data since the time taken to find the right set is less with less number of iterations. Random search is the best parameter search technique when there are less number of dimensions.\n\n#### Bayesian Optimization\n\n*   Bayesian optimization provides a more efficient alternative, where a probabilistic model is used to model the objective function, which is typically the validation accuracy of the model. The probabilistic model is updated after each evaluation, and the next hyperparameters to evaluate are chosen based on a trade-off between exploitation (exploiting areas where the model is likely to perform well based on the surrogate model) and exploration (exploring areas where the surrogate model is uncertain or has high variance).",
      "order": 17,
      "orderInChapter": 17,
      "difficulty": 4,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous",
        "optimization",
        "cross-validation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 484,
        "contentLength": 3551
      },
      "nextCards": [],
      "relatedCards": [
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-pros-and-cons-6",
        "ai-ml-runtimes-implementation-details-22",
        "ai-ml-runtimes-pros-and-cons-23"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/model-debugging/#hyper-parameter-optimization-(hpo)",
      "scrapedAt": "2025-12-28T11:56:46.431Z",
      "siblings": [
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-framework-for-debugging-2",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-model-debugging-data-issues-4",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-common-architectural-layers-1",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Architecture Overview of On-Device ML Runtimes",
      "title": "Common Architectural Layers",
      "subtitle": "Architecture Overview of On-Device ML Runtimes",
      "contentHtml": "<ul>\n  <li>\n    <p>Most on-device ML runtimes follow a layered architecture consisting of the following components:</p>\n\n    <ul>\n      <li>\n        <p><strong>Model Loader / Parser</strong>: Responsible for reading serialized model files (e.g., <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code>, <code class=\"language-plaintext highlighter-rouge\">.tflite</code>, <code class=\"language-plaintext highlighter-rouge\">.onnx</code>, <code class=\"language-plaintext highlighter-rouge\">.pt</code>, etc.) and converting them into an internal representation suitable for execution.</p>\n      </li>\n      <li>\n        <p><strong>Serialization Format:</strong> Defines how models are stored on disk. Most runtimes use specialized formats (e.g., FlatBuffer in TFLite, Protobuf in TensorFlow/ONNX). Protobuf offers fast binary encoding and structured metadata representation, and is common in ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>) and TensorFlow (<code class=\"language-plaintext highlighter-rouge\">.pb</code>) models.</p>\n      </li>\n      <li>\n        <p><strong>Intermediate Representation (IR)</strong>: Some runtimes convert models into an internal graph or IR that enables further optimization and abstraction from the original framework.</p>\n      </li>\n      <li>\n        <p><strong>Kernel / Operator Library</strong>: A collection of pre-implemented mathematical operations (e.g., convolution, matmul, ReLU) that form the backbone of computation. These may be hand-optimized for specific CPU, GPU, NPU, or DSP targets.</p>\n      </li>\n      <li>\n        <p><strong>Execution Engine / Scheduler</strong>: Coordinates the evaluation of the computational graph, manages dependencies, and dispatches workloads to the appropriate hardware accelerators.</p>\n      </li>\n      <li>\n        <p><strong>Hardware Abstraction Layer (HAL)</strong>: Encapsulates hardware-specific APIs and provides runtime support for leveraging specialized units like Apple’s ANE, Qualcomm’s Hexagon DSP, or CUDA cores on NVIDIA GPUs.</p>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p>Most on-device ML runtimes follow a layered architecture consisting of the following components:</p>\n<ul>\n      <li>\n        <p><strong>Model Loader / Parser</strong>: Responsible for reading serialized model files (e.g., <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code>, <code class=\"language-plaintext highlighter-rouge\">.tflite</code>, <code class=\"language-plaintext highlighter-rouge\">.onnx</code>, <code class=\"language-plaintext highlighter-rouge\">.pt</code>, etc.) and converting them into an internal representation suitable for execution.</p>\n      </li>\n      <li>\n        <p><strong>Serialization Format:</strong> Defines how models are stored on disk. Most runtimes use specialized formats (e.g., FlatBuffer in TFLite, Protobuf in TensorFlow/ONNX). Protobuf offers fast binary encoding and structured metadata representation, and is common in ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>) and TensorFlow (<code class=\"language-plaintext highlighter-rouge\">.pb</code>) models.</p>\n      </li>\n      <li>\n        <p><strong>Intermediate Representation (IR)</strong>: Some runtimes convert models into an internal graph or IR that enables further optimization and abstraction from the original framework.</p>\n      </li>\n      <li>\n        <p><strong>Kernel / Operator Library</strong>: A collection of pre-implemented mathematical operations (e.g., convolution, matmul, ReLU) that form the backbone of computation. These may be hand-optimized for specific CPU, GPU, NPU, or DSP targets.</p>\n      </li>\n      <li>\n        <p><strong>Execution Engine / Scheduler</strong>: Coordinates the evaluation of the computational graph, manages dependencies, and dispatches workloads to the appropriate hardware accelerators.</p>\n      </li>\n      <li>\n        <p><strong>Hardware Abstraction Layer (HAL)</strong>: Encapsulates hardware-specific APIs and provides runtime support for leveraging specialized units like Apple’s ANE, Qualcomm’s Hexagon DSP, or CUDA cores on NVIDIA GPUs.</p>\n      </li>\n    </ul>\n<p><strong>Model Loader / Parser</strong>: Responsible for reading serialized model files (e.g., <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code>, <code class=\"language-plaintext highlighter-rouge\">.tflite</code>, <code class=\"language-plaintext highlighter-rouge\">.onnx</code>, <code class=\"language-plaintext highlighter-rouge\">.pt</code>, etc.) and converting them into an internal representation suitable for execution.</p>\n<p><strong>Serialization Format:</strong> Defines how models are stored on disk. Most runtimes use specialized formats (e.g., FlatBuffer in TFLite, Protobuf in TensorFlow/ONNX). Protobuf offers fast binary encoding and structured metadata representation, and is common in ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>) and TensorFlow (<code class=\"language-plaintext highlighter-rouge\">.pb</code>) models.</p>\n<p><strong>Intermediate Representation (IR)</strong>: Some runtimes convert models into an internal graph or IR that enables further optimization and abstraction from the original framework.</p>\n<p><strong>Kernel / Operator Library</strong>: A collection of pre-implemented mathematical operations (e.g., convolution, matmul, ReLU) that form the backbone of computation. These may be hand-optimized for specific CPU, GPU, NPU, or DSP targets.</p>\n<p><strong>Execution Engine / Scheduler</strong>: Coordinates the evaluation of the computational graph, manages dependencies, and dispatches workloads to the appropriate hardware accelerators.</p>\n<p><strong>Hardware Abstraction Layer (HAL)</strong>: Encapsulates hardware-specific APIs and provides runtime support for leveraging specialized units like Apple’s ANE, Qualcomm’s Hexagon DSP, or CUDA cores on NVIDIA GPUs.</p>",
      "contentMarkdown": "*   Most on-device ML runtimes follow a layered architecture consisting of the following components:\n    \n    *   **Model Loader / Parser**: Responsible for reading serialized model files (e.g., `.mlmodel`, `.tflite`, `.onnx`, `.pt`, etc.) and converting them into an internal representation suitable for execution.\n        \n    *   **Serialization Format:** Defines how models are stored on disk. Most runtimes use specialized formats (e.g., FlatBuffer in TFLite, Protobuf in TensorFlow/ONNX). Protobuf offers fast binary encoding and structured metadata representation, and is common in ONNX (`.onnx`) and TensorFlow (`.pb`) models.\n        \n    *   **Intermediate Representation (IR)**: Some runtimes convert models into an internal graph or IR that enables further optimization and abstraction from the original framework.\n        \n    *   **Kernel / Operator Library**: A collection of pre-implemented mathematical operations (e.g., convolution, matmul, ReLU) that form the backbone of computation. These may be hand-optimized for specific CPU, GPU, NPU, or DSP targets.\n        \n    *   **Execution Engine / Scheduler**: Coordinates the evaluation of the computational graph, manages dependencies, and dispatches workloads to the appropriate hardware accelerators.\n        \n    *   **Hardware Abstraction Layer (HAL)**: Encapsulates hardware-specific APIs and provides runtime support for leveraging specialized units like Apple’s ANE, Qualcomm’s Hexagon DSP, or CUDA cores on NVIDIA GPUs.\n        \n\nMost on-device ML runtimes follow a layered architecture consisting of the following components:\n\n*   **Model Loader / Parser**: Responsible for reading serialized model files (e.g., `.mlmodel`, `.tflite`, `.onnx`, `.pt`, etc.) and converting them into an internal representation suitable for execution.\n    \n*   **Serialization Format:** Defines how models are stored on disk. Most runtimes use specialized formats (e.g., FlatBuffer in TFLite, Protobuf in TensorFlow/ONNX). Protobuf offers fast binary encoding and structured metadata representation, and is common in ONNX (`.onnx`) and TensorFlow (`.pb`) models.\n    \n*   **Intermediate Representation (IR)**: Some runtimes convert models into an internal graph or IR that enables further optimization and abstraction from the original framework.\n    \n*   **Kernel / Operator Library**: A collection of pre-implemented mathematical operations (e.g., convolution, matmul, ReLU) that form the backbone of computation. These may be hand-optimized for specific CPU, GPU, NPU, or DSP targets.\n    \n*   **Execution Engine / Scheduler**: Coordinates the evaluation of the computational graph, manages dependencies, and dispatches workloads to the appropriate hardware accelerators.\n    \n*   **Hardware Abstraction Layer (HAL)**: Encapsulates hardware-specific APIs and provides runtime support for leveraging specialized units like Apple’s ANE, Qualcomm’s Hexagon DSP, or CUDA cores on NVIDIA GPUs.\n    \n\n**Model Loader / Parser**: Responsible for reading serialized model files (e.g., `.mlmodel`, `.tflite`, `.onnx`, `.pt`, etc.) and converting them into an internal representation suitable for execution.\n\n**Serialization Format:** Defines how models are stored on disk. Most runtimes use specialized formats (e.g., FlatBuffer in TFLite, Protobuf in TensorFlow/ONNX). Protobuf offers fast binary encoding and structured metadata representation, and is common in ONNX (`.onnx`) and TensorFlow (`.pb`) models.\n\n**Intermediate Representation (IR)**: Some runtimes convert models into an internal graph or IR that enables further optimization and abstraction from the original framework.\n\n**Kernel / Operator Library**: A collection of pre-implemented mathematical operations (e.g., convolution, matmul, ReLU) that form the backbone of computation. These may be hand-optimized for specific CPU, GPU, NPU, or DSP targets.\n\n**Execution Engine / Scheduler**: Coordinates the evaluation of the computational graph, manages dependencies, and dispatches workloads to the appropriate hardware accelerators.\n\n**Hardware Abstraction Layer (HAL)**: Encapsulates hardware-specific APIs and provides runtime support for leveraging specialized units like Apple’s ANE, Qualcomm’s Hexagon DSP, or CUDA cores on NVIDIA GPUs.",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous",
        "convolution",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 546,
        "contentLength": 5903
      },
      "nextCards": [
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3"
      ],
      "relatedCards": [
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-model-definition-and-training-cpugpu-13",
        "ai-gpu-architecture-kernel-launch-and-execution-on-sms-16",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#common-architectural-layers",
      "scrapedAt": "2025-12-28T11:56:51.670Z",
      "siblings": [
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5",
        "ai-ml-runtimes-pros-and-cons-6"
      ]
    },
    {
      "id": "ai-ml-runtimes-architecture-by-runtime-2",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Architecture Overview of On-Device ML Runtimes",
      "title": "Architecture by Runtime",
      "subtitle": "Architecture Overview of On-Device ML Runtimes",
      "contentHtml": "<h4 id=\"tensorrt\">TensorRT</h4>\n<ul>\n  <li><strong>Model Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.plan</code> (TensorRT Engine)</li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>Accepts models in ONNX, TensorFlow, or Caffe formats</li>\n      <li>Optimizes and compiles model into a serialized CUDA engine (<code class=\"language-plaintext highlighter-rouge\">.plan</code>)</li>\n      <li>Engine executes directly via CUDA on supported NVIDIA GPUs</li>\n    </ul>\n  </li>\n  <li><strong>Hardware Support</strong>: NVIDIA GPUs (desktop, embedded, server)</li>\n  <li><strong>Backend Design</strong>: Layer fusion, kernel autotuning, <code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">float16</code> quantization, Tensor Cores</li>\n  <li><strong>Strengths</strong>: Extreme inference speed on NVIDIA hardware, minimal latency, quantization support</li>\n  <li><strong>Weaknesses</strong>: GPU-only, requires CUDA, less flexible for model updates at runtime</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>Accepts models in ONNX, TensorFlow, or Caffe formats</li>\n      <li>Optimizes and compiles model into a serialized CUDA engine (<code class=\"language-plaintext highlighter-rouge\">.plan</code>)</li>\n      <li>Engine executes directly via CUDA on supported NVIDIA GPUs</li>\n    </ul>\n<h4 id=\"core-ml\">Core ML</h4>\n<ul>\n  <li><strong>Model Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code>, optionally converted from other formats using <code class=\"language-plaintext highlighter-rouge\">coremltools</code></li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>Model is compiled into a Core ML model package (<code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code>)</li>\n      <li>Uses internal execution graph</li>\n      <li>Runtime determines target hardware (CPU, GPU, or ANE) dynamically</li>\n    </ul>\n  </li>\n  <li><strong>Hardware Support</strong>: CPU, GPU, Apple Neural Engine (ANE)</li>\n  <li><strong>Backend Design</strong>: Proprietary graph engine, no direct user-accessible IR</li>\n  <li><strong>Strengths</strong>: Seamless Apple integration, high-level API, automatic hardware optimization</li>\n  <li><strong>Weaknesses</strong>: Apple-platform only, opaque architecture, limited transparency for debugging</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>Model is compiled into a Core ML model package (<code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code>)</li>\n      <li>Uses internal execution graph</li>\n      <li>Runtime determines target hardware (CPU, GPU, or ANE) dynamically</li>\n    </ul>\n<h4 id=\"mlx-apple-mlx\">MLX (Apple MLX)</h4>\n<ul>\n  <li><strong>Model Format</strong>: Python-based tensor operations with PyTorch-like syntax</li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>Eager mode and graph execution both supported</li>\n      <li>Uses Metal Performance Shaders and ANE backend where possible</li>\n    </ul>\n  </li>\n  <li><strong>Hardware Support</strong>: Primarily Apple Silicon (M-series CPU, GPU, ANE)</li>\n  <li><strong>Backend Design</strong>: Dynamic execution engine; uses MLX backend API</li>\n  <li><strong>Strengths</strong>: Developer flexibility, research-oriented, direct tensor ops</li>\n  <li><strong>Weaknesses</strong>: Early-stage, Apple-only, smaller community, fewer pre-built models</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>Eager mode and graph execution both supported</li>\n      <li>Uses Metal Performance Shaders and ANE backend where possible</li>\n    </ul>\n<h4 id=\"onnx-runtime\">ONNX Runtime</h4>\n<ul>\n  <li><strong>Model Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.onnx</code></li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>Loads ONNX graph and converts to optimized IR</li>\n      <li>Graph optimization passes applied (e.g., constant folding, fusion)</li>\n      <li>Execution providers (EPs) handle hardware-specific execution</li>\n    </ul>\n  </li>\n  <li><strong>Hardware Support</strong>: CPU, GPU (CUDA, ROCm), NNAPI, DirectML, ARM, OpenVINO</li>\n  <li><strong>Backend Design</strong>: Pluggable EP system, modular kernel dispatch</li>\n  <li><strong>Strengths</strong>: Cross-platform, flexible, highly optimized</li>\n  <li><strong>Weaknesses</strong>: Model conversion may be lossy or complex, mobile-specific tuning needed</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>Loads ONNX graph and converts to optimized IR</li>\n      <li>Graph optimization passes applied (e.g., constant folding, fusion)</li>\n      <li>Execution providers (EPs) handle hardware-specific execution</li>\n    </ul>\n<h4 id=\"executorch\">ExecuTorch</h4>\n<ul>\n  <li><strong>Model Format</strong>: PyTorch Lite models, <code class=\"language-plaintext highlighter-rouge\">ptc</code> compiled bytecode</li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>TorchScript traced models compiled using Ahead-of-Time (AOT) compiler</li>\n      <li>Produces a minimal runtime with only needed ops</li>\n      <li>Bytecode is executed on microcontroller or mobile device</li>\n    </ul>\n  </li>\n  <li><strong>Hardware Support</strong>: CPU, MCU, potentially DSP/NPU</li>\n  <li><strong>Backend Design</strong>: AOT compiler, custom micro runtime, graph executor</li>\n  <li><strong>Strengths</strong>: Lightweight, optimized for resource-constrained environments</li>\n  <li><strong>Weaknesses</strong>: Limited model format support, newer toolchain</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>TorchScript traced models compiled using Ahead-of-Time (AOT) compiler</li>\n      <li>Produces a minimal runtime with only needed ops</li>\n      <li>Bytecode is executed on microcontroller or mobile device</li>\n    </ul>\n<h4 id=\"lidartlm\">LidarTLM</h4>\n<ul>\n  <li><strong>Model Format</strong>: Custom or converted models for lidar data processing</li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>Ingests sparse point cloud or voxel data</li>\n      <li>Uses spatial and temporal inference pipelines</li>\n    </ul>\n  </li>\n  <li><strong>Hardware Support</strong>: ARM CPUs, embedded GPU, or AI co-processors</li>\n  <li><strong>Backend Design</strong>: Spatially-aware computation graph; sensor-fusion modules</li>\n  <li><strong>Strengths</strong>: Specialized for lidar, supports sensor fusion</li>\n  <li><strong>Weaknesses</strong>: Niche use case, limited community and documentation</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>Ingests sparse point cloud or voxel data</li>\n      <li>Uses spatial and temporal inference pipelines</li>\n    </ul>\n<h4 id=\"llamacpp\"><code class=\"language-plaintext Highlighter-rouge\">llama.cpp</code></h4>\n<ul>\n  <li><strong>Model Format</strong>: Quantized LLM formats (GGUF, etc.)</li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>Loads quantized model into memory</li>\n      <li>Performs batched matmul-based transformer inference</li>\n      <li>Multi-threaded CPU execution with optional GPU offload (via OpenCL, Metal)</li>\n    </ul>\n  </li>\n  <li><strong>Hardware Support</strong>: CPU, optionally GPU</li>\n  <li><strong>Backend Design</strong>: Minimalist tensor framework, custom linear algebra, no IR</li>\n  <li><strong>Strengths</strong>: Extremely portable, optimized for low-RAM devices, self-contained</li>\n  <li><strong>Weaknesses</strong>: Focused only on LLMs, lower-level interface</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>Loads quantized model into memory</li>\n      <li>Performs batched matmul-based transformer inference</li>\n      <li>Multi-threaded CPU execution with optional GPU offload (via OpenCL, Metal)</li>\n    </ul>\n<h4 id=\"tensorflow-lite--serving\">TensorFlow Lite / Serving</h4>\n<ul>\n  <li><strong>Model Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.tflite</code> (Lite), <code class=\"language-plaintext highlighter-rouge\">.pb</code> or SavedModel (Serving)</li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ul>\n      <li>TFLite: uses FlatBuffer model, loads and interprets ops</li>\n      <li>Serving: REST/gRPC server for remote model inference</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hardware Support</strong>:</p>\n\n    <ul>\n      <li>TFLite: CPU, GPU, EdgeTPU, NNAPI, Hexagon DSP</li>\n      <li>Serving: Primarily server-side; not for on-device use</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Backend Design</strong>:</p>\n\n    <ul>\n      <li>TFLite: statically compiled interpreters with kernel registry</li>\n      <li>TFLite delegates for hardware acceleration</li>\n    </ul>\n  </li>\n  <li><strong>Strengths</strong>: Broad compatibility, active ecosystem, stable</li>\n  <li><strong>Weaknesses</strong>: Delegate configuration can be tricky, Serving not suitable for offline use</li>\n</ul>\n<p><strong>Execution Flow</strong>:</p>\n<ul>\n      <li>TFLite: uses FlatBuffer model, loads and interprets ops</li>\n      <li>Serving: REST/gRPC server for remote model inference</li>\n    </ul>\n<p><strong>Hardware Support</strong>:</p>\n<ul>\n      <li>TFLite: CPU, GPU, EdgeTPU, NNAPI, Hexagon DSP</li>\n      <li>Serving: Primarily server-side; not for on-device use</li>\n    </ul>\n<p><strong>Backend Design</strong>:</p>\n<ul>\n      <li>TFLite: statically compiled interpreters with kernel registry</li>\n      <li>TFLite delegates for hardware acceleration</li>\n    </ul>",
      "contentMarkdown": "#### TensorRT\n\n*   **Model Format**: `.plan` (TensorRT Engine)\n*   **Execution Flow**:\n    \n    *   Accepts models in ONNX, TensorFlow, or Caffe formats\n    *   Optimizes and compiles model into a serialized CUDA engine (`.plan`)\n    *   Engine executes directly via CUDA on supported NVIDIA GPUs\n*   **Hardware Support**: NVIDIA GPUs (desktop, embedded, server)\n*   **Backend Design**: Layer fusion, kernel autotuning, `int8`/`float16` quantization, Tensor Cores\n*   **Strengths**: Extreme inference speed on NVIDIA hardware, minimal latency, quantization support\n*   **Weaknesses**: GPU-only, requires CUDA, less flexible for model updates at runtime\n\n**Execution Flow**:\n\n*   Accepts models in ONNX, TensorFlow, or Caffe formats\n*   Optimizes and compiles model into a serialized CUDA engine (`.plan`)\n*   Engine executes directly via CUDA on supported NVIDIA GPUs\n\n#### Core ML\n\n*   **Model Format**: `.mlmodel`, optionally converted from other formats using `coremltools`\n*   **Execution Flow**:\n    \n    *   Model is compiled into a Core ML model package (`.mlmodelc`)\n    *   Uses internal execution graph\n    *   Runtime determines target hardware (CPU, GPU, or ANE) dynamically\n*   **Hardware Support**: CPU, GPU, Apple Neural Engine (ANE)\n*   **Backend Design**: Proprietary graph engine, no direct user-accessible IR\n*   **Strengths**: Seamless Apple integration, high-level API, automatic hardware optimization\n*   **Weaknesses**: Apple-platform only, opaque architecture, limited transparency for debugging\n\n**Execution Flow**:\n\n*   Model is compiled into a Core ML model package (`.mlmodelc`)\n*   Uses internal execution graph\n*   Runtime determines target hardware (CPU, GPU, or ANE) dynamically\n\n#### MLX (Apple MLX)\n\n*   **Model Format**: Python-based tensor operations with PyTorch-like syntax\n*   **Execution Flow**:\n    \n    *   Eager mode and graph execution both supported\n    *   Uses Metal Performance Shaders and ANE backend where possible\n*   **Hardware Support**: Primarily Apple Silicon (M-series CPU, GPU, ANE)\n*   **Backend Design**: Dynamic execution engine; uses MLX backend API\n*   **Strengths**: Developer flexibility, research-oriented, direct tensor ops\n*   **Weaknesses**: Early-stage, Apple-only, smaller community, fewer pre-built models\n\n**Execution Flow**:\n\n*   Eager mode and graph execution both supported\n*   Uses Metal Performance Shaders and ANE backend where possible\n\n#### ONNX Runtime\n\n*   **Model Format**: `.onnx`\n*   **Execution Flow**:\n    \n    *   Loads ONNX graph and converts to optimized IR\n    *   Graph optimization passes applied (e.g., constant folding, fusion)\n    *   Execution providers (EPs) handle hardware-specific execution\n*   **Hardware Support**: CPU, GPU (CUDA, ROCm), NNAPI, DirectML, ARM, OpenVINO\n*   **Backend Design**: Pluggable EP system, modular kernel dispatch\n*   **Strengths**: Cross-platform, flexible, highly optimized\n*   **Weaknesses**: Model conversion may be lossy or complex, mobile-specific tuning needed\n\n**Execution Flow**:\n\n*   Loads ONNX graph and converts to optimized IR\n*   Graph optimization passes applied (e.g., constant folding, fusion)\n*   Execution providers (EPs) handle hardware-specific execution\n\n#### ExecuTorch\n\n*   **Model Format**: PyTorch Lite models, `ptc` compiled bytecode\n*   **Execution Flow**:\n    \n    *   TorchScript traced models compiled using Ahead-of-Time (AOT) compiler\n    *   Produces a minimal runtime with only needed ops\n    *   Bytecode is executed on microcontroller or mobile device\n*   **Hardware Support**: CPU, MCU, potentially DSP/NPU\n*   **Backend Design**: AOT compiler, custom micro runtime, graph executor\n*   **Strengths**: Lightweight, optimized for resource-constrained environments\n*   **Weaknesses**: Limited model format support, newer toolchain\n\n**Execution Flow**:\n\n*   TorchScript traced models compiled using Ahead-of-Time (AOT) compiler\n*   Produces a minimal runtime with only needed ops\n*   Bytecode is executed on microcontroller or mobile device\n\n#### LidarTLM\n\n*   **Model Format**: Custom or converted models for lidar data processing\n*   **Execution Flow**:\n    \n    *   Ingests sparse point cloud or voxel data\n    *   Uses spatial and temporal inference pipelines\n*   **Hardware Support**: ARM CPUs, embedded GPU, or AI co-processors\n*   **Backend Design**: Spatially-aware computation graph; sensor-fusion modules\n*   **Strengths**: Specialized for lidar, supports sensor fusion\n*   **Weaknesses**: Niche use case, limited community and documentation\n\n**Execution Flow**:\n\n*   Ingests sparse point cloud or voxel data\n*   Uses spatial and temporal inference pipelines\n\n#### `llama.cpp`\n\n*   **Model Format**: Quantized LLM formats (GGUF, etc.)\n*   **Execution Flow**:\n    \n    *   Loads quantized model into memory\n    *   Performs batched matmul-based transformer inference\n    *   Multi-threaded CPU execution with optional GPU offload (via OpenCL, Metal)\n*   **Hardware Support**: CPU, optionally GPU\n*   **Backend Design**: Minimalist tensor framework, custom linear algebra, no IR\n*   **Strengths**: Extremely portable, optimized for low-RAM devices, self-contained\n*   **Weaknesses**: Focused only on LLMs, lower-level interface\n\n**Execution Flow**:\n\n*   Loads quantized model into memory\n*   Performs batched matmul-based transformer inference\n*   Multi-threaded CPU execution with optional GPU offload (via OpenCL, Metal)\n\n#### TensorFlow Lite / Serving\n\n*   **Model Format**: `.tflite` (Lite), `.pb` or SavedModel (Serving)\n*   **Execution Flow**:\n    \n    *   TFLite: uses FlatBuffer model, loads and interprets ops\n    *   Serving: REST/gRPC server for remote model inference\n*   **Hardware Support**:\n    \n    *   TFLite: CPU, GPU, EdgeTPU, NNAPI, Hexagon DSP\n    *   Serving: Primarily server-side; not for on-device use\n*   **Backend Design**:\n    \n    *   TFLite: statically compiled interpreters with kernel registry\n    *   TFLite delegates for hardware acceleration\n*   **Strengths**: Broad compatibility, active ecosystem, stable\n*   **Weaknesses**: Delegate configuration can be tricky, Serving not suitable for offline use\n\n**Execution Flow**:\n\n*   TFLite: uses FlatBuffer model, loads and interprets ops\n*   Serving: REST/gRPC server for remote model inference\n\n**Hardware Support**:\n\n*   TFLite: CPU, GPU, EdgeTPU, NNAPI, Hexagon DSP\n*   Serving: Primarily server-side; not for on-device use\n\n**Backend Design**:\n\n*   TFLite: statically compiled interpreters with kernel registry\n*   TFLite delegates for hardware acceleration",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 4,
      "estimatedMinutes": 5,
      "tags": [
        "miscellaneous",
        "transformer",
        "llm",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 840,
        "contentLength": 9578
      },
      "nextCards": [
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4"
      ],
      "relatedCards": [
        "ai-gpu-architecture-tensor-core-evolution-23",
        "ai-gpu-architecture-per-generation-precision-support-and-performance-26",
        "ai-gpu-architecture-key-takeaways-38",
        "ai-top-30-papers-better-faster-large-language-models-via-multi-toke-27",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#architecture-by-runtime",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5",
        "ai-ml-runtimes-pros-and-cons-6"
      ]
    },
    {
      "id": "ai-ml-runtimes-overview-3",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorRT Deep Dive",
      "title": "Overview",
      "subtitle": "TensorRT Deep Dive",
      "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: Engineers deploying deep learning models on NVIDIA hardware</li>\n  <li><strong>Use Cases</strong>: Vision inference, robotics, autonomous vehicles, embedded AI with Jetson, high-throughput servers</li>\n  <li><strong>Model Format</strong>: ONNX, Caffe, TensorFlow (converted to <code class=\"language-plaintext highlighter-rouge\">.plan</code> engine)</li>\n  <li><strong>Conversion Tools</strong>: <code class=\"language-plaintext highlighter-rouge\">trtexec</code>, TensorRT Python/C++ APIs</li>\n</ul>",
      "contentMarkdown": "*   **Developer Target**: Engineers deploying deep learning models on NVIDIA hardware\n*   **Use Cases**: Vision inference, robotics, autonomous vehicles, embedded AI with Jetson, high-throughput servers\n*   **Model Format**: ONNX, Caffe, TensorFlow (converted to `.plan` engine)\n*   **Conversion Tools**: `trtexec`, TensorRT Python/C++ APIs",
      "order": 3,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "deep learning"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 42,
        "contentLength": 543
      },
      "nextCards": [
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ],
      "relatedCards": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-overview-of-precision-types-25",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-top-30-papers-deep-residual-learning-for-image-recognition-10",
        "ai-ann-similarity-search-scann-scalable-nearest-neighbors-11"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#overview",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5",
        "ai-ml-runtimes-pros-and-cons-6"
      ]
    },
    {
      "id": "ai-ml-runtimes-architecture-4",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorRT Deep Dive",
      "title": "Architecture",
      "subtitle": "TensorRT Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p>TensorRT transforms trained models into an optimized engine using multiple optimization passes:</p>\n  </li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ol>\n      <li><strong>Model Import</strong>: Loads model (typically ONNX) using TensorRT parser</li>\n      <li>\n        <p><strong>Optimization</strong>:</p>\n\n        <ul>\n          <li>Layer fusion</li>\n          <li>Precision calibration (<code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">int8</code>)</li>\n          <li>Kernel selection and scheduling</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Engine Building</strong>:</p>\n\n        <ul>\n          <li>Generates a <code class=\"language-plaintext highlighter-rouge\">.plan</code> file (serialized CUDA engine)</li>\n          <li>This engine can be reused for fast deployment</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Inference Execution</strong>:</p>\n\n        <ul>\n          <li>Input data fed through pre-allocated CUDA buffers</li>\n          <li>Execution is entirely GPU-bound using CUDA streams</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Key Components</strong>:</p>\n\n    <ul>\n      <li><strong>Builder</strong>: Optimizes and generates runtime engine</li>\n      <li><strong>Runtime</strong>: Loads and executes serialized engine</li>\n      <li><strong>Execution Context</strong>: Holds all buffers and workspace</li>\n      <li><strong>Calibrator</strong>: Generates <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization scale factors using sample data</li>\n    </ul>\n  </li>\n</ul>\n<p>TensorRT transforms trained models into an optimized engine using multiple optimization passes:</p>\n<p><strong>Execution Flow</strong>:</p>\n<ol>\n      <li><strong>Model Import</strong>: Loads model (typically ONNX) using TensorRT parser</li>\n      <li>\n        <p><strong>Optimization</strong>:</p>\n\n        <ul>\n          <li>Layer fusion</li>\n          <li>Precision calibration (<code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">int8</code>)</li>\n          <li>Kernel selection and scheduling</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Engine Building</strong>:</p>\n\n        <ul>\n          <li>Generates a <code class=\"language-plaintext highlighter-rouge\">.plan</code> file (serialized CUDA engine)</li>\n          <li>This engine can be reused for fast deployment</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Inference Execution</strong>:</p>\n\n        <ul>\n          <li>Input data fed through pre-allocated CUDA buffers</li>\n          <li>Execution is entirely GPU-bound using CUDA streams</li>\n        </ul>\n      </li>\n    </ol>\n<p><strong>Optimization</strong>:</p>\n<ul>\n          <li>Layer fusion</li>\n          <li>Precision calibration (<code class=\"language-plaintext highlighter-rouge\">float16</code>, <code class=\"language-plaintext highlighter-rouge\">int8</code>)</li>\n          <li>Kernel selection and scheduling</li>\n        </ul>\n<p><strong>Engine Building</strong>:</p>\n<ul>\n          <li>Generates a <code class=\"language-plaintext highlighter-rouge\">.plan</code> file (serialized CUDA engine)</li>\n          <li>This engine can be reused for fast deployment</li>\n        </ul>\n<p><strong>Inference Execution</strong>:</p>\n<ul>\n          <li>Input data fed through pre-allocated CUDA buffers</li>\n          <li>Execution is entirely GPU-bound using CUDA streams</li>\n        </ul>\n<p><strong>Key Components</strong>:</p>\n<ul>\n      <li><strong>Builder</strong>: Optimizes and generates runtime engine</li>\n      <li><strong>Runtime</strong>: Loads and executes serialized engine</li>\n      <li><strong>Execution Context</strong>: Holds all buffers and workspace</li>\n      <li><strong>Calibrator</strong>: Generates <code class=\"language-plaintext highlighter-rouge\">int8</code> quantization scale factors using sample data</li>\n    </ul>",
      "contentMarkdown": "*   TensorRT transforms trained models into an optimized engine using multiple optimization passes:\n    \n*   **Execution Flow**:\n    \n    1.  **Model Import**: Loads model (typically ONNX) using TensorRT parser\n    2.  **Optimization**:\n        \n        *   Layer fusion\n        *   Precision calibration (`float16`, `int8`)\n        *   Kernel selection and scheduling\n    3.  **Engine Building**:\n        \n        *   Generates a `.plan` file (serialized CUDA engine)\n        *   This engine can be reused for fast deployment\n    4.  **Inference Execution**:\n        \n        *   Input data fed through pre-allocated CUDA buffers\n        *   Execution is entirely GPU-bound using CUDA streams\n*   **Key Components**:\n    \n    *   **Builder**: Optimizes and generates runtime engine\n    *   **Runtime**: Loads and executes serialized engine\n    *   **Execution Context**: Holds all buffers and workspace\n    *   **Calibrator**: Generates `int8` quantization scale factors using sample data\n\nTensorRT transforms trained models into an optimized engine using multiple optimization passes:\n\n**Execution Flow**:\n\n1.  **Model Import**: Loads model (typically ONNX) using TensorRT parser\n2.  **Optimization**:\n    \n    *   Layer fusion\n    *   Precision calibration (`float16`, `int8`)\n    *   Kernel selection and scheduling\n3.  **Engine Building**:\n    \n    *   Generates a `.plan` file (serialized CUDA engine)\n    *   This engine can be reused for fast deployment\n4.  **Inference Execution**:\n    \n    *   Input data fed through pre-allocated CUDA buffers\n    *   Execution is entirely GPU-bound using CUDA streams\n\n**Optimization**:\n\n*   Layer fusion\n*   Precision calibration (`float16`, `int8`)\n*   Kernel selection and scheduling\n\n**Engine Building**:\n\n*   Generates a `.plan` file (serialized CUDA engine)\n*   This engine can be reused for fast deployment\n\n**Inference Execution**:\n\n*   Input data fed through pre-allocated CUDA buffers\n*   Execution is entirely GPU-bound using CUDA streams\n\n**Key Components**:\n\n*   **Builder**: Optimizes and generates runtime engine\n*   **Runtime**: Loads and executes serialized engine\n*   **Execution Context**: Holds all buffers and workspace\n*   **Calibrator**: Generates `int8` quantization scale factors using sample data",
      "order": 4,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 278,
        "contentLength": 4041
      },
      "nextCards": [
        "ai-ml-runtimes-implementation-details-5",
        "ai-ml-runtimes-pros-and-cons-6"
      ],
      "relatedCards": [
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-top-30-papers-deep-residual-learning-for-image-recognition-10",
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#architecture",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-implementation-details-5",
        "ai-ml-runtimes-pros-and-cons-6"
      ]
    },
    {
      "id": "ai-ml-runtimes-implementation-details-5",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorRT Deep Dive",
      "title": "Implementation Details",
      "subtitle": "TensorRT Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Quantization Support</strong>:</p>\n\n    <ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">float32</code>, <code class=\"language-plaintext highlighter-rouge\">float16</code>, and <code class=\"language-plaintext highlighter-rouge\">int8</code> precision modes</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">int8</code> requires calibration dataset (representative samples)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Layer Fusion</strong>:</p>\n\n    <ul>\n      <li>Combines ops like conv + bias + activation into a single kernel</li>\n      <li>Reduces memory overhead and execution latency</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Dynamic Shapes</strong>:</p>\n\n    <ul>\n      <li>Supports engines that accept varying input sizes with shape profiles</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Deployment</strong>:</p>\n\n    <ul>\n      <li>Supports inference from Python or C++</li>\n      <li>Compatible with DeepStream SDK, TensorRT-LLM, and Jetson platforms</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Quantization Support</strong>:</p>\n<ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">float32</code>, <code class=\"language-plaintext highlighter-rouge\">float16</code>, and <code class=\"language-plaintext highlighter-rouge\">int8</code> precision modes</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">int8</code> requires calibration dataset (representative samples)</li>\n    </ul>\n<p><strong>Layer Fusion</strong>:</p>\n<ul>\n      <li>Combines ops like conv + bias + activation into a single kernel</li>\n      <li>Reduces memory overhead and execution latency</li>\n    </ul>\n<p><strong>Dynamic Shapes</strong>:</p>\n<ul>\n      <li>Supports engines that accept varying input sizes with shape profiles</li>\n    </ul>\n<p><strong>Deployment</strong>:</p>\n<ul>\n      <li>Supports inference from Python or C++</li>\n      <li>Compatible with DeepStream SDK, TensorRT-LLM, and Jetson platforms</li>\n    </ul>",
      "contentMarkdown": "*   **Quantization Support**:\n    \n    *   `float32`, `float16`, and `int8` precision modes\n    *   `int8` requires calibration dataset (representative samples)\n*   **Layer Fusion**:\n    \n    *   Combines ops like conv + bias + activation into a single kernel\n    *   Reduces memory overhead and execution latency\n*   **Dynamic Shapes**:\n    \n    *   Supports engines that accept varying input sizes with shape profiles\n*   **Deployment**:\n    \n    *   Supports inference from Python or C++\n    *   Compatible with DeepStream SDK, TensorRT-LLM, and Jetson platforms\n\n**Quantization Support**:\n\n*   `float32`, `float16`, and `int8` precision modes\n*   `int8` requires calibration dataset (representative samples)\n\n**Layer Fusion**:\n\n*   Combines ops like conv + bias + activation into a single kernel\n*   Reduces memory overhead and execution latency\n\n**Dynamic Shapes**:\n\n*   Supports engines that accept varying input sizes with shape profiles\n\n**Deployment**:\n\n*   Supports inference from Python or C++\n*   Compatible with DeepStream SDK, TensorRT-LLM, and Jetson platforms",
      "order": 5,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "llm",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 140,
        "contentLength": 1992
      },
      "nextCards": [
        "ai-ml-runtimes-pros-and-cons-6",
        "ai-ml-runtimes-example-workflow-7"
      ],
      "relatedCards": [
        "ai-gpu-architecture-forward-and-backward-passes-training-17",
        "ai-gpu-architecture-precision-optimization-19",
        "ai-model-debugging-grad-cam-15",
        "ai-top-30-papers-alcuna-large-language-models-meet-new-knowledge-33",
        "ai-top-30-papers-the-perils-promises-of-fact-checking-with-large-la-34"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-pros-and-cons-6"
      ]
    },
    {
      "id": "ai-ml-runtimes-pros-and-cons-6",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorRT Deep Dive",
      "title": "Pros and Cons",
      "subtitle": "TensorRT Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li>Best-in-class GPU inference performance</li>\n      <li>Optimized for Tensor Cores (Ampere, Hopper, etc.)</li>\n      <li>Rich tooling (e.g., <code class=\"language-plaintext highlighter-rouge\">trtexec</code>, calibration tools)</li>\n      <li>Integration with Jetson for embedded AI</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>Requires NVIDIA GPU and CUDA runtime</li>\n      <li>Not suitable for CPU or cross-platform apps</li>\n      <li>Build/optimization pipeline adds complexity</li>\n      <li>Engine regeneration needed if input shape or model changes significantly</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li>Best-in-class GPU inference performance</li>\n      <li>Optimized for Tensor Cores (Ampere, Hopper, etc.)</li>\n      <li>Rich tooling (e.g., <code class=\"language-plaintext highlighter-rouge\">trtexec</code>, calibration tools)</li>\n      <li>Integration with Jetson for embedded AI</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>Requires NVIDIA GPU and CUDA runtime</li>\n      <li>Not suitable for CPU or cross-platform apps</li>\n      <li>Build/optimization pipeline adds complexity</li>\n      <li>Engine regeneration needed if input shape or model changes significantly</li>\n    </ul>",
      "contentMarkdown": "*   **Pros**:\n    \n    *   Best-in-class GPU inference performance\n    *   Optimized for Tensor Cores (Ampere, Hopper, etc.)\n    *   Rich tooling (e.g., `trtexec`, calibration tools)\n    *   Integration with Jetson for embedded AI\n*   **Cons**:\n    \n    *   Requires NVIDIA GPU and CUDA runtime\n    *   Not suitable for CPU or cross-platform apps\n    *   Build/optimization pipeline adds complexity\n    *   Engine regeneration needed if input shape or model changes significantly\n\n**Pros**:\n\n*   Best-in-class GPU inference performance\n*   Optimized for Tensor Cores (Ampere, Hopper, etc.)\n*   Rich tooling (e.g., `trtexec`, calibration tools)\n*   Integration with Jetson for embedded AI\n\n**Cons**:\n\n*   Requires NVIDIA GPU and CUDA runtime\n*   Not suitable for CPU or cross-platform apps\n*   Build/optimization pipeline adds complexity\n*   Engine regeneration needed if input shape or model changes significantly",
      "order": 6,
      "orderInChapter": 4,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 122,
        "contentLength": 1346
      },
      "nextCards": [
        "ai-ml-runtimes-example-workflow-7",
        "ai-ml-runtimes-suitable-applications-8"
      ],
      "relatedCards": [
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-top-30-papers-deep-residual-learning-for-image-recognition-10",
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-example-workflow-7",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorRT Deep Dive",
      "title": "Example Workflow",
      "subtitle": "TensorRT Deep Dive",
      "contentHtml": "<ul>\n  <li><strong>Model Conversion (ONNX <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 1.191em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.984em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.552em, 1000.93em, 2.275em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mo\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">\\rightarrow</script> Engine):</strong></li>\n</ul>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\">trtexec <span class=\"nt\">--onnx</span><span class=\"o\">=</span>model.onnx <span class=\"nt\">--saveEngine</span><span class=\"o\">=</span>model.plan <span class=\"nt\">--</span><span class=\"sb\">`</span>float16<span class=\"sb\">`</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\">trtexec <span class=\"nt\">--onnx</span><span class=\"o\">=</span>model.onnx <span class=\"nt\">--saveEngine</span><span class=\"o\">=</span>model.plan <span class=\"nt\">--</span><span class=\"sb\">`</span>float16<span class=\"sb\">`</span>\n</code></pre>\n<ul>\n  <li><strong>C++ Inference:</strong></li>\n</ul>\n<div class=\"language-cpp highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\"><span class=\"n\">nvinfer1</span><span class=\"o\">::</span><span class=\"n\">IRuntime</span><span class=\"o\">*</span> <span class=\"n\">runtime</span> <span class=\"o\">=</span> <span class=\"n\">nvinfer1</span><span class=\"o\">::</span><span class=\"n\">createInferRuntime</span><span class=\"p\">(</span><span class=\"n\">logger</span><span class=\"p\">);</span>\n<span class=\"n\">std</span><span class=\"o\">::</span><span class=\"n\">ifstream</span> <span class=\"nf\">engineFile</span><span class=\"p\">(</span><span class=\"s\">\"model.plan\"</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">::</span><span class=\"n\">ios</span><span class=\"o\">::</span><span class=\"n\">binary</span><span class=\"p\">);</span>\n<span class=\"n\">nvinfer1</span><span class=\"o\">::</span><span class=\"n\">ICudaEngine</span><span class=\"o\">*</span> <span class=\"n\">engine</span> <span class=\"o\">=</span> <span class=\"n\">runtime</span><span class=\"o\">-&gt;</span><span class=\"n\">deserializeCudaEngine</span><span class=\"p\">(...);</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\"><span class=\"n\">nvinfer1</span><span class=\"o\">::</span><span class=\"n\">IRuntime</span><span class=\"o\">*</span> <span class=\"n\">runtime</span> <span class=\"o\">=</span> <span class=\"n\">nvinfer1</span><span class=\"o\">::</span><span class=\"n\">createInferRuntime</span><span class=\"p\">(</span><span class=\"n\">logger</span><span class=\"p\">);</span>\n<span class=\"n\">std</span><span class=\"o\">::</span><span class=\"n\">ifstream</span> <span class=\"nf\">engineFile</span><span class=\"p\">(</span><span class=\"s\">\"model.plan\"</span><span class=\"p\">,</span> <span class=\"n\">std</span><span class=\"o\">::</span><span class=\"n\">ios</span><span class=\"o\">::</span><span class=\"n\">binary</span><span class=\"p\">);</span>\n<span class=\"n\">nvinfer1</span><span class=\"o\">::</span><span class=\"n\">ICudaEngine</span><span class=\"o\">*</span> <span class=\"n\">engine</span> <span class=\"o\">=</span> <span class=\"n\">runtime</span><span class=\"o\">-&gt;</span><span class=\"n\">deserializeCudaEngine</span><span class=\"p\">(...);</span>\n</code></pre>\n<ul>\n  <li><strong>Python Inference:</strong></li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\"><span class=\"kn\">import</span> <span class=\"nn\">tensorrt</span> <span class=\"k\">as</span> <span class=\"n\">trt</span>\n<span class=\"n\">TRT_LOGGER</span> <span class=\"o\">=</span> <span class=\"n\">trt</span><span class=\"p\">.</span><span class=\"n\">Logger</span><span class=\"p\">()</span>\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">\"model.plan\"</span><span class=\"p\">,</span> <span class=\"s\">\"rb\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"n\">engine</span> <span class=\"o\">=</span> <span class=\"n\">trt</span><span class=\"p\">.</span><span class=\"n\">Runtime</span><span class=\"p\">(</span><span class=\"n\">TRT_LOGGER</span><span class=\"p\">).</span><span class=\"n\">deserialize_cuda_engine</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">read</span><span class=\"p\">())</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\"><span class=\"kn\">import</span> <span class=\"nn\">tensorrt</span> <span class=\"k\">as</span> <span class=\"n\">trt</span>\n<span class=\"n\">TRT_LOGGER</span> <span class=\"o\">=</span> <span class=\"n\">trt</span><span class=\"p\">.</span><span class=\"n\">Logger</span><span class=\"p\">()</span>\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s\">\"model.plan\"</span><span class=\"p\">,</span> <span class=\"s\">\"rb\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"n\">engine</span> <span class=\"o\">=</span> <span class=\"n\">trt</span><span class=\"p\">.</span><span class=\"n\">Runtime</span><span class=\"p\">(</span><span class=\"n\">TRT_LOGGER</span><span class=\"p\">).</span><span class=\"n\">deserialize_cuda_engine</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">read</span><span class=\"p\">())</span>\n</code></pre>",
      "contentMarkdown": "*   **Model Conversion (ONNX →→\\\\rightarrow Engine):**\n\n![](https://aman.ai/images/copy.png)\n\n`` trtexec --onnx=model.onnx --saveEngine=model.plan --`float16` ``\n\n![](https://aman.ai/images/copy.png)\n\n`` trtexec --onnx=model.onnx --saveEngine=model.plan --`float16` ``\n\n*   **C++ Inference:**\n\n![](https://aman.ai/images/copy.png)\n\n`nvinfer1::IRuntime* runtime = nvinfer1::createInferRuntime(logger); std::ifstream engineFile(\"model.plan\", std::ios::binary); nvinfer1::ICudaEngine* engine = runtime->deserializeCudaEngine(...);`\n\n![](https://aman.ai/images/copy.png)\n\n`nvinfer1::IRuntime* runtime = nvinfer1::createInferRuntime(logger); std::ifstream engineFile(\"model.plan\", std::ios::binary); nvinfer1::ICudaEngine* engine = runtime->deserializeCudaEngine(...);`\n\n*   **Python Inference:**\n\n![](https://aman.ai/images/copy.png)\n\n`import tensorrt as trt TRT_LOGGER = trt.Logger() with open(\"model.plan\", \"rb\") as f:     engine = trt.Runtime(TRT_LOGGER).deserialize_cuda_engine(f.read())`\n\n![](https://aman.ai/images/copy.png)\n\n`import tensorrt as trt TRT_LOGGER = trt.Logger() with open(\"model.plan\", \"rb\") as f:     engine = trt.Runtime(TRT_LOGGER).deserialize_cuda_engine(f.read())`",
      "order": 7,
      "orderInChapter": 5,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": true,
        "wordCount": 82,
        "contentLength": 7728
      },
      "nextCards": [
        "ai-ml-runtimes-suitable-applications-8",
        "ai-ml-runtimes-overview-9"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#example-workflow",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-suitable-applications-8",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorRT Deep Dive",
      "title": "Suitable Applications",
      "subtitle": "TensorRT Deep Dive",
      "contentHtml": "<ul>\n  <li>Real-time object detection on Jetson Nano/Xavier</li>\n  <li>Batch inference in ML inference servers</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">int8</code>-quantized NLP models for chatbots</li>\n  <li>\n    <p>High-throughput video analytics (via DeepStream)</p>\n  </li>\n  <li>TensorRT excels in performance-critical scenarios where latency, batch throughput, or GPU utilization is a bottleneck. It’s a specialized, production-grade runtime for teams fully committed to NVIDIA’s platform.</li>\n</ul>\n<p>High-throughput video analytics (via DeepStream)</p>",
      "contentMarkdown": "*   Real-time object detection on Jetson Nano/Xavier\n*   Batch inference in ML inference servers\n*   `int8`\\-quantized NLP models for chatbots\n*   High-throughput video analytics (via DeepStream)\n    \n*   TensorRT excels in performance-critical scenarios where latency, batch throughput, or GPU utilization is a bottleneck. It’s a specialized, production-grade runtime for teams fully committed to NVIDIA’s platform.\n\nHigh-throughput video analytics (via DeepStream)",
      "order": 8,
      "orderInChapter": 6,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "nlp"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 59,
        "contentLength": 583
      },
      "nextCards": [
        "ai-ml-runtimes-overview-9",
        "ai-ml-runtimes-architecture-10"
      ],
      "relatedCards": [
        "ai-conditional-random-fields-undirected-graphical-models-2",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#suitable-applications",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-overview-9",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Core ML Deep Dive",
      "title": "Overview",
      "subtitle": "Core ML Deep Dive",
      "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: iOS/macOS developers</li>\n  <li><strong>Use Cases</strong>: Image recognition, natural language processing, AR/VR, real-time gesture and object detection</li>\n  <li><strong>Model Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code> (converted to <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code> at compile time)</li>\n  <li><strong>Conversion Tools</strong>: <code class=\"language-plaintext highlighter-rouge\">coremltools</code>, Apple Create ML, ONNX to Core ML converters</li>\n</ul>",
      "contentMarkdown": "*   **Developer Target**: iOS/macOS developers\n*   **Use Cases**: Image recognition, natural language processing, AR/VR, real-time gesture and object detection\n*   **Model Format**: `.mlmodel` (converted to `.mlmodelc` at compile time)\n*   **Conversion Tools**: `coremltools`, Apple Create ML, ONNX to Core ML converters",
      "order": 9,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 41,
        "contentLength": 579
      },
      "nextCards": [
        "ai-ml-runtimes-architecture-10",
        "ai-ml-runtimes-supported-model-types-11"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#overview",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-architecture-10",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Core ML Deep Dive",
      "title": "Architecture",
      "subtitle": "Core ML Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Model Compiler</strong>: Converts <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code> to <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code>, a compiled model package optimized for fast execution. It includes a serialized computation graph, weights, metadata, and hardware hints.</p>\n  </li>\n  <li>\n    <p><strong>Execution Pipeline</strong>:</p>\n\n    <ol>\n      <li><strong>Model Load</strong>: App loads the <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code> file at runtime using the <code class=\"language-plaintext highlighter-rouge\">MLModel</code> API.</li>\n      <li><strong>Prediction API</strong>: Developer calls <code class=\"language-plaintext highlighter-rouge\">prediction(input:)</code>, which triggers the internal compute graph.</li>\n      <li><strong>Backend Selection</strong>: Core ML dynamically selects the best available backend (CPU, GPU, ANE) based on model ops and hardware.</li>\n      <li><strong>Execution Engine</strong>: Executes the optimized graph using Apple’s proprietary kernel implementations.</li>\n      <li><strong>Output</strong>: Returns structured model output (class label, bounding box, etc.) as Swift-native objects.</li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Key Components</strong>:</p>\n\n    <ul>\n      <li><em>MLModel Interface</em>: Main interaction point for inference</li>\n      <li><em>MLMultiArray</em>: N-dimensional tensor abstraction</li>\n      <li><em>MLFeatureValue / MLFeatureProvider</em>: Input-output containers</li>\n      <li><em>NeuralNetwork.proto</em>: Defines underlying graph schema for neural network layers</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Model Compiler</strong>: Converts <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code> to <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code>, a compiled model package optimized for fast execution. It includes a serialized computation graph, weights, metadata, and hardware hints.</p>\n<p><strong>Execution Pipeline</strong>:</p>\n<ol>\n      <li><strong>Model Load</strong>: App loads the <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code> file at runtime using the <code class=\"language-plaintext highlighter-rouge\">MLModel</code> API.</li>\n      <li><strong>Prediction API</strong>: Developer calls <code class=\"language-plaintext highlighter-rouge\">prediction(input:)</code>, which triggers the internal compute graph.</li>\n      <li><strong>Backend Selection</strong>: Core ML dynamically selects the best available backend (CPU, GPU, ANE) based on model ops and hardware.</li>\n      <li><strong>Execution Engine</strong>: Executes the optimized graph using Apple’s proprietary kernel implementations.</li>\n      <li><strong>Output</strong>: Returns structured model output (class label, bounding box, etc.) as Swift-native objects.</li>\n    </ol>\n<p><strong>Key Components</strong>:</p>\n<ul>\n      <li><em>MLModel Interface</em>: Main interaction point for inference</li>\n      <li><em>MLMultiArray</em>: N-dimensional tensor abstraction</li>\n      <li><em>MLFeatureValue / MLFeatureProvider</em>: Input-output containers</li>\n      <li><em>NeuralNetwork.proto</em>: Defines underlying graph schema for neural network layers</li>\n    </ul>",
      "contentMarkdown": "*   **Model Compiler**: Converts `.mlmodel` to `.mlmodelc`, a compiled model package optimized for fast execution. It includes a serialized computation graph, weights, metadata, and hardware hints.\n    \n*   **Execution Pipeline**:\n    \n    1.  **Model Load**: App loads the `.mlmodelc` file at runtime using the `MLModel` API.\n    2.  **Prediction API**: Developer calls `prediction(input:)`, which triggers the internal compute graph.\n    3.  **Backend Selection**: Core ML dynamically selects the best available backend (CPU, GPU, ANE) based on model ops and hardware.\n    4.  **Execution Engine**: Executes the optimized graph using Apple’s proprietary kernel implementations.\n    5.  **Output**: Returns structured model output (class label, bounding box, etc.) as Swift-native objects.\n*   **Key Components**:\n    \n    *   _MLModel Interface_: Main interaction point for inference\n    *   _MLMultiArray_: N-dimensional tensor abstraction\n    *   _MLFeatureValue / MLFeatureProvider_: Input-output containers\n    *   _NeuralNetwork.proto_: Defines underlying graph schema for neural network layers\n\n**Model Compiler**: Converts `.mlmodel` to `.mlmodelc`, a compiled model package optimized for fast execution. It includes a serialized computation graph, weights, metadata, and hardware hints.\n\n**Execution Pipeline**:\n\n1.  **Model Load**: App loads the `.mlmodelc` file at runtime using the `MLModel` API.\n2.  **Prediction API**: Developer calls `prediction(input:)`, which triggers the internal compute graph.\n3.  **Backend Selection**: Core ML dynamically selects the best available backend (CPU, GPU, ANE) based on model ops and hardware.\n4.  **Execution Engine**: Executes the optimized graph using Apple’s proprietary kernel implementations.\n5.  **Output**: Returns structured model output (class label, bounding box, etc.) as Swift-native objects.\n\n**Key Components**:\n\n*   _MLModel Interface_: Main interaction point for inference\n*   _MLMultiArray_: N-dimensional tensor abstraction\n*   _MLFeatureValue / MLFeatureProvider_: Input-output containers\n*   _NeuralNetwork.proto_: Defines underlying graph schema for neural network layers",
      "order": 10,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "neural network"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 263,
        "contentLength": 3279
      },
      "nextCards": [
        "ai-ml-runtimes-supported-model-types-11",
        "ai-ml-runtimes-implementation-details-12"
      ],
      "relatedCards": [
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5",
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-neural-message-passing-for-quantum-chemistry-12",
        "ai-top-30-papers-neural-machine-translation-by-jointly-learning-to--14"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#architecture",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-supported-model-types-11",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Core ML Deep Dive",
      "title": "Supported Model Types",
      "subtitle": "Core ML Deep Dive",
      "contentHtml": "<ul>\n  <li>Neural Networks (CNNs, RNNs, Transformers)</li>\n  <li>Decision Trees and Ensembles (from XGBoost, scikit-learn)</li>\n  <li>Natural Language models (tokenizers, embeddings)</li>\n  <li>Audio signal processing</li>\n  <li>Custom models using Core ML’s custom layers</li>\n</ul>",
      "contentMarkdown": "*   Neural Networks (CNNs, RNNs, Transformers)\n*   Decision Trees and Ensembles (from XGBoost, scikit-learn)\n*   Natural Language models (tokenizers, embeddings)\n*   Audio signal processing\n*   Custom models using Core ML’s custom layers",
      "order": 11,
      "orderInChapter": 3,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "neural network",
        "transformer",
        "embedding",
        "cnn",
        "rnn"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 32,
        "contentLength": 283
      },
      "nextCards": [
        "ai-ml-runtimes-implementation-details-12",
        "ai-ml-runtimes-pros-and-cons-13"
      ],
      "relatedCards": [
        "ai-top-30-papers-attention-is-all-you-need-13",
        "ai-cnns-for-text-classification-convolutional-neural-networks-for-sentence-classif-6",
        "ai-cnns-for-text-classification-1-d-convolutions-over-text-4",
        "ai-top-30-papers-neural-machine-translation-by-jointly-learning-to--14",
        "ai-top-30-papers-multi-scale-context-aggregation-by-dilated-convolu-11"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#supported-model-types",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-implementation-details-12",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Core ML Deep Dive",
      "title": "Implementation Details",
      "subtitle": "Core ML Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Conversion Process</strong>:</p>\n\n    <ul>\n      <li>Models from PyTorch, TensorFlow, scikit-learn, or XGBoost are first converted to ONNX or a supported format</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">coremltools.convert()</code> maps ops to Core ML equivalents and produces <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code></li>\n      <li>Optional model quantization (e.g., 16-bit float) can be applied to reduce size</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hardware Utilization</strong>:</p>\n\n    <ul>\n      <li>Automatically uses ANE if available (iPhone 8 and later)</li>\n      <li>Fallback to Metal GPU or CPU if ANE doesn’t support all ops</li>\n      <li>Internal heuristics determine fallback patterns and op partitioning</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Custom Layers</strong>:</p>\n\n    <ul>\n      <li>Developers can define <code class=\"language-plaintext highlighter-rouge\">MLCustomModel</code> classes</li>\n      <li>Useful when Core ML lacks certain ops</li>\n      <li>Requires manual tensor handling and native Swift/Obj-C implementation</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Conversion Process</strong>:</p>\n<ul>\n      <li>Models from PyTorch, TensorFlow, scikit-learn, or XGBoost are first converted to ONNX or a supported format</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">coremltools.convert()</code> maps ops to Core ML equivalents and produces <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code></li>\n      <li>Optional model quantization (e.g., 16-bit float) can be applied to reduce size</li>\n    </ul>\n<p><strong>Hardware Utilization</strong>:</p>\n<ul>\n      <li>Automatically uses ANE if available (iPhone 8 and later)</li>\n      <li>Fallback to Metal GPU or CPU if ANE doesn’t support all ops</li>\n      <li>Internal heuristics determine fallback patterns and op partitioning</li>\n    </ul>\n<p><strong>Custom Layers</strong>:</p>\n<ul>\n      <li>Developers can define <code class=\"language-plaintext highlighter-rouge\">MLCustomModel</code> classes</li>\n      <li>Useful when Core ML lacks certain ops</li>\n      <li>Requires manual tensor handling and native Swift/Obj-C implementation</li>\n    </ul>",
      "contentMarkdown": "*   **Conversion Process**:\n    \n    *   Models from PyTorch, TensorFlow, scikit-learn, or XGBoost are first converted to ONNX or a supported format\n    *   `coremltools.convert()` maps ops to Core ML equivalents and produces `.mlmodel`\n    *   Optional model quantization (e.g., 16-bit float) can be applied to reduce size\n*   **Hardware Utilization**:\n    \n    *   Automatically uses ANE if available (iPhone 8 and later)\n    *   Fallback to Metal GPU or CPU if ANE doesn’t support all ops\n    *   Internal heuristics determine fallback patterns and op partitioning\n*   **Custom Layers**:\n    \n    *   Developers can define `MLCustomModel` classes\n    *   Useful when Core ML lacks certain ops\n    *   Requires manual tensor handling and native Swift/Obj-C implementation\n\n**Conversion Process**:\n\n*   Models from PyTorch, TensorFlow, scikit-learn, or XGBoost are first converted to ONNX or a supported format\n*   `coremltools.convert()` maps ops to Core ML equivalents and produces `.mlmodel`\n*   Optional model quantization (e.g., 16-bit float) can be applied to reduce size\n\n**Hardware Utilization**:\n\n*   Automatically uses ANE if available (iPhone 8 and later)\n*   Fallback to Metal GPU or CPU if ANE doesn’t support all ops\n*   Internal heuristics determine fallback patterns and op partitioning\n\n**Custom Layers**:\n\n*   Developers can define `MLCustomModel` classes\n*   Useful when Core ML lacks certain ops\n*   Requires manual tensor handling and native Swift/Obj-C implementation",
      "order": 12,
      "orderInChapter": 4,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 207,
        "contentLength": 2248
      },
      "nextCards": [
        "ai-ml-runtimes-pros-and-cons-13",
        "ai-ml-runtimes-example-code-snippet-14"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-pros-and-cons-13",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Core ML Deep Dive",
      "title": "Pros and Cons",
      "subtitle": "Core ML Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros:</strong></p>\n\n    <ul>\n      <li>Deep Apple integration (Vision, AVFoundation, ARKit, etc.)</li>\n      <li>Seamless use of hardware accelerators</li>\n      <li>High-level Swift API for rapid development</li>\n      <li>Secure and privacy-focused (no data leaves device)</li>\n      <li>Optimized runtime with minimal latency</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons:</strong></p>\n\n    <ul>\n      <li>Apple-only ecosystem</li>\n      <li>Conversion limitations (unsupported ops in some models)</li>\n      <li>Limited visibility into runtime internals</li>\n      <li>Custom layer interface can be verbose and inflexible</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros:</strong></p>\n<ul>\n      <li>Deep Apple integration (Vision, AVFoundation, ARKit, etc.)</li>\n      <li>Seamless use of hardware accelerators</li>\n      <li>High-level Swift API for rapid development</li>\n      <li>Secure and privacy-focused (no data leaves device)</li>\n      <li>Optimized runtime with minimal latency</li>\n    </ul>\n<p><strong>Cons:</strong></p>\n<ul>\n      <li>Apple-only ecosystem</li>\n      <li>Conversion limitations (unsupported ops in some models)</li>\n      <li>Limited visibility into runtime internals</li>\n      <li>Custom layer interface can be verbose and inflexible</li>\n    </ul>",
      "contentMarkdown": "*   **Pros:**\n    \n    *   Deep Apple integration (Vision, AVFoundation, ARKit, etc.)\n    *   Seamless use of hardware accelerators\n    *   High-level Swift API for rapid development\n    *   Secure and privacy-focused (no data leaves device)\n    *   Optimized runtime with minimal latency\n*   **Cons:**\n    \n    *   Apple-only ecosystem\n    *   Conversion limitations (unsupported ops in some models)\n    *   Limited visibility into runtime internals\n    *   Custom layer interface can be verbose and inflexible\n\n**Pros:**\n\n*   Deep Apple integration (Vision, AVFoundation, ARKit, etc.)\n*   Seamless use of hardware accelerators\n*   High-level Swift API for rapid development\n*   Secure and privacy-focused (no data leaves device)\n*   Optimized runtime with minimal latency\n\n**Cons:**\n\n*   Apple-only ecosystem\n*   Conversion limitations (unsupported ops in some models)\n*   Limited visibility into runtime internals\n*   Custom layer interface can be verbose and inflexible",
      "order": 13,
      "orderInChapter": 5,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 128,
        "contentLength": 1312
      },
      "nextCards": [
        "ai-ml-runtimes-example-code-snippet-14",
        "ai-ml-runtimes-overview-15"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-example-code-snippet-14",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Core ML Deep Dive",
      "title": "Example Code Snippet",
      "subtitle": "Core ML Deep Dive",
      "contentHtml": "<div class=\"language-swift highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\"><span class=\"k\">guard</span> <span class=\"k\">let</span> <span class=\"nv\">model</span> <span class=\"o\">=</span> <span class=\"k\">try</span><span class=\"p\">?</span> <span class=\"kt\">MyImageClassifier</span><span class=\"p\">(</span><span class=\"nv\">configuration</span><span class=\"p\">:</span> <span class=\"kt\">MLModelConfiguration</span><span class=\"p\">())</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\n    <span class=\"nf\">fatalError</span><span class=\"p\">(</span><span class=\"s\">\"Model failed to load\"</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n\n<span class=\"k\">let</span> <span class=\"nv\">input</span> <span class=\"o\">=</span> <span class=\"k\">try</span><span class=\"p\">?</span> <span class=\"kt\">MLMultiArray</span><span class=\"p\">(</span><span class=\"nv\">shape</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">],</span> <span class=\"nv\">dataType</span><span class=\"p\">:</span> <span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n<span class=\"c1\">// Fill input array with pixel data</span>\n\n<span class=\"k\">let</span> <span class=\"nv\">output</span> <span class=\"o\">=</span> <span class=\"k\">try</span><span class=\"p\">?</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"nf\">prediction</span><span class=\"p\">(</span><span class=\"nv\">input</span><span class=\"p\">:</span> <span class=\"n\">input</span><span class=\"o\">!</span><span class=\"p\">)</span>\n<span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">?</span><span class=\"o\">.</span><span class=\"n\">classLabel</span> <span class=\"p\">??</span> <span class=\"s\">\"Prediction failed\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\"><span class=\"k\">guard</span> <span class=\"k\">let</span> <span class=\"nv\">model</span> <span class=\"o\">=</span> <span class=\"k\">try</span><span class=\"p\">?</span> <span class=\"kt\">MyImageClassifier</span><span class=\"p\">(</span><span class=\"nv\">configuration</span><span class=\"p\">:</span> <span class=\"kt\">MLModelConfiguration</span><span class=\"p\">())</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\n    <span class=\"nf\">fatalError</span><span class=\"p\">(</span><span class=\"s\">\"Model failed to load\"</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n\n<span class=\"k\">let</span> <span class=\"nv\">input</span> <span class=\"o\">=</span> <span class=\"k\">try</span><span class=\"p\">?</span> <span class=\"kt\">MLMultiArray</span><span class=\"p\">(</span><span class=\"nv\">shape</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">],</span> <span class=\"nv\">dataType</span><span class=\"p\">:</span> <span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n<span class=\"c1\">// Fill input array with pixel data</span>\n\n<span class=\"k\">let</span> <span class=\"nv\">output</span> <span class=\"o\">=</span> <span class=\"k\">try</span><span class=\"p\">?</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"nf\">prediction</span><span class=\"p\">(</span><span class=\"nv\">input</span><span class=\"p\">:</span> <span class=\"n\">input</span><span class=\"o\">!</span><span class=\"p\">)</span>\n<span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">?</span><span class=\"o\">.</span><span class=\"n\">classLabel</span> <span class=\"p\">??</span> <span class=\"s\">\"Prediction failed\"</span><span class=\"p\">)</span>\n</code></pre>",
      "contentMarkdown": "![](https://aman.ai/images/copy.png)\n\n`guard let model = try? MyImageClassifier(configuration: MLModelConfiguration()) else {     fatalError(\"Model failed to load\") }  let input = try? MLMultiArray(shape: [1, 3, 224, 224], dataType: .float32) // Fill input array with pixel data  let output = try? model.prediction(input: input!) print(output?.classLabel ?? \"Prediction failed\")`\n\n![](https://aman.ai/images/copy.png)\n\n`guard let model = try? MyImageClassifier(configuration: MLModelConfiguration()) else {     fatalError(\"Model failed to load\") }  let input = try? MLMultiArray(shape: [1, 3, 224, 224], dataType: .float32) // Fill input array with pixel data  let output = try? model.prediction(input: input!) print(output?.classLabel ?? \"Prediction failed\")`",
      "order": 14,
      "orderInChapter": 6,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 86,
        "contentLength": 4406
      },
      "nextCards": [
        "ai-ml-runtimes-overview-15",
        "ai-ml-runtimes-architecture-16"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#example-code-snippet",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-overview-15",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "MLX Deep Dive",
      "title": "Overview",
      "subtitle": "MLX Deep Dive",
      "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: ML researchers and developers using Apple Silicon</li>\n  <li><strong>Use Cases</strong>: Research, fine-tuning models on-device, LLM inference, Apple-optimized ML pipelines</li>\n  <li><strong>Model Format</strong>: No proprietary serialized model format; models are expressed in Python source code using <code class=\"language-plaintext highlighter-rouge\">mlx.nn</code> layers</li>\n  <li><strong>Conversion Tools</strong>: Emerging support for PyTorch model import via <code class=\"language-plaintext highlighter-rouge\">mlx-trace</code> and ONNX conversion</li>\n</ul>",
      "contentMarkdown": "*   **Developer Target**: ML researchers and developers using Apple Silicon\n*   **Use Cases**: Research, fine-tuning models on-device, LLM inference, Apple-optimized ML pipelines\n*   **Model Format**: No proprietary serialized model format; models are expressed in Python source code using `mlx.nn` layers\n*   **Conversion Tools**: Emerging support for PyTorch model import via `mlx-trace` and ONNX conversion",
      "order": 15,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "llm",
        "fine-tuning"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 54,
        "contentLength": 612
      },
      "nextCards": [
        "ai-ml-runtimes-architecture-16",
        "ai-ml-runtimes-implementation-details-17"
      ],
      "relatedCards": [
        "ai-top-30-papers-zephyr-direct-distillation-of-lm-alignment-30",
        "ai-top-30-papers-lost-in-the-middle-how-language-models-use-long-co-31",
        "ai-top-30-papers-alcuna-large-language-models-meet-new-knowledge-33",
        "ai-top-30-papers-the-perils-promises-of-fact-checking-with-large-la-34",
        "ai-gpu-architecture-tensor-core-evolution-23"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#overview",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-architecture-16",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "MLX Deep Dive",
      "title": "Architecture",
      "subtitle": "MLX Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p>MLX is a minimal and composable tensor library that uses Apple’s Metal Performance Shaders (MPS) and optionally the Apple Neural Engine (ANE) for hardware acceleration.</p>\n  </li>\n  <li>\n    <p><strong>Execution Modes</strong>:</p>\n\n    <ul>\n      <li><strong>Eager Execution</strong>: Immediate computation for prototyping/debugging</li>\n      <li><strong>Compiled Graph</strong>: Via <code class=\"language-plaintext highlighter-rouge\">mlx.compile()</code> for performance-critical inference</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Core Components</strong>:</p>\n\n    <ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.core</code>: Tensor definitions and low-level math operations</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.nn</code>: High-level neural network module abstraction (analogous to PyTorch’s <code class=\"language-plaintext highlighter-rouge\">nn.Module</code>)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.optimizers</code>: Gradient-based optimizers for training</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.transforms</code>: Preprocessing utilities (e.g., normalization, resizing)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Hardware Abstraction</strong>:</p>\n\n    <ul>\n      <li>Primarily targets the GPU via MPS</li>\n      <li>MLX compiler performs static analysis to optimize kernel dispatch and memory usage</li>\n      <li>ANE support is still evolving and model-dependent</li>\n    </ul>\n  </li>\n</ul>\n<p>MLX is a minimal and composable tensor library that uses Apple’s Metal Performance Shaders (MPS) and optionally the Apple Neural Engine (ANE) for hardware acceleration.</p>\n<p><strong>Execution Modes</strong>:</p>\n<ul>\n      <li><strong>Eager Execution</strong>: Immediate computation for prototyping/debugging</li>\n      <li><strong>Compiled Graph</strong>: Via <code class=\"language-plaintext highlighter-rouge\">mlx.compile()</code> for performance-critical inference</li>\n    </ul>\n<p><strong>Core Components</strong>:</p>\n<ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.core</code>: Tensor definitions and low-level math operations</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.nn</code>: High-level neural network module abstraction (analogous to PyTorch’s <code class=\"language-plaintext highlighter-rouge\">nn.Module</code>)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.optimizers</code>: Gradient-based optimizers for training</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">mlx.transforms</code>: Preprocessing utilities (e.g., normalization, resizing)</li>\n    </ul>\n<p><strong>Hardware Abstraction</strong>:</p>\n<ul>\n      <li>Primarily targets the GPU via MPS</li>\n      <li>MLX compiler performs static analysis to optimize kernel dispatch and memory usage</li>\n      <li>ANE support is still evolving and model-dependent</li>\n    </ul>",
      "contentMarkdown": "*   MLX is a minimal and composable tensor library that uses Apple’s Metal Performance Shaders (MPS) and optionally the Apple Neural Engine (ANE) for hardware acceleration.\n    \n*   **Execution Modes**:\n    \n    *   **Eager Execution**: Immediate computation for prototyping/debugging\n    *   **Compiled Graph**: Via `mlx.compile()` for performance-critical inference\n*   **Core Components**:\n    \n    *   `mlx.core`: Tensor definitions and low-level math operations\n    *   `mlx.nn`: High-level neural network module abstraction (analogous to PyTorch’s `nn.Module`)\n    *   `mlx.optimizers`: Gradient-based optimizers for training\n    *   `mlx.transforms`: Preprocessing utilities (e.g., normalization, resizing)\n*   **Hardware Abstraction**:\n    \n    *   Primarily targets the GPU via MPS\n    *   MLX compiler performs static analysis to optimize kernel dispatch and memory usage\n    *   ANE support is still evolving and model-dependent\n\nMLX is a minimal and composable tensor library that uses Apple’s Metal Performance Shaders (MPS) and optionally the Apple Neural Engine (ANE) for hardware acceleration.\n\n**Execution Modes**:\n\n*   **Eager Execution**: Immediate computation for prototyping/debugging\n*   **Compiled Graph**: Via `mlx.compile()` for performance-critical inference\n\n**Core Components**:\n\n*   `mlx.core`: Tensor definitions and low-level math operations\n*   `mlx.nn`: High-level neural network module abstraction (analogous to PyTorch’s `nn.Module`)\n*   `mlx.optimizers`: Gradient-based optimizers for training\n*   `mlx.transforms`: Preprocessing utilities (e.g., normalization, resizing)\n\n**Hardware Abstraction**:\n\n*   Primarily targets the GPU via MPS\n*   MLX compiler performs static analysis to optimize kernel dispatch and memory usage\n*   ANE support is still evolving and model-dependent",
      "order": 16,
      "orderInChapter": 2,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "neural network"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 216,
        "contentLength": 2983
      },
      "nextCards": [
        "ai-ml-runtimes-implementation-details-17",
        "ai-ml-runtimes-pros-and-cons-18"
      ],
      "relatedCards": [
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5",
        "ai-top-30-papers-neural-message-passing-for-quantum-chemistry-12",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#architecture",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-implementation-details-17",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "MLX Deep Dive",
      "title": "Implementation Details",
      "subtitle": "MLX Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Tensor Memory Model</strong>:</p>\n\n    <ul>\n      <li>MLX tensors are immutable</li>\n      <li>Operations generate new tensors rather than mutating in-place</li>\n      <li>Enables functional purity and easier graph compilation</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>JIT Compilation</strong>:</p>\n\n    <ul>\n      <li>While code is typically run in Python, MLX allows functions to be decorated with <code class=\"language-plaintext highlighter-rouge\">@mlx.compile</code> to trace and compile computation graphs</li>\n      <li>Reduces memory allocations and kernel overhead</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Custom Modules</strong>:</p>\n\n    <ul>\n      <li>Developers can create custom layers by subclassing <code class=\"language-plaintext highlighter-rouge\">mlx.nn.Module</code></li>\n      <li>Supports standard layers like <code class=\"language-plaintext highlighter-rouge\">Linear</code>, <code class=\"language-plaintext highlighter-rouge\">Conv2d</code>, <code class=\"language-plaintext highlighter-rouge\">LayerNorm</code>, etc.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Interoperability</strong>:</p>\n\n    <ul>\n      <li>MLX includes tools to convert PyTorch models using tracing (WIP)</li>\n      <li>No built-in ONNX or TensorFlow Lite importer yet, though development is ongoing</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Tensor Memory Model</strong>:</p>\n<ul>\n      <li>MLX tensors are immutable</li>\n      <li>Operations generate new tensors rather than mutating in-place</li>\n      <li>Enables functional purity and easier graph compilation</li>\n    </ul>\n<p><strong>JIT Compilation</strong>:</p>\n<ul>\n      <li>While code is typically run in Python, MLX allows functions to be decorated with <code class=\"language-plaintext highlighter-rouge\">@mlx.compile</code> to trace and compile computation graphs</li>\n      <li>Reduces memory allocations and kernel overhead</li>\n    </ul>\n<p><strong>Custom Modules</strong>:</p>\n<ul>\n      <li>Developers can create custom layers by subclassing <code class=\"language-plaintext highlighter-rouge\">mlx.nn.Module</code></li>\n      <li>Supports standard layers like <code class=\"language-plaintext highlighter-rouge\">Linear</code>, <code class=\"language-plaintext highlighter-rouge\">Conv2d</code>, <code class=\"language-plaintext highlighter-rouge\">LayerNorm</code>, etc.</li>\n    </ul>\n<p><strong>Interoperability</strong>:</p>\n<ul>\n      <li>MLX includes tools to convert PyTorch models using tracing (WIP)</li>\n      <li>No built-in ONNX or TensorFlow Lite importer yet, though development is ongoing</li>\n    </ul>",
      "contentMarkdown": "*   **Tensor Memory Model**:\n    \n    *   MLX tensors are immutable\n    *   Operations generate new tensors rather than mutating in-place\n    *   Enables functional purity and easier graph compilation\n*   **JIT Compilation**:\n    \n    *   While code is typically run in Python, MLX allows functions to be decorated with `@mlx.compile` to trace and compile computation graphs\n    *   Reduces memory allocations and kernel overhead\n*   **Custom Modules**:\n    \n    *   Developers can create custom layers by subclassing `mlx.nn.Module`\n    *   Supports standard layers like `Linear`, `Conv2d`, `LayerNorm`, etc.\n*   **Interoperability**:\n    \n    *   MLX includes tools to convert PyTorch models using tracing (WIP)\n    *   No built-in ONNX or TensorFlow Lite importer yet, though development is ongoing\n\n**Tensor Memory Model**:\n\n*   MLX tensors are immutable\n*   Operations generate new tensors rather than mutating in-place\n*   Enables functional purity and easier graph compilation\n\n**JIT Compilation**:\n\n*   While code is typically run in Python, MLX allows functions to be decorated with `@mlx.compile` to trace and compile computation graphs\n*   Reduces memory allocations and kernel overhead\n\n**Custom Modules**:\n\n*   Developers can create custom layers by subclassing `mlx.nn.Module`\n*   Supports standard layers like `Linear`, `Conv2d`, `LayerNorm`, etc.\n\n**Interoperability**:\n\n*   MLX includes tools to convert PyTorch models using tracing (WIP)\n*   No built-in ONNX or TensorFlow Lite importer yet, though development is ongoing",
      "order": 17,
      "orderInChapter": 3,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 206,
        "contentLength": 2604
      },
      "nextCards": [
        "ai-ml-runtimes-pros-and-cons-18",
        "ai-ml-runtimes-example-code-snippet-19"
      ],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-synchronization-and-communication-12",
        "ai-gpu-architecture-memory-management-and-transfer-15",
        "ai-gpu-architecture-inference-deployment-18"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-pros-and-cons-18",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "MLX Deep Dive",
      "title": "Pros and Cons",
      "subtitle": "MLX Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros:</strong></p>\n\n    <ul>\n      <li>Highly optimized for Apple Silicon (especially M1/M2)</li>\n      <li>Lightweight and minimalist API with functional programming style</li>\n      <li>Supports training and inference on-device</li>\n      <li>Fast experimentation with eager mode and compilation toggle</li>\n      <li>Tensor API is intuitive for PyTorch users</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons:</strong></p>\n\n    <ul>\n      <li>Only runs on macOS with Apple Silicon (no iOS, no Windows/Linux)</li>\n      <li>Ecosystem still maturing (e.g., fewer pre-trained models, limited documentation)</li>\n      <li>No official deployment format—source code is the model</li>\n      <li>Interop with other frameworks is under active development but not production-ready</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros:</strong></p>\n<ul>\n      <li>Highly optimized for Apple Silicon (especially M1/M2)</li>\n      <li>Lightweight and minimalist API with functional programming style</li>\n      <li>Supports training and inference on-device</li>\n      <li>Fast experimentation with eager mode and compilation toggle</li>\n      <li>Tensor API is intuitive for PyTorch users</li>\n    </ul>\n<p><strong>Cons:</strong></p>\n<ul>\n      <li>Only runs on macOS with Apple Silicon (no iOS, no Windows/Linux)</li>\n      <li>Ecosystem still maturing (e.g., fewer pre-trained models, limited documentation)</li>\n      <li>No official deployment format—source code is the model</li>\n      <li>Interop with other frameworks is under active development but not production-ready</li>\n    </ul>",
      "contentMarkdown": "*   **Pros:**\n    \n    *   Highly optimized for Apple Silicon (especially M1/M2)\n    *   Lightweight and minimalist API with functional programming style\n    *   Supports training and inference on-device\n    *   Fast experimentation with eager mode and compilation toggle\n    *   Tensor API is intuitive for PyTorch users\n*   **Cons:**\n    \n    *   Only runs on macOS with Apple Silicon (no iOS, no Windows/Linux)\n    *   Ecosystem still maturing (e.g., fewer pre-trained models, limited documentation)\n    *   No official deployment format—source code is the model\n    *   Interop with other frameworks is under active development but not production-ready\n\n**Pros:**\n\n*   Highly optimized for Apple Silicon (especially M1/M2)\n*   Lightweight and minimalist API with functional programming style\n*   Supports training and inference on-device\n*   Fast experimentation with eager mode and compilation toggle\n*   Tensor API is intuitive for PyTorch users\n\n**Cons:**\n\n*   Only runs on macOS with Apple Silicon (no iOS, no Windows/Linux)\n*   Ecosystem still maturing (e.g., fewer pre-trained models, limited documentation)\n*   No official deployment format—source code is the model\n*   Interop with other frameworks is under active development but not production-ready",
      "order": 18,
      "orderInChapter": 4,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 172,
        "contentLength": 1602
      },
      "nextCards": [
        "ai-ml-runtimes-example-code-snippet-19",
        "ai-ml-runtimes-overview-20"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-example-code-snippet-19",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "MLX Deep Dive",
      "title": "Example Code Snippet",
      "subtitle": "MLX Deep Dive",
      "contentHtml": "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\"><span class=\"kn\">import</span> <span class=\"nn\">mlx.core</span> <span class=\"k\">as</span> <span class=\"n\">mx</span>\n<span class=\"kn\">import</span> <span class=\"nn\">mlx.nn</span> <span class=\"k\">as</span> <span class=\"n\">nn</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">SimpleMLP</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">().</span><span class=\"n\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">784</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">relu</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">ReLU</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">__call__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">SimpleMLP</span><span class=\"p\">()</span>\n<span class=\"nb\">input</span> <span class=\"o\">=</span> <span class=\"n\">mx</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">normal</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">784</span><span class=\"p\">))</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Prediction:\"</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code4\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code4\"><span class=\"kn\">import</span> <span class=\"nn\">mlx.core</span> <span class=\"k\">as</span> <span class=\"n\">mx</span>\n<span class=\"kn\">import</span> <span class=\"nn\">mlx.nn</span> <span class=\"k\">as</span> <span class=\"n\">nn</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">SimpleMLP</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">().</span><span class=\"n\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">784</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">relu</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">ReLU</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">__call__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">linear2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">SimpleMLP</span><span class=\"p\">()</span>\n<span class=\"nb\">input</span> <span class=\"o\">=</span> <span class=\"n\">mx</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">normal</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">784</span><span class=\"p\">))</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Prediction:\"</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>For accelerated inference:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\"><span class=\"n\">compiled_fn</span> <span class=\"o\">=</span> <span class=\"n\">mx</span><span class=\"p\">.</span><span class=\"nb\">compile</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">compiled_fn</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code5\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code5\"><span class=\"n\">compiled_fn</span> <span class=\"o\">=</span> <span class=\"n\">mx</span><span class=\"p\">.</span><span class=\"nb\">compile</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">compiled_fn</span><span class=\"p\">(</span><span class=\"nb\">input</span><span class=\"p\">)</span>\n</code></pre>",
      "contentMarkdown": "![](https://aman.ai/images/copy.png)\n\n`import mlx.core as mx import mlx.nn as nn  class SimpleMLP(nn.Module):     def __init__(self):         super().__init__()         self.linear1 = nn.Linear(784, 256)         self.relu = nn.ReLU()         self.linear2 = nn.Linear(256, 10)      def __call__(self, x):         x = self.linear1(x)         x = self.relu(x)         return self.linear2(x)  model = SimpleMLP() input = mx.random.normal((1, 784)) output = model(input)  print(\"Prediction:\", output)`\n\n![](https://aman.ai/images/copy.png)\n\n`import mlx.core as mx import mlx.nn as nn  class SimpleMLP(nn.Module):     def __init__(self):         super().__init__()         self.linear1 = nn.Linear(784, 256)         self.relu = nn.ReLU()         self.linear2 = nn.Linear(256, 10)      def __call__(self, x):         x = self.linear1(x)         x = self.relu(x)         return self.linear2(x)  model = SimpleMLP() input = mx.random.normal((1, 784)) output = model(input)  print(\"Prediction:\", output)`\n\n*   For accelerated inference:\n\n![](https://aman.ai/images/copy.png)\n\n`compiled_fn = mx.compile(model) output = compiled_fn(input)`\n\n![](https://aman.ai/images/copy.png)\n\n`compiled_fn = mx.compile(model) output = compiled_fn(input)`",
      "order": 19,
      "orderInChapter": 5,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 114,
        "contentLength": 8334
      },
      "nextCards": [
        "ai-ml-runtimes-overview-20",
        "ai-ml-runtimes-architecture-21"
      ],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-synchronization-and-communication-12",
        "ai-gpu-architecture-memory-management-and-transfer-15",
        "ai-gpu-architecture-inference-deployment-18"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#example-code-snippet",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-overview-20",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "ONNX Runtime Deep Dive",
      "title": "Overview",
      "subtitle": "ONNX Runtime Deep Dive",
      "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: Application developers, MLOps teams, platform architects</li>\n  <li><strong>Use Cases</strong>: Cross-framework inference, model portability, production deployments (cloud + edge), hardware acceleration</li>\n  <li><strong>Model Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.onnx</code> (Open Neural Network Exchange format)</li>\n  <li><strong>Conversion Tools</strong>: <code class=\"language-plaintext highlighter-rouge\">torch.onnx.export</code>, <code class=\"language-plaintext highlighter-rouge\">tf2onnx</code>, <code class=\"language-plaintext highlighter-rouge\">skl2onnx</code>, and many others</li>\n</ul>",
      "contentMarkdown": "*   **Developer Target**: Application developers, MLOps teams, platform architects\n*   **Use Cases**: Cross-framework inference, model portability, production deployments (cloud + edge), hardware acceleration\n*   **Model Format**: `.onnx` (Open Neural Network Exchange format)\n*   **Conversion Tools**: `torch.onnx.export`, `tf2onnx`, `skl2onnx`, and many others",
      "order": 20,
      "orderInChapter": 1,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "neural network"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 41,
        "contentLength": 677
      },
      "nextCards": [
        "ai-ml-runtimes-architecture-21",
        "ai-ml-runtimes-implementation-details-22"
      ],
      "relatedCards": [
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16",
        "ai-top-30-papers-keeping-neural-networks-simple-by-minimizing-the-d-5",
        "ai-top-30-papers-neural-message-passing-for-quantum-chemistry-12",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#overview",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-architecture-21",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "ONNX Runtime Deep Dive",
      "title": "Architecture",
      "subtitle": "ONNX Runtime Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p>ONNX Runtime is structured around a pluggable and modular execution engine, making it suitable for CPU, GPU, and specialized accelerators. It uses an intermediate computation graph optimized at load time, and delegates computation to “Execution Providers” (EPs).</p>\n  </li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ol>\n      <li><strong>Model Load</strong>: Parses the <code class=\"language-plaintext highlighter-rouge\">.onnx</code> model file into an internal graph representation.</li>\n      <li><strong>Graph Optimization</strong>: Applies a set of graph rewrite passes—like constant folding, node fusion, and dead node elimination.</li>\n      <li><strong>Execution Provider Selection</strong>: Based on available hardware and EP priorities, operators are assigned to execution backends.</li>\n      <li><strong>Execution</strong>: ORT schedules and dispatches kernel calls for each partition of the graph.</li>\n      <li><strong>Output Handling</strong>: Results are returned in native types or via C/C++/Python APIs.</li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Key Components</strong>:</p>\n\n    <ul>\n      <li><strong>Session</strong>: <code class=\"language-plaintext highlighter-rouge\">InferenceSession</code> is the main object for loading and running models.</li>\n      <li>\n        <p><strong>Execution Providers (EPs)</strong>: Modular backend plugins such as:</p>\n\n        <ul>\n          <li>CPU (default)</li>\n          <li>CUDA (NVIDIA GPUs)</li>\n          <li>DirectML (Windows GPU)</li>\n          <li>OpenVINO (Intel accelerators)</li>\n          <li>NNAPI (Android)</li>\n          <li>CoreML (iOS/macOS)</li>\n          <li>TensorRT</li>\n          <li>QNN (Qualcomm AI Engine)</li>\n        </ul>\n      </li>\n      <li><strong>Graph Transformer</strong>: Rewrites and optimizes the computation graph</li>\n      <li><strong>Kernel Registry</strong>: Maps ONNX ops to optimized implementations</li>\n    </ul>\n  </li>\n</ul>\n<p>ONNX Runtime is structured around a pluggable and modular execution engine, making it suitable for CPU, GPU, and specialized accelerators. It uses an intermediate computation graph optimized at load time, and delegates computation to “Execution Providers” (EPs).</p>\n<p><strong>Execution Flow</strong>:</p>\n<ol>\n      <li><strong>Model Load</strong>: Parses the <code class=\"language-plaintext highlighter-rouge\">.onnx</code> model file into an internal graph representation.</li>\n      <li><strong>Graph Optimization</strong>: Applies a set of graph rewrite passes—like constant folding, node fusion, and dead node elimination.</li>\n      <li><strong>Execution Provider Selection</strong>: Based on available hardware and EP priorities, operators are assigned to execution backends.</li>\n      <li><strong>Execution</strong>: ORT schedules and dispatches kernel calls for each partition of the graph.</li>\n      <li><strong>Output Handling</strong>: Results are returned in native types or via C/C++/Python APIs.</li>\n    </ol>\n<p><strong>Key Components</strong>:</p>\n<ul>\n      <li><strong>Session</strong>: <code class=\"language-plaintext highlighter-rouge\">InferenceSession</code> is the main object for loading and running models.</li>\n      <li>\n        <p><strong>Execution Providers (EPs)</strong>: Modular backend plugins such as:</p>\n\n        <ul>\n          <li>CPU (default)</li>\n          <li>CUDA (NVIDIA GPUs)</li>\n          <li>DirectML (Windows GPU)</li>\n          <li>OpenVINO (Intel accelerators)</li>\n          <li>NNAPI (Android)</li>\n          <li>CoreML (iOS/macOS)</li>\n          <li>TensorRT</li>\n          <li>QNN (Qualcomm AI Engine)</li>\n        </ul>\n      </li>\n      <li><strong>Graph Transformer</strong>: Rewrites and optimizes the computation graph</li>\n      <li><strong>Kernel Registry</strong>: Maps ONNX ops to optimized implementations</li>\n    </ul>\n<p><strong>Execution Providers (EPs)</strong>: Modular backend plugins such as:</p>\n<ul>\n          <li>CPU (default)</li>\n          <li>CUDA (NVIDIA GPUs)</li>\n          <li>DirectML (Windows GPU)</li>\n          <li>OpenVINO (Intel accelerators)</li>\n          <li>NNAPI (Android)</li>\n          <li>CoreML (iOS/macOS)</li>\n          <li>TensorRT</li>\n          <li>QNN (Qualcomm AI Engine)</li>\n        </ul>",
      "contentMarkdown": "*   ONNX Runtime is structured around a pluggable and modular execution engine, making it suitable for CPU, GPU, and specialized accelerators. It uses an intermediate computation graph optimized at load time, and delegates computation to “Execution Providers” (EPs).\n    \n*   **Execution Flow**:\n    \n    1.  **Model Load**: Parses the `.onnx` model file into an internal graph representation.\n    2.  **Graph Optimization**: Applies a set of graph rewrite passes—like constant folding, node fusion, and dead node elimination.\n    3.  **Execution Provider Selection**: Based on available hardware and EP priorities, operators are assigned to execution backends.\n    4.  **Execution**: ORT schedules and dispatches kernel calls for each partition of the graph.\n    5.  **Output Handling**: Results are returned in native types or via C/C++/Python APIs.\n*   **Key Components**:\n    \n    *   **Session**: `InferenceSession` is the main object for loading and running models.\n    *   **Execution Providers (EPs)**: Modular backend plugins such as:\n        \n        *   CPU (default)\n        *   CUDA (NVIDIA GPUs)\n        *   DirectML (Windows GPU)\n        *   OpenVINO (Intel accelerators)\n        *   NNAPI (Android)\n        *   CoreML (iOS/macOS)\n        *   TensorRT\n        *   QNN (Qualcomm AI Engine)\n    *   **Graph Transformer**: Rewrites and optimizes the computation graph\n    *   **Kernel Registry**: Maps ONNX ops to optimized implementations\n\nONNX Runtime is structured around a pluggable and modular execution engine, making it suitable for CPU, GPU, and specialized accelerators. It uses an intermediate computation graph optimized at load time, and delegates computation to “Execution Providers” (EPs).\n\n**Execution Flow**:\n\n1.  **Model Load**: Parses the `.onnx` model file into an internal graph representation.\n2.  **Graph Optimization**: Applies a set of graph rewrite passes—like constant folding, node fusion, and dead node elimination.\n3.  **Execution Provider Selection**: Based on available hardware and EP priorities, operators are assigned to execution backends.\n4.  **Execution**: ORT schedules and dispatches kernel calls for each partition of the graph.\n5.  **Output Handling**: Results are returned in native types or via C/C++/Python APIs.\n\n**Key Components**:\n\n*   **Session**: `InferenceSession` is the main object for loading and running models.\n*   **Execution Providers (EPs)**: Modular backend plugins such as:\n    \n    *   CPU (default)\n    *   CUDA (NVIDIA GPUs)\n    *   DirectML (Windows GPU)\n    *   OpenVINO (Intel accelerators)\n    *   NNAPI (Android)\n    *   CoreML (iOS/macOS)\n    *   TensorRT\n    *   QNN (Qualcomm AI Engine)\n*   **Graph Transformer**: Rewrites and optimizes the computation graph\n*   **Kernel Registry**: Maps ONNX ops to optimized implementations\n\n**Execution Providers (EPs)**: Modular backend plugins such as:\n\n*   CPU (default)\n*   CUDA (NVIDIA GPUs)\n*   DirectML (Windows GPU)\n*   OpenVINO (Intel accelerators)\n*   NNAPI (Android)\n*   CoreML (iOS/macOS)\n*   TensorRT\n*   QNN (Qualcomm AI Engine)",
      "order": 21,
      "orderInChapter": 2,
      "difficulty": 4,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous",
        "transformer",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 405,
        "contentLength": 4271
      },
      "nextCards": [
        "ai-ml-runtimes-implementation-details-22",
        "ai-ml-runtimes-pros-and-cons-23"
      ],
      "relatedCards": [
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-gpu-architecture-performance-analysis-27",
        "ai-gpu-architecture-cache-hierarchy-changes-29",
        "ai-gpu-architecture-memory-latency-and-efficiency-improvements-30",
        "ai-gpu-architecture-floating-point-precision-performance-evolution-35"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#architecture",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-implementation-details-22",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "ONNX Runtime Deep Dive",
      "title": "Implementation Details",
      "subtitle": "ONNX Runtime Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Model Format</strong>:</p>\n\n    <ul>\n      <li>ONNX models are stored in protobuf format</li>\n      <li>Static computation graph with explicit type and shape information</li>\n      <li>Supports operator versioning to ensure backward compatibility</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Customization</strong>:</p>\n\n    <ul>\n      <li>Developers can register custom ops and execution providers</li>\n      <li>Optional use of external initializers and custom inference contexts</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Execution Optimization</strong>:</p>\n\n    <ul>\n      <li>Graph transformation level can be controlled (basic, extended, all)</li>\n      <li>EPs can share execution (e.g., some layers on CPU, others on GPU)</li>\n      <li>Quantization and sparsity-aware execution supported via tools like <code class=\"language-plaintext highlighter-rouge\">onnxruntime-tools</code></li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Mobile Support</strong>:</p>\n\n    <ul>\n      <li>ONNX Runtime Mobile: A statically linked, size-reduced runtime</li>\n      <li>Works with Android and iOS, using NNAPI, Core ML, or CPU fallback</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Model Format</strong>:</p>\n<ul>\n      <li>ONNX models are stored in protobuf format</li>\n      <li>Static computation graph with explicit type and shape information</li>\n      <li>Supports operator versioning to ensure backward compatibility</li>\n    </ul>\n<p><strong>Customization</strong>:</p>\n<ul>\n      <li>Developers can register custom ops and execution providers</li>\n      <li>Optional use of external initializers and custom inference contexts</li>\n    </ul>\n<p><strong>Execution Optimization</strong>:</p>\n<ul>\n      <li>Graph transformation level can be controlled (basic, extended, all)</li>\n      <li>EPs can share execution (e.g., some layers on CPU, others on GPU)</li>\n      <li>Quantization and sparsity-aware execution supported via tools like <code class=\"language-plaintext highlighter-rouge\">onnxruntime-tools</code></li>\n    </ul>\n<p><strong>Mobile Support</strong>:</p>\n<ul>\n      <li>ONNX Runtime Mobile: A statically linked, size-reduced runtime</li>\n      <li>Works with Android and iOS, using NNAPI, Core ML, or CPU fallback</li>\n    </ul>",
      "contentMarkdown": "*   **Model Format**:\n    \n    *   ONNX models are stored in protobuf format\n    *   Static computation graph with explicit type and shape information\n    *   Supports operator versioning to ensure backward compatibility\n*   **Customization**:\n    \n    *   Developers can register custom ops and execution providers\n    *   Optional use of external initializers and custom inference contexts\n*   **Execution Optimization**:\n    \n    *   Graph transformation level can be controlled (basic, extended, all)\n    *   EPs can share execution (e.g., some layers on CPU, others on GPU)\n    *   Quantization and sparsity-aware execution supported via tools like `onnxruntime-tools`\n*   **Mobile Support**:\n    \n    *   ONNX Runtime Mobile: A statically linked, size-reduced runtime\n    *   Works with Android and iOS, using NNAPI, Core ML, or CPU fallback\n\n**Model Format**:\n\n*   ONNX models are stored in protobuf format\n*   Static computation graph with explicit type and shape information\n*   Supports operator versioning to ensure backward compatibility\n\n**Customization**:\n\n*   Developers can register custom ops and execution providers\n*   Optional use of external initializers and custom inference contexts\n\n**Execution Optimization**:\n\n*   Graph transformation level can be controlled (basic, extended, all)\n*   EPs can share execution (e.g., some layers on CPU, others on GPU)\n*   Quantization and sparsity-aware execution supported via tools like `onnxruntime-tools`\n\n**Mobile Support**:\n\n*   ONNX Runtime Mobile: A statically linked, size-reduced runtime\n*   Works with Android and iOS, using NNAPI, Core ML, or CPU fallback",
      "order": 22,
      "orderInChapter": 3,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 218,
        "contentLength": 2262
      },
      "nextCards": [
        "ai-ml-runtimes-pros-and-cons-23",
        "ai-ml-runtimes-example-code-snippet-python-24"
      ],
      "relatedCards": [
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-top-30-papers-deep-residual-learning-for-image-recognition-10",
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-pros-and-cons-23",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "ONNX Runtime Deep Dive",
      "title": "Pros and Cons",
      "subtitle": "ONNX Runtime Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros:</strong></p>\n\n    <ul>\n      <li>Framework agnostic and highly interoperable</li>\n      <li>Broad hardware support via modular execution providers</li>\n      <li>Strong community and industrial backing (Microsoft, AWS, NVIDIA, etc.)</li>\n      <li>Mobile support with optimized builds and quantized execution</li>\n      <li>Extensive language bindings (Python, C++, C#, Java)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons:</strong></p>\n\n    <ul>\n      <li>Debugging can be complex across EPs</li>\n      <li>Conversion process from other frameworks may require custom scripts</li>\n      <li>ONNX opset compatibility issues can arise across versions</li>\n      <li>Mobile optimization (size, latency) requires manual tuning</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros:</strong></p>\n<ul>\n      <li>Framework agnostic and highly interoperable</li>\n      <li>Broad hardware support via modular execution providers</li>\n      <li>Strong community and industrial backing (Microsoft, AWS, NVIDIA, etc.)</li>\n      <li>Mobile support with optimized builds and quantized execution</li>\n      <li>Extensive language bindings (Python, C++, C#, Java)</li>\n    </ul>\n<p><strong>Cons:</strong></p>\n<ul>\n      <li>Debugging can be complex across EPs</li>\n      <li>Conversion process from other frameworks may require custom scripts</li>\n      <li>ONNX opset compatibility issues can arise across versions</li>\n      <li>Mobile optimization (size, latency) requires manual tuning</li>\n    </ul>",
      "contentMarkdown": "*   **Pros:**\n    \n    *   Framework agnostic and highly interoperable\n    *   Broad hardware support via modular execution providers\n    *   Strong community and industrial backing (Microsoft, AWS, NVIDIA, etc.)\n    *   Mobile support with optimized builds and quantized execution\n    *   Extensive language bindings (Python, C++, C#, Java)\n*   **Cons:**\n    \n    *   Debugging can be complex across EPs\n    *   Conversion process from other frameworks may require custom scripts\n    *   ONNX opset compatibility issues can arise across versions\n    *   Mobile optimization (size, latency) requires manual tuning\n\n**Pros:**\n\n*   Framework agnostic and highly interoperable\n*   Broad hardware support via modular execution providers\n*   Strong community and industrial backing (Microsoft, AWS, NVIDIA, etc.)\n*   Mobile support with optimized builds and quantized execution\n*   Extensive language bindings (Python, C++, C#, Java)\n\n**Cons:**\n\n*   Debugging can be complex across EPs\n*   Conversion process from other frameworks may require custom scripts\n*   ONNX opset compatibility issues can arise across versions\n*   Mobile optimization (size, latency) requires manual tuning",
      "order": 23,
      "orderInChapter": 4,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "optimization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 156,
        "contentLength": 1516
      },
      "nextCards": [
        "ai-ml-runtimes-example-code-snippet-python-24",
        "ai-ml-runtimes-use-in-edge-on-device-scenarios-25"
      ],
      "relatedCards": [
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-top-30-papers-deep-residual-learning-for-image-recognition-10",
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-example-code-snippet-python-24",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "ONNX Runtime Deep Dive",
      "title": "Example Code Snippet (Python)",
      "subtitle": "ONNX Runtime Deep Dive",
      "contentHtml": "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code6\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code6\"><span class=\"kn\">import</span> <span class=\"nn\">onnxruntime</span> <span class=\"k\">as</span> <span class=\"n\">ort</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"c1\"># Load ONNX model\n</span><span class=\"n\">session</span> <span class=\"o\">=</span> <span class=\"n\">ort</span><span class=\"p\">.</span><span class=\"n\">InferenceSession</span><span class=\"p\">(</span><span class=\"s\">\"resnet50.onnx\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Prepare input\n</span><span class=\"n\">input_name</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">get_inputs</span><span class=\"p\">()[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">name</span>\n<span class=\"n\">input_data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">).</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Run inference\n</span><span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"p\">{</span><span class=\"n\">input_name</span><span class=\"p\">:</span> <span class=\"n\">input_data</span><span class=\"p\">})</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Prediction shape:\"</span><span class=\"p\">,</span> <span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code6\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code6\"><span class=\"kn\">import</span> <span class=\"nn\">onnxruntime</span> <span class=\"k\">as</span> <span class=\"n\">ort</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"c1\"># Load ONNX model\n</span><span class=\"n\">session</span> <span class=\"o\">=</span> <span class=\"n\">ort</span><span class=\"p\">.</span><span class=\"n\">InferenceSession</span><span class=\"p\">(</span><span class=\"s\">\"resnet50.onnx\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Prepare input\n</span><span class=\"n\">input_name</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">get_inputs</span><span class=\"p\">()[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">name</span>\n<span class=\"n\">input_data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">).</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Run inference\n</span><span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"p\">{</span><span class=\"n\">input_name</span><span class=\"p\">:</span> <span class=\"n\">input_data</span><span class=\"p\">})</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Prediction shape:\"</span><span class=\"p\">,</span> <span class=\"n\">outputs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n</code></pre>\n<p><strong>Using CUDA Execution Provider</strong>:</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code7\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code7\"><span class=\"n\">session</span> <span class=\"o\">=</span> <span class=\"n\">ort</span><span class=\"p\">.</span><span class=\"n\">InferenceSession</span><span class=\"p\">(</span><span class=\"s\">\"resnet50.onnx\"</span><span class=\"p\">,</span> <span class=\"n\">providers</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'CUDAExecutionProvider'</span><span class=\"p\">])</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code7\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code7\"><span class=\"n\">session</span> <span class=\"o\">=</span> <span class=\"n\">ort</span><span class=\"p\">.</span><span class=\"n\">InferenceSession</span><span class=\"p\">(</span><span class=\"s\">\"resnet50.onnx\"</span><span class=\"p\">,</span> <span class=\"n\">providers</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'CUDAExecutionProvider'</span><span class=\"p\">])</span>\n</code></pre>",
      "contentMarkdown": "![](https://aman.ai/images/copy.png)\n\n`import onnxruntime as ort import numpy as np  # Load ONNX model session = ort.InferenceSession(\"resnet50.onnx\")  # Prepare input input_name = session.get_inputs()[0].name input_data = np.random.rand(1, 3, 224, 224).astype(np.float32)  # Run inference outputs = session.run(None, {input_name: input_data})  print(\"Prediction shape:\", outputs[0].shape)`\n\n![](https://aman.ai/images/copy.png)\n\n`import onnxruntime as ort import numpy as np  # Load ONNX model session = ort.InferenceSession(\"resnet50.onnx\")  # Prepare input input_name = session.get_inputs()[0].name input_data = np.random.rand(1, 3, 224, 224).astype(np.float32)  # Run inference outputs = session.run(None, {input_name: input_data})  print(\"Prediction shape:\", outputs[0].shape)`\n\n**Using CUDA Execution Provider**:\n\n![](https://aman.ai/images/copy.png)\n\n`session = ort.InferenceSession(\"resnet50.onnx\", providers=['CUDAExecutionProvider'])`\n\n![](https://aman.ai/images/copy.png)\n\n`session = ort.InferenceSession(\"resnet50.onnx\", providers=['CUDAExecutionProvider'])`",
      "order": 24,
      "orderInChapter": 5,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 92,
        "contentLength": 6222
      },
      "nextCards": [
        "ai-ml-runtimes-use-in-edge-on-device-scenarios-25",
        "ai-ml-runtimes-overview-26"
      ],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-synchronization-and-communication-12",
        "ai-gpu-architecture-memory-management-and-transfer-15",
        "ai-gpu-architecture-inference-deployment-18"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#example-code-snippet-(python)",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-use-in-edge-on-device-scenarios-25",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "ONNX Runtime Deep Dive",
      "title": "Use in Edge / On-Device Scenarios",
      "subtitle": "ONNX Runtime Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p>ONNX Runtime Mobile is specifically designed for deployment on edge devices. Key features include:</p>\n\n    <ul>\n      <li>Stripped-down build (~1–2 MB)</li>\n      <li>FlatBuffer format support in preview</li>\n      <li>Android NNAPI and iOS Core ML integration</li>\n      <li>Prebuilt minimal runtime packages for specific models</li>\n    </ul>\n  </li>\n  <li>\n    <p>ONNX Runtime is best suited for applications where:</p>\n\n    <ul>\n      <li>Portability across hardware is essential</li>\n      <li>Mixed execution (CPU + accelerator) is beneficial</li>\n      <li>The model pipeline involves multiple frameworks</li>\n    </ul>\n  </li>\n</ul>\n<p>ONNX Runtime Mobile is specifically designed for deployment on edge devices. Key features include:</p>\n<ul>\n      <li>Stripped-down build (~1–2 MB)</li>\n      <li>FlatBuffer format support in preview</li>\n      <li>Android NNAPI and iOS Core ML integration</li>\n      <li>Prebuilt minimal runtime packages for specific models</li>\n    </ul>\n<p>ONNX Runtime is best suited for applications where:</p>\n<ul>\n      <li>Portability across hardware is essential</li>\n      <li>Mixed execution (CPU + accelerator) is beneficial</li>\n      <li>The model pipeline involves multiple frameworks</li>\n    </ul>",
      "contentMarkdown": "*   ONNX Runtime Mobile is specifically designed for deployment on edge devices. Key features include:\n    \n    *   Stripped-down build (~1–2 MB)\n    *   FlatBuffer format support in preview\n    *   Android NNAPI and iOS Core ML integration\n    *   Prebuilt minimal runtime packages for specific models\n*   ONNX Runtime is best suited for applications where:\n    \n    *   Portability across hardware is essential\n    *   Mixed execution (CPU + accelerator) is beneficial\n    *   The model pipeline involves multiple frameworks\n\nONNX Runtime Mobile is specifically designed for deployment on edge devices. Key features include:\n\n*   Stripped-down build (~1–2 MB)\n*   FlatBuffer format support in preview\n*   Android NNAPI and iOS Core ML integration\n*   Prebuilt minimal runtime packages for specific models\n\nONNX Runtime is best suited for applications where:\n\n*   Portability across hardware is essential\n*   Mixed execution (CPU + accelerator) is beneficial\n*   The model pipeline involves multiple frameworks",
      "order": 25,
      "orderInChapter": 6,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 142,
        "contentLength": 1262
      },
      "nextCards": [
        "ai-ml-runtimes-overview-26",
        "ai-ml-runtimes-architecture-27"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#use-in-edge-/-on-device-scenarios",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-overview-26",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "ExecuTorch Deep Dive",
      "title": "Overview",
      "subtitle": "ExecuTorch Deep Dive",
      "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: Embedded ML engineers, mobile and edge system developers</li>\n  <li><strong>Use Cases</strong>: Sensor fusion, vision at the edge, voice command detection, ultra-low-power AI applications</li>\n  <li><strong>Model Format</strong>: Compiled TorchScript bytecode (<code class=\"language-plaintext highlighter-rouge\">.ptc</code>)</li>\n  <li><strong>Conversion Tools</strong>: PyTorch <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-5\"><span class=\"mo\" id=\"MathJax-Span-6\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">\\rightarrow</script> TorchScript <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-7\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-8\"><span class=\"mo\" id=\"MathJax-Span-9\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">\\rightarrow</script> ExecuTorch via AOT pipeline</li>\n</ul>",
      "contentMarkdown": "*   **Developer Target**: Embedded ML engineers, mobile and edge system developers\n*   **Use Cases**: Sensor fusion, vision at the edge, voice command detection, ultra-low-power AI applications\n*   **Model Format**: Compiled TorchScript bytecode (`.ptc`)\n*   **Conversion Tools**: PyTorch →→\\\\rightarrow TorchScript →→\\\\rightarrow ExecuTorch via AOT pipeline",
      "order": 26,
      "orderInChapter": 1,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 44,
        "contentLength": 2949
      },
      "nextCards": [
        "ai-ml-runtimes-architecture-27",
        "ai-ml-runtimes-implementation-details-28"
      ],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-synchronization-and-communication-12",
        "ai-gpu-architecture-memory-management-and-transfer-15",
        "ai-gpu-architecture-inference-deployment-18"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#overview",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-architecture-27",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "ExecuTorch Deep Dive",
      "title": "Architecture",
      "subtitle": "ExecuTorch Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p>ExecuTorch redefines the execution pipeline for PyTorch models in low-resource environments. Its architecture includes a static graph compiler, a runtime interpreter, and pluggable dispatch interfaces for targeting different hardware backends.</p>\n  </li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ol>\n      <li>\n        <p><strong>Model Export</strong>:</p>\n\n        <ul>\n          <li>Model defined in PyTorch and traced/scripted via TorchScript.</li>\n          <li>ExecuTorch’s AOT compiler converts it into a compact bytecode format.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Runtime Embedding</strong>:</p>\n\n        <ul>\n          <li>The bytecode and necessary ops are compiled with the target runtime.</li>\n          <li>Optional op pruning removes unused operations.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Deployment</strong>:</p>\n\n        <ul>\n          <li>Model and runtime are flashed onto the device.</li>\n          <li>Inference is run via a lightweight VM interpreter.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Key Components</strong>:</p>\n\n    <ul>\n      <li><strong>Bytecode Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.ptc</code> files contain compiled operators and control flow</li>\n      <li><strong>VM Runtime</strong>: A minimal interpreter that reads and executes bytecode</li>\n      <li><strong>Dispatcher</strong>: Routes ops to backend implementations</li>\n      <li><strong>Memory Arena</strong>: Static memory model, optionally no dynamic allocation</li>\n    </ul>\n  </li>\n</ul>\n<p>ExecuTorch redefines the execution pipeline for PyTorch models in low-resource environments. Its architecture includes a static graph compiler, a runtime interpreter, and pluggable dispatch interfaces for targeting different hardware backends.</p>\n<p><strong>Execution Flow</strong>:</p>\n<ol>\n      <li>\n        <p><strong>Model Export</strong>:</p>\n\n        <ul>\n          <li>Model defined in PyTorch and traced/scripted via TorchScript.</li>\n          <li>ExecuTorch’s AOT compiler converts it into a compact bytecode format.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Runtime Embedding</strong>:</p>\n\n        <ul>\n          <li>The bytecode and necessary ops are compiled with the target runtime.</li>\n          <li>Optional op pruning removes unused operations.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Deployment</strong>:</p>\n\n        <ul>\n          <li>Model and runtime are flashed onto the device.</li>\n          <li>Inference is run via a lightweight VM interpreter.</li>\n        </ul>\n      </li>\n    </ol>\n<p><strong>Model Export</strong>:</p>\n<ul>\n          <li>Model defined in PyTorch and traced/scripted via TorchScript.</li>\n          <li>ExecuTorch’s AOT compiler converts it into a compact bytecode format.</li>\n        </ul>\n<p><strong>Runtime Embedding</strong>:</p>\n<ul>\n          <li>The bytecode and necessary ops are compiled with the target runtime.</li>\n          <li>Optional op pruning removes unused operations.</li>\n        </ul>\n<p><strong>Deployment</strong>:</p>\n<ul>\n          <li>Model and runtime are flashed onto the device.</li>\n          <li>Inference is run via a lightweight VM interpreter.</li>\n        </ul>\n<p><strong>Key Components</strong>:</p>\n<ul>\n      <li><strong>Bytecode Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.ptc</code> files contain compiled operators and control flow</li>\n      <li><strong>VM Runtime</strong>: A minimal interpreter that reads and executes bytecode</li>\n      <li><strong>Dispatcher</strong>: Routes ops to backend implementations</li>\n      <li><strong>Memory Arena</strong>: Static memory model, optionally no dynamic allocation</li>\n    </ul>",
      "contentMarkdown": "*   ExecuTorch redefines the execution pipeline for PyTorch models in low-resource environments. Its architecture includes a static graph compiler, a runtime interpreter, and pluggable dispatch interfaces for targeting different hardware backends.\n    \n*   **Execution Flow**:\n    \n    1.  **Model Export**:\n        \n        *   Model defined in PyTorch and traced/scripted via TorchScript.\n        *   ExecuTorch’s AOT compiler converts it into a compact bytecode format.\n    2.  **Runtime Embedding**:\n        \n        *   The bytecode and necessary ops are compiled with the target runtime.\n        *   Optional op pruning removes unused operations.\n    3.  **Deployment**:\n        \n        *   Model and runtime are flashed onto the device.\n        *   Inference is run via a lightweight VM interpreter.\n*   **Key Components**:\n    \n    *   **Bytecode Format**: `.ptc` files contain compiled operators and control flow\n    *   **VM Runtime**: A minimal interpreter that reads and executes bytecode\n    *   **Dispatcher**: Routes ops to backend implementations\n    *   **Memory Arena**: Static memory model, optionally no dynamic allocation\n\nExecuTorch redefines the execution pipeline for PyTorch models in low-resource environments. Its architecture includes a static graph compiler, a runtime interpreter, and pluggable dispatch interfaces for targeting different hardware backends.\n\n**Execution Flow**:\n\n1.  **Model Export**:\n    \n    *   Model defined in PyTorch and traced/scripted via TorchScript.\n    *   ExecuTorch’s AOT compiler converts it into a compact bytecode format.\n2.  **Runtime Embedding**:\n    \n    *   The bytecode and necessary ops are compiled with the target runtime.\n    *   Optional op pruning removes unused operations.\n3.  **Deployment**:\n    \n    *   Model and runtime are flashed onto the device.\n    *   Inference is run via a lightweight VM interpreter.\n\n**Model Export**:\n\n*   Model defined in PyTorch and traced/scripted via TorchScript.\n*   ExecuTorch’s AOT compiler converts it into a compact bytecode format.\n\n**Runtime Embedding**:\n\n*   The bytecode and necessary ops are compiled with the target runtime.\n*   Optional op pruning removes unused operations.\n\n**Deployment**:\n\n*   Model and runtime are flashed onto the device.\n*   Inference is run via a lightweight VM interpreter.\n\n**Key Components**:\n\n*   **Bytecode Format**: `.ptc` files contain compiled operators and control flow\n*   **VM Runtime**: A minimal interpreter that reads and executes bytecode\n*   **Dispatcher**: Routes ops to backend implementations\n*   **Memory Arena**: Static memory model, optionally no dynamic allocation",
      "order": 27,
      "orderInChapter": 2,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "embedding"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 341,
        "contentLength": 3827
      },
      "nextCards": [
        "ai-ml-runtimes-implementation-details-28",
        "ai-ml-runtimes-pros-and-cons-29"
      ],
      "relatedCards": [
        "ai-top-30-papers-scaling-laws-for-neural-language-models-22",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-choosing-the-right-ann-algorithm-family-9",
        "ai-top-30-papers-precise-zero-shot-dense-retrieval-without-relevanc-32",
        "ai-ann-similarity-search-graph-based-methods-7"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#architecture",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-implementation-details-28",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "ExecuTorch Deep Dive",
      "title": "Implementation Details",
      "subtitle": "ExecuTorch Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>AOT Compiler</strong>:</p>\n\n    <ul>\n      <li>Converts scripted TorchScript models into bytecode and op kernels</li>\n      <li>Includes a model linker that statically binds required ops</li>\n      <li>Can target C/C++ or platform-specific formats (Zephyr, FreeRTOS)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Operator Handling</strong>:</p>\n\n    <ul>\n      <li>Customizable op kernels allow device-specific optimization</li>\n      <li>Optional kernel fusion via compiler passes for performance</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Runtime Constraints</strong>:</p>\n\n    <ul>\n      <li>Code size: Can be &lt;500 KB with aggressive pruning</li>\n      <li>No reliance on dynamic memory allocation (static buffer planning)</li>\n      <li>Designed for devices with as little as 256 KB RAM</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Integration</strong>:</p>\n\n    <ul>\n      <li>Written in C++</li>\n      <li>Can integrate with sensor pipelines, real-time OS, or MCU firmware</li>\n      <li>Open-sourced with tooling for building and flashing models to hardware</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>AOT Compiler</strong>:</p>\n<ul>\n      <li>Converts scripted TorchScript models into bytecode and op kernels</li>\n      <li>Includes a model linker that statically binds required ops</li>\n      <li>Can target C/C++ or platform-specific formats (Zephyr, FreeRTOS)</li>\n    </ul>\n<p><strong>Operator Handling</strong>:</p>\n<ul>\n      <li>Customizable op kernels allow device-specific optimization</li>\n      <li>Optional kernel fusion via compiler passes for performance</li>\n    </ul>\n<p><strong>Runtime Constraints</strong>:</p>\n<ul>\n      <li>Code size: Can be &lt;500 KB with aggressive pruning</li>\n      <li>No reliance on dynamic memory allocation (static buffer planning)</li>\n      <li>Designed for devices with as little as 256 KB RAM</li>\n    </ul>\n<p><strong>Integration</strong>:</p>\n<ul>\n      <li>Written in C++</li>\n      <li>Can integrate with sensor pipelines, real-time OS, or MCU firmware</li>\n      <li>Open-sourced with tooling for building and flashing models to hardware</li>\n    </ul>",
      "contentMarkdown": "*   **AOT Compiler**:\n    \n    *   Converts scripted TorchScript models into bytecode and op kernels\n    *   Includes a model linker that statically binds required ops\n    *   Can target C/C++ or platform-specific formats (Zephyr, FreeRTOS)\n*   **Operator Handling**:\n    \n    *   Customizable op kernels allow device-specific optimization\n    *   Optional kernel fusion via compiler passes for performance\n*   **Runtime Constraints**:\n    \n    *   Code size: Can be <500 KB with aggressive pruning\n    *   No reliance on dynamic memory allocation (static buffer planning)\n    *   Designed for devices with as little as 256 KB RAM\n*   **Integration**:\n    \n    *   Written in C++\n    *   Can integrate with sensor pipelines, real-time OS, or MCU firmware\n    *   Open-sourced with tooling for building and flashing models to hardware\n\n**AOT Compiler**:\n\n*   Converts scripted TorchScript models into bytecode and op kernels\n*   Includes a model linker that statically binds required ops\n*   Can target C/C++ or platform-specific formats (Zephyr, FreeRTOS)\n\n**Operator Handling**:\n\n*   Customizable op kernels allow device-specific optimization\n*   Optional kernel fusion via compiler passes for performance\n\n**Runtime Constraints**:\n\n*   Code size: Can be <500 KB with aggressive pruning\n*   No reliance on dynamic memory allocation (static buffer planning)\n*   Designed for devices with as little as 256 KB RAM\n\n**Integration**:\n\n*   Written in C++\n*   Can integrate with sensor pipelines, real-time OS, or MCU firmware\n*   Open-sourced with tooling for building and flashing models to hardware",
      "order": 28,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "optimization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 222,
        "contentLength": 2142
      },
      "nextCards": [
        "ai-ml-runtimes-pros-and-cons-29",
        "ai-ml-runtimes-example-workflow-30"
      ],
      "relatedCards": [
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-top-30-papers-deep-residual-learning-for-image-recognition-10",
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-pros-and-cons-29",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "ExecuTorch Deep Dive",
      "title": "Pros and Cons",
      "subtitle": "ExecuTorch Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros:</strong></p>\n\n    <ul>\n      <li>Extremely lightweight, MCU-ready</li>\n      <li>AOT compilation reduces runtime overhead</li>\n      <li>Deterministic memory usage (good for real-time applications)</li>\n      <li>Modular and open-source with low-level control</li>\n      <li>PyTorch-compatible workflow for training and export</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons:</strong></p>\n\n    <ul>\n      <li>Requires model to be written in a static subset of PyTorch</li>\n      <li>Limited dynamic control flow (must be scriptable)</li>\n      <li>Debugging and tooling less mature than mainstream PyTorch or TensorFlow Lite</li>\n      <li>Focused on inference only; no training support on-device</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros:</strong></p>\n<ul>\n      <li>Extremely lightweight, MCU-ready</li>\n      <li>AOT compilation reduces runtime overhead</li>\n      <li>Deterministic memory usage (good for real-time applications)</li>\n      <li>Modular and open-source with low-level control</li>\n      <li>PyTorch-compatible workflow for training and export</li>\n    </ul>\n<p><strong>Cons:</strong></p>\n<ul>\n      <li>Requires model to be written in a static subset of PyTorch</li>\n      <li>Limited dynamic control flow (must be scriptable)</li>\n      <li>Debugging and tooling less mature than mainstream PyTorch or TensorFlow Lite</li>\n      <li>Focused on inference only; no training support on-device</li>\n    </ul>",
      "contentMarkdown": "*   **Pros:**\n    \n    *   Extremely lightweight, MCU-ready\n    *   AOT compilation reduces runtime overhead\n    *   Deterministic memory usage (good for real-time applications)\n    *   Modular and open-source with low-level control\n    *   PyTorch-compatible workflow for training and export\n*   **Cons:**\n    \n    *   Requires model to be written in a static subset of PyTorch\n    *   Limited dynamic control flow (must be scriptable)\n    *   Debugging and tooling less mature than mainstream PyTorch or TensorFlow Lite\n    *   Focused on inference only; no training support on-device\n\n**Pros:**\n\n*   Extremely lightweight, MCU-ready\n*   AOT compilation reduces runtime overhead\n*   Deterministic memory usage (good for real-time applications)\n*   Modular and open-source with low-level control\n*   PyTorch-compatible workflow for training and export\n\n**Cons:**\n\n*   Requires model to be written in a static subset of PyTorch\n*   Limited dynamic control flow (must be scriptable)\n*   Debugging and tooling less mature than mainstream PyTorch or TensorFlow Lite\n*   Focused on inference only; no training support on-device",
      "order": 29,
      "orderInChapter": 4,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 152,
        "contentLength": 1462
      },
      "nextCards": [
        "ai-ml-runtimes-example-workflow-30",
        "ai-ml-runtimes-suitable-applications-31"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-example-workflow-30",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "ExecuTorch Deep Dive",
      "title": "Example Workflow",
      "subtitle": "ExecuTorch Deep Dive",
      "contentHtml": "<ul>\n  <li><strong>Model Export (Python):</strong></li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code8\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code8\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"n\">nn</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">TinyModel</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">().</span><span class=\"n\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">fc</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">fc</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">TinyModel</span><span class=\"p\">()</span>\n<span class=\"n\">scripted</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">jit</span><span class=\"p\">.</span><span class=\"n\">script</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n<span class=\"n\">scripted</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"model.pt\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code8\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code8\"><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n<span class=\"kn\">import</span> <span class=\"nn\">torch.nn</span> <span class=\"k\">as</span> <span class=\"n\">nn</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">TinyModel</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">().</span><span class=\"n\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">fc</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">fc</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">TinyModel</span><span class=\"p\">()</span>\n<span class=\"n\">scripted</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">jit</span><span class=\"p\">.</span><span class=\"n\">script</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">)</span>\n<span class=\"n\">scripted</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"model.pt\"</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li><strong>ExecuTorch AOT Compilation (CLI or CMake):</strong></li>\n</ul>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code9\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code9\">executorchc compile <span class=\"nt\">--model</span> model.pt <span class=\"nt\">--output</span> model.ptc <span class=\"nt\">--target</span> cortex-m\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code9\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code9\">executorchc compile <span class=\"nt\">--model</span> model.pt <span class=\"nt\">--output</span> model.ptc <span class=\"nt\">--target</span> cortex-m\n</code></pre>\n<ul>\n  <li><strong>Embedded Runtime Integration (C++):</strong></li>\n</ul>\n<div class=\"language-cpp highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code10\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code10\"><span class=\"cp\">#include</span> <span class=\"cpf\">\"executorch/runtime/runtime.h\"</span><span class=\"cp\">\n</span>\n<span class=\"n\">executorch</span><span class=\"o\">::</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s\">\"model.ptc\"</span><span class=\"p\">);</span>\n<span class=\"n\">executorch</span><span class=\"o\">::</span><span class=\"n\">run_model</span><span class=\"p\">(</span><span class=\"n\">input_tensor</span><span class=\"p\">,</span> <span class=\"n\">output_tensor</span><span class=\"p\">);</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code10\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code10\"><span class=\"cp\">#include</span> <span class=\"cpf\">\"executorch/runtime/runtime.h\"</span><span class=\"cp\">\n</span>\n<span class=\"n\">executorch</span><span class=\"o\">::</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s\">\"model.ptc\"</span><span class=\"p\">);</span>\n<span class=\"n\">executorch</span><span class=\"o\">::</span><span class=\"n\">run_model</span><span class=\"p\">(</span><span class=\"n\">input_tensor</span><span class=\"p\">,</span> <span class=\"n\">output_tensor</span><span class=\"p\">);</span>\n</code></pre>",
      "contentMarkdown": "*   **Model Export (Python):**\n\n![](https://aman.ai/images/copy.png)\n\n`import torch import torch.nn as nn  class TinyModel(nn.Module):     def __init__(self):         super().__init__()         self.fc = nn.Linear(4, 2)      def forward(self, x):         return self.fc(x)  model = TinyModel() scripted = torch.jit.script(model) scripted.save(\"model.pt\")`\n\n![](https://aman.ai/images/copy.png)\n\n`import torch import torch.nn as nn  class TinyModel(nn.Module):     def __init__(self):         super().__init__()         self.fc = nn.Linear(4, 2)      def forward(self, x):         return self.fc(x)  model = TinyModel() scripted = torch.jit.script(model) scripted.save(\"model.pt\")`\n\n*   **ExecuTorch AOT Compilation (CLI or CMake):**\n\n![](https://aman.ai/images/copy.png)\n\n`executorchc compile --model model.pt --output model.ptc --target cortex-m`\n\n![](https://aman.ai/images/copy.png)\n\n`executorchc compile --model model.pt --output model.ptc --target cortex-m`\n\n*   **Embedded Runtime Integration (C++):**\n\n![](https://aman.ai/images/copy.png)\n\n`#include \"executorch/runtime/runtime.h\" executorch::load_model(\"model.ptc\"); executorch::run_model(input_tensor, output_tensor);`\n\n![](https://aman.ai/images/copy.png)\n\n`#include \"executorch/runtime/runtime.h\" executorch::load_model(\"model.ptc\"); executorch::run_model(input_tensor, output_tensor);`",
      "order": 30,
      "orderInChapter": 5,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 102,
        "contentLength": 7251
      },
      "nextCards": [
        "ai-ml-runtimes-suitable-applications-31",
        "ai-ml-runtimes-overview-32"
      ],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-synchronization-and-communication-12",
        "ai-gpu-architecture-memory-management-and-transfer-15",
        "ai-gpu-architecture-inference-deployment-18"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#example-workflow",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-suitable-applications-31",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "ExecuTorch Deep Dive",
      "title": "Suitable Applications",
      "subtitle": "ExecuTorch Deep Dive",
      "contentHtml": "<ul>\n  <li>Wake-word detection on MCUs</li>\n  <li>Gesture recognition using MEMS sensors</li>\n  <li>Smart agriculture (tiny vision models)</li>\n  <li>\n    <p>Battery-powered health monitoring devices</p>\n  </li>\n  <li>ExecuTorch fills a critical niche for deploying PyTorch-trained models on hardware where traditional runtimes like TensorFlow Lite or ONNX Runtime are too heavy.</li>\n</ul>\n<p>Battery-powered health monitoring devices</p>",
      "contentMarkdown": "*   Wake-word detection on MCUs\n*   Gesture recognition using MEMS sensors\n*   Smart agriculture (tiny vision models)\n*   Battery-powered health monitoring devices\n    \n*   ExecuTorch fills a critical niche for deploying PyTorch-trained models on hardware where traditional runtimes like TensorFlow Lite or ONNX Runtime are too heavy.\n\nBattery-powered health monitoring devices",
      "order": 31,
      "orderInChapter": 6,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 50,
        "contentLength": 439
      },
      "nextCards": [
        "ai-ml-runtimes-overview-32",
        "ai-ml-runtimes-architecture-33"
      ],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-synchronization-and-communication-12",
        "ai-gpu-architecture-memory-management-and-transfer-15",
        "ai-gpu-architecture-inference-deployment-18"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#suitable-applications",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-overview-32",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "LidarTLM Deep Dive",
      "title": "Overview",
      "subtitle": "LidarTLM Deep Dive",
      "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: Robotics, ADAS, and autonomous system engineers</li>\n  <li><strong>Use Cases</strong>: Real-time 3D object detection, SLAM (Simultaneous Localization and Mapping), point cloud segmentation, obstacle avoidance</li>\n  <li><strong>Model Format</strong>: Often custom or adapted from PyTorch/ONNX; serialized as tensors or voxel grids</li>\n  <li><strong>Conversion Tools</strong>: Typically includes preprocessing pipelines from ROS, Open3D, or custom CUDA kernels</li>\n</ul>",
      "contentMarkdown": "*   **Developer Target**: Robotics, ADAS, and autonomous system engineers\n*   **Use Cases**: Real-time 3D object detection, SLAM (Simultaneous Localization and Mapping), point cloud segmentation, obstacle avoidance\n*   **Model Format**: Often custom or adapted from PyTorch/ONNX; serialized as tensors or voxel grids\n*   **Conversion Tools**: Typically includes preprocessing pipelines from ROS, Open3D, or custom CUDA kernels",
      "order": 32,
      "orderInChapter": 1,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 55,
        "contentLength": 517
      },
      "nextCards": [
        "ai-ml-runtimes-architecture-33",
        "ai-ml-runtimes-implementation-details-34"
      ],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-synchronization-and-communication-12",
        "ai-gpu-architecture-memory-management-and-transfer-15",
        "ai-gpu-architecture-inference-deployment-18"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#overview",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-architecture-33",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "LidarTLM Deep Dive",
      "title": "Architecture",
      "subtitle": "LidarTLM Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p>LidarTLM-style systems typically deviate from conventional 2D image-based ML runtimes. They require efficient spatial processing, optimized memory layouts, and hardware support for sparse data structures.</p>\n  </li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ol>\n      <li><strong>Sensor Input</strong>: Raw LiDAR packets or fused multi-sensor data (e.g., IMU + LiDAR) ingested</li>\n      <li><strong>Preprocessing</strong>: Point clouds downsampled, voxelized, or transformed to Bird’s-Eye View (BEV)</li>\n      <li><strong>Inference</strong>: Tensorized data passed through neural layers (e.g., 3D convolutions, attention modules)</li>\n      <li><strong>Postprocessing</strong>: Bounding boxes or semantic maps generated</li>\n      <li><strong>Fusion (Optional)</strong>: Sensor fusion with radar, camera, or odometry</li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Key Components</strong>:</p>\n\n    <ul>\n      <li><strong>Spatial Encoder</strong>: Transforms sparse point clouds into dense tensor formats (e.g., voxel grids, range images)</li>\n      <li><strong>Sparse CNNs or VoxelNet Layers</strong>: Specialized convolution ops for irregular input data</li>\n      <li><strong>Temporal Modules</strong>: Optional RNN, attention, or transformer blocks for sequential scans</li>\n      <li><strong>Hardware Abstraction</strong>: Targets CUDA-enabled GPUs or embedded AI processors (e.g., NVIDIA Xavier, TI Jacinto)</li>\n    </ul>\n  </li>\n</ul>\n<p>LidarTLM-style systems typically deviate from conventional 2D image-based ML runtimes. They require efficient spatial processing, optimized memory layouts, and hardware support for sparse data structures.</p>\n<p><strong>Execution Flow</strong>:</p>\n<ol>\n      <li><strong>Sensor Input</strong>: Raw LiDAR packets or fused multi-sensor data (e.g., IMU + LiDAR) ingested</li>\n      <li><strong>Preprocessing</strong>: Point clouds downsampled, voxelized, or transformed to Bird’s-Eye View (BEV)</li>\n      <li><strong>Inference</strong>: Tensorized data passed through neural layers (e.g., 3D convolutions, attention modules)</li>\n      <li><strong>Postprocessing</strong>: Bounding boxes or semantic maps generated</li>\n      <li><strong>Fusion (Optional)</strong>: Sensor fusion with radar, camera, or odometry</li>\n    </ol>\n<p><strong>Key Components</strong>:</p>\n<ul>\n      <li><strong>Spatial Encoder</strong>: Transforms sparse point clouds into dense tensor formats (e.g., voxel grids, range images)</li>\n      <li><strong>Sparse CNNs or VoxelNet Layers</strong>: Specialized convolution ops for irregular input data</li>\n      <li><strong>Temporal Modules</strong>: Optional RNN, attention, or transformer blocks for sequential scans</li>\n      <li><strong>Hardware Abstraction</strong>: Targets CUDA-enabled GPUs or embedded AI processors (e.g., NVIDIA Xavier, TI Jacinto)</li>\n    </ul>",
      "contentMarkdown": "*   LidarTLM-style systems typically deviate from conventional 2D image-based ML runtimes. They require efficient spatial processing, optimized memory layouts, and hardware support for sparse data structures.\n    \n*   **Execution Flow**:\n    \n    1.  **Sensor Input**: Raw LiDAR packets or fused multi-sensor data (e.g., IMU + LiDAR) ingested\n    2.  **Preprocessing**: Point clouds downsampled, voxelized, or transformed to Bird’s-Eye View (BEV)\n    3.  **Inference**: Tensorized data passed through neural layers (e.g., 3D convolutions, attention modules)\n    4.  **Postprocessing**: Bounding boxes or semantic maps generated\n    5.  **Fusion (Optional)**: Sensor fusion with radar, camera, or odometry\n*   **Key Components**:\n    \n    *   **Spatial Encoder**: Transforms sparse point clouds into dense tensor formats (e.g., voxel grids, range images)\n    *   **Sparse CNNs or VoxelNet Layers**: Specialized convolution ops for irregular input data\n    *   **Temporal Modules**: Optional RNN, attention, or transformer blocks for sequential scans\n    *   **Hardware Abstraction**: Targets CUDA-enabled GPUs or embedded AI processors (e.g., NVIDIA Xavier, TI Jacinto)\n\nLidarTLM-style systems typically deviate from conventional 2D image-based ML runtimes. They require efficient spatial processing, optimized memory layouts, and hardware support for sparse data structures.\n\n**Execution Flow**:\n\n1.  **Sensor Input**: Raw LiDAR packets or fused multi-sensor data (e.g., IMU + LiDAR) ingested\n2.  **Preprocessing**: Point clouds downsampled, voxelized, or transformed to Bird’s-Eye View (BEV)\n3.  **Inference**: Tensorized data passed through neural layers (e.g., 3D convolutions, attention modules)\n4.  **Postprocessing**: Bounding boxes or semantic maps generated\n5.  **Fusion (Optional)**: Sensor fusion with radar, camera, or odometry\n\n**Key Components**:\n\n*   **Spatial Encoder**: Transforms sparse point clouds into dense tensor formats (e.g., voxel grids, range images)\n*   **Sparse CNNs or VoxelNet Layers**: Specialized convolution ops for irregular input data\n*   **Temporal Modules**: Optional RNN, attention, or transformer blocks for sequential scans\n*   **Hardware Abstraction**: Targets CUDA-enabled GPUs or embedded AI processors (e.g., NVIDIA Xavier, TI Jacinto)",
      "order": 33,
      "orderInChapter": 2,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "transformer",
        "attention",
        "convolution",
        "cnn",
        "rnn"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 289,
        "contentLength": 2875
      },
      "nextCards": [
        "ai-ml-runtimes-implementation-details-34",
        "ai-ml-runtimes-pros-and-cons-35"
      ],
      "relatedCards": [
        "ai-top-30-papers-attention-is-all-you-need-13",
        "ai-top-30-papers-variational-lossy-autoencoder-17",
        "ai-cnns-for-text-classification-convolutions-1",
        "ai-cnns-for-text-classification-fully-connected-3",
        "ai-top-30-papers-pointer-networks-6"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#architecture",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-implementation-details-34",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "LidarTLM Deep Dive",
      "title": "Implementation Details",
      "subtitle": "LidarTLM Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Tensor Representation</strong>:</p>\n\n    <ul>\n      <li>Often uses sparse tensors or hybrid dense-sparse structures</li>\n      <li>Libraries like MinkowskiEngine, SpConv, or custom CUDA kernels for voxel ops</li>\n      <li>Quantization may be used to reduce memory footprint in embedded settings</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Optimization Techniques</strong>:</p>\n\n    <ul>\n      <li>Efficient neighbor search (KD-trees, octrees) for local feature aggregation</li>\n      <li>Temporal caching of features from prior scans</li>\n      <li>Batch fusion for multi-sensor inputs</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Deployment</strong>:</p>\n\n    <ul>\n      <li>Embedded platforms like NVIDIA Jetson, TI DSPs, and ADAS-grade microcontrollers</li>\n      <li>Often integrated with ROS (Robot Operating System) for I/O and control flow</li>\n      <li>May use C++, CUDA, or even custom ASIC/NPU firmware for deterministic performance</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Tensor Representation</strong>:</p>\n<ul>\n      <li>Often uses sparse tensors or hybrid dense-sparse structures</li>\n      <li>Libraries like MinkowskiEngine, SpConv, or custom CUDA kernels for voxel ops</li>\n      <li>Quantization may be used to reduce memory footprint in embedded settings</li>\n    </ul>\n<p><strong>Optimization Techniques</strong>:</p>\n<ul>\n      <li>Efficient neighbor search (KD-trees, octrees) for local feature aggregation</li>\n      <li>Temporal caching of features from prior scans</li>\n      <li>Batch fusion for multi-sensor inputs</li>\n    </ul>\n<p><strong>Deployment</strong>:</p>\n<ul>\n      <li>Embedded platforms like NVIDIA Jetson, TI DSPs, and ADAS-grade microcontrollers</li>\n      <li>Often integrated with ROS (Robot Operating System) for I/O and control flow</li>\n      <li>May use C++, CUDA, or even custom ASIC/NPU firmware for deterministic performance</li>\n    </ul>",
      "contentMarkdown": "*   **Tensor Representation**:\n    \n    *   Often uses sparse tensors or hybrid dense-sparse structures\n    *   Libraries like MinkowskiEngine, SpConv, or custom CUDA kernels for voxel ops\n    *   Quantization may be used to reduce memory footprint in embedded settings\n*   **Optimization Techniques**:\n    \n    *   Efficient neighbor search (KD-trees, octrees) for local feature aggregation\n    *   Temporal caching of features from prior scans\n    *   Batch fusion for multi-sensor inputs\n*   **Deployment**:\n    \n    *   Embedded platforms like NVIDIA Jetson, TI DSPs, and ADAS-grade microcontrollers\n    *   Often integrated with ROS (Robot Operating System) for I/O and control flow\n    *   May use C++, CUDA, or even custom ASIC/NPU firmware for deterministic performance\n\n**Tensor Representation**:\n\n*   Often uses sparse tensors or hybrid dense-sparse structures\n*   Libraries like MinkowskiEngine, SpConv, or custom CUDA kernels for voxel ops\n*   Quantization may be used to reduce memory footprint in embedded settings\n\n**Optimization Techniques**:\n\n*   Efficient neighbor search (KD-trees, octrees) for local feature aggregation\n*   Temporal caching of features from prior scans\n*   Batch fusion for multi-sensor inputs\n\n**Deployment**:\n\n*   Embedded platforms like NVIDIA Jetson, TI DSPs, and ADAS-grade microcontrollers\n*   Often integrated with ROS (Robot Operating System) for I/O and control flow\n*   May use C++, CUDA, or even custom ASIC/NPU firmware for deterministic performance",
      "order": 34,
      "orderInChapter": 3,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "optimization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 201,
        "contentLength": 1920
      },
      "nextCards": [
        "ai-ml-runtimes-pros-and-cons-35",
        "ai-ml-runtimes-example-pseudocode-flow-36"
      ],
      "relatedCards": [
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-top-30-papers-deep-residual-learning-for-image-recognition-10",
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-pros-and-cons-35",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "LidarTLM Deep Dive",
      "title": "Pros and Cons",
      "subtitle": "LidarTLM Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros:</strong></p>\n\n    <ul>\n      <li>Designed for spatial and temporal data, not just 2D tensors</li>\n      <li>Optimized for sparse inputs and low-latency inference</li>\n      <li>Supports sensor fusion pipelines, enabling richer context</li>\n      <li>Can run on edge-grade GPUs or embedded NPUs</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons:</strong></p>\n\n    <ul>\n      <li>Fragmented tooling, often bespoke or tightly coupled to hardware</li>\n      <li>Lack of standardized runtime interface (unlike ONNX or TFLite)</li>\n      <li>Difficult to deploy across platforms without custom engineering</li>\n      <li>Sparse community and documentation; often buried in academic or industrial codebases</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros:</strong></p>\n<ul>\n      <li>Designed for spatial and temporal data, not just 2D tensors</li>\n      <li>Optimized for sparse inputs and low-latency inference</li>\n      <li>Supports sensor fusion pipelines, enabling richer context</li>\n      <li>Can run on edge-grade GPUs or embedded NPUs</li>\n    </ul>\n<p><strong>Cons:</strong></p>\n<ul>\n      <li>Fragmented tooling, often bespoke or tightly coupled to hardware</li>\n      <li>Lack of standardized runtime interface (unlike ONNX or TFLite)</li>\n      <li>Difficult to deploy across platforms without custom engineering</li>\n      <li>Sparse community and documentation; often buried in academic or industrial codebases</li>\n    </ul>",
      "contentMarkdown": "*   **Pros:**\n    \n    *   Designed for spatial and temporal data, not just 2D tensors\n    *   Optimized for sparse inputs and low-latency inference\n    *   Supports sensor fusion pipelines, enabling richer context\n    *   Can run on edge-grade GPUs or embedded NPUs\n*   **Cons:**\n    \n    *   Fragmented tooling, often bespoke or tightly coupled to hardware\n    *   Lack of standardized runtime interface (unlike ONNX or TFLite)\n    *   Difficult to deploy across platforms without custom engineering\n    *   Sparse community and documentation; often buried in academic or industrial codebases\n\n**Pros:**\n\n*   Designed for spatial and temporal data, not just 2D tensors\n*   Optimized for sparse inputs and low-latency inference\n*   Supports sensor fusion pipelines, enabling richer context\n*   Can run on edge-grade GPUs or embedded NPUs\n\n**Cons:**\n\n*   Fragmented tooling, often bespoke or tightly coupled to hardware\n*   Lack of standardized runtime interface (unlike ONNX or TFLite)\n*   Difficult to deploy across platforms without custom engineering\n*   Sparse community and documentation; often buried in academic or industrial codebases",
      "order": 35,
      "orderInChapter": 4,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 160,
        "contentLength": 1464
      },
      "nextCards": [
        "ai-ml-runtimes-example-pseudocode-flow-36",
        "ai-ml-runtimes-suitable-applications-37"
      ],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-synchronization-and-communication-12",
        "ai-gpu-architecture-memory-management-and-transfer-15",
        "ai-gpu-architecture-inference-deployment-18"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-example-pseudocode-flow-36",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "LidarTLM Deep Dive",
      "title": "Example Pseudocode Flow",
      "subtitle": "LidarTLM Deep Dive",
      "contentHtml": "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code11\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code11\"><span class=\"c1\"># Step 1: Load point cloud\n</span><span class=\"n\">point_cloud</span> <span class=\"o\">=</span> <span class=\"n\">load_lidar_scan</span><span class=\"p\">(</span><span class=\"s\">\"/scans/frame_001.bin\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 2: Convert to voxel grid\n</span><span class=\"n\">voxel_grid</span> <span class=\"o\">=</span> <span class=\"n\">voxelize</span><span class=\"p\">(</span><span class=\"n\">point_cloud</span><span class=\"p\">,</span> <span class=\"n\">grid_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Step 3: Pass through 3D CNN\n</span><span class=\"n\">features</span> <span class=\"o\">=</span> <span class=\"n\">sparse_conv_net</span><span class=\"p\">(</span><span class=\"n\">voxel_grid</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 4: Predict bounding boxes or labels\n</span><span class=\"n\">detections</span> <span class=\"o\">=</span> <span class=\"n\">decode_bounding_boxes</span><span class=\"p\">(</span><span class=\"n\">features</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 5: Fuse with other sensors (optional)\n</span><span class=\"n\">fused_output</span> <span class=\"o\">=</span> <span class=\"n\">fuse_with_camera</span><span class=\"p\">(</span><span class=\"n\">detections</span><span class=\"p\">,</span> <span class=\"n\">rgb_frame</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code11\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code11\"><span class=\"c1\"># Step 1: Load point cloud\n</span><span class=\"n\">point_cloud</span> <span class=\"o\">=</span> <span class=\"n\">load_lidar_scan</span><span class=\"p\">(</span><span class=\"s\">\"/scans/frame_001.bin\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 2: Convert to voxel grid\n</span><span class=\"n\">voxel_grid</span> <span class=\"o\">=</span> <span class=\"n\">voxelize</span><span class=\"p\">(</span><span class=\"n\">point_cloud</span><span class=\"p\">,</span> <span class=\"n\">grid_size</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Step 3: Pass through 3D CNN\n</span><span class=\"n\">features</span> <span class=\"o\">=</span> <span class=\"n\">sparse_conv_net</span><span class=\"p\">(</span><span class=\"n\">voxel_grid</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 4: Predict bounding boxes or labels\n</span><span class=\"n\">detections</span> <span class=\"o\">=</span> <span class=\"n\">decode_bounding_boxes</span><span class=\"p\">(</span><span class=\"n\">features</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Step 5: Fuse with other sensors (optional)\n</span><span class=\"n\">fused_output</span> <span class=\"o\">=</span> <span class=\"n\">fuse_with_camera</span><span class=\"p\">(</span><span class=\"n\">detections</span><span class=\"p\">,</span> <span class=\"n\">rgb_frame</span><span class=\"p\">)</span>\n</code></pre>",
      "contentMarkdown": "![](https://aman.ai/images/copy.png)\n\n`# Step 1: Load point cloud point_cloud = load_lidar_scan(\"/scans/frame_001.bin\")  # Step 2: Convert to voxel grid voxel_grid = voxelize(point_cloud, grid_size=(0.1, 0.1, 0.1))  # Step 3: Pass through 3D CNN features = sparse_conv_net(voxel_grid)  # Step 4: Predict bounding boxes or labels detections = decode_bounding_boxes(features)  # Step 5: Fuse with other sensors (optional) fused_output = fuse_with_camera(detections, rgb_frame)`\n\n![](https://aman.ai/images/copy.png)\n\n`# Step 1: Load point cloud point_cloud = load_lidar_scan(\"/scans/frame_001.bin\")  # Step 2: Convert to voxel grid voxel_grid = voxelize(point_cloud, grid_size=(0.1, 0.1, 0.1))  # Step 3: Pass through 3D CNN features = sparse_conv_net(voxel_grid)  # Step 4: Predict bounding boxes or labels detections = decode_bounding_boxes(features)  # Step 5: Fuse with other sensors (optional) fused_output = fuse_with_camera(detections, rgb_frame)`",
      "order": 36,
      "orderInChapter": 5,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "cnn"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 112,
        "contentLength": 3663
      },
      "nextCards": [
        "ai-ml-runtimes-suitable-applications-37",
        "ai-ml-runtimes-llamacpp-deep-dive-38"
      ],
      "relatedCards": [
        "ai-cnns-for-text-classification-fully-connected-3",
        "ai-cnns-for-text-classification-convolutional-neural-networks-for-sentence-classif-6",
        "ai-top-30-papers-variational-lossy-autoencoder-17",
        "ai-gpu-architecture-multi-gpu-memory-coherency-31",
        "ai-gpu-architecture-fundamental-architectural-components-amd-32"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#example-pseudocode-flow",
      "scrapedAt": "2025-12-28T11:56:51.671Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-suitable-applications-37",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "LidarTLM Deep Dive",
      "title": "Suitable Applications",
      "subtitle": "LidarTLM Deep Dive",
      "contentHtml": "<ul>\n  <li>Autonomous vehicles (3D perception stacks)</li>\n  <li>Warehouse robots and drones</li>\n  <li>Industrial inspection systems</li>\n  <li>Advanced driver-assistance systems (ADAS)</li>\n  <li>\n    <p>SLAM systems for robotics</p>\n  </li>\n  <li>LidarTLM-like runtimes are not meant for general ML workloads but are highly optimized for 3D spatiotemporal inference, where conventional 2D model runtimes fall short. They tend to be integrated deep into hardware-specific SDKs or research frameworks.</li>\n</ul>\n<p>SLAM systems for robotics</p>",
      "contentMarkdown": "*   Autonomous vehicles (3D perception stacks)\n*   Warehouse robots and drones\n*   Industrial inspection systems\n*   Advanced driver-assistance systems (ADAS)\n*   SLAM systems for robotics\n    \n*   LidarTLM-like runtimes are not meant for general ML workloads but are highly optimized for 3D spatiotemporal inference, where conventional 2D model runtimes fall short. They tend to be integrated deep into hardware-specific SDKs or research frameworks.\n\nSLAM systems for robotics",
      "order": 37,
      "orderInChapter": 6,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 66,
        "contentLength": 546
      },
      "nextCards": [
        "ai-ml-runtimes-llamacpp-deep-dive-38",
        "ai-ml-runtimes-overview-39"
      ],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-synchronization-and-communication-12",
        "ai-gpu-architecture-memory-management-and-transfer-15",
        "ai-gpu-architecture-inference-deployment-18"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#suitable-applications",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-llamacpp-deep-dive-38",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "LidarTLM Deep Dive",
      "title": "llama.cpp Deep Dive",
      "subtitle": "LidarTLM Deep Dive",
      "contentHtml": "<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> is an open-source, C++-based implementation of inference for large language models (LLMs), originally inspired by Meta’s LLaMA family. It focuses on efficient CPU (and optionally GPU) inference for quantized transformer models. Unlike full ML runtimes, <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> is specialized, minimalist, and optimized for running LLMs—particularly on devices with constrained memory and compute budgets such as laptops, desktops, and even smartphones.</li>\n</ul>",
      "contentMarkdown": "*   `llama.cpp` is an open-source, C++-based implementation of inference for large language models (LLMs), originally inspired by Meta’s LLaMA family. It focuses on efficient CPU (and optionally GPU) inference for quantized transformer models. Unlike full ML runtimes, `llama.cpp` is specialized, minimalist, and optimized for running LLMs—particularly on devices with constrained memory and compute budgets such as laptops, desktops, and even smartphones.",
      "order": 38,
      "orderInChapter": 7,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "transformer",
        "llm"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 61,
        "contentLength": 586
      },
      "nextCards": [
        "ai-ml-runtimes-overview-39",
        "ai-ml-runtimes-architecture-40"
      ],
      "relatedCards": [
        "ai-gpu-architecture-tensor-core-evolution-23",
        "ai-gpu-architecture-key-takeaways-38",
        "ai-gpu-architecture-per-generation-precision-support-and-performance-26",
        "ai-top-30-papers-better-faster-large-language-models-via-multi-toke-27",
        "ai-gpu-architecture-floating-point-precision-performance-evolution-35"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#llama.cpp-deep-dive",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-overview-39",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "LidarTLM Deep Dive",
      "title": "Overview",
      "subtitle": "LidarTLM Deep Dive",
      "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: LLM researchers, app developers, hobbyists</li>\n  <li><strong>Use Cases</strong>: Local chatbots, privacy-preserving LLM apps, embedded NLP on edge devices</li>\n  <li><strong>Model Format</strong>: Quantized GGUF (GPT-generated GGML Unified Format)</li>\n  <li><strong>Conversion Tools</strong>: Python conversion scripts from PyTorch checkpoints to GGUF</li>\n</ul>",
      "contentMarkdown": "*   **Developer Target**: LLM researchers, app developers, hobbyists\n*   **Use Cases**: Local chatbots, privacy-preserving LLM apps, embedded NLP on edge devices\n*   **Model Format**: Quantized GGUF (GPT-generated GGML Unified Format)\n*   **Conversion Tools**: Python conversion scripts from PyTorch checkpoints to GGUF",
      "order": 39,
      "orderInChapter": 8,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "gpt",
        "llm",
        "nlp"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 41,
        "contentLength": 410
      },
      "nextCards": [
        "ai-ml-runtimes-architecture-40",
        "ai-ml-runtimes-implementation-details-41"
      ],
      "relatedCards": [
        "ai-top-30-papers-alcuna-large-language-models-meet-new-knowledge-33",
        "ai-top-30-papers-the-perils-promises-of-fact-checking-with-large-la-34",
        "ai-conditional-random-fields-undirected-graphical-models-2",
        "ai-top-30-papers-zephyr-direct-distillation-of-lm-alignment-30",
        "ai-top-30-papers-lost-in-the-middle-how-language-models-use-long-co-31"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#overview",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-architecture-40",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "LidarTLM Deep Dive",
      "title": "Architecture",
      "subtitle": "LidarTLM Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> does not use a traditional ML runtime stack. It is built from the ground up with custom tensor operations and a static execution loop tailored to transformer inference.</p>\n  </li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ol>\n      <li><strong>Model Load</strong>: Quantized GGUF file loaded into memory</li>\n      <li><strong>KV Cache Allocation</strong>: Allocates buffers for key/value attention caching</li>\n      <li><strong>Token Embedding &amp; Input Prep</strong>: Maps token IDs to embeddings</li>\n      <li><strong>Layer Execution Loop</strong>: Runs transformer blocks sequentially</li>\n      <li><strong>Logits Output</strong>: Computes next-token logits, passed to sampler</li>\n      <li><strong>Sampling &amp; Token Generation</strong>: Greedy, top-k, nucleus, or temperature sampling</li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Key Components</strong>:</p>\n\n    <ul>\n      <li><strong>GGML Backend</strong>: Custom tensor library with support for CPU SIMD ops (AVX, FMA, NEON)</li>\n      <li><strong>Quantization Layers</strong>: 4-bit, 5-bit, and 8-bit quantized matmuls</li>\n      <li><strong>Inference Loop</strong>: Manually unrolled transformer stack—one layer at a time</li>\n      <li><strong>KV Cache Management</strong>: Token sequence history for autoregressive decoding</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Optional GPU Support</strong>:</p>\n\n    <ul>\n      <li>Metal (macOS), OpenCL, CUDA support via modular backends</li>\n      <li>Offloading options: attention only, matmuls only, or full GPU</li>\n    </ul>\n  </li>\n</ul>\n<p><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> does not use a traditional ML runtime stack. It is built from the ground up with custom tensor operations and a static execution loop tailored to transformer inference.</p>\n<p><strong>Execution Flow</strong>:</p>\n<ol>\n      <li><strong>Model Load</strong>: Quantized GGUF file loaded into memory</li>\n      <li><strong>KV Cache Allocation</strong>: Allocates buffers for key/value attention caching</li>\n      <li><strong>Token Embedding &amp; Input Prep</strong>: Maps token IDs to embeddings</li>\n      <li><strong>Layer Execution Loop</strong>: Runs transformer blocks sequentially</li>\n      <li><strong>Logits Output</strong>: Computes next-token logits, passed to sampler</li>\n      <li><strong>Sampling &amp; Token Generation</strong>: Greedy, top-k, nucleus, or temperature sampling</li>\n    </ol>\n<p><strong>Key Components</strong>:</p>\n<ul>\n      <li><strong>GGML Backend</strong>: Custom tensor library with support for CPU SIMD ops (AVX, FMA, NEON)</li>\n      <li><strong>Quantization Layers</strong>: 4-bit, 5-bit, and 8-bit quantized matmuls</li>\n      <li><strong>Inference Loop</strong>: Manually unrolled transformer stack—one layer at a time</li>\n      <li><strong>KV Cache Management</strong>: Token sequence history for autoregressive decoding</li>\n    </ul>\n<p><strong>Optional GPU Support</strong>:</p>\n<ul>\n      <li>Metal (macOS), OpenCL, CUDA support via modular backends</li>\n      <li>Offloading options: attention only, matmuls only, or full GPU</li>\n    </ul>",
      "contentMarkdown": "*   `llama.cpp` does not use a traditional ML runtime stack. It is built from the ground up with custom tensor operations and a static execution loop tailored to transformer inference.\n    \n*   **Execution Flow**:\n    \n    1.  **Model Load**: Quantized GGUF file loaded into memory\n    2.  **KV Cache Allocation**: Allocates buffers for key/value attention caching\n    3.  **Token Embedding & Input Prep**: Maps token IDs to embeddings\n    4.  **Layer Execution Loop**: Runs transformer blocks sequentially\n    5.  **Logits Output**: Computes next-token logits, passed to sampler\n    6.  **Sampling & Token Generation**: Greedy, top-k, nucleus, or temperature sampling\n*   **Key Components**:\n    \n    *   **GGML Backend**: Custom tensor library with support for CPU SIMD ops (AVX, FMA, NEON)\n    *   **Quantization Layers**: 4-bit, 5-bit, and 8-bit quantized matmuls\n    *   **Inference Loop**: Manually unrolled transformer stack—one layer at a time\n    *   **KV Cache Management**: Token sequence history for autoregressive decoding\n*   **Optional GPU Support**:\n    \n    *   Metal (macOS), OpenCL, CUDA support via modular backends\n    *   Offloading options: attention only, matmuls only, or full GPU\n\n`llama.cpp` does not use a traditional ML runtime stack. It is built from the ground up with custom tensor operations and a static execution loop tailored to transformer inference.\n\n**Execution Flow**:\n\n1.  **Model Load**: Quantized GGUF file loaded into memory\n2.  **KV Cache Allocation**: Allocates buffers for key/value attention caching\n3.  **Token Embedding & Input Prep**: Maps token IDs to embeddings\n4.  **Layer Execution Loop**: Runs transformer blocks sequentially\n5.  **Logits Output**: Computes next-token logits, passed to sampler\n6.  **Sampling & Token Generation**: Greedy, top-k, nucleus, or temperature sampling\n\n**Key Components**:\n\n*   **GGML Backend**: Custom tensor library with support for CPU SIMD ops (AVX, FMA, NEON)\n*   **Quantization Layers**: 4-bit, 5-bit, and 8-bit quantized matmuls\n*   **Inference Loop**: Manually unrolled transformer stack—one layer at a time\n*   **KV Cache Management**: Token sequence history for autoregressive decoding\n\n**Optional GPU Support**:\n\n*   Metal (macOS), OpenCL, CUDA support via modular backends\n*   Offloading options: attention only, matmuls only, or full GPU",
      "order": 40,
      "orderInChapter": 9,
      "difficulty": 5,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "transformer",
        "attention",
        "embedding"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 320,
        "contentLength": 3221
      },
      "nextCards": [
        "ai-ml-runtimes-implementation-details-41",
        "ai-ml-runtimes-pros-and-cons-42"
      ],
      "relatedCards": [
        "ai-top-30-papers-scaling-laws-for-neural-language-models-22",
        "ai-gpu-architecture-floating-point-precision-performance-evolution-35",
        "ai-gpu-architecture-comparative-analysis-amd-vs-nvidia-architectures-37",
        "ai-top-30-papers-order-matters-sequence-to-sequence-for-sets-8",
        "ai-top-30-papers-attention-is-all-you-need-13"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#architecture",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-implementation-details-41",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "LidarTLM Deep Dive",
      "title": "Implementation Details",
      "subtitle": "LidarTLM Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Model Quantization</strong>:</p>\n\n    <ul>\n      <li>Tools like <code class=\"language-plaintext highlighter-rouge\">quantize.py</code> convert PyTorch models to GGUF format</li>\n      <li>Supports several quantization strategies (Q4_0, Q5_K, Q8_0, etc.)</li>\n      <li>Tradeoff between model size and accuracy</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Tensor Engine</strong>:</p>\n\n    <ul>\n      <li>No external libraries like BLAS, cuDNN, or MKL used by default</li>\n      <li>Uses hand-optimized C++ with platform-specific intrinsics</li>\n      <li>Cross-platform: macOS, Linux, Windows, WebAssembly (via WASM)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Memory Optimization</strong>:</p>\n\n    <ul>\n      <li>Memory mapped file support (<code class=\"language-plaintext highlighter-rouge\">mmap</code>)</li>\n      <li>Low memory mode: restricts KV cache or context length</li>\n      <li>Paging and streaming support for large contexts (e.g., <code class=\"language-plaintext highlighter-rouge\">llama.cpp + vLLM</code>)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Integration</strong>:</p>\n\n    <ul>\n      <li>C API and Python bindings (<code class=\"language-plaintext highlighter-rouge\">llama-cpp-python</code>)</li>\n      <li>Works with tools like LangChain, OpenRouter, and Ollama</li>\n      <li>Compatible with most LLaMA-family models: LLaMA, Alpaca, Vicuna, Mistral, etc.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Model Quantization</strong>:</p>\n<ul>\n      <li>Tools like <code class=\"language-plaintext highlighter-rouge\">quantize.py</code> convert PyTorch models to GGUF format</li>\n      <li>Supports several quantization strategies (Q4_0, Q5_K, Q8_0, etc.)</li>\n      <li>Tradeoff between model size and accuracy</li>\n    </ul>\n<p><strong>Tensor Engine</strong>:</p>\n<ul>\n      <li>No external libraries like BLAS, cuDNN, or MKL used by default</li>\n      <li>Uses hand-optimized C++ with platform-specific intrinsics</li>\n      <li>Cross-platform: macOS, Linux, Windows, WebAssembly (via WASM)</li>\n    </ul>\n<p><strong>Memory Optimization</strong>:</p>\n<ul>\n      <li>Memory mapped file support (<code class=\"language-plaintext highlighter-rouge\">mmap</code>)</li>\n      <li>Low memory mode: restricts KV cache or context length</li>\n      <li>Paging and streaming support for large contexts (e.g., <code class=\"language-plaintext highlighter-rouge\">llama.cpp + vLLM</code>)</li>\n    </ul>\n<p><strong>Integration</strong>:</p>\n<ul>\n      <li>C API and Python bindings (<code class=\"language-plaintext highlighter-rouge\">llama-cpp-python</code>)</li>\n      <li>Works with tools like LangChain, OpenRouter, and Ollama</li>\n      <li>Compatible with most LLaMA-family models: LLaMA, Alpaca, Vicuna, Mistral, etc.</li>\n    </ul>",
      "contentMarkdown": "*   **Model Quantization**:\n    \n    *   Tools like `quantize.py` convert PyTorch models to GGUF format\n    *   Supports several quantization strategies (Q4\\_0, Q5\\_K, Q8\\_0, etc.)\n    *   Tradeoff between model size and accuracy\n*   **Tensor Engine**:\n    \n    *   No external libraries like BLAS, cuDNN, or MKL used by default\n    *   Uses hand-optimized C++ with platform-specific intrinsics\n    *   Cross-platform: macOS, Linux, Windows, WebAssembly (via WASM)\n*   **Memory Optimization**:\n    \n    *   Memory mapped file support (`mmap`)\n    *   Low memory mode: restricts KV cache or context length\n    *   Paging and streaming support for large contexts (e.g., `llama.cpp + vLLM`)\n*   **Integration**:\n    \n    *   C API and Python bindings (`llama-cpp-python`)\n    *   Works with tools like LangChain, OpenRouter, and Ollama\n    *   Compatible with most LLaMA-family models: LLaMA, Alpaca, Vicuna, Mistral, etc.\n\n**Model Quantization**:\n\n*   Tools like `quantize.py` convert PyTorch models to GGUF format\n*   Supports several quantization strategies (Q4\\_0, Q5\\_K, Q8\\_0, etc.)\n*   Tradeoff between model size and accuracy\n\n**Tensor Engine**:\n\n*   No external libraries like BLAS, cuDNN, or MKL used by default\n*   Uses hand-optimized C++ with platform-specific intrinsics\n*   Cross-platform: macOS, Linux, Windows, WebAssembly (via WASM)\n\n**Memory Optimization**:\n\n*   Memory mapped file support (`mmap`)\n*   Low memory mode: restricts KV cache or context length\n*   Paging and streaming support for large contexts (e.g., `llama.cpp + vLLM`)\n\n**Integration**:\n\n*   C API and Python bindings (`llama-cpp-python`)\n*   Works with tools like LangChain, OpenRouter, and Ollama\n*   Compatible with most LLaMA-family models: LLaMA, Alpaca, Vicuna, Mistral, etc.",
      "order": 41,
      "orderInChapter": 10,
      "difficulty": 5,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "llm",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 234,
        "contentLength": 2764
      },
      "nextCards": [
        "ai-ml-runtimes-pros-and-cons-42",
        "ai-ml-runtimes-example-cli-inference-43"
      ],
      "relatedCards": [
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16",
        "ai-top-30-papers-alcuna-large-language-models-meet-new-knowledge-33"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-pros-and-cons-42",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "LidarTLM Deep Dive",
      "title": "Pros and Cons",
      "subtitle": "LidarTLM Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros:</strong></p>\n\n    <ul>\n      <li>Extremely fast CPU inference (real-time on MacBook M1/M2, even some Raspberry Pi 4)</li>\n      <li>Portable and minimal dependencies</li>\n      <li>Quantization enables running models with &lt;4 GB RAM</li>\n      <li>Easily embedded into apps, games, and command-line tools</li>\n      <li>Active community and ecosystem (used in projects like Ollama and LM Studio)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons:</strong></p>\n\n    <ul>\n      <li>Transformer-only; not a general ML runtime</li>\n      <li>No training support—strictly for inference</li>\n      <li>Manual conversion and tuning process required</li>\n      <li>Limited ops support; cannot easily add new ML layers</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros:</strong></p>\n<ul>\n      <li>Extremely fast CPU inference (real-time on MacBook M1/M2, even some Raspberry Pi 4)</li>\n      <li>Portable and minimal dependencies</li>\n      <li>Quantization enables running models with &lt;4 GB RAM</li>\n      <li>Easily embedded into apps, games, and command-line tools</li>\n      <li>Active community and ecosystem (used in projects like Ollama and LM Studio)</li>\n    </ul>\n<p><strong>Cons:</strong></p>\n<ul>\n      <li>Transformer-only; not a general ML runtime</li>\n      <li>No training support—strictly for inference</li>\n      <li>Manual conversion and tuning process required</li>\n      <li>Limited ops support; cannot easily add new ML layers</li>\n    </ul>",
      "contentMarkdown": "*   **Pros:**\n    \n    *   Extremely fast CPU inference (real-time on MacBook M1/M2, even some Raspberry Pi 4)\n    *   Portable and minimal dependencies\n    *   Quantization enables running models with <4 GB RAM\n    *   Easily embedded into apps, games, and command-line tools\n    *   Active community and ecosystem (used in projects like Ollama and LM Studio)\n*   **Cons:**\n    \n    *   Transformer-only; not a general ML runtime\n    *   No training support—strictly for inference\n    *   Manual conversion and tuning process required\n    *   Limited ops support; cannot easily add new ML layers\n\n**Pros:**\n\n*   Extremely fast CPU inference (real-time on MacBook M1/M2, even some Raspberry Pi 4)\n*   Portable and minimal dependencies\n*   Quantization enables running models with <4 GB RAM\n*   Easily embedded into apps, games, and command-line tools\n*   Active community and ecosystem (used in projects like Ollama and LM Studio)\n\n**Cons:**\n\n*   Transformer-only; not a general ML runtime\n*   No training support—strictly for inference\n*   Manual conversion and tuning process required\n*   Limited ops support; cannot easily add new ML layers",
      "order": 42,
      "orderInChapter": 11,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "transformer"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 166,
        "contentLength": 1488
      },
      "nextCards": [
        "ai-ml-runtimes-example-cli-inference-43",
        "ai-ml-runtimes-suitable-applications-44"
      ],
      "relatedCards": [
        "ai-gpu-architecture-performance-analysis-27",
        "ai-gpu-architecture-cache-hierarchy-changes-29",
        "ai-gpu-architecture-memory-latency-and-efficiency-improvements-30",
        "ai-gpu-architecture-floating-point-precision-performance-evolution-35",
        "ai-gpu-architecture-comparative-analysis-amd-vs-nvidia-architectures-37"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-example-cli-inference-43",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "LidarTLM Deep Dive",
      "title": "Example CLI Inference",
      "subtitle": "LidarTLM Deep Dive",
      "contentHtml": "<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code12\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code12\">./main <span class=\"nt\">-m</span> models/llama-7B.Q4_0.gguf <span class=\"nt\">-p</span> <span class=\"s2\">\"What is the capital of France?\"</span> <span class=\"nt\">-n</span> 64\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code12\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code12\">./main <span class=\"nt\">-m</span> models/llama-7B.Q4_0.gguf <span class=\"nt\">-p</span> <span class=\"s2\">\"What is the capital of France?\"</span> <span class=\"nt\">-n</span> 64\n</code></pre>\n<ul>\n  <li><strong>Python Inference (via <code class=\"language-plaintext highlighter-rouge\">llama-cpp-python</code>)</strong>:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code13\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code13\"><span class=\"kn\">from</span> <span class=\"nn\">llama_cpp</span> <span class=\"kn\">import</span> <span class=\"n\">Llama</span>\n\n<span class=\"n\">llm</span> <span class=\"o\">=</span> <span class=\"n\">Llama</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"s\">\"llama-7B.Q4_0.gguf\"</span><span class=\"p\">)</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"p\">(</span><span class=\"s\">\"Q: What is the capital of France?</span><span class=\"se\">\\n</span><span class=\"s\">A:\"</span><span class=\"p\">,</span> <span class=\"n\">max_tokens</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">[</span><span class=\"s\">\"choices\"</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s\">\"text\"</span><span class=\"p\">])</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code13\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code13\"><span class=\"kn\">from</span> <span class=\"nn\">llama_cpp</span> <span class=\"kn\">import</span> <span class=\"n\">Llama</span>\n\n<span class=\"n\">llm</span> <span class=\"o\">=</span> <span class=\"n\">Llama</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"s\">\"llama-7B.Q4_0.gguf\"</span><span class=\"p\">)</span>\n<span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"p\">(</span><span class=\"s\">\"Q: What is the capital of France?</span><span class=\"se\">\\n</span><span class=\"s\">A:\"</span><span class=\"p\">,</span> <span class=\"n\">max_tokens</span><span class=\"o\">=</span><span class=\"mi\">32</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">[</span><span class=\"s\">\"choices\"</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s\">\"text\"</span><span class=\"p\">])</span>\n</code></pre>\n<ul>\n  <li>\n    <p><strong>WebAssembly Example (Browser):</strong></p>\n\n    <ul>\n      <li>Precompiled WASM version can run LLMs client-side using WebGPU</li>\n      <li>Useful for private, offline AI assistants directly in browser</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>WebAssembly Example (Browser):</strong></p>\n<ul>\n      <li>Precompiled WASM version can run LLMs client-side using WebGPU</li>\n      <li>Useful for private, offline AI assistants directly in browser</li>\n    </ul>",
      "contentMarkdown": "![](https://aman.ai/images/copy.png)\n\n`./main -m models/llama-7B.Q4_0.gguf -p \"What is the capital of France?\" -n 64`\n\n![](https://aman.ai/images/copy.png)\n\n`./main -m models/llama-7B.Q4_0.gguf -p \"What is the capital of France?\" -n 64`\n\n*   **Python Inference (via `llama-cpp-python`)**:\n\n![](https://aman.ai/images/copy.png)\n\n`from llama_cpp import Llama  llm = Llama(model_path=\"llama-7B.Q4_0.gguf\") output = llm(\"Q: What is the capital of France?\\nA:\", max_tokens=32) print(output[\"choices\"][0][\"text\"])`\n\n![](https://aman.ai/images/copy.png)\n\n`from llama_cpp import Llama  llm = Llama(model_path=\"llama-7B.Q4_0.gguf\") output = llm(\"Q: What is the capital of France?\\nA:\", max_tokens=32) print(output[\"choices\"][0][\"text\"])`\n\n*   **WebAssembly Example (Browser):**\n    \n    *   Precompiled WASM version can run LLMs client-side using WebGPU\n    *   Useful for private, offline AI assistants directly in browser\n\n**WebAssembly Example (Browser):**\n\n*   Precompiled WASM version can run LLMs client-side using WebGPU\n*   Useful for private, offline AI assistants directly in browser",
      "order": 43,
      "orderInChapter": 12,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "llm"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 116,
        "contentLength": 4222
      },
      "nextCards": [
        "ai-ml-runtimes-suitable-applications-44",
        "ai-ml-runtimes-overview-45"
      ],
      "relatedCards": [
        "ai-top-30-papers-alcuna-large-language-models-meet-new-knowledge-33",
        "ai-top-30-papers-the-perils-promises-of-fact-checking-with-large-la-34",
        "ai-gpu-architecture-tensor-core-evolution-23",
        "ai-gpu-architecture-key-takeaways-38",
        "ai-gpu-architecture-per-generation-precision-support-and-performance-26"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#example-cli-inference",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-suitable-applications-44",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "LidarTLM Deep Dive",
      "title": "Suitable Applications",
      "subtitle": "LidarTLM Deep Dive",
      "contentHtml": "<ul>\n  <li>Private, offline chatbots</li>\n  <li>Voice assistants embedded in hardware</li>\n  <li>Context-aware agents in games or productivity apps</li>\n  <li>\n    <p>Developer tools with local NLP capabilities</p>\n  </li>\n  <li><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> showcases what is possible with small, optimized transformer runtimes and CPU-centric design. It’s not a general-purpose ML runtime but a powerful engine for language inference where privacy, portability, or internet-free operation is desired.</li>\n</ul>\n<p>Developer tools with local NLP capabilities</p>",
      "contentMarkdown": "*   Private, offline chatbots\n*   Voice assistants embedded in hardware\n*   Context-aware agents in games or productivity apps\n*   Developer tools with local NLP capabilities\n    \n*   `llama.cpp` showcases what is possible with small, optimized transformer runtimes and CPU-centric design. It’s not a general-purpose ML runtime but a powerful engine for language inference where privacy, portability, or internet-free operation is desired.\n\nDeveloper tools with local NLP capabilities",
      "order": 44,
      "orderInChapter": 13,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "transformer",
        "nlp"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 66,
        "contentLength": 602
      },
      "nextCards": [
        "ai-ml-runtimes-overview-45",
        "ai-ml-runtimes-tensorflow-lite-architecture-46"
      ],
      "relatedCards": [
        "ai-gpu-architecture-floating-point-precision-performance-evolution-35",
        "ai-gpu-architecture-comparative-analysis-amd-vs-nvidia-architectures-37",
        "ai-top-30-papers-retrieval-augmented-generation-for-knowledge-inten-29",
        "ai-gpu-architecture-performance-analysis-27",
        "ai-gpu-architecture-cache-hierarchy-changes-29"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#suitable-applications",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-overview-45",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "title": "Overview",
      "subtitle": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "contentHtml": "<ul>\n  <li><strong>Developer Target</strong>: Mobile developers, embedded engineers, production ML ops</li>\n  <li><strong>Use Cases</strong>: Real-time image classification, object detection, audio processing, NLP, edge analytics</li>\n  <li><strong>Model Format</strong>: <code class=\"language-plaintext highlighter-rouge\">.tflite</code> (FlatBuffer format)</li>\n  <li><strong>Conversion Tools</strong>: TensorFlow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-10\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-11\"><span class=\"mo\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">\\rightarrow</script> TFLite via <code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code></li>\n</ul>",
      "contentMarkdown": "*   **Developer Target**: Mobile developers, embedded engineers, production ML ops\n*   **Use Cases**: Real-time image classification, object detection, audio processing, NLP, edge analytics\n*   **Model Format**: `.tflite` (FlatBuffer format)\n*   **Conversion Tools**: TensorFlow →→\\\\rightarrow TFLite via `TFLiteConverter`",
      "order": 45,
      "orderInChapter": 1,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "nlp"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 37,
        "contentLength": 1750
      },
      "nextCards": [
        "ai-ml-runtimes-tensorflow-lite-architecture-46",
        "ai-ml-runtimes-implementation-details-47"
      ],
      "relatedCards": [
        "ai-conditional-random-fields-undirected-graphical-models-2",
        "ai-gpu-architecture-multi-gpu-memory-coherency-31",
        "ai-gpu-architecture-fundamental-architectural-components-amd-32",
        "ai-gpu-architecture-execution-paradigms-amd-33",
        "ai-gpu-architecture-compute-architecture-evolution-amd-cdna-rdna-34"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#overview",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-tensorflow-lite-architecture-46",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "title": "TensorFlow Lite Architecture",
      "subtitle": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p>TFLite’s design emphasizes performance, size efficiency, and hardware acceleration. It is structured around a model interpreter, a delegate mechanism for hardware acceleration, and a set of optimized operator kernels.</p>\n  </li>\n  <li>\n    <p><strong>Execution Flow</strong>:</p>\n\n    <ol>\n      <li>\n        <p><strong>Model Conversion</strong>:</p>\n\n        <ul>\n          <li>Uses <code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code> to convert SavedModel or Keras models into a FlatBuffer-encoded <code class=\"language-plaintext highlighter-rouge\">.tflite</code> model.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Model Load</strong>:</p>\n\n        <ul>\n          <li>The model is loaded by the <code class=\"language-plaintext highlighter-rouge\">Interpreter</code> class on the target device.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Tensor Allocation</strong>:</p>\n\n        <ul>\n          <li>Memory buffers for input/output tensors are allocated.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Inference Execution</strong>:</p>\n\n        <ul>\n          <li>The interpreter evaluates the computation graph, optionally using delegates.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Postprocessing</strong>:</p>\n\n        <ul>\n          <li>Output tensors are read and interpreted by the application.</li>\n        </ul>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p><strong>Key Components</strong>:</p>\n\n    <ul>\n      <li><strong>FlatBuffer Model</strong>: Compact, zero-copy, serializable model format</li>\n      <li><strong>Interpreter</strong>: Core engine that evaluates the model graph</li>\n      <li><strong>Delegate Interface</strong>: Offloads subgraphs to specialized hardware (GPU, DSP, NPU)</li>\n      <li><strong>Kernel Registry</strong>: Maps ops to optimized C++ implementations (or delegates)</li>\n    </ul>\n  </li>\n</ul>\n<p>TFLite’s design emphasizes performance, size efficiency, and hardware acceleration. It is structured around a model interpreter, a delegate mechanism for hardware acceleration, and a set of optimized operator kernels.</p>\n<p><strong>Execution Flow</strong>:</p>\n<ol>\n      <li>\n        <p><strong>Model Conversion</strong>:</p>\n\n        <ul>\n          <li>Uses <code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code> to convert SavedModel or Keras models into a FlatBuffer-encoded <code class=\"language-plaintext highlighter-rouge\">.tflite</code> model.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Model Load</strong>:</p>\n\n        <ul>\n          <li>The model is loaded by the <code class=\"language-plaintext highlighter-rouge\">Interpreter</code> class on the target device.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Tensor Allocation</strong>:</p>\n\n        <ul>\n          <li>Memory buffers for input/output tensors are allocated.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Inference Execution</strong>:</p>\n\n        <ul>\n          <li>The interpreter evaluates the computation graph, optionally using delegates.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Postprocessing</strong>:</p>\n\n        <ul>\n          <li>Output tensors are read and interpreted by the application.</li>\n        </ul>\n      </li>\n    </ol>\n<p><strong>Model Conversion</strong>:</p>\n<ul>\n          <li>Uses <code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code> to convert SavedModel or Keras models into a FlatBuffer-encoded <code class=\"language-plaintext highlighter-rouge\">.tflite</code> model.</li>\n        </ul>\n<p><strong>Model Load</strong>:</p>\n<ul>\n          <li>The model is loaded by the <code class=\"language-plaintext highlighter-rouge\">Interpreter</code> class on the target device.</li>\n        </ul>\n<p><strong>Tensor Allocation</strong>:</p>\n<ul>\n          <li>Memory buffers for input/output tensors are allocated.</li>\n        </ul>\n<p><strong>Inference Execution</strong>:</p>\n<ul>\n          <li>The interpreter evaluates the computation graph, optionally using delegates.</li>\n        </ul>\n<p><strong>Postprocessing</strong>:</p>\n<ul>\n          <li>Output tensors are read and interpreted by the application.</li>\n        </ul>\n<p><strong>Key Components</strong>:</p>\n<ul>\n      <li><strong>FlatBuffer Model</strong>: Compact, zero-copy, serializable model format</li>\n      <li><strong>Interpreter</strong>: Core engine that evaluates the model graph</li>\n      <li><strong>Delegate Interface</strong>: Offloads subgraphs to specialized hardware (GPU, DSP, NPU)</li>\n      <li><strong>Kernel Registry</strong>: Maps ops to optimized C++ implementations (or delegates)</li>\n    </ul>",
      "contentMarkdown": "*   TFLite’s design emphasizes performance, size efficiency, and hardware acceleration. It is structured around a model interpreter, a delegate mechanism for hardware acceleration, and a set of optimized operator kernels.\n    \n*   **Execution Flow**:\n    \n    1.  **Model Conversion**:\n        \n        *   Uses `TFLiteConverter` to convert SavedModel or Keras models into a FlatBuffer-encoded `.tflite` model.\n    2.  **Model Load**:\n        \n        *   The model is loaded by the `Interpreter` class on the target device.\n    3.  **Tensor Allocation**:\n        \n        *   Memory buffers for input/output tensors are allocated.\n    4.  **Inference Execution**:\n        \n        *   The interpreter evaluates the computation graph, optionally using delegates.\n    5.  **Postprocessing**:\n        \n        *   Output tensors are read and interpreted by the application.\n*   **Key Components**:\n    \n    *   **FlatBuffer Model**: Compact, zero-copy, serializable model format\n    *   **Interpreter**: Core engine that evaluates the model graph\n    *   **Delegate Interface**: Offloads subgraphs to specialized hardware (GPU, DSP, NPU)\n    *   **Kernel Registry**: Maps ops to optimized C++ implementations (or delegates)\n\nTFLite’s design emphasizes performance, size efficiency, and hardware acceleration. It is structured around a model interpreter, a delegate mechanism for hardware acceleration, and a set of optimized operator kernels.\n\n**Execution Flow**:\n\n1.  **Model Conversion**:\n    \n    *   Uses `TFLiteConverter` to convert SavedModel or Keras models into a FlatBuffer-encoded `.tflite` model.\n2.  **Model Load**:\n    \n    *   The model is loaded by the `Interpreter` class on the target device.\n3.  **Tensor Allocation**:\n    \n    *   Memory buffers for input/output tensors are allocated.\n4.  **Inference Execution**:\n    \n    *   The interpreter evaluates the computation graph, optionally using delegates.\n5.  **Postprocessing**:\n    \n    *   Output tensors are read and interpreted by the application.\n\n**Model Conversion**:\n\n*   Uses `TFLiteConverter` to convert SavedModel or Keras models into a FlatBuffer-encoded `.tflite` model.\n\n**Model Load**:\n\n*   The model is loaded by the `Interpreter` class on the target device.\n\n**Tensor Allocation**:\n\n*   Memory buffers for input/output tensors are allocated.\n\n**Inference Execution**:\n\n*   The interpreter evaluates the computation graph, optionally using delegates.\n\n**Postprocessing**:\n\n*   Output tensors are read and interpreted by the application.\n\n**Key Components**:\n\n*   **FlatBuffer Model**: Compact, zero-copy, serializable model format\n*   **Interpreter**: Core engine that evaluates the model graph\n*   **Delegate Interface**: Offloads subgraphs to specialized hardware (GPU, DSP, NPU)\n*   **Kernel Registry**: Maps ops to optimized C++ implementations (or delegates)",
      "order": 46,
      "orderInChapter": 2,
      "difficulty": 5,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 349,
        "contentLength": 4736
      },
      "nextCards": [
        "ai-ml-runtimes-implementation-details-47",
        "ai-ml-runtimes-tensorflow-serving-short-overview-48"
      ],
      "relatedCards": [
        "ai-gpu-architecture-multi-gpu-memory-coherency-31",
        "ai-gpu-architecture-fundamental-architectural-components-amd-32",
        "ai-gpu-architecture-execution-paradigms-amd-33",
        "ai-gpu-architecture-compute-architecture-evolution-amd-cdna-rdna-34",
        "ai-gpu-architecture-memory-architecture-evolution-36"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#tensorflow-lite-architecture",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-implementation-details-47",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "title": "Implementation Details",
      "subtitle": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Model Conversion</strong>:</p>\n\n    <ul>\n      <li>Converts SavedModels, Keras <code class=\"language-plaintext highlighter-rouge\">.h5</code>, or concrete functions to <code class=\"language-plaintext highlighter-rouge\">.tflite</code></li>\n      <li>Supports post-training quantization (dynamic, full integer, float16)</li>\n      <li>Model optimizations include constant folding, op fusion, and pruning</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Delegates</strong>:</p>\n\n    <ul>\n      <li>\n        <p>Optional hardware acceleration backends:</p>\n\n        <ul>\n          <li><strong>NNAPI</strong> (Android)</li>\n          <li><strong>GPU Delegate</strong> (OpenCL, Metal)</li>\n          <li><strong>Hexagon Delegate</strong> (Qualcomm DSP)</li>\n          <li><strong>Core ML Delegate</strong> (iOS/macOS)</li>\n          <li><strong>EdgeTPU Delegate</strong> (Coral devices)</li>\n        </ul>\n      </li>\n      <li>\n        <p>Delegates work by “claiming” supported subgraphs during interpreter initialization</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Threading and Performance</strong>:</p>\n\n    <ul>\n      <li>Supports multi-threaded inference</li>\n      <li>Interpreter can be run in C++, Java, Kotlin, Python, Swift</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Model Conversion</strong>:</p>\n<ul>\n      <li>Converts SavedModels, Keras <code class=\"language-plaintext highlighter-rouge\">.h5</code>, or concrete functions to <code class=\"language-plaintext highlighter-rouge\">.tflite</code></li>\n      <li>Supports post-training quantization (dynamic, full integer, float16)</li>\n      <li>Model optimizations include constant folding, op fusion, and pruning</li>\n    </ul>\n<p><strong>Delegates</strong>:</p>\n<ul>\n      <li>\n        <p>Optional hardware acceleration backends:</p>\n\n        <ul>\n          <li><strong>NNAPI</strong> (Android)</li>\n          <li><strong>GPU Delegate</strong> (OpenCL, Metal)</li>\n          <li><strong>Hexagon Delegate</strong> (Qualcomm DSP)</li>\n          <li><strong>Core ML Delegate</strong> (iOS/macOS)</li>\n          <li><strong>EdgeTPU Delegate</strong> (Coral devices)</li>\n        </ul>\n      </li>\n      <li>\n        <p>Delegates work by “claiming” supported subgraphs during interpreter initialization</p>\n      </li>\n    </ul>\n<p>Optional hardware acceleration backends:</p>\n<ul>\n          <li><strong>NNAPI</strong> (Android)</li>\n          <li><strong>GPU Delegate</strong> (OpenCL, Metal)</li>\n          <li><strong>Hexagon Delegate</strong> (Qualcomm DSP)</li>\n          <li><strong>Core ML Delegate</strong> (iOS/macOS)</li>\n          <li><strong>EdgeTPU Delegate</strong> (Coral devices)</li>\n        </ul>\n<p>Delegates work by “claiming” supported subgraphs during interpreter initialization</p>\n<p><strong>Threading and Performance</strong>:</p>\n<ul>\n      <li>Supports multi-threaded inference</li>\n      <li>Interpreter can be run in C++, Java, Kotlin, Python, Swift</li>\n    </ul>",
      "contentMarkdown": "*   **Model Conversion**:\n    \n    *   Converts SavedModels, Keras `.h5`, or concrete functions to `.tflite`\n    *   Supports post-training quantization (dynamic, full integer, float16)\n    *   Model optimizations include constant folding, op fusion, and pruning\n*   **Delegates**:\n    \n    *   Optional hardware acceleration backends:\n        \n        *   **NNAPI** (Android)\n        *   **GPU Delegate** (OpenCL, Metal)\n        *   **Hexagon Delegate** (Qualcomm DSP)\n        *   **Core ML Delegate** (iOS/macOS)\n        *   **EdgeTPU Delegate** (Coral devices)\n    *   Delegates work by “claiming” supported subgraphs during interpreter initialization\n        \n*   **Threading and Performance**:\n    \n    *   Supports multi-threaded inference\n    *   Interpreter can be run in C++, Java, Kotlin, Python, Swift\n\n**Model Conversion**:\n\n*   Converts SavedModels, Keras `.h5`, or concrete functions to `.tflite`\n*   Supports post-training quantization (dynamic, full integer, float16)\n*   Model optimizations include constant folding, op fusion, and pruning\n\n**Delegates**:\n\n*   Optional hardware acceleration backends:\n    \n    *   **NNAPI** (Android)\n    *   **GPU Delegate** (OpenCL, Metal)\n    *   **Hexagon Delegate** (Qualcomm DSP)\n    *   **Core ML Delegate** (iOS/macOS)\n    *   **EdgeTPU Delegate** (Coral devices)\n*   Delegates work by “claiming” supported subgraphs during interpreter initialization\n    \n\nOptional hardware acceleration backends:\n\n*   **NNAPI** (Android)\n*   **GPU Delegate** (OpenCL, Metal)\n*   **Hexagon Delegate** (Qualcomm DSP)\n*   **Core ML Delegate** (iOS/macOS)\n*   **EdgeTPU Delegate** (Coral devices)\n\nDelegates work by “claiming” supported subgraphs during interpreter initialization\n\n**Threading and Performance**:\n\n*   Supports multi-threaded inference\n*   Interpreter can be run in C++, Java, Kotlin, Python, Swift",
      "order": 47,
      "orderInChapter": 3,
      "difficulty": 5,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 213,
        "contentLength": 2974
      },
      "nextCards": [
        "ai-ml-runtimes-tensorflow-serving-short-overview-48",
        "ai-ml-runtimes-pros-and-cons-49"
      ],
      "relatedCards": [
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16",
        "ai-model-debugging-hyper-parameter-optimization-hpo-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#implementation-details",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-tensorflow-serving-short-overview-48",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "title": "TensorFlow Serving (Short Overview)",
      "subtitle": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "contentHtml": "<ul>\n  <li>Designed for scalable deployment of TensorFlow models on servers</li>\n  <li>Models are exposed as REST/gRPC endpoints</li>\n  <li>Automatically loads, unloads, and versions models</li>\n  <li>Uses <code class=\"language-plaintext highlighter-rouge\">SavedModel</code> format, not <code class=\"language-plaintext highlighter-rouge\">.tflite</code></li>\n  <li>\n    <p>Not suitable for offline or embedded deployment</p>\n  </li>\n  <li><strong>Use Case Comparison</strong>:</li>\n</ul>\n<p>Not suitable for offline or embedded deployment</p>\n<p>Here is your formatted table following the provided style:</p>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Feature</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Target Device</td>\n<td class=\"tg-tleft-valign-first\">Mobile/Edge</td>\n<td class=\"tg-tleft-valign-second\">Cloud/Server</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Model Format</td>\n<td class=\"tg-tleft-valign-first\"><code>.tflite</code></td>\n<td class=\"tg-tleft-valign-second\">SavedModel</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Communication</td>\n<td class=\"tg-tleft-valign-first\">In-process / Local</td>\n<td class=\"tg-tleft-valign-second\">gRPC / REST</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Latency</td>\n<td class=\"tg-tleft-valign-first\">Milliseconds</td>\n<td class=\"tg-tleft-valign-second\">Sub-second to seconds</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Training Support</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">No (inference only)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Deployment Size</td>\n<td class=\"tg-tleft-valign-first\">Small (~100s of KB)</td>\n<td class=\"tg-tleft-valign-second\">Large, server framework</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Feature</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Target Device</td>\n<td class=\"tg-tleft-valign-first\">Mobile/Edge</td>\n<td class=\"tg-tleft-valign-second\">Cloud/Server</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Model Format</td>\n<td class=\"tg-tleft-valign-first\"><code>.tflite</code></td>\n<td class=\"tg-tleft-valign-second\">SavedModel</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Communication</td>\n<td class=\"tg-tleft-valign-first\">In-process / Local</td>\n<td class=\"tg-tleft-valign-second\">gRPC / REST</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Latency</td>\n<td class=\"tg-tleft-valign-first\">Milliseconds</td>\n<td class=\"tg-tleft-valign-second\">Sub-second to seconds</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Training Support</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-second\">No (inference only)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Deployment Size</td>\n<td class=\"tg-tleft-valign-first\">Small (~100s of KB)</td>\n<td class=\"tg-tleft-valign-second\">Large, server framework</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "*   Designed for scalable deployment of TensorFlow models on servers\n*   Models are exposed as REST/gRPC endpoints\n*   Automatically loads, unloads, and versions models\n*   Uses `SavedModel` format, not `.tflite`\n*   Not suitable for offline or embedded deployment\n    \n*   **Use Case Comparison**:\n\nNot suitable for offline or embedded deployment\n\nHere is your formatted table following the provided style:\n\n**Feature**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nTarget Device\n\nMobile/Edge\n\nCloud/Server\n\nModel Format\n\n`.tflite`\n\nSavedModel\n\nCommunication\n\nIn-process / Local\n\ngRPC / REST\n\nLatency\n\nMilliseconds\n\nSub-second to seconds\n\nTraining Support\n\nNo\n\nNo (inference only)\n\nDeployment Size\n\nSmall (~100s of KB)\n\nLarge, server framework\n\n**Feature**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nTarget Device\n\nMobile/Edge\n\nCloud/Server\n\nModel Format\n\n`.tflite`\n\nSavedModel\n\nCommunication\n\nIn-process / Local\n\ngRPC / REST\n\nLatency\n\nMilliseconds\n\nSub-second to seconds\n\nTraining Support\n\nNo\n\nNo (inference only)\n\nDeployment Size\n\nSmall (~100s of KB)\n\nLarge, server framework",
      "order": 48,
      "orderInChapter": 4,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 138,
        "contentLength": 3309
      },
      "nextCards": [
        "ai-ml-runtimes-pros-and-cons-49",
        "ai-ml-runtimes-example-inference-python---tflite-50"
      ],
      "relatedCards": [
        "ai-gpu-architecture-multi-gpu-memory-coherency-31",
        "ai-gpu-architecture-fundamental-architectural-components-amd-32",
        "ai-gpu-architecture-execution-paradigms-amd-33",
        "ai-gpu-architecture-compute-architecture-evolution-amd-cdna-rdna-34",
        "ai-gpu-architecture-memory-architecture-evolution-36"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#tensorflow-serving-(short-overview)",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-pros-and-cons-49",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "title": "Pros and Cons",
      "subtitle": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Pros (TensorFlow Lite):</strong></p>\n\n    <ul>\n      <li>Compact and efficient format (FlatBuffer)</li>\n      <li>Broad hardware delegate support</li>\n      <li>Quantization-aware and post-training optimizations</li>\n      <li>Cross-platform support (iOS, Android, Linux, microcontrollers)</li>\n      <li>Strong ecosystem and pre-trained model zoo (<code class=\"language-plaintext highlighter-rouge\">tflite-model-maker</code>)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons (TensorFlow Lite):</strong></p>\n\n    <ul>\n      <li>Not a full subset of TensorFlow ops (requires op whitelisting or custom ops)</li>\n      <li>Delegate behavior can be opaque and platform-dependent</li>\n      <li>Conversion can fail silently if unsupported ops are encountered</li>\n      <li>Debugging delegate fallbacks can be non-trivial</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Pros (TensorFlow Lite):</strong></p>\n<ul>\n      <li>Compact and efficient format (FlatBuffer)</li>\n      <li>Broad hardware delegate support</li>\n      <li>Quantization-aware and post-training optimizations</li>\n      <li>Cross-platform support (iOS, Android, Linux, microcontrollers)</li>\n      <li>Strong ecosystem and pre-trained model zoo (<code class=\"language-plaintext highlighter-rouge\">tflite-model-maker</code>)</li>\n    </ul>\n<p><strong>Cons (TensorFlow Lite):</strong></p>\n<ul>\n      <li>Not a full subset of TensorFlow ops (requires op whitelisting or custom ops)</li>\n      <li>Delegate behavior can be opaque and platform-dependent</li>\n      <li>Conversion can fail silently if unsupported ops are encountered</li>\n      <li>Debugging delegate fallbacks can be non-trivial</li>\n    </ul>",
      "contentMarkdown": "*   **Pros (TensorFlow Lite):**\n    \n    *   Compact and efficient format (FlatBuffer)\n    *   Broad hardware delegate support\n    *   Quantization-aware and post-training optimizations\n    *   Cross-platform support (iOS, Android, Linux, microcontrollers)\n    *   Strong ecosystem and pre-trained model zoo (`tflite-model-maker`)\n*   **Cons (TensorFlow Lite):**\n    \n    *   Not a full subset of TensorFlow ops (requires op whitelisting or custom ops)\n    *   Delegate behavior can be opaque and platform-dependent\n    *   Conversion can fail silently if unsupported ops are encountered\n    *   Debugging delegate fallbacks can be non-trivial\n\n**Pros (TensorFlow Lite):**\n\n*   Compact and efficient format (FlatBuffer)\n*   Broad hardware delegate support\n*   Quantization-aware and post-training optimizations\n*   Cross-platform support (iOS, Android, Linux, microcontrollers)\n*   Strong ecosystem and pre-trained model zoo (`tflite-model-maker`)\n\n**Cons (TensorFlow Lite):**\n\n*   Not a full subset of TensorFlow ops (requires op whitelisting or custom ops)\n*   Delegate behavior can be opaque and platform-dependent\n*   Conversion can fail silently if unsupported ops are encountered\n*   Debugging delegate fallbacks can be non-trivial",
      "order": 49,
      "orderInChapter": 5,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 154,
        "contentLength": 1688
      },
      "nextCards": [
        "ai-ml-runtimes-example-inference-python---tflite-50",
        "ai-ml-runtimes-suitable-applications-51"
      ],
      "relatedCards": [
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16",
        "ai-model-debugging-hyper-parameter-optimization-hpo-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#pros-and-cons",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-example-inference-python---tflite-50",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "title": "Example Inference (Python - TFLite)",
      "subtitle": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "contentHtml": "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code14\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code14\"><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"c1\"># Load model\n</span><span class=\"n\">interpreter</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">lite</span><span class=\"p\">.</span><span class=\"n\">Interpreter</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"s\">\"mobilenet_v2.tflite\"</span><span class=\"p\">)</span>\n<span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">allocate_tensors</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Prepare input\n</span><span class=\"n\">input_details</span> <span class=\"o\">=</span> <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_input_details</span><span class=\"p\">()</span>\n<span class=\"n\">output_details</span> <span class=\"o\">=</span> <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_output_details</span><span class=\"p\">()</span>\n<span class=\"n\">input_data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">).</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Run inference\n</span><span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">set_tensor</span><span class=\"p\">(</span><span class=\"n\">input_details</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s\">'index'</span><span class=\"p\">],</span> <span class=\"n\">input_data</span><span class=\"p\">)</span>\n<span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">invoke</span><span class=\"p\">()</span>\n<span class=\"n\">output_data</span> <span class=\"o\">=</span> <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_tensor</span><span class=\"p\">(</span><span class=\"n\">output_details</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s\">'index'</span><span class=\"p\">])</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Prediction:\"</span><span class=\"p\">,</span> <span class=\"n\">output_data</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code14\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code14\"><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n\n<span class=\"c1\"># Load model\n</span><span class=\"n\">interpreter</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">lite</span><span class=\"p\">.</span><span class=\"n\">Interpreter</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"s\">\"mobilenet_v2.tflite\"</span><span class=\"p\">)</span>\n<span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">allocate_tensors</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Prepare input\n</span><span class=\"n\">input_details</span> <span class=\"o\">=</span> <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_input_details</span><span class=\"p\">()</span>\n<span class=\"n\">output_details</span> <span class=\"o\">=</span> <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_output_details</span><span class=\"p\">()</span>\n<span class=\"n\">input_data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">224</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">).</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Run inference\n</span><span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">set_tensor</span><span class=\"p\">(</span><span class=\"n\">input_details</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s\">'index'</span><span class=\"p\">],</span> <span class=\"n\">input_data</span><span class=\"p\">)</span>\n<span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">invoke</span><span class=\"p\">()</span>\n<span class=\"n\">output_data</span> <span class=\"o\">=</span> <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_tensor</span><span class=\"p\">(</span><span class=\"n\">output_details</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"s\">'index'</span><span class=\"p\">])</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">\"Prediction:\"</span><span class=\"p\">,</span> <span class=\"n\">output_data</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li><strong>Delegate usage (Android NNAPI, example via Java/Kotlin):</strong></li>\n</ul>\n<div class=\"language-java highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code15\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code15\"><span class=\"nc\">Interpreter</span><span class=\"o\">.</span><span class=\"na\">Options</span> <span class=\"n\">options</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nc\">Interpreter</span><span class=\"o\">.</span><span class=\"na\">Options</span><span class=\"o\">();</span>\n<span class=\"n\">options</span><span class=\"o\">.</span><span class=\"na\">addDelegate</span><span class=\"o\">(</span><span class=\"k\">new</span> <span class=\"nc\">NnApiDelegate</span><span class=\"o\">());</span>\n<span class=\"nc\">Interpreter</span> <span class=\"n\">interpreter</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nc\">Interpreter</span><span class=\"o\">(</span><span class=\"n\">tfliteModel</span><span class=\"o\">,</span> <span class=\"n\">options</span><span class=\"o\">);</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code15\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code15\"><span class=\"nc\">Interpreter</span><span class=\"o\">.</span><span class=\"na\">Options</span> <span class=\"n\">options</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nc\">Interpreter</span><span class=\"o\">.</span><span class=\"na\">Options</span><span class=\"o\">();</span>\n<span class=\"n\">options</span><span class=\"o\">.</span><span class=\"na\">addDelegate</span><span class=\"o\">(</span><span class=\"k\">new</span> <span class=\"nc\">NnApiDelegate</span><span class=\"o\">());</span>\n<span class=\"nc\">Interpreter</span> <span class=\"n\">interpreter</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nc\">Interpreter</span><span class=\"o\">(</span><span class=\"n\">tfliteModel</span><span class=\"o\">,</span> <span class=\"n\">options</span><span class=\"o\">);</span>\n</code></pre>",
      "contentMarkdown": "![](https://aman.ai/images/copy.png)\n\n`import tensorflow as tf import numpy as np  # Load model interpreter = tf.lite.Interpreter(model_path=\"mobilenet_v2.tflite\") interpreter.allocate_tensors()  # Prepare input input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() input_data = np.random.rand(1, 224, 224, 3).astype(np.float32)  # Run inference interpreter.set_tensor(input_details[0]['index'], input_data) interpreter.invoke() output_data = interpreter.get_tensor(output_details[0]['index']) print(\"Prediction:\", output_data)`\n\n![](https://aman.ai/images/copy.png)\n\n`import tensorflow as tf import numpy as np  # Load model interpreter = tf.lite.Interpreter(model_path=\"mobilenet_v2.tflite\") interpreter.allocate_tensors()  # Prepare input input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() input_data = np.random.rand(1, 224, 224, 3).astype(np.float32)  # Run inference interpreter.set_tensor(input_details[0]['index'], input_data) interpreter.invoke() output_data = interpreter.get_tensor(output_details[0]['index']) print(\"Prediction:\", output_data)`\n\n*   **Delegate usage (Android NNAPI, example via Java/Kotlin):**\n\n![](https://aman.ai/images/copy.png)\n\n`Interpreter.Options options = new Interpreter.Options(); options.addDelegate(new NnApiDelegate()); Interpreter interpreter = new Interpreter(tfliteModel, options);`\n\n![](https://aman.ai/images/copy.png)\n\n`Interpreter.Options options = new Interpreter.Options(); options.addDelegate(new NnApiDelegate()); Interpreter interpreter = new Interpreter(tfliteModel, options);`",
      "order": 50,
      "orderInChapter": 6,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 120,
        "contentLength": 8461
      },
      "nextCards": [
        "ai-ml-runtimes-suitable-applications-51",
        "ai-ml-runtimes-comparative-analysis-52"
      ],
      "relatedCards": [
        "ai-gpu-architecture-multi-gpu-memory-coherency-31",
        "ai-gpu-architecture-fundamental-architectural-components-amd-32",
        "ai-gpu-architecture-execution-paradigms-amd-33",
        "ai-gpu-architecture-compute-architecture-evolution-amd-cdna-rdna-34",
        "ai-gpu-architecture-memory-architecture-evolution-36"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#example-inference-(python---tflite)",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-suitable-applications-51",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "title": "Suitable Applications",
      "subtitle": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "contentHtml": "<ul>\n  <li>On-device health and fitness apps</li>\n  <li>Real-time object detection in AR</li>\n  <li>Offline voice recognition</li>\n  <li>Edge anomaly detection</li>\n  <li>\n    <p>TinyML deployments with <code class=\"language-plaintext highlighter-rouge\">TensorFlow Lite for Microcontrollers</code></p>\n  </li>\n  <li>TensorFlow Lite remains one of the most production-hardened and flexible runtimes for on-device ML, particularly in mobile and embedded contexts. Its support for multiple delegates and optimizations makes it a go-to choice for developers deploying models outside the cloud.</li>\n</ul>\n<p>TinyML deployments with <code class=\"language-plaintext highlighter-rouge\">TensorFlow Lite for Microcontrollers</code></p>",
      "contentMarkdown": "*   On-device health and fitness apps\n*   Real-time object detection in AR\n*   Offline voice recognition\n*   Edge anomaly detection\n*   TinyML deployments with `TensorFlow Lite for Microcontrollers`\n    \n*   TensorFlow Lite remains one of the most production-hardened and flexible runtimes for on-device ML, particularly in mobile and embedded contexts. Its support for multiple delegates and optimizations makes it a go-to choice for developers deploying models outside the cloud.\n\nTinyML deployments with `TensorFlow Lite for Microcontrollers`",
      "order": 51,
      "orderInChapter": 7,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 75,
        "contentLength": 726
      },
      "nextCards": [
        "ai-ml-runtimes-comparative-analysis-52",
        "ai-ml-runtimes-comparative-summary-and-guidance-53"
      ],
      "relatedCards": [
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16",
        "ai-model-debugging-hyper-parameter-optimization-hpo-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#suitable-applications",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-comparative-analysis-52",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "title": "Comparative Analysis",
      "subtitle": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "contentHtml": "<ul>\n  <li>Here are detailed tabular comparisons that encapsulates all key aspects across the different on-device ML runtimes discussed in the primer.</li>\n</ul>\n<h4 id=\"general-characteristics\">General Characteristics</h4>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Target Platform(s)</td>\n<td class=\"tg-tleft-valign-first\">NVIDIA Jetson, Desktop, Server</td>\n<td class=\"tg-tleft-valign-first\">Apple devices (iOS/macOS)</td>\n<td class=\"tg-tleft-valign-first\">Apple Silicon (macOS only)</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform</td>\n<td class=\"tg-tleft-valign-first\">Embedded, mobile, MCU</td>\n<td class=\"tg-tleft-valign-first\">Robotics, automotive, ADAS</td>\n<td class=\"tg-tleft-valign-first\">Desktop, mobile, browser</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform (mobile/edge)</td>\n<td class=\"tg-tleft-valign-second\">Cloud / server environments</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">ML Task Focus</td>\n<td class=\"tg-tleft-valign-first\">Optimized inference</td>\n<td class=\"tg-tleft-valign-first\">General ML (vision, NLP)</td>\n<td class=\"tg-tleft-valign-first\">Research, transformer/NLP</td>\n<td class=\"tg-tleft-valign-first\">General ML</td>\n<td class=\"tg-tleft-valign-first\">Ultra-light inference</td>\n<td class=\"tg-tleft-valign-first\">3D spatial perception</td>\n<td class=\"tg-tleft-valign-first\">Large language model inference</td>\n<td class=\"tg-tleft-valign-first\">General ML</td>\n<td class=\"tg-tleft-valign-second\">Scalable inference serving</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Inference Only?</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">No (supports training)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Open Source?</td>\n<td class=\"tg-tleft-valign-first\">Partially (binaries open, tools closed)</td>\n<td class=\"tg-tleft-valign-first\">Partially (via tools)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Partially / variable</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Target Platform(s)</td>\n<td class=\"tg-tleft-valign-first\">NVIDIA Jetson, Desktop, Server</td>\n<td class=\"tg-tleft-valign-first\">Apple devices (iOS/macOS)</td>\n<td class=\"tg-tleft-valign-first\">Apple Silicon (macOS only)</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform</td>\n<td class=\"tg-tleft-valign-first\">Embedded, mobile, MCU</td>\n<td class=\"tg-tleft-valign-first\">Robotics, automotive, ADAS</td>\n<td class=\"tg-tleft-valign-first\">Desktop, mobile, browser</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform (mobile/edge)</td>\n<td class=\"tg-tleft-valign-second\">Cloud / server environments</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">ML Task Focus</td>\n<td class=\"tg-tleft-valign-first\">Optimized inference</td>\n<td class=\"tg-tleft-valign-first\">General ML (vision, NLP)</td>\n<td class=\"tg-tleft-valign-first\">Research, transformer/NLP</td>\n<td class=\"tg-tleft-valign-first\">General ML</td>\n<td class=\"tg-tleft-valign-first\">Ultra-light inference</td>\n<td class=\"tg-tleft-valign-first\">3D spatial perception</td>\n<td class=\"tg-tleft-valign-first\">Large language model inference</td>\n<td class=\"tg-tleft-valign-first\">General ML</td>\n<td class=\"tg-tleft-valign-second\">Scalable inference serving</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Inference Only?</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">No (supports training)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Open Source?</td>\n<td class=\"tg-tleft-valign-first\">Partially (binaries open, tools closed)</td>\n<td class=\"tg-tleft-valign-first\">Partially (via tools)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Partially / variable</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"model-formats-and-conversion\">Model Formats and Conversion</h4>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Primary Format</td>\n<td class=\"tg-tleft-valign-first\">.plan (TensorRT engine file)</td>\n<td class=\"tg-tleft-valign-first\">.mlmodelc</td>\n<td class=\"tg-tleft-valign-first\">Python-defined layers</td>\n<td class=\"tg-tleft-valign-first\">.onnx</td>\n<td class=\"tg-tleft-valign-first\">.ptc (compiled TorchScript)</td>\n<td class=\"tg-tleft-valign-first\">Custom / converted .onnx / raw tensors</td>\n<td class=\"tg-tleft-valign-first\">.gguf (quantized LLMs)</td>\n<td class=\"tg-tleft-valign-first\">.tflite (FlatBuffer)</td>\n<td class=\"tg-tleft-valign-second\">SavedModel (.pb, .pbtxt)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Supported Frameworks</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, ONNX</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, TF (via converters)</td>\n<td class=\"tg-tleft-valign-first\">Native Python API</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, TensorFlow, others</td>\n<td class=\"tg-tleft-valign-first\">PyTorch (TorchScript subset)</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, TensorFlow (via export)</td>\n<td class=\"tg-tleft-valign-first\">LLaMA-family only</td>\n<td class=\"tg-tleft-valign-first\">TensorFlow, Keras</td>\n<td class=\"tg-tleft-valign-second\">TensorFlow only</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Conversion Required?</td>\n<td class=\"tg-tleft-valign-first\">Yes (from ONNX or PyTorch export)</td>\n<td class=\"tg-tleft-valign-first\">Yes (via coremltools)</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">Yes (usually from PyTorch)</td>\n<td class=\"tg-tleft-valign-first\">Yes (via AOT compiler)</td>\n<td class=\"tg-tleft-valign-first\">Yes, often includes preprocessing</td>\n<td class=\"tg-tleft-valign-first\">Yes (convert + quantize)</td>\n<td class=\"tg-tleft-valign-first\">Yes (TFLiteConverter)</td>\n<td class=\"tg-tleft-valign-second\">No (already in target format)</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Primary Format</td>\n<td class=\"tg-tleft-valign-first\">.plan (TensorRT engine file)</td>\n<td class=\"tg-tleft-valign-first\">.mlmodelc</td>\n<td class=\"tg-tleft-valign-first\">Python-defined layers</td>\n<td class=\"tg-tleft-valign-first\">.onnx</td>\n<td class=\"tg-tleft-valign-first\">.ptc (compiled TorchScript)</td>\n<td class=\"tg-tleft-valign-first\">Custom / converted .onnx / raw tensors</td>\n<td class=\"tg-tleft-valign-first\">.gguf (quantized LLMs)</td>\n<td class=\"tg-tleft-valign-first\">.tflite (FlatBuffer)</td>\n<td class=\"tg-tleft-valign-second\">SavedModel (.pb, .pbtxt)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Supported Frameworks</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, ONNX</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, TF (via converters)</td>\n<td class=\"tg-tleft-valign-first\">Native Python API</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, TensorFlow, others</td>\n<td class=\"tg-tleft-valign-first\">PyTorch (TorchScript subset)</td>\n<td class=\"tg-tleft-valign-first\">PyTorch, TensorFlow (via export)</td>\n<td class=\"tg-tleft-valign-first\">LLaMA-family only</td>\n<td class=\"tg-tleft-valign-first\">TensorFlow, Keras</td>\n<td class=\"tg-tleft-valign-second\">TensorFlow only</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Conversion Required?</td>\n<td class=\"tg-tleft-valign-first\">Yes (from ONNX or PyTorch export)</td>\n<td class=\"tg-tleft-valign-first\">Yes (via coremltools)</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">Yes (usually from PyTorch)</td>\n<td class=\"tg-tleft-valign-first\">Yes (via AOT compiler)</td>\n<td class=\"tg-tleft-valign-first\">Yes, often includes preprocessing</td>\n<td class=\"tg-tleft-valign-first\">Yes (convert + quantize)</td>\n<td class=\"tg-tleft-valign-first\">Yes (TFLiteConverter)</td>\n<td class=\"tg-tleft-valign-second\">No (already in target format)</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"execution-model-and-hardware-support\">Execution Model and Hardware Support</h4>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Execution Type</td>\n<td class=\"tg-tleft-valign-first\">AOT compiled CUDA graph</td>\n<td class=\"tg-tleft-valign-first\">Eager, dynamic hardware assignment</td>\n<td class=\"tg-tleft-valign-first\">Eager + compiled graph</td>\n<td class=\"tg-tleft-valign-first\">Static graph with runtime optimizations</td>\n<td class=\"tg-tleft-valign-first\">Bytecode VM interpreter</td>\n<td class=\"tg-tleft-valign-first\">Sparse 3D graph + temporal flow</td>\n<td class=\"tg-tleft-valign-first\">Manual loop over transformer layers</td>\n<td class=\"tg-tleft-valign-first\">Static interpreter + delegates</td>\n<td class=\"tg-tleft-valign-second\">REST/gRPC inference pipeline</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">CPU Support</td>\n<td class=\"tg-tleft-valign-first\">No (GPU only)</td>\n<td class=\"tg-tleft-valign-first\">Yes (fallback)</td>\n<td class=\"tg-tleft-valign-first\">Yes (M1/M2 optimized)</td>\n<td class=\"tg-tleft-valign-first\">Yes (default EP)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes (highly optimized)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">GPU Support</td>\n<td class=\"tg-tleft-valign-first\">Yes (CUDA, Tensor Cores)</td>\n<td class=\"tg-tleft-valign-first\">Yes (Metal)</td>\n<td class=\"tg-tleft-valign-first\">Yes (via MPS)</td>\n<td class=\"tg-tleft-valign-first\">Yes (CUDA, DirectML, etc.)</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-first\">Yes (CUDA, embedded GPUs)</td>\n<td class=\"tg-tleft-valign-first\">Optional (Metal, CUDA, OpenCL)</td>\n<td class=\"tg-tleft-valign-first\">Yes (OpenCL, Metal)</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">NPU / DSP Support</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">Yes (Apple ANE)</td>\n<td class=\"tg-tleft-valign-first\">Emerging ANE support</td>\n<td class=\"tg-tleft-valign-first\">Yes (via NNAPI, OpenVINO, etc.)</td>\n<td class=\"tg-tleft-valign-first\">Potential via backend interface</td>\n<td class=\"tg-tleft-valign-first\">Yes (TI, Nvidia, ADAS accelerators)</td>\n<td class=\"tg-tleft-valign-first\">No (LLM-focused, CPU-oriented)</td>\n<td class=\"tg-tleft-valign-first\">Yes (NNAPI, EdgeTPU, Hexagon)</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Hardware Abstraction</td>\n<td class=\"tg-tleft-valign-first\">Low-level plugin engine, manual tuning</td>\n<td class=\"tg-tleft-valign-first\">Automatic</td>\n<td class=\"tg-tleft-valign-first\">Manual tuning via MLX</td>\n<td class=\"tg-tleft-valign-first\">Modular Execution Providers (EPs)</td>\n<td class=\"tg-tleft-valign-first\">Compiled dispatcher with targets</td>\n<td class=\"tg-tleft-valign-first\">Device-specific optimization required</td>\n<td class=\"tg-tleft-valign-first\">Low-level SIMD/CUDA offload</td>\n<td class=\"tg-tleft-valign-first\">Delegate-based (pluggable)</td>\n<td class=\"tg-tleft-valign-second\">N/A</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Execution Type</td>\n<td class=\"tg-tleft-valign-first\">AOT compiled CUDA graph</td>\n<td class=\"tg-tleft-valign-first\">Eager, dynamic hardware assignment</td>\n<td class=\"tg-tleft-valign-first\">Eager + compiled graph</td>\n<td class=\"tg-tleft-valign-first\">Static graph with runtime optimizations</td>\n<td class=\"tg-tleft-valign-first\">Bytecode VM interpreter</td>\n<td class=\"tg-tleft-valign-first\">Sparse 3D graph + temporal flow</td>\n<td class=\"tg-tleft-valign-first\">Manual loop over transformer layers</td>\n<td class=\"tg-tleft-valign-first\">Static interpreter + delegates</td>\n<td class=\"tg-tleft-valign-second\">REST/gRPC inference pipeline</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">CPU Support</td>\n<td class=\"tg-tleft-valign-first\">No (GPU only)</td>\n<td class=\"tg-tleft-valign-first\">Yes (fallback)</td>\n<td class=\"tg-tleft-valign-first\">Yes (M1/M2 optimized)</td>\n<td class=\"tg-tleft-valign-first\">Yes (default EP)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-first\">Yes (highly optimized)</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">GPU Support</td>\n<td class=\"tg-tleft-valign-first\">Yes (CUDA, Tensor Cores)</td>\n<td class=\"tg-tleft-valign-first\">Yes (Metal)</td>\n<td class=\"tg-tleft-valign-first\">Yes (via MPS)</td>\n<td class=\"tg-tleft-valign-first\">Yes (CUDA, DirectML, etc.)</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-first\">Yes (CUDA, embedded GPUs)</td>\n<td class=\"tg-tleft-valign-first\">Optional (Metal, CUDA, OpenCL)</td>\n<td class=\"tg-tleft-valign-first\">Yes (OpenCL, Metal)</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">NPU / DSP Support</td>\n<td class=\"tg-tleft-valign-first\">No</td>\n<td class=\"tg-tleft-valign-first\">Yes (Apple ANE)</td>\n<td class=\"tg-tleft-valign-first\">Emerging ANE support</td>\n<td class=\"tg-tleft-valign-first\">Yes (via NNAPI, OpenVINO, etc.)</td>\n<td class=\"tg-tleft-valign-first\">Potential via backend interface</td>\n<td class=\"tg-tleft-valign-first\">Yes (TI, Nvidia, ADAS accelerators)</td>\n<td class=\"tg-tleft-valign-first\">No (LLM-focused, CPU-oriented)</td>\n<td class=\"tg-tleft-valign-first\">Yes (NNAPI, EdgeTPU, Hexagon)</td>\n<td class=\"tg-tleft-valign-second\">No</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Hardware Abstraction</td>\n<td class=\"tg-tleft-valign-first\">Low-level plugin engine, manual tuning</td>\n<td class=\"tg-tleft-valign-first\">Automatic</td>\n<td class=\"tg-tleft-valign-first\">Manual tuning via MLX</td>\n<td class=\"tg-tleft-valign-first\">Modular Execution Providers (EPs)</td>\n<td class=\"tg-tleft-valign-first\">Compiled dispatcher with targets</td>\n<td class=\"tg-tleft-valign-first\">Device-specific optimization required</td>\n<td class=\"tg-tleft-valign-first\">Low-level SIMD/CUDA offload</td>\n<td class=\"tg-tleft-valign-first\">Delegate-based (pluggable)</td>\n<td class=\"tg-tleft-valign-second\">N/A</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"optimization-size-and-constraints\">Optimization, Size, and Constraints</h4>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Model Optimization Support</td>\n<td class=\"tg-tleft-valign-first\">Yes (kernel tuning, quantization, `float16`/`int8`)</td>\n<td class=\"tg-tleft-valign-first\">Yes (ANE targeting, quantization)</td>\n<td class=\"tg-tleft-valign-first\">No built-in, manual scripting</td>\n<td class=\"tg-tleft-valign-first\">Yes (quantization, pruning, graph fusion)</td>\n<td class=\"tg-tleft-valign-first\">Yes (operator pruning, bytecode fusion)</td>\n<td class=\"tg-tleft-valign-first\">Yes (3D-aware compression and fusions)</td>\n<td class=\"tg-tleft-valign-first\">Yes (quantized GGUF)</td>\n<td class=\"tg-tleft-valign-first\">Yes (quantization, fusion)</td>\n<td class=\"tg-tleft-valign-second\">Yes (batching, threading)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Runtime Size</td>\n<td class=\"tg-tleft-valign-first\">Medium (~5–15 MB)</td>\n<td class=\"tg-tleft-valign-first\">Medium (~5–10 MB)</td>\n<td class=\"tg-tleft-valign-first\">Medium</td>\n<td class=\"tg-tleft-valign-first\">Large (5–30 MB)</td>\n<td class=\"tg-tleft-valign-first\">Very small (&lt;1 MB)</td>\n<td class=\"tg-tleft-valign-first\">Medium–Large</td>\n<td class=\"tg-tleft-valign-first\">Small–Medium</td>\n<td class=\"tg-tleft-valign-first\">Small (~0.5–5 MB)</td>\n<td class=\"tg-tleft-valign-second\">Very large (&gt;100 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Memory Footprint (Inference)</td>\n<td class=\"tg-tleft-valign-first\">Low to moderate (GPU memory bound)</td>\n<td class=\"tg-tleft-valign-first\">Low to moderate</td>\n<td class=\"tg-tleft-valign-first\">Moderate (GPU-heavy)</td>\n<td class=\"tg-tleft-valign-first\">Variable (depends on EPs)</td>\n<td class=\"tg-tleft-valign-first\">Ultra-low (sub-MB possible)</td>\n<td class=\"tg-tleft-valign-first\">High (large point cloud buffers)</td>\n<td class=\"tg-tleft-valign-first\">Low (~3–6 GB RAM for 7B models)</td>\n<td class=\"tg-tleft-valign-first\">Low</td>\n<td class=\"tg-tleft-valign-second\">High</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Latency</td>\n<td class=\"tg-tleft-valign-first\">Very low (sub-ms possible)</td>\n<td class=\"tg-tleft-valign-first\">Low (with ANE/GPU)</td>\n<td class=\"tg-tleft-valign-first\">Medium (eager mode)</td>\n<td class=\"tg-tleft-valign-first\">Variable (highly EP dependent)</td>\n<td class=\"tg-tleft-valign-first\">Very low</td>\n<td class=\"tg-tleft-valign-first\">Moderate to high (depends on density)</td>\n<td class=\"tg-tleft-valign-first\">Low (for small LLMs)</td>\n<td class=\"tg-tleft-valign-first\">Low (under 10ms typical)</td>\n<td class=\"tg-tleft-valign-second\">Moderate to high</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Model Optimization Support</td>\n<td class=\"tg-tleft-valign-first\">Yes (kernel tuning, quantization, `float16`/`int8`)</td>\n<td class=\"tg-tleft-valign-first\">Yes (ANE targeting, quantization)</td>\n<td class=\"tg-tleft-valign-first\">No built-in, manual scripting</td>\n<td class=\"tg-tleft-valign-first\">Yes (quantization, pruning, graph fusion)</td>\n<td class=\"tg-tleft-valign-first\">Yes (operator pruning, bytecode fusion)</td>\n<td class=\"tg-tleft-valign-first\">Yes (3D-aware compression and fusions)</td>\n<td class=\"tg-tleft-valign-first\">Yes (quantized GGUF)</td>\n<td class=\"tg-tleft-valign-first\">Yes (quantization, fusion)</td>\n<td class=\"tg-tleft-valign-second\">Yes (batching, threading)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Runtime Size</td>\n<td class=\"tg-tleft-valign-first\">Medium (~5–15 MB)</td>\n<td class=\"tg-tleft-valign-first\">Medium (~5–10 MB)</td>\n<td class=\"tg-tleft-valign-first\">Medium</td>\n<td class=\"tg-tleft-valign-first\">Large (5–30 MB)</td>\n<td class=\"tg-tleft-valign-first\">Very small (&lt;1 MB)</td>\n<td class=\"tg-tleft-valign-first\">Medium–Large</td>\n<td class=\"tg-tleft-valign-first\">Small–Medium</td>\n<td class=\"tg-tleft-valign-first\">Small (~0.5–5 MB)</td>\n<td class=\"tg-tleft-valign-second\">Very large (&gt;100 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Memory Footprint (Inference)</td>\n<td class=\"tg-tleft-valign-first\">Low to moderate (GPU memory bound)</td>\n<td class=\"tg-tleft-valign-first\">Low to moderate</td>\n<td class=\"tg-tleft-valign-first\">Moderate (GPU-heavy)</td>\n<td class=\"tg-tleft-valign-first\">Variable (depends on EPs)</td>\n<td class=\"tg-tleft-valign-first\">Ultra-low (sub-MB possible)</td>\n<td class=\"tg-tleft-valign-first\">High (large point cloud buffers)</td>\n<td class=\"tg-tleft-valign-first\">Low (~3–6 GB RAM for 7B models)</td>\n<td class=\"tg-tleft-valign-first\">Low</td>\n<td class=\"tg-tleft-valign-second\">High</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Latency</td>\n<td class=\"tg-tleft-valign-first\">Very low (sub-ms possible)</td>\n<td class=\"tg-tleft-valign-first\">Low (with ANE/GPU)</td>\n<td class=\"tg-tleft-valign-first\">Medium (eager mode)</td>\n<td class=\"tg-tleft-valign-first\">Variable (highly EP dependent)</td>\n<td class=\"tg-tleft-valign-first\">Very low</td>\n<td class=\"tg-tleft-valign-first\">Moderate to high (depends on density)</td>\n<td class=\"tg-tleft-valign-first\">Low (for small LLMs)</td>\n<td class=\"tg-tleft-valign-first\">Low (under 10ms typical)</td>\n<td class=\"tg-tleft-valign-second\">Moderate to high</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"flexibility-debugging-and-ecosystem\">Flexibility, Debugging, and Ecosystem</h4>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Custom Ops Support</td>\n<td class=\"tg-tleft-valign-first\">Yes (via plugin library API)</td>\n<td class=\"tg-tleft-valign-first\">Limited (via <code>MLCustomModel</code>)</td>\n<td class=\"tg-tleft-valign-first\">Full (via Python subclassing)</td>\n<td class=\"tg-tleft-valign-first\">Yes (custom EPs and ops)</td>\n<td class=\"tg-tleft-valign-first\">Yes (C++ op authoring)</td>\n<td class=\"tg-tleft-valign-first\">Yes (often required)</td>\n<td class=\"tg-tleft-valign-first\">No (fixed transformer kernel set)</td>\n<td class=\"tg-tleft-valign-first\">Yes (C++/C custom kernels)</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Community &amp; Documentation</td>\n<td class=\"tg-tleft-valign-first\">Strong NVIDIA developer support, active forums</td>\n<td class=\"tg-tleft-valign-first\">Strong, Apple developer-centric</td>\n<td class=\"tg-tleft-valign-first\">Niche, growing</td>\n<td class=\"tg-tleft-valign-first\">Very strong</td>\n<td class=\"tg-tleft-valign-first\">Growing (Meta-sponsored)</td>\n<td class=\"tg-tleft-valign-first\">Limited / hardware-vendor specific</td>\n<td class=\"tg-tleft-valign-first\">Active open-source base</td>\n<td class=\"tg-tleft-valign-first\">Mature, large community</td>\n<td class=\"tg-tleft-valign-second\">Very mature in production</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Debugger Support</td>\n<td class=\"tg-tleft-valign-first\">Nsight Systems, profiling tools, verbose logging</td>\n<td class=\"tg-tleft-valign-first\">Xcode tools</td>\n<td class=\"tg-tleft-valign-first\">Python debug console</td>\n<td class=\"tg-tleft-valign-first\">Moderate (model inspection tools)</td>\n<td class=\"tg-tleft-valign-first\">Minimal (CLI, log-based)</td>\n<td class=\"tg-tleft-valign-first\">Custom tooling per device</td>\n<td class=\"tg-tleft-valign-first\">Log-level output only</td>\n<td class=\"tg-tleft-valign-first\">TensorBoard-lite, CLI tools</td>\n<td class=\"tg-tleft-valign-second\">Monitoring via Prometheus, etc.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ease of Use</td>\n<td class=\"tg-tleft-valign-first\">Medium (manual optimization, engine building)</td>\n<td class=\"tg-tleft-valign-first\">High for Apple developers</td>\n<td class=\"tg-tleft-valign-first\">Medium (researchers, tinkerers)</td>\n<td class=\"tg-tleft-valign-first\">Moderate to high (depends on EP)</td>\n<td class=\"tg-tleft-valign-first\">Medium (steep setup curve)</td>\n<td class=\"tg-tleft-valign-first\">Low (requires system integration)</td>\n<td class=\"tg-tleft-valign-first\">High (once model is quantized)</td>\n<td class=\"tg-tleft-valign-first\">High (especially with <code>model maker</code>)</td>\n<td class=\"tg-tleft-valign-second\">Medium to high (requires infra)</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorRT</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Core ML</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>MLX</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ONNX Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>ExecuTorch</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>LidarTLM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong><code>llama.cpp</code></strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>TensorFlow Lite</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>TensorFlow Serving</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Custom Ops Support</td>\n<td class=\"tg-tleft-valign-first\">Yes (via plugin library API)</td>\n<td class=\"tg-tleft-valign-first\">Limited (via <code>MLCustomModel</code>)</td>\n<td class=\"tg-tleft-valign-first\">Full (via Python subclassing)</td>\n<td class=\"tg-tleft-valign-first\">Yes (custom EPs and ops)</td>\n<td class=\"tg-tleft-valign-first\">Yes (C++ op authoring)</td>\n<td class=\"tg-tleft-valign-first\">Yes (often required)</td>\n<td class=\"tg-tleft-valign-first\">No (fixed transformer kernel set)</td>\n<td class=\"tg-tleft-valign-first\">Yes (C++/C custom kernels)</td>\n<td class=\"tg-tleft-valign-second\">Yes</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Community &amp; Documentation</td>\n<td class=\"tg-tleft-valign-first\">Strong NVIDIA developer support, active forums</td>\n<td class=\"tg-tleft-valign-first\">Strong, Apple developer-centric</td>\n<td class=\"tg-tleft-valign-first\">Niche, growing</td>\n<td class=\"tg-tleft-valign-first\">Very strong</td>\n<td class=\"tg-tleft-valign-first\">Growing (Meta-sponsored)</td>\n<td class=\"tg-tleft-valign-first\">Limited / hardware-vendor specific</td>\n<td class=\"tg-tleft-valign-first\">Active open-source base</td>\n<td class=\"tg-tleft-valign-first\">Mature, large community</td>\n<td class=\"tg-tleft-valign-second\">Very mature in production</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Debugger Support</td>\n<td class=\"tg-tleft-valign-first\">Nsight Systems, profiling tools, verbose logging</td>\n<td class=\"tg-tleft-valign-first\">Xcode tools</td>\n<td class=\"tg-tleft-valign-first\">Python debug console</td>\n<td class=\"tg-tleft-valign-first\">Moderate (model inspection tools)</td>\n<td class=\"tg-tleft-valign-first\">Minimal (CLI, log-based)</td>\n<td class=\"tg-tleft-valign-first\">Custom tooling per device</td>\n<td class=\"tg-tleft-valign-first\">Log-level output only</td>\n<td class=\"tg-tleft-valign-first\">TensorBoard-lite, CLI tools</td>\n<td class=\"tg-tleft-valign-second\">Monitoring via Prometheus, etc.</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Ease of Use</td>\n<td class=\"tg-tleft-valign-first\">Medium (manual optimization, engine building)</td>\n<td class=\"tg-tleft-valign-first\">High for Apple developers</td>\n<td class=\"tg-tleft-valign-first\">Medium (researchers, tinkerers)</td>\n<td class=\"tg-tleft-valign-first\">Moderate to high (depends on EP)</td>\n<td class=\"tg-tleft-valign-first\">Medium (steep setup curve)</td>\n<td class=\"tg-tleft-valign-first\">Low (requires system integration)</td>\n<td class=\"tg-tleft-valign-first\">High (once model is quantized)</td>\n<td class=\"tg-tleft-valign-first\">High (especially with <code>model maker</code>)</td>\n<td class=\"tg-tleft-valign-second\">Medium to high (requires infra)</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "*   Here are detailed tabular comparisons that encapsulates all key aspects across the different on-device ML runtimes discussed in the primer.\n\n#### General Characteristics\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nTarget Platform(s)\n\nNVIDIA Jetson, Desktop, Server\n\nApple devices (iOS/macOS)\n\nApple Silicon (macOS only)\n\nCross-platform\n\nEmbedded, mobile, MCU\n\nRobotics, automotive, ADAS\n\nDesktop, mobile, browser\n\nCross-platform (mobile/edge)\n\nCloud / server environments\n\nML Task Focus\n\nOptimized inference\n\nGeneral ML (vision, NLP)\n\nResearch, transformer/NLP\n\nGeneral ML\n\nUltra-light inference\n\n3D spatial perception\n\nLarge language model inference\n\nGeneral ML\n\nScalable inference serving\n\nInference Only?\n\nYes\n\nYes\n\nNo (supports training)\n\nYes\n\nYes\n\nYes\n\nYes\n\nYes\n\nYes\n\nOpen Source?\n\nPartially (binaries open, tools closed)\n\nPartially (via tools)\n\nYes\n\nYes\n\nYes\n\nPartially / variable\n\nYes\n\nYes\n\nYes\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nTarget Platform(s)\n\nNVIDIA Jetson, Desktop, Server\n\nApple devices (iOS/macOS)\n\nApple Silicon (macOS only)\n\nCross-platform\n\nEmbedded, mobile, MCU\n\nRobotics, automotive, ADAS\n\nDesktop, mobile, browser\n\nCross-platform (mobile/edge)\n\nCloud / server environments\n\nML Task Focus\n\nOptimized inference\n\nGeneral ML (vision, NLP)\n\nResearch, transformer/NLP\n\nGeneral ML\n\nUltra-light inference\n\n3D spatial perception\n\nLarge language model inference\n\nGeneral ML\n\nScalable inference serving\n\nInference Only?\n\nYes\n\nYes\n\nNo (supports training)\n\nYes\n\nYes\n\nYes\n\nYes\n\nYes\n\nYes\n\nOpen Source?\n\nPartially (binaries open, tools closed)\n\nPartially (via tools)\n\nYes\n\nYes\n\nYes\n\nPartially / variable\n\nYes\n\nYes\n\nYes\n\n#### Model Formats and Conversion\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nPrimary Format\n\n.plan (TensorRT engine file)\n\n.mlmodelc\n\nPython-defined layers\n\n.onnx\n\n.ptc (compiled TorchScript)\n\nCustom / converted .onnx / raw tensors\n\n.gguf (quantized LLMs)\n\n.tflite (FlatBuffer)\n\nSavedModel (.pb, .pbtxt)\n\nSupported Frameworks\n\nPyTorch, ONNX\n\nPyTorch, TF (via converters)\n\nNative Python API\n\nPyTorch, TensorFlow, others\n\nPyTorch (TorchScript subset)\n\nPyTorch, TensorFlow (via export)\n\nLLaMA-family only\n\nTensorFlow, Keras\n\nTensorFlow only\n\nConversion Required?\n\nYes (from ONNX or PyTorch export)\n\nYes (via coremltools)\n\nNo\n\nYes (usually from PyTorch)\n\nYes (via AOT compiler)\n\nYes, often includes preprocessing\n\nYes (convert + quantize)\n\nYes (TFLiteConverter)\n\nNo (already in target format)\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nPrimary Format\n\n.plan (TensorRT engine file)\n\n.mlmodelc\n\nPython-defined layers\n\n.onnx\n\n.ptc (compiled TorchScript)\n\nCustom / converted .onnx / raw tensors\n\n.gguf (quantized LLMs)\n\n.tflite (FlatBuffer)\n\nSavedModel (.pb, .pbtxt)\n\nSupported Frameworks\n\nPyTorch, ONNX\n\nPyTorch, TF (via converters)\n\nNative Python API\n\nPyTorch, TensorFlow, others\n\nPyTorch (TorchScript subset)\n\nPyTorch, TensorFlow (via export)\n\nLLaMA-family only\n\nTensorFlow, Keras\n\nTensorFlow only\n\nConversion Required?\n\nYes (from ONNX or PyTorch export)\n\nYes (via coremltools)\n\nNo\n\nYes (usually from PyTorch)\n\nYes (via AOT compiler)\n\nYes, often includes preprocessing\n\nYes (convert + quantize)\n\nYes (TFLiteConverter)\n\nNo (already in target format)\n\n#### Execution Model and Hardware Support\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nExecution Type\n\nAOT compiled CUDA graph\n\nEager, dynamic hardware assignment\n\nEager + compiled graph\n\nStatic graph with runtime optimizations\n\nBytecode VM interpreter\n\nSparse 3D graph + temporal flow\n\nManual loop over transformer layers\n\nStatic interpreter + delegates\n\nREST/gRPC inference pipeline\n\nCPU Support\n\nNo (GPU only)\n\nYes (fallback)\n\nYes (M1/M2 optimized)\n\nYes (default EP)\n\nYes\n\nYes\n\nYes (highly optimized)\n\nYes\n\nYes\n\nGPU Support\n\nYes (CUDA, Tensor Cores)\n\nYes (Metal)\n\nYes (via MPS)\n\nYes (CUDA, DirectML, etc.)\n\nLimited\n\nYes (CUDA, embedded GPUs)\n\nOptional (Metal, CUDA, OpenCL)\n\nYes (OpenCL, Metal)\n\nNo\n\nNPU / DSP Support\n\nNo\n\nYes (Apple ANE)\n\nEmerging ANE support\n\nYes (via NNAPI, OpenVINO, etc.)\n\nPotential via backend interface\n\nYes (TI, Nvidia, ADAS accelerators)\n\nNo (LLM-focused, CPU-oriented)\n\nYes (NNAPI, EdgeTPU, Hexagon)\n\nNo\n\nHardware Abstraction\n\nLow-level plugin engine, manual tuning\n\nAutomatic\n\nManual tuning via MLX\n\nModular Execution Providers (EPs)\n\nCompiled dispatcher with targets\n\nDevice-specific optimization required\n\nLow-level SIMD/CUDA offload\n\nDelegate-based (pluggable)\n\nN/A\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nExecution Type\n\nAOT compiled CUDA graph\n\nEager, dynamic hardware assignment\n\nEager + compiled graph\n\nStatic graph with runtime optimizations\n\nBytecode VM interpreter\n\nSparse 3D graph + temporal flow\n\nManual loop over transformer layers\n\nStatic interpreter + delegates\n\nREST/gRPC inference pipeline\n\nCPU Support\n\nNo (GPU only)\n\nYes (fallback)\n\nYes (M1/M2 optimized)\n\nYes (default EP)\n\nYes\n\nYes\n\nYes (highly optimized)\n\nYes\n\nYes\n\nGPU Support\n\nYes (CUDA, Tensor Cores)\n\nYes (Metal)\n\nYes (via MPS)\n\nYes (CUDA, DirectML, etc.)\n\nLimited\n\nYes (CUDA, embedded GPUs)\n\nOptional (Metal, CUDA, OpenCL)\n\nYes (OpenCL, Metal)\n\nNo\n\nNPU / DSP Support\n\nNo\n\nYes (Apple ANE)\n\nEmerging ANE support\n\nYes (via NNAPI, OpenVINO, etc.)\n\nPotential via backend interface\n\nYes (TI, Nvidia, ADAS accelerators)\n\nNo (LLM-focused, CPU-oriented)\n\nYes (NNAPI, EdgeTPU, Hexagon)\n\nNo\n\nHardware Abstraction\n\nLow-level plugin engine, manual tuning\n\nAutomatic\n\nManual tuning via MLX\n\nModular Execution Providers (EPs)\n\nCompiled dispatcher with targets\n\nDevice-specific optimization required\n\nLow-level SIMD/CUDA offload\n\nDelegate-based (pluggable)\n\nN/A\n\n#### Optimization, Size, and Constraints\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nModel Optimization Support\n\nYes (kernel tuning, quantization, \\`float16\\`/\\`int8\\`)\n\nYes (ANE targeting, quantization)\n\nNo built-in, manual scripting\n\nYes (quantization, pruning, graph fusion)\n\nYes (operator pruning, bytecode fusion)\n\nYes (3D-aware compression and fusions)\n\nYes (quantized GGUF)\n\nYes (quantization, fusion)\n\nYes (batching, threading)\n\nRuntime Size\n\nMedium (~5–15 MB)\n\nMedium (~5–10 MB)\n\nMedium\n\nLarge (5–30 MB)\n\nVery small (<1 MB)\n\nMedium–Large\n\nSmall–Medium\n\nSmall (~0.5–5 MB)\n\nVery large (>100 MB)\n\nMemory Footprint (Inference)\n\nLow to moderate (GPU memory bound)\n\nLow to moderate\n\nModerate (GPU-heavy)\n\nVariable (depends on EPs)\n\nUltra-low (sub-MB possible)\n\nHigh (large point cloud buffers)\n\nLow (~3–6 GB RAM for 7B models)\n\nLow\n\nHigh\n\nLatency\n\nVery low (sub-ms possible)\n\nLow (with ANE/GPU)\n\nMedium (eager mode)\n\nVariable (highly EP dependent)\n\nVery low\n\nModerate to high (depends on density)\n\nLow (for small LLMs)\n\nLow (under 10ms typical)\n\nModerate to high\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nModel Optimization Support\n\nYes (kernel tuning, quantization, \\`float16\\`/\\`int8\\`)\n\nYes (ANE targeting, quantization)\n\nNo built-in, manual scripting\n\nYes (quantization, pruning, graph fusion)\n\nYes (operator pruning, bytecode fusion)\n\nYes (3D-aware compression and fusions)\n\nYes (quantized GGUF)\n\nYes (quantization, fusion)\n\nYes (batching, threading)\n\nRuntime Size\n\nMedium (~5–15 MB)\n\nMedium (~5–10 MB)\n\nMedium\n\nLarge (5–30 MB)\n\nVery small (<1 MB)\n\nMedium–Large\n\nSmall–Medium\n\nSmall (~0.5–5 MB)\n\nVery large (>100 MB)\n\nMemory Footprint (Inference)\n\nLow to moderate (GPU memory bound)\n\nLow to moderate\n\nModerate (GPU-heavy)\n\nVariable (depends on EPs)\n\nUltra-low (sub-MB possible)\n\nHigh (large point cloud buffers)\n\nLow (~3–6 GB RAM for 7B models)\n\nLow\n\nHigh\n\nLatency\n\nVery low (sub-ms possible)\n\nLow (with ANE/GPU)\n\nMedium (eager mode)\n\nVariable (highly EP dependent)\n\nVery low\n\nModerate to high (depends on density)\n\nLow (for small LLMs)\n\nLow (under 10ms typical)\n\nModerate to high\n\n#### Flexibility, Debugging, and Ecosystem\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nCustom Ops Support\n\nYes (via plugin library API)\n\nLimited (via `MLCustomModel`)\n\nFull (via Python subclassing)\n\nYes (custom EPs and ops)\n\nYes (C++ op authoring)\n\nYes (often required)\n\nNo (fixed transformer kernel set)\n\nYes (C++/C custom kernels)\n\nYes\n\nCommunity & Documentation\n\nStrong NVIDIA developer support, active forums\n\nStrong, Apple developer-centric\n\nNiche, growing\n\nVery strong\n\nGrowing (Meta-sponsored)\n\nLimited / hardware-vendor specific\n\nActive open-source base\n\nMature, large community\n\nVery mature in production\n\nDebugger Support\n\nNsight Systems, profiling tools, verbose logging\n\nXcode tools\n\nPython debug console\n\nModerate (model inspection tools)\n\nMinimal (CLI, log-based)\n\nCustom tooling per device\n\nLog-level output only\n\nTensorBoard-lite, CLI tools\n\nMonitoring via Prometheus, etc.\n\nEase of Use\n\nMedium (manual optimization, engine building)\n\nHigh for Apple developers\n\nMedium (researchers, tinkerers)\n\nModerate to high (depends on EP)\n\nMedium (steep setup curve)\n\nLow (requires system integration)\n\nHigh (once model is quantized)\n\nHigh (especially with `model maker`)\n\nMedium to high (requires infra)\n\n**Attribute**\n\n**TensorRT**\n\n**Core ML**\n\n**MLX**\n\n**ONNX Runtime**\n\n**ExecuTorch**\n\n**LidarTLM**\n\n**`llama.cpp`**\n\n**TensorFlow Lite**\n\n**TensorFlow Serving**\n\nCustom Ops Support\n\nYes (via plugin library API)\n\nLimited (via `MLCustomModel`)\n\nFull (via Python subclassing)\n\nYes (custom EPs and ops)\n\nYes (C++ op authoring)\n\nYes (often required)\n\nNo (fixed transformer kernel set)\n\nYes (C++/C custom kernels)\n\nYes\n\nCommunity & Documentation\n\nStrong NVIDIA developer support, active forums\n\nStrong, Apple developer-centric\n\nNiche, growing\n\nVery strong\n\nGrowing (Meta-sponsored)\n\nLimited / hardware-vendor specific\n\nActive open-source base\n\nMature, large community\n\nVery mature in production\n\nDebugger Support\n\nNsight Systems, profiling tools, verbose logging\n\nXcode tools\n\nPython debug console\n\nModerate (model inspection tools)\n\nMinimal (CLI, log-based)\n\nCustom tooling per device\n\nLog-level output only\n\nTensorBoard-lite, CLI tools\n\nMonitoring via Prometheus, etc.\n\nEase of Use\n\nMedium (manual optimization, engine building)\n\nHigh for Apple developers\n\nMedium (researchers, tinkerers)\n\nModerate to high (depends on EP)\n\nMedium (steep setup curve)\n\nLow (requires system integration)\n\nHigh (once model is quantized)\n\nHigh (especially with `model maker`)\n\nMedium to high (requires infra)",
      "order": 52,
      "orderInChapter": 8,
      "difficulty": 5,
      "estimatedMinutes": 8,
      "tags": [
        "miscellaneous",
        "transformer",
        "llm",
        "nlp",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 1403,
        "contentLength": 33435
      },
      "nextCards": [
        "ai-ml-runtimes-comparative-summary-and-guidance-53",
        "ai-ml-runtimes-runtime-selection-guidance-54"
      ],
      "relatedCards": [
        "ai-gpu-architecture-tensor-core-evolution-23",
        "ai-gpu-architecture-key-takeaways-38",
        "ai-top-30-papers-better-faster-large-language-models-via-multi-toke-27",
        "ai-gpu-architecture-per-generation-precision-support-and-performance-26",
        "ai-top-30-papers-retrieval-augmented-generation-for-knowledge-inten-29"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#comparative-analysis",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-comparative-summary-and-guidance-53",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "title": "Comparative Summary and Guidance",
      "subtitle": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "contentHtml": "<h4 id=\"feature-comparison-table\">Feature Comparison Table</h4>\n<ul>\n  <li>This section provides a side-by-side comparison of the on-device ML runtimes discussed, highlighting their architectural differences, platform support, performance characteristics, and ideal use cases. This helps clarify which runtime best fits various project needs, from embedded development to mobile apps and language model inference.</li>\n</ul>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Platform Support</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Model Format</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Hardware Acceleration</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Optimized For</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Custom Ops</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Size Footprint</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">TensorRT</td>\n<td class=\"tg-tleft-valign-first\">NVIDIA GPUs (desktop, Jetson, server)</td>\n<td class=\"tg-tleft-valign-first\">ONNX, `.plan` (engine file)</td>\n<td class=\"tg-tleft-valign-first\">CUDA, Tensor Cores</td>\n<td class=\"tg-tleft-valign-first\">Low-latency GPU inference</td>\n<td class=\"tg-tleft-valign-first\">Yes (via plugin system)</td>\n<td class=\"tg-tleft-valign-second\">Medium (~5–15 MB)</td>\n</tr>    \n<tr>\n<td class=\"tg-tleft-valign-first\">Core ML</td>\n<td class=\"tg-tleft-valign-first\">Apple only (iOS/macOS)</td>\n<td class=\"tg-tleft-valign-first\">`.mlmodelc`</td>\n<td class=\"tg-tleft-valign-first\">CPU, GPU, ANE</td>\n<td class=\"tg-tleft-valign-first\">App integration on Apple devices</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-second\">Medium (~2–10 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">MLX</td>\n<td class=\"tg-tleft-valign-first\">Apple Silicon (macOS)</td>\n<td class=\"tg-tleft-valign-first\">Python code</td>\n<td class=\"tg-tleft-valign-first\">MPS, ANE (partial)</td>\n<td class=\"tg-tleft-valign-first\">Research &amp; experimentation</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Medium (~2–5 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">ONNX Runtime</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform (Mobile &amp; Desktop)</td>\n<td class=\"tg-tleft-valign-first\">`.onnx`</td>\n<td class=\"tg-tleft-valign-first\">CUDA, NNAPI, DirectML, etc.</td>\n<td class=\"tg-tleft-valign-first\">Cross-framework interoperability</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Large (~5–30 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">ExecuTorch</td>\n<td class=\"tg-tleft-valign-first\">Embedded, MCUs, Android</td>\n<td class=\"tg-tleft-valign-first\">Compiled TorchScript (`.ptc`)</td>\n<td class=\"tg-tleft-valign-first\">CPU, MCU, DSP</td>\n<td class=\"tg-tleft-valign-first\">Ultra-light edge inference</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Very small (&lt;1 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">LidarTLM</td>\n<td class=\"tg-tleft-valign-first\">Embedded/Robotics</td>\n<td class=\"tg-tleft-valign-first\">Custom/ONNX</td>\n<td class=\"tg-tleft-valign-first\">CUDA, DSP, NPU</td>\n<td class=\"tg-tleft-valign-first\">Sparse point cloud inference</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Medium–Large</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><code>llama.cpp</code></td>\n<td class=\"tg-tleft-valign-first\">Desktop, Mobile, WASM</td>\n<td class=\"tg-tleft-valign-first\">Quantized GGUF</td>\n<td class=\"tg-tleft-valign-first\">CPU, Optional GPU</td>\n<td class=\"tg-tleft-valign-first\">Efficient LLM inference</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-second\">Small–Medium (CPU)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">TFLite</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform (MCU to mobile)</td>\n<td class=\"tg-tleft-valign-first\">`.tflite`</td>\n<td class=\"tg-tleft-valign-first\">NNAPI, GPU, DSP, EdgeTPU</td>\n<td class=\"tg-tleft-valign-first\">Mobile and embedded AI</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Small (~500 KB–5 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">TF Serving</td>\n<td class=\"tg-tleft-valign-first\">Cloud/Server</td>\n<td class=\"tg-tleft-valign-first\">SavedModel</td>\n<td class=\"tg-tleft-valign-first\">N/A</td>\n<td class=\"tg-tleft-valign-first\">Scalable online inference</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Very large (&gt;100 MB)</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Runtime</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Platform Support</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Model Format</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Hardware Acceleration</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Optimized For</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Custom Ops</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Size Footprint</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">TensorRT</td>\n<td class=\"tg-tleft-valign-first\">NVIDIA GPUs (desktop, Jetson, server)</td>\n<td class=\"tg-tleft-valign-first\">ONNX, `.plan` (engine file)</td>\n<td class=\"tg-tleft-valign-first\">CUDA, Tensor Cores</td>\n<td class=\"tg-tleft-valign-first\">Low-latency GPU inference</td>\n<td class=\"tg-tleft-valign-first\">Yes (via plugin system)</td>\n<td class=\"tg-tleft-valign-second\">Medium (~5–15 MB)</td>\n</tr>    \n<tr>\n<td class=\"tg-tleft-valign-first\">Core ML</td>\n<td class=\"tg-tleft-valign-first\">Apple only (iOS/macOS)</td>\n<td class=\"tg-tleft-valign-first\">`.mlmodelc`</td>\n<td class=\"tg-tleft-valign-first\">CPU, GPU, ANE</td>\n<td class=\"tg-tleft-valign-first\">App integration on Apple devices</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-second\">Medium (~2–10 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">MLX</td>\n<td class=\"tg-tleft-valign-first\">Apple Silicon (macOS)</td>\n<td class=\"tg-tleft-valign-first\">Python code</td>\n<td class=\"tg-tleft-valign-first\">MPS, ANE (partial)</td>\n<td class=\"tg-tleft-valign-first\">Research &amp; experimentation</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Medium (~2–5 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">ONNX Runtime</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform (Mobile &amp; Desktop)</td>\n<td class=\"tg-tleft-valign-first\">`.onnx`</td>\n<td class=\"tg-tleft-valign-first\">CUDA, NNAPI, DirectML, etc.</td>\n<td class=\"tg-tleft-valign-first\">Cross-framework interoperability</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Large (~5–30 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">ExecuTorch</td>\n<td class=\"tg-tleft-valign-first\">Embedded, MCUs, Android</td>\n<td class=\"tg-tleft-valign-first\">Compiled TorchScript (`.ptc`)</td>\n<td class=\"tg-tleft-valign-first\">CPU, MCU, DSP</td>\n<td class=\"tg-tleft-valign-first\">Ultra-light edge inference</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Very small (&lt;1 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">LidarTLM</td>\n<td class=\"tg-tleft-valign-first\">Embedded/Robotics</td>\n<td class=\"tg-tleft-valign-first\">Custom/ONNX</td>\n<td class=\"tg-tleft-valign-first\">CUDA, DSP, NPU</td>\n<td class=\"tg-tleft-valign-first\">Sparse point cloud inference</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Medium–Large</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\"><code>llama.cpp</code></td>\n<td class=\"tg-tleft-valign-first\">Desktop, Mobile, WASM</td>\n<td class=\"tg-tleft-valign-first\">Quantized GGUF</td>\n<td class=\"tg-tleft-valign-first\">CPU, Optional GPU</td>\n<td class=\"tg-tleft-valign-first\">Efficient LLM inference</td>\n<td class=\"tg-tleft-valign-first\">Limited</td>\n<td class=\"tg-tleft-valign-second\">Small–Medium (CPU)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">TFLite</td>\n<td class=\"tg-tleft-valign-first\">Cross-platform (MCU to mobile)</td>\n<td class=\"tg-tleft-valign-first\">`.tflite`</td>\n<td class=\"tg-tleft-valign-first\">NNAPI, GPU, DSP, EdgeTPU</td>\n<td class=\"tg-tleft-valign-first\">Mobile and embedded AI</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Small (~500 KB–5 MB)</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">TF Serving</td>\n<td class=\"tg-tleft-valign-first\">Cloud/Server</td>\n<td class=\"tg-tleft-valign-first\">SavedModel</td>\n<td class=\"tg-tleft-valign-first\">N/A</td>\n<td class=\"tg-tleft-valign-first\">Scalable online inference</td>\n<td class=\"tg-tleft-valign-first\">Yes</td>\n<td class=\"tg-tleft-valign-second\">Very large (&gt;100 MB)</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"strengths-by-runtime\">Strengths by Runtime</h4>\n<ul>\n  <li>\n    <p><strong>Core ML</strong>: Best for iOS/macOS developers needing deep system integration with the Apple ecosystem. Ideal for apps that use Vision, SiriKit, or ARKit.</p>\n  </li>\n  <li>\n    <p><strong>MLX</strong>: Best for Mac-based researchers and developers who want PyTorch-like flexibility and native hardware performance without deploying to iOS.</p>\n  </li>\n  <li>\n    <p><strong>ONNX Runtime</strong>: Best for cross-platform deployments and teams needing a unified inference backend across mobile, desktop, and cloud. Excellent hardware flexibility.</p>\n  </li>\n  <li>\n    <p><strong>ExecuTorch</strong>: Best for extremely constrained devices like MCUs, or custom silicon. Perfect for edge intelligence with hard memory and latency budgets.</p>\n  </li>\n  <li>\n    <p><strong>LidarTLM</strong>: Best for autonomous systems, robotics, and 3D SLAM applications that involve high-bandwidth spatial data like LiDAR or radar.</p>\n  </li>\n  <li>\n    <p><strong><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code></strong>: Best for private, local LLM inference on personal devices or embedding transformer models into apps without requiring cloud or heavy runtimes.</p>\n  </li>\n  <li>\n    <p><strong>TFLite</strong>: Best all-around runtime for mobile and embedded ML. Huge ecosystem, widespread delegate support, and tooling maturity.</p>\n  </li>\n  <li>\n    <p><strong>TF Serving</strong>: Best for cloud applications needing high-volume model serving (e.g., for APIs). Not designed for local or offline inference.</p>\n  </li>\n</ul>\n<p><strong>Core ML</strong>: Best for iOS/macOS developers needing deep system integration with the Apple ecosystem. Ideal for apps that use Vision, SiriKit, or ARKit.</p>\n<p><strong>MLX</strong>: Best for Mac-based researchers and developers who want PyTorch-like flexibility and native hardware performance without deploying to iOS.</p>\n<p><strong>ONNX Runtime</strong>: Best for cross-platform deployments and teams needing a unified inference backend across mobile, desktop, and cloud. Excellent hardware flexibility.</p>\n<p><strong>ExecuTorch</strong>: Best for extremely constrained devices like MCUs, or custom silicon. Perfect for edge intelligence with hard memory and latency budgets.</p>\n<p><strong>LidarTLM</strong>: Best for autonomous systems, robotics, and 3D SLAM applications that involve high-bandwidth spatial data like LiDAR or radar.</p>\n<p><strong><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code></strong>: Best for private, local LLM inference on personal devices or embedding transformer models into apps without requiring cloud or heavy runtimes.</p>\n<p><strong>TFLite</strong>: Best all-around runtime for mobile and embedded ML. Huge ecosystem, widespread delegate support, and tooling maturity.</p>\n<p><strong>TF Serving</strong>: Best for cloud applications needing high-volume model serving (e.g., for APIs). Not designed for local or offline inference.</p>",
      "contentMarkdown": "#### Feature Comparison Table\n\n*   This section provides a side-by-side comparison of the on-device ML runtimes discussed, highlighting their architectural differences, platform support, performance characteristics, and ideal use cases. This helps clarify which runtime best fits various project needs, from embedded development to mobile apps and language model inference.\n\n**Runtime**\n\n**Platform Support**\n\n**Model Format**\n\n**Hardware Acceleration**\n\n**Optimized For**\n\n**Custom Ops**\n\n**Size Footprint**\n\nTensorRT\n\nNVIDIA GPUs (desktop, Jetson, server)\n\nONNX, \\`.plan\\` (engine file)\n\nCUDA, Tensor Cores\n\nLow-latency GPU inference\n\nYes (via plugin system)\n\nMedium (~5–15 MB)\n\nCore ML\n\nApple only (iOS/macOS)\n\n\\`.mlmodelc\\`\n\nCPU, GPU, ANE\n\nApp integration on Apple devices\n\nLimited\n\nMedium (~2–10 MB)\n\nMLX\n\nApple Silicon (macOS)\n\nPython code\n\nMPS, ANE (partial)\n\nResearch & experimentation\n\nYes\n\nMedium (~2–5 MB)\n\nONNX Runtime\n\nCross-platform (Mobile & Desktop)\n\n\\`.onnx\\`\n\nCUDA, NNAPI, DirectML, etc.\n\nCross-framework interoperability\n\nYes\n\nLarge (~5–30 MB)\n\nExecuTorch\n\nEmbedded, MCUs, Android\n\nCompiled TorchScript (\\`.ptc\\`)\n\nCPU, MCU, DSP\n\nUltra-light edge inference\n\nYes\n\nVery small (<1 MB)\n\nLidarTLM\n\nEmbedded/Robotics\n\nCustom/ONNX\n\nCUDA, DSP, NPU\n\nSparse point cloud inference\n\nYes\n\nMedium–Large\n\n`llama.cpp`\n\nDesktop, Mobile, WASM\n\nQuantized GGUF\n\nCPU, Optional GPU\n\nEfficient LLM inference\n\nLimited\n\nSmall–Medium (CPU)\n\nTFLite\n\nCross-platform (MCU to mobile)\n\n\\`.tflite\\`\n\nNNAPI, GPU, DSP, EdgeTPU\n\nMobile and embedded AI\n\nYes\n\nSmall (~500 KB–5 MB)\n\nTF Serving\n\nCloud/Server\n\nSavedModel\n\nN/A\n\nScalable online inference\n\nYes\n\nVery large (>100 MB)\n\n**Runtime**\n\n**Platform Support**\n\n**Model Format**\n\n**Hardware Acceleration**\n\n**Optimized For**\n\n**Custom Ops**\n\n**Size Footprint**\n\nTensorRT\n\nNVIDIA GPUs (desktop, Jetson, server)\n\nONNX, \\`.plan\\` (engine file)\n\nCUDA, Tensor Cores\n\nLow-latency GPU inference\n\nYes (via plugin system)\n\nMedium (~5–15 MB)\n\nCore ML\n\nApple only (iOS/macOS)\n\n\\`.mlmodelc\\`\n\nCPU, GPU, ANE\n\nApp integration on Apple devices\n\nLimited\n\nMedium (~2–10 MB)\n\nMLX\n\nApple Silicon (macOS)\n\nPython code\n\nMPS, ANE (partial)\n\nResearch & experimentation\n\nYes\n\nMedium (~2–5 MB)\n\nONNX Runtime\n\nCross-platform (Mobile & Desktop)\n\n\\`.onnx\\`\n\nCUDA, NNAPI, DirectML, etc.\n\nCross-framework interoperability\n\nYes\n\nLarge (~5–30 MB)\n\nExecuTorch\n\nEmbedded, MCUs, Android\n\nCompiled TorchScript (\\`.ptc\\`)\n\nCPU, MCU, DSP\n\nUltra-light edge inference\n\nYes\n\nVery small (<1 MB)\n\nLidarTLM\n\nEmbedded/Robotics\n\nCustom/ONNX\n\nCUDA, DSP, NPU\n\nSparse point cloud inference\n\nYes\n\nMedium–Large\n\n`llama.cpp`\n\nDesktop, Mobile, WASM\n\nQuantized GGUF\n\nCPU, Optional GPU\n\nEfficient LLM inference\n\nLimited\n\nSmall–Medium (CPU)\n\nTFLite\n\nCross-platform (MCU to mobile)\n\n\\`.tflite\\`\n\nNNAPI, GPU, DSP, EdgeTPU\n\nMobile and embedded AI\n\nYes\n\nSmall (~500 KB–5 MB)\n\nTF Serving\n\nCloud/Server\n\nSavedModel\n\nN/A\n\nScalable online inference\n\nYes\n\nVery large (>100 MB)\n\n#### Strengths by Runtime\n\n*   **Core ML**: Best for iOS/macOS developers needing deep system integration with the Apple ecosystem. Ideal for apps that use Vision, SiriKit, or ARKit.\n    \n*   **MLX**: Best for Mac-based researchers and developers who want PyTorch-like flexibility and native hardware performance without deploying to iOS.\n    \n*   **ONNX Runtime**: Best for cross-platform deployments and teams needing a unified inference backend across mobile, desktop, and cloud. Excellent hardware flexibility.\n    \n*   **ExecuTorch**: Best for extremely constrained devices like MCUs, or custom silicon. Perfect for edge intelligence with hard memory and latency budgets.\n    \n*   **LidarTLM**: Best for autonomous systems, robotics, and 3D SLAM applications that involve high-bandwidth spatial data like LiDAR or radar.\n    \n*   **`llama.cpp`**: Best for private, local LLM inference on personal devices or embedding transformer models into apps without requiring cloud or heavy runtimes.\n    \n*   **TFLite**: Best all-around runtime for mobile and embedded ML. Huge ecosystem, widespread delegate support, and tooling maturity.\n    \n*   **TF Serving**: Best for cloud applications needing high-volume model serving (e.g., for APIs). Not designed for local or offline inference.\n    \n\n**Core ML**: Best for iOS/macOS developers needing deep system integration with the Apple ecosystem. Ideal for apps that use Vision, SiriKit, or ARKit.\n\n**MLX**: Best for Mac-based researchers and developers who want PyTorch-like flexibility and native hardware performance without deploying to iOS.\n\n**ONNX Runtime**: Best for cross-platform deployments and teams needing a unified inference backend across mobile, desktop, and cloud. Excellent hardware flexibility.\n\n**ExecuTorch**: Best for extremely constrained devices like MCUs, or custom silicon. Perfect for edge intelligence with hard memory and latency budgets.\n\n**LidarTLM**: Best for autonomous systems, robotics, and 3D SLAM applications that involve high-bandwidth spatial data like LiDAR or radar.\n\n**`llama.cpp`**: Best for private, local LLM inference on personal devices or embedding transformer models into apps without requiring cloud or heavy runtimes.\n\n**TFLite**: Best all-around runtime for mobile and embedded ML. Huge ecosystem, widespread delegate support, and tooling maturity.\n\n**TF Serving**: Best for cloud applications needing high-volume model serving (e.g., for APIs). Not designed for local or offline inference.",
      "order": 53,
      "orderInChapter": 9,
      "difficulty": 5,
      "estimatedMinutes": 4,
      "tags": [
        "miscellaneous",
        "transformer",
        "embedding",
        "llm"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 713,
        "contentLength": 11989
      },
      "nextCards": [
        "ai-ml-runtimes-runtime-selection-guidance-54",
        "ai-ml-runtimes-final-thoughts-55"
      ],
      "relatedCards": [
        "ai-gpu-architecture-tensor-core-evolution-23",
        "ai-gpu-architecture-key-takeaways-38",
        "ai-top-30-papers-better-faster-large-language-models-via-multi-toke-27",
        "ai-gpu-architecture-per-generation-precision-support-and-performance-26",
        "ai-top-30-papers-scaling-laws-for-neural-language-models-22"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#comparative-summary-and-guidance",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-runtime-selection-guidance-54",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "title": "Runtime Selection Guidance",
      "subtitle": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>If you’re deploying to iOS or macOS</strong>:</p>\n\n    <ul>\n      <li>Use <strong>Core ML</strong> for production apps.</li>\n      <li>Use <strong>MLX</strong> for research, local experimentation, or custom modeling.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>If you’re deploying to embedded edge devices</strong>:</p>\n\n    <ul>\n      <li>Use <strong>ExecuTorch</strong> for PyTorch-based workflows.</li>\n      <li>Use <strong>TensorFlow Lite for Microcontrollers</strong> for tight memory constraints.</li>\n      <li>Consider <strong>LidarTLM</strong>-style tools if dealing with 3D spatial data.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>If you’re targeting Android or need portability</strong>:</p>\n\n    <ul>\n      <li>Use <strong>TensorFlow Lite</strong> or <strong>ONNX Runtime</strong> with delegates like NNAPI or GPU.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>If you’re working with LLMs locally</strong>:</p>\n\n    <ul>\n      <li>Use <strong><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code></strong> for best CPU-based inference and minimal setup.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>If you want cross-framework model portability</strong>:</p>\n\n    <ul>\n      <li>Use <strong>ONNX Runtime</strong> with models exported from PyTorch, TensorFlow, or others.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>If you require real-time, high-volume cloud inference</strong>:</p>\n\n    <ul>\n      <li>Use <strong>TensorFlow Serving</strong> or ONNX Runtime Server.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>If you’re deploying to iOS or macOS</strong>:</p>\n<ul>\n      <li>Use <strong>Core ML</strong> for production apps.</li>\n      <li>Use <strong>MLX</strong> for research, local experimentation, or custom modeling.</li>\n    </ul>\n<p><strong>If you’re deploying to embedded edge devices</strong>:</p>\n<ul>\n      <li>Use <strong>ExecuTorch</strong> for PyTorch-based workflows.</li>\n      <li>Use <strong>TensorFlow Lite for Microcontrollers</strong> for tight memory constraints.</li>\n      <li>Consider <strong>LidarTLM</strong>-style tools if dealing with 3D spatial data.</li>\n    </ul>\n<p><strong>If you’re targeting Android or need portability</strong>:</p>\n<ul>\n      <li>Use <strong>TensorFlow Lite</strong> or <strong>ONNX Runtime</strong> with delegates like NNAPI or GPU.</li>\n    </ul>\n<p><strong>If you’re working with LLMs locally</strong>:</p>\n<ul>\n      <li>Use <strong><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code></strong> for best CPU-based inference and minimal setup.</li>\n    </ul>\n<p><strong>If you want cross-framework model portability</strong>:</p>\n<ul>\n      <li>Use <strong>ONNX Runtime</strong> with models exported from PyTorch, TensorFlow, or others.</li>\n    </ul>\n<p><strong>If you require real-time, high-volume cloud inference</strong>:</p>\n<ul>\n      <li>Use <strong>TensorFlow Serving</strong> or ONNX Runtime Server.</li>\n    </ul>",
      "contentMarkdown": "*   **If you’re deploying to iOS or macOS**:\n    \n    *   Use **Core ML** for production apps.\n    *   Use **MLX** for research, local experimentation, or custom modeling.\n*   **If you’re deploying to embedded edge devices**:\n    \n    *   Use **ExecuTorch** for PyTorch-based workflows.\n    *   Use **TensorFlow Lite for Microcontrollers** for tight memory constraints.\n    *   Consider **LidarTLM**\\-style tools if dealing with 3D spatial data.\n*   **If you’re targeting Android or need portability**:\n    \n    *   Use **TensorFlow Lite** or **ONNX Runtime** with delegates like NNAPI or GPU.\n*   **If you’re working with LLMs locally**:\n    \n    *   Use **`llama.cpp`** for best CPU-based inference and minimal setup.\n*   **If you want cross-framework model portability**:\n    \n    *   Use **ONNX Runtime** with models exported from PyTorch, TensorFlow, or others.\n*   **If you require real-time, high-volume cloud inference**:\n    \n    *   Use **TensorFlow Serving** or ONNX Runtime Server.\n\n**If you’re deploying to iOS or macOS**:\n\n*   Use **Core ML** for production apps.\n*   Use **MLX** for research, local experimentation, or custom modeling.\n\n**If you’re deploying to embedded edge devices**:\n\n*   Use **ExecuTorch** for PyTorch-based workflows.\n*   Use **TensorFlow Lite for Microcontrollers** for tight memory constraints.\n*   Consider **LidarTLM**\\-style tools if dealing with 3D spatial data.\n\n**If you’re targeting Android or need portability**:\n\n*   Use **TensorFlow Lite** or **ONNX Runtime** with delegates like NNAPI or GPU.\n\n**If you’re working with LLMs locally**:\n\n*   Use **`llama.cpp`** for best CPU-based inference and minimal setup.\n\n**If you want cross-framework model portability**:\n\n*   Use **ONNX Runtime** with models exported from PyTorch, TensorFlow, or others.\n\n**If you require real-time, high-volume cloud inference**:\n\n*   Use **TensorFlow Serving** or ONNX Runtime Server.",
      "order": 54,
      "orderInChapter": 10,
      "difficulty": 5,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "llm"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 258,
        "contentLength": 2950
      },
      "nextCards": [
        "ai-ml-runtimes-final-thoughts-55",
        "ai-ml-runtimes-protocol-buffers-protobuf-56"
      ],
      "relatedCards": [
        "ai-top-30-papers-alcuna-large-language-models-meet-new-knowledge-33",
        "ai-top-30-papers-the-perils-promises-of-fact-checking-with-large-la-34",
        "ai-gpu-architecture-tensor-core-evolution-23",
        "ai-gpu-architecture-key-takeaways-38",
        "ai-gpu-architecture-per-generation-precision-support-and-performance-26"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#runtime-selection-guidance",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-final-thoughts-55",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "title": "Final Thoughts",
      "subtitle": "TensorFlow Lite / TensorFlow Serving Deep Dive",
      "contentHtml": "<ul>\n  <li>\n    <p>Choosing the right on-device ML runtime depends heavily on the following factors:</p>\n\n    <ul>\n      <li>Deployment environment (mobile, embedded, desktop, web, cloud)</li>\n      <li>Model architecture (CNN, RNN, transformer, etc.)</li>\n      <li>Performance requirements (latency, FPS, memory usage)</li>\n      <li>Development preferences (PyTorch, TensorFlow, raw C++, etc.)</li>\n      <li>Hardware capabilities (CPU, GPU, NPU, DSP, etc.)</li>\n    </ul>\n  </li>\n  <li>\n    <p>Each runtime discussed in this primer is best-in-class for a certain domain or design constraint. Rather than a “one-size-fits-all” solution, success in on-device ML depends on thoughtful matching between the model, target platform, and available tools. Here’s a summary of which is the best runtime across a range of scenarios:</p>\n\n    <ul>\n      <li><strong>Best for Apple-native app development</strong>: <em>Core ML</em></li>\n      <li><strong>Best for Apple-based model experimentation</strong>: <em>MLX</em></li>\n      <li><strong>Best for cross-platform portability and hardware access</strong>: <em>ONNX Runtime</em></li>\n      <li><strong>Best for minimal embedded inference</strong>: <em>ExecuTorch</em></li>\n      <li><strong>Best for 3D LiDAR/robotics</strong>: <em>LidarTLM-like stacks</em></li>\n      <li><strong>Best for on-device LLM inference</strong>: <em><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code></em></li>\n      <li><strong>Best for mobile/embedded general ML</strong>: <em>TensorFlow Lite</em></li>\n      <li><strong>Best for scalable cloud inference</strong>: <em>TensorFlow Serving</em></li>\n    </ul>\n  </li>\n</ul>\n<p>Choosing the right on-device ML runtime depends heavily on the following factors:</p>\n<ul>\n      <li>Deployment environment (mobile, embedded, desktop, web, cloud)</li>\n      <li>Model architecture (CNN, RNN, transformer, etc.)</li>\n      <li>Performance requirements (latency, FPS, memory usage)</li>\n      <li>Development preferences (PyTorch, TensorFlow, raw C++, etc.)</li>\n      <li>Hardware capabilities (CPU, GPU, NPU, DSP, etc.)</li>\n    </ul>\n<p>Each runtime discussed in this primer is best-in-class for a certain domain or design constraint. Rather than a “one-size-fits-all” solution, success in on-device ML depends on thoughtful matching between the model, target platform, and available tools. Here’s a summary of which is the best runtime across a range of scenarios:</p>\n<ul>\n      <li><strong>Best for Apple-native app development</strong>: <em>Core ML</em></li>\n      <li><strong>Best for Apple-based model experimentation</strong>: <em>MLX</em></li>\n      <li><strong>Best for cross-platform portability and hardware access</strong>: <em>ONNX Runtime</em></li>\n      <li><strong>Best for minimal embedded inference</strong>: <em>ExecuTorch</em></li>\n      <li><strong>Best for 3D LiDAR/robotics</strong>: <em>LidarTLM-like stacks</em></li>\n      <li><strong>Best for on-device LLM inference</strong>: <em><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code></em></li>\n      <li><strong>Best for mobile/embedded general ML</strong>: <em>TensorFlow Lite</em></li>\n      <li><strong>Best for scalable cloud inference</strong>: <em>TensorFlow Serving</em></li>\n    </ul>",
      "contentMarkdown": "*   Choosing the right on-device ML runtime depends heavily on the following factors:\n    \n    *   Deployment environment (mobile, embedded, desktop, web, cloud)\n    *   Model architecture (CNN, RNN, transformer, etc.)\n    *   Performance requirements (latency, FPS, memory usage)\n    *   Development preferences (PyTorch, TensorFlow, raw C++, etc.)\n    *   Hardware capabilities (CPU, GPU, NPU, DSP, etc.)\n*   Each runtime discussed in this primer is best-in-class for a certain domain or design constraint. Rather than a “one-size-fits-all” solution, success in on-device ML depends on thoughtful matching between the model, target platform, and available tools. Here’s a summary of which is the best runtime across a range of scenarios:\n    \n    *   **Best for Apple-native app development**: _Core ML_\n    *   **Best for Apple-based model experimentation**: _MLX_\n    *   **Best for cross-platform portability and hardware access**: _ONNX Runtime_\n    *   **Best for minimal embedded inference**: _ExecuTorch_\n    *   **Best for 3D LiDAR/robotics**: _LidarTLM-like stacks_\n    *   **Best for on-device LLM inference**: _`llama.cpp`_\n    *   **Best for mobile/embedded general ML**: _TensorFlow Lite_\n    *   **Best for scalable cloud inference**: _TensorFlow Serving_\n\nChoosing the right on-device ML runtime depends heavily on the following factors:\n\n*   Deployment environment (mobile, embedded, desktop, web, cloud)\n*   Model architecture (CNN, RNN, transformer, etc.)\n*   Performance requirements (latency, FPS, memory usage)\n*   Development preferences (PyTorch, TensorFlow, raw C++, etc.)\n*   Hardware capabilities (CPU, GPU, NPU, DSP, etc.)\n\nEach runtime discussed in this primer is best-in-class for a certain domain or design constraint. Rather than a “one-size-fits-all” solution, success in on-device ML depends on thoughtful matching between the model, target platform, and available tools. Here’s a summary of which is the best runtime across a range of scenarios:\n\n*   **Best for Apple-native app development**: _Core ML_\n*   **Best for Apple-based model experimentation**: _MLX_\n*   **Best for cross-platform portability and hardware access**: _ONNX Runtime_\n*   **Best for minimal embedded inference**: _ExecuTorch_\n*   **Best for 3D LiDAR/robotics**: _LidarTLM-like stacks_\n*   **Best for on-device LLM inference**: _`llama.cpp`_\n*   **Best for mobile/embedded general ML**: _TensorFlow Lite_\n*   **Best for scalable cloud inference**: _TensorFlow Serving_",
      "order": 55,
      "orderInChapter": 11,
      "difficulty": 5,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "transformer",
        "cnn",
        "rnn",
        "llm"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 326,
        "contentLength": 3268
      },
      "nextCards": [
        "ai-ml-runtimes-protocol-buffers-protobuf-56",
        "ai-ml-runtimes-flatbuffer-57"
      ],
      "relatedCards": [
        "ai-gpu-architecture-tensor-core-evolution-23",
        "ai-gpu-architecture-key-takeaways-38",
        "ai-top-30-papers-variational-lossy-autoencoder-17",
        "ai-top-30-papers-better-faster-large-language-models-via-multi-toke-27",
        "ai-gpu-architecture-per-generation-precision-support-and-performance-26"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#final-thoughts",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-protocol-buffers-protobuf-56",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Related: Serialization Formats Across Runtimes",
      "title": "Protocol Buffers (Protobuf)",
      "subtitle": "Related: Serialization Formats Across Runtimes",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Used by</strong>: TensorFlow (SavedModel, <code class=\"language-plaintext highlighter-rouge\">.pb</code>), ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>)</p>\n  </li>\n  <li>\n    <p><strong>Developed by</strong>: Google</p>\n  </li>\n  <li>\n    <p><strong>Type</strong>: Binary serialization framework</p>\n  </li>\n  <li>\n    <p><strong>Key Characteristics</strong>:</p>\n\n    <ul>\n      <li>Encodes structured data using <code class=\"language-plaintext highlighter-rouge\">.proto</code> schemas</li>\n      <li>Supports code generation in multiple languages (Python, C++, Java, etc.)</li>\n      <li>Strict type definitions with schema versioning</li>\n      <li>Produces portable, efficient, extensible binary files</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Advantages</strong>:</p>\n\n    <ul>\n      <li>Highly compact, faster than JSON/XML</li>\n      <li>Strong backward and forward compatibility through schema evolution</li>\n      <li>Ideal for representing complex hierarchical graphs (e.g., model computation trees)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>In ML context</strong>:</p>\n\n    <ul>\n      <li><strong>TensorFlow</strong>: Stores entire computation graph, tensor shapes, and metadata in <code class=\"language-plaintext highlighter-rouge\">.pb</code> (protobuf binary)</li>\n      <li><strong>ONNX</strong>: Defines all model ops, weights, and IR-level metadata via Protobuf-defined schema</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Limitations</strong>:</p>\n\n    <ul>\n      <li>Parsing requires full message decoding into memory</li>\n      <li>Less suited for minimal-footprint scenarios (e.g., MCUs)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Example</strong>:</p>\n\n    <ul>\n      <li>\n        <p><em>Used in: TensorFlow (<code class=\"language-plaintext highlighter-rouge\">.pb</code>, SavedModel), ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>)</em></p>\n      </li>\n      <li>\n        <p>Protobuf defines a schema in <code class=\"language-plaintext highlighter-rouge\">.proto</code> files and serializes structured binary data. Here’s a simplified view:</p>\n      </li>\n      <li>\n        <p><strong>Schema Definition (<code class=\"language-plaintext highlighter-rouge\">graph.proto</code>):</strong></p>\n\n        <div class=\"language-protobuf highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code16\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code16\">  <span class=\"kd\">message</span> <span class=\"nc\">TensorShape</span> <span class=\"p\">{</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">int64</span> <span class=\"na\">dim</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"kd\">message</span> <span class=\"nc\">Node</span> <span class=\"p\">{</span>\n    <span class=\"kt\">string</span> <span class=\"na\">op_type</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n    <span class=\"kt\">string</span> <span class=\"na\">name</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">string</span> <span class=\"na\">input</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">string</span> <span class=\"na\">output</span> <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"kd\">message</span> <span class=\"nc\">Graph</span> <span class=\"p\">{</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">Node</span> <span class=\"na\">node</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">TensorShape</span> <span class=\"na\">input_shape</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">TensorShape</span> <span class=\"na\">output_shape</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Example Python Usage (ONNX-style):</strong></p>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code17\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code17\">  <span class=\"kn\">import</span> <span class=\"nn\">onnx</span>\n\n  <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">onnx</span><span class=\"p\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s\">\"resnet50.onnx\"</span><span class=\"p\">)</span>\n  <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">graph</span><span class=\"p\">.</span><span class=\"n\">node</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>  <span class=\"c1\"># Shows first operation (e.g., Conv)\n</span></code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n\n        <ul>\n          <li>A binary <code class=\"language-plaintext highlighter-rouge\">.onnx</code> or <code class=\"language-plaintext highlighter-rouge\">.pb</code> file that’s unreadable in plain text but represents a complete computation graph, including ops, shapes, attributes, and weights.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Used by</strong>: TensorFlow (SavedModel, <code class=\"language-plaintext highlighter-rouge\">.pb</code>), ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>)</p>\n<p><strong>Developed by</strong>: Google</p>\n<p><strong>Type</strong>: Binary serialization framework</p>\n<p><strong>Key Characteristics</strong>:</p>\n<ul>\n      <li>Encodes structured data using <code class=\"language-plaintext highlighter-rouge\">.proto</code> schemas</li>\n      <li>Supports code generation in multiple languages (Python, C++, Java, etc.)</li>\n      <li>Strict type definitions with schema versioning</li>\n      <li>Produces portable, efficient, extensible binary files</li>\n    </ul>\n<p><strong>Advantages</strong>:</p>\n<ul>\n      <li>Highly compact, faster than JSON/XML</li>\n      <li>Strong backward and forward compatibility through schema evolution</li>\n      <li>Ideal for representing complex hierarchical graphs (e.g., model computation trees)</li>\n    </ul>\n<p><strong>In ML context</strong>:</p>\n<ul>\n      <li><strong>TensorFlow</strong>: Stores entire computation graph, tensor shapes, and metadata in <code class=\"language-plaintext highlighter-rouge\">.pb</code> (protobuf binary)</li>\n      <li><strong>ONNX</strong>: Defines all model ops, weights, and IR-level metadata via Protobuf-defined schema</li>\n    </ul>\n<p><strong>Limitations</strong>:</p>\n<ul>\n      <li>Parsing requires full message decoding into memory</li>\n      <li>Less suited for minimal-footprint scenarios (e.g., MCUs)</li>\n    </ul>\n<p><strong>Example</strong>:</p>\n<ul>\n      <li>\n        <p><em>Used in: TensorFlow (<code class=\"language-plaintext highlighter-rouge\">.pb</code>, SavedModel), ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>)</em></p>\n      </li>\n      <li>\n        <p>Protobuf defines a schema in <code class=\"language-plaintext highlighter-rouge\">.proto</code> files and serializes structured binary data. Here’s a simplified view:</p>\n      </li>\n      <li>\n        <p><strong>Schema Definition (<code class=\"language-plaintext highlighter-rouge\">graph.proto</code>):</strong></p>\n\n        <div class=\"language-protobuf highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code16\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code16\">  <span class=\"kd\">message</span> <span class=\"nc\">TensorShape</span> <span class=\"p\">{</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">int64</span> <span class=\"na\">dim</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"kd\">message</span> <span class=\"nc\">Node</span> <span class=\"p\">{</span>\n    <span class=\"kt\">string</span> <span class=\"na\">op_type</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n    <span class=\"kt\">string</span> <span class=\"na\">name</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">string</span> <span class=\"na\">input</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">string</span> <span class=\"na\">output</span> <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"kd\">message</span> <span class=\"nc\">Graph</span> <span class=\"p\">{</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">Node</span> <span class=\"na\">node</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">TensorShape</span> <span class=\"na\">input_shape</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">TensorShape</span> <span class=\"na\">output_shape</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Example Python Usage (ONNX-style):</strong></p>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code17\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code17\">  <span class=\"kn\">import</span> <span class=\"nn\">onnx</span>\n\n  <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">onnx</span><span class=\"p\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s\">\"resnet50.onnx\"</span><span class=\"p\">)</span>\n  <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">graph</span><span class=\"p\">.</span><span class=\"n\">node</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>  <span class=\"c1\"># Shows first operation (e.g., Conv)\n</span></code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n\n        <ul>\n          <li>A binary <code class=\"language-plaintext highlighter-rouge\">.onnx</code> or <code class=\"language-plaintext highlighter-rouge\">.pb</code> file that’s unreadable in plain text but represents a complete computation graph, including ops, shapes, attributes, and weights.</li>\n        </ul>\n      </li>\n    </ul>\n<p><em>Used in: TensorFlow (<code class=\"language-plaintext highlighter-rouge\">.pb</code>, SavedModel), ONNX (<code class=\"language-plaintext highlighter-rouge\">.onnx</code>)</em></p>\n<p>Protobuf defines a schema in <code class=\"language-plaintext highlighter-rouge\">.proto</code> files and serializes structured binary data. Here’s a simplified view:</p>\n<p><strong>Schema Definition (<code class=\"language-plaintext highlighter-rouge\">graph.proto</code>):</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code16\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code16\">  <span class=\"kd\">message</span> <span class=\"nc\">TensorShape</span> <span class=\"p\">{</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">int64</span> <span class=\"na\">dim</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"kd\">message</span> <span class=\"nc\">Node</span> <span class=\"p\">{</span>\n    <span class=\"kt\">string</span> <span class=\"na\">op_type</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n    <span class=\"kt\">string</span> <span class=\"na\">name</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">string</span> <span class=\"na\">input</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"kt\">string</span> <span class=\"na\">output</span> <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"kd\">message</span> <span class=\"nc\">Graph</span> <span class=\"p\">{</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">Node</span> <span class=\"na\">node</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">TensorShape</span> <span class=\"na\">input_shape</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">;</span>\n    <span class=\"k\">repeated</span> <span class=\"n\">TensorShape</span> <span class=\"na\">output_shape</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n</code></pre>\n<p><strong>Example Python Usage (ONNX-style):</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code17\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code17\">  <span class=\"kn\">import</span> <span class=\"nn\">onnx</span>\n\n  <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">onnx</span><span class=\"p\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s\">\"resnet50.onnx\"</span><span class=\"p\">)</span>\n  <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">graph</span><span class=\"p\">.</span><span class=\"n\">node</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>  <span class=\"c1\"># Shows first operation (e.g., Conv)\n</span></code></pre>\n<p><strong>Serialized File:</strong></p>\n<ul>\n          <li>A binary <code class=\"language-plaintext highlighter-rouge\">.onnx</code> or <code class=\"language-plaintext highlighter-rouge\">.pb</code> file that’s unreadable in plain text but represents a complete computation graph, including ops, shapes, attributes, and weights.</li>\n        </ul>",
      "contentMarkdown": "*   **Used by**: TensorFlow (SavedModel, `.pb`), ONNX (`.onnx`)\n    \n*   **Developed by**: Google\n    \n*   **Type**: Binary serialization framework\n    \n*   **Key Characteristics**:\n    \n    *   Encodes structured data using `.proto` schemas\n    *   Supports code generation in multiple languages (Python, C++, Java, etc.)\n    *   Strict type definitions with schema versioning\n    *   Produces portable, efficient, extensible binary files\n*   **Advantages**:\n    \n    *   Highly compact, faster than JSON/XML\n    *   Strong backward and forward compatibility through schema evolution\n    *   Ideal for representing complex hierarchical graphs (e.g., model computation trees)\n*   **In ML context**:\n    \n    *   **TensorFlow**: Stores entire computation graph, tensor shapes, and metadata in `.pb` (protobuf binary)\n    *   **ONNX**: Defines all model ops, weights, and IR-level metadata via Protobuf-defined schema\n*   **Limitations**:\n    \n    *   Parsing requires full message decoding into memory\n    *   Less suited for minimal-footprint scenarios (e.g., MCUs)\n*   **Example**:\n    \n    *   _Used in: TensorFlow (`.pb`, SavedModel), ONNX (`.onnx`)_\n        \n    *   Protobuf defines a schema in `.proto` files and serializes structured binary data. Here’s a simplified view:\n        \n    *   **Schema Definition (`graph.proto`):**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `message TensorShape {     repeated int64 dim = 1;   }    message Node {     string op_type = 1;     string name = 2;     repeated string input = 3;     repeated string output = 4;   }    message Graph {     repeated Node node = 1;     repeated TensorShape input_shape = 2;     repeated TensorShape output_shape = 3;   }`\n        \n    *   **Example Python Usage (ONNX-style):**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `import onnx    model = onnx.load(\"resnet50.onnx\")   print(model.graph.node[0])  # Shows first operation (e.g., Conv)`\n        \n    *   **Serialized File:**\n        \n        *   A binary `.onnx` or `.pb` file that’s unreadable in plain text but represents a complete computation graph, including ops, shapes, attributes, and weights.\n\n**Used by**: TensorFlow (SavedModel, `.pb`), ONNX (`.onnx`)\n\n**Developed by**: Google\n\n**Type**: Binary serialization framework\n\n**Key Characteristics**:\n\n*   Encodes structured data using `.proto` schemas\n*   Supports code generation in multiple languages (Python, C++, Java, etc.)\n*   Strict type definitions with schema versioning\n*   Produces portable, efficient, extensible binary files\n\n**Advantages**:\n\n*   Highly compact, faster than JSON/XML\n*   Strong backward and forward compatibility through schema evolution\n*   Ideal for representing complex hierarchical graphs (e.g., model computation trees)\n\n**In ML context**:\n\n*   **TensorFlow**: Stores entire computation graph, tensor shapes, and metadata in `.pb` (protobuf binary)\n*   **ONNX**: Defines all model ops, weights, and IR-level metadata via Protobuf-defined schema\n\n**Limitations**:\n\n*   Parsing requires full message decoding into memory\n*   Less suited for minimal-footprint scenarios (e.g., MCUs)\n\n**Example**:\n\n*   _Used in: TensorFlow (`.pb`, SavedModel), ONNX (`.onnx`)_\n    \n*   Protobuf defines a schema in `.proto` files and serializes structured binary data. Here’s a simplified view:\n    \n*   **Schema Definition (`graph.proto`):**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `message TensorShape {     repeated int64 dim = 1;   }    message Node {     string op_type = 1;     string name = 2;     repeated string input = 3;     repeated string output = 4;   }    message Graph {     repeated Node node = 1;     repeated TensorShape input_shape = 2;     repeated TensorShape output_shape = 3;   }`\n    \n*   **Example Python Usage (ONNX-style):**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `import onnx    model = onnx.load(\"resnet50.onnx\")   print(model.graph.node[0])  # Shows first operation (e.g., Conv)`\n    \n*   **Serialized File:**\n    \n    *   A binary `.onnx` or `.pb` file that’s unreadable in plain text but represents a complete computation graph, including ops, shapes, attributes, and weights.\n\n_Used in: TensorFlow (`.pb`, SavedModel), ONNX (`.onnx`)_\n\nProtobuf defines a schema in `.proto` files and serializes structured binary data. Here’s a simplified view:\n\n**Schema Definition (`graph.proto`):**\n\n![](https://aman.ai/images/copy.png)\n\n  `message TensorShape {     repeated int64 dim = 1;   }    message Node {     string op_type = 1;     string name = 2;     repeated string input = 3;     repeated string output = 4;   }    message Graph {     repeated Node node = 1;     repeated TensorShape input_shape = 2;     repeated TensorShape output_shape = 3;   }`\n\n**Example Python Usage (ONNX-style):**\n\n![](https://aman.ai/images/copy.png)\n\n  `import onnx    model = onnx.load(\"resnet50.onnx\")   print(model.graph.node[0])  # Shows first operation (e.g., Conv)`\n\n**Serialized File:**\n\n*   A binary `.onnx` or `.pb` file that’s unreadable in plain text but represents a complete computation graph, including ops, shapes, attributes, and weights.",
      "order": 56,
      "orderInChapter": 1,
      "difficulty": 5,
      "estimatedMinutes": 4,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 624,
        "contentLength": 15303
      },
      "nextCards": [
        "ai-ml-runtimes-flatbuffer-57",
        "ai-ml-runtimes-gguf-gpt-generated-ggml-unified-format-58"
      ],
      "relatedCards": [
        "ai-gpu-architecture-multi-gpu-memory-coherency-31",
        "ai-gpu-architecture-fundamental-architectural-components-amd-32",
        "ai-gpu-architecture-execution-paradigms-amd-33",
        "ai-gpu-architecture-compute-architecture-evolution-amd-cdna-rdna-34",
        "ai-gpu-architecture-memory-architecture-evolution-36"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#protocol-buffers-(protobuf)",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-flatbuffer-57",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Related: Serialization Formats Across Runtimes",
      "title": "FlatBuffer",
      "subtitle": "Related: Serialization Formats Across Runtimes",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Used by</strong>: TensorFlow Lite (<code class=\"language-plaintext highlighter-rouge\">.tflite</code>)</p>\n  </li>\n  <li>\n    <p><strong>Developed by</strong>: Google</p>\n  </li>\n  <li>\n    <p><strong>Type</strong>: Binary serialization library with zero-copy design</p>\n  </li>\n  <li>\n    <p><strong>Key Characteristics</strong>:</p>\n\n    <ul>\n      <li>Allows direct access to data without unpacking (zero-copy reads)</li>\n      <li>Compact binary representation optimized for low-latency parsing</li>\n      <li>Built-in schema evolution support</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Advantages</strong>:</p>\n\n    <ul>\n      <li>Near-instantaneous loading—no deserialization overhead</li>\n      <li>Perfect for mobile/embedded devices with tight latency or startup constraints</li>\n      <li>Schema-aware tooling for validation</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>In ML context</strong>:</p>\n\n    <ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">.tflite</code> files store computation graphs, tensors, and metadata using FlatBuffer encoding</li>\n      <li>Facilitates runtime interpretation without converting the graph into a different memory format</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Limitations</strong>:</p>\n\n    <ul>\n      <li>Harder to inspect/debug than JSON or Protobuf</li>\n      <li>Limited dynamic structure capabilities compared to Protobuf</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Example</strong>:</p>\n\n    <ul>\n      <li>\n        <p><em>Used in: TensorFlow Lite (<code class=\"language-plaintext highlighter-rouge\">.tflite</code>)</em></p>\n      </li>\n      <li>\n        <p>FlatBuffer does not require unpacking into memory. Instead, the graph is directly accessed as a binary blob using precompiled accessors.</p>\n      </li>\n      <li>\n        <p><strong>FlatBuffer Schema (simplified):</strong></p>\n\n        <pre><code class=\"language-idl\">  table Tensor {\n    shape: [int];\n    type: int;\n    buffer: int;\n  }\n\n  table Operator {\n    opcode_index: int;\n    inputs: [int];\n    outputs: [int];\n  }\n\n  table Model {\n    tensors: [Tensor];\n    operators: [Operator];\n  }\n</code></pre>\n      </li>\n      <li>\n        <p><strong>Example Python Usage:</strong></p>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code18\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code18\">  <span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n\n  <span class=\"n\">interpreter</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">lite</span><span class=\"p\">.</span><span class=\"n\">Interpreter</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"s\">\"mobilenet_v2.tflite\"</span><span class=\"p\">)</span>\n  <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">allocate_tensors</span><span class=\"p\">()</span>\n  <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_input_details</span><span class=\"p\">())</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n        <ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.tflite</code> file with FlatBuffer encoding, which includes all tensors, ops, and buffers in an efficient, zero-copy layout.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Used by</strong>: TensorFlow Lite (<code class=\"language-plaintext highlighter-rouge\">.tflite</code>)</p>\n<p><strong>Developed by</strong>: Google</p>\n<p><strong>Type</strong>: Binary serialization library with zero-copy design</p>\n<p><strong>Key Characteristics</strong>:</p>\n<ul>\n      <li>Allows direct access to data without unpacking (zero-copy reads)</li>\n      <li>Compact binary representation optimized for low-latency parsing</li>\n      <li>Built-in schema evolution support</li>\n    </ul>\n<p><strong>Advantages</strong>:</p>\n<ul>\n      <li>Near-instantaneous loading—no deserialization overhead</li>\n      <li>Perfect for mobile/embedded devices with tight latency or startup constraints</li>\n      <li>Schema-aware tooling for validation</li>\n    </ul>\n<p><strong>In ML context</strong>:</p>\n<ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">.tflite</code> files store computation graphs, tensors, and metadata using FlatBuffer encoding</li>\n      <li>Facilitates runtime interpretation without converting the graph into a different memory format</li>\n    </ul>\n<p><strong>Limitations</strong>:</p>\n<ul>\n      <li>Harder to inspect/debug than JSON or Protobuf</li>\n      <li>Limited dynamic structure capabilities compared to Protobuf</li>\n    </ul>\n<p><strong>Example</strong>:</p>\n<ul>\n      <li>\n        <p><em>Used in: TensorFlow Lite (<code class=\"language-plaintext highlighter-rouge\">.tflite</code>)</em></p>\n      </li>\n      <li>\n        <p>FlatBuffer does not require unpacking into memory. Instead, the graph is directly accessed as a binary blob using precompiled accessors.</p>\n      </li>\n      <li>\n        <p><strong>FlatBuffer Schema (simplified):</strong></p>\n\n        <pre><code class=\"language-idl\">  table Tensor {\n    shape: [int];\n    type: int;\n    buffer: int;\n  }\n\n  table Operator {\n    opcode_index: int;\n    inputs: [int];\n    outputs: [int];\n  }\n\n  table Model {\n    tensors: [Tensor];\n    operators: [Operator];\n  }\n</code></pre>\n      </li>\n      <li>\n        <p><strong>Example Python Usage:</strong></p>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code18\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code18\">  <span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n\n  <span class=\"n\">interpreter</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">lite</span><span class=\"p\">.</span><span class=\"n\">Interpreter</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"s\">\"mobilenet_v2.tflite\"</span><span class=\"p\">)</span>\n  <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">allocate_tensors</span><span class=\"p\">()</span>\n  <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_input_details</span><span class=\"p\">())</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n        <ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.tflite</code> file with FlatBuffer encoding, which includes all tensors, ops, and buffers in an efficient, zero-copy layout.</li>\n        </ul>\n      </li>\n    </ul>\n<p><em>Used in: TensorFlow Lite (<code class=\"language-plaintext highlighter-rouge\">.tflite</code>)</em></p>\n<p>FlatBuffer does not require unpacking into memory. Instead, the graph is directly accessed as a binary blob using precompiled accessors.</p>\n<p><strong>FlatBuffer Schema (simplified):</strong></p>\n<pre><code class=\"language-idl\">  table Tensor {\n    shape: [int];\n    type: int;\n    buffer: int;\n  }\n\n  table Operator {\n    opcode_index: int;\n    inputs: [int];\n    outputs: [int];\n  }\n\n  table Model {\n    tensors: [Tensor];\n    operators: [Operator];\n  }\n</code></pre>\n<p><strong>Example Python Usage:</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code18\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code18\">  <span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n\n  <span class=\"n\">interpreter</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">lite</span><span class=\"p\">.</span><span class=\"n\">Interpreter</span><span class=\"p\">(</span><span class=\"n\">model_path</span><span class=\"o\">=</span><span class=\"s\">\"mobilenet_v2.tflite\"</span><span class=\"p\">)</span>\n  <span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">allocate_tensors</span><span class=\"p\">()</span>\n  <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">interpreter</span><span class=\"p\">.</span><span class=\"n\">get_input_details</span><span class=\"p\">())</span>\n</code></pre>\n<p><strong>Serialized File:</strong></p>\n<ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.tflite</code> file with FlatBuffer encoding, which includes all tensors, ops, and buffers in an efficient, zero-copy layout.</li>\n        </ul>",
      "contentMarkdown": "*   **Used by**: TensorFlow Lite (`.tflite`)\n    \n*   **Developed by**: Google\n    \n*   **Type**: Binary serialization library with zero-copy design\n    \n*   **Key Characteristics**:\n    \n    *   Allows direct access to data without unpacking (zero-copy reads)\n    *   Compact binary representation optimized for low-latency parsing\n    *   Built-in schema evolution support\n*   **Advantages**:\n    \n    *   Near-instantaneous loading—no deserialization overhead\n    *   Perfect for mobile/embedded devices with tight latency or startup constraints\n    *   Schema-aware tooling for validation\n*   **In ML context**:\n    \n    *   `.tflite` files store computation graphs, tensors, and metadata using FlatBuffer encoding\n    *   Facilitates runtime interpretation without converting the graph into a different memory format\n*   **Limitations**:\n    \n    *   Harder to inspect/debug than JSON or Protobuf\n    *   Limited dynamic structure capabilities compared to Protobuf\n*   **Example**:\n    \n    *   _Used in: TensorFlow Lite (`.tflite`)_\n        \n    *   FlatBuffer does not require unpacking into memory. Instead, the graph is directly accessed as a binary blob using precompiled accessors.\n        \n    *   **FlatBuffer Schema (simplified):**\n        \n        ```idl\n          table Tensor {\n            shape: [int];\n            type: int;\n            buffer: int;\n          }\n        \n          table Operator {\n            opcode_index: int;\n            inputs: [int];\n            outputs: [int];\n          }\n        \n          table Model {\n            tensors: [Tensor];\n            operators: [Operator];\n          }\n        ```\n        \n    *   **Example Python Usage:**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `import tensorflow as tf    interpreter = tf.lite.Interpreter(model_path=\"mobilenet_v2.tflite\")   interpreter.allocate_tensors()   print(interpreter.get_input_details())`\n        \n    *   **Serialized File:**\n        \n        *   A `.tflite` file with FlatBuffer encoding, which includes all tensors, ops, and buffers in an efficient, zero-copy layout.\n\n**Used by**: TensorFlow Lite (`.tflite`)\n\n**Developed by**: Google\n\n**Type**: Binary serialization library with zero-copy design\n\n**Key Characteristics**:\n\n*   Allows direct access to data without unpacking (zero-copy reads)\n*   Compact binary representation optimized for low-latency parsing\n*   Built-in schema evolution support\n\n**Advantages**:\n\n*   Near-instantaneous loading—no deserialization overhead\n*   Perfect for mobile/embedded devices with tight latency or startup constraints\n*   Schema-aware tooling for validation\n\n**In ML context**:\n\n*   `.tflite` files store computation graphs, tensors, and metadata using FlatBuffer encoding\n*   Facilitates runtime interpretation without converting the graph into a different memory format\n\n**Limitations**:\n\n*   Harder to inspect/debug than JSON or Protobuf\n*   Limited dynamic structure capabilities compared to Protobuf\n\n**Example**:\n\n*   _Used in: TensorFlow Lite (`.tflite`)_\n    \n*   FlatBuffer does not require unpacking into memory. Instead, the graph is directly accessed as a binary blob using precompiled accessors.\n    \n*   **FlatBuffer Schema (simplified):**\n    \n    ```idl\n      table Tensor {\n        shape: [int];\n        type: int;\n        buffer: int;\n      }\n    \n      table Operator {\n        opcode_index: int;\n        inputs: [int];\n        outputs: [int];\n      }\n    \n      table Model {\n        tensors: [Tensor];\n        operators: [Operator];\n      }\n    ```\n    \n*   **Example Python Usage:**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `import tensorflow as tf    interpreter = tf.lite.Interpreter(model_path=\"mobilenet_v2.tflite\")   interpreter.allocate_tensors()   print(interpreter.get_input_details())`\n    \n*   **Serialized File:**\n    \n    *   A `.tflite` file with FlatBuffer encoding, which includes all tensors, ops, and buffers in an efficient, zero-copy layout.\n\n_Used in: TensorFlow Lite (`.tflite`)_\n\nFlatBuffer does not require unpacking into memory. Instead, the graph is directly accessed as a binary blob using precompiled accessors.\n\n**FlatBuffer Schema (simplified):**\n\n```idl\n  table Tensor {\n    shape: [int];\n    type: int;\n    buffer: int;\n  }\n\n  table Operator {\n    opcode_index: int;\n    inputs: [int];\n    outputs: [int];\n  }\n\n  table Model {\n    tensors: [Tensor];\n    operators: [Operator];\n  }\n```\n\n**Example Python Usage:**\n\n![](https://aman.ai/images/copy.png)\n\n  `import tensorflow as tf    interpreter = tf.lite.Interpreter(model_path=\"mobilenet_v2.tflite\")   interpreter.allocate_tensors()   print(interpreter.get_input_details())`\n\n**Serialized File:**\n\n*   A `.tflite` file with FlatBuffer encoding, which includes all tensors, ops, and buffers in an efficient, zero-copy layout.",
      "order": 57,
      "orderInChapter": 2,
      "difficulty": 5,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 510,
        "contentLength": 9244
      },
      "nextCards": [
        "ai-ml-runtimes-gguf-gpt-generated-ggml-unified-format-58",
        "ai-ml-runtimes-bytecode-format-executorch-59"
      ],
      "relatedCards": [
        "ai-gpu-architecture-multi-gpu-memory-coherency-31",
        "ai-gpu-architecture-fundamental-architectural-components-amd-32",
        "ai-gpu-architecture-execution-paradigms-amd-33",
        "ai-gpu-architecture-compute-architecture-evolution-amd-cdna-rdna-34",
        "ai-gpu-architecture-memory-architecture-evolution-36"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#flatbuffer",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-gguf-gpt-generated-ggml-unified-format-58",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Related: Serialization Formats Across Runtimes",
      "title": "GGUF (GPT-generated GGML Unified Format)",
      "subtitle": "Related: Serialization Formats Across Runtimes",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Used by</strong>: <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> and its LLM-compatible ecosystem</p>\n  </li>\n  <li>\n    <p><strong>Developed by</strong>: Community (successor to GGML model format)</p>\n  </li>\n  <li>\n    <p><strong>Type</strong>: Lightweight binary tensor format for large language models</p>\n  </li>\n  <li>\n    <p><strong>Key Characteristics</strong>:</p>\n\n    <ul>\n      <li>Encodes quantized transformer weights and architecture metadata</li>\n      <li>Designed for efficient memory mapping and low-RAM usage</li>\n      <li>Built for CPU-first inference (with optional GPU support)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Advantages</strong>:</p>\n\n    <ul>\n      <li>Extremely compact, especially with quantization (4–8 bit)</li>\n      <li>Simple, fast memory-mapped loading (<code class=\"language-plaintext highlighter-rouge\">mmap</code>)</li>\n      <li>Compatible with CPU-based inference engines (no dependencies)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>In ML context</strong>:</p>\n\n    <ul>\n      <li>Stores models like LLaMA, Mistral, Alpaca after quantization</li>\n      <li>Used by <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, <code class=\"language-plaintext highlighter-rouge\">llm.cpp</code>, <code class=\"language-plaintext highlighter-rouge\">text-generation-webui</code>, and other local LLM tools</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Limitations</strong>:</p>\n\n    <ul>\n      <li>Not general-purpose—only suitable for transformer LLMs</li>\n      <li>Lacks complex graph control (branching, dynamic ops)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Example</strong>:</p>\n\n    <ul>\n      <li>\n        <p>Used in: <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, quantized LLMs*</p>\n      </li>\n      <li>\n        <p>GGUF (GGML Unified Format) is a binary container for transformer weights and metadata.</p>\n      </li>\n      <li>\n        <p><strong>Header Block (example layout in binary format):</strong></p>\n\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code19\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code19\">  GGUF\n  version: 3\n  tensor_count: 397\n  metadata:\n    model_type: llama\n    vocab_size: 32000\n    quantization: Q4_0\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Python conversion (from PyTorch):</strong></p>\n\n        <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code20\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code20\">  python convert.py <span class=\"nt\">--input</span> model.bin <span class=\"nt\">--output</span> model.gguf <span class=\"nt\">--format</span> Q4_0\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Reading from <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>:</strong></p>\n\n        <div class=\"language-cpp highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code21\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code21\">  <span class=\"n\">gguf_context</span> <span class=\"o\">*</span><span class=\"n\">ctx</span> <span class=\"o\">=</span> <span class=\"n\">gguf_init_from_file</span><span class=\"p\">(</span><span class=\"s\">\"llama-7B.Q4_0.gguf\"</span><span class=\"p\">);</span>\n  <span class=\"n\">ggml_tensor</span> <span class=\"o\">*</span><span class=\"n\">wq</span> <span class=\"o\">=</span> <span class=\"n\">gguf_get_tensor_by_name</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"s\">\"layers.0.attn.wq\"</span><span class=\"p\">);</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n        <ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.gguf</code> file storing quantized tensors, model metadata, and attention layer structure—compact and mmap-compatible.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Used by</strong>: <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> and its LLM-compatible ecosystem</p>\n<p><strong>Developed by</strong>: Community (successor to GGML model format)</p>\n<p><strong>Type</strong>: Lightweight binary tensor format for large language models</p>\n<p><strong>Key Characteristics</strong>:</p>\n<ul>\n      <li>Encodes quantized transformer weights and architecture metadata</li>\n      <li>Designed for efficient memory mapping and low-RAM usage</li>\n      <li>Built for CPU-first inference (with optional GPU support)</li>\n    </ul>\n<p><strong>Advantages</strong>:</p>\n<ul>\n      <li>Extremely compact, especially with quantization (4–8 bit)</li>\n      <li>Simple, fast memory-mapped loading (<code class=\"language-plaintext highlighter-rouge\">mmap</code>)</li>\n      <li>Compatible with CPU-based inference engines (no dependencies)</li>\n    </ul>\n<p><strong>In ML context</strong>:</p>\n<ul>\n      <li>Stores models like LLaMA, Mistral, Alpaca after quantization</li>\n      <li>Used by <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, <code class=\"language-plaintext highlighter-rouge\">llm.cpp</code>, <code class=\"language-plaintext highlighter-rouge\">text-generation-webui</code>, and other local LLM tools</li>\n    </ul>\n<p><strong>Limitations</strong>:</p>\n<ul>\n      <li>Not general-purpose—only suitable for transformer LLMs</li>\n      <li>Lacks complex graph control (branching, dynamic ops)</li>\n    </ul>\n<p><strong>Example</strong>:</p>\n<ul>\n      <li>\n        <p>Used in: <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, quantized LLMs*</p>\n      </li>\n      <li>\n        <p>GGUF (GGML Unified Format) is a binary container for transformer weights and metadata.</p>\n      </li>\n      <li>\n        <p><strong>Header Block (example layout in binary format):</strong></p>\n\n        <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code19\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code19\">  GGUF\n  version: 3\n  tensor_count: 397\n  metadata:\n    model_type: llama\n    vocab_size: 32000\n    quantization: Q4_0\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Python conversion (from PyTorch):</strong></p>\n\n        <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code20\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code20\">  python convert.py <span class=\"nt\">--input</span> model.bin <span class=\"nt\">--output</span> model.gguf <span class=\"nt\">--format</span> Q4_0\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Reading from <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>:</strong></p>\n\n        <div class=\"language-cpp highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code21\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code21\">  <span class=\"n\">gguf_context</span> <span class=\"o\">*</span><span class=\"n\">ctx</span> <span class=\"o\">=</span> <span class=\"n\">gguf_init_from_file</span><span class=\"p\">(</span><span class=\"s\">\"llama-7B.Q4_0.gguf\"</span><span class=\"p\">);</span>\n  <span class=\"n\">ggml_tensor</span> <span class=\"o\">*</span><span class=\"n\">wq</span> <span class=\"o\">=</span> <span class=\"n\">gguf_get_tensor_by_name</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"s\">\"layers.0.attn.wq\"</span><span class=\"p\">);</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n        <ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.gguf</code> file storing quantized tensors, model metadata, and attention layer structure—compact and mmap-compatible.</li>\n        </ul>\n      </li>\n    </ul>\n<p>Used in: <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>, quantized LLMs*</p>\n<p>GGUF (GGML Unified Format) is a binary container for transformer weights and metadata.</p>\n<p><strong>Header Block (example layout in binary format):</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code19\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code19\">  GGUF\n  version: 3\n  tensor_count: 397\n  metadata:\n    model_type: llama\n    vocab_size: 32000\n    quantization: Q4_0\n</code></pre>\n<p><strong>Python conversion (from PyTorch):</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code20\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code20\">  python convert.py <span class=\"nt\">--input</span> model.bin <span class=\"nt\">--output</span> model.gguf <span class=\"nt\">--format</span> Q4_0\n</code></pre>\n<p><strong>Reading from <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>:</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code21\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code21\">  <span class=\"n\">gguf_context</span> <span class=\"o\">*</span><span class=\"n\">ctx</span> <span class=\"o\">=</span> <span class=\"n\">gguf_init_from_file</span><span class=\"p\">(</span><span class=\"s\">\"llama-7B.Q4_0.gguf\"</span><span class=\"p\">);</span>\n  <span class=\"n\">ggml_tensor</span> <span class=\"o\">*</span><span class=\"n\">wq</span> <span class=\"o\">=</span> <span class=\"n\">gguf_get_tensor_by_name</span><span class=\"p\">(</span><span class=\"n\">ctx</span><span class=\"p\">,</span> <span class=\"s\">\"layers.0.attn.wq\"</span><span class=\"p\">);</span>\n</code></pre>\n<p><strong>Serialized File:</strong></p>\n<ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.gguf</code> file storing quantized tensors, model metadata, and attention layer structure—compact and mmap-compatible.</li>\n        </ul>",
      "contentMarkdown": "*   **Used by**: `llama.cpp` and its LLM-compatible ecosystem\n    \n*   **Developed by**: Community (successor to GGML model format)\n    \n*   **Type**: Lightweight binary tensor format for large language models\n    \n*   **Key Characteristics**:\n    \n    *   Encodes quantized transformer weights and architecture metadata\n    *   Designed for efficient memory mapping and low-RAM usage\n    *   Built for CPU-first inference (with optional GPU support)\n*   **Advantages**:\n    \n    *   Extremely compact, especially with quantization (4–8 bit)\n    *   Simple, fast memory-mapped loading (`mmap`)\n    *   Compatible with CPU-based inference engines (no dependencies)\n*   **In ML context**:\n    \n    *   Stores models like LLaMA, Mistral, Alpaca after quantization\n    *   Used by `llama.cpp`, `llm.cpp`, `text-generation-webui`, and other local LLM tools\n*   **Limitations**:\n    \n    *   Not general-purpose—only suitable for transformer LLMs\n    *   Lacks complex graph control (branching, dynamic ops)\n*   **Example**:\n    \n    *   Used in: `llama.cpp`, quantized LLMs\\*\n        \n    *   GGUF (GGML Unified Format) is a binary container for transformer weights and metadata.\n        \n    *   **Header Block (example layout in binary format):**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `GGUF   version: 3   tensor_count: 397   metadata:     model_type: llama     vocab_size: 32000     quantization: Q4_0`\n        \n    *   **Python conversion (from PyTorch):**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `python convert.py --input model.bin --output model.gguf --format Q4_0`\n        \n    *   **Reading from `llama.cpp`:**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `gguf_context *ctx = gguf_init_from_file(\"llama-7B.Q4_0.gguf\");   ggml_tensor *wq = gguf_get_tensor_by_name(ctx, \"layers.0.attn.wq\");`\n        \n    *   **Serialized File:**\n        \n        *   A `.gguf` file storing quantized tensors, model metadata, and attention layer structure—compact and mmap-compatible.\n\n**Used by**: `llama.cpp` and its LLM-compatible ecosystem\n\n**Developed by**: Community (successor to GGML model format)\n\n**Type**: Lightweight binary tensor format for large language models\n\n**Key Characteristics**:\n\n*   Encodes quantized transformer weights and architecture metadata\n*   Designed for efficient memory mapping and low-RAM usage\n*   Built for CPU-first inference (with optional GPU support)\n\n**Advantages**:\n\n*   Extremely compact, especially with quantization (4–8 bit)\n*   Simple, fast memory-mapped loading (`mmap`)\n*   Compatible with CPU-based inference engines (no dependencies)\n\n**In ML context**:\n\n*   Stores models like LLaMA, Mistral, Alpaca after quantization\n*   Used by `llama.cpp`, `llm.cpp`, `text-generation-webui`, and other local LLM tools\n\n**Limitations**:\n\n*   Not general-purpose—only suitable for transformer LLMs\n*   Lacks complex graph control (branching, dynamic ops)\n\n**Example**:\n\n*   Used in: `llama.cpp`, quantized LLMs\\*\n    \n*   GGUF (GGML Unified Format) is a binary container for transformer weights and metadata.\n    \n*   **Header Block (example layout in binary format):**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `GGUF   version: 3   tensor_count: 397   metadata:     model_type: llama     vocab_size: 32000     quantization: Q4_0`\n    \n*   **Python conversion (from PyTorch):**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `python convert.py --input model.bin --output model.gguf --format Q4_0`\n    \n*   **Reading from `llama.cpp`:**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `gguf_context *ctx = gguf_init_from_file(\"llama-7B.Q4_0.gguf\");   ggml_tensor *wq = gguf_get_tensor_by_name(ctx, \"layers.0.attn.wq\");`\n    \n*   **Serialized File:**\n    \n    *   A `.gguf` file storing quantized tensors, model metadata, and attention layer structure—compact and mmap-compatible.\n\nUsed in: `llama.cpp`, quantized LLMs\\*\n\nGGUF (GGML Unified Format) is a binary container for transformer weights and metadata.\n\n**Header Block (example layout in binary format):**\n\n![](https://aman.ai/images/copy.png)\n\n  `GGUF   version: 3   tensor_count: 397   metadata:     model_type: llama     vocab_size: 32000     quantization: Q4_0`\n\n**Python conversion (from PyTorch):**\n\n![](https://aman.ai/images/copy.png)\n\n  `python convert.py --input model.bin --output model.gguf --format Q4_0`\n\n**Reading from `llama.cpp`:**\n\n![](https://aman.ai/images/copy.png)\n\n  `gguf_context *ctx = gguf_init_from_file(\"llama-7B.Q4_0.gguf\");   ggml_tensor *wq = gguf_get_tensor_by_name(ctx, \"layers.0.attn.wq\");`\n\n**Serialized File:**\n\n*   A `.gguf` file storing quantized tensors, model metadata, and attention layer structure—compact and mmap-compatible.",
      "order": 58,
      "orderInChapter": 3,
      "difficulty": 5,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous",
        "transformer",
        "attention",
        "llm"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 493,
        "contentLength": 11325
      },
      "nextCards": [
        "ai-ml-runtimes-bytecode-format-executorch-59",
        "ai-ml-runtimes-comparative-analysis-60"
      ],
      "relatedCards": [
        "ai-gpu-architecture-tensor-core-evolution-23",
        "ai-gpu-architecture-key-takeaways-38",
        "ai-top-30-papers-better-faster-large-language-models-via-multi-toke-27",
        "ai-gpu-architecture-per-generation-precision-support-and-performance-26",
        "ai-gpu-architecture-floating-point-precision-performance-evolution-35"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#gguf-(gpt-generated-ggml-unified-format)",
      "scrapedAt": "2025-12-28T11:56:51.672Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-bytecode-format-executorch-59",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Related: Serialization Formats Across Runtimes",
      "title": "Bytecode Format (ExecuTorch)",
      "subtitle": "Related: Serialization Formats Across Runtimes",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Used by</strong>: ExecuTorch</p>\n  </li>\n  <li>\n    <p><strong>Developed by</strong>: Meta</p>\n  </li>\n  <li>\n    <p><strong>Type</strong>: Custom AOT-compiled bytecode</p>\n  </li>\n  <li>\n    <p><strong>Key Characteristics</strong>:</p>\n\n    <ul>\n      <li>Outputs compact bytecode (<code class=\"language-plaintext highlighter-rouge\">.ptc</code>) from PyTorch models via TorchScript tracing</li>\n      <li>Prunes unused operators to reduce binary size</li>\n      <li>Embeds minimal op metadata needed for runtime VM</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Advantages</strong>:</p>\n\n    <ul>\n      <li>Highly portable and minimal—can run on MCUs and RTOS platforms</li>\n      <li>Deterministic memory usage and low overhead</li>\n      <li>Enables static linking of models and kernels for bare-metal systems</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>In ML context</strong>:</p>\n\n    <ul>\n      <li>Targets constrained devices (sub-MB RAM)</li>\n      <li>Supports fixed operator sets with predictable memory and runtime behavior</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Limitations</strong>:</p>\n\n    <ul>\n      <li>Rigid format—not well suited for dynamic models or rich graph structures</li>\n      <li>Tied closely to PyTorch tracing and compilation pipeline.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Example</strong>:</p>\n\n    <ul>\n      <li>\n        <p><em>Used in: ExecuTorch (<code class=\"language-plaintext highlighter-rouge\">.ptc</code> format)</em></p>\n      </li>\n      <li>\n        <p>ExecuTorch compiles PyTorch models into bytecode similar to a virtual machine instruction set.</p>\n      </li>\n      <li>\n        <p><strong>Model Compilation:</strong></p>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code22\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code22\">  <span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n  <span class=\"k\">class</span> <span class=\"nc\">Net</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n      <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n          <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n  <span class=\"n\">scripted</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">jit</span><span class=\"p\">.</span><span class=\"n\">script</span><span class=\"p\">(</span><span class=\"n\">Net</span><span class=\"p\">())</span>\n  <span class=\"n\">scripted</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"net.pt\"</span><span class=\"p\">)</span>  <span class=\"c1\"># TorchScript\n</span>\n  <span class=\"c1\"># Compile to ExecuTorch format\n</span>  <span class=\"err\">!</span><span class=\"n\">executorchc</span> <span class=\"nb\">compile</span> <span class=\"o\">--</span><span class=\"n\">model</span> <span class=\"n\">net</span><span class=\"p\">.</span><span class=\"n\">pt</span> <span class=\"o\">--</span><span class=\"n\">output</span> <span class=\"n\">net</span><span class=\"p\">.</span><span class=\"n\">ptc</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Runtime Use in C++:</strong></p>\n\n        <div class=\"language-cpp highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code23\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code23\">  <span class=\"n\">executorch</span><span class=\"o\">::</span><span class=\"n\">Runtime</span> <span class=\"n\">runtime</span><span class=\"p\">;</span>\n  <span class=\"n\">runtime</span><span class=\"p\">.</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s\">\"net.ptc\"</span><span class=\"p\">);</span>\n  <span class=\"n\">runtime</span><span class=\"p\">.</span><span class=\"n\">invoke</span><span class=\"p\">(</span><span class=\"n\">input_tensor</span><span class=\"p\">,</span> <span class=\"n\">output_tensor</span><span class=\"p\">);</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n        <ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.ptc</code> file containing static bytecode for model logic, stripped of unused ops, ready for microcontroller inference.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Used by</strong>: ExecuTorch</p>\n<p><strong>Developed by</strong>: Meta</p>\n<p><strong>Type</strong>: Custom AOT-compiled bytecode</p>\n<p><strong>Key Characteristics</strong>:</p>\n<ul>\n      <li>Outputs compact bytecode (<code class=\"language-plaintext highlighter-rouge\">.ptc</code>) from PyTorch models via TorchScript tracing</li>\n      <li>Prunes unused operators to reduce binary size</li>\n      <li>Embeds minimal op metadata needed for runtime VM</li>\n    </ul>\n<p><strong>Advantages</strong>:</p>\n<ul>\n      <li>Highly portable and minimal—can run on MCUs and RTOS platforms</li>\n      <li>Deterministic memory usage and low overhead</li>\n      <li>Enables static linking of models and kernels for bare-metal systems</li>\n    </ul>\n<p><strong>In ML context</strong>:</p>\n<ul>\n      <li>Targets constrained devices (sub-MB RAM)</li>\n      <li>Supports fixed operator sets with predictable memory and runtime behavior</li>\n    </ul>\n<p><strong>Limitations</strong>:</p>\n<ul>\n      <li>Rigid format—not well suited for dynamic models or rich graph structures</li>\n      <li>Tied closely to PyTorch tracing and compilation pipeline.</li>\n    </ul>\n<p><strong>Example</strong>:</p>\n<ul>\n      <li>\n        <p><em>Used in: ExecuTorch (<code class=\"language-plaintext highlighter-rouge\">.ptc</code> format)</em></p>\n      </li>\n      <li>\n        <p>ExecuTorch compiles PyTorch models into bytecode similar to a virtual machine instruction set.</p>\n      </li>\n      <li>\n        <p><strong>Model Compilation:</strong></p>\n\n        <div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code22\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code22\">  <span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n  <span class=\"k\">class</span> <span class=\"nc\">Net</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n      <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n          <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n  <span class=\"n\">scripted</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">jit</span><span class=\"p\">.</span><span class=\"n\">script</span><span class=\"p\">(</span><span class=\"n\">Net</span><span class=\"p\">())</span>\n  <span class=\"n\">scripted</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"net.pt\"</span><span class=\"p\">)</span>  <span class=\"c1\"># TorchScript\n</span>\n  <span class=\"c1\"># Compile to ExecuTorch format\n</span>  <span class=\"err\">!</span><span class=\"n\">executorchc</span> <span class=\"nb\">compile</span> <span class=\"o\">--</span><span class=\"n\">model</span> <span class=\"n\">net</span><span class=\"p\">.</span><span class=\"n\">pt</span> <span class=\"o\">--</span><span class=\"n\">output</span> <span class=\"n\">net</span><span class=\"p\">.</span><span class=\"n\">ptc</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Runtime Use in C++:</strong></p>\n\n        <div class=\"language-cpp highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code23\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code23\">  <span class=\"n\">executorch</span><span class=\"o\">::</span><span class=\"n\">Runtime</span> <span class=\"n\">runtime</span><span class=\"p\">;</span>\n  <span class=\"n\">runtime</span><span class=\"p\">.</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s\">\"net.ptc\"</span><span class=\"p\">);</span>\n  <span class=\"n\">runtime</span><span class=\"p\">.</span><span class=\"n\">invoke</span><span class=\"p\">(</span><span class=\"n\">input_tensor</span><span class=\"p\">,</span> <span class=\"n\">output_tensor</span><span class=\"p\">);</span>\n</code></pre></div>        </div>\n      </li>\n      <li>\n        <p><strong>Serialized File:</strong></p>\n        <ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.ptc</code> file containing static bytecode for model logic, stripped of unused ops, ready for microcontroller inference.</li>\n        </ul>\n      </li>\n    </ul>\n<p><em>Used in: ExecuTorch (<code class=\"language-plaintext highlighter-rouge\">.ptc</code> format)</em></p>\n<p>ExecuTorch compiles PyTorch models into bytecode similar to a virtual machine instruction set.</p>\n<p><strong>Model Compilation:</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code22\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code22\">  <span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n\n  <span class=\"k\">class</span> <span class=\"nc\">Net</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">nn</span><span class=\"p\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n      <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n          <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n  <span class=\"n\">scripted</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"p\">.</span><span class=\"n\">jit</span><span class=\"p\">.</span><span class=\"n\">script</span><span class=\"p\">(</span><span class=\"n\">Net</span><span class=\"p\">())</span>\n  <span class=\"n\">scripted</span><span class=\"p\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"s\">\"net.pt\"</span><span class=\"p\">)</span>  <span class=\"c1\"># TorchScript\n</span>\n  <span class=\"c1\"># Compile to ExecuTorch format\n</span>  <span class=\"err\">!</span><span class=\"n\">executorchc</span> <span class=\"nb\">compile</span> <span class=\"o\">--</span><span class=\"n\">model</span> <span class=\"n\">net</span><span class=\"p\">.</span><span class=\"n\">pt</span> <span class=\"o\">--</span><span class=\"n\">output</span> <span class=\"n\">net</span><span class=\"p\">.</span><span class=\"n\">ptc</span>\n</code></pre>\n<p><strong>Runtime Use in C++:</strong></p>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code23\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code23\">  <span class=\"n\">executorch</span><span class=\"o\">::</span><span class=\"n\">Runtime</span> <span class=\"n\">runtime</span><span class=\"p\">;</span>\n  <span class=\"n\">runtime</span><span class=\"p\">.</span><span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"s\">\"net.ptc\"</span><span class=\"p\">);</span>\n  <span class=\"n\">runtime</span><span class=\"p\">.</span><span class=\"n\">invoke</span><span class=\"p\">(</span><span class=\"n\">input_tensor</span><span class=\"p\">,</span> <span class=\"n\">output_tensor</span><span class=\"p\">);</span>\n</code></pre>\n<p><strong>Serialized File:</strong></p>\n<ul>\n          <li>A <code class=\"language-plaintext highlighter-rouge\">.ptc</code> file containing static bytecode for model logic, stripped of unused ops, ready for microcontroller inference.</li>\n        </ul>",
      "contentMarkdown": "*   **Used by**: ExecuTorch\n    \n*   **Developed by**: Meta\n    \n*   **Type**: Custom AOT-compiled bytecode\n    \n*   **Key Characteristics**:\n    \n    *   Outputs compact bytecode (`.ptc`) from PyTorch models via TorchScript tracing\n    *   Prunes unused operators to reduce binary size\n    *   Embeds minimal op metadata needed for runtime VM\n*   **Advantages**:\n    \n    *   Highly portable and minimal—can run on MCUs and RTOS platforms\n    *   Deterministic memory usage and low overhead\n    *   Enables static linking of models and kernels for bare-metal systems\n*   **In ML context**:\n    \n    *   Targets constrained devices (sub-MB RAM)\n    *   Supports fixed operator sets with predictable memory and runtime behavior\n*   **Limitations**:\n    \n    *   Rigid format—not well suited for dynamic models or rich graph structures\n    *   Tied closely to PyTorch tracing and compilation pipeline.\n*   **Example**:\n    \n    *   _Used in: ExecuTorch (`.ptc` format)_\n        \n    *   ExecuTorch compiles PyTorch models into bytecode similar to a virtual machine instruction set.\n        \n    *   **Model Compilation:**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `import torch    class Net(torch.nn.Module):       def forward(self, x):           return torch.relu(x)    scripted = torch.jit.script(Net())   scripted.save(\"net.pt\")  # TorchScript   # Compile to ExecuTorch format   !executorchc compile --model net.pt --output net.ptc`\n        \n    *   **Runtime Use in C++:**\n        \n        ![](https://aman.ai/images/copy.png)\n        \n          `executorch::Runtime runtime;   runtime.load_model(\"net.ptc\");   runtime.invoke(input_tensor, output_tensor);`\n        \n    *   **Serialized File:**\n        \n        *   A `.ptc` file containing static bytecode for model logic, stripped of unused ops, ready for microcontroller inference.\n\n**Used by**: ExecuTorch\n\n**Developed by**: Meta\n\n**Type**: Custom AOT-compiled bytecode\n\n**Key Characteristics**:\n\n*   Outputs compact bytecode (`.ptc`) from PyTorch models via TorchScript tracing\n*   Prunes unused operators to reduce binary size\n*   Embeds minimal op metadata needed for runtime VM\n\n**Advantages**:\n\n*   Highly portable and minimal—can run on MCUs and RTOS platforms\n*   Deterministic memory usage and low overhead\n*   Enables static linking of models and kernels for bare-metal systems\n\n**In ML context**:\n\n*   Targets constrained devices (sub-MB RAM)\n*   Supports fixed operator sets with predictable memory and runtime behavior\n\n**Limitations**:\n\n*   Rigid format—not well suited for dynamic models or rich graph structures\n*   Tied closely to PyTorch tracing and compilation pipeline.\n\n**Example**:\n\n*   _Used in: ExecuTorch (`.ptc` format)_\n    \n*   ExecuTorch compiles PyTorch models into bytecode similar to a virtual machine instruction set.\n    \n*   **Model Compilation:**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `import torch    class Net(torch.nn.Module):       def forward(self, x):           return torch.relu(x)    scripted = torch.jit.script(Net())   scripted.save(\"net.pt\")  # TorchScript   # Compile to ExecuTorch format   !executorchc compile --model net.pt --output net.ptc`\n    \n*   **Runtime Use in C++:**\n    \n    ![](https://aman.ai/images/copy.png)\n    \n      `executorch::Runtime runtime;   runtime.load_model(\"net.ptc\");   runtime.invoke(input_tensor, output_tensor);`\n    \n*   **Serialized File:**\n    \n    *   A `.ptc` file containing static bytecode for model logic, stripped of unused ops, ready for microcontroller inference.\n\n_Used in: ExecuTorch (`.ptc` format)_\n\nExecuTorch compiles PyTorch models into bytecode similar to a virtual machine instruction set.\n\n**Model Compilation:**\n\n![](https://aman.ai/images/copy.png)\n\n  `import torch    class Net(torch.nn.Module):       def forward(self, x):           return torch.relu(x)    scripted = torch.jit.script(Net())   scripted.save(\"net.pt\")  # TorchScript   # Compile to ExecuTorch format   !executorchc compile --model net.pt --output net.ptc`\n\n**Runtime Use in C++:**\n\n![](https://aman.ai/images/copy.png)\n\n  `executorch::Runtime runtime;   runtime.load_model(\"net.ptc\");   runtime.invoke(input_tensor, output_tensor);`\n\n**Serialized File:**\n\n*   A `.ptc` file containing static bytecode for model logic, stripped of unused ops, ready for microcontroller inference.",
      "order": 59,
      "orderInChapter": 4,
      "difficulty": 5,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 475,
        "contentLength": 13069
      },
      "nextCards": [
        "ai-ml-runtimes-comparative-analysis-60",
        "ai-ml-runtimes-general-workflow-from-model-to-inference-61"
      ],
      "relatedCards": [
        "ai-gpu-architecture-multi-gpu-memory-coherency-31",
        "ai-gpu-architecture-fundamental-architectural-components-amd-32",
        "ai-gpu-architecture-execution-paradigms-amd-33",
        "ai-gpu-architecture-compute-architecture-evolution-amd-cdna-rdna-34",
        "ai-gpu-architecture-memory-architecture-evolution-36"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#bytecode-format-(executorch)",
      "scrapedAt": "2025-12-28T11:56:51.673Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-comparative-analysis-60",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Related: Serialization Formats Across Runtimes",
      "title": "Comparative Analysis",
      "subtitle": "Related: Serialization Formats Across Runtimes",
      "contentHtml": "<ul>\n  <li>Understanding the serialization format is crucial when choosing a runtime—especially for performance, portability, and debugging. Developers targeting mobile and embedded environments often prefer FlatBuffer or bytecode for efficiency, while cloud/server or cross-platform projects benefit from Protobuf’s rich graph encoding.</li>\n</ul>\n<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Format</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Used By</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Format Type</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Example File</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Viewability</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Tool to Inspect</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Strengths</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Limitations</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Protobuf</td>\n<td class=\"tg-tleft-valign-first\">TensorFlow, ONNX</td>\n<td class=\"tg-tleft-valign-first\">Binary (schema-driven)</td>\n<td class=\"tg-tleft-valign-first\"><code>model.onnx</code>, <code>model.pb</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>onnx</code>, <code>tf.saved_model_cli</code></td>\n<td class=\"tg-tleft-valign-first\">Cross-platform, schema evolution, rich structure</td>\n<td class=\"tg-tleft-valign-second\">Larger footprint, full deserialization</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FlatBuffer</td>\n<td class=\"tg-tleft-valign-first\">TensorFlow Lite</td>\n<td class=\"tg-tleft-valign-first\">Zero-copy binary</td>\n<td class=\"tg-tleft-valign-first\"><code>model.tflite</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>flatc</code>, <code>tflite</code> API</td>\n<td class=\"tg-tleft-valign-first\">Instant loading, ideal for embedded use</td>\n<td class=\"tg-tleft-valign-second\">Harder to inspect/debug</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">GGUF</td>\n<td class=\"tg-tleft-valign-first\"><code>llama.cpp</code></td>\n<td class=\"tg-tleft-valign-first\">Binary tensor map</td>\n<td class=\"tg-tleft-valign-first\"><code>llama-7B.Q4_0.gguf</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>llama.cpp</code>, <code>gguf_dump.py</code></td>\n<td class=\"tg-tleft-valign-first\">Ultra-compact, mmap-friendly, quantized</td>\n<td class=\"tg-tleft-valign-second\">LLM-specific only</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Bytecode</td>\n<td class=\"tg-tleft-valign-first\">ExecuTorch</td>\n<td class=\"tg-tleft-valign-first\">Compiled AOT VM</td>\n<td class=\"tg-tleft-valign-first\"><code>model.ptc</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>executorchc</code>, ExecuTorch API</td>\n<td class=\"tg-tleft-valign-first\">Tiny runtime, embedded-friendly</td>\n<td class=\"tg-tleft-valign-second\">Limited flexibility, PyTorch-only</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">TensorRT Engine</td>\n<td class=\"tg-tleft-valign-first\">TensorRT</td>\n<td class=\"tg-tleft-valign-first\">Binary CUDA engine</td>\n<td class=\"tg-tleft-valign-first\"><code>model.plan</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\">TensorRT API (<code>trtexec</code>)</td>\n<td class=\"tg-tleft-valign-first\">Hardware-optimized, precompiled inference</td>\n<td class=\"tg-tleft-valign-second\">NVIDIA-only, not portable</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Format</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Used By</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Format Type</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Example File</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Viewability</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Tool to Inspect</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Strengths</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Limitations</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Protobuf</td>\n<td class=\"tg-tleft-valign-first\">TensorFlow, ONNX</td>\n<td class=\"tg-tleft-valign-first\">Binary (schema-driven)</td>\n<td class=\"tg-tleft-valign-first\"><code>model.onnx</code>, <code>model.pb</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>onnx</code>, <code>tf.saved_model_cli</code></td>\n<td class=\"tg-tleft-valign-first\">Cross-platform, schema evolution, rich structure</td>\n<td class=\"tg-tleft-valign-second\">Larger footprint, full deserialization</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FlatBuffer</td>\n<td class=\"tg-tleft-valign-first\">TensorFlow Lite</td>\n<td class=\"tg-tleft-valign-first\">Zero-copy binary</td>\n<td class=\"tg-tleft-valign-first\"><code>model.tflite</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>flatc</code>, <code>tflite</code> API</td>\n<td class=\"tg-tleft-valign-first\">Instant loading, ideal for embedded use</td>\n<td class=\"tg-tleft-valign-second\">Harder to inspect/debug</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">GGUF</td>\n<td class=\"tg-tleft-valign-first\"><code>llama.cpp</code></td>\n<td class=\"tg-tleft-valign-first\">Binary tensor map</td>\n<td class=\"tg-tleft-valign-first\"><code>llama-7B.Q4_0.gguf</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>llama.cpp</code>, <code>gguf_dump.py</code></td>\n<td class=\"tg-tleft-valign-first\">Ultra-compact, mmap-friendly, quantized</td>\n<td class=\"tg-tleft-valign-second\">LLM-specific only</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Bytecode</td>\n<td class=\"tg-tleft-valign-first\">ExecuTorch</td>\n<td class=\"tg-tleft-valign-first\">Compiled AOT VM</td>\n<td class=\"tg-tleft-valign-first\"><code>model.ptc</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\"><code>executorchc</code>, ExecuTorch API</td>\n<td class=\"tg-tleft-valign-first\">Tiny runtime, embedded-friendly</td>\n<td class=\"tg-tleft-valign-second\">Limited flexibility, PyTorch-only</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">TensorRT Engine</td>\n<td class=\"tg-tleft-valign-first\">TensorRT</td>\n<td class=\"tg-tleft-valign-first\">Binary CUDA engine</td>\n<td class=\"tg-tleft-valign-first\"><code>model.plan</code></td>\n<td class=\"tg-tleft-valign-first\">Binary</td>\n<td class=\"tg-tleft-valign-first\">TensorRT API (<code>trtexec</code>)</td>\n<td class=\"tg-tleft-valign-first\">Hardware-optimized, precompiled inference</td>\n<td class=\"tg-tleft-valign-second\">NVIDIA-only, not portable</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "*   Understanding the serialization format is crucial when choosing a runtime—especially for performance, portability, and debugging. Developers targeting mobile and embedded environments often prefer FlatBuffer or bytecode for efficiency, while cloud/server or cross-platform projects benefit from Protobuf’s rich graph encoding.\n\n**Format**\n\n**Used By**\n\n**Format Type**\n\n**Example File**\n\n**Viewability**\n\n**Tool to Inspect**\n\n**Strengths**\n\n**Limitations**\n\nProtobuf\n\nTensorFlow, ONNX\n\nBinary (schema-driven)\n\n`model.onnx`, `model.pb`\n\nBinary\n\n`onnx`, `tf.saved_model_cli`\n\nCross-platform, schema evolution, rich structure\n\nLarger footprint, full deserialization\n\nFlatBuffer\n\nTensorFlow Lite\n\nZero-copy binary\n\n`model.tflite`\n\nBinary\n\n`flatc`, `tflite` API\n\nInstant loading, ideal for embedded use\n\nHarder to inspect/debug\n\nGGUF\n\n`llama.cpp`\n\nBinary tensor map\n\n`llama-7B.Q4_0.gguf`\n\nBinary\n\n`llama.cpp`, `gguf_dump.py`\n\nUltra-compact, mmap-friendly, quantized\n\nLLM-specific only\n\nBytecode\n\nExecuTorch\n\nCompiled AOT VM\n\n`model.ptc`\n\nBinary\n\n`executorchc`, ExecuTorch API\n\nTiny runtime, embedded-friendly\n\nLimited flexibility, PyTorch-only\n\nTensorRT Engine\n\nTensorRT\n\nBinary CUDA engine\n\n`model.plan`\n\nBinary\n\nTensorRT API (`trtexec`)\n\nHardware-optimized, precompiled inference\n\nNVIDIA-only, not portable\n\n**Format**\n\n**Used By**\n\n**Format Type**\n\n**Example File**\n\n**Viewability**\n\n**Tool to Inspect**\n\n**Strengths**\n\n**Limitations**\n\nProtobuf\n\nTensorFlow, ONNX\n\nBinary (schema-driven)\n\n`model.onnx`, `model.pb`\n\nBinary\n\n`onnx`, `tf.saved_model_cli`\n\nCross-platform, schema evolution, rich structure\n\nLarger footprint, full deserialization\n\nFlatBuffer\n\nTensorFlow Lite\n\nZero-copy binary\n\n`model.tflite`\n\nBinary\n\n`flatc`, `tflite` API\n\nInstant loading, ideal for embedded use\n\nHarder to inspect/debug\n\nGGUF\n\n`llama.cpp`\n\nBinary tensor map\n\n`llama-7B.Q4_0.gguf`\n\nBinary\n\n`llama.cpp`, `gguf_dump.py`\n\nUltra-compact, mmap-friendly, quantized\n\nLLM-specific only\n\nBytecode\n\nExecuTorch\n\nCompiled AOT VM\n\n`model.ptc`\n\nBinary\n\n`executorchc`, ExecuTorch API\n\nTiny runtime, embedded-friendly\n\nLimited flexibility, PyTorch-only\n\nTensorRT Engine\n\nTensorRT\n\nBinary CUDA engine\n\n`model.plan`\n\nBinary\n\nTensorRT API (`trtexec`)\n\nHardware-optimized, precompiled inference\n\nNVIDIA-only, not portable",
      "order": 60,
      "orderInChapter": 5,
      "difficulty": 5,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "llm"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 236,
        "contentLength": 6816
      },
      "nextCards": [
        "ai-ml-runtimes-general-workflow-from-model-to-inference-61",
        "ai-ml-runtimes-runtime-specific-execution-lifecycles-62"
      ],
      "relatedCards": [
        "ai-top-30-papers-alcuna-large-language-models-meet-new-knowledge-33",
        "ai-top-30-papers-the-perils-promises-of-fact-checking-with-large-la-34",
        "ai-gpu-architecture-tensor-core-evolution-23",
        "ai-gpu-architecture-key-takeaways-38",
        "ai-gpu-architecture-per-generation-precision-support-and-performance-26"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#comparative-analysis",
      "scrapedAt": "2025-12-28T11:56:51.673Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-general-workflow-from-model-to-inference-61",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Model Execution Lifecycle Across ML Runtimes",
      "title": "General Workflow: from Model to Inference",
      "subtitle": "Model Execution Lifecycle Across ML Runtimes",
      "contentHtml": "<h4 id=\"model-training\">Model Training</h4>\n<ul>\n  <li>\n    <p>Although training is typically performed in a full ML framework (e.g., PyTorch, TensorFlow), it is critical to know that the trained model must be <strong>exported or converted</strong> into a format compatible with the intended runtime.</p>\n  </li>\n  <li>\n    <p>This stage outputs:</p>\n\n    <ul>\n      <li>A trained model file (e.g., <code class=\"language-plaintext highlighter-rouge\">.onnx</code>, <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code>, <code class=\"language-plaintext highlighter-rouge\">.pt</code>, <code class=\"language-plaintext highlighter-rouge\">.tflite</code>, etc.)</li>\n      <li>Associated metadata (input/output shapes, quantization info, etc.)</li>\n    </ul>\n  </li>\n</ul>\n<p>Although training is typically performed in a full ML framework (e.g., PyTorch, TensorFlow), it is critical to know that the trained model must be <strong>exported or converted</strong> into a format compatible with the intended runtime.</p>\n<p>This stage outputs:</p>\n<ul>\n      <li>A trained model file (e.g., <code class=\"language-plaintext highlighter-rouge\">.onnx</code>, <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code>, <code class=\"language-plaintext highlighter-rouge\">.pt</code>, <code class=\"language-plaintext highlighter-rouge\">.tflite</code>, etc.)</li>\n      <li>Associated metadata (input/output shapes, quantization info, etc.)</li>\n    </ul>\n<h4 id=\"model-conversion\">Model Conversion</h4>\n<ul>\n  <li>\n    <p>This phase adapts the trained model into a runtime-specific format. Conversion tools may also apply <strong>graph simplification</strong>, <strong>quantization</strong>, or <strong>operator fusion</strong>.</p>\n  </li>\n  <li>\n    <p>Typical tools used:</p>\n\n    <ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">torch.onnx.export()</code> (PyTorch <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-13\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mo\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">\\rightarrow</script> ONNX)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">coremltools.convert()</code> (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-16\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-17\"><span class=\"mo\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">\\rightarrow</script> Core ML)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code> (TensorFlow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-19\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-20\"><span class=\"mo\" id=\"MathJax-Span-21\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">.tflite</code>)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">executorchc</code> (TorchScript <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-22\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-23\"><span class=\"mo\" id=\"MathJax-Span-24\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">\\rightarrow</script> ExecuTorch bytecode)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">quantize.py</code> (for GGUF / <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>)</li>\n    </ul>\n  </li>\n  <li>\n    <p>This phase outputs:</p>\n\n    <ul>\n      <li>Serialized model file tailored for the target runtime</li>\n      <li>Optional quantized or optimized variant</li>\n    </ul>\n  </li>\n</ul>\n<p>This phase adapts the trained model into a runtime-specific format. Conversion tools may also apply <strong>graph simplification</strong>, <strong>quantization</strong>, or <strong>operator fusion</strong>.</p>\n<p>Typical tools used:</p>\n<ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">torch.onnx.export()</code> (PyTorch <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-13\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mo\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">\\rightarrow</script> ONNX)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">coremltools.convert()</code> (<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-16\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-17\"><span class=\"mo\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">\\rightarrow</script> Core ML)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code> (TensorFlow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-19\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-20\"><span class=\"mo\" id=\"MathJax-Span-21\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">.tflite</code>)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">executorchc</code> (TorchScript <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-22\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-23\"><span class=\"mo\" id=\"MathJax-Span-24\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">\\rightarrow</script> ExecuTorch bytecode)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">quantize.py</code> (for GGUF / <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>)</li>\n    </ul>\n<p>This phase outputs:</p>\n<ul>\n      <li>Serialized model file tailored for the target runtime</li>\n      <li>Optional quantized or optimized variant</li>\n    </ul>\n<h4 id=\"model-loading\">Model Loading</h4>\n<ul>\n  <li>\n    <p>At runtime, the model file is loaded and deserialized into memory. Runtimes may parse the file into:</p>\n\n    <ul>\n      <li>Internal intermediate representation (IR)</li>\n      <li>Execution graph</li>\n      <li>Bytecode or linear transformer stack (as in <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>)</li>\n    </ul>\n  </li>\n  <li>\n    <p>Some runtimes use <strong>zero-copy formats</strong> (e.g., FlatBuffer in TFLite) to avoid overhead.</p>\n  </li>\n</ul>\n<p>At runtime, the model file is loaded and deserialized into memory. Runtimes may parse the file into:</p>\n<ul>\n      <li>Internal intermediate representation (IR)</li>\n      <li>Execution graph</li>\n      <li>Bytecode or linear transformer stack (as in <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>)</li>\n    </ul>\n<p>Some runtimes use <strong>zero-copy formats</strong> (e.g., FlatBuffer in TFLite) to avoid overhead.</p>\n<h4 id=\"memory-allocation\">Memory Allocation</h4>\n<ul>\n  <li>\n    <p>Before inference can occur, the runtime must allocate:</p>\n\n    <ul>\n      <li>Input and output tensor buffers</li>\n      <li>Working memory for intermediate computations</li>\n      <li>(If applicable) KV cache (for LLMs), kernel workspaces, or delegate buffers</li>\n    </ul>\n  </li>\n  <li>\n    <p>Advanced runtimes may precompute memory plans to avoid dynamic allocations (e.g., ExecuTorch, <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>).</p>\n  </li>\n</ul>\n<p>Before inference can occur, the runtime must allocate:</p>\n<ul>\n      <li>Input and output tensor buffers</li>\n      <li>Working memory for intermediate computations</li>\n      <li>(If applicable) KV cache (for LLMs), kernel workspaces, or delegate buffers</li>\n    </ul>\n<p>Advanced runtimes may precompute memory plans to avoid dynamic allocations (e.g., ExecuTorch, <code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>).</p>\n<h4 id=\"inference-execution\">Inference Execution</h4>\n<ul>\n  <li>\n    <p>The core execution stage involves:</p>\n\n    <ul>\n      <li>Running the model graph or stack</li>\n      <li>Dispatching operations (ops) to the appropriate hardware backend (CPU, GPU, NPU)</li>\n      <li>Managing control flow, caching, and batching</li>\n    </ul>\n  </li>\n  <li>\n    <p>Different runtimes handle scheduling and dispatch differently:</p>\n\n    <ul>\n      <li>TensorRT: CUDA engine with explicit graph scheduling</li>\n      <li>TFLite: Static interpreter with delegate hand-off</li>\n      <li>ONNX Runtime: Execution Providers (EPs)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>: Single-threaded or parallel transformer loop</li>\n    </ul>\n  </li>\n</ul>\n<p>The core execution stage involves:</p>\n<ul>\n      <li>Running the model graph or stack</li>\n      <li>Dispatching operations (ops) to the appropriate hardware backend (CPU, GPU, NPU)</li>\n      <li>Managing control flow, caching, and batching</li>\n    </ul>\n<p>Different runtimes handle scheduling and dispatch differently:</p>\n<ul>\n      <li>TensorRT: CUDA engine with explicit graph scheduling</li>\n      <li>TFLite: Static interpreter with delegate hand-off</li>\n      <li>ONNX Runtime: Execution Providers (EPs)</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code>: Single-threaded or parallel transformer loop</li>\n    </ul>\n<h4 id=\"postprocessing--output\">Postprocessing &amp; Output</h4>\n<ul>\n  <li>\n    <p>The final outputs are:</p>\n\n    <ul>\n      <li>Raw logits, class probabilities, bounding boxes, text, etc.</li>\n      <li>Returned via API calls (C++, Python, Swift, etc.)</li>\n    </ul>\n  </li>\n  <li>\n    <p>This stage may also include:</p>\n\n    <ul>\n      <li>Dequantization</li>\n      <li>Formatting into app-native types (e.g., Swift structs in Core ML)</li>\n      <li>Logging and telemetry</li>\n    </ul>\n  </li>\n</ul>\n<p>The final outputs are:</p>\n<ul>\n      <li>Raw logits, class probabilities, bounding boxes, text, etc.</li>\n      <li>Returned via API calls (C++, Python, Swift, etc.)</li>\n    </ul>\n<p>This stage may also include:</p>\n<ul>\n      <li>Dequantization</li>\n      <li>Formatting into app-native types (e.g., Swift structs in Core ML)</li>\n      <li>Logging and telemetry</li>\n    </ul>\n<h4 id=\"lifecycle-optimization-optional-but-critical\">Lifecycle Optimization (Optional but Critical)</h4>\n<ul>\n  <li>\n    <p>For deployment, optimization techniques may be inserted at multiple points:</p>\n\n    <ul>\n      <li>Quantization (during conversion)</li>\n      <li>Delegate configuration (runtime initialization)</li>\n      <li>Memory pruning and op fusion (during compile/AOT phase)</li>\n      <li>Execution profiling and tuning</li>\n    </ul>\n  </li>\n</ul>\n<p>For deployment, optimization techniques may be inserted at multiple points:</p>\n<ul>\n      <li>Quantization (during conversion)</li>\n      <li>Delegate configuration (runtime initialization)</li>\n      <li>Memory pruning and op fusion (during compile/AOT phase)</li>\n      <li>Execution profiling and tuning</li>\n    </ul>",
      "contentMarkdown": "#### Model Training\n\n*   Although training is typically performed in a full ML framework (e.g., PyTorch, TensorFlow), it is critical to know that the trained model must be **exported or converted** into a format compatible with the intended runtime.\n    \n*   This stage outputs:\n    \n    *   A trained model file (e.g., `.onnx`, `.mlmodel`, `.pt`, `.tflite`, etc.)\n    *   Associated metadata (input/output shapes, quantization info, etc.)\n\nAlthough training is typically performed in a full ML framework (e.g., PyTorch, TensorFlow), it is critical to know that the trained model must be **exported or converted** into a format compatible with the intended runtime.\n\nThis stage outputs:\n\n*   A trained model file (e.g., `.onnx`, `.mlmodel`, `.pt`, `.tflite`, etc.)\n*   Associated metadata (input/output shapes, quantization info, etc.)\n\n#### Model Conversion\n\n*   This phase adapts the trained model into a runtime-specific format. Conversion tools may also apply **graph simplification**, **quantization**, or **operator fusion**.\n    \n*   Typical tools used:\n    \n    *   `torch.onnx.export()` (PyTorch →→\\\\rightarrow ONNX)\n    *   `coremltools.convert()` (→→\\\\rightarrow Core ML)\n    *   `TFLiteConverter` (TensorFlow →→\\\\rightarrow `.tflite`)\n    *   `executorchc` (TorchScript →→\\\\rightarrow ExecuTorch bytecode)\n    *   `quantize.py` (for GGUF / `llama.cpp`)\n*   This phase outputs:\n    \n    *   Serialized model file tailored for the target runtime\n    *   Optional quantized or optimized variant\n\nThis phase adapts the trained model into a runtime-specific format. Conversion tools may also apply **graph simplification**, **quantization**, or **operator fusion**.\n\nTypical tools used:\n\n*   `torch.onnx.export()` (PyTorch →→\\\\rightarrow ONNX)\n*   `coremltools.convert()` (→→\\\\rightarrow Core ML)\n*   `TFLiteConverter` (TensorFlow →→\\\\rightarrow `.tflite`)\n*   `executorchc` (TorchScript →→\\\\rightarrow ExecuTorch bytecode)\n*   `quantize.py` (for GGUF / `llama.cpp`)\n\nThis phase outputs:\n\n*   Serialized model file tailored for the target runtime\n*   Optional quantized or optimized variant\n\n#### Model Loading\n\n*   At runtime, the model file is loaded and deserialized into memory. Runtimes may parse the file into:\n    \n    *   Internal intermediate representation (IR)\n    *   Execution graph\n    *   Bytecode or linear transformer stack (as in `llama.cpp`)\n*   Some runtimes use **zero-copy formats** (e.g., FlatBuffer in TFLite) to avoid overhead.\n    \n\nAt runtime, the model file is loaded and deserialized into memory. Runtimes may parse the file into:\n\n*   Internal intermediate representation (IR)\n*   Execution graph\n*   Bytecode or linear transformer stack (as in `llama.cpp`)\n\nSome runtimes use **zero-copy formats** (e.g., FlatBuffer in TFLite) to avoid overhead.\n\n#### Memory Allocation\n\n*   Before inference can occur, the runtime must allocate:\n    \n    *   Input and output tensor buffers\n    *   Working memory for intermediate computations\n    *   (If applicable) KV cache (for LLMs), kernel workspaces, or delegate buffers\n*   Advanced runtimes may precompute memory plans to avoid dynamic allocations (e.g., ExecuTorch, `llama.cpp`).\n    \n\nBefore inference can occur, the runtime must allocate:\n\n*   Input and output tensor buffers\n*   Working memory for intermediate computations\n*   (If applicable) KV cache (for LLMs), kernel workspaces, or delegate buffers\n\nAdvanced runtimes may precompute memory plans to avoid dynamic allocations (e.g., ExecuTorch, `llama.cpp`).\n\n#### Inference Execution\n\n*   The core execution stage involves:\n    \n    *   Running the model graph or stack\n    *   Dispatching operations (ops) to the appropriate hardware backend (CPU, GPU, NPU)\n    *   Managing control flow, caching, and batching\n*   Different runtimes handle scheduling and dispatch differently:\n    \n    *   TensorRT: CUDA engine with explicit graph scheduling\n    *   TFLite: Static interpreter with delegate hand-off\n    *   ONNX Runtime: Execution Providers (EPs)\n    *   `llama.cpp`: Single-threaded or parallel transformer loop\n\nThe core execution stage involves:\n\n*   Running the model graph or stack\n*   Dispatching operations (ops) to the appropriate hardware backend (CPU, GPU, NPU)\n*   Managing control flow, caching, and batching\n\nDifferent runtimes handle scheduling and dispatch differently:\n\n*   TensorRT: CUDA engine with explicit graph scheduling\n*   TFLite: Static interpreter with delegate hand-off\n*   ONNX Runtime: Execution Providers (EPs)\n*   `llama.cpp`: Single-threaded or parallel transformer loop\n\n#### Postprocessing & Output\n\n*   The final outputs are:\n    \n    *   Raw logits, class probabilities, bounding boxes, text, etc.\n    *   Returned via API calls (C++, Python, Swift, etc.)\n*   This stage may also include:\n    \n    *   Dequantization\n    *   Formatting into app-native types (e.g., Swift structs in Core ML)\n    *   Logging and telemetry\n\nThe final outputs are:\n\n*   Raw logits, class probabilities, bounding boxes, text, etc.\n*   Returned via API calls (C++, Python, Swift, etc.)\n\nThis stage may also include:\n\n*   Dequantization\n*   Formatting into app-native types (e.g., Swift structs in Core ML)\n*   Logging and telemetry\n\n#### Lifecycle Optimization (Optional but Critical)\n\n*   For deployment, optimization techniques may be inserted at multiple points:\n    \n    *   Quantization (during conversion)\n    *   Delegate configuration (runtime initialization)\n    *   Memory pruning and op fusion (during compile/AOT phase)\n    *   Execution profiling and tuning\n\nFor deployment, optimization techniques may be inserted at multiple points:\n\n*   Quantization (during conversion)\n*   Delegate configuration (runtime initialization)\n*   Memory pruning and op fusion (during compile/AOT phase)\n*   Execution profiling and tuning",
      "order": 61,
      "orderInChapter": 1,
      "difficulty": 5,
      "estimatedMinutes": 4,
      "tags": [
        "miscellaneous",
        "transformer",
        "llm",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 759,
        "contentLength": 18791
      },
      "nextCards": [
        "ai-ml-runtimes-runtime-specific-execution-lifecycles-62",
        "ai-ml-runtimes-overview-63"
      ],
      "relatedCards": [
        "ai-gpu-architecture-tensor-core-evolution-23",
        "ai-gpu-architecture-key-takeaways-38",
        "ai-top-30-papers-better-faster-large-language-models-via-multi-toke-27",
        "ai-gpu-architecture-per-generation-precision-support-and-performance-26",
        "ai-gpu-architecture-floating-point-precision-performance-evolution-35"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#general-workflow:-from-model-to-inference",
      "scrapedAt": "2025-12-28T11:56:51.673Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-runtime-specific-execution-lifecycles-62",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Model Execution Lifecycle Across ML Runtimes",
      "title": "Runtime-Specific Execution Lifecycles",
      "subtitle": "Model Execution Lifecycle Across ML Runtimes",
      "contentHtml": "<ul>\n  <li>While the general lifecycle described earlier applies to all runtimes, each ML runtime adapts or specializes this flow to match its architectural goals and supported hardware.</li>\n  <li>This section provides an <strong>execution lifecycle breakdown</strong> for each runtime discussed in the original primer, with particular focus on runtime-specific logic during model loading, graph execution, memory management, and hardware dispatch.</li>\n</ul>\n<h4 id=\"tensorrt-execution-lifecycle-nvidia-gpus\">TensorRT Execution Lifecycle (NVIDIA GPUs)</h4>\n<ul>\n  <li>\n    <p>TensorRT uses an <strong>Ahead-of-Time (AOT) engine-building</strong> process that transforms a model into a highly optimized CUDA execution plan. Once compiled, the <code class=\"language-plaintext highlighter-rouge\">.plan</code> file encapsulates a pre-fused, quantized, and hardware-specific execution graph.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>Model Import &amp; Parsing:</strong> Parses ONNX, TensorFlow, or Caffe model using TensorRT parsers.</li>\n      <li><strong>Builder Optimization:</strong> Applies kernel selection, op fusion, <code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">float16</code> quantization, and layer scheduling.</li>\n      <li><strong>Engine Generation:</strong> Outputs a <code class=\"language-plaintext highlighter-rouge\">.plan</code> file containing the serialized CUDA engine.</li>\n      <li><strong>Runtime Load:</strong> Loads the plan into memory via <code class=\"language-plaintext highlighter-rouge\">IRuntime</code>, allocates CUDA buffers.</li>\n      <li><strong>Execution Context:</strong> Prepares <code class=\"language-plaintext highlighter-rouge\">ExecutionContext</code> with shape bindings, input/output memory views.</li>\n      <li><strong>Inference Loop:</strong> Launches CUDA kernels via streams with async execution.</li>\n      <li><strong>Output Retrieval:</strong> Copies GPU output buffers back to host (if needed).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Extremely low latency, precompiled execution</li>\n      <li>Requires regeneration if model shape changes</li>\n      <li>All ops dispatched on GPU only</li>\n    </ul>\n  </li>\n</ul>\n<p>TensorRT uses an <strong>Ahead-of-Time (AOT) engine-building</strong> process that transforms a model into a highly optimized CUDA execution plan. Once compiled, the <code class=\"language-plaintext highlighter-rouge\">.plan</code> file encapsulates a pre-fused, quantized, and hardware-specific execution graph.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>Model Import &amp; Parsing:</strong> Parses ONNX, TensorFlow, or Caffe model using TensorRT parsers.</li>\n      <li><strong>Builder Optimization:</strong> Applies kernel selection, op fusion, <code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">float16</code> quantization, and layer scheduling.</li>\n      <li><strong>Engine Generation:</strong> Outputs a <code class=\"language-plaintext highlighter-rouge\">.plan</code> file containing the serialized CUDA engine.</li>\n      <li><strong>Runtime Load:</strong> Loads the plan into memory via <code class=\"language-plaintext highlighter-rouge\">IRuntime</code>, allocates CUDA buffers.</li>\n      <li><strong>Execution Context:</strong> Prepares <code class=\"language-plaintext highlighter-rouge\">ExecutionContext</code> with shape bindings, input/output memory views.</li>\n      <li><strong>Inference Loop:</strong> Launches CUDA kernels via streams with async execution.</li>\n      <li><strong>Output Retrieval:</strong> Copies GPU output buffers back to host (if needed).</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Extremely low latency, precompiled execution</li>\n      <li>Requires regeneration if model shape changes</li>\n      <li>All ops dispatched on GPU only</li>\n    </ul>\n<h4 id=\"core-ml-execution-lifecycle-apple-platforms\">Core ML Execution Lifecycle (Apple Platforms)</h4>\n<ul>\n  <li>\n    <p>Core ML performs inference via <strong>runtime graph execution</strong> of a compiled <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code> package. It abstracts backend selection and heavily integrates with Apple’s APIs.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>Model Compilation:</strong> <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-25\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-26\"><span class=\"mo\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code> via Xcode or <code class=\"language-plaintext highlighter-rouge\">coremltools</code></li>\n      <li><strong>App Initialization:</strong> Loads model via <code class=\"language-plaintext highlighter-rouge\">MLModel(configuration:)</code></li>\n      <li><strong>Backend Dispatch:</strong> Chooses CPU, GPU, or ANE depending on hardware availability and op support.</li>\n      <li><strong>Inference Call:</strong> <code class=\"language-plaintext highlighter-rouge\">model.prediction(input:)</code> executes internal graph</li>\n      <li><strong>Result Handling:</strong> Outputs are returned as native Swift types (e.g., strings, arrays, dicts)</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Dynamic backend selection with op-level granularity</li>\n      <li>Opaque execution graph, no public access to IR</li>\n      <li>Secure, sandboxed memory isolation for inference</li>\n    </ul>\n  </li>\n</ul>\n<p>Core ML performs inference via <strong>runtime graph execution</strong> of a compiled <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code> package. It abstracts backend selection and heavily integrates with Apple’s APIs.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>Model Compilation:</strong> <code class=\"language-plaintext highlighter-rouge\">.mlmodel</code> <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-25\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-26\"><span class=\"mo\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">.mlmodelc</code> via Xcode or <code class=\"language-plaintext highlighter-rouge\">coremltools</code></li>\n      <li><strong>App Initialization:</strong> Loads model via <code class=\"language-plaintext highlighter-rouge\">MLModel(configuration:)</code></li>\n      <li><strong>Backend Dispatch:</strong> Chooses CPU, GPU, or ANE depending on hardware availability and op support.</li>\n      <li><strong>Inference Call:</strong> <code class=\"language-plaintext highlighter-rouge\">model.prediction(input:)</code> executes internal graph</li>\n      <li><strong>Result Handling:</strong> Outputs are returned as native Swift types (e.g., strings, arrays, dicts)</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Dynamic backend selection with op-level granularity</li>\n      <li>Opaque execution graph, no public access to IR</li>\n      <li>Secure, sandboxed memory isolation for inference</li>\n    </ul>\n<h4 id=\"mlx-execution-lifecycle-apple-silicon\">MLX Execution Lifecycle (Apple Silicon)</h4>\n<ul>\n  <li>\n    <p>MLX uses a <strong>Python-based tensor programming model</strong> and optionally compiles graphs via JIT. It is most similar to PyTorch but tightly integrated with Metal.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>Model Definition:</strong> Model is defined in Python using <code class=\"language-plaintext highlighter-rouge\">mlx.nn.Module</code></li>\n      <li><strong>Eager Execution (default):</strong> Runs ops immediately using Metal Performance Shaders (MPS)</li>\n      <li><strong>Compiled Graph (optional):</strong> <code class=\"language-plaintext highlighter-rouge\">@mlx.compile</code> transforms a function into a static kernel sequence</li>\n      <li><strong>Tensor Handling:</strong> All tensors are immutable; memory reuse is managed by the MLX runtime</li>\n      <li><strong>Execution:</strong> Kernel invocations are dispatched via Metal; ANE support is under development</li>\n      <li><strong>Output:</strong> Results returned as MLX tensors, convertible to NumPy or PyTorch</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Developer-centric and Pythonic</li>\n      <li>Targets M1/M2 GPU via Metal</li>\n      <li>No external model serialization—code <em>is</em> the model</li>\n    </ul>\n  </li>\n</ul>\n<p>MLX uses a <strong>Python-based tensor programming model</strong> and optionally compiles graphs via JIT. It is most similar to PyTorch but tightly integrated with Metal.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>Model Definition:</strong> Model is defined in Python using <code class=\"language-plaintext highlighter-rouge\">mlx.nn.Module</code></li>\n      <li><strong>Eager Execution (default):</strong> Runs ops immediately using Metal Performance Shaders (MPS)</li>\n      <li><strong>Compiled Graph (optional):</strong> <code class=\"language-plaintext highlighter-rouge\">@mlx.compile</code> transforms a function into a static kernel sequence</li>\n      <li><strong>Tensor Handling:</strong> All tensors are immutable; memory reuse is managed by the MLX runtime</li>\n      <li><strong>Execution:</strong> Kernel invocations are dispatched via Metal; ANE support is under development</li>\n      <li><strong>Output:</strong> Results returned as MLX tensors, convertible to NumPy or PyTorch</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Developer-centric and Pythonic</li>\n      <li>Targets M1/M2 GPU via Metal</li>\n      <li>No external model serialization—code <em>is</em> the model</li>\n    </ul>\n<h4 id=\"onnx-runtime-execution-lifecycle\">ONNX Runtime Execution Lifecycle</h4>\n<ul>\n  <li>\n    <p>ONNX Runtime is built around an <strong>intermediate computation graph</strong>, modular kernel registry, and <strong>Execution Providers (EPs)</strong> that delegate ops to appropriate hardware.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>Model Load:</strong> Parses <code class=\"language-plaintext highlighter-rouge\">.onnx</code> file (protobuf format) into IR</li>\n      <li><strong>Graph Optimization:</strong> Applies passes (e.g., constant folding, op fusion, node elimination)</li>\n      <li><strong>EP Assignment:</strong> Ops are split across available EPs (CPU, CUDA, NNAPI, etc.)</li>\n      <li><strong>Session Initialization:</strong> Prepares <code class=\"language-plaintext highlighter-rouge\">InferenceSession</code> with input/output bindings</li>\n      <li><strong>Execution:</strong> Each partition of the graph is dispatched to its EP</li>\n      <li><strong>Result Aggregation:</strong> Output tensors are collected and returned in native types</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Pluggable backend system for flexible hardware support</li>\n      <li>Static graph, dynamic shape support with constraints</li>\n      <li>Strong cross-platform model portability</li>\n    </ul>\n  </li>\n</ul>\n<p>ONNX Runtime is built around an <strong>intermediate computation graph</strong>, modular kernel registry, and <strong>Execution Providers (EPs)</strong> that delegate ops to appropriate hardware.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>Model Load:</strong> Parses <code class=\"language-plaintext highlighter-rouge\">.onnx</code> file (protobuf format) into IR</li>\n      <li><strong>Graph Optimization:</strong> Applies passes (e.g., constant folding, op fusion, node elimination)</li>\n      <li><strong>EP Assignment:</strong> Ops are split across available EPs (CPU, CUDA, NNAPI, etc.)</li>\n      <li><strong>Session Initialization:</strong> Prepares <code class=\"language-plaintext highlighter-rouge\">InferenceSession</code> with input/output bindings</li>\n      <li><strong>Execution:</strong> Each partition of the graph is dispatched to its EP</li>\n      <li><strong>Result Aggregation:</strong> Output tensors are collected and returned in native types</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Pluggable backend system for flexible hardware support</li>\n      <li>Static graph, dynamic shape support with constraints</li>\n      <li>Strong cross-platform model portability</li>\n    </ul>\n<h4 id=\"executorch-execution-lifecycle-mcuembedded-focus\">ExecuTorch Execution Lifecycle (MCU/Embedded Focus)</h4>\n<ul>\n  <li>\n    <p>ExecuTorch employs a <strong>bytecode VM</strong> model with AOT compilation for PyTorch models. It is built for microcontrollers and embedded edge devices.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>TorchScript Compilation:</strong> PyTorch model scripted and converted into <code class=\"language-plaintext highlighter-rouge\">.pt</code> (TorchScript)</li>\n      <li><strong>AOT Bytecode Generation:</strong> <code class=\"language-plaintext highlighter-rouge\">executorchc</code> compiles model to <code class=\"language-plaintext highlighter-rouge\">.ptc</code> (bytecode)</li>\n      <li><strong>Runtime Embedding:</strong> Bytecode and interpreter embedded into firmware or C++ app</li>\n      <li><strong>Interpreter Loop:</strong> Model execution performed by a tiny VM that reads bytecode</li>\n      <li><strong>Op Dispatch:</strong> Ops are routed to statically compiled function pointers</li>\n      <li><strong>Output Return:</strong> Inference results written to statically allocated output buffer</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Deterministic memory, static allocation only</li>\n      <li>Supports sub-MB runtime environments</li>\n      <li>Highly tunable; model format ≠ PyTorch IR</li>\n    </ul>\n  </li>\n</ul>\n<p>ExecuTorch employs a <strong>bytecode VM</strong> model with AOT compilation for PyTorch models. It is built for microcontrollers and embedded edge devices.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>TorchScript Compilation:</strong> PyTorch model scripted and converted into <code class=\"language-plaintext highlighter-rouge\">.pt</code> (TorchScript)</li>\n      <li><strong>AOT Bytecode Generation:</strong> <code class=\"language-plaintext highlighter-rouge\">executorchc</code> compiles model to <code class=\"language-plaintext highlighter-rouge\">.ptc</code> (bytecode)</li>\n      <li><strong>Runtime Embedding:</strong> Bytecode and interpreter embedded into firmware or C++ app</li>\n      <li><strong>Interpreter Loop:</strong> Model execution performed by a tiny VM that reads bytecode</li>\n      <li><strong>Op Dispatch:</strong> Ops are routed to statically compiled function pointers</li>\n      <li><strong>Output Return:</strong> Inference results written to statically allocated output buffer</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Deterministic memory, static allocation only</li>\n      <li>Supports sub-MB runtime environments</li>\n      <li>Highly tunable; model format ≠ PyTorch IR</li>\n    </ul>\n<h4 id=\"lidartlm-execution-lifecycle-lidar-focused-embedded-stacks\">LidarTLM Execution Lifecycle (LiDAR-Focused Embedded Stacks)</h4>\n<ul>\n  <li>\n    <p>LidarTLM-style runtimes are not general-purpose, but highly optimized for 3D spatial inference using sparse tensor pipelines.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>Sensor Input:</strong> LiDAR frames streamed in real-time</li>\n      <li><strong>Preprocessing:</strong> Voxelization or range transformation into tensor-friendly formats</li>\n      <li><strong>Tensor Pipeline:</strong> Sparse CNNs, 3D convolutions, and attention modules process data</li>\n      <li><strong>Temporal Fusion:</strong> RNN or transformer-based modules optionally applied across frames</li>\n      <li><strong>Postprocessing:</strong> Generates semantic maps or bounding boxes</li>\n      <li><strong>Sensor Fusion:</strong> Optionally integrates radar or camera data for final outputs</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Sparse tensors and voxel grids dominate memory model</li>\n      <li>CUDA, Open3D, or MinkowskiEngine often used</li>\n      <li>Hard real-time constraints for robotics/ADAS</li>\n    </ul>\n  </li>\n</ul>\n<p>LidarTLM-style runtimes are not general-purpose, but highly optimized for 3D spatial inference using sparse tensor pipelines.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>Sensor Input:</strong> LiDAR frames streamed in real-time</li>\n      <li><strong>Preprocessing:</strong> Voxelization or range transformation into tensor-friendly formats</li>\n      <li><strong>Tensor Pipeline:</strong> Sparse CNNs, 3D convolutions, and attention modules process data</li>\n      <li><strong>Temporal Fusion:</strong> RNN or transformer-based modules optionally applied across frames</li>\n      <li><strong>Postprocessing:</strong> Generates semantic maps or bounding boxes</li>\n      <li><strong>Sensor Fusion:</strong> Optionally integrates radar or camera data for final outputs</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Sparse tensors and voxel grids dominate memory model</li>\n      <li>CUDA, Open3D, or MinkowskiEngine often used</li>\n      <li>Hard real-time constraints for robotics/ADAS</li>\n    </ul>\n<h4 id=\"llamacpp-execution-lifecycle-quantized-llms\"><code class=\"language-plaintext Highlighter-rouge\">llama.cpp</code> Execution Lifecycle (Quantized LLMs)</h4>\n<ul>\n  <li>\n    <p><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> is a minimalist CPU-first runtime for LLMs using quantized models in the GGUF format. It has no graph engine—just a static transformer loop.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>Model Load:</strong> GGUF model memory-mapped into RAM</li>\n      <li><strong>KV Cache Setup:</strong> Pre-allocates attention buffers</li>\n      <li><strong>Embedding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-28\" style=\"width: 1.191em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.984em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.552em, 1000.93em, 2.275em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-29\"><span class=\"mo\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">\\rightarrow</script> Transformer Loop:</strong> Sequentially executes transformer layers</li>\n      <li><strong>Sampling:</strong> Next token is selected via greedy/top-k/top-p logic</li>\n      <li><strong>Tokenization:</strong> Output string is constructed from sampled token IDs</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Highly portable, CPU-optimized, extremely low memory usage</li>\n      <li>No dynamic graph, no scheduler, no intermediate representation</li>\n      <li>Offload options (e.g., Metal, CUDA) are modu#### TensorFlow Lite Execution Lifecycle</li>\n    </ul>\n  </li>\n</ul>\n<p><code class=\"language-plaintext highlighter-rouge\">llama.cpp</code> is a minimalist CPU-first runtime for LLMs using quantized models in the GGUF format. It has no graph engine—just a static transformer loop.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>Model Load:</strong> GGUF model memory-mapped into RAM</li>\n      <li><strong>KV Cache Setup:</strong> Pre-allocates attention buffers</li>\n      <li><strong>Embedding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-28\" style=\"width: 1.191em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.984em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.552em, 1000.93em, 2.275em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-29\"><span class=\"mo\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">\\rightarrow</script> Transformer Loop:</strong> Sequentially executes transformer layers</li>\n      <li><strong>Sampling:</strong> Next token is selected via greedy/top-k/top-p logic</li>\n      <li><strong>Tokenization:</strong> Output string is constructed from sampled token IDs</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Highly portable, CPU-optimized, extremely low memory usage</li>\n      <li>No dynamic graph, no scheduler, no intermediate representation</li>\n      <li>Offload options (e.g., Metal, CUDA) are modu#### TensorFlow Lite Execution Lifecycle</li>\n    </ul>\n<h4 id=\"tensorflow-lite-execution-lifecycle\">TensorFlow Lite Execution Lifecycle</h4>\n<ul>\n  <li>\n    <p>TFLite uses a <strong>FlatBuffer interpreter</strong> architecture with optional delegates for acceleration.</p>\n  </li>\n  <li>\n    <p><strong>Lifecycle Stages:</strong></p>\n\n    <ul>\n      <li><strong>Model Conversion:</strong> TensorFlow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-31\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-32\"><span class=\"mo\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">.tflite</code> via <code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code></li>\n      <li><strong>FlatBuffer Load:</strong> Model loaded with <code class=\"language-plaintext highlighter-rouge\">Interpreter(model_path=...)</code></li>\n      <li><strong>Tensor Allocation:</strong> Input/output buffers allocated via <code class=\"language-plaintext highlighter-rouge\">allocate_tensors()</code></li>\n      <li><strong>Delegate Attachment (optional):</strong> NNAPI, GPU, Hexagon delegate claims subgraphs</li>\n      <li><strong>Inference:</strong> Static interpreter walks the computation graph</li>\n      <li><strong>Output Access:</strong> Results extracted via <code class=\"language-plaintext highlighter-rouge\">get_tensor()</code> APIs</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Unique Characteristics:</strong></p>\n\n    <ul>\n      <li>Very compact format with zero-copy access</li>\n      <li>Delegate design separates concerns for CPU vs. accelerators</li>\n      <li>Strong ecosystem with tooling (e.g., Model Maker, Visualizer)</li>\n    </ul>\n  </li>\n</ul>\n<p>TFLite uses a <strong>FlatBuffer interpreter</strong> architecture with optional delegates for acceleration.</p>\n<p><strong>Lifecycle Stages:</strong></p>\n<ul>\n      <li><strong>Model Conversion:</strong> TensorFlow <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2192;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-31\" style=\"width: 1.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.294em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-32\"><span class=\"mo\" id=\"MathJax-Span-33\" style=\"font-family: STIXGeneral-Regular;\">→</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: 0.003em; border-left: 0px solid; width: 0px; height: 0.628em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">→</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">\\rightarrow</script> <code class=\"language-plaintext highlighter-rouge\">.tflite</code> via <code class=\"language-plaintext highlighter-rouge\">TFLiteConverter</code></li>\n      <li><strong>FlatBuffer Load:</strong> Model loaded with <code class=\"language-plaintext highlighter-rouge\">Interpreter(model_path=...)</code></li>\n      <li><strong>Tensor Allocation:</strong> Input/output buffers allocated via <code class=\"language-plaintext highlighter-rouge\">allocate_tensors()</code></li>\n      <li><strong>Delegate Attachment (optional):</strong> NNAPI, GPU, Hexagon delegate claims subgraphs</li>\n      <li><strong>Inference:</strong> Static interpreter walks the computation graph</li>\n      <li><strong>Output Access:</strong> Results extracted via <code class=\"language-plaintext highlighter-rouge\">get_tensor()</code> APIs</li>\n    </ul>\n<p><strong>Unique Characteristics:</strong></p>\n<ul>\n      <li>Very compact format with zero-copy access</li>\n      <li>Delegate design separates concerns for CPU vs. accelerators</li>\n      <li>Strong ecosystem with tooling (e.g., Model Maker, Visualizer)</li>\n    </ul>",
      "contentMarkdown": "*   While the general lifecycle described earlier applies to all runtimes, each ML runtime adapts or specializes this flow to match its architectural goals and supported hardware.\n*   This section provides an **execution lifecycle breakdown** for each runtime discussed in the original primer, with particular focus on runtime-specific logic during model loading, graph execution, memory management, and hardware dispatch.\n\n#### TensorRT Execution Lifecycle (NVIDIA GPUs)\n\n*   TensorRT uses an **Ahead-of-Time (AOT) engine-building** process that transforms a model into a highly optimized CUDA execution plan. Once compiled, the `.plan` file encapsulates a pre-fused, quantized, and hardware-specific execution graph.\n    \n*   **Lifecycle Stages:**\n    \n    *   **Model Import & Parsing:** Parses ONNX, TensorFlow, or Caffe model using TensorRT parsers.\n    *   **Builder Optimization:** Applies kernel selection, op fusion, `int8`/`float16` quantization, and layer scheduling.\n    *   **Engine Generation:** Outputs a `.plan` file containing the serialized CUDA engine.\n    *   **Runtime Load:** Loads the plan into memory via `IRuntime`, allocates CUDA buffers.\n    *   **Execution Context:** Prepares `ExecutionContext` with shape bindings, input/output memory views.\n    *   **Inference Loop:** Launches CUDA kernels via streams with async execution.\n    *   **Output Retrieval:** Copies GPU output buffers back to host (if needed).\n*   **Unique Characteristics:**\n    \n    *   Extremely low latency, precompiled execution\n    *   Requires regeneration if model shape changes\n    *   All ops dispatched on GPU only\n\nTensorRT uses an **Ahead-of-Time (AOT) engine-building** process that transforms a model into a highly optimized CUDA execution plan. Once compiled, the `.plan` file encapsulates a pre-fused, quantized, and hardware-specific execution graph.\n\n**Lifecycle Stages:**\n\n*   **Model Import & Parsing:** Parses ONNX, TensorFlow, or Caffe model using TensorRT parsers.\n*   **Builder Optimization:** Applies kernel selection, op fusion, `int8`/`float16` quantization, and layer scheduling.\n*   **Engine Generation:** Outputs a `.plan` file containing the serialized CUDA engine.\n*   **Runtime Load:** Loads the plan into memory via `IRuntime`, allocates CUDA buffers.\n*   **Execution Context:** Prepares `ExecutionContext` with shape bindings, input/output memory views.\n*   **Inference Loop:** Launches CUDA kernels via streams with async execution.\n*   **Output Retrieval:** Copies GPU output buffers back to host (if needed).\n\n**Unique Characteristics:**\n\n*   Extremely low latency, precompiled execution\n*   Requires regeneration if model shape changes\n*   All ops dispatched on GPU only\n\n#### Core ML Execution Lifecycle (Apple Platforms)\n\n*   Core ML performs inference via **runtime graph execution** of a compiled `.mlmodelc` package. It abstracts backend selection and heavily integrates with Apple’s APIs.\n    \n*   **Lifecycle Stages:**\n    \n    *   **Model Compilation:** `.mlmodel` →→\\\\rightarrow `.mlmodelc` via Xcode or `coremltools`\n    *   **App Initialization:** Loads model via `MLModel(configuration:)`\n    *   **Backend Dispatch:** Chooses CPU, GPU, or ANE depending on hardware availability and op support.\n    *   **Inference Call:** `model.prediction(input:)` executes internal graph\n    *   **Result Handling:** Outputs are returned as native Swift types (e.g., strings, arrays, dicts)\n*   **Unique Characteristics:**\n    \n    *   Dynamic backend selection with op-level granularity\n    *   Opaque execution graph, no public access to IR\n    *   Secure, sandboxed memory isolation for inference\n\nCore ML performs inference via **runtime graph execution** of a compiled `.mlmodelc` package. It abstracts backend selection and heavily integrates with Apple’s APIs.\n\n**Lifecycle Stages:**\n\n*   **Model Compilation:** `.mlmodel` →→\\\\rightarrow `.mlmodelc` via Xcode or `coremltools`\n*   **App Initialization:** Loads model via `MLModel(configuration:)`\n*   **Backend Dispatch:** Chooses CPU, GPU, or ANE depending on hardware availability and op support.\n*   **Inference Call:** `model.prediction(input:)` executes internal graph\n*   **Result Handling:** Outputs are returned as native Swift types (e.g., strings, arrays, dicts)\n\n**Unique Characteristics:**\n\n*   Dynamic backend selection with op-level granularity\n*   Opaque execution graph, no public access to IR\n*   Secure, sandboxed memory isolation for inference\n\n#### MLX Execution Lifecycle (Apple Silicon)\n\n*   MLX uses a **Python-based tensor programming model** and optionally compiles graphs via JIT. It is most similar to PyTorch but tightly integrated with Metal.\n    \n*   **Lifecycle Stages:**\n    \n    *   **Model Definition:** Model is defined in Python using `mlx.nn.Module`\n    *   **Eager Execution (default):** Runs ops immediately using Metal Performance Shaders (MPS)\n    *   **Compiled Graph (optional):** `@mlx.compile` transforms a function into a static kernel sequence\n    *   **Tensor Handling:** All tensors are immutable; memory reuse is managed by the MLX runtime\n    *   **Execution:** Kernel invocations are dispatched via Metal; ANE support is under development\n    *   **Output:** Results returned as MLX tensors, convertible to NumPy or PyTorch\n*   **Unique Characteristics:**\n    \n    *   Developer-centric and Pythonic\n    *   Targets M1/M2 GPU via Metal\n    *   No external model serialization—code _is_ the model\n\nMLX uses a **Python-based tensor programming model** and optionally compiles graphs via JIT. It is most similar to PyTorch but tightly integrated with Metal.\n\n**Lifecycle Stages:**\n\n*   **Model Definition:** Model is defined in Python using `mlx.nn.Module`\n*   **Eager Execution (default):** Runs ops immediately using Metal Performance Shaders (MPS)\n*   **Compiled Graph (optional):** `@mlx.compile` transforms a function into a static kernel sequence\n*   **Tensor Handling:** All tensors are immutable; memory reuse is managed by the MLX runtime\n*   **Execution:** Kernel invocations are dispatched via Metal; ANE support is under development\n*   **Output:** Results returned as MLX tensors, convertible to NumPy or PyTorch\n\n**Unique Characteristics:**\n\n*   Developer-centric and Pythonic\n*   Targets M1/M2 GPU via Metal\n*   No external model serialization—code _is_ the model\n\n#### ONNX Runtime Execution Lifecycle\n\n*   ONNX Runtime is built around an **intermediate computation graph**, modular kernel registry, and **Execution Providers (EPs)** that delegate ops to appropriate hardware.\n    \n*   **Lifecycle Stages:**\n    \n    *   **Model Load:** Parses `.onnx` file (protobuf format) into IR\n    *   **Graph Optimization:** Applies passes (e.g., constant folding, op fusion, node elimination)\n    *   **EP Assignment:** Ops are split across available EPs (CPU, CUDA, NNAPI, etc.)\n    *   **Session Initialization:** Prepares `InferenceSession` with input/output bindings\n    *   **Execution:** Each partition of the graph is dispatched to its EP\n    *   **Result Aggregation:** Output tensors are collected and returned in native types\n*   **Unique Characteristics:**\n    \n    *   Pluggable backend system for flexible hardware support\n    *   Static graph, dynamic shape support with constraints\n    *   Strong cross-platform model portability\n\nONNX Runtime is built around an **intermediate computation graph**, modular kernel registry, and **Execution Providers (EPs)** that delegate ops to appropriate hardware.\n\n**Lifecycle Stages:**\n\n*   **Model Load:** Parses `.onnx` file (protobuf format) into IR\n*   **Graph Optimization:** Applies passes (e.g., constant folding, op fusion, node elimination)\n*   **EP Assignment:** Ops are split across available EPs (CPU, CUDA, NNAPI, etc.)\n*   **Session Initialization:** Prepares `InferenceSession` with input/output bindings\n*   **Execution:** Each partition of the graph is dispatched to its EP\n*   **Result Aggregation:** Output tensors are collected and returned in native types\n\n**Unique Characteristics:**\n\n*   Pluggable backend system for flexible hardware support\n*   Static graph, dynamic shape support with constraints\n*   Strong cross-platform model portability\n\n#### ExecuTorch Execution Lifecycle (MCU/Embedded Focus)\n\n*   ExecuTorch employs a **bytecode VM** model with AOT compilation for PyTorch models. It is built for microcontrollers and embedded edge devices.\n    \n*   **Lifecycle Stages:**\n    \n    *   **TorchScript Compilation:** PyTorch model scripted and converted into `.pt` (TorchScript)\n    *   **AOT Bytecode Generation:** `executorchc` compiles model to `.ptc` (bytecode)\n    *   **Runtime Embedding:** Bytecode and interpreter embedded into firmware or C++ app\n    *   **Interpreter Loop:** Model execution performed by a tiny VM that reads bytecode\n    *   **Op Dispatch:** Ops are routed to statically compiled function pointers\n    *   **Output Return:** Inference results written to statically allocated output buffer\n*   **Unique Characteristics:**\n    \n    *   Deterministic memory, static allocation only\n    *   Supports sub-MB runtime environments\n    *   Highly tunable; model format ≠ PyTorch IR\n\nExecuTorch employs a **bytecode VM** model with AOT compilation for PyTorch models. It is built for microcontrollers and embedded edge devices.\n\n**Lifecycle Stages:**\n\n*   **TorchScript Compilation:** PyTorch model scripted and converted into `.pt` (TorchScript)\n*   **AOT Bytecode Generation:** `executorchc` compiles model to `.ptc` (bytecode)\n*   **Runtime Embedding:** Bytecode and interpreter embedded into firmware or C++ app\n*   **Interpreter Loop:** Model execution performed by a tiny VM that reads bytecode\n*   **Op Dispatch:** Ops are routed to statically compiled function pointers\n*   **Output Return:** Inference results written to statically allocated output buffer\n\n**Unique Characteristics:**\n\n*   Deterministic memory, static allocation only\n*   Supports sub-MB runtime environments\n*   Highly tunable; model format ≠ PyTorch IR\n\n#### LidarTLM Execution Lifecycle (LiDAR-Focused Embedded Stacks)\n\n*   LidarTLM-style runtimes are not general-purpose, but highly optimized for 3D spatial inference using sparse tensor pipelines.\n    \n*   **Lifecycle Stages:**\n    \n    *   **Sensor Input:** LiDAR frames streamed in real-time\n    *   **Preprocessing:** Voxelization or range transformation into tensor-friendly formats\n    *   **Tensor Pipeline:** Sparse CNNs, 3D convolutions, and attention modules process data\n    *   **Temporal Fusion:** RNN or transformer-based modules optionally applied across frames\n    *   **Postprocessing:** Generates semantic maps or bounding boxes\n    *   **Sensor Fusion:** Optionally integrates radar or camera data for final outputs\n*   **Unique Characteristics:**\n    \n    *   Sparse tensors and voxel grids dominate memory model\n    *   CUDA, Open3D, or MinkowskiEngine often used\n    *   Hard real-time constraints for robotics/ADAS\n\nLidarTLM-style runtimes are not general-purpose, but highly optimized for 3D spatial inference using sparse tensor pipelines.\n\n**Lifecycle Stages:**\n\n*   **Sensor Input:** LiDAR frames streamed in real-time\n*   **Preprocessing:** Voxelization or range transformation into tensor-friendly formats\n*   **Tensor Pipeline:** Sparse CNNs, 3D convolutions, and attention modules process data\n*   **Temporal Fusion:** RNN or transformer-based modules optionally applied across frames\n*   **Postprocessing:** Generates semantic maps or bounding boxes\n*   **Sensor Fusion:** Optionally integrates radar or camera data for final outputs\n\n**Unique Characteristics:**\n\n*   Sparse tensors and voxel grids dominate memory model\n*   CUDA, Open3D, or MinkowskiEngine often used\n*   Hard real-time constraints for robotics/ADAS\n\n#### `llama.cpp` Execution Lifecycle (Quantized LLMs)\n\n*   `llama.cpp` is a minimalist CPU-first runtime for LLMs using quantized models in the GGUF format. It has no graph engine—just a static transformer loop.\n    \n*   **Lifecycle Stages:**\n    \n    *   **Model Load:** GGUF model memory-mapped into RAM\n    *   **KV Cache Setup:** Pre-allocates attention buffers\n    *   **Embedding →→\\\\rightarrow Transformer Loop:** Sequentially executes transformer layers\n    *   **Sampling:** Next token is selected via greedy/top-k/top-p logic\n    *   **Tokenization:** Output string is constructed from sampled token IDs\n*   **Unique Characteristics:**\n    \n    *   Highly portable, CPU-optimized, extremely low memory usage\n    *   No dynamic graph, no scheduler, no intermediate representation\n    *   Offload options (e.g., Metal, CUDA) are modu#### TensorFlow Lite Execution Lifecycle\n\n`llama.cpp` is a minimalist CPU-first runtime for LLMs using quantized models in the GGUF format. It has no graph engine—just a static transformer loop.\n\n**Lifecycle Stages:**\n\n*   **Model Load:** GGUF model memory-mapped into RAM\n*   **KV Cache Setup:** Pre-allocates attention buffers\n*   **Embedding →→\\\\rightarrow Transformer Loop:** Sequentially executes transformer layers\n*   **Sampling:** Next token is selected via greedy/top-k/top-p logic\n*   **Tokenization:** Output string is constructed from sampled token IDs\n\n**Unique Characteristics:**\n\n*   Highly portable, CPU-optimized, extremely low memory usage\n*   No dynamic graph, no scheduler, no intermediate representation\n*   Offload options (e.g., Metal, CUDA) are modu#### TensorFlow Lite Execution Lifecycle\n\n#### TensorFlow Lite Execution Lifecycle\n\n*   TFLite uses a **FlatBuffer interpreter** architecture with optional delegates for acceleration.\n    \n*   **Lifecycle Stages:**\n    \n    *   **Model Conversion:** TensorFlow →→\\\\rightarrow `.tflite` via `TFLiteConverter`\n    *   **FlatBuffer Load:** Model loaded with `Interpreter(model_path=...)`\n    *   **Tensor Allocation:** Input/output buffers allocated via `allocate_tensors()`\n    *   **Delegate Attachment (optional):** NNAPI, GPU, Hexagon delegate claims subgraphs\n    *   **Inference:** Static interpreter walks the computation graph\n    *   **Output Access:** Results extracted via `get_tensor()` APIs\n*   **Unique Characteristics:**\n    \n    *   Very compact format with zero-copy access\n    *   Delegate design separates concerns for CPU vs. accelerators\n    *   Strong ecosystem with tooling (e.g., Model Maker, Visualizer)\n\nTFLite uses a **FlatBuffer interpreter** architecture with optional delegates for acceleration.\n\n**Lifecycle Stages:**\n\n*   **Model Conversion:** TensorFlow →→\\\\rightarrow `.tflite` via `TFLiteConverter`\n*   **FlatBuffer Load:** Model loaded with `Interpreter(model_path=...)`\n*   **Tensor Allocation:** Input/output buffers allocated via `allocate_tensors()`\n*   **Delegate Attachment (optional):** NNAPI, GPU, Hexagon delegate claims subgraphs\n*   **Inference:** Static interpreter walks the computation graph\n*   **Output Access:** Results extracted via `get_tensor()` APIs\n\n**Unique Characteristics:**\n\n*   Very compact format with zero-copy access\n*   Delegate design separates concerns for CPU vs. accelerators\n*   Strong ecosystem with tooling (e.g., Model Maker, Visualizer)",
      "order": 62,
      "orderInChapter": 2,
      "difficulty": 5,
      "estimatedMinutes": 10,
      "tags": [
        "miscellaneous",
        "transformer",
        "attention",
        "embedding",
        "convolution",
        "cnn",
        "rnn",
        "llm"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 1895,
        "contentLength": 30010
      },
      "nextCards": [
        "ai-ml-runtimes-overview-63",
        "ai-ml-runtimes-fbgemm-by-meta-server-cpus-64"
      ],
      "relatedCards": [
        "ai-top-30-papers-attention-is-all-you-need-13",
        "ai-gpu-architecture-tensor-core-evolution-23",
        "ai-gpu-architecture-key-takeaways-38",
        "ai-model-compression-principles-of-lightweight-design-50",
        "ai-top-30-papers-variational-lossy-autoencoder-17"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#runtime-specific-execution-lifecycles",
      "scrapedAt": "2025-12-28T11:56:51.673Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-overview-63",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Related: CPU Operator Libraries/Backends",
      "title": "Overview",
      "subtitle": "Related: CPU Operator Libraries/Backends",
      "contentHtml": "<ul>\n  <li>\n    <p>This quick overview shows how <a href=\"https://github.com/pytorch/FBGEMM\"><code class=\"language-plaintext highlighter-rouge\">FBGEMM</code></a>, <a href=\"https://github.com/pytorch/QNNPACK\"><code class=\"language-plaintext highlighter-rouge\">QNNPACK</code></a>, and <a href=\"https://github.com/google/XNNPACK\"><code class=\"language-plaintext highlighter-rouge\">XNNPACK</code></a> fit into the CPU inference stack—what they do, what they don’t, and when runtimes route ops to them:</p>\n\n    <ul>\n      <li><strong>Primary purpose</strong>: Efficient execution of CPU kernels (conv/GEMM, depthwise conv, elementwise ops) for float and/or 8-bit quantized tensors.</li>\n      <li><strong>They do not</strong>: Choose quantization parameters, calibrate ranges, or convert weights—framework/tooling does that.</li>\n      <li><strong>Where they fit</strong>: After model conversion and quantization, the runtime dispatches supported ops to one of these libraries based on platform and data type.</li>\n    </ul>\n  </li>\n</ul>\n<p>This quick overview shows how <a href=\"https://github.com/pytorch/FBGEMM\"><code class=\"language-plaintext highlighter-rouge\">FBGEMM</code></a>, <a href=\"https://github.com/pytorch/QNNPACK\"><code class=\"language-plaintext highlighter-rouge\">QNNPACK</code></a>, and <a href=\"https://github.com/google/XNNPACK\"><code class=\"language-plaintext highlighter-rouge\">XNNPACK</code></a> fit into the CPU inference stack—what they do, what they don’t, and when runtimes route ops to them:</p>\n<ul>\n      <li><strong>Primary purpose</strong>: Efficient execution of CPU kernels (conv/GEMM, depthwise conv, elementwise ops) for float and/or 8-bit quantized tensors.</li>\n      <li><strong>They do not</strong>: Choose quantization parameters, calibrate ranges, or convert weights—framework/tooling does that.</li>\n      <li><strong>Where they fit</strong>: After model conversion and quantization, the runtime dispatches supported ops to one of these libraries based on platform and data type.</li>\n    </ul>",
      "contentMarkdown": "*   This quick overview shows how [`FBGEMM`](https://github.com/pytorch/FBGEMM), [`QNNPACK`](https://github.com/pytorch/QNNPACK), and [`XNNPACK`](https://github.com/google/XNNPACK) fit into the CPU inference stack—what they do, what they don’t, and when runtimes route ops to them:\n    \n    *   **Primary purpose**: Efficient execution of CPU kernels (conv/GEMM, depthwise conv, elementwise ops) for float and/or 8-bit quantized tensors.\n    *   **They do not**: Choose quantization parameters, calibrate ranges, or convert weights—framework/tooling does that.\n    *   **Where they fit**: After model conversion and quantization, the runtime dispatches supported ops to one of these libraries based on platform and data type.\n\nThis quick overview shows how [`FBGEMM`](https://github.com/pytorch/FBGEMM), [`QNNPACK`](https://github.com/pytorch/QNNPACK), and [`XNNPACK`](https://github.com/google/XNNPACK) fit into the CPU inference stack—what they do, what they don’t, and when runtimes route ops to them:\n\n*   **Primary purpose**: Efficient execution of CPU kernels (conv/GEMM, depthwise conv, elementwise ops) for float and/or 8-bit quantized tensors.\n*   **They do not**: Choose quantization parameters, calibrate ranges, or convert weights—framework/tooling does that.\n*   **Where they fit**: After model conversion and quantization, the runtime dispatches supported ops to one of these libraries based on platform and data type.",
      "order": 63,
      "orderInChapter": 1,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 171,
        "contentLength": 2034
      },
      "nextCards": [
        "ai-ml-runtimes-fbgemm-by-meta-server-cpus-64",
        "ai-ml-runtimes-xnnpack-by-google-both-server-and-mobile-cpus-65"
      ],
      "relatedCards": [
        "ai-gpu-architecture-multi-gpu-memory-coherency-31",
        "ai-gpu-architecture-fundamental-architectural-components-amd-32",
        "ai-gpu-architecture-execution-paradigms-amd-33",
        "ai-gpu-architecture-compute-architecture-evolution-amd-cdna-rdna-34",
        "ai-gpu-architecture-memory-architecture-evolution-36"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#overview",
      "scrapedAt": "2025-12-28T11:56:51.673Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-fbgemm-by-meta-server-cpus-64",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Related: CPU Operator Libraries/Backends",
      "title": "FBGEMM (by Meta,; Server CPUs)",
      "subtitle": "Related: CPU Operator Libraries/Backends",
      "contentHtml": "<ul>\n  <li><strong>Target platforms</strong>: x86-64 server/desktop CPUs with SIMD (AVX2, AVX-512; newer stacks can leverage AMX on recent Intel parts via higher-level integrations).</li>\n  <li><strong>Data types and quant schemes</strong>: <code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">uint8</code> activations (affine per-tensor), <code class=\"language-plaintext highlighter-rouge\">int8</code> weights (often symmetric per-channel). 32-bit accumulation with requantization to <code class=\"language-plaintext highlighter-rouge\">int8</code>/<code class=\"language-plaintext highlighter-rouge\">uint8</code> or dequantization to float. Also provides row-wise and 4-bit embedding quantization utilities for recommendation models.</li>\n  <li><strong>Operator coverage</strong>: Linear/GEMM and convolution (including groupwise), prepacked weight paths; optimized im2col/IGEMM; embedding bag and sparse length ops for recsys.</li>\n  <li><strong>Optimizations</strong>: Weight pre-packing into cache-friendly blocked layouts; vectorized micro-kernels; cache- and register-blocking; fused bias+activation+requant paths; threadpool parallelism.</li>\n  <li>\n    <p><strong>Typical use</strong>: PyTorch quantized ops on server/desktop CPUs (e.g., dynamic quantized Linear/LSTM, static <code class=\"language-plaintext highlighter-rouge\">int8</code> conv/linear). Best when you need maximum x86 performance for <code class=\"language-plaintext highlighter-rouge\">int8</code> inference.\n; Mobile CPUs)</p>\n  </li>\n  <li><strong>Target platforms</strong>: ARM/ARM64 mobile CPUs with NEON (Android/iOS); designed for low-power cores.</li>\n  <li><strong>Data types and quant schemes</strong>: <code class=\"language-plaintext highlighter-rouge\">uint8</code>/<code class=\"language-plaintext highlighter-rouge\">int8</code> activations (affine per-tensor), <code class=\"language-plaintext highlighter-rouge\">int8</code> per-channel weights; 32-bit accumulation with efficient requantization.</li>\n  <li><strong>Operator coverage</strong>: Quantized convolution/IGEMM, depthwise conv, deconvolution, fully connected (GEMM), pooling, various activation/elementwise ops.</li>\n  <li><strong>Optimizations</strong>: NHWC-friendly kernels; careful cache use for small batch/small filters; per-thread micro-kernels; fused post-ops to reduce memory traffic.</li>\n  <li><strong>Typical use</strong>: PyTorch Mobile’s quantized back end on ARM; good default for mobile <code class=\"language-plaintext highlighter-rouge\">int8</code> CNNs and fully connected layers where you need predictable latency on phones.</li>\n</ul>\n<p><strong>Typical use</strong>: PyTorch quantized ops on server/desktop CPUs (e.g., dynamic quantized Linear/LSTM, static <code class=\"language-plaintext highlighter-rouge\">int8</code> conv/linear). Best when you need maximum x86 performance for <code class=\"language-plaintext highlighter-rouge\">int8</code> inference.\n; Mobile CPUs)</p>",
      "contentMarkdown": "*   **Target platforms**: x86-64 server/desktop CPUs with SIMD (AVX2, AVX-512; newer stacks can leverage AMX on recent Intel parts via higher-level integrations).\n*   **Data types and quant schemes**: `int8`/`uint8` activations (affine per-tensor), `int8` weights (often symmetric per-channel). 32-bit accumulation with requantization to `int8`/`uint8` or dequantization to float. Also provides row-wise and 4-bit embedding quantization utilities for recommendation models.\n*   **Operator coverage**: Linear/GEMM and convolution (including groupwise), prepacked weight paths; optimized im2col/IGEMM; embedding bag and sparse length ops for recsys.\n*   **Optimizations**: Weight pre-packing into cache-friendly blocked layouts; vectorized micro-kernels; cache- and register-blocking; fused bias+activation+requant paths; threadpool parallelism.\n*   **Typical use**: PyTorch quantized ops on server/desktop CPUs (e.g., dynamic quantized Linear/LSTM, static `int8` conv/linear). Best when you need maximum x86 performance for `int8` inference. ; Mobile CPUs)\n    \n*   **Target platforms**: ARM/ARM64 mobile CPUs with NEON (Android/iOS); designed for low-power cores.\n*   **Data types and quant schemes**: `uint8`/`int8` activations (affine per-tensor), `int8` per-channel weights; 32-bit accumulation with efficient requantization.\n*   **Operator coverage**: Quantized convolution/IGEMM, depthwise conv, deconvolution, fully connected (GEMM), pooling, various activation/elementwise ops.\n*   **Optimizations**: NHWC-friendly kernels; careful cache use for small batch/small filters; per-thread micro-kernels; fused post-ops to reduce memory traffic.\n*   **Typical use**: PyTorch Mobile’s quantized back end on ARM; good default for mobile `int8` CNNs and fully connected layers where you need predictable latency on phones.\n\n**Typical use**: PyTorch quantized ops on server/desktop CPUs (e.g., dynamic quantized Linear/LSTM, static `int8` conv/linear). Best when you need maximum x86 performance for `int8` inference. ; Mobile CPUs)",
      "order": 64,
      "orderInChapter": 2,
      "difficulty": 5,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "embedding",
        "convolution",
        "cnn",
        "lstm",
        "optimization",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 246,
        "contentLength": 2997
      },
      "nextCards": [
        "ai-ml-runtimes-xnnpack-by-google-both-server-and-mobile-cpus-65",
        "ai-ml-runtimes-what-to-choose-when-66"
      ],
      "relatedCards": [
        "ai-ann-similarity-search-graph-based-methods-7",
        "ai-cnns-for-text-classification-1-d-convolutions-over-text-4",
        "ai-top-30-papers-dense-passage-retrieval-for-open-domain-question-a-28",
        "ai-top-30-papers-scaling-laws-for-neural-language-models-22",
        "ai-gpu-architecture-precision-optimization-19"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#fbgemm-(by-meta,;-server-cpus)",
      "scrapedAt": "2025-12-28T11:56:51.673Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-xnnpack-by-google-both-server-and-mobile-cpus-65",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Related: CPU Operator Libraries/Backends",
      "title": "XNNPACK (by Google,; Both Server and Mobile CPUs)",
      "subtitle": "Related: CPU Operator Libraries/Backends",
      "contentHtml": "<ul>\n  <li><strong>Target platforms</strong>: ARM/ARM64, x86-64, and WebAssembly (WASM); broadly portable and actively maintained.</li>\n  <li><strong>Data types and quant schemes</strong>: Strong <code class=\"language-plaintext highlighter-rouge\">float32</code>/<code class=\"language-plaintext highlighter-rouge\">float16</code>/<code class=\"language-plaintext highlighter-rouge\">bfloat16</code> coverage; mature QS8/QU8 (signed/unsigned 8-bit) inference for conv/GEMM/elementwise with per-channel weight scales. 32-bit accumulation and precise requantization.</li>\n  <li><strong>Operator coverage</strong>: Convolution (standard and depthwise), fully connected, pooling, deconvolution, elementwise math, softmax, activation functions, resize, etc.</li>\n  <li><strong>Optimizations</strong>: Handwritten micro-kernels per ISA (NEON/AVX/AVX512), NHWC dataflow, weight prepacking, GEMM/IGEMM families with cache-aware blocking, parallel work-stealing.</li>\n  <li><strong>Typical use</strong>: TensorFlow Lite’s XNNPACK delegate on CPU (float and <code class=\"language-plaintext highlighter-rouge\">int8</code>), and increasingly as a CPU backend in other frameworks for both float and quantized inference.</li>\n</ul>",
      "contentMarkdown": "*   **Target platforms**: ARM/ARM64, x86-64, and WebAssembly (WASM); broadly portable and actively maintained.\n*   **Data types and quant schemes**: Strong `float32`/`float16`/`bfloat16` coverage; mature QS8/QU8 (signed/unsigned 8-bit) inference for conv/GEMM/elementwise with per-channel weight scales. 32-bit accumulation and precise requantization.\n*   **Operator coverage**: Convolution (standard and depthwise), fully connected, pooling, deconvolution, elementwise math, softmax, activation functions, resize, etc.\n*   **Optimizations**: Handwritten micro-kernels per ISA (NEON/AVX/AVX512), NHWC dataflow, weight prepacking, GEMM/IGEMM families with cache-aware blocking, parallel work-stealing.\n*   **Typical use**: TensorFlow Lite’s XNNPACK delegate on CPU (float and `int8`), and increasingly as a CPU backend in other frameworks for both float and quantized inference.",
      "order": 65,
      "orderInChapter": 3,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "convolution",
        "optimization",
        "activation"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 101,
        "contentLength": 1212
      },
      "nextCards": [
        "ai-ml-runtimes-what-to-choose-when-66",
        "ai-ml-runtimes-design-notes-67"
      ],
      "relatedCards": [
        "ai-gpu-architecture-precision-optimization-19",
        "ai-top-30-papers-a-simple-neural-network-module-for-relational-reas-16",
        "ai-model-debugging-hyper-parameter-optimization-hpo-17",
        "ai-ann-similarity-search-graph-based-methods-7",
        "ai-conditional-random-fields-parameter-estimation-5"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#xnnpack-(by-google,;-both-server-and-mobile-cpus)",
      "scrapedAt": "2025-12-28T11:56:51.673Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-what-to-choose-when-66",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Related: CPU Operator Libraries/Backends",
      "title": "What to Choose When?",
      "subtitle": "Related: CPU Operator Libraries/Backends",
      "contentHtml": "<ul>\n  <li><strong>PyTorch (desktop/server CPU)</strong>: <code class=\"language-plaintext highlighter-rouge\">FBGEMM</code> is the usual backend for quantized ops; dynamic quantized Linear/LSTM also route here.</li>\n  <li><strong>PyTorch Mobile (ARM)</strong>: <code class=\"language-plaintext highlighter-rouge\">QNNPACK</code> is the historical default for quantized ops; some float operators can use XNNPACK.</li>\n  <li><strong>TensorFlow Lite (CPU)</strong>: <code class=\"language-plaintext highlighter-rouge\">XNNPACK</code> delegate accelerates many <code class=\"language-plaintext highlighter-rouge\">float32</code> and <code class=\"language-plaintext highlighter-rouge\">int8</code> ops; the interpreter falls back to reference kernels when needed.</li>\n  <li><strong>ONNX Runtime (CPU)</strong>: Uses its own CPU kernels by default, but can be built/integrated with these libraries in certain configurations; on mobile, builds commonly leverage <code class=\"language-plaintext highlighter-rouge\">XNNPACK</code>.</li>\n</ul>",
      "contentMarkdown": "*   **PyTorch (desktop/server CPU)**: `FBGEMM` is the usual backend for quantized ops; dynamic quantized Linear/LSTM also route here.\n*   **PyTorch Mobile (ARM)**: `QNNPACK` is the historical default for quantized ops; some float operators can use XNNPACK.\n*   **TensorFlow Lite (CPU)**: `XNNPACK` delegate accelerates many `float32` and `int8` ops; the interpreter falls back to reference kernels when needed.\n*   **ONNX Runtime (CPU)**: Uses its own CPU kernels by default, but can be built/integrated with these libraries in certain configurations; on mobile, builds commonly leverage `XNNPACK`.",
      "order": 66,
      "orderInChapter": 4,
      "difficulty": 5,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "lstm"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 84,
        "contentLength": 1025
      },
      "nextCards": [
        "ai-ml-runtimes-design-notes-67",
        "ai-ml-runtimes-comparative-analysis-68"
      ],
      "relatedCards": [
        "ai-gpu-architecture-multi-gpu-memory-coherency-31",
        "ai-gpu-architecture-fundamental-architectural-components-amd-32",
        "ai-gpu-architecture-execution-paradigms-amd-33",
        "ai-gpu-architecture-compute-architecture-evolution-amd-cdna-rdna-34",
        "ai-gpu-architecture-memory-architecture-evolution-36"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#what-to-choose-when?",
      "scrapedAt": "2025-12-28T11:56:51.673Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-design-notes-67",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Related: CPU Operator Libraries/Backends",
      "title": "Design Notes",
      "subtitle": "Related: CPU Operator Libraries/Backends",
      "contentHtml": "<ul>\n  <li><strong>Quant params are part of tensors</strong>: Kernels need correct scales/zero-points. For per-channel weights, pass channel-wise scales; activations are usually per-tensor.</li>\n  <li><strong>Accumulation width</strong>: 8-bit multiply-accumulates are summed into 32-bit accumulators to avoid overflow, then requantized. Watch for saturation when chaining ops.</li>\n  <li><strong>Prepack once</strong>: Pre-pack and reuse weights to avoid paying packing costs per inference. Many APIs expose prepacked weight objects.</li>\n  <li><strong>Layout matters</strong>: These libraries typically prefer NHWC for conv on mobile; mismatched layouts cause costly transposes.</li>\n  <li><strong>Dynamic vs static quant</strong>: Dynamic quantizes activations on-the-fly (common for Linear/LSTM), static uses calibration ranges. FBGEMM has strong dynamic Linear/LSTM paths.</li>\n  <li><strong>Activation ranges</strong>: Calibrate representative inputs to pick good scales and avoid clamp-heavy requantization.</li>\n</ul>",
      "contentMarkdown": "*   **Quant params are part of tensors**: Kernels need correct scales/zero-points. For per-channel weights, pass channel-wise scales; activations are usually per-tensor.\n*   **Accumulation width**: 8-bit multiply-accumulates are summed into 32-bit accumulators to avoid overflow, then requantized. Watch for saturation when chaining ops.\n*   **Prepack once**: Pre-pack and reuse weights to avoid paying packing costs per inference. Many APIs expose prepacked weight objects.\n*   **Layout matters**: These libraries typically prefer NHWC for conv on mobile; mismatched layouts cause costly transposes.\n*   **Dynamic vs static quant**: Dynamic quantizes activations on-the-fly (common for Linear/LSTM), static uses calibration ranges. FBGEMM has strong dynamic Linear/LSTM paths.\n*   **Activation ranges**: Calibrate representative inputs to pick good scales and avoid clamp-heavy requantization.",
      "order": 67,
      "orderInChapter": 5,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "lstm",
        "activation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 115,
        "contentLength": 1025
      },
      "nextCards": [
        "ai-ml-runtimes-comparative-analysis-68"
      ],
      "relatedCards": [
        "ai-gpu-architecture-forward-and-backward-passes-training-17",
        "ai-gpu-architecture-precision-optimization-19",
        "ai-model-debugging-grad-cam-15",
        "ai-tensorboard-plotting-losses-accuracies-and-weight-distribution-3",
        "ai-tensorboard-logging-images-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#design-notes",
      "scrapedAt": "2025-12-28T11:56:51.673Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-ml-runtimes-comparative-analysis-68",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "ML Runtimes",
      "articleSlug": "ml-runtimes",
      "chapter": "Related: CPU Operator Libraries/Backends",
      "title": "Comparative Analysis",
      "subtitle": "Related: CPU Operator Libraries/Backends",
      "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FBGEMM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>QNNPACK</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>XNNPACK</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Primary target</td>\n<td class=\"tg-tleft-valign-first\">x86-64 servers/desktops</td>\n<td class=\"tg-tleft-valign-first\">ARM/ARM64 mobile</td>\n<td class=\"tg-tleft-valign-second\">ARM/ARM64, x86-64, WASM</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Best precision</td>\n<td class=\"tg-tleft-valign-first\">`int8` quant (server)</td>\n<td class=\"tg-tleft-valign-first\">`int8` quant (mobile)</td>\n<td class=\"tg-tleft-valign-second\">`float32`/`float16` plus `int8`</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Typical consumers</td>\n<td class=\"tg-tleft-valign-first\">PyTorch quant (server)</td>\n<td class=\"tg-tleft-valign-first\">PyTorch Mobile quant</td>\n<td class=\"tg-tleft-valign-second\">TFLite delegate; some PyTorch CPU paths</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Conv layout</td>\n<td class=\"tg-tleft-valign-first\">NCHW/NHWC with prepack</td>\n<td class=\"tg-tleft-valign-first\">NHWC</td>\n<td class=\"tg-tleft-valign-second\">NHWC</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Weight handling</td>\n<td class=\"tg-tleft-valign-first\">Prepacked per-channel `int8`</td>\n<td class=\"tg-tleft-valign-first\">Prepacked per-channel `int8`</td>\n<td class=\"tg-tleft-valign-second\">Prepacked per-channel `int8`</td>\n</tr>\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Attribute</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>FBGEMM</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>QNNPACK</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>XNNPACK</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class=\"tg-tleft-valign-first\">Primary target</td>\n<td class=\"tg-tleft-valign-first\">x86-64 servers/desktops</td>\n<td class=\"tg-tleft-valign-first\">ARM/ARM64 mobile</td>\n<td class=\"tg-tleft-valign-second\">ARM/ARM64, x86-64, WASM</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Best precision</td>\n<td class=\"tg-tleft-valign-first\">`int8` quant (server)</td>\n<td class=\"tg-tleft-valign-first\">`int8` quant (mobile)</td>\n<td class=\"tg-tleft-valign-second\">`float32`/`float16` plus `int8`</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Typical consumers</td>\n<td class=\"tg-tleft-valign-first\">PyTorch quant (server)</td>\n<td class=\"tg-tleft-valign-first\">PyTorch Mobile quant</td>\n<td class=\"tg-tleft-valign-second\">TFLite delegate; some PyTorch CPU paths</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Conv layout</td>\n<td class=\"tg-tleft-valign-first\">NCHW/NHWC with prepack</td>\n<td class=\"tg-tleft-valign-first\">NHWC</td>\n<td class=\"tg-tleft-valign-second\">NHWC</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Weight handling</td>\n<td class=\"tg-tleft-valign-first\">Prepacked per-channel `int8`</td>\n<td class=\"tg-tleft-valign-first\">Prepacked per-channel `int8`</td>\n<td class=\"tg-tleft-valign-second\">Prepacked per-channel `int8`</td>\n</tr>\n</tbody>\n</table>",
      "contentMarkdown": "**Attribute**\n\n**FBGEMM**\n\n**QNNPACK**\n\n**XNNPACK**\n\nPrimary target\n\nx86-64 servers/desktops\n\nARM/ARM64 mobile\n\nARM/ARM64, x86-64, WASM\n\nBest precision\n\n\\`int8\\` quant (server)\n\n\\`int8\\` quant (mobile)\n\n\\`float32\\`/\\`float16\\` plus \\`int8\\`\n\nTypical consumers\n\nPyTorch quant (server)\n\nPyTorch Mobile quant\n\nTFLite delegate; some PyTorch CPU paths\n\nConv layout\n\nNCHW/NHWC with prepack\n\nNHWC\n\nNHWC\n\nWeight handling\n\nPrepacked per-channel \\`int8\\`\n\nPrepacked per-channel \\`int8\\`\n\nPrepacked per-channel \\`int8\\`\n\n**Attribute**\n\n**FBGEMM**\n\n**QNNPACK**\n\n**XNNPACK**\n\nPrimary target\n\nx86-64 servers/desktops\n\nARM/ARM64 mobile\n\nARM/ARM64, x86-64, WASM\n\nBest precision\n\n\\`int8\\` quant (server)\n\n\\`int8\\` quant (mobile)\n\n\\`float32\\`/\\`float16\\` plus \\`int8\\`\n\nTypical consumers\n\nPyTorch quant (server)\n\nPyTorch Mobile quant\n\nTFLite delegate; some PyTorch CPU paths\n\nConv layout\n\nNCHW/NHWC with prepack\n\nNHWC\n\nNHWC\n\nWeight handling\n\nPrepacked per-channel \\`int8\\`\n\nPrepacked per-channel \\`int8\\`\n\nPrepacked per-channel \\`int8\\`",
      "order": 68,
      "orderInChapter": 6,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 112,
        "contentLength": 3227
      },
      "nextCards": [],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-synchronization-and-communication-12",
        "ai-gpu-architecture-memory-management-and-transfer-15",
        "ai-gpu-architecture-inference-deployment-18"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ml-runtimes/#comparative-analysis",
      "scrapedAt": "2025-12-28T11:56:51.673Z",
      "siblings": [
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-ml-runtimes-architecture-by-runtime-2",
        "ai-ml-runtimes-overview-3",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-implementation-details-5"
      ]
    },
    {
      "id": "ai-bayes-theorem-drug-screening-1",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Bayes’ Theorem",
      "articleSlug": "bayes-theorem",
      "chapter": "Practical Example with Python Code",
      "title": "Drug Screening",
      "subtitle": "Practical Example with Python Code",
      "contentHtml": "<ul>\n  <li>\n    <p>As an example, let’s apply Bayes’ rule to a problem of drug screening (e.g. mandatory testing for federal or many other jobs which promise a drug-free work environment).</p>\n  </li>\n  <li>\n    <p>Suppose that a test for using a particular drug is 97% sensitive and 95% specific. That is, the test will produce 97% true positive results for drug users and 95% true negative results for non-drug users. These are the pieces of data that any screening test will have from their history of tests. Bayes’ rule allows us to use this kind of data-driven knowledge to calculate the final probability.</p>\n  </li>\n  <li>\n    <p>Suppose, we also know that 0.5% of the general population are users of the drug. What is the probability that a randomly selected individual with a positive test is a drug user?</p>\n  </li>\n  <li>\n    <p><strong>Note, this is the crucial piece of ‘Prior’</strong> which is a piece of generalized knowledge about the common prevalence rate. This is our prior belief about the probability of a random test subject being a drug user. That means if <strong>we choose a random person from the general population, without any testing, we can only say that there is a 0.5% chance of that person being a drug-user.</strong></p>\n  </li>\n  <li>\n    <p>How to use Bayes’ rule then, in this situation?</p>\n  </li>\n  <li>\n    <p>We will write a custom function that accepts the test capabilities and the prior knowledge of drug user percentage as input and produces the output probability of a test-taker being a user based on a positive result.</p>\n  </li>\n  <li>\n    <p>Here is the formula for computing as per the Bayes’ rule:</p>\n  </li>\n</ul>\n<p>As an example, let’s apply Bayes’ rule to a problem of drug screening (e.g. mandatory testing for federal or many other jobs which promise a drug-free work environment).</p>\n<p>Suppose that a test for using a particular drug is 97% sensitive and 95% specific. That is, the test will produce 97% true positive results for drug users and 95% true negative results for non-drug users. These are the pieces of data that any screening test will have from their history of tests. Bayes’ rule allows us to use this kind of data-driven knowledge to calculate the final probability.</p>\n<p>Suppose, we also know that 0.5% of the general population are users of the drug. What is the probability that a randomly selected individual with a positive test is a drug user?</p>\n<p><strong>Note, this is the crucial piece of ‘Prior’</strong> which is a piece of generalized knowledge about the common prevalence rate. This is our prior belief about the probability of a random test subject being a drug user. That means if <strong>we choose a random person from the general population, without any testing, we can only say that there is a 0.5% chance of that person being a drug-user.</strong></p>\n<p>How to use Bayes’ rule then, in this situation?</p>\n<p>We will write a custom function that accepts the test capabilities and the prior knowledge of drug user percentage as input and produces the output probability of a test-taker being a user based on a positive result.</p>\n<p>Here is the formula for computing as per the Bayes’ rule:</p>\n<p><img src=\"/primers/ai/assets/bayes-theorem/bayes5.png\" alt=\"\"></p>\n<ul>\n  <li>If we run the function with the given data, we get the following result,</li>\n</ul>\n<p><img src=\"/primers/ai/assets/bayes-theorem/bayes10.png\" alt=\"\"></p>",
      "contentMarkdown": "*   As an example, let’s apply Bayes’ rule to a problem of drug screening (e.g. mandatory testing for federal or many other jobs which promise a drug-free work environment).\n    \n*   Suppose that a test for using a particular drug is 97% sensitive and 95% specific. That is, the test will produce 97% true positive results for drug users and 95% true negative results for non-drug users. These are the pieces of data that any screening test will have from their history of tests. Bayes’ rule allows us to use this kind of data-driven knowledge to calculate the final probability.\n    \n*   Suppose, we also know that 0.5% of the general population are users of the drug. What is the probability that a randomly selected individual with a positive test is a drug user?\n    \n*   **Note, this is the crucial piece of ‘Prior’** which is a piece of generalized knowledge about the common prevalence rate. This is our prior belief about the probability of a random test subject being a drug user. That means if **we choose a random person from the general population, without any testing, we can only say that there is a 0.5% chance of that person being a drug-user.**\n    \n*   How to use Bayes’ rule then, in this situation?\n    \n*   We will write a custom function that accepts the test capabilities and the prior knowledge of drug user percentage as input and produces the output probability of a test-taker being a user based on a positive result.\n    \n*   Here is the formula for computing as per the Bayes’ rule:\n    \n\nAs an example, let’s apply Bayes’ rule to a problem of drug screening (e.g. mandatory testing for federal or many other jobs which promise a drug-free work environment).\n\nSuppose that a test for using a particular drug is 97% sensitive and 95% specific. That is, the test will produce 97% true positive results for drug users and 95% true negative results for non-drug users. These are the pieces of data that any screening test will have from their history of tests. Bayes’ rule allows us to use this kind of data-driven knowledge to calculate the final probability.\n\nSuppose, we also know that 0.5% of the general population are users of the drug. What is the probability that a randomly selected individual with a positive test is a drug user?\n\n**Note, this is the crucial piece of ‘Prior’** which is a piece of generalized knowledge about the common prevalence rate. This is our prior belief about the probability of a random test subject being a drug user. That means if **we choose a random person from the general population, without any testing, we can only say that there is a 0.5% chance of that person being a drug-user.**\n\nHow to use Bayes’ rule then, in this situation?\n\nWe will write a custom function that accepts the test capabilities and the prior knowledge of drug user percentage as input and produces the output probability of a test-taker being a user based on a positive result.\n\nHere is the formula for computing as per the Bayes’ rule:\n\n![](/primers/ai/assets/bayes-theorem/bayes5.png)\n\n*   If we run the function with the given data, we get the following result,\n\n![](/primers/ai/assets/bayes-theorem/bayes10.png)",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 3,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 532,
        "contentLength": 3437
      },
      "nextCards": [
        "ai-bayes-theorem-what-is-fascinating-here-2",
        "ai-bayes-theorem-what-level-of-test-capability-is-needed-to-improve-3"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/bayes-theorem/#drug-screening",
      "scrapedAt": "2025-12-28T11:57:01.819Z",
      "siblings": [
        "ai-bayes-theorem-what-is-fascinating-here-2",
        "ai-bayes-theorem-what-level-of-test-capability-is-needed-to-improve-3"
      ]
    },
    {
      "id": "ai-bayes-theorem-what-is-fascinating-here-2",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Bayes’ Theorem",
      "articleSlug": "bayes-theorem",
      "chapter": "Practical Example with Python Code",
      "title": "What is Fascinating Here?",
      "subtitle": "Practical Example with Python Code",
      "contentHtml": "<ul>\n  <li>\n    <p>Even with a test that is 97% correct for catching positive cases, and 95% correct for rejecting negative cases, the true probability of being a drug-user with a positive result is only 8.9%!</p>\n  </li>\n  <li>\n    <p>If you look at the computations, this is because of the extremely low prevalence rate. <strong>The number of false positives outweighs the number of true positives.</strong></p>\n  </li>\n  <li>\n    <p>For example, if 1000 individuals are tested, there are expected to be 995 non-users and 5 users. From the 995 non-users, 0.05 × 995 ≃ 50 false positives are expected. From the 5 users, 0.95 × 5 ≈ 5 true positives are expected. Out of 55 positive results, only 5 are genuine!</p>\n  </li>\n  <li>\n    <p>Let’s see how the probability changes with the prevalence rate.</p>\n  </li>\n</ul>\n<p>Even with a test that is 97% correct for catching positive cases, and 95% correct for rejecting negative cases, the true probability of being a drug-user with a positive result is only 8.9%!</p>\n<p>If you look at the computations, this is because of the extremely low prevalence rate. <strong>The number of false positives outweighs the number of true positives.</strong></p>\n<p>For example, if 1000 individuals are tested, there are expected to be 995 non-users and 5 users. From the 995 non-users, 0.05 × 995 ≃ 50 false positives are expected. From the 5 users, 0.95 × 5 ≈ 5 true positives are expected. Out of 55 positive results, only 5 are genuine!</p>\n<p>Let’s see how the probability changes with the prevalence rate.</p>\n<p><img src=\"/primers/ai/assets/bayes-theorem/bayes6.png\" alt=\"\"></p>\n<ul>\n  <li>Note, your decision depends on the probability threshold. Currently, it is set to 0.5. You can lower it if necessary. But, at the threshold of 0.5, <strong>you need to have an almost 4.8% prevalence rate to catch a user with a single positive test result.</strong></li>\n</ul>",
      "contentMarkdown": "*   Even with a test that is 97% correct for catching positive cases, and 95% correct for rejecting negative cases, the true probability of being a drug-user with a positive result is only 8.9%!\n    \n*   If you look at the computations, this is because of the extremely low prevalence rate. **The number of false positives outweighs the number of true positives.**\n    \n*   For example, if 1000 individuals are tested, there are expected to be 995 non-users and 5 users. From the 995 non-users, 0.05 × 995 ≃ 50 false positives are expected. From the 5 users, 0.95 × 5 ≈ 5 true positives are expected. Out of 55 positive results, only 5 are genuine!\n    \n*   Let’s see how the probability changes with the prevalence rate.\n    \n\nEven with a test that is 97% correct for catching positive cases, and 95% correct for rejecting negative cases, the true probability of being a drug-user with a positive result is only 8.9%!\n\nIf you look at the computations, this is because of the extremely low prevalence rate. **The number of false positives outweighs the number of true positives.**\n\nFor example, if 1000 individuals are tested, there are expected to be 995 non-users and 5 users. From the 995 non-users, 0.05 × 995 ≃ 50 false positives are expected. From the 5 users, 0.95 × 5 ≈ 5 true positives are expected. Out of 55 positive results, only 5 are genuine!\n\nLet’s see how the probability changes with the prevalence rate.\n\n![](/primers/ai/assets/bayes-theorem/bayes6.png)\n\n*   Note, your decision depends on the probability threshold. Currently, it is set to 0.5. You can lower it if necessary. But, at the threshold of 0.5, **you need to have an almost 4.8% prevalence rate to catch a user with a single positive test result.**",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 2,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 293,
        "contentLength": 1907
      },
      "nextCards": [
        "ai-bayes-theorem-what-level-of-test-capability-is-needed-to-improve-3"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/bayes-theorem/#what-is-fascinating-here?",
      "scrapedAt": "2025-12-28T11:57:01.820Z",
      "siblings": [
        "ai-bayes-theorem-drug-screening-1",
        "ai-bayes-theorem-what-level-of-test-capability-is-needed-to-improve-3"
      ]
    },
    {
      "id": "ai-bayes-theorem-what-level-of-test-capability-is-needed-to-improve-3",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Bayes’ Theorem",
      "articleSlug": "bayes-theorem",
      "chapter": "Practical Example with Python Code",
      "title": "What Level of Test Capability is Needed to Improve This Scenario?",
      "subtitle": "Practical Example with Python Code",
      "contentHtml": "<ul>\n  <li>We saw that the test sensitivity and specificity impact this computation strongly. So, we may like to see what kind of capabilities are needed to improve the likelihood of catching drug users.</li>\n</ul>\n<p><img src=\"../assets/bayes-theorem/bayes7.png\" align=\"center\" style=\"background-color: #fff; margin: 10px auto\"></p>\n<p><img src=\"../assets/bayes-theorem/bayes8.png\" align=\"center\" style=\"background-color: #fff; margin: 10px auto\"></p>\n<ul>\n  <li>\n    <p>The plots above clearly show that even with close to 100% sensitivity, we don’t gain much at all. However, <strong>the probability response is highly non-linear with respect to the specificity of the test</strong> and as it reaches perfection, we get a large increase in the probability. Therefore, <strong>all R&amp;D efforts should be focused on how to improve the specificity of the test.</strong></p>\n  </li>\n  <li>\n    <p>This conclusion can be intuitively derived from the fact that the main issue with having low probability is the low prevalence rate. Therefore, catching non-users correctly (i.e., improving specificity) is the area where we should focus on because they are much larger in numbers than the user.</p>\n  </li>\n  <li>\n    <p>Negative examples are much higher in number than the Positive examples in this problem. Therefore, the True Negative performance of the test should be excellent.</p>\n  </li>\n</ul>\n<p>The plots above clearly show that even with close to 100% sensitivity, we don’t gain much at all. However, <strong>the probability response is highly non-linear with respect to the specificity of the test</strong> and as it reaches perfection, we get a large increase in the probability. Therefore, <strong>all R&amp;D efforts should be focused on how to improve the specificity of the test.</strong></p>\n<p>This conclusion can be intuitively derived from the fact that the main issue with having low probability is the low prevalence rate. Therefore, catching non-users correctly (i.e., improving specificity) is the area where we should focus on because they are much larger in numbers than the user.</p>\n<p>Negative examples are much higher in number than the Positive examples in this problem. Therefore, the True Negative performance of the test should be excellent.</p>",
      "contentMarkdown": "*   We saw that the test sensitivity and specificity impact this computation strongly. So, we may like to see what kind of capabilities are needed to improve the likelihood of catching drug users.\n\n![](../assets/bayes-theorem/bayes7.png)\n\n![](../assets/bayes-theorem/bayes8.png)\n\n*   The plots above clearly show that even with close to 100% sensitivity, we don’t gain much at all. However, **the probability response is highly non-linear with respect to the specificity of the test** and as it reaches perfection, we get a large increase in the probability. Therefore, **all R&D efforts should be focused on how to improve the specificity of the test.**\n    \n*   This conclusion can be intuitively derived from the fact that the main issue with having low probability is the low prevalence rate. Therefore, catching non-users correctly (i.e., improving specificity) is the area where we should focus on because they are much larger in numbers than the user.\n    \n*   Negative examples are much higher in number than the Positive examples in this problem. Therefore, the True Negative performance of the test should be excellent.\n    \n\nThe plots above clearly show that even with close to 100% sensitivity, we don’t gain much at all. However, **the probability response is highly non-linear with respect to the specificity of the test** and as it reaches perfection, we get a large increase in the probability. Therefore, **all R&D efforts should be focused on how to improve the specificity of the test.**\n\nThis conclusion can be intuitively derived from the fact that the main issue with having low probability is the low prevalence rate. Therefore, catching non-users correctly (i.e., improving specificity) is the area where we should focus on because they are much larger in numbers than the user.\n\nNegative examples are much higher in number than the Positive examples in this problem. Therefore, the True Negative performance of the test should be excellent.",
      "order": 3,
      "orderInChapter": 3,
      "difficulty": 2,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 306,
        "contentLength": 2278
      },
      "nextCards": [],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/bayes-theorem/#what-level-of-test-capability-is-needed-to-improve-this-scenario?",
      "scrapedAt": "2025-12-28T11:57:01.820Z",
      "siblings": [
        "ai-bayes-theorem-drug-screening-1",
        "ai-bayes-theorem-what-is-fascinating-here-2"
      ]
    },
    {
      "id": "ai-probability-calibration-calibrating-a-classifier-1",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Probability Calibration",
      "articleSlug": "probability-calibration",
      "chapter": "Calibration Curves",
      "title": "Calibrating a Classifier",
      "subtitle": "Calibration Curves",
      "contentHtml": "<ul>\n  <li>Calibrating a classifier consists of fitting a regressor (called a calibrator) that maps the output of the classifier (as given by <a href=\"https://scikit-learn.org/stable/glossary.html#term-decision_function\"><code class=\"language-plaintext highlighter-rouge\">decision_function</code></a> or <a href=\"https://scikit-learn.org/stable/glossary.html#term-predict_proba\"><code class=\"language-plaintext highlighter-rouge\">predict_proba</code></a>) to a calibrated probability in <code class=\"language-plaintext highlighter-rouge\">[0, 1]</code>. Denoting the output of the classifier for a given sample by <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.58em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"msubsup\" id=\"MathJax-Span-3\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-5\"><span class=\"mrow\" id=\"MathJax-Span-6\"><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">f_{i}</script>, the calibrator tries to predict <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mrow><mo>(</mo><mrow><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>&amp;#x2223;</mo><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-8\" style=\"width: 6.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1005.21em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-9\"><span class=\"mi\" id=\"MathJax-Span-10\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mrow\" id=\"MathJax-Span-11\" style=\"padding-left: 0.211em;\"><span class=\"mo\" id=\"MathJax-Span-12\" style=\"vertical-align: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">(</span></span><span class=\"mrow\" id=\"MathJax-Span-13\"><span class=\"msubsup\" id=\"MathJax-Span-14\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-16\"><span class=\"mrow\" id=\"MathJax-Span-17\"><span class=\"mi\" id=\"MathJax-Span-18\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-19\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mn\" id=\"MathJax-Span-20\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">1</span><span class=\"mo\" id=\"MathJax-Span-21\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-22\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-24\"><span class=\"mrow\" id=\"MathJax-Span-25\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-27\" style=\"vertical-align: 0em;\"><span style=\"font-family: STIXGeneral-Regular;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mrow><mo>(</mo><mrow><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>∣</mo><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">p\\left(y_{i}=1 \\mid f_{i}\\right)</script>.</li>\n  <li>The samples that are used to fit the calibrator should not be the same samples used to fit the classifier, as this would introduce bias. This is because performance of the classifier on its training data would be better than for novel data. Using the classifier output of training data to fit the calibrator would thus result in a biased calibrator that maps to probabilities closer to 0 and 1 than it should.</li>\n</ul>",
      "contentMarkdown": "*   Calibrating a classifier consists of fitting a regressor (called a calibrator) that maps the output of the classifier (as given by [`decision_function`](https://scikit-learn.org/stable/glossary.html#term-decision_function) or [`predict_proba`](https://scikit-learn.org/stable/glossary.html#term-predict_proba)) to a calibrated probability in `[0, 1]`. Denoting the output of the classifier for a given sample by fifif\\_{i}, the calibrator tries to predict p(yi\\=1∣fi)p(yi\\=1∣fi)p\\\\left(y\\_{i}=1 \\\\mid f\\_{i}\\\\right).\n*   The samples that are used to fit the calibrator should not be the same samples used to fit the classifier, as this would introduce bias. This is because performance of the classifier on its training data would be better than for novel data. Using the classifier output of training data to fit the calibrator would thus result in a biased calibrator that maps to probabilities closer to 0 and 1 than it should.",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 126,
        "contentLength": 7197
      },
      "nextCards": [
        "ai-probability-calibration-usage-2"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/probability-calibration/#calibrating-a-classifier",
      "scrapedAt": "2025-12-28T11:57:07.018Z",
      "siblings": [
        "ai-probability-calibration-usage-2"
      ]
    },
    {
      "id": "ai-probability-calibration-usage-2",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Probability Calibration",
      "articleSlug": "probability-calibration",
      "chapter": "Calibration Curves",
      "title": "Usage",
      "subtitle": "Calibration Curves",
      "contentHtml": "<ul>\n  <li>The <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV\">CalibratedClassifierCV</a> class from scikit-Learn is used to calibrate a classifier.</li>\n</ul>",
      "contentMarkdown": "*   The [CalibratedClassifierCV](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV) class from scikit-Learn is used to calibrate a classifier.",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 12,
        "contentLength": 262
      },
      "nextCards": [],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/probability-calibration/#usage",
      "scrapedAt": "2025-12-28T11:57:07.018Z",
      "siblings": [
        "ai-probability-calibration-calibrating-a-classifier-1"
      ]
    },
    {
      "id": "ai-multiclass-vs-multilabel-classification-multi-class-classification-1",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Multiclass vs. Multilabel Classification",
      "articleSlug": "multiclass-vs-multilabel-classification",
      "chapter": "Graphical Interpretation",
      "title": "Multi-class Classification",
      "subtitle": "Graphical Interpretation",
      "contentHtml": "<ul>\n  <li>Recall that binary classification involves bucketing a sample into either of two categories. On the other hand, in the multi-class case, there are more than two classes in total.</li>\n  <li>One column = one class (one-hot encoding).</li>\n</ul>\n<p><img src=\"/primers/ai/assets/multiclass-vs-multilabel-classification/mc.png\" alt=\"\"></p>",
      "contentMarkdown": "*   Recall that binary classification involves bucketing a sample into either of two categories. On the other hand, in the multi-class case, there are more than two classes in total.\n*   One column = one class (one-hot encoding).\n\n![](/primers/ai/assets/multiclass-vs-multilabel-classification/mc.png)",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 39,
        "contentLength": 346
      },
      "nextCards": [
        "ai-multiclass-vs-multilabel-classification-multi-label-classification-2",
        "ai-multiclass-vs-multilabel-classification-multilabel-multi-class-classification-3"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/multiclass-vs-multilabel-classification/#multi-class-classification",
      "scrapedAt": "2025-12-28T11:57:12.009Z",
      "siblings": [
        "ai-multiclass-vs-multilabel-classification-multi-label-classification-2",
        "ai-multiclass-vs-multilabel-classification-multilabel-multi-class-classification-3"
      ]
    },
    {
      "id": "ai-multiclass-vs-multilabel-classification-multi-label-classification-2",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Multiclass vs. Multilabel Classification",
      "articleSlug": "multiclass-vs-multilabel-classification",
      "chapter": "Graphical Interpretation",
      "title": "Multi-label Classification",
      "subtitle": "Graphical Interpretation",
      "contentHtml": "<ul>\n  <li>One column = one class,</li>\n  <li>In the multi-label case, one sample might be assigned more than one class.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/multiclass-vs-multilabel-classification/ml.png\" alt=\"\"></p>",
      "contentMarkdown": "*   One column = one class,\n*   In the multi-label case, one sample might be assigned more than one class.\n\n![](/primers/ai/assets/multiclass-vs-multilabel-classification/ml.png)",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 21,
        "contentLength": 223
      },
      "nextCards": [
        "ai-multiclass-vs-multilabel-classification-multilabel-multi-class-classification-3"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/multiclass-vs-multilabel-classification/#multi-label-classification",
      "scrapedAt": "2025-12-28T11:57:12.009Z",
      "siblings": [
        "ai-multiclass-vs-multilabel-classification-multi-class-classification-1",
        "ai-multiclass-vs-multilabel-classification-multilabel-multi-class-classification-3"
      ]
    },
    {
      "id": "ai-multiclass-vs-multilabel-classification-multilabel-multi-class-classification-3",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Multiclass vs. Multilabel Classification",
      "articleSlug": "multiclass-vs-multilabel-classification",
      "chapter": "Graphical Interpretation",
      "title": "Multilabel Multi-class Classification",
      "subtitle": "Graphical Interpretation",
      "contentHtml": "<ul>\n  <li>As a side note, nothing prevents you from having a multilabel multi-class classification problem, e.g.:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/multiclass-vs-multilabel-classification/momc.png\" alt=\"\"></p>",
      "contentMarkdown": "*   As a side note, nothing prevents you from having a multilabel multi-class classification problem, e.g.:\n\n![](/primers/ai/assets/multiclass-vs-multilabel-classification/momc.png)",
      "order": 3,
      "orderInChapter": 3,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 17,
        "contentLength": 219
      },
      "nextCards": [],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/multiclass-vs-multilabel-classification/#multilabel-multi-class-classification",
      "scrapedAt": "2025-12-28T11:57:12.009Z",
      "siblings": [
        "ai-multiclass-vs-multilabel-classification-multi-class-classification-1",
        "ai-multiclass-vs-multilabel-classification-multi-label-classification-2"
      ]
    },
    {
      "id": "ai-pytorch-vs-tensorflow-pytorch-1",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "PyTorch vs. TensorFlow",
      "articleSlug": "pytorch-vs-tensorflow",
      "chapter": "PyTorch or TensorFlow?",
      "title": "PyTorch",
      "subtitle": "PyTorch or TensorFlow?",
      "contentHtml": "<h4>(+)</h4>\n<ul>\n  <li>Younger, but also well documented and fast-growing community.</li>\n  <li>Preferred in research/academia.</li>\n  <li>More pythonic and NumPy-like approach, designed for faster prototyping and research.</li>\n  <li>Automatic differentiation using <a href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\">Autograd</a> to compute the backward pass given a forward pass of a network (note that TensorFlow v2 has this capability).</li>\n  <li>Uses eager execution mode by default (i.e., dynamic graph), compared to TensorFlow v1’s static graph paradigm.</li>\n  <li>Follows the channel-first (also called spatial-first) convention, i.e., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 5.523em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.586em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.59em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-4\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-5\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-6\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-8\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-9\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">N, C, H, W</script> for images which makes it <a href=\"https://stackoverflow.com/questions/44280335/how-much-faster-is-nchw-compared-to-nhwc-in-tensorflow-cudnn\">faster</a> than TensorFlow’s channel-last convention.</li>\n  <li>Easy to debug and customize.</li>\n</ul>\n<h4 id=\"-\">(-)</h4>\n<ul>\n  <li>Need to manually zero out gradients using <code class=\"language-plaintext highlighter-rouge\">zero_grad()</code> at the start of a new mini-batch.\n    <ul>\n      <li>this is because <code class=\"language-plaintext highlighter-rouge\">loss.backward()</code> accumulates gradients (and doesn’t overwrite them), and you don’t want to mix up gradients between mini-batches.</li>\n    </ul>\n  </li>\n  <li>Using a GPU requires code changes to copy your model’s parameters/tensors over to your GPU.</li>\n</ul>\n<ul>\n      <li>this is because <code class=\"language-plaintext highlighter-rouge\">loss.backward()</code> accumulates gradients (and doesn’t overwrite them), and you don’t want to mix up gradients between mini-batches.</li>\n    </ul>",
      "contentMarkdown": "#### (+)\n\n*   Younger, but also well documented and fast-growing community.\n*   Preferred in research/academia.\n*   More pythonic and NumPy-like approach, designed for faster prototyping and research.\n*   Automatic differentiation using [Autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) to compute the backward pass given a forward pass of a network (note that TensorFlow v2 has this capability).\n*   Uses eager execution mode by default (i.e., dynamic graph), compared to TensorFlow v1’s static graph paradigm.\n*   Follows the channel-first (also called spatial-first) convention, i.e., N,C,H,WN,C,H,WN, C, H, W for images which makes it [faster](https://stackoverflow.com/questions/44280335/how-much-faster-is-nchw-compared-to-nhwc-in-tensorflow-cudnn) than TensorFlow’s channel-last convention.\n*   Easy to debug and customize.\n\n#### (-)\n\n*   Need to manually zero out gradients using `zero_grad()` at the start of a new mini-batch.\n    *   this is because `loss.backward()` accumulates gradients (and doesn’t overwrite them), and you don’t want to mix up gradients between mini-batches.\n*   Using a GPU requires code changes to copy your model’s parameters/tensors over to your GPU.\n\n*   this is because `loss.backward()` accumulates gradients (and doesn’t overwrite them), and you don’t want to mix up gradients between mini-batches.",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 173,
        "contentLength": 3937
      },
      "nextCards": [
        "ai-pytorch-vs-tensorflow-tensorflow-2"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/pytorch-vs-tensorflow/#pytorch",
      "scrapedAt": "2025-12-28T11:57:21.883Z",
      "siblings": [
        "ai-pytorch-vs-tensorflow-tensorflow-2"
      ]
    },
    {
      "id": "ai-pytorch-vs-tensorflow-tensorflow-2",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "PyTorch vs. TensorFlow",
      "articleSlug": "pytorch-vs-tensorflow",
      "chapter": "PyTorch or TensorFlow?",
      "title": "TensorFlow",
      "subtitle": "PyTorch or TensorFlow?",
      "contentHtml": "<h4 id=\"-1\">(+)</h4>\n<ul>\n  <li>Mature, most of the models and layers are already implemented in the library (has <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras\">Keras</a> builtin at <code class=\"language-plaintext highlighter-rouge\">tf.keras</code>).</li>\n  <li>Built for large-scale deployment and is the tool-of-choice in the industry.</li>\n  <li>Has some very useful tools like TensorBoard for visualization (although <a href=\"https://github.com/lanpa/tensorboardX\">TensorBoardX</a> now exists for PyTorch).</li>\n  <li>TensorFlow v2 uses eager execution/dynamic graphs (but TensorFlow v1) just like PyTorch v1.</li>\n  <li>No need to manually zero out gradients for the backward pass.</li>\n  <li>Transparent use of the GPU.</li>\n</ul>\n<h4 id=\"--1\">(-)</h4>\n<ul>\n  <li>Some ramp-up time is needed to understand some of the concepts (session, graph, variable scope, etc.), especially with TensorFlow v1.</li>\n  <li>Follows the channel-last convention, i.e., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>N</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo>,</mo><mi>C</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-10\" style=\"width: 5.523em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.586em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.59em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-11\"><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-13\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-14\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">H<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-16\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">W<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-17\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">C<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>W</mi><mo>,</mo><mi>C</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">N, H, W, C</script> for images due to legacy reasons, which makes it <a href=\"https://stackoverflow.com/questions/44280335/how-much-faster-is-nchw-compared-to-nhwc-in-tensorflow-cudnn\">slower</a>.</li>\n  <li>Can be harder to debug.</li>\n</ul>",
      "contentMarkdown": "#### (+)\n\n*   Mature, most of the models and layers are already implemented in the library (has [Keras](https://www.tensorflow.org/api_docs/python/tf/keras) builtin at `tf.keras`).\n*   Built for large-scale deployment and is the tool-of-choice in the industry.\n*   Has some very useful tools like TensorBoard for visualization (although [TensorBoardX](https://github.com/lanpa/tensorboardX) now exists for PyTorch).\n*   TensorFlow v2 uses eager execution/dynamic graphs (but TensorFlow v1) just like PyTorch v1.\n*   No need to manually zero out gradients for the backward pass.\n*   Transparent use of the GPU.\n\n#### (-)\n\n*   Some ramp-up time is needed to understand some of the concepts (session, graph, variable scope, etc.), especially with TensorFlow v1.\n*   Follows the channel-last convention, i.e., N,H,W,CN,H,W,CN, H, W, C for images due to legacy reasons, which makes it [slower](https://stackoverflow.com/questions/44280335/how-much-faster-is-nchw-compared-to-nhwc-in-tensorflow-cudnn).\n*   Can be harder to debug.",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 130,
        "contentLength": 3457
      },
      "nextCards": [],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/pytorch-vs-tensorflow/#tensorflow",
      "scrapedAt": "2025-12-28T11:57:21.883Z",
      "siblings": [
        "ai-pytorch-vs-tensorflow-pytorch-1"
      ]
    },
    {
      "id": "ai-ann-similarity-search-real-world-applications-1",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "What is Similarity Search?",
      "title": "Real-World Applications",
      "subtitle": "What is Similarity Search?",
      "contentHtml": "<ul>\n  <li>\n    <p>In modern machine learning and artificial intelligence systems, similarity search serves as a core component in scenarios such as:</p>\n\n    <ul>\n      <li><strong>Image retrieval</strong>: Given a sample image, retrieve visually or contextually similar images from a database (e.g., identifying similar fashion items or duplicate product listings).</li>\n      <li><strong>Document retrieval</strong>: Powering search engines that return semantically relevant documents in response to a textual query.</li>\n      <li><strong>Recommendation systems</strong>: Suggesting products, movies, or music that align with a user’s preferences by identifying similar users or items.</li>\n      <li><strong>Facial recognition and biometric systems</strong>: Matching a given face or fingerprint to a stored identity profile.</li>\n      <li><strong>Medical imaging</strong>: Retrieving similar radiology scans to assist in diagnosis by comparing new cases with historical datasets.</li>\n    </ul>\n  </li>\n  <li>\n    <p>These applications share a common requirement: the ability to measure and compare the semantic similarity between high-dimensional data objects such as images, text, audio, or even multi-modal inputs.</p>\n  </li>\n</ul>\n<p>In modern machine learning and artificial intelligence systems, similarity search serves as a core component in scenarios such as:</p>\n<ul>\n      <li><strong>Image retrieval</strong>: Given a sample image, retrieve visually or contextually similar images from a database (e.g., identifying similar fashion items or duplicate product listings).</li>\n      <li><strong>Document retrieval</strong>: Powering search engines that return semantically relevant documents in response to a textual query.</li>\n      <li><strong>Recommendation systems</strong>: Suggesting products, movies, or music that align with a user’s preferences by identifying similar users or items.</li>\n      <li><strong>Facial recognition and biometric systems</strong>: Matching a given face or fingerprint to a stored identity profile.</li>\n      <li><strong>Medical imaging</strong>: Retrieving similar radiology scans to assist in diagnosis by comparing new cases with historical datasets.</li>\n    </ul>\n<p>These applications share a common requirement: the ability to measure and compare the semantic similarity between high-dimensional data objects such as images, text, audio, or even multi-modal inputs.</p>",
      "contentMarkdown": "*   In modern machine learning and artificial intelligence systems, similarity search serves as a core component in scenarios such as:\n    \n    *   **Image retrieval**: Given a sample image, retrieve visually or contextually similar images from a database (e.g., identifying similar fashion items or duplicate product listings).\n    *   **Document retrieval**: Powering search engines that return semantically relevant documents in response to a textual query.\n    *   **Recommendation systems**: Suggesting products, movies, or music that align with a user’s preferences by identifying similar users or items.\n    *   **Facial recognition and biometric systems**: Matching a given face or fingerprint to a stored identity profile.\n    *   **Medical imaging**: Retrieving similar radiology scans to assist in diagnosis by comparing new cases with historical datasets.\n*   These applications share a common requirement: the ability to measure and compare the semantic similarity between high-dimensional data objects such as images, text, audio, or even multi-modal inputs.\n    \n\nIn modern machine learning and artificial intelligence systems, similarity search serves as a core component in scenarios such as:\n\n*   **Image retrieval**: Given a sample image, retrieve visually or contextually similar images from a database (e.g., identifying similar fashion items or duplicate product listings).\n*   **Document retrieval**: Powering search engines that return semantically relevant documents in response to a textual query.\n*   **Recommendation systems**: Suggesting products, movies, or music that align with a user’s preferences by identifying similar users or items.\n*   **Facial recognition and biometric systems**: Matching a given face or fingerprint to a stored identity profile.\n*   **Medical imaging**: Retrieving similar radiology scans to assist in diagnosis by comparing new cases with historical datasets.\n\nThese applications share a common requirement: the ability to measure and compare the semantic similarity between high-dimensional data objects such as images, text, audio, or even multi-modal inputs.",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "machine learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 290,
        "contentLength": 2431
      },
      "nextCards": [
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3"
      ],
      "relatedCards": [
        "ai-top-30-papers-a-tutorial-introduction-to-the-minimum-description-23",
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-pipeline-issues-6",
        "ai-top-30-papers-machine-super-intelligence-24",
        "ai-model-debugging-data-issues-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#real-world-applications",
      "scrapedAt": "2025-12-28T11:57:26.971Z",
      "siblings": [
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5",
        "ai-ann-similarity-search-clustering-based-methods-6"
      ]
    },
    {
      "id": "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "What is Similarity Search?",
      "title": "From Exact to Approximate Nearest Neighbor Search",
      "subtitle": "What is Similarity Search?",
      "contentHtml": "<ul>\n  <li>\n    <p>At the heart of similarity search is the concept of nearest neighbor (NN) search. This involves representing each item (e.g., a document, image, or user profile) as a high-dimensional vector using embeddings derived from machine learning models. The task is to find the nearest vectors to a query vector using a defined distance metric, commonly Euclidean distance or cosine similarity.</p>\n  </li>\n  <li>\n    <p><strong>Exact Nearest Neighbor (ENN) search</strong> involves scanning all vectors in the dataset to compute distances to the query vector, and then selecting the closest ones. While accurate, this method becomes computationally expensive and inefficient as the dataset size and dimensionality grow.</p>\n  </li>\n  <li>\n    <p><strong>Approximate Nearest Neighbor (ANN) search</strong> offers a scalable alternative by sacrificing a small degree of accuracy to achieve significant gains in speed and memory efficiency. ANN methods use specialized data structures and algorithms to rapidly locate vectors that are close to the query vector, without exhaustively comparing all options.</p>\n  </li>\n  <li>\n    <p>This trade-off makes ANN especially valuable in real-time applications where speed and scalability are paramount, such as live recommendation engines or large-scale search systems.</p>\n  </li>\n  <li>To compare and evaluate the performance of different ANN methods, practitioners often refer to standardized benchmarks such as <a href=\"https://ann-benchmarks.com/\">ANN-Benchmarks</a>, which test algorithms across datasets and metrics.</li>\n  <li>This primer will focus on the exploration of Approximate Nearest Neighbors in greater detail.</li>\n</ul>\n<p>At the heart of similarity search is the concept of nearest neighbor (NN) search. This involves representing each item (e.g., a document, image, or user profile) as a high-dimensional vector using embeddings derived from machine learning models. The task is to find the nearest vectors to a query vector using a defined distance metric, commonly Euclidean distance or cosine similarity.</p>\n<p><strong>Exact Nearest Neighbor (ENN) search</strong> involves scanning all vectors in the dataset to compute distances to the query vector, and then selecting the closest ones. While accurate, this method becomes computationally expensive and inefficient as the dataset size and dimensionality grow.</p>\n<p><strong>Approximate Nearest Neighbor (ANN) search</strong> offers a scalable alternative by sacrificing a small degree of accuracy to achieve significant gains in speed and memory efficiency. ANN methods use specialized data structures and algorithms to rapidly locate vectors that are close to the query vector, without exhaustively comparing all options.</p>\n<p>This trade-off makes ANN especially valuable in real-time applications where speed and scalability are paramount, such as live recommendation engines or large-scale search systems.</p>",
      "contentMarkdown": "*   At the heart of similarity search is the concept of nearest neighbor (NN) search. This involves representing each item (e.g., a document, image, or user profile) as a high-dimensional vector using embeddings derived from machine learning models. The task is to find the nearest vectors to a query vector using a defined distance metric, commonly Euclidean distance or cosine similarity.\n    \n*   **Exact Nearest Neighbor (ENN) search** involves scanning all vectors in the dataset to compute distances to the query vector, and then selecting the closest ones. While accurate, this method becomes computationally expensive and inefficient as the dataset size and dimensionality grow.\n    \n*   **Approximate Nearest Neighbor (ANN) search** offers a scalable alternative by sacrificing a small degree of accuracy to achieve significant gains in speed and memory efficiency. ANN methods use specialized data structures and algorithms to rapidly locate vectors that are close to the query vector, without exhaustively comparing all options.\n    \n*   This trade-off makes ANN especially valuable in real-time applications where speed and scalability are paramount, such as live recommendation engines or large-scale search systems.\n    \n*   To compare and evaluate the performance of different ANN methods, practitioners often refer to standardized benchmarks such as [ANN-Benchmarks](https://ann-benchmarks.com/), which test algorithms across datasets and metrics.\n*   This primer will focus on the exploration of Approximate Nearest Neighbors in greater detail.\n\nAt the heart of similarity search is the concept of nearest neighbor (NN) search. This involves representing each item (e.g., a document, image, or user profile) as a high-dimensional vector using embeddings derived from machine learning models. The task is to find the nearest vectors to a query vector using a defined distance metric, commonly Euclidean distance or cosine similarity.\n\n**Exact Nearest Neighbor (ENN) search** involves scanning all vectors in the dataset to compute distances to the query vector, and then selecting the closest ones. While accurate, this method becomes computationally expensive and inefficient as the dataset size and dimensionality grow.\n\n**Approximate Nearest Neighbor (ANN) search** offers a scalable alternative by sacrificing a small degree of accuracy to achieve significant gains in speed and memory efficiency. ANN methods use specialized data structures and algorithms to rapidly locate vectors that are close to the query vector, without exhaustively comparing all options.\n\nThis trade-off makes ANN especially valuable in real-time applications where speed and scalability are paramount, such as live recommendation engines or large-scale search systems.",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 2,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "machine learning",
        "embedding"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 394,
        "contentLength": 2944
      },
      "nextCards": [
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4"
      ],
      "relatedCards": [
        "ai-top-30-papers-a-tutorial-introduction-to-the-minimum-description-23",
        "ai-model-debugging-debugging-model-training-1",
        "ai-model-debugging-pipeline-issues-6",
        "ai-top-30-papers-scaling-laws-for-neural-language-models-22",
        "ai-ml-runtimes-architecture-27"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#from-exact-to-approximate-nearest-neighbor-search",
      "scrapedAt": "2025-12-28T11:57:26.971Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5",
        "ai-ann-similarity-search-clustering-based-methods-6"
      ]
    },
    {
      "id": "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "Approximate Nearest Neighbors (ANN)",
      "title": "Role of ANN in Recommendation Systems",
      "subtitle": "Approximate Nearest Neighbors (ANN)",
      "contentHtml": "<ul>\n  <li>\n    <p>ANN methods are widely integrated into modern recommender systems to complement or enhance traditional collaborative and content-based filtering. Their utility arises from addressing several limitations of classical approaches, particularly in large-scale or real-time environments.\nTo understand their impact more concretely, consider how ANN techniques address four critical challenges in recommender system design: scalability, latency, diversity, and the cold start problem:</p>\n\n    <ol>\n      <li>\n        <p><strong>Scalability</strong>: Collaborative filtering and content-based filtering often require computing pairwise similarity between large numbers of users or items. As datasets grow to millions or billions of records, this becomes computationally infeasible using exact methods. ANN techniques introduce indexing and search strategies that scale sub-linearly with dataset size, enabling efficient candidate generation for downstream ranking stages.</p>\n      </li>\n      <li>\n        <p><strong>Real-Time Recommendation</strong>: In user-facing applications, recommendations must often be served in milliseconds. ANN algorithms support low-latency query execution, making them suitable for real-time contexts such as e-commerce search, streaming media recommendations, and social media feeds. By pre-indexing the dataset and optimizing search traversal, ANN methods ensure timely responses without compromising system throughput.</p>\n      </li>\n      <li>\n        <p><strong>Diversity and Serendipity</strong>: One downside of traditional filtering techniques is their tendency to recommend highly similar items, which can lead to filter bubbles. ANN allows recommender systems to retrieve a broader and more diverse candidate pool by identifying not only the closest items but also those within a proximity threshold. This promotes user engagement through serendipitous discoveries while still respecting personalization constraints.</p>\n      </li>\n      <li>\n        <p><strong>Cold Start Handling</strong>: When new users or items are introduced to the system, classical models often lack sufficient interaction data to generate accurate recommendations. ANN methods help mitigate the cold start problem by leveraging metadata or embedding vectors derived from auxiliary data (e.g., textual descriptions, visual content). These embeddings can be used to identify similar existing users or items, facilitating meaningful initial recommendations.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p>By integrating ANN techniques into recommender architectures, systems achieve enhanced scalability, responsiveness, recommendation diversity, and robustness to data sparsity—all critical for modern, dynamic applications.</p>\n  </li>\n</ul>\n<p>ANN methods are widely integrated into modern recommender systems to complement or enhance traditional collaborative and content-based filtering. Their utility arises from addressing several limitations of classical approaches, particularly in large-scale or real-time environments.\nTo understand their impact more concretely, consider how ANN techniques address four critical challenges in recommender system design: scalability, latency, diversity, and the cold start problem:</p>\n<ol>\n      <li>\n        <p><strong>Scalability</strong>: Collaborative filtering and content-based filtering often require computing pairwise similarity between large numbers of users or items. As datasets grow to millions or billions of records, this becomes computationally infeasible using exact methods. ANN techniques introduce indexing and search strategies that scale sub-linearly with dataset size, enabling efficient candidate generation for downstream ranking stages.</p>\n      </li>\n      <li>\n        <p><strong>Real-Time Recommendation</strong>: In user-facing applications, recommendations must often be served in milliseconds. ANN algorithms support low-latency query execution, making them suitable for real-time contexts such as e-commerce search, streaming media recommendations, and social media feeds. By pre-indexing the dataset and optimizing search traversal, ANN methods ensure timely responses without compromising system throughput.</p>\n      </li>\n      <li>\n        <p><strong>Diversity and Serendipity</strong>: One downside of traditional filtering techniques is their tendency to recommend highly similar items, which can lead to filter bubbles. ANN allows recommender systems to retrieve a broader and more diverse candidate pool by identifying not only the closest items but also those within a proximity threshold. This promotes user engagement through serendipitous discoveries while still respecting personalization constraints.</p>\n      </li>\n      <li>\n        <p><strong>Cold Start Handling</strong>: When new users or items are introduced to the system, classical models often lack sufficient interaction data to generate accurate recommendations. ANN methods help mitigate the cold start problem by leveraging metadata or embedding vectors derived from auxiliary data (e.g., textual descriptions, visual content). These embeddings can be used to identify similar existing users or items, facilitating meaningful initial recommendations.</p>\n      </li>\n    </ol>\n<p><strong>Scalability</strong>: Collaborative filtering and content-based filtering often require computing pairwise similarity between large numbers of users or items. As datasets grow to millions or billions of records, this becomes computationally infeasible using exact methods. ANN techniques introduce indexing and search strategies that scale sub-linearly with dataset size, enabling efficient candidate generation for downstream ranking stages.</p>\n<p><strong>Real-Time Recommendation</strong>: In user-facing applications, recommendations must often be served in milliseconds. ANN algorithms support low-latency query execution, making them suitable for real-time contexts such as e-commerce search, streaming media recommendations, and social media feeds. By pre-indexing the dataset and optimizing search traversal, ANN methods ensure timely responses without compromising system throughput.</p>\n<p><strong>Diversity and Serendipity</strong>: One downside of traditional filtering techniques is their tendency to recommend highly similar items, which can lead to filter bubbles. ANN allows recommender systems to retrieve a broader and more diverse candidate pool by identifying not only the closest items but also those within a proximity threshold. This promotes user engagement through serendipitous discoveries while still respecting personalization constraints.</p>\n<p><strong>Cold Start Handling</strong>: When new users or items are introduced to the system, classical models often lack sufficient interaction data to generate accurate recommendations. ANN methods help mitigate the cold start problem by leveraging metadata or embedding vectors derived from auxiliary data (e.g., textual descriptions, visual content). These embeddings can be used to identify similar existing users or items, facilitating meaningful initial recommendations.</p>\n<p>By integrating ANN techniques into recommender architectures, systems achieve enhanced scalability, responsiveness, recommendation diversity, and robustness to data sparsity—all critical for modern, dynamic applications.</p>",
      "contentMarkdown": "*   ANN methods are widely integrated into modern recommender systems to complement or enhance traditional collaborative and content-based filtering. Their utility arises from addressing several limitations of classical approaches, particularly in large-scale or real-time environments. To understand their impact more concretely, consider how ANN techniques address four critical challenges in recommender system design: scalability, latency, diversity, and the cold start problem:\n    \n    1.  **Scalability**: Collaborative filtering and content-based filtering often require computing pairwise similarity between large numbers of users or items. As datasets grow to millions or billions of records, this becomes computationally infeasible using exact methods. ANN techniques introduce indexing and search strategies that scale sub-linearly with dataset size, enabling efficient candidate generation for downstream ranking stages.\n        \n    2.  **Real-Time Recommendation**: In user-facing applications, recommendations must often be served in milliseconds. ANN algorithms support low-latency query execution, making them suitable for real-time contexts such as e-commerce search, streaming media recommendations, and social media feeds. By pre-indexing the dataset and optimizing search traversal, ANN methods ensure timely responses without compromising system throughput.\n        \n    3.  **Diversity and Serendipity**: One downside of traditional filtering techniques is their tendency to recommend highly similar items, which can lead to filter bubbles. ANN allows recommender systems to retrieve a broader and more diverse candidate pool by identifying not only the closest items but also those within a proximity threshold. This promotes user engagement through serendipitous discoveries while still respecting personalization constraints.\n        \n    4.  **Cold Start Handling**: When new users or items are introduced to the system, classical models often lack sufficient interaction data to generate accurate recommendations. ANN methods help mitigate the cold start problem by leveraging metadata or embedding vectors derived from auxiliary data (e.g., textual descriptions, visual content). These embeddings can be used to identify similar existing users or items, facilitating meaningful initial recommendations.\n        \n*   By integrating ANN techniques into recommender architectures, systems achieve enhanced scalability, responsiveness, recommendation diversity, and robustness to data sparsity—all critical for modern, dynamic applications.\n    \n\nANN methods are widely integrated into modern recommender systems to complement or enhance traditional collaborative and content-based filtering. Their utility arises from addressing several limitations of classical approaches, particularly in large-scale or real-time environments. To understand their impact more concretely, consider how ANN techniques address four critical challenges in recommender system design: scalability, latency, diversity, and the cold start problem:\n\n1.  **Scalability**: Collaborative filtering and content-based filtering often require computing pairwise similarity between large numbers of users or items. As datasets grow to millions or billions of records, this becomes computationally infeasible using exact methods. ANN techniques introduce indexing and search strategies that scale sub-linearly with dataset size, enabling efficient candidate generation for downstream ranking stages.\n    \n2.  **Real-Time Recommendation**: In user-facing applications, recommendations must often be served in milliseconds. ANN algorithms support low-latency query execution, making them suitable for real-time contexts such as e-commerce search, streaming media recommendations, and social media feeds. By pre-indexing the dataset and optimizing search traversal, ANN methods ensure timely responses without compromising system throughput.\n    \n3.  **Diversity and Serendipity**: One downside of traditional filtering techniques is their tendency to recommend highly similar items, which can lead to filter bubbles. ANN allows recommender systems to retrieve a broader and more diverse candidate pool by identifying not only the closest items but also those within a proximity threshold. This promotes user engagement through serendipitous discoveries while still respecting personalization constraints.\n    \n4.  **Cold Start Handling**: When new users or items are introduced to the system, classical models often lack sufficient interaction data to generate accurate recommendations. ANN methods help mitigate the cold start problem by leveraging metadata or embedding vectors derived from auxiliary data (e.g., textual descriptions, visual content). These embeddings can be used to identify similar existing users or items, facilitating meaningful initial recommendations.\n    \n\n**Scalability**: Collaborative filtering and content-based filtering often require computing pairwise similarity between large numbers of users or items. As datasets grow to millions or billions of records, this becomes computationally infeasible using exact methods. ANN techniques introduce indexing and search strategies that scale sub-linearly with dataset size, enabling efficient candidate generation for downstream ranking stages.\n\n**Real-Time Recommendation**: In user-facing applications, recommendations must often be served in milliseconds. ANN algorithms support low-latency query execution, making them suitable for real-time contexts such as e-commerce search, streaming media recommendations, and social media feeds. By pre-indexing the dataset and optimizing search traversal, ANN methods ensure timely responses without compromising system throughput.\n\n**Diversity and Serendipity**: One downside of traditional filtering techniques is their tendency to recommend highly similar items, which can lead to filter bubbles. ANN allows recommender systems to retrieve a broader and more diverse candidate pool by identifying not only the closest items but also those within a proximity threshold. This promotes user engagement through serendipitous discoveries while still respecting personalization constraints.\n\n**Cold Start Handling**: When new users or items are introduced to the system, classical models often lack sufficient interaction data to generate accurate recommendations. ANN methods help mitigate the cold start problem by leveraging metadata or embedding vectors derived from auxiliary data (e.g., textual descriptions, visual content). These embeddings can be used to identify similar existing users or items, facilitating meaningful initial recommendations.\n\nBy integrating ANN techniques into recommender architectures, systems achieve enhanced scalability, responsiveness, recommendation diversity, and robustness to data sparsity—all critical for modern, dynamic applications.",
      "order": 3,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 5,
      "tags": [
        "miscellaneous",
        "embedding"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 874,
        "contentLength": 7357
      },
      "nextCards": [
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5"
      ],
      "relatedCards": [
        "ai-top-30-papers-scaling-laws-for-neural-language-models-22",
        "ai-ml-runtimes-architecture-27",
        "ai-cnns-for-text-classification-convolutional-neural-networks-for-sentence-classif-6",
        "ai-top-30-papers-precise-zero-shot-dense-retrieval-without-relevanc-32",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#role-of-ann-in-recommendation-systems",
      "scrapedAt": "2025-12-28T11:57:26.971Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5",
        "ai-ann-similarity-search-clustering-based-methods-6"
      ]
    },
    {
      "id": "ai-ann-similarity-search-tree-based-methods-4",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "ANN Algorithms",
      "title": "Tree-Based Methods",
      "subtitle": "ANN Algorithms",
      "contentHtml": "<ul>\n  <li>Tree-based methods form one of the earliest and most widely adopted strategies for nearest neighbor search. They operate by recursively dividing the vector space using hyperplanes (such as axis-aligned or randomly oriented splits), resulting in hierarchical structures like binary trees or forests. During querying, these structures enable the algorithm to efficiently prune large portions of the search space, reducing the number of distance computations required.</li>\n  <li>\n    <p>In real-world scenarios, tree-based methods are frequently used in settings where:</p>\n\n    <ul>\n      <li><strong>Dimensionality is moderate</strong> (typically under 100 dimensions), and datasets are either static or change infrequently.</li>\n      <li><strong>Speed and interpretability are critical</strong>, such as in robotics, computer vision, and industrial inspection systems, where tree traversal paths can be inspected or constrained.</li>\n      <li><strong>Low-latency and real-time processing is required</strong>, such as in SLAM (Simultaneous Localization and Mapping), object recognition in AR/VR applications, or drone navigation systems, where rapid approximate lookups are essential.</li>\n      <li><strong>Embedded or resource-constrained environments</strong> are involved, where lightweight in-memory or file-based indices (as supported by Annoy) are more suitable than GPU-intensive solutions.</li>\n    </ul>\n  </li>\n  <li>Tree-based methods also serve as foundational components in hybrid pipelines—often combined with graph-based or quantization-based modules to accelerate coarse filtering before refined re-ranking stages.</li>\n  <li>Despite their limitations in high-dimensional contexts, their speed, simplicity, and practical effectiveness make them a valuable tool in many production-grade systems.</li>\n</ul>\n<p>In real-world scenarios, tree-based methods are frequently used in settings where:</p>\n<ul>\n      <li><strong>Dimensionality is moderate</strong> (typically under 100 dimensions), and datasets are either static or change infrequently.</li>\n      <li><strong>Speed and interpretability are critical</strong>, such as in robotics, computer vision, and industrial inspection systems, where tree traversal paths can be inspected or constrained.</li>\n      <li><strong>Low-latency and real-time processing is required</strong>, such as in SLAM (Simultaneous Localization and Mapping), object recognition in AR/VR applications, or drone navigation systems, where rapid approximate lookups are essential.</li>\n      <li><strong>Embedded or resource-constrained environments</strong> are involved, where lightweight in-memory or file-based indices (as supported by Annoy) are more suitable than GPU-intensive solutions.</li>\n    </ul>\n<h4 id=\"kd-trees\">KD-Trees</h4>\n<ul>\n  <li>\n    <p><strong>Partition Strategy</strong>: Recursive binary splits along dimensions with maximum variance.</p>\n  </li>\n  <li>\n    <p><strong>Search</strong>:</p>\n\n    <ul>\n      <li>Traverses down the tree to a leaf.</li>\n      <li>Backtracking is used to check sibling branches when necessary.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Implementation</strong>:</p>\n\n    <ul>\n      <li>Fast for dimensions &lt; 30.</li>\n      <li>Can use priority queues to simulate best-first search.</li>\n      <li>Common in libraries like FLANN (Fast Library for Approximate Nearest Neighbors).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li>Simple and interpretable structure.</li>\n      <li>Low memory overhead.</li>\n      <li>Fast for low-dimensional, static datasets.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>Performance degrades rapidly in high-dimensional spaces.</li>\n      <li>Not suitable for dynamic datasets or frequent updates.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Use Case</strong>:</p>\n\n    <ul>\n      <li>KD-Trees are suited for structured and low-dimensional datasets such as geospatial data, robotics, and image metadata. They are widely used in scientific computing and real-time systems requiring deterministic and interpretable behavior. They’re ideal for exact or nearly exact queries in small- to medium-sized vector collections.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Partition Strategy</strong>: Recursive binary splits along dimensions with maximum variance.</p>\n<p><strong>Search</strong>:</p>\n<ul>\n      <li>Traverses down the tree to a leaf.</li>\n      <li>Backtracking is used to check sibling branches when necessary.</li>\n    </ul>\n<p><strong>Implementation</strong>:</p>\n<ul>\n      <li>Fast for dimensions &lt; 30.</li>\n      <li>Can use priority queues to simulate best-first search.</li>\n      <li>Common in libraries like FLANN (Fast Library for Approximate Nearest Neighbors).</li>\n    </ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li>Simple and interpretable structure.</li>\n      <li>Low memory overhead.</li>\n      <li>Fast for low-dimensional, static datasets.</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>Performance degrades rapidly in high-dimensional spaces.</li>\n      <li>Not suitable for dynamic datasets or frequent updates.</li>\n    </ul>\n<p><strong>Use Case</strong>:</p>\n<ul>\n      <li>KD-Trees are suited for structured and low-dimensional datasets such as geospatial data, robotics, and image metadata. They are widely used in scientific computing and real-time systems requiring deterministic and interpretable behavior. They’re ideal for exact or nearly exact queries in small- to medium-sized vector collections.</li>\n    </ul>\n<h4 id=\"randomized-kd-forests\">Randomized KD-Forests</h4>\n<ul>\n  <li>\n    <p><strong>Multiple KD-Trees</strong>: Built with randomized split choices or axis shuffling.</p>\n  </li>\n  <li>\n    <p><strong>Search</strong>:</p>\n\n    <ul>\n      <li>Queries traverse multiple trees in parallel.</li>\n      <li>Results from all trees are merged.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Parameters</strong>:</p>\n\n    <ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">n_trees</code>: Number of trees to build (impacts accuracy and index size).</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">leaf_size</code>: Minimum number of items in leaf nodes.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">search_checks</code>: Max nodes visited during querying.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li>Improves robustness over standard KD-trees.</li>\n      <li>Offers tunable accuracy-speed trade-offs.</li>\n      <li>Better suited to moderate dimensionality (up to ~100 dimensions).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>Higher memory footprint due to multiple trees.</li>\n      <li>Requires careful tuning to avoid overfitting or redundancy.</li>\n      <li>Performance still degrades in high-dimensional regimes.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Use Case</strong>:</p>\n\n    <ul>\n      <li>Randomized KD-Forests are effective in handling medium-scale datasets where standard KD-trees fail due to dimensionality. They’re often used in real-time search applications like 3D point cloud matching and visual SLAM in robotics, offering a balance between accuracy and responsiveness.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Multiple KD-Trees</strong>: Built with randomized split choices or axis shuffling.</p>\n<p><strong>Search</strong>:</p>\n<ul>\n      <li>Queries traverse multiple trees in parallel.</li>\n      <li>Results from all trees are merged.</li>\n    </ul>\n<p><strong>Parameters</strong>:</p>\n<ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">n_trees</code>: Number of trees to build (impacts accuracy and index size).</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">leaf_size</code>: Minimum number of items in leaf nodes.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">search_checks</code>: Max nodes visited during querying.</li>\n    </ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li>Improves robustness over standard KD-trees.</li>\n      <li>Offers tunable accuracy-speed trade-offs.</li>\n      <li>Better suited to moderate dimensionality (up to ~100 dimensions).</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>Higher memory footprint due to multiple trees.</li>\n      <li>Requires careful tuning to avoid overfitting or redundancy.</li>\n      <li>Performance still degrades in high-dimensional regimes.</li>\n    </ul>\n<p><strong>Use Case</strong>:</p>\n<ul>\n      <li>Randomized KD-Forests are effective in handling medium-scale datasets where standard KD-trees fail due to dimensionality. They’re often used in real-time search applications like 3D point cloud matching and visual SLAM in robotics, offering a balance between accuracy and responsiveness.</li>\n    </ul>\n<h4 id=\"annoys-random-projection-forest\">Annoy’s Random Projection Forest</h4>\n<ul>\n  <li>\n    <p><strong>Partition Strategy</strong>:</p>\n\n    <ul>\n      <li>Randomly select two points, compute their midpoint hyperplane, and recursively partition the dataset.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Index</strong>:</p>\n\n    <ul>\n      <li>A forest of such trees is constructed.</li>\n      <li>Each tree introduces different hyperplane splits, improving diversity.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Search</strong>:</p>\n\n    <ul>\n      <li>Each tree yields a list of candidate points.</li>\n      <li>Combined and sorted using brute-force on small candidate sets.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>File-based Indexing</strong>:</p>\n\n    <ul>\n      <li>Indexes saved to disk as static files and memory-mapped at query time.</li>\n      <li>Efficient for multi-process access and low RAM environments.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li>Memory-efficient via on-disk index loading.</li>\n      <li>Supports shared access across processes.</li>\n      <li>Simple to implement and tune (<code class=\"language-plaintext highlighter-rouge\">n_trees</code>, <code class=\"language-plaintext highlighter-rouge\">search_k</code>).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>Index is static—no support for incremental updates.</li>\n      <li>Lacks GPU and batch processing support.</li>\n      <li>Recall suffers at high dimensionality or when optimal tree coverage is hard to achieve.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Use Case</strong>:</p>\n\n    <ul>\n      <li>Annoy is used extensively in static recommendation systems, such as music and content suggestion engines (e.g., Spotify). Its design favors read-heavy workloads, and it’s especially effective when indexes must be shared across processes or run in constrained environments like embedded systems.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Partition Strategy</strong>:</p>\n<ul>\n      <li>Randomly select two points, compute their midpoint hyperplane, and recursively partition the dataset.</li>\n    </ul>\n<p><strong>Index</strong>:</p>\n<ul>\n      <li>A forest of such trees is constructed.</li>\n      <li>Each tree introduces different hyperplane splits, improving diversity.</li>\n    </ul>\n<p><strong>Search</strong>:</p>\n<ul>\n      <li>Each tree yields a list of candidate points.</li>\n      <li>Combined and sorted using brute-force on small candidate sets.</li>\n    </ul>\n<p><strong>File-based Indexing</strong>:</p>\n<ul>\n      <li>Indexes saved to disk as static files and memory-mapped at query time.</li>\n      <li>Efficient for multi-process access and low RAM environments.</li>\n    </ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li>Memory-efficient via on-disk index loading.</li>\n      <li>Supports shared access across processes.</li>\n      <li>Simple to implement and tune (<code class=\"language-plaintext highlighter-rouge\">n_trees</code>, <code class=\"language-plaintext highlighter-rouge\">search_k</code>).</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>Index is static—no support for incremental updates.</li>\n      <li>Lacks GPU and batch processing support.</li>\n      <li>Recall suffers at high dimensionality or when optimal tree coverage is hard to achieve.</li>\n    </ul>\n<p><strong>Use Case</strong>:</p>\n<ul>\n      <li>Annoy is used extensively in static recommendation systems, such as music and content suggestion engines (e.g., Spotify). Its design favors read-heavy workloads, and it’s especially effective when indexes must be shared across processes or run in constrained environments like embedded systems.</li>\n    </ul>",
      "contentMarkdown": "*   Tree-based methods form one of the earliest and most widely adopted strategies for nearest neighbor search. They operate by recursively dividing the vector space using hyperplanes (such as axis-aligned or randomly oriented splits), resulting in hierarchical structures like binary trees or forests. During querying, these structures enable the algorithm to efficiently prune large portions of the search space, reducing the number of distance computations required.\n*   In real-world scenarios, tree-based methods are frequently used in settings where:\n    \n    *   **Dimensionality is moderate** (typically under 100 dimensions), and datasets are either static or change infrequently.\n    *   **Speed and interpretability are critical**, such as in robotics, computer vision, and industrial inspection systems, where tree traversal paths can be inspected or constrained.\n    *   **Low-latency and real-time processing is required**, such as in SLAM (Simultaneous Localization and Mapping), object recognition in AR/VR applications, or drone navigation systems, where rapid approximate lookups are essential.\n    *   **Embedded or resource-constrained environments** are involved, where lightweight in-memory or file-based indices (as supported by Annoy) are more suitable than GPU-intensive solutions.\n*   Tree-based methods also serve as foundational components in hybrid pipelines—often combined with graph-based or quantization-based modules to accelerate coarse filtering before refined re-ranking stages.\n*   Despite their limitations in high-dimensional contexts, their speed, simplicity, and practical effectiveness make them a valuable tool in many production-grade systems.\n\nIn real-world scenarios, tree-based methods are frequently used in settings where:\n\n*   **Dimensionality is moderate** (typically under 100 dimensions), and datasets are either static or change infrequently.\n*   **Speed and interpretability are critical**, such as in robotics, computer vision, and industrial inspection systems, where tree traversal paths can be inspected or constrained.\n*   **Low-latency and real-time processing is required**, such as in SLAM (Simultaneous Localization and Mapping), object recognition in AR/VR applications, or drone navigation systems, where rapid approximate lookups are essential.\n*   **Embedded or resource-constrained environments** are involved, where lightweight in-memory or file-based indices (as supported by Annoy) are more suitable than GPU-intensive solutions.\n\n#### KD-Trees\n\n*   **Partition Strategy**: Recursive binary splits along dimensions with maximum variance.\n    \n*   **Search**:\n    \n    *   Traverses down the tree to a leaf.\n    *   Backtracking is used to check sibling branches when necessary.\n*   **Implementation**:\n    \n    *   Fast for dimensions < 30.\n    *   Can use priority queues to simulate best-first search.\n    *   Common in libraries like FLANN (Fast Library for Approximate Nearest Neighbors).\n*   **Pros**:\n    \n    *   Simple and interpretable structure.\n    *   Low memory overhead.\n    *   Fast for low-dimensional, static datasets.\n*   **Cons**:\n    \n    *   Performance degrades rapidly in high-dimensional spaces.\n    *   Not suitable for dynamic datasets or frequent updates.\n*   **Use Case**:\n    \n    *   KD-Trees are suited for structured and low-dimensional datasets such as geospatial data, robotics, and image metadata. They are widely used in scientific computing and real-time systems requiring deterministic and interpretable behavior. They’re ideal for exact or nearly exact queries in small- to medium-sized vector collections.\n\n**Partition Strategy**: Recursive binary splits along dimensions with maximum variance.\n\n**Search**:\n\n*   Traverses down the tree to a leaf.\n*   Backtracking is used to check sibling branches when necessary.\n\n**Implementation**:\n\n*   Fast for dimensions < 30.\n*   Can use priority queues to simulate best-first search.\n*   Common in libraries like FLANN (Fast Library for Approximate Nearest Neighbors).\n\n**Pros**:\n\n*   Simple and interpretable structure.\n*   Low memory overhead.\n*   Fast for low-dimensional, static datasets.\n\n**Cons**:\n\n*   Performance degrades rapidly in high-dimensional spaces.\n*   Not suitable for dynamic datasets or frequent updates.\n\n**Use Case**:\n\n*   KD-Trees are suited for structured and low-dimensional datasets such as geospatial data, robotics, and image metadata. They are widely used in scientific computing and real-time systems requiring deterministic and interpretable behavior. They’re ideal for exact or nearly exact queries in small- to medium-sized vector collections.\n\n#### Randomized KD-Forests\n\n*   **Multiple KD-Trees**: Built with randomized split choices or axis shuffling.\n    \n*   **Search**:\n    \n    *   Queries traverse multiple trees in parallel.\n    *   Results from all trees are merged.\n*   **Parameters**:\n    \n    *   `n_trees`: Number of trees to build (impacts accuracy and index size).\n    *   `leaf_size`: Minimum number of items in leaf nodes.\n    *   `search_checks`: Max nodes visited during querying.\n*   **Pros**:\n    \n    *   Improves robustness over standard KD-trees.\n    *   Offers tunable accuracy-speed trade-offs.\n    *   Better suited to moderate dimensionality (up to ~100 dimensions).\n*   **Cons**:\n    \n    *   Higher memory footprint due to multiple trees.\n    *   Requires careful tuning to avoid overfitting or redundancy.\n    *   Performance still degrades in high-dimensional regimes.\n*   **Use Case**:\n    \n    *   Randomized KD-Forests are effective in handling medium-scale datasets where standard KD-trees fail due to dimensionality. They’re often used in real-time search applications like 3D point cloud matching and visual SLAM in robotics, offering a balance between accuracy and responsiveness.\n\n**Multiple KD-Trees**: Built with randomized split choices or axis shuffling.\n\n**Search**:\n\n*   Queries traverse multiple trees in parallel.\n*   Results from all trees are merged.\n\n**Parameters**:\n\n*   `n_trees`: Number of trees to build (impacts accuracy and index size).\n*   `leaf_size`: Minimum number of items in leaf nodes.\n*   `search_checks`: Max nodes visited during querying.\n\n**Pros**:\n\n*   Improves robustness over standard KD-trees.\n*   Offers tunable accuracy-speed trade-offs.\n*   Better suited to moderate dimensionality (up to ~100 dimensions).\n\n**Cons**:\n\n*   Higher memory footprint due to multiple trees.\n*   Requires careful tuning to avoid overfitting or redundancy.\n*   Performance still degrades in high-dimensional regimes.\n\n**Use Case**:\n\n*   Randomized KD-Forests are effective in handling medium-scale datasets where standard KD-trees fail due to dimensionality. They’re often used in real-time search applications like 3D point cloud matching and visual SLAM in robotics, offering a balance between accuracy and responsiveness.\n\n#### Annoy’s Random Projection Forest\n\n*   **Partition Strategy**:\n    \n    *   Randomly select two points, compute their midpoint hyperplane, and recursively partition the dataset.\n*   **Index**:\n    \n    *   A forest of such trees is constructed.\n    *   Each tree introduces different hyperplane splits, improving diversity.\n*   **Search**:\n    \n    *   Each tree yields a list of candidate points.\n    *   Combined and sorted using brute-force on small candidate sets.\n*   **File-based Indexing**:\n    \n    *   Indexes saved to disk as static files and memory-mapped at query time.\n    *   Efficient for multi-process access and low RAM environments.\n*   **Pros**:\n    \n    *   Memory-efficient via on-disk index loading.\n    *   Supports shared access across processes.\n    *   Simple to implement and tune (`n_trees`, `search_k`).\n*   **Cons**:\n    \n    *   Index is static—no support for incremental updates.\n    *   Lacks GPU and batch processing support.\n    *   Recall suffers at high dimensionality or when optimal tree coverage is hard to achieve.\n*   **Use Case**:\n    \n    *   Annoy is used extensively in static recommendation systems, such as music and content suggestion engines (e.g., Spotify). Its design favors read-heavy workloads, and it’s especially effective when indexes must be shared across processes or run in constrained environments like embedded systems.\n\n**Partition Strategy**:\n\n*   Randomly select two points, compute their midpoint hyperplane, and recursively partition the dataset.\n\n**Index**:\n\n*   A forest of such trees is constructed.\n*   Each tree introduces different hyperplane splits, improving diversity.\n\n**Search**:\n\n*   Each tree yields a list of candidate points.\n*   Combined and sorted using brute-force on small candidate sets.\n\n**File-based Indexing**:\n\n*   Indexes saved to disk as static files and memory-mapped at query time.\n*   Efficient for multi-process access and low RAM environments.\n\n**Pros**:\n\n*   Memory-efficient via on-disk index loading.\n*   Supports shared access across processes.\n*   Simple to implement and tune (`n_trees`, `search_k`).\n\n**Cons**:\n\n*   Index is static—no support for incremental updates.\n*   Lacks GPU and batch processing support.\n*   Recall suffers at high dimensionality or when optimal tree coverage is hard to achieve.\n\n**Use Case**:\n\n*   Annoy is used extensively in static recommendation systems, such as music and content suggestion engines (e.g., Spotify). Its design favors read-heavy workloads, and it’s especially effective when indexes must be shared across processes or run in constrained environments like embedded systems.",
      "order": 4,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 7,
      "tags": [
        "miscellaneous",
        "computer vision"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 1263,
        "contentLength": 12616
      },
      "nextCards": [
        "ai-ann-similarity-search-quantization-based-methods-5",
        "ai-ann-similarity-search-clustering-based-methods-6"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-identity-mappings-in-deep-residual-networks-15",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#tree-based-methods",
      "scrapedAt": "2025-12-28T11:57:26.972Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-quantization-based-methods-5",
        "ai-ann-similarity-search-clustering-based-methods-6"
      ]
    },
    {
      "id": "ai-ann-similarity-search-quantization-based-methods-5",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "ANN Algorithms",
      "title": "Quantization-Based Methods",
      "subtitle": "ANN Algorithms",
      "contentHtml": "<ul>\n  <li>\n    <p>Quantization-based methods are built on the idea of compressing high-dimensional vectors into compact, lossy representations, typically by mapping them to a discrete set of centroids or codes. These compact codes allow extremely fast approximate distance computation using precomputed lookup tables, making them ideal for billion-scale vector search tasks.</p>\n  </li>\n  <li>\n    <p>These methods are widely used in industry for:</p>\n\n    <ul>\n      <li>Large-scale image/video/text retrieval where space and speed are at a premium.</li>\n      <li>Real-time inference pipelines that need rapid ranking of semantically similar results.</li>\n      <li>Hybrid ANN pipelines, often paired with clustering (IVF) or re-ranking modules.</li>\n      <li>Scenarios that require deployment on limited-memory devices or optimized GPU environments.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Despite their reliance on lossy compression, modern quantization approaches (e.g., OPQ, AVQ) offer a high degree of accuracy, particularly when tuned carefully.</p>\n  </li>\n</ul>\n<p>Quantization-based methods are built on the idea of compressing high-dimensional vectors into compact, lossy representations, typically by mapping them to a discrete set of centroids or codes. These compact codes allow extremely fast approximate distance computation using precomputed lookup tables, making them ideal for billion-scale vector search tasks.</p>\n<p>These methods are widely used in industry for:</p>\n<ul>\n      <li>Large-scale image/video/text retrieval where space and speed are at a premium.</li>\n      <li>Real-time inference pipelines that need rapid ranking of semantically similar results.</li>\n      <li>Hybrid ANN pipelines, often paired with clustering (IVF) or re-ranking modules.</li>\n      <li>Scenarios that require deployment on limited-memory devices or optimized GPU environments.</li>\n    </ul>\n<p>Despite their reliance on lossy compression, modern quantization approaches (e.g., OPQ, AVQ) offer a high degree of accuracy, particularly when tuned carefully.</p>\n<h4 id=\"product-quantization-pq\">Product Quantization (PQ)</h4>\n<ul>\n  <li>\n    <p><strong>Overview</strong>:</p>\n\n    <ul>\n      <li>Vector <code class=\"language-plaintext highlighter-rouge\">x</code> is split into <code class=\"language-plaintext highlighter-rouge\">m</code> sub-vectors.</li>\n      <li>Each sub-vector is quantized into one of <code class=\"language-plaintext highlighter-rouge\">k</code> centroids (learned using k-means).</li>\n      <li>Final code is a tuple of centroid indices.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Search</strong>:</p>\n\n    <ul>\n      <li>Precompute a distance table for each query sub-vector and all subspace centroids.</li>\n      <li>Final distance is the sum of subspace distances from lookup tables.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Implementation</strong>:</p>\n\n    <ul>\n      <li>In FAISS, PQ is implemented with SIMD intrinsics and fused operations.</li>\n      <li>Typically used with IVF (Inverted File Index) for additional speed-up.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li>Excellent compression with minimal storage cost.</li>\n      <li>Efficient for both CPU and GPU architectures.</li>\n      <li>Fast distance computation using lookups, no full vector arithmetic needed.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>Sensitive to data distribution; uniform quantization can lose detail.</li>\n      <li>May degrade recall for fine-grained queries or tightly clustered data.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Use Case</strong>:</p>\n\n    <ul>\n      <li>PQ is commonly used in large-scale image and video retrieval systems. It’s a standard method in FAISS-based deployments at billion-scale for deduplication, facial recognition, and fast similarity ranking in content libraries, where compact representation is essential.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Overview</strong>:</p>\n<ul>\n      <li>Vector <code class=\"language-plaintext highlighter-rouge\">x</code> is split into <code class=\"language-plaintext highlighter-rouge\">m</code> sub-vectors.</li>\n      <li>Each sub-vector is quantized into one of <code class=\"language-plaintext highlighter-rouge\">k</code> centroids (learned using k-means).</li>\n      <li>Final code is a tuple of centroid indices.</li>\n    </ul>\n<p><strong>Search</strong>:</p>\n<ul>\n      <li>Precompute a distance table for each query sub-vector and all subspace centroids.</li>\n      <li>Final distance is the sum of subspace distances from lookup tables.</li>\n    </ul>\n<p><strong>Implementation</strong>:</p>\n<ul>\n      <li>In FAISS, PQ is implemented with SIMD intrinsics and fused operations.</li>\n      <li>Typically used with IVF (Inverted File Index) for additional speed-up.</li>\n    </ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li>Excellent compression with minimal storage cost.</li>\n      <li>Efficient for both CPU and GPU architectures.</li>\n      <li>Fast distance computation using lookups, no full vector arithmetic needed.</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>Sensitive to data distribution; uniform quantization can lose detail.</li>\n      <li>May degrade recall for fine-grained queries or tightly clustered data.</li>\n    </ul>\n<p><strong>Use Case</strong>:</p>\n<ul>\n      <li>PQ is commonly used in large-scale image and video retrieval systems. It’s a standard method in FAISS-based deployments at billion-scale for deduplication, facial recognition, and fast similarity ranking in content libraries, where compact representation is essential.</li>\n    </ul>\n<h4 id=\"optimized-product-quantization-opq\">Optimized Product Quantization (OPQ)</h4>\n<ul>\n  <li>\n    <p><strong>Extension to PQ</strong>:</p>\n\n    <ul>\n      <li>Learn a rotation matrix to decorrelate input vectors before PQ encoding.</li>\n      <li>Reduces quantization loss, improving accuracy.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Training</strong>:</p>\n\n    <ul>\n      <li>Alternates between PCA-like optimization and quantization.</li>\n      <li>Computationally heavier but yields significant accuracy improvements.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li>Lower distortion than basic PQ, especially for high-dimensional, correlated data.</li>\n      <li>Flexible, modular integration with other indexing structures (e.g., IVF).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>Slower training and more complex parameter tuning.</li>\n      <li>Requires retraining if the data distribution shifts significantly.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Use Case</strong>:</p>\n\n    <ul>\n      <li>OPQ is deployed in precision-sensitive search scenarios such as e-commerce or financial recommendation systems, where even small gains in accuracy translate to meaningful impact. It’s ideal for embedding pipelines where dimension correlations hurt performance.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Extension to PQ</strong>:</p>\n<ul>\n      <li>Learn a rotation matrix to decorrelate input vectors before PQ encoding.</li>\n      <li>Reduces quantization loss, improving accuracy.</li>\n    </ul>\n<p><strong>Training</strong>:</p>\n<ul>\n      <li>Alternates between PCA-like optimization and quantization.</li>\n      <li>Computationally heavier but yields significant accuracy improvements.</li>\n    </ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li>Lower distortion than basic PQ, especially for high-dimensional, correlated data.</li>\n      <li>Flexible, modular integration with other indexing structures (e.g., IVF).</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>Slower training and more complex parameter tuning.</li>\n      <li>Requires retraining if the data distribution shifts significantly.</li>\n    </ul>\n<p><strong>Use Case</strong>:</p>\n<ul>\n      <li>OPQ is deployed in precision-sensitive search scenarios such as e-commerce or financial recommendation systems, where even small gains in accuracy translate to meaningful impact. It’s ideal for embedding pipelines where dimension correlations hurt performance.</li>\n    </ul>\n<h4 id=\"locality-sensitive-hashing-lsh\">Locality Sensitive Hashing (LSH)</h4>\n<ul>\n  <li>\n    <p><strong>Hashing Scheme</strong>:</p>\n\n    <ul>\n      <li>Family of hash functions ensures that similar vectors hash to the same bucket with high probability.</li>\n      <li>For cosine similarity: random hyperplane hash functions.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Multi-Probe LSH</strong>:</p>\n\n    <ul>\n      <li>During query, multiple nearby buckets are probed to improve recall.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Challenges</strong>:</p>\n\n    <ul>\n      <li>Works best for low to moderate dimensions.</li>\n      <li>Often memory-intensive and suffers in high-density regions of vector space.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li>Extremely fast sublinear search; constant-time hash lookups.</li>\n      <li>Simple to implement with theoretical performance guarantees.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>Poor recall for complex, high-dimensional datasets.</li>\n      <li>High memory usage for large numbers of hash tables.</li>\n      <li>Tuning hash functions and probing depth is non-trivial.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Use Case</strong>:</p>\n\n    <ul>\n      <li>LSH is suitable for real-time systems with strict latency requirements, such as fraud detection, duplicate detection, or real-time alerting, where a fast but approximate match is preferable to delayed precision.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Hashing Scheme</strong>:</p>\n<ul>\n      <li>Family of hash functions ensures that similar vectors hash to the same bucket with high probability.</li>\n      <li>For cosine similarity: random hyperplane hash functions.</li>\n    </ul>\n<p><strong>Multi-Probe LSH</strong>:</p>\n<ul>\n      <li>During query, multiple nearby buckets are probed to improve recall.</li>\n    </ul>\n<p><strong>Challenges</strong>:</p>\n<ul>\n      <li>Works best for low to moderate dimensions.</li>\n      <li>Often memory-intensive and suffers in high-density regions of vector space.</li>\n    </ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li>Extremely fast sublinear search; constant-time hash lookups.</li>\n      <li>Simple to implement with theoretical performance guarantees.</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>Poor recall for complex, high-dimensional datasets.</li>\n      <li>High memory usage for large numbers of hash tables.</li>\n      <li>Tuning hash functions and probing depth is non-trivial.</li>\n    </ul>\n<p><strong>Use Case</strong>:</p>\n<ul>\n      <li>LSH is suitable for real-time systems with strict latency requirements, such as fraud detection, duplicate detection, or real-time alerting, where a fast but approximate match is preferable to delayed precision.</li>\n    </ul>\n<h4 id=\"anisotropic-vector-quantization-avq\">Anisotropic Vector Quantization (AVQ)</h4>\n<ul>\n  <li>\n    <p><strong>Used in ScaNN</strong>:</p>\n\n    <ul>\n      <li>Unlike PQ, AVQ allows the shape of quantization cells to adapt to the data distribution.</li>\n      <li>Each centroid has an anisotropic (elliptical) region rather than spherical.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Index Construction</strong>:</p>\n\n    <ul>\n      <li>Uses SVD-like analysis to deform Voronoi cells.</li>\n      <li>Quantization boundaries are optimized to better reflect real data spread.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Search</strong>:</p>\n\n    <ul>\n      <li>Improves Maximum Inner Product Search (MIPS) accuracy by focusing on high-value candidates.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li>Adapts well to non-uniform data densities.</li>\n      <li>Superior recall in semantic search applications compared to isotropic quantizers.</li>\n      <li>Well-suited to modern ML-generated embeddings.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>More complex to construct and train than standard PQ.</li>\n      <li>Not yet widely supported outside of ScaNN.</li>\n      <li>Longer index build time due to centroid deformation.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Use Case</strong>:</p>\n\n    <ul>\n      <li>AVQ is tailored for semantic search tasks in NLP and vision systems, where embedding density varies. It powers systems like ScaNN that handle document retrieval, passage re-ranking, or intent classification using deep-learned embeddings.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Used in ScaNN</strong>:</p>\n<ul>\n      <li>Unlike PQ, AVQ allows the shape of quantization cells to adapt to the data distribution.</li>\n      <li>Each centroid has an anisotropic (elliptical) region rather than spherical.</li>\n    </ul>\n<p><strong>Index Construction</strong>:</p>\n<ul>\n      <li>Uses SVD-like analysis to deform Voronoi cells.</li>\n      <li>Quantization boundaries are optimized to better reflect real data spread.</li>\n    </ul>\n<p><strong>Search</strong>:</p>\n<ul>\n      <li>Improves Maximum Inner Product Search (MIPS) accuracy by focusing on high-value candidates.</li>\n    </ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li>Adapts well to non-uniform data densities.</li>\n      <li>Superior recall in semantic search applications compared to isotropic quantizers.</li>\n      <li>Well-suited to modern ML-generated embeddings.</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>More complex to construct and train than standard PQ.</li>\n      <li>Not yet widely supported outside of ScaNN.</li>\n      <li>Longer index build time due to centroid deformation.</li>\n    </ul>\n<p><strong>Use Case</strong>:</p>\n<ul>\n      <li>AVQ is tailored for semantic search tasks in NLP and vision systems, where embedding density varies. It powers systems like ScaNN that handle document retrieval, passage re-ranking, or intent classification using deep-learned embeddings.</li>\n    </ul>",
      "contentMarkdown": "*   Quantization-based methods are built on the idea of compressing high-dimensional vectors into compact, lossy representations, typically by mapping them to a discrete set of centroids or codes. These compact codes allow extremely fast approximate distance computation using precomputed lookup tables, making them ideal for billion-scale vector search tasks.\n    \n*   These methods are widely used in industry for:\n    \n    *   Large-scale image/video/text retrieval where space and speed are at a premium.\n    *   Real-time inference pipelines that need rapid ranking of semantically similar results.\n    *   Hybrid ANN pipelines, often paired with clustering (IVF) or re-ranking modules.\n    *   Scenarios that require deployment on limited-memory devices or optimized GPU environments.\n*   Despite their reliance on lossy compression, modern quantization approaches (e.g., OPQ, AVQ) offer a high degree of accuracy, particularly when tuned carefully.\n    \n\nQuantization-based methods are built on the idea of compressing high-dimensional vectors into compact, lossy representations, typically by mapping them to a discrete set of centroids or codes. These compact codes allow extremely fast approximate distance computation using precomputed lookup tables, making them ideal for billion-scale vector search tasks.\n\nThese methods are widely used in industry for:\n\n*   Large-scale image/video/text retrieval where space and speed are at a premium.\n*   Real-time inference pipelines that need rapid ranking of semantically similar results.\n*   Hybrid ANN pipelines, often paired with clustering (IVF) or re-ranking modules.\n*   Scenarios that require deployment on limited-memory devices or optimized GPU environments.\n\nDespite their reliance on lossy compression, modern quantization approaches (e.g., OPQ, AVQ) offer a high degree of accuracy, particularly when tuned carefully.\n\n#### Product Quantization (PQ)\n\n*   **Overview**:\n    \n    *   Vector `x` is split into `m` sub-vectors.\n    *   Each sub-vector is quantized into one of `k` centroids (learned using k-means).\n    *   Final code is a tuple of centroid indices.\n*   **Search**:\n    \n    *   Precompute a distance table for each query sub-vector and all subspace centroids.\n    *   Final distance is the sum of subspace distances from lookup tables.\n*   **Implementation**:\n    \n    *   In FAISS, PQ is implemented with SIMD intrinsics and fused operations.\n    *   Typically used with IVF (Inverted File Index) for additional speed-up.\n*   **Pros**:\n    \n    *   Excellent compression with minimal storage cost.\n    *   Efficient for both CPU and GPU architectures.\n    *   Fast distance computation using lookups, no full vector arithmetic needed.\n*   **Cons**:\n    \n    *   Sensitive to data distribution; uniform quantization can lose detail.\n    *   May degrade recall for fine-grained queries or tightly clustered data.\n*   **Use Case**:\n    \n    *   PQ is commonly used in large-scale image and video retrieval systems. It’s a standard method in FAISS-based deployments at billion-scale for deduplication, facial recognition, and fast similarity ranking in content libraries, where compact representation is essential.\n\n**Overview**:\n\n*   Vector `x` is split into `m` sub-vectors.\n*   Each sub-vector is quantized into one of `k` centroids (learned using k-means).\n*   Final code is a tuple of centroid indices.\n\n**Search**:\n\n*   Precompute a distance table for each query sub-vector and all subspace centroids.\n*   Final distance is the sum of subspace distances from lookup tables.\n\n**Implementation**:\n\n*   In FAISS, PQ is implemented with SIMD intrinsics and fused operations.\n*   Typically used with IVF (Inverted File Index) for additional speed-up.\n\n**Pros**:\n\n*   Excellent compression with minimal storage cost.\n*   Efficient for both CPU and GPU architectures.\n*   Fast distance computation using lookups, no full vector arithmetic needed.\n\n**Cons**:\n\n*   Sensitive to data distribution; uniform quantization can lose detail.\n*   May degrade recall for fine-grained queries or tightly clustered data.\n\n**Use Case**:\n\n*   PQ is commonly used in large-scale image and video retrieval systems. It’s a standard method in FAISS-based deployments at billion-scale for deduplication, facial recognition, and fast similarity ranking in content libraries, where compact representation is essential.\n\n#### Optimized Product Quantization (OPQ)\n\n*   **Extension to PQ**:\n    \n    *   Learn a rotation matrix to decorrelate input vectors before PQ encoding.\n    *   Reduces quantization loss, improving accuracy.\n*   **Training**:\n    \n    *   Alternates between PCA-like optimization and quantization.\n    *   Computationally heavier but yields significant accuracy improvements.\n*   **Pros**:\n    \n    *   Lower distortion than basic PQ, especially for high-dimensional, correlated data.\n    *   Flexible, modular integration with other indexing structures (e.g., IVF).\n*   **Cons**:\n    \n    *   Slower training and more complex parameter tuning.\n    *   Requires retraining if the data distribution shifts significantly.\n*   **Use Case**:\n    \n    *   OPQ is deployed in precision-sensitive search scenarios such as e-commerce or financial recommendation systems, where even small gains in accuracy translate to meaningful impact. It’s ideal for embedding pipelines where dimension correlations hurt performance.\n\n**Extension to PQ**:\n\n*   Learn a rotation matrix to decorrelate input vectors before PQ encoding.\n*   Reduces quantization loss, improving accuracy.\n\n**Training**:\n\n*   Alternates between PCA-like optimization and quantization.\n*   Computationally heavier but yields significant accuracy improvements.\n\n**Pros**:\n\n*   Lower distortion than basic PQ, especially for high-dimensional, correlated data.\n*   Flexible, modular integration with other indexing structures (e.g., IVF).\n\n**Cons**:\n\n*   Slower training and more complex parameter tuning.\n*   Requires retraining if the data distribution shifts significantly.\n\n**Use Case**:\n\n*   OPQ is deployed in precision-sensitive search scenarios such as e-commerce or financial recommendation systems, where even small gains in accuracy translate to meaningful impact. It’s ideal for embedding pipelines where dimension correlations hurt performance.\n\n#### Locality Sensitive Hashing (LSH)\n\n*   **Hashing Scheme**:\n    \n    *   Family of hash functions ensures that similar vectors hash to the same bucket with high probability.\n    *   For cosine similarity: random hyperplane hash functions.\n*   **Multi-Probe LSH**:\n    \n    *   During query, multiple nearby buckets are probed to improve recall.\n*   **Challenges**:\n    \n    *   Works best for low to moderate dimensions.\n    *   Often memory-intensive and suffers in high-density regions of vector space.\n*   **Pros**:\n    \n    *   Extremely fast sublinear search; constant-time hash lookups.\n    *   Simple to implement with theoretical performance guarantees.\n*   **Cons**:\n    \n    *   Poor recall for complex, high-dimensional datasets.\n    *   High memory usage for large numbers of hash tables.\n    *   Tuning hash functions and probing depth is non-trivial.\n*   **Use Case**:\n    \n    *   LSH is suitable for real-time systems with strict latency requirements, such as fraud detection, duplicate detection, or real-time alerting, where a fast but approximate match is preferable to delayed precision.\n\n**Hashing Scheme**:\n\n*   Family of hash functions ensures that similar vectors hash to the same bucket with high probability.\n*   For cosine similarity: random hyperplane hash functions.\n\n**Multi-Probe LSH**:\n\n*   During query, multiple nearby buckets are probed to improve recall.\n\n**Challenges**:\n\n*   Works best for low to moderate dimensions.\n*   Often memory-intensive and suffers in high-density regions of vector space.\n\n**Pros**:\n\n*   Extremely fast sublinear search; constant-time hash lookups.\n*   Simple to implement with theoretical performance guarantees.\n\n**Cons**:\n\n*   Poor recall for complex, high-dimensional datasets.\n*   High memory usage for large numbers of hash tables.\n*   Tuning hash functions and probing depth is non-trivial.\n\n**Use Case**:\n\n*   LSH is suitable for real-time systems with strict latency requirements, such as fraud detection, duplicate detection, or real-time alerting, where a fast but approximate match is preferable to delayed precision.\n\n#### Anisotropic Vector Quantization (AVQ)\n\n*   **Used in ScaNN**:\n    \n    *   Unlike PQ, AVQ allows the shape of quantization cells to adapt to the data distribution.\n    *   Each centroid has an anisotropic (elliptical) region rather than spherical.\n*   **Index Construction**:\n    \n    *   Uses SVD-like analysis to deform Voronoi cells.\n    *   Quantization boundaries are optimized to better reflect real data spread.\n*   **Search**:\n    \n    *   Improves Maximum Inner Product Search (MIPS) accuracy by focusing on high-value candidates.\n*   **Pros**:\n    \n    *   Adapts well to non-uniform data densities.\n    *   Superior recall in semantic search applications compared to isotropic quantizers.\n    *   Well-suited to modern ML-generated embeddings.\n*   **Cons**:\n    \n    *   More complex to construct and train than standard PQ.\n    *   Not yet widely supported outside of ScaNN.\n    *   Longer index build time due to centroid deformation.\n*   **Use Case**:\n    \n    *   AVQ is tailored for semantic search tasks in NLP and vision systems, where embedding density varies. It powers systems like ScaNN that handle document retrieval, passage re-ranking, or intent classification using deep-learned embeddings.\n\n**Used in ScaNN**:\n\n*   Unlike PQ, AVQ allows the shape of quantization cells to adapt to the data distribution.\n*   Each centroid has an anisotropic (elliptical) region rather than spherical.\n\n**Index Construction**:\n\n*   Uses SVD-like analysis to deform Voronoi cells.\n*   Quantization boundaries are optimized to better reflect real data spread.\n\n**Search**:\n\n*   Improves Maximum Inner Product Search (MIPS) accuracy by focusing on high-value candidates.\n\n**Pros**:\n\n*   Adapts well to non-uniform data densities.\n*   Superior recall in semantic search applications compared to isotropic quantizers.\n*   Well-suited to modern ML-generated embeddings.\n\n**Cons**:\n\n*   More complex to construct and train than standard PQ.\n*   Not yet widely supported outside of ScaNN.\n*   Longer index build time due to centroid deformation.\n\n**Use Case**:\n\n*   AVQ is tailored for semantic search tasks in NLP and vision systems, where embedding density varies. It powers systems like ScaNN that handle document retrieval, passage re-ranking, or intent classification using deep-learned embeddings.",
      "order": 5,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 8,
      "tags": [
        "miscellaneous",
        "embedding",
        "nlp",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 1445,
        "contentLength": 14132
      },
      "nextCards": [
        "ai-ann-similarity-search-clustering-based-methods-6",
        "ai-ann-similarity-search-graph-based-methods-7"
      ],
      "relatedCards": [
        "ai-top-30-papers-dense-passage-retrieval-for-open-domain-question-a-28",
        "ai-top-30-papers-scaling-laws-for-neural-language-models-22",
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#quantization-based-methods",
      "scrapedAt": "2025-12-28T11:57:26.972Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-clustering-based-methods-6"
      ]
    },
    {
      "id": "ai-ann-similarity-search-clustering-based-methods-6",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "ANN Algorithms",
      "title": "Clustering-Based Methods",
      "subtitle": "ANN Algorithms",
      "contentHtml": "<ul>\n  <li>Clustering-based approaches are a foundational class of ANN algorithms that partition the dataset into discrete groups or clusters using unsupervised learning techniques, most commonly k-means. These clusters serve as coarse partitions of the search space, enabling fast and scalable nearest neighbor search by reducing the number of direct comparisons required at query time.</li>\n  <li>\n    <p>Clustering-based methods are particularly effective when used as the first stage in multi-step retrieval architectures. They are often combined with quantization and re-ranking techniques to build high-performance ANN systems for web-scale search and recommendation.  Each of these clustering-based approaches is most effective when integrated into layered ANN architectures. For example:</p>\n\n    <ul>\n      <li><strong>FAISS IVF-PQ</strong>: Combines IVF for coarse partitioning with PQ for fine-grained, compressed distance approximation.</li>\n      <li><strong>ScaNN</strong>: Employs k-means-style partitioning trees with AVQ and late-stage re-ranking.</li>\n      <li><strong>Two-Stage Retrieval Systems</strong>: Use clustering to shortlist candidates and exact scoring for ranking.</li>\n    </ul>\n  </li>\n</ul>\n<p>Clustering-based methods are particularly effective when used as the first stage in multi-step retrieval architectures. They are often combined with quantization and re-ranking techniques to build high-performance ANN systems for web-scale search and recommendation.  Each of these clustering-based approaches is most effective when integrated into layered ANN architectures. For example:</p>\n<ul>\n      <li><strong>FAISS IVF-PQ</strong>: Combines IVF for coarse partitioning with PQ for fine-grained, compressed distance approximation.</li>\n      <li><strong>ScaNN</strong>: Employs k-means-style partitioning trees with AVQ and late-stage re-ranking.</li>\n      <li><strong>Two-Stage Retrieval Systems</strong>: Use clustering to shortlist candidates and exact scoring for ranking.</li>\n    </ul>\n<h4 id=\"inverted-file-index-ivf\">Inverted File Index (IVF)</h4>\n<ul>\n  <li>\n    <p><strong>Partition Strategy</strong>:</p>\n\n    <ul>\n      <li>The dataset is partitioned using <em>k-means clustering</em> into <code class=\"language-plaintext highlighter-rouge\">nlist</code> coarse centroids. Each data point is assigned to its nearest centroid, forming an inverted list for that cluster.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Search</strong>:</p>\n\n    <ul>\n      <li>At query time, the query vector is compared to all centroids, and the top <code class=\"language-plaintext highlighter-rouge\">nprobe</code> closest centroids are selected.</li>\n      <li>Only the vectors in the selected clusters are searched, significantly narrowing down the search space.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Index Construction</strong>:</p>\n\n    <ul>\n      <li>A training phase precedes indexing, where representative centroids are learned from a sample of the dataset.</li>\n      <li>Vectors are then indexed into inverted lists corresponding to their nearest centroid.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Implementation</strong>:</p>\n\n    <ul>\n      <li>IVF is commonly used in combination with Product Quantization (e.g., IVF-PQ in FAISS) to compress vectors and reduce memory usage while maintaining retrieval performance.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li>Scalable to billions of vectors.</li>\n      <li>Fast query execution due to aggressive pruning.</li>\n      <li>Integrates well with quantization and re-ranking modules.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>Requires careful tuning of <code class=\"language-plaintext highlighter-rouge\">nlist</code> (cluster count) and <code class=\"language-plaintext highlighter-rouge\">nprobe</code> (clusters to scan at query time).</li>\n      <li>Clustering quality impacts recall and precision.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Use Case</strong>:</p>\n\n    <ul>\n      <li>Powers large-scale similarity search in systems such as Facebook’s visual search infrastructure and semantic embedding-based recommendation engines. Widely deployed in <strong>FAISS IVF-PQ</strong> and hybrid indexing pipelines.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Partition Strategy</strong>:</p>\n<ul>\n      <li>The dataset is partitioned using <em>k-means clustering</em> into <code class=\"language-plaintext highlighter-rouge\">nlist</code> coarse centroids. Each data point is assigned to its nearest centroid, forming an inverted list for that cluster.</li>\n    </ul>\n<p><strong>Search</strong>:</p>\n<ul>\n      <li>At query time, the query vector is compared to all centroids, and the top <code class=\"language-plaintext highlighter-rouge\">nprobe</code> closest centroids are selected.</li>\n      <li>Only the vectors in the selected clusters are searched, significantly narrowing down the search space.</li>\n    </ul>\n<p><strong>Index Construction</strong>:</p>\n<ul>\n      <li>A training phase precedes indexing, where representative centroids are learned from a sample of the dataset.</li>\n      <li>Vectors are then indexed into inverted lists corresponding to their nearest centroid.</li>\n    </ul>\n<p><strong>Implementation</strong>:</p>\n<ul>\n      <li>IVF is commonly used in combination with Product Quantization (e.g., IVF-PQ in FAISS) to compress vectors and reduce memory usage while maintaining retrieval performance.</li>\n    </ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li>Scalable to billions of vectors.</li>\n      <li>Fast query execution due to aggressive pruning.</li>\n      <li>Integrates well with quantization and re-ranking modules.</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>Requires careful tuning of <code class=\"language-plaintext highlighter-rouge\">nlist</code> (cluster count) and <code class=\"language-plaintext highlighter-rouge\">nprobe</code> (clusters to scan at query time).</li>\n      <li>Clustering quality impacts recall and precision.</li>\n    </ul>\n<p><strong>Use Case</strong>:</p>\n<ul>\n      <li>Powers large-scale similarity search in systems such as Facebook’s visual search infrastructure and semantic embedding-based recommendation engines. Widely deployed in <strong>FAISS IVF-PQ</strong> and hybrid indexing pipelines.</li>\n    </ul>\n<h4 id=\"residual-vector-quantization-rvq\">Residual Vector Quantization (RVQ)</h4>\n<ul>\n  <li>\n    <p><strong>Overview</strong>:</p>\n\n    <ul>\n      <li>RVQ recursively encodes a vector by applying multiple layers of quantization, each capturing the residual error left by the previous layer.</li>\n      <li>This hierarchical structure allows for progressively finer approximations of the original vector.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Search</strong>:</p>\n\n    <ul>\n      <li>Queries are encoded using the same residual structure and matched against compound codes generated during indexing.</li>\n      <li>Suitable for scenarios where single-code quantization (as in PQ) fails to preserve sufficient detail.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Index Construction</strong>:</p>\n\n    <ul>\n      <li>Involves training multiple codebooks sequentially, each learning to quantize the residual of the prior stage.</li>\n      <li>Typically implemented in vector search systems that require higher recall than basic PQ can offer.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li>Delivers better accuracy than single-pass quantization.</li>\n      <li>Supports fast computation with table lookups and SIMD-accelerated operations.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>More complex to train and tune.</li>\n      <li>Increased query-time latency due to deeper decoding stages.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Use Case</strong>:</p>\n\n    <ul>\n      <li>Used in high-recall scenarios such as document retrieval, legal and biomedical search engines, and advanced FAISS configurations where accuracy is prioritized over absolute speed.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Overview</strong>:</p>\n<ul>\n      <li>RVQ recursively encodes a vector by applying multiple layers of quantization, each capturing the residual error left by the previous layer.</li>\n      <li>This hierarchical structure allows for progressively finer approximations of the original vector.</li>\n    </ul>\n<p><strong>Search</strong>:</p>\n<ul>\n      <li>Queries are encoded using the same residual structure and matched against compound codes generated during indexing.</li>\n      <li>Suitable for scenarios where single-code quantization (as in PQ) fails to preserve sufficient detail.</li>\n    </ul>\n<p><strong>Index Construction</strong>:</p>\n<ul>\n      <li>Involves training multiple codebooks sequentially, each learning to quantize the residual of the prior stage.</li>\n      <li>Typically implemented in vector search systems that require higher recall than basic PQ can offer.</li>\n    </ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li>Delivers better accuracy than single-pass quantization.</li>\n      <li>Supports fast computation with table lookups and SIMD-accelerated operations.</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>More complex to train and tune.</li>\n      <li>Increased query-time latency due to deeper decoding stages.</li>\n    </ul>\n<p><strong>Use Case</strong>:</p>\n<ul>\n      <li>Used in high-recall scenarios such as document retrieval, legal and biomedical search engines, and advanced FAISS configurations where accuracy is prioritized over absolute speed.</li>\n    </ul>\n<h4 id=\"scalable-k-means-clustering-mini-batch-k-means\">Scalable K-Means Clustering (Mini-Batch K-Means)</h4>\n<ul>\n  <li>\n    <p><strong>Purpose</strong>:</p>\n\n    <ul>\n      <li>Designed to handle clustering on very large datasets by using small, randomly selected subsets (mini-batches) for iterative updates.</li>\n      <li>Enables efficient, online construction of cluster centroids without processing the entire dataset at once.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Search Integration</strong>:</p>\n\n    <ul>\n      <li>Frequently used to generate cluster centroids in IVF-like indexing schemes or to enable dynamic re-clustering of streaming data.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Training Efficiency</strong>:</p>\n\n    <ul>\n      <li>Orders of magnitude faster than traditional k-means for large datasets.</li>\n      <li>Supports continual training or incremental centroid updates.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li>Suitable for massive datasets that exceed memory constraints.</li>\n      <li>Faster training with minimal accuracy loss compared to full-batch methods.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>Quality of clusters depends on batch size and sampling strategy.</li>\n      <li>Can converge to suboptimal centroids in non-uniform or complex vector distributions.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Use Case</strong>:</p>\n\n    <ul>\n      <li>Ideal for dynamic or streaming data environments, such as online recommendation platforms, personalization systems, or large-scale ad retrieval where vector distributions evolve over time.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Purpose</strong>:</p>\n<ul>\n      <li>Designed to handle clustering on very large datasets by using small, randomly selected subsets (mini-batches) for iterative updates.</li>\n      <li>Enables efficient, online construction of cluster centroids without processing the entire dataset at once.</li>\n    </ul>\n<p><strong>Search Integration</strong>:</p>\n<ul>\n      <li>Frequently used to generate cluster centroids in IVF-like indexing schemes or to enable dynamic re-clustering of streaming data.</li>\n    </ul>\n<p><strong>Training Efficiency</strong>:</p>\n<ul>\n      <li>Orders of magnitude faster than traditional k-means for large datasets.</li>\n      <li>Supports continual training or incremental centroid updates.</li>\n    </ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li>Suitable for massive datasets that exceed memory constraints.</li>\n      <li>Faster training with minimal accuracy loss compared to full-batch methods.</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>Quality of clusters depends on batch size and sampling strategy.</li>\n      <li>Can converge to suboptimal centroids in non-uniform or complex vector distributions.</li>\n    </ul>\n<p><strong>Use Case</strong>:</p>\n<ul>\n      <li>Ideal for dynamic or streaming data environments, such as online recommendation platforms, personalization systems, or large-scale ad retrieval where vector distributions evolve over time.</li>\n    </ul>",
      "contentMarkdown": "*   Clustering-based approaches are a foundational class of ANN algorithms that partition the dataset into discrete groups or clusters using unsupervised learning techniques, most commonly k-means. These clusters serve as coarse partitions of the search space, enabling fast and scalable nearest neighbor search by reducing the number of direct comparisons required at query time.\n*   Clustering-based methods are particularly effective when used as the first stage in multi-step retrieval architectures. They are often combined with quantization and re-ranking techniques to build high-performance ANN systems for web-scale search and recommendation. Each of these clustering-based approaches is most effective when integrated into layered ANN architectures. For example:\n    \n    *   **FAISS IVF-PQ**: Combines IVF for coarse partitioning with PQ for fine-grained, compressed distance approximation.\n    *   **ScaNN**: Employs k-means-style partitioning trees with AVQ and late-stage re-ranking.\n    *   **Two-Stage Retrieval Systems**: Use clustering to shortlist candidates and exact scoring for ranking.\n\nClustering-based methods are particularly effective when used as the first stage in multi-step retrieval architectures. They are often combined with quantization and re-ranking techniques to build high-performance ANN systems for web-scale search and recommendation. Each of these clustering-based approaches is most effective when integrated into layered ANN architectures. For example:\n\n*   **FAISS IVF-PQ**: Combines IVF for coarse partitioning with PQ for fine-grained, compressed distance approximation.\n*   **ScaNN**: Employs k-means-style partitioning trees with AVQ and late-stage re-ranking.\n*   **Two-Stage Retrieval Systems**: Use clustering to shortlist candidates and exact scoring for ranking.\n\n#### Inverted File Index (IVF)\n\n*   **Partition Strategy**:\n    \n    *   The dataset is partitioned using _k-means clustering_ into `nlist` coarse centroids. Each data point is assigned to its nearest centroid, forming an inverted list for that cluster.\n*   **Search**:\n    \n    *   At query time, the query vector is compared to all centroids, and the top `nprobe` closest centroids are selected.\n    *   Only the vectors in the selected clusters are searched, significantly narrowing down the search space.\n*   **Index Construction**:\n    \n    *   A training phase precedes indexing, where representative centroids are learned from a sample of the dataset.\n    *   Vectors are then indexed into inverted lists corresponding to their nearest centroid.\n*   **Implementation**:\n    \n    *   IVF is commonly used in combination with Product Quantization (e.g., IVF-PQ in FAISS) to compress vectors and reduce memory usage while maintaining retrieval performance.\n*   **Pros**:\n    \n    *   Scalable to billions of vectors.\n    *   Fast query execution due to aggressive pruning.\n    *   Integrates well with quantization and re-ranking modules.\n*   **Cons**:\n    \n    *   Requires careful tuning of `nlist` (cluster count) and `nprobe` (clusters to scan at query time).\n    *   Clustering quality impacts recall and precision.\n*   **Use Case**:\n    \n    *   Powers large-scale similarity search in systems such as Facebook’s visual search infrastructure and semantic embedding-based recommendation engines. Widely deployed in **FAISS IVF-PQ** and hybrid indexing pipelines.\n\n**Partition Strategy**:\n\n*   The dataset is partitioned using _k-means clustering_ into `nlist` coarse centroids. Each data point is assigned to its nearest centroid, forming an inverted list for that cluster.\n\n**Search**:\n\n*   At query time, the query vector is compared to all centroids, and the top `nprobe` closest centroids are selected.\n*   Only the vectors in the selected clusters are searched, significantly narrowing down the search space.\n\n**Index Construction**:\n\n*   A training phase precedes indexing, where representative centroids are learned from a sample of the dataset.\n*   Vectors are then indexed into inverted lists corresponding to their nearest centroid.\n\n**Implementation**:\n\n*   IVF is commonly used in combination with Product Quantization (e.g., IVF-PQ in FAISS) to compress vectors and reduce memory usage while maintaining retrieval performance.\n\n**Pros**:\n\n*   Scalable to billions of vectors.\n*   Fast query execution due to aggressive pruning.\n*   Integrates well with quantization and re-ranking modules.\n\n**Cons**:\n\n*   Requires careful tuning of `nlist` (cluster count) and `nprobe` (clusters to scan at query time).\n*   Clustering quality impacts recall and precision.\n\n**Use Case**:\n\n*   Powers large-scale similarity search in systems such as Facebook’s visual search infrastructure and semantic embedding-based recommendation engines. Widely deployed in **FAISS IVF-PQ** and hybrid indexing pipelines.\n\n#### Residual Vector Quantization (RVQ)\n\n*   **Overview**:\n    \n    *   RVQ recursively encodes a vector by applying multiple layers of quantization, each capturing the residual error left by the previous layer.\n    *   This hierarchical structure allows for progressively finer approximations of the original vector.\n*   **Search**:\n    \n    *   Queries are encoded using the same residual structure and matched against compound codes generated during indexing.\n    *   Suitable for scenarios where single-code quantization (as in PQ) fails to preserve sufficient detail.\n*   **Index Construction**:\n    \n    *   Involves training multiple codebooks sequentially, each learning to quantize the residual of the prior stage.\n    *   Typically implemented in vector search systems that require higher recall than basic PQ can offer.\n*   **Pros**:\n    \n    *   Delivers better accuracy than single-pass quantization.\n    *   Supports fast computation with table lookups and SIMD-accelerated operations.\n*   **Cons**:\n    \n    *   More complex to train and tune.\n    *   Increased query-time latency due to deeper decoding stages.\n*   **Use Case**:\n    \n    *   Used in high-recall scenarios such as document retrieval, legal and biomedical search engines, and advanced FAISS configurations where accuracy is prioritized over absolute speed.\n\n**Overview**:\n\n*   RVQ recursively encodes a vector by applying multiple layers of quantization, each capturing the residual error left by the previous layer.\n*   This hierarchical structure allows for progressively finer approximations of the original vector.\n\n**Search**:\n\n*   Queries are encoded using the same residual structure and matched against compound codes generated during indexing.\n*   Suitable for scenarios where single-code quantization (as in PQ) fails to preserve sufficient detail.\n\n**Index Construction**:\n\n*   Involves training multiple codebooks sequentially, each learning to quantize the residual of the prior stage.\n*   Typically implemented in vector search systems that require higher recall than basic PQ can offer.\n\n**Pros**:\n\n*   Delivers better accuracy than single-pass quantization.\n*   Supports fast computation with table lookups and SIMD-accelerated operations.\n\n**Cons**:\n\n*   More complex to train and tune.\n*   Increased query-time latency due to deeper decoding stages.\n\n**Use Case**:\n\n*   Used in high-recall scenarios such as document retrieval, legal and biomedical search engines, and advanced FAISS configurations where accuracy is prioritized over absolute speed.\n\n#### Scalable K-Means Clustering (Mini-Batch K-Means)\n\n*   **Purpose**:\n    \n    *   Designed to handle clustering on very large datasets by using small, randomly selected subsets (mini-batches) for iterative updates.\n    *   Enables efficient, online construction of cluster centroids without processing the entire dataset at once.\n*   **Search Integration**:\n    \n    *   Frequently used to generate cluster centroids in IVF-like indexing schemes or to enable dynamic re-clustering of streaming data.\n*   **Training Efficiency**:\n    \n    *   Orders of magnitude faster than traditional k-means for large datasets.\n    *   Supports continual training or incremental centroid updates.\n*   **Pros**:\n    \n    *   Suitable for massive datasets that exceed memory constraints.\n    *   Faster training with minimal accuracy loss compared to full-batch methods.\n*   **Cons**:\n    \n    *   Quality of clusters depends on batch size and sampling strategy.\n    *   Can converge to suboptimal centroids in non-uniform or complex vector distributions.\n*   **Use Case**:\n    \n    *   Ideal for dynamic or streaming data environments, such as online recommendation platforms, personalization systems, or large-scale ad retrieval where vector distributions evolve over time.\n\n**Purpose**:\n\n*   Designed to handle clustering on very large datasets by using small, randomly selected subsets (mini-batches) for iterative updates.\n*   Enables efficient, online construction of cluster centroids without processing the entire dataset at once.\n\n**Search Integration**:\n\n*   Frequently used to generate cluster centroids in IVF-like indexing schemes or to enable dynamic re-clustering of streaming data.\n\n**Training Efficiency**:\n\n*   Orders of magnitude faster than traditional k-means for large datasets.\n*   Supports continual training or incremental centroid updates.\n\n**Pros**:\n\n*   Suitable for massive datasets that exceed memory constraints.\n*   Faster training with minimal accuracy loss compared to full-batch methods.\n\n**Cons**:\n\n*   Quality of clusters depends on batch size and sampling strategy.\n*   Can converge to suboptimal centroids in non-uniform or complex vector distributions.\n\n**Use Case**:\n\n*   Ideal for dynamic or streaming data environments, such as online recommendation platforms, personalization systems, or large-scale ad retrieval where vector distributions evolve over time.",
      "order": 6,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 7,
      "tags": [
        "miscellaneous",
        "embedding",
        "supervised learning",
        "unsupervised learning"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 1300,
        "contentLength": 12799
      },
      "nextCards": [
        "ai-ann-similarity-search-graph-based-methods-7",
        "ai-ann-similarity-search-tabular-comparison-8"
      ],
      "relatedCards": [
        "ai-top-30-papers-scaling-laws-for-neural-language-models-22",
        "ai-ml-runtimes-architecture-27",
        "ai-top-30-papers-precise-zero-shot-dense-retrieval-without-relevanc-32",
        "ai-cnns-for-text-classification-convolutional-neural-networks-for-sentence-classif-6",
        "ai-interview-dimensionality-reduction-techniques-33"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#clustering-based-methods",
      "scrapedAt": "2025-12-28T11:57:26.972Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5"
      ]
    },
    {
      "id": "ai-ann-similarity-search-graph-based-methods-7",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "ANN Algorithms",
      "title": "Graph-Based Methods",
      "subtitle": "ANN Algorithms",
      "contentHtml": "<ul>\n  <li>\n    <p>Graph-based methods construct navigable proximity graphs where each node represents a data point and edges connect it to a selection of its approximate nearest neighbors. These methods enable efficient similarity search through <strong>greedy traversal</strong>, allowing the system to reach relevant regions of the data space without exhaustively scanning all vectors.</p>\n  </li>\n  <li>\n    <p>These algorithms are especially effective in high-dimensional spaces where traditional methods struggle. They are widely used in real-world systems because of:</p>\n\n    <ul>\n      <li><strong>Excellent accuracy-speed trade-offs</strong>, particularly at high recall thresholds.</li>\n      <li><strong>Support for dynamic updates</strong>, which is crucial for evolving datasets.</li>\n      <li><strong>High empirical performance</strong> across embedding types (text, image, video, audio).</li>\n      <li><strong>Adoption in commercial-grade vector databases and search systems</strong>, including Vespa, Milvus, Weaviate, Pinecone, and OpenSearch extensions.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Applications include semantic search, recommendations, online personalization, ad retrieval, fraud detection, and large-scale analytics.</p>\n  </li>\n</ul>\n<p>Graph-based methods construct navigable proximity graphs where each node represents a data point and edges connect it to a selection of its approximate nearest neighbors. These methods enable efficient similarity search through <strong>greedy traversal</strong>, allowing the system to reach relevant regions of the data space without exhaustively scanning all vectors.</p>\n<p>These algorithms are especially effective in high-dimensional spaces where traditional methods struggle. They are widely used in real-world systems because of:</p>\n<ul>\n      <li><strong>Excellent accuracy-speed trade-offs</strong>, particularly at high recall thresholds.</li>\n      <li><strong>Support for dynamic updates</strong>, which is crucial for evolving datasets.</li>\n      <li><strong>High empirical performance</strong> across embedding types (text, image, video, audio).</li>\n      <li><strong>Adoption in commercial-grade vector databases and search systems</strong>, including Vespa, Milvus, Weaviate, Pinecone, and OpenSearch extensions.</li>\n    </ul>\n<p>Applications include semantic search, recommendations, online personalization, ad retrieval, fraud detection, and large-scale analytics.</p>\n<h4 id=\"navigable-small-worlds-nsw\">Navigable Small Worlds (NSW)</h4>\n<ul>\n  <li>\n    <p>NSW is a foundational graph-based ANN algorithm that constructs a proximity graph without hierarchical layers, relying instead on local navigation and randomized connectivity to explore the data space.</p>\n  </li>\n  <li>\n    <p><strong>Data Structure</strong>: A single-layer navigable graph where each node is connected to a subset of its nearest neighbors based on proximity.</p>\n  </li>\n  <li>\n    <p><strong>Construction</strong>:</p>\n\n    <ul>\n      <li>Nodes are added one by one using randomized greedy strategies.</li>\n      <li>For each new node, a set of connections is formed to existing nodes that are closest in terms of distance.</li>\n      <li>Edge formation favors a small-world topology: short paths exist between any two nodes, enabling quick traversal.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Search</strong>:</p>\n\n    <ul>\n      <li>Begins at a randomly selected node.</li>\n      <li>Follows a greedy walk, moving to the neighbor closest to the query.</li>\n      <li>Terminates once no neighbor is closer than the current node—reaching a local optimum.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li>Straightforward to implement and debug.</li>\n      <li>Requires minimal configuration—fewer hyperparameters than hierarchical models.</li>\n      <li>Lower memory usage than multilayer graphs.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>No hierarchical navigation; search can be slower or less reliable.</li>\n      <li>Convergence to the global nearest neighbor is not guaranteed.</li>\n      <li>Less suited for very large datasets or high-recall applications.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Use Case</strong>:</p>\n\n    <ul>\n      <li>NSW is best suited for simpler, smaller-scale applications or environments with limited computational resources. Common in research settings, educational tools, or embedded systems where a lightweight ANN solution is needed.</li>\n    </ul>\n  </li>\n</ul>\n<p>NSW is a foundational graph-based ANN algorithm that constructs a proximity graph without hierarchical layers, relying instead on local navigation and randomized connectivity to explore the data space.</p>\n<p><strong>Data Structure</strong>: A single-layer navigable graph where each node is connected to a subset of its nearest neighbors based on proximity.</p>\n<p><strong>Construction</strong>:</p>\n<ul>\n      <li>Nodes are added one by one using randomized greedy strategies.</li>\n      <li>For each new node, a set of connections is formed to existing nodes that are closest in terms of distance.</li>\n      <li>Edge formation favors a small-world topology: short paths exist between any two nodes, enabling quick traversal.</li>\n    </ul>\n<p><strong>Search</strong>:</p>\n<ul>\n      <li>Begins at a randomly selected node.</li>\n      <li>Follows a greedy walk, moving to the neighbor closest to the query.</li>\n      <li>Terminates once no neighbor is closer than the current node—reaching a local optimum.</li>\n    </ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li>Straightforward to implement and debug.</li>\n      <li>Requires minimal configuration—fewer hyperparameters than hierarchical models.</li>\n      <li>Lower memory usage than multilayer graphs.</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>No hierarchical navigation; search can be slower or less reliable.</li>\n      <li>Convergence to the global nearest neighbor is not guaranteed.</li>\n      <li>Less suited for very large datasets or high-recall applications.</li>\n    </ul>\n<p><strong>Use Case</strong>:</p>\n<ul>\n      <li>NSW is best suited for simpler, smaller-scale applications or environments with limited computational resources. Common in research settings, educational tools, or embedded systems where a lightweight ANN solution is needed.</li>\n    </ul>\n<h4 id=\"fast-inference-for-graph-based-ann-finger\">Fast Inference for Graph-Based ANN (FINGER)</h4>\n<ul>\n  <li>\n    <p>FINGER is a graph search optimization that accelerates nearest neighbor search by reducing the number of exact distance computations during traversal.</p>\n  </li>\n  <li>\n    <p><strong>Algorithmic Enhancement</strong>:</p>\n\n    <ul>\n      <li>FINGER is not a standalone graph algorithm but a technique that enhances existing graph-based methods such as NSW or HNSW.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Technique</strong>:</p>\n\n    <ul>\n      <li>Decomposes vectors into components (projections and residuals).</li>\n      <li>Estimates distances using angular relationships and dot products.</li>\n      <li>Avoids costly full-vector comparisons for less promising candidates.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Integration</strong>:</p>\n\n    <ul>\n      <li>Applied exclusively at query time.</li>\n      <li>Does not alter the underlying graph; can be used on top of existing indexes.</li>\n      <li>Particularly beneficial for large graphs with dense connectivity.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li>Reduces search latency by 20–60% in practice.</li>\n      <li>Easy to integrate without retraining or rebuilding the index.</li>\n      <li>Compatible with both simple (NSW) and hierarchical (HNSW) graphs.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>Performance gains vary with data characteristics (e.g., vector distribution).</li>\n      <li>Adds complexity to the search logic.</li>\n      <li>May slightly reduce recall if approximation is too aggressive.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Use Case</strong>:</p>\n\n    <ul>\n      <li>FINGER is ideal for latency-critical applications such as real-time recommendations, search-as-you-type interfaces, and live personalization systems. It helps accelerate existing HNSW or NSW graphs without requiring structural changes.</li>\n    </ul>\n  </li>\n</ul>\n<p>FINGER is a graph search optimization that accelerates nearest neighbor search by reducing the number of exact distance computations during traversal.</p>\n<p><strong>Algorithmic Enhancement</strong>:</p>\n<ul>\n      <li>FINGER is not a standalone graph algorithm but a technique that enhances existing graph-based methods such as NSW or HNSW.</li>\n    </ul>\n<p><strong>Technique</strong>:</p>\n<ul>\n      <li>Decomposes vectors into components (projections and residuals).</li>\n      <li>Estimates distances using angular relationships and dot products.</li>\n      <li>Avoids costly full-vector comparisons for less promising candidates.</li>\n    </ul>\n<p><strong>Integration</strong>:</p>\n<ul>\n      <li>Applied exclusively at query time.</li>\n      <li>Does not alter the underlying graph; can be used on top of existing indexes.</li>\n      <li>Particularly beneficial for large graphs with dense connectivity.</li>\n    </ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li>Reduces search latency by 20–60% in practice.</li>\n      <li>Easy to integrate without retraining or rebuilding the index.</li>\n      <li>Compatible with both simple (NSW) and hierarchical (HNSW) graphs.</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>Performance gains vary with data characteristics (e.g., vector distribution).</li>\n      <li>Adds complexity to the search logic.</li>\n      <li>May slightly reduce recall if approximation is too aggressive.</li>\n    </ul>\n<p><strong>Use Case</strong>:</p>\n<ul>\n      <li>FINGER is ideal for latency-critical applications such as real-time recommendations, search-as-you-type interfaces, and live personalization systems. It helps accelerate existing HNSW or NSW graphs without requiring structural changes.</li>\n    </ul>\n<h4 id=\"hierarchical-navigable-small-worlds-hnsw\">Hierarchical Navigable Small Worlds (HNSW)</h4>\n<ul>\n  <li>\n    <p>HNSW is a state-of-the-art ANN graph algorithm that improves upon NSW by introducing a hierarchical structure and multiple levels of graph connectivity.</p>\n  </li>\n  <li>\n    <p><strong>Data Structure</strong>:</p>\n\n    <ul>\n      <li>A multilayer graph in which higher levels connect long-range links and lower levels represent local neighborhoods.</li>\n      <li>Nodes appear at multiple levels, with decreasing density as the level increases.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Construction</strong>:</p>\n\n    <ul>\n      <li>Each point is assigned a random maximum layer.</li>\n      <li>Insertions start at the top and descend layer by layer, connecting to the closest nodes at each level.</li>\n      <li>Parameters like <code class=\"language-plaintext highlighter-rouge\">M</code> (max edges per node) and <code class=\"language-plaintext highlighter-rouge\">efConstruction</code> control connectivity.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Search</strong>:</p>\n\n    <ul>\n      <li>Starts from the topmost layer using a greedy search.</li>\n      <li>Progressively moves down levels, narrowing the search to increasingly local areas.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">efSearch</code> determines the number of nodes visited during querying.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Implementation Notes</strong>:</p>\n\n    <ul>\n      <li>Well-optimized in libraries such as NMSLIB, FAISS, and hnswlib.</li>\n      <li>Supports dynamic insertions and deletions, making it suitable for production systems.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Pros</strong>:</p>\n\n    <ul>\n      <li>Achieves high recall at low latency.</li>\n      <li>Scales efficiently to millions of high-dimensional vectors.</li>\n      <li>Tunable for different performance requirements.</li>\n      <li>Robust in both static and dynamic settings.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Cons</strong>:</p>\n\n    <ul>\n      <li>Higher memory usage due to multilayer structure.</li>\n      <li>Longer index build times, especially with large datasets.</li>\n      <li>Requires hyperparameter tuning for optimal performance.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Use Case</strong>:</p>\n\n    <ul>\n      <li>HNSW is the industry-standard graph-based ANN algorithm used in high-performance vector search platforms. It powers semantic retrieval in document search, personalized content feeds, similarity-based product recommendation, and large-scale embedding search in databases like Vespa, Weaviate, and Pinecone.</li>\n    </ul>\n  </li>\n</ul>\n<p>HNSW is a state-of-the-art ANN graph algorithm that improves upon NSW by introducing a hierarchical structure and multiple levels of graph connectivity.</p>\n<p><strong>Data Structure</strong>:</p>\n<ul>\n      <li>A multilayer graph in which higher levels connect long-range links and lower levels represent local neighborhoods.</li>\n      <li>Nodes appear at multiple levels, with decreasing density as the level increases.</li>\n    </ul>\n<p><strong>Construction</strong>:</p>\n<ul>\n      <li>Each point is assigned a random maximum layer.</li>\n      <li>Insertions start at the top and descend layer by layer, connecting to the closest nodes at each level.</li>\n      <li>Parameters like <code class=\"language-plaintext highlighter-rouge\">M</code> (max edges per node) and <code class=\"language-plaintext highlighter-rouge\">efConstruction</code> control connectivity.</li>\n    </ul>\n<p><strong>Search</strong>:</p>\n<ul>\n      <li>Starts from the topmost layer using a greedy search.</li>\n      <li>Progressively moves down levels, narrowing the search to increasingly local areas.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">efSearch</code> determines the number of nodes visited during querying.</li>\n    </ul>\n<p><strong>Implementation Notes</strong>:</p>\n<ul>\n      <li>Well-optimized in libraries such as NMSLIB, FAISS, and hnswlib.</li>\n      <li>Supports dynamic insertions and deletions, making it suitable for production systems.</li>\n    </ul>\n<p><strong>Pros</strong>:</p>\n<ul>\n      <li>Achieves high recall at low latency.</li>\n      <li>Scales efficiently to millions of high-dimensional vectors.</li>\n      <li>Tunable for different performance requirements.</li>\n      <li>Robust in both static and dynamic settings.</li>\n    </ul>\n<p><strong>Cons</strong>:</p>\n<ul>\n      <li>Higher memory usage due to multilayer structure.</li>\n      <li>Longer index build times, especially with large datasets.</li>\n      <li>Requires hyperparameter tuning for optimal performance.</li>\n    </ul>\n<p><strong>Use Case</strong>:</p>\n<ul>\n      <li>HNSW is the industry-standard graph-based ANN algorithm used in high-performance vector search platforms. It powers semantic retrieval in document search, personalized content feeds, similarity-based product recommendation, and large-scale embedding search in databases like Vespa, Weaviate, and Pinecone.</li>\n    </ul>",
      "contentMarkdown": "*   Graph-based methods construct navigable proximity graphs where each node represents a data point and edges connect it to a selection of its approximate nearest neighbors. These methods enable efficient similarity search through **greedy traversal**, allowing the system to reach relevant regions of the data space without exhaustively scanning all vectors.\n    \n*   These algorithms are especially effective in high-dimensional spaces where traditional methods struggle. They are widely used in real-world systems because of:\n    \n    *   **Excellent accuracy-speed trade-offs**, particularly at high recall thresholds.\n    *   **Support for dynamic updates**, which is crucial for evolving datasets.\n    *   **High empirical performance** across embedding types (text, image, video, audio).\n    *   **Adoption in commercial-grade vector databases and search systems**, including Vespa, Milvus, Weaviate, Pinecone, and OpenSearch extensions.\n*   Applications include semantic search, recommendations, online personalization, ad retrieval, fraud detection, and large-scale analytics.\n    \n\nGraph-based methods construct navigable proximity graphs where each node represents a data point and edges connect it to a selection of its approximate nearest neighbors. These methods enable efficient similarity search through **greedy traversal**, allowing the system to reach relevant regions of the data space without exhaustively scanning all vectors.\n\nThese algorithms are especially effective in high-dimensional spaces where traditional methods struggle. They are widely used in real-world systems because of:\n\n*   **Excellent accuracy-speed trade-offs**, particularly at high recall thresholds.\n*   **Support for dynamic updates**, which is crucial for evolving datasets.\n*   **High empirical performance** across embedding types (text, image, video, audio).\n*   **Adoption in commercial-grade vector databases and search systems**, including Vespa, Milvus, Weaviate, Pinecone, and OpenSearch extensions.\n\nApplications include semantic search, recommendations, online personalization, ad retrieval, fraud detection, and large-scale analytics.\n\n#### Navigable Small Worlds (NSW)\n\n*   NSW is a foundational graph-based ANN algorithm that constructs a proximity graph without hierarchical layers, relying instead on local navigation and randomized connectivity to explore the data space.\n    \n*   **Data Structure**: A single-layer navigable graph where each node is connected to a subset of its nearest neighbors based on proximity.\n    \n*   **Construction**:\n    \n    *   Nodes are added one by one using randomized greedy strategies.\n    *   For each new node, a set of connections is formed to existing nodes that are closest in terms of distance.\n    *   Edge formation favors a small-world topology: short paths exist between any two nodes, enabling quick traversal.\n*   **Search**:\n    \n    *   Begins at a randomly selected node.\n    *   Follows a greedy walk, moving to the neighbor closest to the query.\n    *   Terminates once no neighbor is closer than the current node—reaching a local optimum.\n*   **Pros**:\n    \n    *   Straightforward to implement and debug.\n    *   Requires minimal configuration—fewer hyperparameters than hierarchical models.\n    *   Lower memory usage than multilayer graphs.\n*   **Cons**:\n    \n    *   No hierarchical navigation; search can be slower or less reliable.\n    *   Convergence to the global nearest neighbor is not guaranteed.\n    *   Less suited for very large datasets or high-recall applications.\n*   **Use Case**:\n    \n    *   NSW is best suited for simpler, smaller-scale applications or environments with limited computational resources. Common in research settings, educational tools, or embedded systems where a lightweight ANN solution is needed.\n\nNSW is a foundational graph-based ANN algorithm that constructs a proximity graph without hierarchical layers, relying instead on local navigation and randomized connectivity to explore the data space.\n\n**Data Structure**: A single-layer navigable graph where each node is connected to a subset of its nearest neighbors based on proximity.\n\n**Construction**:\n\n*   Nodes are added one by one using randomized greedy strategies.\n*   For each new node, a set of connections is formed to existing nodes that are closest in terms of distance.\n*   Edge formation favors a small-world topology: short paths exist between any two nodes, enabling quick traversal.\n\n**Search**:\n\n*   Begins at a randomly selected node.\n*   Follows a greedy walk, moving to the neighbor closest to the query.\n*   Terminates once no neighbor is closer than the current node—reaching a local optimum.\n\n**Pros**:\n\n*   Straightforward to implement and debug.\n*   Requires minimal configuration—fewer hyperparameters than hierarchical models.\n*   Lower memory usage than multilayer graphs.\n\n**Cons**:\n\n*   No hierarchical navigation; search can be slower or less reliable.\n*   Convergence to the global nearest neighbor is not guaranteed.\n*   Less suited for very large datasets or high-recall applications.\n\n**Use Case**:\n\n*   NSW is best suited for simpler, smaller-scale applications or environments with limited computational resources. Common in research settings, educational tools, or embedded systems where a lightweight ANN solution is needed.\n\n#### Fast Inference for Graph-Based ANN (FINGER)\n\n*   FINGER is a graph search optimization that accelerates nearest neighbor search by reducing the number of exact distance computations during traversal.\n    \n*   **Algorithmic Enhancement**:\n    \n    *   FINGER is not a standalone graph algorithm but a technique that enhances existing graph-based methods such as NSW or HNSW.\n*   **Technique**:\n    \n    *   Decomposes vectors into components (projections and residuals).\n    *   Estimates distances using angular relationships and dot products.\n    *   Avoids costly full-vector comparisons for less promising candidates.\n*   **Integration**:\n    \n    *   Applied exclusively at query time.\n    *   Does not alter the underlying graph; can be used on top of existing indexes.\n    *   Particularly beneficial for large graphs with dense connectivity.\n*   **Pros**:\n    \n    *   Reduces search latency by 20–60% in practice.\n    *   Easy to integrate without retraining or rebuilding the index.\n    *   Compatible with both simple (NSW) and hierarchical (HNSW) graphs.\n*   **Cons**:\n    \n    *   Performance gains vary with data characteristics (e.g., vector distribution).\n    *   Adds complexity to the search logic.\n    *   May slightly reduce recall if approximation is too aggressive.\n*   **Use Case**:\n    \n    *   FINGER is ideal for latency-critical applications such as real-time recommendations, search-as-you-type interfaces, and live personalization systems. It helps accelerate existing HNSW or NSW graphs without requiring structural changes.\n\nFINGER is a graph search optimization that accelerates nearest neighbor search by reducing the number of exact distance computations during traversal.\n\n**Algorithmic Enhancement**:\n\n*   FINGER is not a standalone graph algorithm but a technique that enhances existing graph-based methods such as NSW or HNSW.\n\n**Technique**:\n\n*   Decomposes vectors into components (projections and residuals).\n*   Estimates distances using angular relationships and dot products.\n*   Avoids costly full-vector comparisons for less promising candidates.\n\n**Integration**:\n\n*   Applied exclusively at query time.\n*   Does not alter the underlying graph; can be used on top of existing indexes.\n*   Particularly beneficial for large graphs with dense connectivity.\n\n**Pros**:\n\n*   Reduces search latency by 20–60% in practice.\n*   Easy to integrate without retraining or rebuilding the index.\n*   Compatible with both simple (NSW) and hierarchical (HNSW) graphs.\n\n**Cons**:\n\n*   Performance gains vary with data characteristics (e.g., vector distribution).\n*   Adds complexity to the search logic.\n*   May slightly reduce recall if approximation is too aggressive.\n\n**Use Case**:\n\n*   FINGER is ideal for latency-critical applications such as real-time recommendations, search-as-you-type interfaces, and live personalization systems. It helps accelerate existing HNSW or NSW graphs without requiring structural changes.\n\n#### Hierarchical Navigable Small Worlds (HNSW)\n\n*   HNSW is a state-of-the-art ANN graph algorithm that improves upon NSW by introducing a hierarchical structure and multiple levels of graph connectivity.\n    \n*   **Data Structure**:\n    \n    *   A multilayer graph in which higher levels connect long-range links and lower levels represent local neighborhoods.\n    *   Nodes appear at multiple levels, with decreasing density as the level increases.\n*   **Construction**:\n    \n    *   Each point is assigned a random maximum layer.\n    *   Insertions start at the top and descend layer by layer, connecting to the closest nodes at each level.\n    *   Parameters like `M` (max edges per node) and `efConstruction` control connectivity.\n*   **Search**:\n    \n    *   Starts from the topmost layer using a greedy search.\n    *   Progressively moves down levels, narrowing the search to increasingly local areas.\n    *   `efSearch` determines the number of nodes visited during querying.\n*   **Implementation Notes**:\n    \n    *   Well-optimized in libraries such as NMSLIB, FAISS, and hnswlib.\n    *   Supports dynamic insertions and deletions, making it suitable for production systems.\n*   **Pros**:\n    \n    *   Achieves high recall at low latency.\n    *   Scales efficiently to millions of high-dimensional vectors.\n    *   Tunable for different performance requirements.\n    *   Robust in both static and dynamic settings.\n*   **Cons**:\n    \n    *   Higher memory usage due to multilayer structure.\n    *   Longer index build times, especially with large datasets.\n    *   Requires hyperparameter tuning for optimal performance.\n*   **Use Case**:\n    \n    *   HNSW is the industry-standard graph-based ANN algorithm used in high-performance vector search platforms. It powers semantic retrieval in document search, personalized content feeds, similarity-based product recommendation, and large-scale embedding search in databases like Vespa, Weaviate, and Pinecone.\n\nHNSW is a state-of-the-art ANN graph algorithm that improves upon NSW by introducing a hierarchical structure and multiple levels of graph connectivity.\n\n**Data Structure**:\n\n*   A multilayer graph in which higher levels connect long-range links and lower levels represent local neighborhoods.\n*   Nodes appear at multiple levels, with decreasing density as the level increases.\n\n**Construction**:\n\n*   Each point is assigned a random maximum layer.\n*   Insertions start at the top and descend layer by layer, connecting to the closest nodes at each level.\n*   Parameters like `M` (max edges per node) and `efConstruction` control connectivity.\n\n**Search**:\n\n*   Starts from the topmost layer using a greedy search.\n*   Progressively moves down levels, narrowing the search to increasingly local areas.\n*   `efSearch` determines the number of nodes visited during querying.\n\n**Implementation Notes**:\n\n*   Well-optimized in libraries such as NMSLIB, FAISS, and hnswlib.\n*   Supports dynamic insertions and deletions, making it suitable for production systems.\n\n**Pros**:\n\n*   Achieves high recall at low latency.\n*   Scales efficiently to millions of high-dimensional vectors.\n*   Tunable for different performance requirements.\n*   Robust in both static and dynamic settings.\n\n**Cons**:\n\n*   Higher memory usage due to multilayer structure.\n*   Longer index build times, especially with large datasets.\n*   Requires hyperparameter tuning for optimal performance.\n\n**Use Case**:\n\n*   HNSW is the industry-standard graph-based ANN algorithm used in high-performance vector search platforms. It powers semantic retrieval in document search, personalized content feeds, similarity-based product recommendation, and large-scale embedding search in databases like Vespa, Weaviate, and Pinecone.",
      "order": 7,
      "orderInChapter": 4,
      "difficulty": 4,
      "estimatedMinutes": 9,
      "tags": [
        "miscellaneous",
        "embedding",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 1625,
        "contentLength": 15287
      },
      "nextCards": [
        "ai-ann-similarity-search-tabular-comparison-8",
        "ai-ann-similarity-search-choosing-the-right-ann-algorithm-family-9"
      ],
      "relatedCards": [
        "ai-top-30-papers-scaling-laws-for-neural-language-models-22",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-pros-and-cons-6",
        "ai-ml-runtimes-implementation-details-22"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#graph-based-methods",
      "scrapedAt": "2025-12-28T11:57:26.972Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5"
      ]
    },
    {
      "id": "ai-ann-similarity-search-tabular-comparison-8",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "ANN Algorithms",
      "title": "Tabular Comparison",
      "subtitle": "ANN Algorithms",
      "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Algorithm</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Data Structure</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Construction Strategy</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Search Strategy</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Key Strengths</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Limitations</strong></th>\n</tr>\n</thead>\n<tbody>\n\n<tr style=\"border-top: 2px solid var(--table-border-color); border-bottom: 2px solid var(--table-border-color);\">\n<td colspan=\"6\" align=\"center\"><strong>Tree-Based Methods</strong></td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\">KD-Tree</td>\n<td class=\"tg-tleft-valign-first\">Binary tree with axis-aligned splits</td>\n<td class=\"tg-tleft-valign-first\">Recursive partitioning along max-variance axes</td>\n<td class=\"tg-tleft-valign-first\">Tree traversal with backtracking</td>\n<td class=\"tg-tleft-valign-first\">Fast for low dimensions (&lt;30), simple structure</td>\n<td class=\"tg-tleft-valign-second\">Degrades in high-dimensional spaces</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Randomized KD-Forest</td>\n<td class=\"tg-tleft-valign-first\">Multiple KD-Trees with random splits</td>\n<td class=\"tg-tleft-valign-first\">Parallel tree building with randomized axes</td>\n<td class=\"tg-tleft-valign-first\">Aggregate candidates from all trees</td>\n<td class=\"tg-tleft-valign-first\">Improved recall over single KD-tree</td>\n<td class=\"tg-tleft-valign-second\">Higher memory and tuning complexity</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Annoy Forest</td>\n<td class=\"tg-tleft-valign-first\">Forest of binary trees via random projections</td>\n<td class=\"tg-tleft-valign-first\">Random hyperplane splits</td>\n<td class=\"tg-tleft-valign-first\">Tree-wise candidate extraction with brute-force refinement</td>\n<td class=\"tg-tleft-valign-first\">Memory-mapped files, low RAM use, fast lookup</td>\n<td class=\"tg-tleft-valign-second\">Static index, no GPU/batch support</td>\n</tr>\n\n<tr style=\"border-top: 2px solid var(--table-border-color); border-bottom: 2px solid var(--table-border-color);\">\n<td colspan=\"6\" align=\"center\"><strong>Quantization-Based Methods</strong></td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\">Product Quantization (PQ)</td>\n<td class=\"tg-tleft-valign-first\">Codebooks for each subspace of vector</td>\n<td class=\"tg-tleft-valign-first\">K-means on vector subspaces</td>\n<td class=\"tg-tleft-valign-first\">Lookup-table based distance approximation</td>\n<td class=\"tg-tleft-valign-first\">Compact codes, efficient on CPUs/GPUs</td>\n<td class=\"tg-tleft-valign-second\">Accuracy loss due to quantization</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Optimized PQ (OPQ)</td>\n<td class=\"tg-tleft-valign-first\">Rotated subspace codebooks</td>\n<td class=\"tg-tleft-valign-first\">Learned rotation + PQ training</td>\n<td class=\"tg-tleft-valign-first\">Same as PQ, with lower quantization error</td>\n<td class=\"tg-tleft-valign-first\">Better accuracy than PQ, widely supported</td>\n<td class=\"tg-tleft-valign-second\">Heavier training, complex tuning</td>\n</tr>\n\n<tr style=\"border-top: 2px solid var(--table-border-color); border-bottom: 2px solid var(--table-border-color);\">\n<td colspan=\"6\" align=\"center\"><strong>Clustering-Based Methods</strong></td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\">Inverted File Index (IVF)</td>\n<td class=\"tg-tleft-valign-first\">Cluster centroids with inverted lists</td>\n<td class=\"tg-tleft-valign-first\">k-means clustering for coarse partitioning</td>\n<td class=\"tg-tleft-valign-first\">Select nearest clusters and search within them</td>\n<td class=\"tg-tleft-valign-first\">Scalable to billions of vectors, integrates with quantization</td>\n<td class=\"tg-tleft-valign-second\">Clustering quality impacts recall and precision</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Residual Vector Quantization (RVQ)</td>\n<td class=\"tg-tleft-valign-first\">Hierarchical residual codebooks</td>\n<td class=\"tg-tleft-valign-first\">Recursive quantization of residuals</td>\n<td class=\"tg-tleft-valign-first\">Multi-stage decoding and comparison</td>\n<td class=\"tg-tleft-valign-first\">Higher recall than single-level quantizers</td>\n<td class=\"tg-tleft-valign-second\">Increased query latency, complex training</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Mini-Batch K-Means</td>\n<td class=\"tg-tleft-valign-first\">Incrementally updated cluster centroids</td>\n<td class=\"tg-tleft-valign-first\">Iterative mini-batch updates on sampled data</td>\n<td class=\"tg-tleft-valign-first\">Used to assign points or create IVF partitions</td>\n<td class=\"tg-tleft-valign-first\">Efficient training on large datasets, supports streaming data</td>\n<td class=\"tg-tleft-valign-second\">Cluster quality depends on batch configuration</td>\n</tr>\n\n<tr style=\"border-top: 2px solid var(--table-border-color); border-bottom: 2px solid var(--table-border-color);\">\n<td colspan=\"6\" align=\"center\"><strong>Graph-Based Methods</strong></td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\">NSW</td>\n<td class=\"tg-tleft-valign-first\">Single-layer navigable small-world graph</td>\n<td class=\"tg-tleft-valign-first\">Randomized edge insertion based on proximity</td>\n<td class=\"tg-tleft-valign-first\">Greedy walk guided by local nearest neighbors</td>\n<td class=\"tg-tleft-valign-first\">Lightweight, easy to implement, lower memory</td>\n<td class=\"tg-tleft-valign-second\">No hierarchy, slower convergence, reduced recall in complex data</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FINGER</td>\n<td class=\"tg-tleft-valign-first\">Augmentation over navigable graph</td>\n<td class=\"tg-tleft-valign-first\">No structural changes; integrates into search phase</td>\n<td class=\"tg-tleft-valign-first\">Approximate distance estimation via vector projections</td>\n<td class=\"tg-tleft-valign-first\">Accelerates search in existing graphs, reduces computation by 20–60%</td>\n<td class=\"tg-tleft-valign-second\">Dependent on vector distribution, adds search-time complexity</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">HNSW</td>\n<td class=\"tg-tleft-valign-first\">Multilayer navigable proximity graph</td>\n<td class=\"tg-tleft-valign-first\">Greedy layer-wise insertion with long- and short-range links</td>\n<td class=\"tg-tleft-valign-first\">Hierarchical greedy search with priority queue</td>\n<td class=\"tg-tleft-valign-first\">High recall, dynamic updates, excellent accuracy-speed trade-off</td>\n<td class=\"tg-tleft-valign-second\">High memory usage, longer build time, parameter tuning required</td>\n</tr>\n\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Algorithm</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Data Structure</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Construction Strategy</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Search Strategy</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Key Strengths</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Limitations</strong></th>\n</tr>\n</thead>\n<tbody>\n\n<tr style=\"border-top: 2px solid var(--table-border-color); border-bottom: 2px solid var(--table-border-color);\">\n<td colspan=\"6\" align=\"center\"><strong>Tree-Based Methods</strong></td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\">KD-Tree</td>\n<td class=\"tg-tleft-valign-first\">Binary tree with axis-aligned splits</td>\n<td class=\"tg-tleft-valign-first\">Recursive partitioning along max-variance axes</td>\n<td class=\"tg-tleft-valign-first\">Tree traversal with backtracking</td>\n<td class=\"tg-tleft-valign-first\">Fast for low dimensions (&lt;30), simple structure</td>\n<td class=\"tg-tleft-valign-second\">Degrades in high-dimensional spaces</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Randomized KD-Forest</td>\n<td class=\"tg-tleft-valign-first\">Multiple KD-Trees with random splits</td>\n<td class=\"tg-tleft-valign-first\">Parallel tree building with randomized axes</td>\n<td class=\"tg-tleft-valign-first\">Aggregate candidates from all trees</td>\n<td class=\"tg-tleft-valign-first\">Improved recall over single KD-tree</td>\n<td class=\"tg-tleft-valign-second\">Higher memory and tuning complexity</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Annoy Forest</td>\n<td class=\"tg-tleft-valign-first\">Forest of binary trees via random projections</td>\n<td class=\"tg-tleft-valign-first\">Random hyperplane splits</td>\n<td class=\"tg-tleft-valign-first\">Tree-wise candidate extraction with brute-force refinement</td>\n<td class=\"tg-tleft-valign-first\">Memory-mapped files, low RAM use, fast lookup</td>\n<td class=\"tg-tleft-valign-second\">Static index, no GPU/batch support</td>\n</tr>\n\n<tr style=\"border-top: 2px solid var(--table-border-color); border-bottom: 2px solid var(--table-border-color);\">\n<td colspan=\"6\" align=\"center\"><strong>Quantization-Based Methods</strong></td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\">Product Quantization (PQ)</td>\n<td class=\"tg-tleft-valign-first\">Codebooks for each subspace of vector</td>\n<td class=\"tg-tleft-valign-first\">K-means on vector subspaces</td>\n<td class=\"tg-tleft-valign-first\">Lookup-table based distance approximation</td>\n<td class=\"tg-tleft-valign-first\">Compact codes, efficient on CPUs/GPUs</td>\n<td class=\"tg-tleft-valign-second\">Accuracy loss due to quantization</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Optimized PQ (OPQ)</td>\n<td class=\"tg-tleft-valign-first\">Rotated subspace codebooks</td>\n<td class=\"tg-tleft-valign-first\">Learned rotation + PQ training</td>\n<td class=\"tg-tleft-valign-first\">Same as PQ, with lower quantization error</td>\n<td class=\"tg-tleft-valign-first\">Better accuracy than PQ, widely supported</td>\n<td class=\"tg-tleft-valign-second\">Heavier training, complex tuning</td>\n</tr>\n\n<tr style=\"border-top: 2px solid var(--table-border-color); border-bottom: 2px solid var(--table-border-color);\">\n<td colspan=\"6\" align=\"center\"><strong>Clustering-Based Methods</strong></td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\">Inverted File Index (IVF)</td>\n<td class=\"tg-tleft-valign-first\">Cluster centroids with inverted lists</td>\n<td class=\"tg-tleft-valign-first\">k-means clustering for coarse partitioning</td>\n<td class=\"tg-tleft-valign-first\">Select nearest clusters and search within them</td>\n<td class=\"tg-tleft-valign-first\">Scalable to billions of vectors, integrates with quantization</td>\n<td class=\"tg-tleft-valign-second\">Clustering quality impacts recall and precision</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Residual Vector Quantization (RVQ)</td>\n<td class=\"tg-tleft-valign-first\">Hierarchical residual codebooks</td>\n<td class=\"tg-tleft-valign-first\">Recursive quantization of residuals</td>\n<td class=\"tg-tleft-valign-first\">Multi-stage decoding and comparison</td>\n<td class=\"tg-tleft-valign-first\">Higher recall than single-level quantizers</td>\n<td class=\"tg-tleft-valign-second\">Increased query latency, complex training</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">Mini-Batch K-Means</td>\n<td class=\"tg-tleft-valign-first\">Incrementally updated cluster centroids</td>\n<td class=\"tg-tleft-valign-first\">Iterative mini-batch updates on sampled data</td>\n<td class=\"tg-tleft-valign-first\">Used to assign points or create IVF partitions</td>\n<td class=\"tg-tleft-valign-first\">Efficient training on large datasets, supports streaming data</td>\n<td class=\"tg-tleft-valign-second\">Cluster quality depends on batch configuration</td>\n</tr>\n\n<tr style=\"border-top: 2px solid var(--table-border-color); border-bottom: 2px solid var(--table-border-color);\">\n<td colspan=\"6\" align=\"center\"><strong>Graph-Based Methods</strong></td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\">NSW</td>\n<td class=\"tg-tleft-valign-first\">Single-layer navigable small-world graph</td>\n<td class=\"tg-tleft-valign-first\">Randomized edge insertion based on proximity</td>\n<td class=\"tg-tleft-valign-first\">Greedy walk guided by local nearest neighbors</td>\n<td class=\"tg-tleft-valign-first\">Lightweight, easy to implement, lower memory</td>\n<td class=\"tg-tleft-valign-second\">No hierarchy, slower convergence, reduced recall in complex data</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">FINGER</td>\n<td class=\"tg-tleft-valign-first\">Augmentation over navigable graph</td>\n<td class=\"tg-tleft-valign-first\">No structural changes; integrates into search phase</td>\n<td class=\"tg-tleft-valign-first\">Approximate distance estimation via vector projections</td>\n<td class=\"tg-tleft-valign-first\">Accelerates search in existing graphs, reduces computation by 20–60%</td>\n<td class=\"tg-tleft-valign-second\">Dependent on vector distribution, adds search-time complexity</td>\n</tr>\n<tr>\n<td class=\"tg-tleft-valign-first\">HNSW</td>\n<td class=\"tg-tleft-valign-first\">Multilayer navigable proximity graph</td>\n<td class=\"tg-tleft-valign-first\">Greedy layer-wise insertion with long- and short-range links</td>\n<td class=\"tg-tleft-valign-first\">Hierarchical greedy search with priority queue</td>\n<td class=\"tg-tleft-valign-first\">High recall, dynamic updates, excellent accuracy-speed trade-off</td>\n<td class=\"tg-tleft-valign-second\">High memory usage, longer build time, parameter tuning required</td>\n</tr>\n\n</tbody>\n</table>",
      "contentMarkdown": "**Algorithm**\n\n**Data Structure**\n\n**Construction Strategy**\n\n**Search Strategy**\n\n**Key Strengths**\n\n**Limitations**\n\n**Tree-Based Methods**\n\nKD-Tree\n\nBinary tree with axis-aligned splits\n\nRecursive partitioning along max-variance axes\n\nTree traversal with backtracking\n\nFast for low dimensions (<30), simple structure\n\nDegrades in high-dimensional spaces\n\nRandomized KD-Forest\n\nMultiple KD-Trees with random splits\n\nParallel tree building with randomized axes\n\nAggregate candidates from all trees\n\nImproved recall over single KD-tree\n\nHigher memory and tuning complexity\n\nAnnoy Forest\n\nForest of binary trees via random projections\n\nRandom hyperplane splits\n\nTree-wise candidate extraction with brute-force refinement\n\nMemory-mapped files, low RAM use, fast lookup\n\nStatic index, no GPU/batch support\n\n**Quantization-Based Methods**\n\nProduct Quantization (PQ)\n\nCodebooks for each subspace of vector\n\nK-means on vector subspaces\n\nLookup-table based distance approximation\n\nCompact codes, efficient on CPUs/GPUs\n\nAccuracy loss due to quantization\n\nOptimized PQ (OPQ)\n\nRotated subspace codebooks\n\nLearned rotation + PQ training\n\nSame as PQ, with lower quantization error\n\nBetter accuracy than PQ, widely supported\n\nHeavier training, complex tuning\n\n**Clustering-Based Methods**\n\nInverted File Index (IVF)\n\nCluster centroids with inverted lists\n\nk-means clustering for coarse partitioning\n\nSelect nearest clusters and search within them\n\nScalable to billions of vectors, integrates with quantization\n\nClustering quality impacts recall and precision\n\nResidual Vector Quantization (RVQ)\n\nHierarchical residual codebooks\n\nRecursive quantization of residuals\n\nMulti-stage decoding and comparison\n\nHigher recall than single-level quantizers\n\nIncreased query latency, complex training\n\nMini-Batch K-Means\n\nIncrementally updated cluster centroids\n\nIterative mini-batch updates on sampled data\n\nUsed to assign points or create IVF partitions\n\nEfficient training on large datasets, supports streaming data\n\nCluster quality depends on batch configuration\n\n**Graph-Based Methods**\n\nNSW\n\nSingle-layer navigable small-world graph\n\nRandomized edge insertion based on proximity\n\nGreedy walk guided by local nearest neighbors\n\nLightweight, easy to implement, lower memory\n\nNo hierarchy, slower convergence, reduced recall in complex data\n\nFINGER\n\nAugmentation over navigable graph\n\nNo structural changes; integrates into search phase\n\nApproximate distance estimation via vector projections\n\nAccelerates search in existing graphs, reduces computation by 20–60%\n\nDependent on vector distribution, adds search-time complexity\n\nHNSW\n\nMultilayer navigable proximity graph\n\nGreedy layer-wise insertion with long- and short-range links\n\nHierarchical greedy search with priority queue\n\nHigh recall, dynamic updates, excellent accuracy-speed trade-off\n\nHigh memory usage, longer build time, parameter tuning required\n\n**Algorithm**\n\n**Data Structure**\n\n**Construction Strategy**\n\n**Search Strategy**\n\n**Key Strengths**\n\n**Limitations**\n\n**Tree-Based Methods**\n\nKD-Tree\n\nBinary tree with axis-aligned splits\n\nRecursive partitioning along max-variance axes\n\nTree traversal with backtracking\n\nFast for low dimensions (<30), simple structure\n\nDegrades in high-dimensional spaces\n\nRandomized KD-Forest\n\nMultiple KD-Trees with random splits\n\nParallel tree building with randomized axes\n\nAggregate candidates from all trees\n\nImproved recall over single KD-tree\n\nHigher memory and tuning complexity\n\nAnnoy Forest\n\nForest of binary trees via random projections\n\nRandom hyperplane splits\n\nTree-wise candidate extraction with brute-force refinement\n\nMemory-mapped files, low RAM use, fast lookup\n\nStatic index, no GPU/batch support\n\n**Quantization-Based Methods**\n\nProduct Quantization (PQ)\n\nCodebooks for each subspace of vector\n\nK-means on vector subspaces\n\nLookup-table based distance approximation\n\nCompact codes, efficient on CPUs/GPUs\n\nAccuracy loss due to quantization\n\nOptimized PQ (OPQ)\n\nRotated subspace codebooks\n\nLearned rotation + PQ training\n\nSame as PQ, with lower quantization error\n\nBetter accuracy than PQ, widely supported\n\nHeavier training, complex tuning\n\n**Clustering-Based Methods**\n\nInverted File Index (IVF)\n\nCluster centroids with inverted lists\n\nk-means clustering for coarse partitioning\n\nSelect nearest clusters and search within them\n\nScalable to billions of vectors, integrates with quantization\n\nClustering quality impacts recall and precision\n\nResidual Vector Quantization (RVQ)\n\nHierarchical residual codebooks\n\nRecursive quantization of residuals\n\nMulti-stage decoding and comparison\n\nHigher recall than single-level quantizers\n\nIncreased query latency, complex training\n\nMini-Batch K-Means\n\nIncrementally updated cluster centroids\n\nIterative mini-batch updates on sampled data\n\nUsed to assign points or create IVF partitions\n\nEfficient training on large datasets, supports streaming data\n\nCluster quality depends on batch configuration\n\n**Graph-Based Methods**\n\nNSW\n\nSingle-layer navigable small-world graph\n\nRandomized edge insertion based on proximity\n\nGreedy walk guided by local nearest neighbors\n\nLightweight, easy to implement, lower memory\n\nNo hierarchy, slower convergence, reduced recall in complex data\n\nFINGER\n\nAugmentation over navigable graph\n\nNo structural changes; integrates into search phase\n\nApproximate distance estimation via vector projections\n\nAccelerates search in existing graphs, reduces computation by 20–60%\n\nDependent on vector distribution, adds search-time complexity\n\nHNSW\n\nMultilayer navigable proximity graph\n\nGreedy layer-wise insertion with long- and short-range links\n\nHierarchical greedy search with priority queue\n\nHigh recall, dynamic updates, excellent accuracy-speed trade-off\n\nHigh memory usage, longer build time, parameter tuning required",
      "order": 8,
      "orderInChapter": 5,
      "difficulty": 2,
      "estimatedMinutes": 4,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 706,
        "contentLength": 13241
      },
      "nextCards": [
        "ai-ann-similarity-search-choosing-the-right-ann-algorithm-family-9",
        "ai-ann-similarity-search-faiss-facebook-ai-similarity-search-10"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#tabular-comparison",
      "scrapedAt": "2025-12-28T11:57:26.972Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5"
      ]
    },
    {
      "id": "ai-ann-similarity-search-choosing-the-right-ann-algorithm-family-9",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "ANN Algorithms",
      "title": "Choosing the Right ANN Algorithm Family",
      "subtitle": "ANN Algorithms",
      "contentHtml": "<ul>\n  <li>\n    <p>Here’s a guide to help you choose the right ANN algorithm family based on your data characteristics and application requirements:</p>\n\n    <ul>\n      <li>\n        <p><strong>Tree-Based Methods (KD-Tree, Randomized KD-Forest, Annoy)</strong>: These methods shine when your vectors live in low to moderate dimensions (roughly up to 100), and you need interpretable, lightweight indices that rarely change. They’re ideal for static or slowly evolving datasets where you can afford occasional full-index rebuilds. Use them in robotics or AR/VR for real-time pose estimation and mapping, in embedded recommendation systems with tight memory budgets, or whenever you want deterministic search paths you can debug or constrain.</p>\n      </li>\n      <li>\n        <p><strong>Quantization-Based Methods (PQ, OPQ, LSH, AVQ)</strong>: If you’re dealing with millions or billions of high-dimensional embeddings and need extreme compression without sacrificing too much accuracy, quantization is your go-to. Product Quantization (PQ) and its optimized variant (OPQ) deliver compact codes and very fast lookups on CPU or GPU, perfect for large-scale image/video retrieval or e-commerce rankers. Locality Sensitive Hashing (LSH) works best when you need sub-millisecond lookups and can tolerate lower recall, as in fraud detection or duplicate alerting. Anisotropic Vector Quantization (AVQ) is a niche choice when embedding distributions vary wildly and you need better recall than PQ can offer—common in semantic search with neural embeddings.</p>\n      </li>\n      <li>\n        <p><strong>Clustering-Based Methods (IVF, RVQ, Mini-Batch K-Means)</strong>: Use clustering to get a coarse shortlist before applying fine-grained search. IVF (often paired with PQ) scales to web-scale collections by scanning only a few inverted lists, making it the backbone of billion-vector systems like FAISS deployments. If you need higher recall than PQ-only pipelines, Residual Vector Quantization (RVQ) provides deeper approximation at the cost of slightly higher query latency. For streaming or dynamic data where clusters must evolve over time, Mini-Batch K-Means lets you update centroids online with minimal compute.</p>\n      </li>\n      <li>\n        <p><strong>Graph-Based Methods (NSW, FINGER, HNSW)</strong>: When your vectors are very high-dimensional and you demand both high recall and low latency—especially in dynamic settings—graph methods dominate. Hierarchical Navigable Small Worlds (HNSW) offers state-of-the-art accuracy-speed trade-offs and supports incremental updates, making it the de facto choice in production vector databases and semantic search engines. If you already have an HNSW or NSW graph and need to shave off extra microseconds at query time, apply FINGER on top to reduce exact distance computations. For smaller or research-oriented setups where simplicity matters, pure NSW can be a lightweight starting point.</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>Choose based on your primary constraints—dimensionality, scale, update frequency, memory footprint, latency budget, and accuracy targets. Mixing families (for example, IVF-PQ or tree pre-filtering before a graph search) often yields the best balance in large-scale, production ANN pipelines.</p>\n  </li>\n</ul>\n<p>Here’s a guide to help you choose the right ANN algorithm family based on your data characteristics and application requirements:</p>\n<ul>\n      <li>\n        <p><strong>Tree-Based Methods (KD-Tree, Randomized KD-Forest, Annoy)</strong>: These methods shine when your vectors live in low to moderate dimensions (roughly up to 100), and you need interpretable, lightweight indices that rarely change. They’re ideal for static or slowly evolving datasets where you can afford occasional full-index rebuilds. Use them in robotics or AR/VR for real-time pose estimation and mapping, in embedded recommendation systems with tight memory budgets, or whenever you want deterministic search paths you can debug or constrain.</p>\n      </li>\n      <li>\n        <p><strong>Quantization-Based Methods (PQ, OPQ, LSH, AVQ)</strong>: If you’re dealing with millions or billions of high-dimensional embeddings and need extreme compression without sacrificing too much accuracy, quantization is your go-to. Product Quantization (PQ) and its optimized variant (OPQ) deliver compact codes and very fast lookups on CPU or GPU, perfect for large-scale image/video retrieval or e-commerce rankers. Locality Sensitive Hashing (LSH) works best when you need sub-millisecond lookups and can tolerate lower recall, as in fraud detection or duplicate alerting. Anisotropic Vector Quantization (AVQ) is a niche choice when embedding distributions vary wildly and you need better recall than PQ can offer—common in semantic search with neural embeddings.</p>\n      </li>\n      <li>\n        <p><strong>Clustering-Based Methods (IVF, RVQ, Mini-Batch K-Means)</strong>: Use clustering to get a coarse shortlist before applying fine-grained search. IVF (often paired with PQ) scales to web-scale collections by scanning only a few inverted lists, making it the backbone of billion-vector systems like FAISS deployments. If you need higher recall than PQ-only pipelines, Residual Vector Quantization (RVQ) provides deeper approximation at the cost of slightly higher query latency. For streaming or dynamic data where clusters must evolve over time, Mini-Batch K-Means lets you update centroids online with minimal compute.</p>\n      </li>\n      <li>\n        <p><strong>Graph-Based Methods (NSW, FINGER, HNSW)</strong>: When your vectors are very high-dimensional and you demand both high recall and low latency—especially in dynamic settings—graph methods dominate. Hierarchical Navigable Small Worlds (HNSW) offers state-of-the-art accuracy-speed trade-offs and supports incremental updates, making it the de facto choice in production vector databases and semantic search engines. If you already have an HNSW or NSW graph and need to shave off extra microseconds at query time, apply FINGER on top to reduce exact distance computations. For smaller or research-oriented setups where simplicity matters, pure NSW can be a lightweight starting point.</p>\n      </li>\n    </ul>\n<p><strong>Tree-Based Methods (KD-Tree, Randomized KD-Forest, Annoy)</strong>: These methods shine when your vectors live in low to moderate dimensions (roughly up to 100), and you need interpretable, lightweight indices that rarely change. They’re ideal for static or slowly evolving datasets where you can afford occasional full-index rebuilds. Use them in robotics or AR/VR for real-time pose estimation and mapping, in embedded recommendation systems with tight memory budgets, or whenever you want deterministic search paths you can debug or constrain.</p>\n<p><strong>Quantization-Based Methods (PQ, OPQ, LSH, AVQ)</strong>: If you’re dealing with millions or billions of high-dimensional embeddings and need extreme compression without sacrificing too much accuracy, quantization is your go-to. Product Quantization (PQ) and its optimized variant (OPQ) deliver compact codes and very fast lookups on CPU or GPU, perfect for large-scale image/video retrieval or e-commerce rankers. Locality Sensitive Hashing (LSH) works best when you need sub-millisecond lookups and can tolerate lower recall, as in fraud detection or duplicate alerting. Anisotropic Vector Quantization (AVQ) is a niche choice when embedding distributions vary wildly and you need better recall than PQ can offer—common in semantic search with neural embeddings.</p>\n<p><strong>Clustering-Based Methods (IVF, RVQ, Mini-Batch K-Means)</strong>: Use clustering to get a coarse shortlist before applying fine-grained search. IVF (often paired with PQ) scales to web-scale collections by scanning only a few inverted lists, making it the backbone of billion-vector systems like FAISS deployments. If you need higher recall than PQ-only pipelines, Residual Vector Quantization (RVQ) provides deeper approximation at the cost of slightly higher query latency. For streaming or dynamic data where clusters must evolve over time, Mini-Batch K-Means lets you update centroids online with minimal compute.</p>\n<p><strong>Graph-Based Methods (NSW, FINGER, HNSW)</strong>: When your vectors are very high-dimensional and you demand both high recall and low latency—especially in dynamic settings—graph methods dominate. Hierarchical Navigable Small Worlds (HNSW) offers state-of-the-art accuracy-speed trade-offs and supports incremental updates, making it the de facto choice in production vector databases and semantic search engines. If you already have an HNSW or NSW graph and need to shave off extra microseconds at query time, apply FINGER on top to reduce exact distance computations. For smaller or research-oriented setups where simplicity matters, pure NSW can be a lightweight starting point.</p>\n<p>Choose based on your primary constraints—dimensionality, scale, update frequency, memory footprint, latency budget, and accuracy targets. Mixing families (for example, IVF-PQ or tree pre-filtering before a graph search) often yields the best balance in large-scale, production ANN pipelines.</p>",
      "contentMarkdown": "*   Here’s a guide to help you choose the right ANN algorithm family based on your data characteristics and application requirements:\n    \n    *   **Tree-Based Methods (KD-Tree, Randomized KD-Forest, Annoy)**: These methods shine when your vectors live in low to moderate dimensions (roughly up to 100), and you need interpretable, lightweight indices that rarely change. They’re ideal for static or slowly evolving datasets where you can afford occasional full-index rebuilds. Use them in robotics or AR/VR for real-time pose estimation and mapping, in embedded recommendation systems with tight memory budgets, or whenever you want deterministic search paths you can debug or constrain.\n        \n    *   **Quantization-Based Methods (PQ, OPQ, LSH, AVQ)**: If you’re dealing with millions or billions of high-dimensional embeddings and need extreme compression without sacrificing too much accuracy, quantization is your go-to. Product Quantization (PQ) and its optimized variant (OPQ) deliver compact codes and very fast lookups on CPU or GPU, perfect for large-scale image/video retrieval or e-commerce rankers. Locality Sensitive Hashing (LSH) works best when you need sub-millisecond lookups and can tolerate lower recall, as in fraud detection or duplicate alerting. Anisotropic Vector Quantization (AVQ) is a niche choice when embedding distributions vary wildly and you need better recall than PQ can offer—common in semantic search with neural embeddings.\n        \n    *   **Clustering-Based Methods (IVF, RVQ, Mini-Batch K-Means)**: Use clustering to get a coarse shortlist before applying fine-grained search. IVF (often paired with PQ) scales to web-scale collections by scanning only a few inverted lists, making it the backbone of billion-vector systems like FAISS deployments. If you need higher recall than PQ-only pipelines, Residual Vector Quantization (RVQ) provides deeper approximation at the cost of slightly higher query latency. For streaming or dynamic data where clusters must evolve over time, Mini-Batch K-Means lets you update centroids online with minimal compute.\n        \n    *   **Graph-Based Methods (NSW, FINGER, HNSW)**: When your vectors are very high-dimensional and you demand both high recall and low latency—especially in dynamic settings—graph methods dominate. Hierarchical Navigable Small Worlds (HNSW) offers state-of-the-art accuracy-speed trade-offs and supports incremental updates, making it the de facto choice in production vector databases and semantic search engines. If you already have an HNSW or NSW graph and need to shave off extra microseconds at query time, apply FINGER on top to reduce exact distance computations. For smaller or research-oriented setups where simplicity matters, pure NSW can be a lightweight starting point.\n        \n*   Choose based on your primary constraints—dimensionality, scale, update frequency, memory footprint, latency budget, and accuracy targets. Mixing families (for example, IVF-PQ or tree pre-filtering before a graph search) often yields the best balance in large-scale, production ANN pipelines.\n    \n\nHere’s a guide to help you choose the right ANN algorithm family based on your data characteristics and application requirements:\n\n*   **Tree-Based Methods (KD-Tree, Randomized KD-Forest, Annoy)**: These methods shine when your vectors live in low to moderate dimensions (roughly up to 100), and you need interpretable, lightweight indices that rarely change. They’re ideal for static or slowly evolving datasets where you can afford occasional full-index rebuilds. Use them in robotics or AR/VR for real-time pose estimation and mapping, in embedded recommendation systems with tight memory budgets, or whenever you want deterministic search paths you can debug or constrain.\n    \n*   **Quantization-Based Methods (PQ, OPQ, LSH, AVQ)**: If you’re dealing with millions or billions of high-dimensional embeddings and need extreme compression without sacrificing too much accuracy, quantization is your go-to. Product Quantization (PQ) and its optimized variant (OPQ) deliver compact codes and very fast lookups on CPU or GPU, perfect for large-scale image/video retrieval or e-commerce rankers. Locality Sensitive Hashing (LSH) works best when you need sub-millisecond lookups and can tolerate lower recall, as in fraud detection or duplicate alerting. Anisotropic Vector Quantization (AVQ) is a niche choice when embedding distributions vary wildly and you need better recall than PQ can offer—common in semantic search with neural embeddings.\n    \n*   **Clustering-Based Methods (IVF, RVQ, Mini-Batch K-Means)**: Use clustering to get a coarse shortlist before applying fine-grained search. IVF (often paired with PQ) scales to web-scale collections by scanning only a few inverted lists, making it the backbone of billion-vector systems like FAISS deployments. If you need higher recall than PQ-only pipelines, Residual Vector Quantization (RVQ) provides deeper approximation at the cost of slightly higher query latency. For streaming or dynamic data where clusters must evolve over time, Mini-Batch K-Means lets you update centroids online with minimal compute.\n    \n*   **Graph-Based Methods (NSW, FINGER, HNSW)**: When your vectors are very high-dimensional and you demand both high recall and low latency—especially in dynamic settings—graph methods dominate. Hierarchical Navigable Small Worlds (HNSW) offers state-of-the-art accuracy-speed trade-offs and supports incremental updates, making it the de facto choice in production vector databases and semantic search engines. If you already have an HNSW or NSW graph and need to shave off extra microseconds at query time, apply FINGER on top to reduce exact distance computations. For smaller or research-oriented setups where simplicity matters, pure NSW can be a lightweight starting point.\n    \n\n**Tree-Based Methods (KD-Tree, Randomized KD-Forest, Annoy)**: These methods shine when your vectors live in low to moderate dimensions (roughly up to 100), and you need interpretable, lightweight indices that rarely change. They’re ideal for static or slowly evolving datasets where you can afford occasional full-index rebuilds. Use them in robotics or AR/VR for real-time pose estimation and mapping, in embedded recommendation systems with tight memory budgets, or whenever you want deterministic search paths you can debug or constrain.\n\n**Quantization-Based Methods (PQ, OPQ, LSH, AVQ)**: If you’re dealing with millions or billions of high-dimensional embeddings and need extreme compression without sacrificing too much accuracy, quantization is your go-to. Product Quantization (PQ) and its optimized variant (OPQ) deliver compact codes and very fast lookups on CPU or GPU, perfect for large-scale image/video retrieval or e-commerce rankers. Locality Sensitive Hashing (LSH) works best when you need sub-millisecond lookups and can tolerate lower recall, as in fraud detection or duplicate alerting. Anisotropic Vector Quantization (AVQ) is a niche choice when embedding distributions vary wildly and you need better recall than PQ can offer—common in semantic search with neural embeddings.\n\n**Clustering-Based Methods (IVF, RVQ, Mini-Batch K-Means)**: Use clustering to get a coarse shortlist before applying fine-grained search. IVF (often paired with PQ) scales to web-scale collections by scanning only a few inverted lists, making it the backbone of billion-vector systems like FAISS deployments. If you need higher recall than PQ-only pipelines, Residual Vector Quantization (RVQ) provides deeper approximation at the cost of slightly higher query latency. For streaming or dynamic data where clusters must evolve over time, Mini-Batch K-Means lets you update centroids online with minimal compute.\n\n**Graph-Based Methods (NSW, FINGER, HNSW)**: When your vectors are very high-dimensional and you demand both high recall and low latency—especially in dynamic settings—graph methods dominate. Hierarchical Navigable Small Worlds (HNSW) offers state-of-the-art accuracy-speed trade-offs and supports incremental updates, making it the de facto choice in production vector databases and semantic search engines. If you already have an HNSW or NSW graph and need to shave off extra microseconds at query time, apply FINGER on top to reduce exact distance computations. For smaller or research-oriented setups where simplicity matters, pure NSW can be a lightweight starting point.\n\nChoose based on your primary constraints—dimensionality, scale, update frequency, memory footprint, latency budget, and accuracy targets. Mixing families (for example, IVF-PQ or tree pre-filtering before a graph search) often yields the best balance in large-scale, production ANN pipelines.",
      "order": 9,
      "orderInChapter": 6,
      "difficulty": 2,
      "estimatedMinutes": 7,
      "tags": [
        "miscellaneous",
        "embedding"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 1230,
        "contentLength": 9216
      },
      "nextCards": [
        "ai-ann-similarity-search-faiss-facebook-ai-similarity-search-10",
        "ai-ann-similarity-search-scann-scalable-nearest-neighbors-11"
      ],
      "relatedCards": [
        "ai-top-30-papers-scaling-laws-for-neural-language-models-22",
        "ai-ml-runtimes-architecture-27",
        "ai-cnns-for-text-classification-convolutional-neural-networks-for-sentence-classif-6",
        "ai-top-30-papers-precise-zero-shot-dense-retrieval-without-relevanc-32",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#choosing-the-right-ann-algorithm-family",
      "scrapedAt": "2025-12-28T11:57:26.972Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5"
      ]
    },
    {
      "id": "ai-ann-similarity-search-faiss-facebook-ai-similarity-search-10",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "ANN Libraries",
      "title": "FAISS (Facebook AI Similarity Search)",
      "subtitle": "ANN Libraries",
      "contentHtml": "<ul>\n  <li>FAISS is a highly optimized C++ library with Python bindings, developed by Facebook AI Research to support efficient similarity search over large-scale datasets of dense vectors. It is widely used in both academic research and industrial systems.</li>\n</ul>\n<h4 id=\"key-features\">Key Features</h4>\n<ul>\n  <li>\n    <p><strong>Multiple Indexing Strategies</strong>:</p>\n\n    <ul>\n      <li><em>Flat Index</em>: Exhaustive search for exact results.</li>\n      <li><em>Inverted File Index (IVF)</em>: Partitions data using k-means centroids; search occurs in the closest partitions.</li>\n      <li><em>Product Quantization (PQ)</em> and <em>Optimized PQ (OPQ)</em>: Compresses vectors into compact codes.</li>\n      <li><em>IVF-PQ/IVF-OPQ</em>: Combines inverted indexing with quantization for speed-accuracy trade-offs.</li>\n      <li><em>HNSW</em>: Integrated for graph-based search.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>GPU Acceleration</strong>:</p>\n\n    <ul>\n      <li>FAISS provides CUDA-based implementations for key indexing methods.</li>\n      <li>Enables real-time search on billion-scale datasets.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Scalability</strong>:</p>\n\n    <ul>\n      <li>Supports distributed indexing using sharding.</li>\n      <li>Can operate on datasets that do not fit into RAM using memory-mapped files.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Multiple Indexing Strategies</strong>:</p>\n<ul>\n      <li><em>Flat Index</em>: Exhaustive search for exact results.</li>\n      <li><em>Inverted File Index (IVF)</em>: Partitions data using k-means centroids; search occurs in the closest partitions.</li>\n      <li><em>Product Quantization (PQ)</em> and <em>Optimized PQ (OPQ)</em>: Compresses vectors into compact codes.</li>\n      <li><em>IVF-PQ/IVF-OPQ</em>: Combines inverted indexing with quantization for speed-accuracy trade-offs.</li>\n      <li><em>HNSW</em>: Integrated for graph-based search.</li>\n    </ul>\n<p><strong>GPU Acceleration</strong>:</p>\n<ul>\n      <li>FAISS provides CUDA-based implementations for key indexing methods.</li>\n      <li>Enables real-time search on billion-scale datasets.</li>\n    </ul>\n<p><strong>Scalability</strong>:</p>\n<ul>\n      <li>Supports distributed indexing using sharding.</li>\n      <li>Can operate on datasets that do not fit into RAM using memory-mapped files.</li>\n    </ul>\n<h4 id=\"search-workflow\">Search Workflow</h4>\n<ol>\n  <li><strong>Training</strong>: For PQ or IVF, the index must first be trained on a representative sample.</li>\n  <li><strong>Indexing</strong>: Vectors are added to the index using <code class=\"language-plaintext highlighter-rouge\">.add()</code> or <code class=\"language-plaintext highlighter-rouge\">.add_with_ids()</code>.</li>\n  <li><strong>Querying</strong>: The <code class=\"language-plaintext highlighter-rouge\">.search()</code> method returns the k nearest neighbors for each query.</li>\n  <li><strong>Tuning</strong>: Parameters like <code class=\"language-plaintext highlighter-rouge\">nlist</code> (number of clusters), <code class=\"language-plaintext highlighter-rouge\">nprobe</code> (number of partitions to search), and PQ code size significantly affect accuracy and speed.</li>\n</ol>\n<h4 id=\"evaluation-metrics\">Evaluation Metrics</h4>\n<ul>\n  <li><strong>Speed</strong>: Number of queries per second or average query latency.</li>\n  <li><strong>Memory Usage</strong>: Especially critical when using PQ or GPU mode.</li>\n  <li><strong>Accuracy</strong>: Measured using recall metrics (e.g., recall\\@1, recall\\@10).</li>\n</ul>\n<h4 id=\"pros\">Pros</h4>\n<ul>\n  <li>High flexibility and modular design.</li>\n  <li>GPU acceleration with multi-threaded CPU support.</li>\n  <li>Rich suite of index types and hybrid approaches.</li>\n</ul>\n<h4 id=\"cons\">Cons</h4>\n<ul>\n  <li>Can be complex to configure optimally.</li>\n  <li>GPU integration requires familiarity with CUDA.</li>\n  <li>No built-in support for dynamic (online) index updates; best suited for batch ingestion.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/similarity-search/faiss.webp\" alt=\"\"></p>",
      "contentMarkdown": "*   FAISS is a highly optimized C++ library with Python bindings, developed by Facebook AI Research to support efficient similarity search over large-scale datasets of dense vectors. It is widely used in both academic research and industrial systems.\n\n#### Key Features\n\n*   **Multiple Indexing Strategies**:\n    \n    *   _Flat Index_: Exhaustive search for exact results.\n    *   _Inverted File Index (IVF)_: Partitions data using k-means centroids; search occurs in the closest partitions.\n    *   _Product Quantization (PQ)_ and _Optimized PQ (OPQ)_: Compresses vectors into compact codes.\n    *   _IVF-PQ/IVF-OPQ_: Combines inverted indexing with quantization for speed-accuracy trade-offs.\n    *   _HNSW_: Integrated for graph-based search.\n*   **GPU Acceleration**:\n    \n    *   FAISS provides CUDA-based implementations for key indexing methods.\n    *   Enables real-time search on billion-scale datasets.\n*   **Scalability**:\n    \n    *   Supports distributed indexing using sharding.\n    *   Can operate on datasets that do not fit into RAM using memory-mapped files.\n\n**Multiple Indexing Strategies**:\n\n*   _Flat Index_: Exhaustive search for exact results.\n*   _Inverted File Index (IVF)_: Partitions data using k-means centroids; search occurs in the closest partitions.\n*   _Product Quantization (PQ)_ and _Optimized PQ (OPQ)_: Compresses vectors into compact codes.\n*   _IVF-PQ/IVF-OPQ_: Combines inverted indexing with quantization for speed-accuracy trade-offs.\n*   _HNSW_: Integrated for graph-based search.\n\n**GPU Acceleration**:\n\n*   FAISS provides CUDA-based implementations for key indexing methods.\n*   Enables real-time search on billion-scale datasets.\n\n**Scalability**:\n\n*   Supports distributed indexing using sharding.\n*   Can operate on datasets that do not fit into RAM using memory-mapped files.\n\n#### Search Workflow\n\n1.  **Training**: For PQ or IVF, the index must first be trained on a representative sample.\n2.  **Indexing**: Vectors are added to the index using `.add()` or `.add_with_ids()`.\n3.  **Querying**: The `.search()` method returns the k nearest neighbors for each query.\n4.  **Tuning**: Parameters like `nlist` (number of clusters), `nprobe` (number of partitions to search), and PQ code size significantly affect accuracy and speed.\n\n#### Evaluation Metrics\n\n*   **Speed**: Number of queries per second or average query latency.\n*   **Memory Usage**: Especially critical when using PQ or GPU mode.\n*   **Accuracy**: Measured using recall metrics (e.g., recall\\\\@1, recall\\\\@10).\n\n#### Pros\n\n*   High flexibility and modular design.\n*   GPU acceleration with multi-threaded CPU support.\n*   Rich suite of index types and hybrid approaches.\n\n#### Cons\n\n*   Can be complex to configure optimally.\n*   GPU integration requires familiarity with CUDA.\n*   No built-in support for dynamic (online) index updates; best suited for batch ingestion.\n\n![](/primers/ai/assets/similarity-search/faiss.webp)",
      "order": 10,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 390,
        "contentLength": 4080
      },
      "nextCards": [
        "ai-ann-similarity-search-scann-scalable-nearest-neighbors-11",
        "ai-ann-similarity-search-annoy-approximate-nearest-neighbors-oh-yeah-12"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#faiss-(facebook-ai-similarity-search)",
      "scrapedAt": "2025-12-28T11:57:26.973Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5"
      ]
    },
    {
      "id": "ai-ann-similarity-search-scann-scalable-nearest-neighbors-11",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "ANN Libraries",
      "title": "ScaNN (Scalable Nearest Neighbors)",
      "subtitle": "ANN Libraries",
      "contentHtml": "<ul>\n  <li>ScaNN is an efficient similarity search library developed by Google Research, tailored for Maximum Inner Product Search (MIPS) and high-accuracy ANN in embedding-based retrieval systems.</li>\n</ul>\n<h4 id=\"key-features-1\">Key Features</h4>\n<ul>\n  <li>\n    <p><strong>Hybrid Indexing Architecture</strong>:</p>\n\n    <ul>\n      <li><strong>Partitioning Tree</strong>: Efficiently narrows the candidate set using k-means clustering or partition trees.</li>\n      <li><strong>Asymmetric Distance Computation (ADC)</strong>: For fast dot-product or Euclidean similarity.</li>\n      <li><strong>Anisotropic Vector Quantization (AVQ)</strong>: Optimized for high-dimensional and non-uniform data, offering more accurate and balanced indexing.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Hybrid Indexing Architecture</strong>:</p>\n<ul>\n      <li><strong>Partitioning Tree</strong>: Efficiently narrows the candidate set using k-means clustering or partition trees.</li>\n      <li><strong>Asymmetric Distance Computation (ADC)</strong>: For fast dot-product or Euclidean similarity.</li>\n      <li><strong>Anisotropic Vector Quantization (AVQ)</strong>: Optimized for high-dimensional and non-uniform data, offering more accurate and balanced indexing.</li>\n    </ul>\n<h4 id=\"anisotropic-vector-quantization-avq-1\">Anisotropic Vector Quantization (AVQ)</h4>\n<ul>\n  <li>\n    <p>Anisotropic Vector Quantization (AVQ) is a core indexing technique used in ScaNN to enhance performance in real-world, high-dimensional datasets. AVQ is specifically designed for anisotropic distributions, where vector densities vary significantly across the embedding space.</p>\n  </li>\n  <li>\n    <p><strong>Quantization Grid</strong>:</p>\n\n    <ul>\n      <li>AVQ begins by dividing the space into a grid of quantization cells. Each cell corresponds to a cluster of vectors grouped by proximity under Euclidean distance.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Anisotropic Adjustment</strong>:</p>\n\n    <ul>\n      <li>Unlike standard product quantization, AVQ adapts the <strong>shape and size</strong> of each grid cell to reflect the <strong>local density and orientation</strong> of the data.</li>\n      <li>This is achieved via transformations that deform cells based on principal directions, producing elliptical (rather than spherical) boundaries.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Indexing Phase</strong>:</p>\n\n    <ul>\n      <li>During index construction, vectors are assigned to their nearest quantization cell using modified centroids.</li>\n      <li>This mapping improves clustering accuracy, especially in regions where traditional uniform quantization fails.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Search Phase</strong>:</p>\n\n    <ul>\n      <li>For a given query, ScaNN identifies the most relevant cells and retrieves candidate vectors.</li>\n      <li>These candidates are further refined through re-ranking using higher-precision computations, typically involving dot products or exact distances.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Performance Benefits</strong>:</p>\n\n    <ul>\n      <li>AVQ significantly reduces the number of comparisons by focusing the search on well-shaped regions of the vector space.</li>\n      <li>On benchmarks like <a href=\"http://ann-benchmarks.com/glove-100-angular_10_angular.html\">glove-100-angular</a>, ScaNN has demonstrated superior performance, handling nearly twice the query load at the same accuracy compared to the next-best library (NGT-onng).</li>\n      <li>This quantization method is especially useful in deep learning workloads where embeddings exhibit high variance in density.</li>\n    </ul>\n  </li>\n</ul>\n<p>Anisotropic Vector Quantization (AVQ) is a core indexing technique used in ScaNN to enhance performance in real-world, high-dimensional datasets. AVQ is specifically designed for anisotropic distributions, where vector densities vary significantly across the embedding space.</p>\n<p><strong>Quantization Grid</strong>:</p>\n<ul>\n      <li>AVQ begins by dividing the space into a grid of quantization cells. Each cell corresponds to a cluster of vectors grouped by proximity under Euclidean distance.</li>\n    </ul>\n<p><strong>Anisotropic Adjustment</strong>:</p>\n<ul>\n      <li>Unlike standard product quantization, AVQ adapts the <strong>shape and size</strong> of each grid cell to reflect the <strong>local density and orientation</strong> of the data.</li>\n      <li>This is achieved via transformations that deform cells based on principal directions, producing elliptical (rather than spherical) boundaries.</li>\n    </ul>\n<p><strong>Indexing Phase</strong>:</p>\n<ul>\n      <li>During index construction, vectors are assigned to their nearest quantization cell using modified centroids.</li>\n      <li>This mapping improves clustering accuracy, especially in regions where traditional uniform quantization fails.</li>\n    </ul>\n<p><strong>Search Phase</strong>:</p>\n<ul>\n      <li>For a given query, ScaNN identifies the most relevant cells and retrieves candidate vectors.</li>\n      <li>These candidates are further refined through re-ranking using higher-precision computations, typically involving dot products or exact distances.</li>\n    </ul>\n<p><strong>Performance Benefits</strong>:</p>\n<ul>\n      <li>AVQ significantly reduces the number of comparisons by focusing the search on well-shaped regions of the vector space.</li>\n      <li>On benchmarks like <a href=\"http://ann-benchmarks.com/glove-100-angular_10_angular.html\">glove-100-angular</a>, ScaNN has demonstrated superior performance, handling nearly twice the query load at the same accuracy compared to the next-best library (NGT-onng).</li>\n      <li>This quantization method is especially useful in deep learning workloads where embeddings exhibit high variance in density.</li>\n    </ul>\n<p><img src=\"/primers/ai/assets/similarity-search/scann1.jpg\" alt=\"\"></p>\n<h4 id=\"use-case-semantic-search\">Use Case: Semantic Search</h4>\n<ul>\n  <li>ScaNN excels in use cases where embeddings are generated from text, images, or product metadata using deep learning models. Its design is aligned with the two-tower model, where both queries and items are independently embedded into a shared vector space.</li>\n</ul>\n<h4 id=\"implementation-considerations\">Implementation Considerations</h4>\n<ul>\n  <li><strong>Training</strong>: Required for quantization and partitioning.</li>\n  <li><strong>Search Configuration</strong>: Parameters such as the number of clusters, quantization depth, and number of re-ranked results need tuning.</li>\n  <li><strong>Deployment</strong>: Can be used with TensorFlow Serving or directly integrated into a backend service.</li>\n  <li>ScaNN is open-source and available via <a href=\"https://github.com/google-research/google-research/tree/master/scann\">GitHub</a>. It can be installed via Pip and supports both TensorFlow and NumPy inputs.</li>\n</ul>\n<h4 id=\"pros-1\">Pros</h4>\n<ul>\n  <li>High accuracy and throughput for MIPS.</li>\n  <li>Strong performance on semantically rich embeddings.</li>\n  <li>Open-source and actively maintained.</li>\n</ul>\n<h4 id=\"cons-1\">Cons</h4>\n<ul>\n  <li>Less mature ecosystem than FAISS.</li>\n  <li>GPU support is limited.</li>\n  <li>Initial index build time can be high.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/similarity-search/scann.gif\" alt=\"\">\n<img src=\"/primers/ai/assets/similarity-search/scann1.jpg\" alt=\"\"></p>",
      "contentMarkdown": "*   ScaNN is an efficient similarity search library developed by Google Research, tailored for Maximum Inner Product Search (MIPS) and high-accuracy ANN in embedding-based retrieval systems.\n\n#### Key Features\n\n*   **Hybrid Indexing Architecture**:\n    \n    *   **Partitioning Tree**: Efficiently narrows the candidate set using k-means clustering or partition trees.\n    *   **Asymmetric Distance Computation (ADC)**: For fast dot-product or Euclidean similarity.\n    *   **Anisotropic Vector Quantization (AVQ)**: Optimized for high-dimensional and non-uniform data, offering more accurate and balanced indexing.\n\n**Hybrid Indexing Architecture**:\n\n*   **Partitioning Tree**: Efficiently narrows the candidate set using k-means clustering or partition trees.\n*   **Asymmetric Distance Computation (ADC)**: For fast dot-product or Euclidean similarity.\n*   **Anisotropic Vector Quantization (AVQ)**: Optimized for high-dimensional and non-uniform data, offering more accurate and balanced indexing.\n\n#### Anisotropic Vector Quantization (AVQ)\n\n*   Anisotropic Vector Quantization (AVQ) is a core indexing technique used in ScaNN to enhance performance in real-world, high-dimensional datasets. AVQ is specifically designed for anisotropic distributions, where vector densities vary significantly across the embedding space.\n    \n*   **Quantization Grid**:\n    \n    *   AVQ begins by dividing the space into a grid of quantization cells. Each cell corresponds to a cluster of vectors grouped by proximity under Euclidean distance.\n*   **Anisotropic Adjustment**:\n    \n    *   Unlike standard product quantization, AVQ adapts the **shape and size** of each grid cell to reflect the **local density and orientation** of the data.\n    *   This is achieved via transformations that deform cells based on principal directions, producing elliptical (rather than spherical) boundaries.\n*   **Indexing Phase**:\n    \n    *   During index construction, vectors are assigned to their nearest quantization cell using modified centroids.\n    *   This mapping improves clustering accuracy, especially in regions where traditional uniform quantization fails.\n*   **Search Phase**:\n    \n    *   For a given query, ScaNN identifies the most relevant cells and retrieves candidate vectors.\n    *   These candidates are further refined through re-ranking using higher-precision computations, typically involving dot products or exact distances.\n*   **Performance Benefits**:\n    \n    *   AVQ significantly reduces the number of comparisons by focusing the search on well-shaped regions of the vector space.\n    *   On benchmarks like [glove-100-angular](http://ann-benchmarks.com/glove-100-angular_10_angular.html), ScaNN has demonstrated superior performance, handling nearly twice the query load at the same accuracy compared to the next-best library (NGT-onng).\n    *   This quantization method is especially useful in deep learning workloads where embeddings exhibit high variance in density.\n\nAnisotropic Vector Quantization (AVQ) is a core indexing technique used in ScaNN to enhance performance in real-world, high-dimensional datasets. AVQ is specifically designed for anisotropic distributions, where vector densities vary significantly across the embedding space.\n\n**Quantization Grid**:\n\n*   AVQ begins by dividing the space into a grid of quantization cells. Each cell corresponds to a cluster of vectors grouped by proximity under Euclidean distance.\n\n**Anisotropic Adjustment**:\n\n*   Unlike standard product quantization, AVQ adapts the **shape and size** of each grid cell to reflect the **local density and orientation** of the data.\n*   This is achieved via transformations that deform cells based on principal directions, producing elliptical (rather than spherical) boundaries.\n\n**Indexing Phase**:\n\n*   During index construction, vectors are assigned to their nearest quantization cell using modified centroids.\n*   This mapping improves clustering accuracy, especially in regions where traditional uniform quantization fails.\n\n**Search Phase**:\n\n*   For a given query, ScaNN identifies the most relevant cells and retrieves candidate vectors.\n*   These candidates are further refined through re-ranking using higher-precision computations, typically involving dot products or exact distances.\n\n**Performance Benefits**:\n\n*   AVQ significantly reduces the number of comparisons by focusing the search on well-shaped regions of the vector space.\n*   On benchmarks like [glove-100-angular](http://ann-benchmarks.com/glove-100-angular_10_angular.html), ScaNN has demonstrated superior performance, handling nearly twice the query load at the same accuracy compared to the next-best library (NGT-onng).\n*   This quantization method is especially useful in deep learning workloads where embeddings exhibit high variance in density.\n\n![](/primers/ai/assets/similarity-search/scann1.jpg)\n\n#### Use Case: Semantic Search\n\n*   ScaNN excels in use cases where embeddings are generated from text, images, or product metadata using deep learning models. Its design is aligned with the two-tower model, where both queries and items are independently embedded into a shared vector space.\n\n#### Implementation Considerations\n\n*   **Training**: Required for quantization and partitioning.\n*   **Search Configuration**: Parameters such as the number of clusters, quantization depth, and number of re-ranked results need tuning.\n*   **Deployment**: Can be used with TensorFlow Serving or directly integrated into a backend service.\n*   ScaNN is open-source and available via [GitHub](https://github.com/google-research/google-research/tree/master/scann). It can be installed via Pip and supports both TensorFlow and NumPy inputs.\n\n#### Pros\n\n*   High accuracy and throughput for MIPS.\n*   Strong performance on semantically rich embeddings.\n*   Open-source and actively maintained.\n\n#### Cons\n\n*   Less mature ecosystem than FAISS.\n*   GPU support is limited.\n*   Initial index build time can be high.\n\n![](/primers/ai/assets/similarity-search/scann.gif) ![](/primers/ai/assets/similarity-search/scann1.jpg)",
      "order": 11,
      "orderInChapter": 2,
      "difficulty": 2,
      "estimatedMinutes": 4,
      "tags": [
        "miscellaneous",
        "deep learning",
        "embedding"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 769,
        "contentLength": 7426
      },
      "nextCards": [
        "ai-ann-similarity-search-annoy-approximate-nearest-neighbors-oh-yeah-12",
        "ai-ann-similarity-search-tabular-comparison-13"
      ],
      "relatedCards": [
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-overview-of-precision-types-25",
        "ai-model-debugging-sanity-check-model-architecture-and-data-3",
        "ai-ml-runtimes-overview-3",
        "ai-top-30-papers-scaling-laws-for-neural-language-models-22"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#scann-(scalable-nearest-neighbors)",
      "scrapedAt": "2025-12-28T11:57:26.973Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5"
      ]
    },
    {
      "id": "ai-ann-similarity-search-annoy-approximate-nearest-neighbors-oh-yeah-12",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "ANN Libraries",
      "title": "ANNOY (Approximate Nearest Neighbors Oh Yeah)",
      "subtitle": "ANN Libraries",
      "contentHtml": "<ul>\n  <li>Developed by Spotify, ANNOY is a lightweight and efficient C++ library with Python bindings for performing fast similarity search using random projection forests. It is particularly suitable for static datasets and read-heavy workloads.</li>\n</ul>\n<h4 id=\"key-features-2\">Key Features</h4>\n<ul>\n  <li>\n    <p><strong>Indexing Method</strong>:</p>\n\n    <ul>\n      <li>Constructs multiple binary trees using random hyperplane splits.</li>\n      <li>Each tree represents a different partitioning of the space.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Memory Mapping</strong>:</p>\n\n    <ul>\n      <li>Indexes are written to disk and memory-mapped at query time.</li>\n      <li>Enables extremely fast lookups with minimal RAM usage.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Runtime Parameters</strong>:</p>\n\n    <ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">n_trees</code>: Affects accuracy and index size. More trees = better recall.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">search_k</code>: Controls number of nodes checked during search. Higher values = better accuracy, slower speed.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Indexing Method</strong>:</p>\n<ul>\n      <li>Constructs multiple binary trees using random hyperplane splits.</li>\n      <li>Each tree represents a different partitioning of the space.</li>\n    </ul>\n<p><strong>Memory Mapping</strong>:</p>\n<ul>\n      <li>Indexes are written to disk and memory-mapped at query time.</li>\n      <li>Enables extremely fast lookups with minimal RAM usage.</li>\n    </ul>\n<p><strong>Runtime Parameters</strong>:</p>\n<ul>\n      <li><code class=\"language-plaintext highlighter-rouge\">n_trees</code>: Affects accuracy and index size. More trees = better recall.</li>\n      <li><code class=\"language-plaintext highlighter-rouge\">search_k</code>: Controls number of nodes checked during search. Higher values = better accuracy, slower speed.</li>\n    </ul>\n<h4 id=\"use-case-music-recommendation\">Use Case: Music Recommendation</h4>\n<ul>\n  <li>Spotify uses ANNOY for fast retrieval of similar songs, playlists, or users based on vector representations of user behavior or audio features.</li>\n</ul>\n<h4 id=\"implementation-notes\">Implementation Notes</h4>\n<ul>\n  <li>Static indexing: Does not support dynamic insertions (though <code class=\"language-plaintext highlighter-rouge\">annoy2</code> aims to address this).</li>\n  <li>No GPU support.</li>\n  <li>No native batch processing interface; needs custom implementation for throughput optimization.</li>\n</ul>\n<h4 id=\"pros-2\">Pros</h4>\n<ul>\n  <li>Simple to use and highly portable.</li>\n  <li>Memory-efficient due to disk-based index.</li>\n  <li>Suitable for multi-process environments.</li>\n</ul>\n<h4 id=\"cons-2\">Cons</h4>\n<ul>\n  <li>Limited support for dynamic updates.</li>\n  <li>Slower than FAISS or ScaNN for very large datasets.</li>\n  <li>No GPU acceleration or quantization techniques.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/similarity-search/annoy.jpg\" alt=\"\"></p>\n<ul>\n  <li>These libraries encapsulate the most widely adopted implementations of ANN techniques in industry today. Each has a different performance profile and is better suited to specific use cases:\n    <ul>\n      <li><strong>FAISS</strong>: Ideal for large-scale, high-performance search with GPU support.</li>\n      <li><strong>ScaNN</strong>: Excellent for semantically meaningful embeddings and MIPS-heavy workloads.</li>\n      <li><strong>ANNOY</strong>: Lightweight and efficient for static, read-optimized systems.</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li><strong>FAISS</strong>: Ideal for large-scale, high-performance search with GPU support.</li>\n      <li><strong>ScaNN</strong>: Excellent for semantically meaningful embeddings and MIPS-heavy workloads.</li>\n      <li><strong>ANNOY</strong>: Lightweight and efficient for static, read-optimized systems.</li>\n    </ul>",
      "contentMarkdown": "*   Developed by Spotify, ANNOY is a lightweight and efficient C++ library with Python bindings for performing fast similarity search using random projection forests. It is particularly suitable for static datasets and read-heavy workloads.\n\n#### Key Features\n\n*   **Indexing Method**:\n    \n    *   Constructs multiple binary trees using random hyperplane splits.\n    *   Each tree represents a different partitioning of the space.\n*   **Memory Mapping**:\n    \n    *   Indexes are written to disk and memory-mapped at query time.\n    *   Enables extremely fast lookups with minimal RAM usage.\n*   **Runtime Parameters**:\n    \n    *   `n_trees`: Affects accuracy and index size. More trees = better recall.\n    *   `search_k`: Controls number of nodes checked during search. Higher values = better accuracy, slower speed.\n\n**Indexing Method**:\n\n*   Constructs multiple binary trees using random hyperplane splits.\n*   Each tree represents a different partitioning of the space.\n\n**Memory Mapping**:\n\n*   Indexes are written to disk and memory-mapped at query time.\n*   Enables extremely fast lookups with minimal RAM usage.\n\n**Runtime Parameters**:\n\n*   `n_trees`: Affects accuracy and index size. More trees = better recall.\n*   `search_k`: Controls number of nodes checked during search. Higher values = better accuracy, slower speed.\n\n#### Use Case: Music Recommendation\n\n*   Spotify uses ANNOY for fast retrieval of similar songs, playlists, or users based on vector representations of user behavior or audio features.\n\n#### Implementation Notes\n\n*   Static indexing: Does not support dynamic insertions (though `annoy2` aims to address this).\n*   No GPU support.\n*   No native batch processing interface; needs custom implementation for throughput optimization.\n\n#### Pros\n\n*   Simple to use and highly portable.\n*   Memory-efficient due to disk-based index.\n*   Suitable for multi-process environments.\n\n#### Cons\n\n*   Limited support for dynamic updates.\n*   Slower than FAISS or ScaNN for very large datasets.\n*   No GPU acceleration or quantization techniques.\n\n![](/primers/ai/assets/similarity-search/annoy.jpg)\n\n*   These libraries encapsulate the most widely adopted implementations of ANN techniques in industry today. Each has a different performance profile and is better suited to specific use cases:\n    *   **FAISS**: Ideal for large-scale, high-performance search with GPU support.\n    *   **ScaNN**: Excellent for semantically meaningful embeddings and MIPS-heavy workloads.\n    *   **ANNOY**: Lightweight and efficient for static, read-optimized systems.\n\n*   **FAISS**: Ideal for large-scale, high-performance search with GPU support.\n*   **ScaNN**: Excellent for semantically meaningful embeddings and MIPS-heavy workloads.\n*   **ANNOY**: Lightweight and efficient for static, read-optimized systems.",
      "order": 12,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "embedding",
        "optimization"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 380,
        "contentLength": 3923
      },
      "nextCards": [
        "ai-ann-similarity-search-tabular-comparison-13",
        "ai-ann-similarity-search-purpose-and-scope-14"
      ],
      "relatedCards": [
        "ai-top-30-papers-scaling-laws-for-neural-language-models-22",
        "ai-gpu-architecture-key-architectural-design-goals-7",
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-model-debugging-misaligned-optimization-and-evaluation-5",
        "ai-ml-runtimes-architecture-4"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#annoy-(approximate-nearest-neighbors-oh-yeah)",
      "scrapedAt": "2025-12-28T11:57:26.973Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5"
      ]
    },
    {
      "id": "ai-ann-similarity-search-tabular-comparison-13",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "Comparative Analysis",
      "title": "Tabular Comparison",
      "subtitle": "Comparative Analysis",
      "contentHtml": "<div align=\"center\">\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Library</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Definition/Functioning</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Pros</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Cons</strong></th>\n</tr>\n</thead>\n<tbody>\n\n<tr>\n<td class=\"tg-tleft-valign-first\"><strong>FAISS</strong></td>\n<td class=\"tg-tleft-valign-first\">Modular similarity search library supporting flat (brute-force), inverted file (IVF), product quantization (PQ), optimized PQ (OPQ), and graph-based (HNSW) indexing. Offers strong support for hybrid strategies and CUDA-based GPU acceleration.</td>\n<td class=\"tg-tleft-valign-first\">\n<ul>\n<li>Handles billion-scale datasets efficiently with GPU and SIMD acceleration</li>\n<li>Supports both L2 and inner product distance functions</li>\n<li>Highly modular: custom pipelines can be built by combining indexing strategies</li>\n<li>Supports on-disk indexing, IVF quantization, and parallel queries</li>\n</ul>\n</td>\n<td class=\"tg-tleft-valign-second\">\n<ul>\n<li>No native support for dynamic indexing (most indexes are immutable)</li>\n<li>Advanced configurations (e.g., PQ with OPQ over IVF) require tuning expertise</li>\n<li>Limited integration with non-PyTorch ML stacks (e.g., TensorFlow)</li>\n</ul>\n</td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\"><strong>ScaNN</strong></td>\n<td class=\"tg-tleft-valign-first\">Google's ANN framework optimized for Maximum Inner Product Search (MIPS). Uses hybrid strategies: tree-based partitioning, anisotropic vector quantization (AVQ), and re-ranking with exact computation. Especially well-suited for semantic embeddings.</td>\n<td class=\"tg-tleft-valign-first\">\n<ul>\n<li>Designed for embedding-based search (e.g., NLP, recommendation systems)</li>\n<li>Highly accurate top-k results in inner product space (MIPS)</li>\n<li>Well-suited for TensorFlow and Numpy workflows; easy to integrate with model pipelines</li>\n<li>Significantly faster than LSH or naive MIPS approaches</li>\n</ul>\n</td>\n<td class=\"tg-tleft-valign-second\">\n<ul>\n<li>No official GPU acceleration (primarily CPU-based)</li>\n<li>Less flexible than FAISS for custom index architectures</li>\n<li>Longer index construction time due to partitioning and quantization training</li>\n</ul>\n</td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\"><strong>ANNOY</strong></td>\n<td class=\"tg-tleft-valign-first\">Tree-based approach using multiple random projection trees. Each tree splits the space using randomly selected hyperplanes. Designed to trade off build time and search accuracy with minimal memory overhead using memory-mapped files.</td>\n<td class=\"tg-tleft-valign-first\">\n<ul>\n<li>Index files can be memory-mapped and shared across processes</li>\n<li>Supports very large datasets on low-memory systems (e.g., embedded devices)</li>\n<li>Simple API and fast to build indexes for static data</li>\n<li>Ideal for batch processing and edge deployments</li>\n</ul>\n</td>\n<td class=\"tg-tleft-valign-second\">\n<ul>\n<li>Cannot update index after construction (no dynamic insertions)</li>\n<li>Not designed for GPU acceleration or vector quantization</li>\n<li>Recall suffers for very high-dimensional data (&gt;200D)</li>\n<li>No native batch-query optimization; needs manual parallelization</li>\n</ul>\n</td>\n</tr>\n\n</tbody>\n</table>\n</div>\n<table class=\"tg\">\n<thead>\n<tr>\n<th class=\"tg-hcenter-valign-first\"><strong>Library</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Definition/Functioning</strong></th>\n<th class=\"tg-hcenter-valign-first\"><strong>Pros</strong></th>\n<th class=\"tg-hcenter-valign-second\"><strong>Cons</strong></th>\n</tr>\n</thead>\n<tbody>\n\n<tr>\n<td class=\"tg-tleft-valign-first\"><strong>FAISS</strong></td>\n<td class=\"tg-tleft-valign-first\">Modular similarity search library supporting flat (brute-force), inverted file (IVF), product quantization (PQ), optimized PQ (OPQ), and graph-based (HNSW) indexing. Offers strong support for hybrid strategies and CUDA-based GPU acceleration.</td>\n<td class=\"tg-tleft-valign-first\">\n<ul>\n<li>Handles billion-scale datasets efficiently with GPU and SIMD acceleration</li>\n<li>Supports both L2 and inner product distance functions</li>\n<li>Highly modular: custom pipelines can be built by combining indexing strategies</li>\n<li>Supports on-disk indexing, IVF quantization, and parallel queries</li>\n</ul>\n</td>\n<td class=\"tg-tleft-valign-second\">\n<ul>\n<li>No native support for dynamic indexing (most indexes are immutable)</li>\n<li>Advanced configurations (e.g., PQ with OPQ over IVF) require tuning expertise</li>\n<li>Limited integration with non-PyTorch ML stacks (e.g., TensorFlow)</li>\n</ul>\n</td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\"><strong>ScaNN</strong></td>\n<td class=\"tg-tleft-valign-first\">Google's ANN framework optimized for Maximum Inner Product Search (MIPS). Uses hybrid strategies: tree-based partitioning, anisotropic vector quantization (AVQ), and re-ranking with exact computation. Especially well-suited for semantic embeddings.</td>\n<td class=\"tg-tleft-valign-first\">\n<ul>\n<li>Designed for embedding-based search (e.g., NLP, recommendation systems)</li>\n<li>Highly accurate top-k results in inner product space (MIPS)</li>\n<li>Well-suited for TensorFlow and Numpy workflows; easy to integrate with model pipelines</li>\n<li>Significantly faster than LSH or naive MIPS approaches</li>\n</ul>\n</td>\n<td class=\"tg-tleft-valign-second\">\n<ul>\n<li>No official GPU acceleration (primarily CPU-based)</li>\n<li>Less flexible than FAISS for custom index architectures</li>\n<li>Longer index construction time due to partitioning and quantization training</li>\n</ul>\n</td>\n</tr>\n\n<tr>\n<td class=\"tg-tleft-valign-first\"><strong>ANNOY</strong></td>\n<td class=\"tg-tleft-valign-first\">Tree-based approach using multiple random projection trees. Each tree splits the space using randomly selected hyperplanes. Designed to trade off build time and search accuracy with minimal memory overhead using memory-mapped files.</td>\n<td class=\"tg-tleft-valign-first\">\n<ul>\n<li>Index files can be memory-mapped and shared across processes</li>\n<li>Supports very large datasets on low-memory systems (e.g., embedded devices)</li>\n<li>Simple API and fast to build indexes for static data</li>\n<li>Ideal for batch processing and edge deployments</li>\n</ul>\n</td>\n<td class=\"tg-tleft-valign-second\">\n<ul>\n<li>Cannot update index after construction (no dynamic insertions)</li>\n<li>Not designed for GPU acceleration or vector quantization</li>\n<li>Recall suffers for very high-dimensional data (&gt;200D)</li>\n<li>No native batch-query optimization; needs manual parallelization</li>\n</ul>\n</td>\n</tr>\n\n</tbody>\n</table>\n<ul>\n<li>Handles billion-scale datasets efficiently with GPU and SIMD acceleration</li>\n<li>Supports both L2 and inner product distance functions</li>\n<li>Highly modular: custom pipelines can be built by combining indexing strategies</li>\n<li>Supports on-disk indexing, IVF quantization, and parallel queries</li>\n</ul>\n<ul>\n<li>No native support for dynamic indexing (most indexes are immutable)</li>\n<li>Advanced configurations (e.g., PQ with OPQ over IVF) require tuning expertise</li>\n<li>Limited integration with non-PyTorch ML stacks (e.g., TensorFlow)</li>\n</ul>\n<ul>\n<li>Designed for embedding-based search (e.g., NLP, recommendation systems)</li>\n<li>Highly accurate top-k results in inner product space (MIPS)</li>\n<li>Well-suited for TensorFlow and Numpy workflows; easy to integrate with model pipelines</li>\n<li>Significantly faster than LSH or naive MIPS approaches</li>\n</ul>\n<ul>\n<li>No official GPU acceleration (primarily CPU-based)</li>\n<li>Less flexible than FAISS for custom index architectures</li>\n<li>Longer index construction time due to partitioning and quantization training</li>\n</ul>\n<ul>\n<li>Index files can be memory-mapped and shared across processes</li>\n<li>Supports very large datasets on low-memory systems (e.g., embedded devices)</li>\n<li>Simple API and fast to build indexes for static data</li>\n<li>Ideal for batch processing and edge deployments</li>\n</ul>\n<ul>\n<li>Cannot update index after construction (no dynamic insertions)</li>\n<li>Not designed for GPU acceleration or vector quantization</li>\n<li>Recall suffers for very high-dimensional data (&gt;200D)</li>\n<li>No native batch-query optimization; needs manual parallelization</li>\n</ul>\n<ul>\n  <li>\n    <p>This comparison should guide you toward the best ANN system for your specific technical needs:</p>\n\n    <ul>\n      <li>Choose <strong>FAISS</strong> for large-scale indexing with advanced GPU acceleration and tight performance tuning.</li>\n      <li>Use <strong>ScaNN</strong> if you’re operating on semantic embeddings or deep learning pipelines, particularly with MIPS.</li>\n      <li>Opt for <strong>ANNOY</strong> when working with static datasets in constrained memory environments or needing file-based deployment.</li>\n      <li>Prefer <strong>HNSW</strong> when low latency and high recall are critical, and memory usage is acceptable.</li>\n      <li>Consider <strong>FINGER</strong> as a low-overhead performance enhancer if you already employ graph-based indexes.</li>\n    </ul>\n  </li>\n</ul>\n<p>This comparison should guide you toward the best ANN system for your specific technical needs:</p>\n<ul>\n      <li>Choose <strong>FAISS</strong> for large-scale indexing with advanced GPU acceleration and tight performance tuning.</li>\n      <li>Use <strong>ScaNN</strong> if you’re operating on semantic embeddings or deep learning pipelines, particularly with MIPS.</li>\n      <li>Opt for <strong>ANNOY</strong> when working with static datasets in constrained memory environments or needing file-based deployment.</li>\n      <li>Prefer <strong>HNSW</strong> when low latency and high recall are critical, and memory usage is acceptable.</li>\n      <li>Consider <strong>FINGER</strong> as a low-overhead performance enhancer if you already employ graph-based indexes.</li>\n    </ul>",
      "contentMarkdown": "**Library**\n\n**Definition/Functioning**\n\n**Pros**\n\n**Cons**\n\n**FAISS**\n\nModular similarity search library supporting flat (brute-force), inverted file (IVF), product quantization (PQ), optimized PQ (OPQ), and graph-based (HNSW) indexing. Offers strong support for hybrid strategies and CUDA-based GPU acceleration.\n\n*   Handles billion-scale datasets efficiently with GPU and SIMD acceleration\n*   Supports both L2 and inner product distance functions\n*   Highly modular: custom pipelines can be built by combining indexing strategies\n*   Supports on-disk indexing, IVF quantization, and parallel queries\n\n*   No native support for dynamic indexing (most indexes are immutable)\n*   Advanced configurations (e.g., PQ with OPQ over IVF) require tuning expertise\n*   Limited integration with non-PyTorch ML stacks (e.g., TensorFlow)\n\n**ScaNN**\n\nGoogle's ANN framework optimized for Maximum Inner Product Search (MIPS). Uses hybrid strategies: tree-based partitioning, anisotropic vector quantization (AVQ), and re-ranking with exact computation. Especially well-suited for semantic embeddings.\n\n*   Designed for embedding-based search (e.g., NLP, recommendation systems)\n*   Highly accurate top-k results in inner product space (MIPS)\n*   Well-suited for TensorFlow and Numpy workflows; easy to integrate with model pipelines\n*   Significantly faster than LSH or naive MIPS approaches\n\n*   No official GPU acceleration (primarily CPU-based)\n*   Less flexible than FAISS for custom index architectures\n*   Longer index construction time due to partitioning and quantization training\n\n**ANNOY**\n\nTree-based approach using multiple random projection trees. Each tree splits the space using randomly selected hyperplanes. Designed to trade off build time and search accuracy with minimal memory overhead using memory-mapped files.\n\n*   Index files can be memory-mapped and shared across processes\n*   Supports very large datasets on low-memory systems (e.g., embedded devices)\n*   Simple API and fast to build indexes for static data\n*   Ideal for batch processing and edge deployments\n\n*   Cannot update index after construction (no dynamic insertions)\n*   Not designed for GPU acceleration or vector quantization\n*   Recall suffers for very high-dimensional data (>200D)\n*   No native batch-query optimization; needs manual parallelization\n\n**Library**\n\n**Definition/Functioning**\n\n**Pros**\n\n**Cons**\n\n**FAISS**\n\nModular similarity search library supporting flat (brute-force), inverted file (IVF), product quantization (PQ), optimized PQ (OPQ), and graph-based (HNSW) indexing. Offers strong support for hybrid strategies and CUDA-based GPU acceleration.\n\n*   Handles billion-scale datasets efficiently with GPU and SIMD acceleration\n*   Supports both L2 and inner product distance functions\n*   Highly modular: custom pipelines can be built by combining indexing strategies\n*   Supports on-disk indexing, IVF quantization, and parallel queries\n\n*   No native support for dynamic indexing (most indexes are immutable)\n*   Advanced configurations (e.g., PQ with OPQ over IVF) require tuning expertise\n*   Limited integration with non-PyTorch ML stacks (e.g., TensorFlow)\n\n**ScaNN**\n\nGoogle's ANN framework optimized for Maximum Inner Product Search (MIPS). Uses hybrid strategies: tree-based partitioning, anisotropic vector quantization (AVQ), and re-ranking with exact computation. Especially well-suited for semantic embeddings.\n\n*   Designed for embedding-based search (e.g., NLP, recommendation systems)\n*   Highly accurate top-k results in inner product space (MIPS)\n*   Well-suited for TensorFlow and Numpy workflows; easy to integrate with model pipelines\n*   Significantly faster than LSH or naive MIPS approaches\n\n*   No official GPU acceleration (primarily CPU-based)\n*   Less flexible than FAISS for custom index architectures\n*   Longer index construction time due to partitioning and quantization training\n\n**ANNOY**\n\nTree-based approach using multiple random projection trees. Each tree splits the space using randomly selected hyperplanes. Designed to trade off build time and search accuracy with minimal memory overhead using memory-mapped files.\n\n*   Index files can be memory-mapped and shared across processes\n*   Supports very large datasets on low-memory systems (e.g., embedded devices)\n*   Simple API and fast to build indexes for static data\n*   Ideal for batch processing and edge deployments\n\n*   Cannot update index after construction (no dynamic insertions)\n*   Not designed for GPU acceleration or vector quantization\n*   Recall suffers for very high-dimensional data (>200D)\n*   No native batch-query optimization; needs manual parallelization\n\n*   Handles billion-scale datasets efficiently with GPU and SIMD acceleration\n*   Supports both L2 and inner product distance functions\n*   Highly modular: custom pipelines can be built by combining indexing strategies\n*   Supports on-disk indexing, IVF quantization, and parallel queries\n\n*   No native support for dynamic indexing (most indexes are immutable)\n*   Advanced configurations (e.g., PQ with OPQ over IVF) require tuning expertise\n*   Limited integration with non-PyTorch ML stacks (e.g., TensorFlow)\n\n*   Designed for embedding-based search (e.g., NLP, recommendation systems)\n*   Highly accurate top-k results in inner product space (MIPS)\n*   Well-suited for TensorFlow and Numpy workflows; easy to integrate with model pipelines\n*   Significantly faster than LSH or naive MIPS approaches\n\n*   No official GPU acceleration (primarily CPU-based)\n*   Less flexible than FAISS for custom index architectures\n*   Longer index construction time due to partitioning and quantization training\n\n*   Index files can be memory-mapped and shared across processes\n*   Supports very large datasets on low-memory systems (e.g., embedded devices)\n*   Simple API and fast to build indexes for static data\n*   Ideal for batch processing and edge deployments\n\n*   Cannot update index after construction (no dynamic insertions)\n*   Not designed for GPU acceleration or vector quantization\n*   Recall suffers for very high-dimensional data (>200D)\n*   No native batch-query optimization; needs manual parallelization\n\n*   This comparison should guide you toward the best ANN system for your specific technical needs:\n    \n    *   Choose **FAISS** for large-scale indexing with advanced GPU acceleration and tight performance tuning.\n    *   Use **ScaNN** if you’re operating on semantic embeddings or deep learning pipelines, particularly with MIPS.\n    *   Opt for **ANNOY** when working with static datasets in constrained memory environments or needing file-based deployment.\n    *   Prefer **HNSW** when low latency and high recall are critical, and memory usage is acceptable.\n    *   Consider **FINGER** as a low-overhead performance enhancer if you already employ graph-based indexes.\n\nThis comparison should guide you toward the best ANN system for your specific technical needs:\n\n*   Choose **FAISS** for large-scale indexing with advanced GPU acceleration and tight performance tuning.\n*   Use **ScaNN** if you’re operating on semantic embeddings or deep learning pipelines, particularly with MIPS.\n*   Opt for **ANNOY** when working with static datasets in constrained memory environments or needing file-based deployment.\n*   Prefer **HNSW** when low latency and high recall are critical, and memory usage is acceptable.\n*   Consider **FINGER** as a low-overhead performance enhancer if you already employ graph-based indexes.",
      "order": 13,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 6,
      "tags": [
        "miscellaneous",
        "deep learning",
        "embedding",
        "nlp",
        "optimization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 1023,
        "contentLength": 10019
      },
      "nextCards": [
        "ai-ann-similarity-search-purpose-and-scope-14",
        "ai-ann-similarity-search-key-features-15"
      ],
      "relatedCards": [
        "ai-top-30-papers-dense-passage-retrieval-for-open-domain-question-a-28",
        "ai-top-30-papers-deep-residual-learning-for-image-recognition-10",
        "ai-top-30-papers-gpipe-easy-scaling-with-micro-batch-pipeline-paral-9",
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-key-architectural-design-goals-7"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#tabular-comparison",
      "scrapedAt": "2025-12-28T11:57:26.973Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5"
      ]
    },
    {
      "id": "ai-ann-similarity-search-purpose-and-scope-14",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "ANN-Benchmarks",
      "title": "Purpose and Scope",
      "subtitle": "ANN-Benchmarks",
      "contentHtml": "<ul>\n  <li>The benchmark suite is designed to answer key questions such as:\n    <ul>\n      <li>Which ANN method offers the best speed/accuracy trade-off for a particular type of data?</li>\n      <li>How do different libraries perform under the same distance metric?</li>\n      <li>What is the relative performance of quantization vs. graph-based indexing on benchmark datasets?</li>\n    </ul>\n  </li>\n</ul>\n<ul>\n      <li>Which ANN method offers the best speed/accuracy trade-off for a particular type of data?</li>\n      <li>How do different libraries perform under the same distance metric?</li>\n      <li>What is the relative performance of quantization vs. graph-based indexing on benchmark datasets?</li>\n    </ul>",
      "contentMarkdown": "*   The benchmark suite is designed to answer key questions such as:\n    *   Which ANN method offers the best speed/accuracy trade-off for a particular type of data?\n    *   How do different libraries perform under the same distance metric?\n    *   What is the relative performance of quantization vs. graph-based indexing on benchmark datasets?\n\n*   Which ANN method offers the best speed/accuracy trade-off for a particular type of data?\n*   How do different libraries perform under the same distance metric?\n*   What is the relative performance of quantization vs. graph-based indexing on benchmark datasets?",
      "order": 14,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 92,
        "contentLength": 719
      },
      "nextCards": [
        "ai-ann-similarity-search-key-features-15",
        "ai-ann-similarity-search-using-ann-benchmarks-16"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#purpose-and-scope",
      "scrapedAt": "2025-12-28T11:57:26.973Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5"
      ]
    },
    {
      "id": "ai-ann-similarity-search-key-features-15",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "ANN-Benchmarks",
      "title": "Key Features",
      "subtitle": "ANN-Benchmarks",
      "contentHtml": "<ul>\n  <li>\n    <p><strong>Common Datasets</strong>:</p>\n\n    <ul>\n      <li>Includes real-world and synthetic datasets like <code class=\"language-plaintext highlighter-rouge\">SIFT</code>, <code class=\"language-plaintext highlighter-rouge\">GloVe</code>, <code class=\"language-plaintext highlighter-rouge\">Fashion-MNIST</code>, and <code class=\"language-plaintext highlighter-rouge\">Deep Image Descriptors</code>.</li>\n      <li>Each dataset varies in dimensionality, density, and semantics, allowing for broad evaluation.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Distance Metrics</strong>:</p>\n\n    <ul>\n      <li>Supports various metrics such as Euclidean distance (L2), angular distance (cosine similarity), and inner product (for MIPS scenarios).</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Evaluation Metrics</strong>:</p>\n\n    <ul>\n      <li><strong>Recall@<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 0.622em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.519em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.52em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">k</script></strong>: Measures the fraction of true nearest neighbors found in the top-k results.</li>\n      <li><strong>QPS (Queries per second)</strong>: Throughput of the method, showing speed under load.</li>\n      <li><strong>Index size and build time</strong>: Assesses memory footprint and pre-processing requirements.</li>\n    </ul>\n  </li>\n  <li>\n    <p><strong>Leaderboard Interface</strong>:</p>\n\n    <ul>\n      <li>Live comparison of algorithmic results across datasets.</li>\n      <li>Interactive plots showing recall vs. query time, making it easy to identify optimal configurations.</li>\n    </ul>\n  </li>\n</ul>\n<p><strong>Common Datasets</strong>:</p>\n<ul>\n      <li>Includes real-world and synthetic datasets like <code class=\"language-plaintext highlighter-rouge\">SIFT</code>, <code class=\"language-plaintext highlighter-rouge\">GloVe</code>, <code class=\"language-plaintext highlighter-rouge\">Fashion-MNIST</code>, and <code class=\"language-plaintext highlighter-rouge\">Deep Image Descriptors</code>.</li>\n      <li>Each dataset varies in dimensionality, density, and semantics, allowing for broad evaluation.</li>\n    </ul>\n<p><strong>Distance Metrics</strong>:</p>\n<ul>\n      <li>Supports various metrics such as Euclidean distance (L2), angular distance (cosine similarity), and inner product (for MIPS scenarios).</li>\n    </ul>\n<p><strong>Evaluation Metrics</strong>:</p>\n<ul>\n      <li><strong>Recall@<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 0.622em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.519em; height: 0px; font-size: 121%;\"><span style=\"position: absolute; clip: rect(1.346em, 1000.52em, 2.327em, -999.997em); top: -2.167em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.172em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">k</script></strong>: Measures the fraction of true nearest neighbors found in the top-k results.</li>\n      <li><strong>QPS (Queries per second)</strong>: Throughput of the method, showing speed under load.</li>\n      <li><strong>Index size and build time</strong>: Assesses memory footprint and pre-processing requirements.</li>\n    </ul>\n<p><strong>Leaderboard Interface</strong>:</p>\n<ul>\n      <li>Live comparison of algorithmic results across datasets.</li>\n      <li>Interactive plots showing recall vs. query time, making it easy to identify optimal configurations.</li>\n    </ul>",
      "contentMarkdown": "*   **Common Datasets**:\n    \n    *   Includes real-world and synthetic datasets like `SIFT`, `GloVe`, `Fashion-MNIST`, and `Deep Image Descriptors`.\n    *   Each dataset varies in dimensionality, density, and semantics, allowing for broad evaluation.\n*   **Distance Metrics**:\n    \n    *   Supports various metrics such as Euclidean distance (L2), angular distance (cosine similarity), and inner product (for MIPS scenarios).\n*   **Evaluation Metrics**:\n    \n    *   **Recall@kkk**: Measures the fraction of true nearest neighbors found in the top-k results.\n    *   **QPS (Queries per second)**: Throughput of the method, showing speed under load.\n    *   **Index size and build time**: Assesses memory footprint and pre-processing requirements.\n*   **Leaderboard Interface**:\n    \n    *   Live comparison of algorithmic results across datasets.\n    *   Interactive plots showing recall vs. query time, making it easy to identify optimal configurations.\n\n**Common Datasets**:\n\n*   Includes real-world and synthetic datasets like `SIFT`, `GloVe`, `Fashion-MNIST`, and `Deep Image Descriptors`.\n*   Each dataset varies in dimensionality, density, and semantics, allowing for broad evaluation.\n\n**Distance Metrics**:\n\n*   Supports various metrics such as Euclidean distance (L2), angular distance (cosine similarity), and inner product (for MIPS scenarios).\n\n**Evaluation Metrics**:\n\n*   **Recall@kkk**: Measures the fraction of true nearest neighbors found in the top-k results.\n*   **QPS (Queries per second)**: Throughput of the method, showing speed under load.\n*   **Index size and build time**: Assesses memory footprint and pre-processing requirements.\n\n**Leaderboard Interface**:\n\n*   Live comparison of algorithmic results across datasets.\n*   Interactive plots showing recall vs. query time, making it easy to identify optimal configurations.",
      "order": 15,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 236,
        "contentLength": 5384
      },
      "nextCards": [
        "ai-ann-similarity-search-using-ann-benchmarks-16",
        "ai-ann-similarity-search-practical-use-cases-17"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#key-features",
      "scrapedAt": "2025-12-28T11:57:26.973Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5"
      ]
    },
    {
      "id": "ai-ann-similarity-search-using-ann-benchmarks-16",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "ANN-Benchmarks",
      "title": "Using ANN-Benchmarks",
      "subtitle": "ANN-Benchmarks",
      "contentHtml": "<p>The benchmarks can be run locally via Docker and Python, allowing users to test their own ANN implementations or configurations. It supports integrating custom methods into the benchmarking pipeline, which makes it a valuable research tool.</p>\n<p><img src=\"../../../images/read/ANN-Benchmarks.jpg\" alt=\"\"></p>",
      "contentMarkdown": "The benchmarks can be run locally via Docker and Python, allowing users to test their own ANN implementations or configurations. It supports integrating custom methods into the benchmarking pipeline, which makes it a valuable research tool.\n\n![](../../../images/read/ANN-Benchmarks.jpg)",
      "order": 16,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 37,
        "contentLength": 313
      },
      "nextCards": [
        "ai-ann-similarity-search-practical-use-cases-17"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#using-ann-benchmarks",
      "scrapedAt": "2025-12-28T11:57:26.973Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5"
      ]
    },
    {
      "id": "ai-ann-similarity-search-practical-use-cases-17",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Approximate Nearest Neighbors – Similarity Search",
      "articleSlug": "ann-similarity-search",
      "chapter": "ANN-Benchmarks",
      "title": "Practical Use Cases",
      "subtitle": "ANN-Benchmarks",
      "contentHtml": "<ul>\n  <li><strong>Model Selection</strong>: Choose the best ANN library for your specific task and hardware constraints.</li>\n  <li><strong>Algorithm Tuning</strong>: Understand how hyperparameters like <code class=\"language-plaintext highlighter-rouge\">nlist</code>, <code class=\"language-plaintext highlighter-rouge\">nprobe</code>, or <code class=\"language-plaintext highlighter-rouge\">search_k</code> affect real-world performance.</li>\n  <li><strong>Regression Testing</strong>: Evaluate how updates to ANN methods impact speed or recall.</li>\n</ul>",
      "contentMarkdown": "*   **Model Selection**: Choose the best ANN library for your specific task and hardware constraints.\n*   **Algorithm Tuning**: Understand how hyperparameters like `nlist`, `nprobe`, or `search_k` affect real-world performance.\n*   **Regression Testing**: Evaluate how updates to ANN methods impact speed or recall.",
      "order": 17,
      "orderInChapter": 4,
      "difficulty": 4,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 42,
        "contentLength": 554
      },
      "nextCards": [],
      "relatedCards": [
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-synchronization-and-communication-12",
        "ai-gpu-architecture-memory-management-and-transfer-15",
        "ai-gpu-architecture-inference-deployment-18"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/ann-similarity-search/#practical-use-cases",
      "scrapedAt": "2025-12-28T11:57:26.973Z",
      "siblings": [
        "ai-ann-similarity-search-real-world-applications-1",
        "ai-ann-similarity-search-from-exact-to-approximate-nearest-neighbor-search-2",
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-tree-based-methods-4",
        "ai-ann-similarity-search-quantization-based-methods-5"
      ]
    },
    {
      "id": "ai-transferability-estimation-log-expected-empirical-prediction-leep-1",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Transferability Estimation",
      "articleSlug": "transferability-estimation",
      "chapter": "Overview",
      "title": "Log Expected Empirical Prediction (LEEP)",
      "subtitle": "Overview",
      "contentHtml": "<ul>\n  <li><a href=\"https://arxiv.org/abs/2002.12462\">LEEP</a> by Nguyen et al. from Amazon Web Services and Facebook AI in ICML 2020 proposes to measure the transferability from the source dataset to the target dataset by evaluating the log likelihood of the correct prediction on the target dataset. The individual probability of the correct prediction on the target dataset is calculated through a predictive distribution based on two conditional probabilities:\n    <ol>\n      <li>The probability of the dummy label based on the categorical distribution of the trained model (trained on the source dataset) evaluated on the input of the target dataset.</li>\n      <li>The conditional density of the target dataset’s label given the dummy label from the previous step. The predictive distribution is then evaluated through integrating over all possible dummy labels.</li>\n    </ol>\n  </li>\n</ul>\n<ol>\n      <li>The probability of the dummy label based on the categorical distribution of the trained model (trained on the source dataset) evaluated on the input of the target dataset.</li>\n      <li>The conditional density of the target dataset’s label given the dummy label from the previous step. The predictive distribution is then evaluated through integrating over all possible dummy labels.</li>\n    </ol>",
      "contentMarkdown": "*   [LEEP](https://arxiv.org/abs/2002.12462) by Nguyen et al. from Amazon Web Services and Facebook AI in ICML 2020 proposes to measure the transferability from the source dataset to the target dataset by evaluating the log likelihood of the correct prediction on the target dataset. The individual probability of the correct prediction on the target dataset is calculated through a predictive distribution based on two conditional probabilities:\n    1.  The probability of the dummy label based on the categorical distribution of the trained model (trained on the source dataset) evaluated on the input of the target dataset.\n    2.  The conditional density of the target dataset’s label given the dummy label from the previous step. The predictive distribution is then evaluated through integrating over all possible dummy labels.\n\n1.  The probability of the dummy label based on the categorical distribution of the trained model (trained on the source dataset) evaluated on the input of the target dataset.\n2.  The conditional density of the target dataset’s label given the dummy label from the previous step. The predictive distribution is then evaluated through integrating over all possible dummy labels.",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 182,
        "contentLength": 1312
      },
      "nextCards": [
        "ai-transferability-estimation-optimal-transport-dataset-distance-otdd-2",
        "ai-transferability-estimation-leep-vs-otdd-3"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/transferability-estimation/#log-expected-empirical-prediction-(leep)",
      "scrapedAt": "2025-12-28T11:57:31.937Z",
      "siblings": [
        "ai-transferability-estimation-optimal-transport-dataset-distance-otdd-2",
        "ai-transferability-estimation-leep-vs-otdd-3"
      ]
    },
    {
      "id": "ai-transferability-estimation-optimal-transport-dataset-distance-otdd-2",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Transferability Estimation",
      "articleSlug": "transferability-estimation",
      "chapter": "Overview",
      "title": "Optimal Transport Dataset Distance (OTDD)",
      "subtitle": "Overview",
      "contentHtml": "<ul>\n  <li><a href=\"https://proceedings.neurips.cc/paper/2020/file/f52a7b2610fb4d3f74b4106fb80b233d-Paper.pdf\">OTDD</a> by Alvarez-Melis et al. from Microsoft Research in NeurIPS 2020 proposes to measure distances between datasets through optimal transport as an estimation for transferability. Ideally, smaller distance indicates better transferability.</li>\n</ul>",
      "contentMarkdown": "*   [OTDD](https://proceedings.neurips.cc/paper/2020/file/f52a7b2610fb4d3f74b4106fb80b233d-Paper.pdf) by Alvarez-Melis et al. from Microsoft Research in NeurIPS 2020 proposes to measure distances between datasets through optimal transport as an estimation for transferability. Ideally, smaller distance indicates better transferability.",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 32,
        "contentLength": 365
      },
      "nextCards": [
        "ai-transferability-estimation-leep-vs-otdd-3"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/transferability-estimation/#optimal-transport-dataset-distance-(otdd)",
      "scrapedAt": "2025-12-28T11:57:31.937Z",
      "siblings": [
        "ai-transferability-estimation-log-expected-empirical-prediction-leep-1",
        "ai-transferability-estimation-leep-vs-otdd-3"
      ]
    },
    {
      "id": "ai-transferability-estimation-leep-vs-otdd-3",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Transferability Estimation",
      "articleSlug": "transferability-estimation",
      "chapter": "Overview",
      "title": "LEEP vs. OTDD",
      "subtitle": "Overview",
      "contentHtml": "<ul>\n  <li>Compared to LEEP, OTDD does not require training a model on the source dataset. It only needs the feature-label pairs of the two datasets. Specifically, the distance measure is composed of two parts:\n    <ol>\n      <li>The distance between feature vectors of the two datasets.</li>\n      <li>The distance between the labels of the two datasets, where each label is represented by the distribution of the associated feature vectors.</li>\n    </ol>\n  </li>\n  <li>However, the drawback of the OTDD approach is obvious. Wasserstein distance is known to be computationally expensive. Therefore, OTDD needs to rely on approximation algorithms. Although the authors propose that it is possible to use Gaussian distribution as the modeling choice for the feature vector distribution under each label so that the 2-Wasserstein distance can be calculated through an analytic form, the approximation of this approach is too coarse. In comparison, the LEEP approach only involves one iteration of trained model inference on the target dataset to acquire the dummy label distribution.</li>\n  <li>In terms of experiments, both papers validated the statistical correlation between their proposed transferability estimation approaches and the model performance on the target dataset on several transfer learning tasks. Specifically, the LEEP approach witnessed larger than 0.94 correlation coefficients between the LEEP score and the test accuracy (closer to 1 correlation coefficient indicates better transferability measurement) when transferring from the ImageNet dataset to the CIFAR-100 dataset and from the CIFAR-10 dataset to the CIFAR-100 dataset. The OTDD approach witnessed -0.85 correlation between the dataset distance and the relative drop in test error (closer to -1 correlation coefficient indicates better distance measure) when transferring from the MNIST dataset (with augmentations) to the USPS dataset. However, when not performing augmentations, the correlation when transferring among the MNIST dataset, its variations and the USPS dataset is only -0.59 for OTDD.</li>\n  <li>Overall, neither of the two approaches require re-training a model on the target dataset.</li>\n  <li>The following illustration compares the major differences between OTDD and LEEP.</li>\n</ul>\n<ol>\n      <li>The distance between feature vectors of the two datasets.</li>\n      <li>The distance between the labels of the two datasets, where each label is represented by the distribution of the associated feature vectors.</li>\n    </ol>\n<p><img src=\"/primers/ai/assets/transferability-estimation/OTDD_LEEP_Visualization.png\" alt=\"\"></p>",
      "contentMarkdown": "*   Compared to LEEP, OTDD does not require training a model on the source dataset. It only needs the feature-label pairs of the two datasets. Specifically, the distance measure is composed of two parts:\n    1.  The distance between feature vectors of the two datasets.\n    2.  The distance between the labels of the two datasets, where each label is represented by the distribution of the associated feature vectors.\n*   However, the drawback of the OTDD approach is obvious. Wasserstein distance is known to be computationally expensive. Therefore, OTDD needs to rely on approximation algorithms. Although the authors propose that it is possible to use Gaussian distribution as the modeling choice for the feature vector distribution under each label so that the 2-Wasserstein distance can be calculated through an analytic form, the approximation of this approach is too coarse. In comparison, the LEEP approach only involves one iteration of trained model inference on the target dataset to acquire the dummy label distribution.\n*   In terms of experiments, both papers validated the statistical correlation between their proposed transferability estimation approaches and the model performance on the target dataset on several transfer learning tasks. Specifically, the LEEP approach witnessed larger than 0.94 correlation coefficients between the LEEP score and the test accuracy (closer to 1 correlation coefficient indicates better transferability measurement) when transferring from the ImageNet dataset to the CIFAR-100 dataset and from the CIFAR-10 dataset to the CIFAR-100 dataset. The OTDD approach witnessed -0.85 correlation between the dataset distance and the relative drop in test error (closer to -1 correlation coefficient indicates better distance measure) when transferring from the MNIST dataset (with augmentations) to the USPS dataset. However, when not performing augmentations, the correlation when transferring among the MNIST dataset, its variations and the USPS dataset is only -0.59 for OTDD.\n*   Overall, neither of the two approaches require re-training a model on the target dataset.\n*   The following illustration compares the major differences between OTDD and LEEP.\n\n1.  The distance between feature vectors of the two datasets.\n2.  The distance between the labels of the two datasets, where each label is represented by the distribution of the associated feature vectors.\n\n![](/primers/ai/assets/transferability-estimation/OTDD_LEEP_Visualization.png)",
      "order": 3,
      "orderInChapter": 3,
      "difficulty": 2,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "transfer learning"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 359,
        "contentLength": 2627
      },
      "nextCards": [],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/transferability-estimation/#leep-vs.-otdd",
      "scrapedAt": "2025-12-28T11:57:31.937Z",
      "siblings": [
        "ai-transferability-estimation-log-expected-empirical-prediction-leep-1",
        "ai-transferability-estimation-optimal-transport-dataset-distance-otdd-2"
      ]
    },
    {
      "id": "ai-tensorboard-running-tensorboard-locally-1",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "TensorBoard",
      "articleSlug": "tensorboard",
      "chapter": "TensorBoard",
      "title": "Running TensorBoard Locally",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>We’ve also created a few Tensorflow 2/Keras examples that you can run on your local machine. These examples demonstrate how to show how to display loss curves, images, and figures like confusion matrices for the MNIST classification task on your local machine.</li>\n</ul>",
      "contentMarkdown": "*   We’ve also created a few Tensorflow 2/Keras examples that you can run on your local machine. These examples demonstrate how to show how to display loss curves, images, and figures like confusion matrices for the MNIST classification task on your local machine.",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 43,
        "contentLength": 282
      },
      "nextCards": [
        "ai-tensorboard-setup-2",
        "ai-tensorboard-plotting-losses-accuracies-and-weight-distribution-3"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/tensorboard/#running-tensorboard-locally",
      "scrapedAt": "2025-12-28T11:57:36.923Z",
      "siblings": [
        "ai-tensorboard-setup-2",
        "ai-tensorboard-plotting-losses-accuracies-and-weight-distribution-3",
        "ai-tensorboard-logging-images-4",
        "ai-tensorboard-custom-logging-callbacks-5"
      ]
    },
    {
      "id": "ai-tensorboard-setup-2",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "TensorBoard",
      "articleSlug": "tensorboard",
      "chapter": "TensorBoard",
      "title": "Setup",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>You’ll need a couple of python packages to get started with these examples. Run these commands to create a virtual environment for this tutorial and start a TensorBoard server:</li>\n</ul>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\">virtualenv <span class=\"nt\">-p</span> python3 .venv\n<span class=\"nb\">source</span> .venv/bin/activate\npip <span class=\"nb\">install </span>numpy matplotlib tensorflow tensorboard scikit-learn\ntensorboard <span class=\"nt\">--logdir</span> logs &amp;\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code0\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code0\">virtualenv <span class=\"nt\">-p</span> python3 .venv\n<span class=\"nb\">source</span> .venv/bin/activate\npip <span class=\"nb\">install </span>numpy matplotlib tensorflow tensorboard scikit-learn\ntensorboard <span class=\"nt\">--logdir</span> logs &amp;\n</code></pre>\n<ul>\n  <li>Running <code class=\"language-plaintext highlighter-rouge\">tensorboard --logdir logs &amp;</code> will create a directory called <code class=\"language-plaintext highlighter-rouge\">logs</code> where TensorBoard will store the metrics from your training runs and start a TensorBoard server as a background process.</li>\n  <li>Open the TensorBoard dashboard by going to <code class=\"language-plaintext highlighter-rouge\">localhost:6006</code> in your browser (or whichever port number your server is running on).</li>\n</ul>",
      "contentMarkdown": "*   You’ll need a couple of python packages to get started with these examples. Run these commands to create a virtual environment for this tutorial and start a TensorBoard server:\n\n![](https://aman.ai/images/copy.png)\n\n`virtualenv -p python3 .venv source .venv/bin/activate pip install numpy matplotlib tensorflow tensorboard scikit-learn tensorboard --logdir logs &`\n\n![](https://aman.ai/images/copy.png)\n\n`virtualenv -p python3 .venv source .venv/bin/activate pip install numpy matplotlib tensorflow tensorboard scikit-learn tensorboard --logdir logs &`\n\n*   Running `tensorboard --logdir logs &` will create a directory called `logs` where TensorBoard will store the metrics from your training runs and start a TensorBoard server as a background process.\n*   Open the TensorBoard dashboard by going to `localhost:6006` in your browser (or whichever port number your server is running on).",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 118,
        "contentLength": 1878
      },
      "nextCards": [
        "ai-tensorboard-plotting-losses-accuracies-and-weight-distribution-3",
        "ai-tensorboard-logging-images-4"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/tensorboard/#setup",
      "scrapedAt": "2025-12-28T11:57:36.923Z",
      "siblings": [
        "ai-tensorboard-running-tensorboard-locally-1",
        "ai-tensorboard-plotting-losses-accuracies-and-weight-distribution-3",
        "ai-tensorboard-logging-images-4",
        "ai-tensorboard-custom-logging-callbacks-5"
      ]
    },
    {
      "id": "ai-tensorboard-plotting-losses-accuracies-and-weight-distribution-3",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "TensorBoard",
      "articleSlug": "tensorboard",
      "chapter": "TensorBoard",
      "title": "Plotting Losses, Accuracies, and Weight Distributions",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>For this first example we’ll plot train and val loss and accuracy curves in addition to histograms of the weights of our network as it trains. To do this, we import the TensorBoard callback,\nconfigure where it will store the training logs (we name this directory as a formatted string representing the current time), and pass this callback to <code class=\"language-plaintext highlighter-rouge\">model.fit()</code>:</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\"><span class=\"kn\">import</span> <span class=\"nn\">datetime</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">mnist</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.models</span> <span class=\"kn\">import</span> <span class=\"n\">Sequential</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Flatten</span><span class=\"p\">,</span> <span class=\"n\">Dense</span><span class=\"p\">,</span> <span class=\"n\">Dropout</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.callbacks</span> <span class=\"kn\">import</span> <span class=\"n\">TensorBoard</span>\n\n<span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">mnist</span><span class=\"p\">.</span><span class=\"n\">load_data</span><span class=\"p\">()</span>\n<span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">x_test</span> <span class=\"o\">=</span> <span class=\"n\">x_train</span> <span class=\"o\">/</span> <span class=\"mf\">255.0</span><span class=\"p\">,</span> <span class=\"n\">x_test</span> <span class=\"o\">/</span> <span class=\"mf\">255.0</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">([</span>\n    <span class=\"n\">Flatten</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">)),</span>\n    <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">),</span>\n    <span class=\"n\">Dropout</span><span class=\"p\">(</span><span class=\"mf\">0.2</span><span class=\"p\">),</span>\n    <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'softmax'</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s\">'adam'</span><span class=\"p\">,</span>\n              <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s\">'sparse_categorical_crossentropy'</span><span class=\"p\">,</span>\n              <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'accuracy'</span><span class=\"p\">])</span>\n\n<span class=\"n\">time</span> <span class=\"o\">=</span> <span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">now</span><span class=\"p\">().</span><span class=\"n\">strftime</span><span class=\"p\">(</span><span class=\"s\">\"%Y%m%d-%H%M%S\"</span><span class=\"p\">)</span>\n<span class=\"n\">log_dir</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s\">\"logs/</span><span class=\"si\">{</span><span class=\"n\">time</span><span class=\"si\">}</span><span class=\"s\">\"</span>\n<span class=\"n\">tensorboard</span> <span class=\"o\">=</span> <span class=\"n\">TensorBoard</span><span class=\"p\">(</span><span class=\"n\">log_dir</span><span class=\"o\">=</span><span class=\"n\">log_dir</span><span class=\"p\">,</span> <span class=\"n\">histogram_freq</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"c1\"># added\n</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span>\n    <span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">x_train</span><span class=\"p\">,</span>\n    <span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">y_train</span><span class=\"p\">,</span>\n    <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n    <span class=\"n\">validation_data</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">),</span>\n    <span class=\"n\">callbacks</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">tensorboard</span><span class=\"p\">]</span>\n    <span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code1\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code1\"><span class=\"kn\">import</span> <span class=\"nn\">datetime</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">mnist</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.models</span> <span class=\"kn\">import</span> <span class=\"n\">Sequential</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Flatten</span><span class=\"p\">,</span> <span class=\"n\">Dense</span><span class=\"p\">,</span> <span class=\"n\">Dropout</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.callbacks</span> <span class=\"kn\">import</span> <span class=\"n\">TensorBoard</span>\n\n<span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">mnist</span><span class=\"p\">.</span><span class=\"n\">load_data</span><span class=\"p\">()</span>\n<span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">x_test</span> <span class=\"o\">=</span> <span class=\"n\">x_train</span> <span class=\"o\">/</span> <span class=\"mf\">255.0</span><span class=\"p\">,</span> <span class=\"n\">x_test</span> <span class=\"o\">/</span> <span class=\"mf\">255.0</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">([</span>\n    <span class=\"n\">Flatten</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">)),</span>\n    <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">),</span>\n    <span class=\"n\">Dropout</span><span class=\"p\">(</span><span class=\"mf\">0.2</span><span class=\"p\">),</span>\n    <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'softmax'</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s\">'adam'</span><span class=\"p\">,</span>\n              <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s\">'sparse_categorical_crossentropy'</span><span class=\"p\">,</span>\n              <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'accuracy'</span><span class=\"p\">])</span>\n\n<span class=\"n\">time</span> <span class=\"o\">=</span> <span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">now</span><span class=\"p\">().</span><span class=\"n\">strftime</span><span class=\"p\">(</span><span class=\"s\">\"%Y%m%d-%H%M%S\"</span><span class=\"p\">)</span>\n<span class=\"n\">log_dir</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s\">\"logs/</span><span class=\"si\">{</span><span class=\"n\">time</span><span class=\"si\">}</span><span class=\"s\">\"</span>\n<span class=\"n\">tensorboard</span> <span class=\"o\">=</span> <span class=\"n\">TensorBoard</span><span class=\"p\">(</span><span class=\"n\">log_dir</span><span class=\"o\">=</span><span class=\"n\">log_dir</span><span class=\"p\">,</span> <span class=\"n\">histogram_freq</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"c1\"># added\n</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span>\n    <span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">x_train</span><span class=\"p\">,</span>\n    <span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">y_train</span><span class=\"p\">,</span>\n    <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n    <span class=\"n\">validation_data</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">),</span>\n    <span class=\"n\">callbacks</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">tensorboard</span><span class=\"p\">]</span>\n    <span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li>We can see plots of the train and val losses and accuracies by navigating to the ‘Scalars’ tab of the TensorBoard dashboard at <code class=\"language-plaintext highlighter-rouge\">localhost:6000</code> in a browser.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/hyperparam-tuning-and-tensorboard/tboard.png\" alt=\"\"></p>\n<ul>\n  <li>\n    <p>By setting <code class=\"language-plaintext highlighter-rouge\">histogram_freq=1</code> in the TensorBoard callback constructor, we can track the distributions of the weights in each layer at each epoch.</p>\n  </li>\n  <li>\n    <p>The ‘Histograms’ tab shows histograms of the weights in each layer at each epoch.</p>\n  </li>\n</ul>\n<p>By setting <code class=\"language-plaintext highlighter-rouge\">histogram_freq=1</code> in the TensorBoard callback constructor, we can track the distributions of the weights in each layer at each epoch.</p>\n<p>The ‘Histograms’ tab shows histograms of the weights in each layer at each epoch.</p>\n<p><img src=\"/primers/ai/assets/hyperparam-tuning-and-tensorboard/hist.png\" alt=\"\"></p>",
      "contentMarkdown": "*   For this first example we’ll plot train and val loss and accuracy curves in addition to histograms of the weights of our network as it trains. To do this, we import the TensorBoard callback, configure where it will store the training logs (we name this directory as a formatted string representing the current time), and pass this callback to `model.fit()`:\n\n![](https://aman.ai/images/copy.png)\n\n`import datetime import tensorflow as tf from tensorflow.keras.datasets import mnist from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Flatten, Dense, Dropout from tensorflow.keras.callbacks import TensorBoard  (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0  model = Sequential([     Flatten(input_shape=(28, 28)),     Dense(512, activation='relu'),     Dropout(0.2),     Dense(10, activation='softmax') ]) model.compile(optimizer='adam',               loss='sparse_categorical_crossentropy',               metrics=['accuracy'])  time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") log_dir = f\"logs/{time}\" tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1) # added model.fit(     x=x_train,     y=y_train,     epochs=5,     validation_data=(x_test, y_test),     callbacks=[tensorboard]     )`\n\n![](https://aman.ai/images/copy.png)\n\n`import datetime import tensorflow as tf from tensorflow.keras.datasets import mnist from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Flatten, Dense, Dropout from tensorflow.keras.callbacks import TensorBoard  (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0  model = Sequential([     Flatten(input_shape=(28, 28)),     Dense(512, activation='relu'),     Dropout(0.2),     Dense(10, activation='softmax') ]) model.compile(optimizer='adam',               loss='sparse_categorical_crossentropy',               metrics=['accuracy'])  time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") log_dir = f\"logs/{time}\" tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1) # added model.fit(     x=x_train,     y=y_train,     epochs=5,     validation_data=(x_test, y_test),     callbacks=[tensorboard]     )`\n\n*   We can see plots of the train and val losses and accuracies by navigating to the ‘Scalars’ tab of the TensorBoard dashboard at `localhost:6000` in a browser.\n\n![](/primers/ai/assets/hyperparam-tuning-and-tensorboard/tboard.png)\n\n*   By setting `histogram_freq=1` in the TensorBoard callback constructor, we can track the distributions of the weights in each layer at each epoch.\n    \n*   The ‘Histograms’ tab shows histograms of the weights in each layer at each epoch.\n    \n\nBy setting `histogram_freq=1` in the TensorBoard callback constructor, we can track the distributions of the weights in each layer at each epoch.\n\nThe ‘Histograms’ tab shows histograms of the weights in each layer at each epoch.\n\n![](/primers/ai/assets/hyperparam-tuning-and-tensorboard/hist.png)",
      "order": 3,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "activation",
        "dropout"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 313,
        "contentLength": 11602
      },
      "nextCards": [
        "ai-tensorboard-logging-images-4",
        "ai-tensorboard-custom-logging-callbacks-5"
      ],
      "relatedCards": [
        "ai-gpu-architecture-forward-and-backward-passes-training-17",
        "ai-gpu-architecture-precision-optimization-19",
        "ai-model-debugging-grad-cam-15",
        "ai-model-debugging-gradient-checking-10",
        "ai-model-debugging-regularization-14"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/tensorboard/#plotting-losses,-accuracies,-and-weight-distributions",
      "scrapedAt": "2025-12-28T11:57:36.923Z",
      "siblings": [
        "ai-tensorboard-running-tensorboard-locally-1",
        "ai-tensorboard-setup-2",
        "ai-tensorboard-logging-images-4",
        "ai-tensorboard-custom-logging-callbacks-5"
      ]
    },
    {
      "id": "ai-tensorboard-logging-images-4",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "TensorBoard",
      "articleSlug": "tensorboard",
      "chapter": "TensorBoard",
      "title": "Logging Images",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>This next example shows how to use <code class=\"language-plaintext highlighter-rouge\">tf.summary</code> to log image data for visualization. Here we simply log the first five examples in the training set so that we can inspect them in the ‘Images’ tab of the TensorBoard dashboard.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\"><span class=\"kn\">import</span> <span class=\"nn\">datetime</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">mnist</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.models</span> <span class=\"kn\">import</span> <span class=\"n\">Sequential</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Flatten</span><span class=\"p\">,</span> <span class=\"n\">Dense</span><span class=\"p\">,</span> <span class=\"n\">Dropout</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.callbacks</span> <span class=\"kn\">import</span> <span class=\"n\">TensorBoard</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span> <span class=\"c1\"># added\n</span><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span> <span class=\"c1\"># added\n</span><span class=\"kn\">import</span> <span class=\"nn\">sklearn.metrics</span> <span class=\"c1\"># added\n</span>\n<span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">mnist</span><span class=\"p\">.</span><span class=\"n\">load_data</span><span class=\"p\">()</span>\n<span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">x_test</span> <span class=\"o\">=</span> <span class=\"n\">x_train</span> <span class=\"o\">/</span> <span class=\"mf\">255.0</span><span class=\"p\">,</span> <span class=\"n\">x_test</span> <span class=\"o\">/</span> <span class=\"mf\">255.0</span>\n\n<span class=\"n\">time</span> <span class=\"o\">=</span> <span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">now</span><span class=\"p\">().</span><span class=\"n\">strftime</span><span class=\"p\">(</span><span class=\"s\">\"%Y%m%d-%H%M%S\"</span><span class=\"p\">)</span>\n<span class=\"n\">log_dir</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s\">\"logs/</span><span class=\"si\">{</span><span class=\"n\">time</span><span class=\"si\">}</span><span class=\"s\">\"</span>\n<span class=\"n\">tensorboard</span> <span class=\"o\">=</span> <span class=\"n\">TensorBoard</span><span class=\"p\">(</span><span class=\"n\">log_dir</span><span class=\"o\">=</span><span class=\"n\">log_dir</span><span class=\"p\">,</span> <span class=\"n\">histogram_freq</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Visualize image, added\n</span><span class=\"n\">images</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"mi\">5</span><span class=\"p\">],</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span> <span class=\"c1\"># batch_size first\n</span><span class=\"n\">file_writer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">summary</span><span class=\"p\">.</span><span class=\"n\">create_file_writer</span><span class=\"p\">(</span><span class=\"n\">log_dir</span><span class=\"p\">)</span>\n<span class=\"k\">with</span> <span class=\"n\">file_writer</span><span class=\"p\">.</span><span class=\"n\">as_default</span><span class=\"p\">():</span>\n\t<span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">summary</span><span class=\"p\">.</span><span class=\"n\">image</span><span class=\"p\">(</span><span class=\"s\">\"Training Data\"</span><span class=\"p\">,</span> <span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">max_outputs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">step</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">([</span>\n    <span class=\"n\">Flatten</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">)),</span>\n    <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">),</span>\n    <span class=\"n\">Dropout</span><span class=\"p\">(</span><span class=\"mf\">0.2</span><span class=\"p\">),</span>\n    <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'softmax'</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s\">'adam'</span><span class=\"p\">,</span>\n              <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s\">'sparse_categorical_crossentropy'</span><span class=\"p\">,</span>\n              <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'accuracy'</span><span class=\"p\">])</span>\n\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span>\n    <span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">x_train</span><span class=\"p\">,</span> \n    <span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">y_train</span><span class=\"p\">,</span> \n    <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> \n    <span class=\"n\">validation_data</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">),</span> \n    <span class=\"n\">callbacks</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">tensorboard</span><span class=\"p\">]</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code2\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code2\"><span class=\"kn\">import</span> <span class=\"nn\">datetime</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">mnist</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.models</span> <span class=\"kn\">import</span> <span class=\"n\">Sequential</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Flatten</span><span class=\"p\">,</span> <span class=\"n\">Dense</span><span class=\"p\">,</span> <span class=\"n\">Dropout</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.callbacks</span> <span class=\"kn\">import</span> <span class=\"n\">TensorBoard</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span> <span class=\"c1\"># added\n</span><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span> <span class=\"c1\"># added\n</span><span class=\"kn\">import</span> <span class=\"nn\">sklearn.metrics</span> <span class=\"c1\"># added\n</span>\n<span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">mnist</span><span class=\"p\">.</span><span class=\"n\">load_data</span><span class=\"p\">()</span>\n<span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">x_test</span> <span class=\"o\">=</span> <span class=\"n\">x_train</span> <span class=\"o\">/</span> <span class=\"mf\">255.0</span><span class=\"p\">,</span> <span class=\"n\">x_test</span> <span class=\"o\">/</span> <span class=\"mf\">255.0</span>\n\n<span class=\"n\">time</span> <span class=\"o\">=</span> <span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">now</span><span class=\"p\">().</span><span class=\"n\">strftime</span><span class=\"p\">(</span><span class=\"s\">\"%Y%m%d-%H%M%S\"</span><span class=\"p\">)</span>\n<span class=\"n\">log_dir</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s\">\"logs/</span><span class=\"si\">{</span><span class=\"n\">time</span><span class=\"si\">}</span><span class=\"s\">\"</span>\n<span class=\"n\">tensorboard</span> <span class=\"o\">=</span> <span class=\"n\">TensorBoard</span><span class=\"p\">(</span><span class=\"n\">log_dir</span><span class=\"o\">=</span><span class=\"n\">log_dir</span><span class=\"p\">,</span> <span class=\"n\">histogram_freq</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Visualize image, added\n</span><span class=\"n\">images</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"mi\">5</span><span class=\"p\">],</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span> <span class=\"c1\"># batch_size first\n</span><span class=\"n\">file_writer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">summary</span><span class=\"p\">.</span><span class=\"n\">create_file_writer</span><span class=\"p\">(</span><span class=\"n\">log_dir</span><span class=\"p\">)</span>\n<span class=\"k\">with</span> <span class=\"n\">file_writer</span><span class=\"p\">.</span><span class=\"n\">as_default</span><span class=\"p\">():</span>\n\t<span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">summary</span><span class=\"p\">.</span><span class=\"n\">image</span><span class=\"p\">(</span><span class=\"s\">\"Training Data\"</span><span class=\"p\">,</span> <span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">max_outputs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"n\">step</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">([</span>\n    <span class=\"n\">Flatten</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">)),</span>\n    <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">),</span>\n    <span class=\"n\">Dropout</span><span class=\"p\">(</span><span class=\"mf\">0.2</span><span class=\"p\">),</span>\n    <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'softmax'</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s\">'adam'</span><span class=\"p\">,</span>\n              <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s\">'sparse_categorical_crossentropy'</span><span class=\"p\">,</span>\n              <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'accuracy'</span><span class=\"p\">])</span>\n\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span>\n    <span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">x_train</span><span class=\"p\">,</span> \n    <span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">y_train</span><span class=\"p\">,</span> \n    <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> \n    <span class=\"n\">validation_data</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">),</span> \n    <span class=\"n\">callbacks</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">tensorboard</span><span class=\"p\">]</span>\n<span class=\"p\">)</span>\n</code></pre>\n<p><img src=\"/primers/ai/assets/hyperparam-tuning-and-tensorboard/data.png\" alt=\"\"></p>",
      "contentMarkdown": "*   This next example shows how to use `tf.summary` to log image data for visualization. Here we simply log the first five examples in the training set so that we can inspect them in the ‘Images’ tab of the TensorBoard dashboard.\n\n![](https://aman.ai/images/copy.png)\n\n`import datetime import tensorflow as tf from tensorflow.keras.datasets import mnist from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Flatten, Dense, Dropout from tensorflow.keras.callbacks import TensorBoard  import matplotlib.pyplot as plt # added import numpy as np # added import sklearn.metrics # added (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0  time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") log_dir = f\"logs/{time}\" tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)  # Visualize image, added images = np.reshape(x_train[0:5], (-1, 28, 28, 1)) # batch_size first file_writer = tf.summary.create_file_writer(log_dir) with file_writer.as_default(): \ttf.summary.image(\"Training Data\", images, max_outputs=5, step=0)  model = Sequential([     Flatten(input_shape=(28, 28)),     Dense(512, activation='relu'),     Dropout(0.2),     Dense(10, activation='softmax') ]) model.compile(optimizer='adam',               loss='sparse_categorical_crossentropy',               metrics=['accuracy'])  model.fit(     x=x_train,      y=y_train,      epochs=5,      validation_data=(x_test, y_test),      callbacks=[tensorboard] )`\n\n![](https://aman.ai/images/copy.png)\n\n`import datetime import tensorflow as tf from tensorflow.keras.datasets import mnist from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Flatten, Dense, Dropout from tensorflow.keras.callbacks import TensorBoard  import matplotlib.pyplot as plt # added import numpy as np # added import sklearn.metrics # added (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0  time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") log_dir = f\"logs/{time}\" tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)  # Visualize image, added images = np.reshape(x_train[0:5], (-1, 28, 28, 1)) # batch_size first file_writer = tf.summary.create_file_writer(log_dir) with file_writer.as_default(): \ttf.summary.image(\"Training Data\", images, max_outputs=5, step=0)  model = Sequential([     Flatten(input_shape=(28, 28)),     Dense(512, activation='relu'),     Dropout(0.2),     Dense(10, activation='softmax') ]) model.compile(optimizer='adam',               loss='sparse_categorical_crossentropy',               metrics=['accuracy'])  model.fit(     x=x_train,      y=y_train,      epochs=5,      validation_data=(x_test, y_test),      callbacks=[tensorboard] )`\n\n![](/primers/ai/assets/hyperparam-tuning-and-tensorboard/data.png)",
      "order": 4,
      "orderInChapter": 4,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "activation",
        "dropout"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 266,
        "contentLength": 14383
      },
      "nextCards": [
        "ai-tensorboard-custom-logging-callbacks-5"
      ],
      "relatedCards": [
        "ai-gpu-architecture-forward-and-backward-passes-training-17",
        "ai-gpu-architecture-precision-optimization-19",
        "ai-model-debugging-grad-cam-15",
        "ai-model-debugging-gradient-checking-10",
        "ai-model-debugging-regularization-14"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/tensorboard/#logging-images",
      "scrapedAt": "2025-12-28T11:57:36.923Z",
      "siblings": [
        "ai-tensorboard-running-tensorboard-locally-1",
        "ai-tensorboard-setup-2",
        "ai-tensorboard-plotting-losses-accuracies-and-weight-distribution-3",
        "ai-tensorboard-custom-logging-callbacks-5"
      ]
    },
    {
      "id": "ai-tensorboard-custom-logging-callbacks-5",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "TensorBoard",
      "articleSlug": "tensorboard",
      "chapter": "TensorBoard",
      "title": "Custom Logging Callbacks",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>The previous example showed how to log images from the training set at the beginning of training, but it would be more helpful to be able to log images and figures\ncontinusouly over the course of training.</li>\n  <li>For example, if we were training a GAN, monitoring the loss curves of the generator and discriminator over time wouldn’t give much insight into the performance of the model, and it would be much more illuminating to be able to view samples generated over time to see if sample quality is improving.</li>\n  <li>Likewise, for a classification task like MNIST digit classification, being able to see the evolution of confusion matrices or ROC curves over time can give a better sense of model performance than a single number like accuracy. Here we show how to plot confusion matrices at the end of each epoch.</li>\n  <li>We can define custom logging behavior that executes at fixed intervals during training using a <code class=\"language-plaintext highlighter-rouge\">LambdaCallback</code>.</li>\n  <li>In the code below, we define a function <code class=\"language-plaintext highlighter-rouge\">log_confusion_matrix</code> that generates the model’s predictions on the val set and creates a confusion matrix image using the <code class=\"language-plaintext highlighter-rouge\">sklearn.metrics.confusion_matrix()</code> function, and create a callback that plots the confusion matrix at the end of every epoch with <code class=\"language-plaintext highlighter-rouge\">cm_callback = LambdaCallback(on_epoch_end=log_confusion_matrix)</code>.</li>\n</ul>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\"><span class=\"kn\">import</span> <span class=\"nn\">datetime</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">mnist</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.models</span> <span class=\"kn\">import</span> <span class=\"n\">Sequential</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Flatten</span><span class=\"p\">,</span> <span class=\"n\">Dense</span><span class=\"p\">,</span> <span class=\"n\">Dropout</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.callbacks</span> <span class=\"kn\">import</span> <span class=\"n\">TensorBoard</span><span class=\"p\">,</span> <span class=\"n\">LambdaCallback</span> <span class=\"c1\"># added\n</span>\n<span class=\"kn\">import</span> <span class=\"nn\">io</span> <span class=\"c1\"># added\n</span><span class=\"kn\">import</span> <span class=\"nn\">itertools</span> <span class=\"c1\"># added\n</span><span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">sklearn.metrics</span>\n\n<span class=\"c1\"># Added\n</span><span class=\"k\">def</span> <span class=\"nf\">plot_to_image</span><span class=\"p\">(</span><span class=\"n\">fig</span><span class=\"p\">):</span>\n\t<span class=\"n\">buf</span> <span class=\"o\">=</span> <span class=\"n\">io</span><span class=\"p\">.</span><span class=\"n\">BytesIO</span><span class=\"p\">()</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">savefig</span><span class=\"p\">(</span><span class=\"n\">buf</span><span class=\"p\">,</span> <span class=\"nb\">format</span><span class=\"o\">=</span><span class=\"s\">'png'</span><span class=\"p\">)</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">close</span><span class=\"p\">(</span><span class=\"n\">fig</span><span class=\"p\">)</span>\n\t<span class=\"n\">buf</span><span class=\"p\">.</span><span class=\"n\">seek</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\t<span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">image</span><span class=\"p\">.</span><span class=\"n\">decode_png</span><span class=\"p\">(</span><span class=\"n\">buf</span><span class=\"p\">.</span><span class=\"n\">getvalue</span><span class=\"p\">(),</span> <span class=\"n\">channels</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n\t<span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">expand_dims</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n\t<span class=\"k\">return</span> <span class=\"n\">img</span>\n\n<span class=\"c1\"># Added\n</span><span class=\"k\">def</span> <span class=\"nf\">plot_confusion_matrix</span><span class=\"p\">(</span><span class=\"n\">cm</span><span class=\"p\">,</span> <span class=\"n\">class_names</span><span class=\"p\">):</span>\n\t<span class=\"n\">figure</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">))</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">imshow</span><span class=\"p\">(</span><span class=\"n\">cm</span><span class=\"p\">,</span> <span class=\"n\">interpolation</span><span class=\"o\">=</span><span class=\"s\">'nearest'</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">cm</span><span class=\"p\">.</span><span class=\"n\">Blues</span><span class=\"p\">)</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s\">\"Confusion matrix\"</span><span class=\"p\">)</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">colorbar</span><span class=\"p\">()</span>\n\t<span class=\"n\">tick_marks</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">class_names</span><span class=\"p\">))</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">xticks</span><span class=\"p\">(</span><span class=\"n\">tick_marks</span><span class=\"p\">,</span> <span class=\"n\">class_names</span><span class=\"p\">,</span> <span class=\"n\">rotation</span><span class=\"o\">=</span><span class=\"mi\">45</span><span class=\"p\">)</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">yticks</span><span class=\"p\">(</span><span class=\"n\">tick_marks</span><span class=\"p\">,</span> <span class=\"n\">class_names</span><span class=\"p\">)</span>\n\n\t<span class=\"n\">cm</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">around</span><span class=\"p\">(</span><span class=\"n\">cm</span><span class=\"p\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"s\">'float'</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">cm</span><span class=\"p\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)[:,</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">newaxis</span><span class=\"p\">],</span> <span class=\"n\">decimals</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n\t<span class=\"n\">threshold</span> <span class=\"o\">=</span> <span class=\"n\">cm</span><span class=\"p\">.</span><span class=\"nb\">max</span><span class=\"p\">()</span> <span class=\"o\">/</span> <span class=\"mf\">2.</span>\n\t<span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"n\">itertools</span><span class=\"p\">.</span><span class=\"n\">product</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">cm</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">cm</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])):</span>\n\t\t<span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s\">\"white\"</span> <span class=\"k\">if</span> <span class=\"n\">cm</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">&gt;</span> <span class=\"n\">threshold</span> <span class=\"k\">else</span> <span class=\"s\">\"black\"</span>\n\t\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">text</span><span class=\"p\">(</span><span class=\"n\">j</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">cm</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">j</span><span class=\"p\">],</span> <span class=\"n\">horizontalalignment</span><span class=\"o\">=</span><span class=\"s\">\"center\"</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"n\">color</span><span class=\"p\">)</span>\n\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">tight_layout</span><span class=\"p\">()</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s\">'True label'</span><span class=\"p\">)</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s\">'Predicted label'</span><span class=\"p\">)</span>\n\t<span class=\"k\">return</span> <span class=\"n\">figure</span>\n\n<span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">mnist</span><span class=\"p\">.</span><span class=\"n\">load_data</span><span class=\"p\">()</span>\n<span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">x_test</span> <span class=\"o\">=</span> <span class=\"n\">x_train</span> <span class=\"o\">/</span> <span class=\"mf\">255.0</span><span class=\"p\">,</span> <span class=\"n\">x_test</span> <span class=\"o\">/</span> <span class=\"mf\">255.0</span>\n\n<span class=\"n\">time</span> <span class=\"o\">=</span> <span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">now</span><span class=\"p\">().</span><span class=\"n\">strftime</span><span class=\"p\">(</span><span class=\"s\">\"%Y%m%d-%H%M%S\"</span><span class=\"p\">)</span>\n<span class=\"n\">log_dir</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s\">\"logs/</span><span class=\"si\">{</span><span class=\"n\">time</span><span class=\"si\">}</span><span class=\"s\">\"</span>\n<span class=\"n\">tensorboard</span> <span class=\"o\">=</span> <span class=\"n\">TensorBoard</span><span class=\"p\">(</span><span class=\"n\">log_dir</span><span class=\"o\">=</span><span class=\"n\">log_dir</span><span class=\"p\">,</span> <span class=\"n\">histogram_freq</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Added\n</span><span class=\"n\">file_writer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">summary</span><span class=\"p\">.</span><span class=\"n\">create_file_writer</span><span class=\"p\">(</span><span class=\"n\">log_dir</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">log_confusion_matrix</span><span class=\"p\">(</span><span class=\"n\">epoch</span><span class=\"p\">,</span> <span class=\"n\">logs</span><span class=\"p\">):</span>\n\t<span class=\"n\">test_pred_raw</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">)</span>\n\t<span class=\"n\">test_pred</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">test_pred_raw</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n\t<span class=\"n\">cm</span> <span class=\"o\">=</span> <span class=\"n\">sklearn</span><span class=\"p\">.</span><span class=\"n\">metrics</span><span class=\"p\">.</span><span class=\"n\">confusion_matrix</span><span class=\"p\">(</span><span class=\"n\">y_test</span><span class=\"p\">,</span> <span class=\"n\">test_pred</span><span class=\"p\">)</span>\n\t<span class=\"n\">figure</span> <span class=\"o\">=</span> <span class=\"n\">plot_confusion_matrix</span><span class=\"p\">(</span><span class=\"n\">cm</span><span class=\"p\">,</span> <span class=\"n\">class_names</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)))</span>\n\t<span class=\"n\">cm_image</span> <span class=\"o\">=</span> <span class=\"n\">plot_to_image</span><span class=\"p\">(</span><span class=\"n\">figure</span><span class=\"p\">)</span>\n\t<span class=\"k\">with</span> <span class=\"n\">file_writer</span><span class=\"p\">.</span><span class=\"n\">as_default</span><span class=\"p\">():</span>\n\t\t<span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">summary</span><span class=\"p\">.</span><span class=\"n\">image</span><span class=\"p\">(</span><span class=\"s\">\"Confusion Matrix\"</span><span class=\"p\">,</span> <span class=\"n\">cm_image</span><span class=\"p\">,</span> <span class=\"n\">step</span><span class=\"o\">=</span><span class=\"n\">epoch</span><span class=\"p\">)</span>\n<span class=\"n\">cm_callback</span> <span class=\"o\">=</span> <span class=\"n\">LambdaCallback</span><span class=\"p\">(</span><span class=\"n\">on_epoch_end</span><span class=\"o\">=</span><span class=\"n\">log_confusion_matrix</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">([</span>\n    <span class=\"n\">Flatten</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">)),</span>\n    <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">),</span>\n    <span class=\"n\">Dropout</span><span class=\"p\">(</span><span class=\"mf\">0.2</span><span class=\"p\">),</span>\n    <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'softmax'</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s\">'adam'</span><span class=\"p\">,</span>\n              <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s\">'sparse_categorical_crossentropy'</span><span class=\"p\">,</span>\n              <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'accuracy'</span><span class=\"p\">])</span>\n\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span>\n    <span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">x_train</span><span class=\"p\">,</span> \n    <span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">y_train</span><span class=\"p\">,</span> \n    <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> \n    <span class=\"n\">validation_data</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">),</span> \n    <span class=\"n\">callbacks</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">tensorboard</span><span class=\"p\">,</span> <span class=\"n\">cm_callback</span><span class=\"p\">]</span> <span class=\"c1\"># added\n</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<pre class=\"highlight\"><div><button class=\"btn-copy\" data-clipboard-action=\"copy\" data-clipboard-target=\"#code3\"><img src=\"https://aman.ai/images/copy.png\" style=\"margin:0; border:none; padding:2px 0px; width:100%; height:18px; width:18px;\"></button></div><code id=\"code3\"><span class=\"kn\">import</span> <span class=\"nn\">datetime</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">mnist</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.models</span> <span class=\"kn\">import</span> <span class=\"n\">Sequential</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Flatten</span><span class=\"p\">,</span> <span class=\"n\">Dense</span><span class=\"p\">,</span> <span class=\"n\">Dropout</span>\n<span class=\"kn\">from</span> <span class=\"nn\">tensorflow.keras.callbacks</span> <span class=\"kn\">import</span> <span class=\"n\">TensorBoard</span><span class=\"p\">,</span> <span class=\"n\">LambdaCallback</span> <span class=\"c1\"># added\n</span>\n<span class=\"kn\">import</span> <span class=\"nn\">io</span> <span class=\"c1\"># added\n</span><span class=\"kn\">import</span> <span class=\"nn\">itertools</span> <span class=\"c1\"># added\n</span><span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">sklearn.metrics</span>\n\n<span class=\"c1\"># Added\n</span><span class=\"k\">def</span> <span class=\"nf\">plot_to_image</span><span class=\"p\">(</span><span class=\"n\">fig</span><span class=\"p\">):</span>\n\t<span class=\"n\">buf</span> <span class=\"o\">=</span> <span class=\"n\">io</span><span class=\"p\">.</span><span class=\"n\">BytesIO</span><span class=\"p\">()</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">savefig</span><span class=\"p\">(</span><span class=\"n\">buf</span><span class=\"p\">,</span> <span class=\"nb\">format</span><span class=\"o\">=</span><span class=\"s\">'png'</span><span class=\"p\">)</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">close</span><span class=\"p\">(</span><span class=\"n\">fig</span><span class=\"p\">)</span>\n\t<span class=\"n\">buf</span><span class=\"p\">.</span><span class=\"n\">seek</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\t<span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">image</span><span class=\"p\">.</span><span class=\"n\">decode_png</span><span class=\"p\">(</span><span class=\"n\">buf</span><span class=\"p\">.</span><span class=\"n\">getvalue</span><span class=\"p\">(),</span> <span class=\"n\">channels</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n\t<span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">expand_dims</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n\t<span class=\"k\">return</span> <span class=\"n\">img</span>\n\n<span class=\"c1\"># Added\n</span><span class=\"k\">def</span> <span class=\"nf\">plot_confusion_matrix</span><span class=\"p\">(</span><span class=\"n\">cm</span><span class=\"p\">,</span> <span class=\"n\">class_names</span><span class=\"p\">):</span>\n\t<span class=\"n\">figure</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">))</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">imshow</span><span class=\"p\">(</span><span class=\"n\">cm</span><span class=\"p\">,</span> <span class=\"n\">interpolation</span><span class=\"o\">=</span><span class=\"s\">'nearest'</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">cm</span><span class=\"p\">.</span><span class=\"n\">Blues</span><span class=\"p\">)</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s\">\"Confusion matrix\"</span><span class=\"p\">)</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">colorbar</span><span class=\"p\">()</span>\n\t<span class=\"n\">tick_marks</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">class_names</span><span class=\"p\">))</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">xticks</span><span class=\"p\">(</span><span class=\"n\">tick_marks</span><span class=\"p\">,</span> <span class=\"n\">class_names</span><span class=\"p\">,</span> <span class=\"n\">rotation</span><span class=\"o\">=</span><span class=\"mi\">45</span><span class=\"p\">)</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">yticks</span><span class=\"p\">(</span><span class=\"n\">tick_marks</span><span class=\"p\">,</span> <span class=\"n\">class_names</span><span class=\"p\">)</span>\n\n\t<span class=\"n\">cm</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">around</span><span class=\"p\">(</span><span class=\"n\">cm</span><span class=\"p\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"s\">'float'</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">cm</span><span class=\"p\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)[:,</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">newaxis</span><span class=\"p\">],</span> <span class=\"n\">decimals</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n\t<span class=\"n\">threshold</span> <span class=\"o\">=</span> <span class=\"n\">cm</span><span class=\"p\">.</span><span class=\"nb\">max</span><span class=\"p\">()</span> <span class=\"o\">/</span> <span class=\"mf\">2.</span>\n\t<span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"n\">itertools</span><span class=\"p\">.</span><span class=\"n\">product</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">cm</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">cm</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])):</span>\n\t\t<span class=\"n\">color</span> <span class=\"o\">=</span> <span class=\"s\">\"white\"</span> <span class=\"k\">if</span> <span class=\"n\">cm</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">&gt;</span> <span class=\"n\">threshold</span> <span class=\"k\">else</span> <span class=\"s\">\"black\"</span>\n\t\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">text</span><span class=\"p\">(</span><span class=\"n\">j</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">cm</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">j</span><span class=\"p\">],</span> <span class=\"n\">horizontalalignment</span><span class=\"o\">=</span><span class=\"s\">\"center\"</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"n\">color</span><span class=\"p\">)</span>\n\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">tight_layout</span><span class=\"p\">()</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s\">'True label'</span><span class=\"p\">)</span>\n\t<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s\">'Predicted label'</span><span class=\"p\">)</span>\n\t<span class=\"k\">return</span> <span class=\"n\">figure</span>\n\n<span class=\"p\">(</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">mnist</span><span class=\"p\">.</span><span class=\"n\">load_data</span><span class=\"p\">()</span>\n<span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">x_test</span> <span class=\"o\">=</span> <span class=\"n\">x_train</span> <span class=\"o\">/</span> <span class=\"mf\">255.0</span><span class=\"p\">,</span> <span class=\"n\">x_test</span> <span class=\"o\">/</span> <span class=\"mf\">255.0</span>\n\n<span class=\"n\">time</span> <span class=\"o\">=</span> <span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">datetime</span><span class=\"p\">.</span><span class=\"n\">now</span><span class=\"p\">().</span><span class=\"n\">strftime</span><span class=\"p\">(</span><span class=\"s\">\"%Y%m%d-%H%M%S\"</span><span class=\"p\">)</span>\n<span class=\"n\">log_dir</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s\">\"logs/</span><span class=\"si\">{</span><span class=\"n\">time</span><span class=\"si\">}</span><span class=\"s\">\"</span>\n<span class=\"n\">tensorboard</span> <span class=\"o\">=</span> <span class=\"n\">TensorBoard</span><span class=\"p\">(</span><span class=\"n\">log_dir</span><span class=\"o\">=</span><span class=\"n\">log_dir</span><span class=\"p\">,</span> <span class=\"n\">histogram_freq</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Added\n</span><span class=\"n\">file_writer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">summary</span><span class=\"p\">.</span><span class=\"n\">create_file_writer</span><span class=\"p\">(</span><span class=\"n\">log_dir</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">log_confusion_matrix</span><span class=\"p\">(</span><span class=\"n\">epoch</span><span class=\"p\">,</span> <span class=\"n\">logs</span><span class=\"p\">):</span>\n\t<span class=\"n\">test_pred_raw</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">)</span>\n\t<span class=\"n\">test_pred</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">test_pred_raw</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n\t<span class=\"n\">cm</span> <span class=\"o\">=</span> <span class=\"n\">sklearn</span><span class=\"p\">.</span><span class=\"n\">metrics</span><span class=\"p\">.</span><span class=\"n\">confusion_matrix</span><span class=\"p\">(</span><span class=\"n\">y_test</span><span class=\"p\">,</span> <span class=\"n\">test_pred</span><span class=\"p\">)</span>\n\t<span class=\"n\">figure</span> <span class=\"o\">=</span> <span class=\"n\">plot_confusion_matrix</span><span class=\"p\">(</span><span class=\"n\">cm</span><span class=\"p\">,</span> <span class=\"n\">class_names</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">)))</span>\n\t<span class=\"n\">cm_image</span> <span class=\"o\">=</span> <span class=\"n\">plot_to_image</span><span class=\"p\">(</span><span class=\"n\">figure</span><span class=\"p\">)</span>\n\t<span class=\"k\">with</span> <span class=\"n\">file_writer</span><span class=\"p\">.</span><span class=\"n\">as_default</span><span class=\"p\">():</span>\n\t\t<span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">summary</span><span class=\"p\">.</span><span class=\"n\">image</span><span class=\"p\">(</span><span class=\"s\">\"Confusion Matrix\"</span><span class=\"p\">,</span> <span class=\"n\">cm_image</span><span class=\"p\">,</span> <span class=\"n\">step</span><span class=\"o\">=</span><span class=\"n\">epoch</span><span class=\"p\">)</span>\n<span class=\"n\">cm_callback</span> <span class=\"o\">=</span> <span class=\"n\">LambdaCallback</span><span class=\"p\">(</span><span class=\"n\">on_epoch_end</span><span class=\"o\">=</span><span class=\"n\">log_confusion_matrix</span><span class=\"p\">)</span>\n\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">([</span>\n    <span class=\"n\">Flatten</span><span class=\"p\">(</span><span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">28</span><span class=\"p\">,</span> <span class=\"mi\">28</span><span class=\"p\">)),</span>\n    <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">512</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'relu'</span><span class=\"p\">),</span>\n    <span class=\"n\">Dropout</span><span class=\"p\">(</span><span class=\"mf\">0.2</span><span class=\"p\">),</span>\n    <span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"s\">'softmax'</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nb\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"s\">'adam'</span><span class=\"p\">,</span>\n              <span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s\">'sparse_categorical_crossentropy'</span><span class=\"p\">,</span>\n              <span class=\"n\">metrics</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">'accuracy'</span><span class=\"p\">])</span>\n\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span>\n    <span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">x_train</span><span class=\"p\">,</span> \n    <span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">y_train</span><span class=\"p\">,</span> \n    <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span> \n    <span class=\"n\">validation_data</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">),</span> \n    <span class=\"n\">callbacks</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">tensorboard</span><span class=\"p\">,</span> <span class=\"n\">cm_callback</span><span class=\"p\">]</span> <span class=\"c1\"># added\n</span><span class=\"p\">)</span>\n</code></pre>\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">LambdaCallback</code>s allow us to view the history of confusion matrices summarizing our model’s performance on the val set over time:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/hyperparam-tuning-and-tensorboard/confusion.png\" alt=\"\"></p>",
      "contentMarkdown": "*   The previous example showed how to log images from the training set at the beginning of training, but it would be more helpful to be able to log images and figures continusouly over the course of training.\n*   For example, if we were training a GAN, monitoring the loss curves of the generator and discriminator over time wouldn’t give much insight into the performance of the model, and it would be much more illuminating to be able to view samples generated over time to see if sample quality is improving.\n*   Likewise, for a classification task like MNIST digit classification, being able to see the evolution of confusion matrices or ROC curves over time can give a better sense of model performance than a single number like accuracy. Here we show how to plot confusion matrices at the end of each epoch.\n*   We can define custom logging behavior that executes at fixed intervals during training using a `LambdaCallback`.\n*   In the code below, we define a function `log_confusion_matrix` that generates the model’s predictions on the val set and creates a confusion matrix image using the `sklearn.metrics.confusion_matrix()` function, and create a callback that plots the confusion matrix at the end of every epoch with `cm_callback = LambdaCallback(on_epoch_end=log_confusion_matrix)`.\n\n![](https://aman.ai/images/copy.png)\n\n`import datetime import tensorflow as tf from tensorflow.keras.datasets import mnist from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Flatten, Dense, Dropout from tensorflow.keras.callbacks import TensorBoard, LambdaCallback # added import io # added import itertools # added import matplotlib.pyplot as plt import numpy as np import sklearn.metrics  # Added def plot_to_image(fig): \tbuf = io.BytesIO() \tplt.savefig(buf, format='png') \tplt.close(fig) \tbuf.seek(0) \timg = tf.image.decode_png(buf.getvalue(), channels=4) \timg = tf.expand_dims(img, 0) \treturn img  # Added def plot_confusion_matrix(cm, class_names): \tfigure = plt.figure(figsize=(8, 8)) \tplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues) \tplt.title(\"Confusion matrix\") \tplt.colorbar() \ttick_marks = np.arange(len(class_names)) \tplt.xticks(tick_marks, class_names, rotation=45) \tplt.yticks(tick_marks, class_names)  \tcm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)  \tthreshold = cm.max() / 2. \tfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): \t\tcolor = \"white\" if cm[i, j] > threshold else \"black\" \t\tplt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)  \tplt.tight_layout() \tplt.ylabel('True label') \tplt.xlabel('Predicted label') \treturn figure  (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0  time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") log_dir = f\"logs/{time}\" tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)  # Added file_writer = tf.summary.create_file_writer(log_dir) def log_confusion_matrix(epoch, logs): \ttest_pred_raw = model.predict(x_test) \ttest_pred = np.argmax(test_pred_raw, axis=1)  \tcm = sklearn.metrics.confusion_matrix(y_test, test_pred) \tfigure = plot_confusion_matrix(cm, class_names=list(range(10))) \tcm_image = plot_to_image(figure) \twith file_writer.as_default(): \t\ttf.summary.image(\"Confusion Matrix\", cm_image, step=epoch) cm_callback = LambdaCallback(on_epoch_end=log_confusion_matrix)  model = Sequential([     Flatten(input_shape=(28, 28)),     Dense(512, activation='relu'),     Dropout(0.2),     Dense(10, activation='softmax') ]) model.compile(optimizer='adam',               loss='sparse_categorical_crossentropy',               metrics=['accuracy'])  model.fit(     x=x_train,      y=y_train,      epochs=5,      validation_data=(x_test, y_test),      callbacks=[tensorboard, cm_callback] # added )`\n\n![](https://aman.ai/images/copy.png)\n\n`import datetime import tensorflow as tf from tensorflow.keras.datasets import mnist from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Flatten, Dense, Dropout from tensorflow.keras.callbacks import TensorBoard, LambdaCallback # added import io # added import itertools # added import matplotlib.pyplot as plt import numpy as np import sklearn.metrics  # Added def plot_to_image(fig): \tbuf = io.BytesIO() \tplt.savefig(buf, format='png') \tplt.close(fig) \tbuf.seek(0) \timg = tf.image.decode_png(buf.getvalue(), channels=4) \timg = tf.expand_dims(img, 0) \treturn img  # Added def plot_confusion_matrix(cm, class_names): \tfigure = plt.figure(figsize=(8, 8)) \tplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues) \tplt.title(\"Confusion matrix\") \tplt.colorbar() \ttick_marks = np.arange(len(class_names)) \tplt.xticks(tick_marks, class_names, rotation=45) \tplt.yticks(tick_marks, class_names)  \tcm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)  \tthreshold = cm.max() / 2. \tfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): \t\tcolor = \"white\" if cm[i, j] > threshold else \"black\" \t\tplt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)  \tplt.tight_layout() \tplt.ylabel('True label') \tplt.xlabel('Predicted label') \treturn figure  (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0  time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") log_dir = f\"logs/{time}\" tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)  # Added file_writer = tf.summary.create_file_writer(log_dir) def log_confusion_matrix(epoch, logs): \ttest_pred_raw = model.predict(x_test) \ttest_pred = np.argmax(test_pred_raw, axis=1)  \tcm = sklearn.metrics.confusion_matrix(y_test, test_pred) \tfigure = plot_confusion_matrix(cm, class_names=list(range(10))) \tcm_image = plot_to_image(figure) \twith file_writer.as_default(): \t\ttf.summary.image(\"Confusion Matrix\", cm_image, step=epoch) cm_callback = LambdaCallback(on_epoch_end=log_confusion_matrix)  model = Sequential([     Flatten(input_shape=(28, 28)),     Dense(512, activation='relu'),     Dropout(0.2),     Dense(10, activation='softmax') ]) model.compile(optimizer='adam',               loss='sparse_categorical_crossentropy',               metrics=['accuracy'])  model.fit(     x=x_train,      y=y_train,      epochs=5,      validation_data=(x_test, y_test),      callbacks=[tensorboard, cm_callback] # added )`\n\n*   `LambdaCallback`s allow us to view the history of confusion matrices summarizing our model’s performance on the val set over time:\n\n![](/primers/ai/assets/hyperparam-tuning-and-tensorboard/confusion.png)",
      "order": 5,
      "orderInChapter": 5,
      "difficulty": 3,
      "estimatedMinutes": 4,
      "tags": [
        "miscellaneous",
        "activation",
        "dropout"
      ],
      "metadata": {
        "hasCode": true,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 662,
        "contentLength": 32500
      },
      "nextCards": [],
      "relatedCards": [
        "ai-gpu-architecture-forward-and-backward-passes-training-17",
        "ai-gpu-architecture-precision-optimization-19",
        "ai-model-debugging-grad-cam-15",
        "ai-model-debugging-gradient-checking-10",
        "ai-model-debugging-regularization-14"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/tensorboard/#custom-logging-callbacks",
      "scrapedAt": "2025-12-28T11:57:36.923Z",
      "siblings": [
        "ai-tensorboard-running-tensorboard-locally-1",
        "ai-tensorboard-setup-2",
        "ai-tensorboard-plotting-losses-accuracies-and-weight-distribution-3",
        "ai-tensorboard-logging-images-4"
      ]
    },
    {
      "id": "ai-cnns-for-text-classification-convolutions-1",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Convolutional Neural Networks for Text Classification",
      "articleSlug": "cnns-for-text-classification",
      "chapter": "Convolutional Neural Networks",
      "title": "Convolutions",
      "subtitle": "Convolutional Neural Networks",
      "contentHtml": "<ul>\n  <li>We can think about the input image as a matrix, where each entry represents each pixel, and a value between 0 and 255 representing the brightness intensity. Let’s assume it’s a black and white image with just one <a href=\"https://www.wikiwand.com/en/Channel_(digital_image)\"><strong>channel</strong></a> representing the grayscale. If you would be processing a colour image, and taking into account the colours one would have 3 channels, following the <a href=\"https://www.wikiwand.com/en/RGB_color_model\"><strong>RGB colour mode</strong></a>.</li>\n  <li>One way to understand the convolution operation is to imagine placing the <strong>convolution filter</strong> or <strong>kernel</strong> on the top of the input image, positioned in a way so that the <strong>kernel</strong> and the image upper left corners coincide, and then multiplying the values of the input image matrix with the corresponding values in the <strong>convolution filter</strong>.</li>\n  <li>All of the multiplied values are then added together resulting in a single scalar, which is placed in the first position of a result matrix.</li>\n  <li>The <strong>kernel</strong> is then moved <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">x</script> pixels to the right, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-5\"><span class=\"mi\" id=\"MathJax-Span-6\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">x</script> is denoted <strong>stride length</strong> and is a parameter of the ConvNet structure. The process of multiplication is then repeated, so that the next value in the result matrix is computed and filled.</li>\n  <li>This process is then repeated, by first covering an entire row, and then shifting down the columns by the same <strong>stride length</strong>, until all the entries in the input image have been covered.</li>\n  <li>The output of this process is a matrix with all it’s entries filled, called the <strong>convoluted feature</strong> or <strong>input feature map</strong>.</li>\n  <li>An input image can be convolved with multiple convolution kernels at once, creating one output for each kernel.</li>\n  <li>The following diagram from “Deep Learning” by Adam Gibson, Josh Patterson shows an example of a convolution operation.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/cnns-for-text-classification/dpln_0412_cnn.png\" alt=\"\"></p>",
      "contentMarkdown": "*   We can think about the input image as a matrix, where each entry represents each pixel, and a value between 0 and 255 representing the brightness intensity. Let’s assume it’s a black and white image with just one [**channel**](https://www.wikiwand.com/en/Channel_\\(digital_image\\)) representing the grayscale. If you would be processing a colour image, and taking into account the colours one would have 3 channels, following the [**RGB colour mode**](https://www.wikiwand.com/en/RGB_color_model).\n*   One way to understand the convolution operation is to imagine placing the **convolution filter** or **kernel** on the top of the input image, positioned in a way so that the **kernel** and the image upper left corners coincide, and then multiplying the values of the input image matrix with the corresponding values in the **convolution filter**.\n*   All of the multiplied values are then added together resulting in a single scalar, which is placed in the first position of a result matrix.\n*   The **kernel** is then moved xxx pixels to the right, where xxx is denoted **stride length** and is a parameter of the ConvNet structure. The process of multiplication is then repeated, so that the next value in the result matrix is computed and filled.\n*   This process is then repeated, by first covering an entire row, and then shifting down the columns by the same **stride length**, until all the entries in the input image have been covered.\n*   The output of this process is a matrix with all it’s entries filled, called the **convoluted feature** or **input feature map**.\n*   An input image can be convolved with multiple convolution kernels at once, creating one output for each kernel.\n*   The following diagram from “Deep Learning” by Adam Gibson, Josh Patterson shows an example of a convolution operation.\n\n![](/primers/ai/assets/cnns-for-text-classification/dpln_0412_cnn.png)",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "deep learning",
        "convolution",
        "cnn"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": true,
        "wordCount": 290,
        "contentLength": 4661
      },
      "nextCards": [
        "ai-cnns-for-text-classification-pooling-2",
        "ai-cnns-for-text-classification-fully-connected-3"
      ],
      "relatedCards": [
        "ai-top-30-papers-imagenet-classification-with-deep-convolutional-ne-7",
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-model-definition-and-training-cpugpu-13",
        "ai-gpu-architecture-operator-dispatch-and-kernel-mapping-14",
        "ai-gpu-architecture-kernel-launch-and-execution-on-sms-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/cnns-for-text-classification/#convolutions",
      "scrapedAt": "2025-12-28T11:57:41.797Z",
      "siblings": [
        "ai-cnns-for-text-classification-pooling-2",
        "ai-cnns-for-text-classification-fully-connected-3",
        "ai-cnns-for-text-classification-1-d-convolutions-over-text-4",
        "ai-cnns-for-text-classification-pooling-5",
        "ai-cnns-for-text-classification-convolutional-neural-networks-for-sentence-classif-6"
      ]
    },
    {
      "id": "ai-cnns-for-text-classification-pooling-2",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Convolutional Neural Networks for Text Classification",
      "articleSlug": "cnns-for-text-classification",
      "chapter": "Convolutional Neural Networks",
      "title": "Pooling",
      "subtitle": "Convolutional Neural Networks",
      "contentHtml": "<ul>\n  <li>\n    <p>Next comes the  <strong>pooling</strong> or <strong>downsampling</strong> layer, which consists of applying some operation over regions/patches in the <strong>input feature map</strong> and extracting some representative value for each of the analysed regions/patches.</p>\n  </li>\n  <li>\n    <p>This process is somehow similar to the convolution described before, but instead of transforming local patches via a learned linear transformation (i.e., the <strong>convolution filter</strong>), they’re transformed via a hardcoded operation.</p>\n  </li>\n  <li>Two of the most common pooling operations are max- and average-pooling. <strong>Max-pooling</strong> selects the maximum of the values in the <strong>input feature map</strong> region of each step and <strong>average-pooling</strong> the average value of the values in the region. The output in each step is therefore a single scalar, resulting in significant size reduction in output size.</li>\n  <li>The following diagram from <a href=\"https://medium.com/@Aj.Cheng/convolutional-neural-network-d9f69e473feb\">AJ Cheng’s blog</a> shows an example of a pooling operation with stride length of 2.</li>\n</ul>\n<p>Next comes the  <strong>pooling</strong> or <strong>downsampling</strong> layer, which consists of applying some operation over regions/patches in the <strong>input feature map</strong> and extracting some representative value for each of the analysed regions/patches.</p>\n<p>This process is somehow similar to the convolution described before, but instead of transforming local patches via a learned linear transformation (i.e., the <strong>convolution filter</strong>), they’re transformed via a hardcoded operation.</p>\n<p><img src=\"/primers/ai/assets/cnns-for-text-classification/cnn_pooling.jpg\" alt=\"\"></p>\n<ul>\n  <li>Why do we downsample the feature maps and simply just don’t remove the pooling layers and keep possibly large feature maps? François Chollet in <em>“Deep Learning with Python”</em> summarises it well in this sentence:</li>\n</ul>\n<blockquote>\n  <p>“The reason to use downsampling is to reduce the number of feature-map coefficients to process, as well as to induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows (in terms of the fraction of the original input they cover).”</p>\n</blockquote>\n<p>“The reason to use downsampling is to reduce the number of feature-map coefficients to process, as well as to induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows (in terms of the fraction of the original input they cover).”</p>",
      "contentMarkdown": "*   Next comes the **pooling** or **downsampling** layer, which consists of applying some operation over regions/patches in the **input feature map** and extracting some representative value for each of the analysed regions/patches.\n    \n*   This process is somehow similar to the convolution described before, but instead of transforming local patches via a learned linear transformation (i.e., the **convolution filter**), they’re transformed via a hardcoded operation.\n    \n*   Two of the most common pooling operations are max- and average-pooling. **Max-pooling** selects the maximum of the values in the **input feature map** region of each step and **average-pooling** the average value of the values in the region. The output in each step is therefore a single scalar, resulting in significant size reduction in output size.\n*   The following diagram from [AJ Cheng’s blog](https://medium.com/@Aj.Cheng/convolutional-neural-network-d9f69e473feb) shows an example of a pooling operation with stride length of 2.\n\nNext comes the **pooling** or **downsampling** layer, which consists of applying some operation over regions/patches in the **input feature map** and extracting some representative value for each of the analysed regions/patches.\n\nThis process is somehow similar to the convolution described before, but instead of transforming local patches via a learned linear transformation (i.e., the **convolution filter**), they’re transformed via a hardcoded operation.\n\n![](/primers/ai/assets/cnns-for-text-classification/cnn_pooling.jpg)\n\n*   Why do we downsample the feature maps and simply just don’t remove the pooling layers and keep possibly large feature maps? François Chollet in _“Deep Learning with Python”_ summarises it well in this sentence:\n\n> “The reason to use downsampling is to reduce the number of feature-map coefficients to process, as well as to induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows (in terms of the fraction of the original input they cover).”\n\n“The reason to use downsampling is to reduce the number of feature-map coefficients to process, as well as to induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows (in terms of the fraction of the original input they cover).”",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 2,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "deep learning",
        "convolution",
        "cnn"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 326,
        "contentLength": 2641
      },
      "nextCards": [
        "ai-cnns-for-text-classification-fully-connected-3",
        "ai-cnns-for-text-classification-1-d-convolutions-over-text-4"
      ],
      "relatedCards": [
        "ai-top-30-papers-imagenet-classification-with-deep-convolutional-ne-7",
        "ai-gpu-architecture-streaming-multiprocessors-sms-1",
        "ai-gpu-architecture-model-definition-and-training-cpugpu-13",
        "ai-gpu-architecture-operator-dispatch-and-kernel-mapping-14",
        "ai-gpu-architecture-kernel-launch-and-execution-on-sms-16"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/cnns-for-text-classification/#pooling",
      "scrapedAt": "2025-12-28T11:57:41.797Z",
      "siblings": [
        "ai-cnns-for-text-classification-convolutions-1",
        "ai-cnns-for-text-classification-fully-connected-3",
        "ai-cnns-for-text-classification-1-d-convolutions-over-text-4",
        "ai-cnns-for-text-classification-pooling-5",
        "ai-cnns-for-text-classification-convolutional-neural-networks-for-sentence-classif-6"
      ]
    },
    {
      "id": "ai-cnns-for-text-classification-fully-connected-3",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Convolutional Neural Networks for Text Classification",
      "articleSlug": "cnns-for-text-classification",
      "chapter": "Convolutional Neural Networks",
      "title": "Fully Connected",
      "subtitle": "Convolutional Neural Networks",
      "contentHtml": "<ul>\n  <li>The two processes described before, i.e., convolutions and pooling, can been thought of as a feature extractors, then we pass this features, usually as a reshaped vector of one row, further to the network, for instance, a multi-layer perceptron to be trained for classification. The following diagram shows an example of multi-layer perceptron network used to train for classification.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/cnns-for-text-classification/mlp.png\" alt=\"\"></p>\n<ul>\n  <li>This was a briefly description of the ConvNet architecture when applied to image processing, let’s now see how we can adapt this architecture to Natural Language Processing tasks.</li>\n</ul>",
      "contentMarkdown": "*   The two processes described before, i.e., convolutions and pooling, can been thought of as a feature extractors, then we pass this features, usually as a reshaped vector of one row, further to the network, for instance, a multi-layer perceptron to be trained for classification. The following diagram shows an example of multi-layer perceptron network used to train for classification.\n\n![](/primers/ai/assets/cnns-for-text-classification/mlp.png)\n\n*   This was a briefly description of the ConvNet architecture when applied to image processing, let’s now see how we can adapt this architecture to Natural Language Processing tasks.",
      "order": 3,
      "orderInChapter": 3,
      "difficulty": 2,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "convolution",
        "cnn"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 90,
        "contentLength": 691
      },
      "nextCards": [
        "ai-cnns-for-text-classification-1-d-convolutions-over-text-4",
        "ai-cnns-for-text-classification-pooling-5"
      ],
      "relatedCards": [
        "ai-gpu-architecture-model-definition-and-training-cpugpu-13",
        "ai-gpu-architecture-kernel-launch-and-execution-on-sms-16",
        "ai-ml-runtimes-example-pseudocode-flow-36",
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-top-30-papers-imagenet-classification-with-deep-convolutional-ne-7"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/cnns-for-text-classification/#fully-connected",
      "scrapedAt": "2025-12-28T11:57:41.797Z",
      "siblings": [
        "ai-cnns-for-text-classification-convolutions-1",
        "ai-cnns-for-text-classification-pooling-2",
        "ai-cnns-for-text-classification-1-d-convolutions-over-text-4",
        "ai-cnns-for-text-classification-pooling-5",
        "ai-cnns-for-text-classification-convolutional-neural-networks-for-sentence-classif-6"
      ]
    },
    {
      "id": "ai-cnns-for-text-classification-1-d-convolutions-over-text-4",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Convolutional Neural Networks for Text Classification",
      "articleSlug": "cnns-for-text-classification",
      "chapter": "Convolutional Neural Networks for NLP",
      "title": "1-D Convolutions Over Text",
      "subtitle": "Convolutional Neural Networks for NLP",
      "contentHtml": "<ul>\n  <li>Given a sequence of words <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>:</mo><mi>n</mi></mrow></msub><mo>=</mo><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-10\" style=\"width: 8.648em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1007.19em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-11\"><span class=\"msubsup\" id=\"MathJax-Span-12\"><span style=\"display: inline-block; position: relative; width: 1.669em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-13\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-14\"><span class=\"mrow\" id=\"MathJax-Span-15\"><span class=\"mn\" id=\"MathJax-Span-16\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-17\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mi\" id=\"MathJax-Span-18\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-19\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"msubsup\" id=\"MathJax-Span-20\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-21\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-22\"><span class=\"mrow\" id=\"MathJax-Span-23\"><span class=\"mn\" id=\"MathJax-Span-24\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-25\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-28\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-30\"><span class=\"mrow\" id=\"MathJax-Span-31\"><span class=\"mi\" id=\"MathJax-Span-32\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>:</mo><mi>n</mi></mrow></msub><mo>=</mo><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">w_{1:n} = w_{1}, \\ldots, w_{n}</script>, where each is associated with an embedding vector of dimension <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-33\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.58em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-34\"><span class=\"mi\" id=\"MathJax-Span-35\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>d</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">d</script>. A 1D convolution of width-<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-36\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-37\"><span class=\"mi\" id=\"MathJax-Span-38\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">k</script> is the result of moving a sliding-window of size <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-39\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-40\"><span class=\"mi\" id=\"MathJax-Span-41\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">k</script> over the sentence, and applying the same <strong>convolution filter</strong> or <strong>kernel</strong> to each window in the sequence, i.e., a dot-product between the concatenation of the embedding vectors in a given window and a weight vector <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>u</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-42\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-43\"><span class=\"mi\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Italic;\">u</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>u</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">u</script>, which is then often followed by a non-linear activation function <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>g</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-45\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-46\"><span class=\"mi\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Italic;\">g</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>g</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">g</script>.</li>\n  <li>Considering a window of words <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>+</mo><mi>k</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-48\" style=\"width: 5.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.846em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1004.85em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-49\"><span class=\"msubsup\" id=\"MathJax-Span-50\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-51\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-52\"><span class=\"mrow\" id=\"MathJax-Span-53\"><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-55\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-56\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-58\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-60\"><span class=\"mrow\" id=\"MathJax-Span-61\"><span class=\"mi\" id=\"MathJax-Span-62\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-63\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-64\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>+</mo><mi>k</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">w_{i}, \\ldots, w_{i+k}</script> the concatenated vector of the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-65\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-66\"><span class=\"mi\" id=\"MathJax-Span-67\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">i</script>th window is then:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>=</mo><mo stretchy=&quot;false&quot;>[</mo><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>+</mo><mi>k</mi></mrow></msub><mo stretchy=&quot;false&quot;>]</mo><mo>&amp;#x2208;</mo><msup><mi>R</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>k</mi><mtext>&amp;#xA0;</mtext><mo>&amp;#x00D7;</mo><mtext>&amp;#xA0;</mtext><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-68\" style=\"width: 15.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.232em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1013.23em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-69\"><span class=\"msubsup\" id=\"MathJax-Span-70\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-72\"><span class=\"mrow\" id=\"MathJax-Span-73\"><span class=\"mi\" id=\"MathJax-Span-74\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-75\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-76\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">[</span><span class=\"msubsup\" id=\"MathJax-Span-77\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-78\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-79\"><span class=\"mrow\" id=\"MathJax-Span-80\"><span class=\"mi\" id=\"MathJax-Span-81\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-83\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-84\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-85\"><span class=\"mrow\" id=\"MathJax-Span-86\"><span class=\"mi\" id=\"MathJax-Span-87\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-88\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mn\" id=\"MathJax-Span-89\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-90\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-91\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-92\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-93\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.773em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-94\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-95\"><span class=\"mrow\" id=\"MathJax-Span-96\"><span class=\"mi\" id=\"MathJax-Span-97\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-98\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">+</span><span class=\"mi\" id=\"MathJax-Span-99\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-100\" style=\"font-family: STIXGeneral-Regular;\">]</span><span class=\"mo\" id=\"MathJax-Span-101\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-102\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-104\"><span class=\"mrow\" id=\"MathJax-Span-105\"><span class=\"mtext\" id=\"MathJax-Span-106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-107\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mtext\" id=\"MathJax-Span-108\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mo\" id=\"MathJax-Span-109\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mtext\" id=\"MathJax-Span-110\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-111\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>=</mo><mo stretchy=\"false\">[</mo><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>+</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">]</mo><mo>∈</mo><msup><mi>R</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>&nbsp;</mtext><mi>k</mi><mtext>&nbsp;</mtext><mo>×</mo><mtext>&nbsp;</mtext><mi>d</mi></mrow></msup></math></span></span></div>\n<ul>\n  <li>The <strong>convolution filter</strong> is applied to each window, resulting in scalar values <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-112\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.68em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-113\"><span class=\"msubsup\" id=\"MathJax-Span-114\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-115\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-116\"><span class=\"mrow\" id=\"MathJax-Span-117\"><span class=\"mi\" id=\"MathJax-Span-118\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>r</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">r_{i}</script>, each for the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-119\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-120\"><span class=\"mi\" id=\"MathJax-Span-121\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\">i</script>th window:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>=</mo><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x22C5;</mo><mi>u</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2208;</mo><mi>R</mi></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-122\" style=\"width: 8.596em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1007.14em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-123\"><span class=\"msubsup\" id=\"MathJax-Span-124\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-125\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-126\"><span class=\"mrow\" id=\"MathJax-Span-127\"><span class=\"mi\" id=\"MathJax-Span-128\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-129\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-130\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">g</span><span class=\"mo\" id=\"MathJax-Span-131\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-132\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-133\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-134\"><span class=\"mrow\" id=\"MathJax-Span-135\"><span class=\"mi\" id=\"MathJax-Span-136\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">u</span><span class=\"mo\" id=\"MathJax-Span-139\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-140\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"mi\" id=\"MathJax-Span-141\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">R</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>r</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>=</mo><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>⋅</mo><mi>u</mi><mo stretchy=\"false\">)</mo><mo>∈</mo><mi>R</mi></math></span></span></div>\n<ul>\n  <li>\n    <p>In practice one typically applies more filters, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>u</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>u</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>l</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-142\" style=\"width: 4.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1003.86em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-143\"><span class=\"msubsup\" id=\"MathJax-Span-144\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Italic;\">u</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-146\"><span class=\"mrow\" id=\"MathJax-Span-147\"><span class=\"mn\" id=\"MathJax-Span-148\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-149\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-150\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-152\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Italic;\">u</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-154\"><span class=\"mrow\" id=\"MathJax-Span-155\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>u</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>u</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>l</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">u_{1}, \\ldots, u_{l}</script>, which can then be represented as a vector multiplied by a matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>U</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-157\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-158\"><span class=\"mi\" id=\"MathJax-Span-159\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>U</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">U</script> and with an addition of a bias term <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>b</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-160\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-161\"><span class=\"mi\" id=\"MathJax-Span-162\" style=\"font-family: STIXGeneral-Italic;\">b</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>b</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">b</script>:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mtext>r</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>=</mo><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x22C5;</mo><mi>U</mi><mo>+</mo><mi>b</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-163\" style=\"width: 8.44em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.034em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.98em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-164\"><span class=\"msubsup\" id=\"MathJax-Span-165\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mtext\" id=\"MathJax-Span-166\" style=\"font-family: STIXGeneral-Regular;\">r</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-167\"><span class=\"mrow\" id=\"MathJax-Span-168\"><span class=\"mi\" id=\"MathJax-Span-169\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-170\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">g</span><span class=\"mo\" id=\"MathJax-Span-172\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-173\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-174\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-175\"><span class=\"mrow\" id=\"MathJax-Span-176\"><span class=\"mi\" id=\"MathJax-Span-177\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-178\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-180\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-181\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">b</span><span class=\"mo\" id=\"MathJax-Span-182\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mtext>r</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>=</mo><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>⋅</mo><mi>U</mi><mo>+</mo><mi>b</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-19\">\\text{r}_{i} = g(x_{i} \\cdot U + b)</script>\n\n    <ul>\n      <li>with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mtext>r</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2208;</mo><msup><mi>R</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>l</mi></mrow></msup><mo>,</mo><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2208;</mo><msup><mi>R</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>k</mi><mtext>&amp;#xA0;</mtext><mo>&amp;#x00D7;</mo><mtext>&amp;#xA0;</mtext><mi>d</mi></mrow></msup><mo>,</mo><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mi>U</mi><mo>&amp;#x2208;</mo><msup><mi>R</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>k</mi><mtext>&amp;#xA0;</mtext><mo>&amp;#x22C5;</mo><mtext>&amp;#xA0;</mtext><mi>d</mi><mtext>&amp;#xA0;</mtext><mo>&amp;#x00D7;</mo><mi>l</mi></mrow></msup><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mtext>and</mtext><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mi>b</mi><mo>&amp;#x2208;</mo><msup><mi>R</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>l</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-183\" style=\"width: 23.961em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 19.951em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1019.95em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-184\"><span class=\"msubsup\" id=\"MathJax-Span-185\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mtext\" id=\"MathJax-Span-186\" style=\"font-family: STIXGeneral-Regular;\">r</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-187\"><span class=\"mrow\" id=\"MathJax-Span-188\"><span class=\"mi\" id=\"MathJax-Span-189\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-190\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-191\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-192\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-193\"><span class=\"mrow\" id=\"MathJax-Span-194\"><span class=\"mi\" id=\"MathJax-Span-195\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-196\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-197\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-198\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-199\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"msubsup\" id=\"MathJax-Span-200\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-201\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-202\"><span class=\"mrow\" id=\"MathJax-Span-203\"><span class=\"mi\" id=\"MathJax-Span-204\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-205\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-206\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-207\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-208\"><span class=\"mrow\" id=\"MathJax-Span-209\"><span class=\"mtext\" id=\"MathJax-Span-210\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-211\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mtext\" id=\"MathJax-Span-212\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mo\" id=\"MathJax-Span-213\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mtext\" id=\"MathJax-Span-214\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-215\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-216\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-217\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-219\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-220\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-221\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-222\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.971em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-223\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-224\"><span class=\"mrow\" id=\"MathJax-Span-225\"><span class=\"mtext\" id=\"MathJax-Span-226\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-227\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mtext\" id=\"MathJax-Span-228\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mo\" id=\"MathJax-Span-229\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mtext\" id=\"MathJax-Span-230\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-231\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mtext\" id=\"MathJax-Span-232\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mo\" id=\"MathJax-Span-233\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-234\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-235\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-236\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-237\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-238\" style=\"font-family: STIXGeneral-Regular;\">and</span><span class=\"mtext\" id=\"MathJax-Span-239\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-240\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-241\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-242\" style=\"font-family: STIXGeneral-Italic;\">b</span><span class=\"mo\" id=\"MathJax-Span-243\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-244\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-245\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-246\"><span class=\"mrow\" id=\"MathJax-Span-247\"><span class=\"mi\" id=\"MathJax-Span-248\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mtext>r</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>∈</mo><msup><mi>R</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>l</mi></mrow></msup><mo>,</mo><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>∈</mo><msup><mi>R</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>&nbsp;</mtext><mi>k</mi><mtext>&nbsp;</mtext><mo>×</mo><mtext>&nbsp;</mtext><mi>d</mi></mrow></msup><mo>,</mo><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mi>U</mi><mo>∈</mo><msup><mi>R</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>&nbsp;</mtext><mi>k</mi><mtext>&nbsp;</mtext><mo>⋅</mo><mtext>&nbsp;</mtext><mi>d</mi><mtext>&nbsp;</mtext><mo>×</mo><mi>l</mi></mrow></msup><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mtext>and</mtext><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mi>b</mi><mo>∈</mo><msup><mi>R</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>l</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">\\text{r}_{i} \\in R^{l},\\ \\ \\ x_{i} \\in R^{\\ k\\ \\times\\  d},\\ \\ \\  U \\in R^{\\ k\\ \\cdot\\  d \\ \\times l}\\ \\ \\  \\text{and}\\ \\ \\  b \\in R^{l}</script></li>\n    </ul>\n  </li>\n  <li>\n    <p>The following diagram from Yoav Goldberg’s book <a href=\"http://u.cs.biu.ac.il/~yogo/\">“Neural Network Methods for NLP”</a> shows an example of a sentence convolution in a vector-concatenation notation with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-249\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-250\"><span class=\"mi\" id=\"MathJax-Span-251\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">k</script>=2 and dimensional output <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>l</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-252\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-253\"><span class=\"mi\" id=\"MathJax-Span-254\" style=\"font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>l</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">l</script>=3.</p>\n  </li>\n</ul>\n<p>In practice one typically applies more filters, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>u</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>u</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>l</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-142\" style=\"width: 4.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1003.86em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-143\"><span class=\"msubsup\" id=\"MathJax-Span-144\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Italic;\">u</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-146\"><span class=\"mrow\" id=\"MathJax-Span-147\"><span class=\"mn\" id=\"MathJax-Span-148\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-149\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-150\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-152\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-153\" style=\"font-family: STIXGeneral-Italic;\">u</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-154\"><span class=\"mrow\" id=\"MathJax-Span-155\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>u</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>u</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>l</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">u_{1}, \\ldots, u_{l}</script>, which can then be represented as a vector multiplied by a matrix <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>U</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-157\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-158\"><span class=\"mi\" id=\"MathJax-Span-159\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>U</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">U</script> and with an addition of a bias term <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>b</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-160\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-161\"><span class=\"mi\" id=\"MathJax-Span-162\" style=\"font-family: STIXGeneral-Italic;\">b</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>b</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">b</script>:</p>\n<ul>\n      <li>with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mtext>r</mtext><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2208;</mo><msup><mi>R</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>l</mi></mrow></msup><mo>,</mo><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2208;</mo><msup><mi>R</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>k</mi><mtext>&amp;#xA0;</mtext><mo>&amp;#x00D7;</mo><mtext>&amp;#xA0;</mtext><mi>d</mi></mrow></msup><mo>,</mo><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mi>U</mi><mo>&amp;#x2208;</mo><msup><mi>R</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>k</mi><mtext>&amp;#xA0;</mtext><mo>&amp;#x22C5;</mo><mtext>&amp;#xA0;</mtext><mi>d</mi><mtext>&amp;#xA0;</mtext><mo>&amp;#x00D7;</mo><mi>l</mi></mrow></msup><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mtext>and</mtext><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mi>b</mi><mo>&amp;#x2208;</mo><msup><mi>R</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>l</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-183\" style=\"width: 23.961em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 19.951em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1019.95em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-184\"><span class=\"msubsup\" id=\"MathJax-Span-185\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mtext\" id=\"MathJax-Span-186\" style=\"font-family: STIXGeneral-Regular;\">r</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-187\"><span class=\"mrow\" id=\"MathJax-Span-188\"><span class=\"mi\" id=\"MathJax-Span-189\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-190\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-191\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-192\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-193\"><span class=\"mrow\" id=\"MathJax-Span-194\"><span class=\"mi\" id=\"MathJax-Span-195\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-196\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-197\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-198\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-199\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"msubsup\" id=\"MathJax-Span-200\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-201\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-202\"><span class=\"mrow\" id=\"MathJax-Span-203\"><span class=\"mi\" id=\"MathJax-Span-204\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-205\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-206\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.346em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-207\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-208\"><span class=\"mrow\" id=\"MathJax-Span-209\"><span class=\"mtext\" id=\"MathJax-Span-210\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-211\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mtext\" id=\"MathJax-Span-212\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mo\" id=\"MathJax-Span-213\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mtext\" id=\"MathJax-Span-214\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-215\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-216\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mtext\" id=\"MathJax-Span-217\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-219\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-220\" style=\"font-family: STIXGeneral-Italic;\">U<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-221\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-222\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 2.971em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-223\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-224\"><span class=\"mrow\" id=\"MathJax-Span-225\"><span class=\"mtext\" id=\"MathJax-Span-226\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-227\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mtext\" id=\"MathJax-Span-228\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mo\" id=\"MathJax-Span-229\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">⋅</span><span class=\"mtext\" id=\"MathJax-Span-230\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-231\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mtext\" id=\"MathJax-Span-232\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mo\" id=\"MathJax-Span-233\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">×</span><span class=\"mi\" id=\"MathJax-Span-234\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-235\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-236\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-237\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-238\" style=\"font-family: STIXGeneral-Regular;\">and</span><span class=\"mtext\" id=\"MathJax-Span-239\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-240\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-241\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-242\" style=\"font-family: STIXGeneral-Italic;\">b</span><span class=\"mo\" id=\"MathJax-Span-243\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-244\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.58em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-245\" style=\"font-family: STIXGeneral-Italic;\">R</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-246\"><span class=\"mrow\" id=\"MathJax-Span-247\"><span class=\"mi\" id=\"MathJax-Span-248\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mtext>r</mtext><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>∈</mo><msup><mi>R</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>l</mi></mrow></msup><mo>,</mo><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>∈</mo><msup><mi>R</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>&nbsp;</mtext><mi>k</mi><mtext>&nbsp;</mtext><mo>×</mo><mtext>&nbsp;</mtext><mi>d</mi></mrow></msup><mo>,</mo><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mi>U</mi><mo>∈</mo><msup><mi>R</mi><mrow class=\"MJX-TeXAtom-ORD\"><mtext>&nbsp;</mtext><mi>k</mi><mtext>&nbsp;</mtext><mo>⋅</mo><mtext>&nbsp;</mtext><mi>d</mi><mtext>&nbsp;</mtext><mo>×</mo><mi>l</mi></mrow></msup><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mtext>and</mtext><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mi>b</mi><mo>∈</mo><msup><mi>R</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>l</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">\\text{r}_{i} \\in R^{l},\\ \\ \\ x_{i} \\in R^{\\ k\\ \\times\\  d},\\ \\ \\  U \\in R^{\\ k\\ \\cdot\\  d \\ \\times l}\\ \\ \\  \\text{and}\\ \\ \\  b \\in R^{l}</script></li>\n    </ul>\n<p>The following diagram from Yoav Goldberg’s book <a href=\"http://u.cs.biu.ac.il/~yogo/\">“Neural Network Methods for NLP”</a> shows an example of a sentence convolution in a vector-concatenation notation with <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-249\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-250\"><span class=\"mi\" id=\"MathJax-Span-251\" style=\"font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">k</script>=2 and dimensional output <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>l</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-252\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-253\"><span class=\"mi\" id=\"MathJax-Span-254\" style=\"font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>l</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">l</script>=3.</p>\n<p><img src=\"/primers/ai/assets/cnns-for-text-classification/sentence_convolution-example.png\" alt=\"\"></p>\n<h4 id=\"channels\">Channels</h4>\n<ul>\n  <li>\n    <p>In the introduction above we assumed we were processing a black and white image, and therefore we have one matrix representing the grayscale intensity of each pixel. With the <a href=\"https://www.wikiwand.com/en/RGB_color_model\"><strong>RGB colour mode</strong></a> each pixel would be a combination of three intensity values instead, one for each of Red, Green and Blue components, and such representation would be stored in three different matrices, providing different characteristics or view of the image, referred to as a <a href=\"https://www.wikiwand.com/en/Channel_(digital_image)\"><strong>Channel</strong></a>. It’s common to apply a different set of filters to each channel, and then combine the three resulting vectors into a single vector.</p>\n  </li>\n  <li>\n    <p>We can also apply the multiple channels paradigm in text processing as well. For example, for a given phrase or window of text, one channel could be the sequence of words, a second channel could be the sequence of corresponding part-of-speech (POS) tags, and a third one could be the shape of the words:</p>\n  </li>\n</ul>\n<p>In the introduction above we assumed we were processing a black and white image, and therefore we have one matrix representing the grayscale intensity of each pixel. With the <a href=\"https://www.wikiwand.com/en/RGB_color_model\"><strong>RGB colour mode</strong></a> each pixel would be a combination of three intensity values instead, one for each of Red, Green and Blue components, and such representation would be stored in three different matrices, providing different characteristics or view of the image, referred to as a <a href=\"https://www.wikiwand.com/en/Channel_(digital_image)\"><strong>Channel</strong></a>. It’s common to apply a different set of filters to each channel, and then combine the three resulting vectors into a single vector.</p>\n<p>We can also apply the multiple channels paradigm in text processing as well. For example, for a given phrase or window of text, one channel could be the sequence of words, a second channel could be the sequence of corresponding part-of-speech (POS) tags, and a third one could be the shape of the words:</p>\n<div align=\"center\">\n<table class=\"tg\">\n  <tbody><tr>\n    <th class=\"tg-hcenter-valign-first\">Word:</th>\n    <th class=\"tg-hcenter-valign-first\">The</th>\n    <th class=\"tg-hcenter-valign-first\">plane</th>\n    <th class=\"tg-hcenter-valign-first\">lands</th>\n    <th class=\"tg-hcenter-valign-first\">in</th>\n    <th class=\"tg-hcenter-valign-second\">Lisbon</th>\n  </tr>\n  <tr>\n    <td class=\"tg-tleft-valign-first\">PoS-tag:</td>\n    <td class=\"tg-tleft-valign-first\">DET</td>\n    <td class=\"tg-tleft-valign-first\">NOUN</td>\n    <td class=\"tg-tleft-valign-first\">VERB</td>\n    <td class=\"tg-tleft-valign-first\">PROP</td>\n    <td class=\"tg-tleft-valign-second\">NOUN</td>\n  </tr>\n  <tr>\n    <td class=\"tg-tleft-valign-first\">Shape:</td>\n    <td class=\"tg-tleft-valign-first\">Xxx</td>\n    <td class=\"tg-tleft-valign-first\">xxxx</td>\n    <td class=\"tg-tleft-valign-first\">xxxx</td>\n    <td class=\"tg-tleft-valign-first\">xx</td>\n    <td class=\"tg-tleft-valign-second\">Xxxxxx</td>\n  </tr>\n</tbody></table>\n</div>\n<table class=\"tg\">\n  <tbody><tr>\n    <th class=\"tg-hcenter-valign-first\">Word:</th>\n    <th class=\"tg-hcenter-valign-first\">The</th>\n    <th class=\"tg-hcenter-valign-first\">plane</th>\n    <th class=\"tg-hcenter-valign-first\">lands</th>\n    <th class=\"tg-hcenter-valign-first\">in</th>\n    <th class=\"tg-hcenter-valign-second\">Lisbon</th>\n  </tr>\n  <tr>\n    <td class=\"tg-tleft-valign-first\">PoS-tag:</td>\n    <td class=\"tg-tleft-valign-first\">DET</td>\n    <td class=\"tg-tleft-valign-first\">NOUN</td>\n    <td class=\"tg-tleft-valign-first\">VERB</td>\n    <td class=\"tg-tleft-valign-first\">PROP</td>\n    <td class=\"tg-tleft-valign-second\">NOUN</td>\n  </tr>\n  <tr>\n    <td class=\"tg-tleft-valign-first\">Shape:</td>\n    <td class=\"tg-tleft-valign-first\">Xxx</td>\n    <td class=\"tg-tleft-valign-first\">xxxx</td>\n    <td class=\"tg-tleft-valign-first\">xxxx</td>\n    <td class=\"tg-tleft-valign-first\">xx</td>\n    <td class=\"tg-tleft-valign-second\">Xxxxxx</td>\n  </tr>\n</tbody></table>\n<ul>\n  <li>Applying the convolution over the words will result in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>m</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-255\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-256\"><span class=\"mi\" id=\"MathJax-Span-257\" style=\"font-family: STIXGeneral-Italic;\">m</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>m</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\">m</script> vectors <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-258\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-259\"><span class=\"mi\" id=\"MathJax-Span-260\" style=\"font-family: STIXGeneral-Italic;\">w</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">w</script>, applying it over the PoS-tags will result also in <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>m</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-261\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-262\"><span class=\"mi\" id=\"MathJax-Span-263\" style=\"font-family: STIXGeneral-Italic;\">m</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>m</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">m</script> vectors, and the same for the shapes, again <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>m</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-264\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-265\"><span class=\"mi\" id=\"MathJax-Span-266\" style=\"font-family: STIXGeneral-Italic;\">m</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>m</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">m</script> vectors. These three different channels can then be combined either by summation:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><msub><mi>s</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>:</mo><mi>m</mi></mrow></msub><mo>+</mo><mi>p</mi><mi>o</mi><msub><mi>s</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>:</mo><mi>m</mi></mrow></msub><mo>+</mo><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><msub><mi>s</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>:</mo><mi>m</mi></mrow></msub></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-267\" style=\"width: 16.982em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.117em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1014.12em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-268\"><span class=\"msubsup\" id=\"MathJax-Span-269\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-270\" style=\"font-family: STIXGeneral-Italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-271\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-272\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-273\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">w</span><span class=\"mi\" id=\"MathJax-Span-274\" style=\"font-family: STIXGeneral-Italic;\">o</span><span class=\"mi\" id=\"MathJax-Span-275\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-276\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-277\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-278\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-279\"><span class=\"mrow\" id=\"MathJax-Span-280\"><span class=\"mn\" id=\"MathJax-Span-281\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-282\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mi\" id=\"MathJax-Span-283\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-284\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-285\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">p</span><span class=\"mi\" id=\"MathJax-Span-286\" style=\"font-family: STIXGeneral-Italic;\">o</span><span class=\"msubsup\" id=\"MathJax-Span-287\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-288\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-289\"><span class=\"mrow\" id=\"MathJax-Span-290\"><span class=\"mn\" id=\"MathJax-Span-291\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-292\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mi\" id=\"MathJax-Span-293\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-294\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">+</span><span class=\"mi\" id=\"MathJax-Span-295\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">s</span><span class=\"mi\" id=\"MathJax-Span-296\" style=\"font-family: STIXGeneral-Italic;\">h</span><span class=\"mi\" id=\"MathJax-Span-297\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-298\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mi\" id=\"MathJax-Span-299\" style=\"font-family: STIXGeneral-Italic;\">e</span><span class=\"msubsup\" id=\"MathJax-Span-300\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-301\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-302\"><span class=\"mrow\" id=\"MathJax-Span-303\"><span class=\"mn\" id=\"MathJax-Span-304\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-305\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mi\" id=\"MathJax-Span-306\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><msub><mi>s</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>:</mo><mi>m</mi></mrow></msub><mo>+</mo><mi>p</mi><mi>o</mi><msub><mi>s</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>:</mo><mi>m</mi></mrow></msub><mo>+</mo><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><msub><mi>s</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>:</mo><mi>m</mi></mrow></msub></math></span></span></div>\n<ul>\n  <li>or by concatenation:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mo stretchy=&quot;false&quot;>[</mo><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><msub><mi>s</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>:</mo><mi>m</mi></mrow></msub><mo>;</mo><mi>p</mi><mi>o</mi><msub><mi>s</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>:</mo><mi>m</mi></mrow></msub><mo>;</mo><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><msub><mi>s</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn><mo>:</mo><mi>m</mi></mrow></msub><mo stretchy=&quot;false&quot;>]</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-307\" style=\"width: 16.253em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.544em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1013.44em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-308\"><span class=\"msubsup\" id=\"MathJax-Span-309\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-310\" style=\"font-family: STIXGeneral-Italic;\">p</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.523em;\"><span class=\"mi\" id=\"MathJax-Span-311\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-312\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-313\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">[</span><span class=\"mi\" id=\"MathJax-Span-314\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mi\" id=\"MathJax-Span-315\" style=\"font-family: STIXGeneral-Italic;\">o</span><span class=\"mi\" id=\"MathJax-Span-316\" style=\"font-family: STIXGeneral-Italic;\">r<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mi\" id=\"MathJax-Span-317\" style=\"font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"msubsup\" id=\"MathJax-Span-318\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-319\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-320\"><span class=\"mrow\" id=\"MathJax-Span-321\"><span class=\"mn\" id=\"MathJax-Span-322\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-323\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mi\" id=\"MathJax-Span-324\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-325\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"mi\" id=\"MathJax-Span-326\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">p</span><span class=\"mi\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Italic;\">o</span><span class=\"msubsup\" id=\"MathJax-Span-328\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-329\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-330\"><span class=\"mrow\" id=\"MathJax-Span-331\"><span class=\"mn\" id=\"MathJax-Span-332\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-333\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mi\" id=\"MathJax-Span-334\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-335\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"mi\" id=\"MathJax-Span-336\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">s</span><span class=\"mi\" id=\"MathJax-Span-337\" style=\"font-family: STIXGeneral-Italic;\">h</span><span class=\"mi\" id=\"MathJax-Span-338\" style=\"font-family: STIXGeneral-Italic;\">a</span><span class=\"mi\" id=\"MathJax-Span-339\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mi\" id=\"MathJax-Span-340\" style=\"font-family: STIXGeneral-Italic;\">e</span><span class=\"msubsup\" id=\"MathJax-Span-341\"><span style=\"display: inline-block; position: relative; width: 1.513em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-342\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-343\"><span class=\"mrow\" id=\"MathJax-Span-344\"><span class=\"mn\" id=\"MathJax-Span-345\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span><span class=\"mo\" id=\"MathJax-Span-346\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">:</span><span class=\"mi\" id=\"MathJax-Span-347\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-348\" style=\"font-family: STIXGeneral-Regular;\">]</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mo stretchy=\"false\">[</mo><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><msub><mi>s</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>:</mo><mi>m</mi></mrow></msub><mo>;</mo><mi>p</mi><mi>o</mi><msub><mi>s</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>:</mo><mi>m</mi></mrow></msub><mo>;</mo><mi>s</mi><mi>h</mi><mi>a</mi><mi>p</mi><mi>e</mi><msub><mi>s</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn><mo>:</mo><mi>m</mi></mrow></msub><mo stretchy=\"false\">]</mo></math></span></span></div>\n<p><strong>NOTE</strong>: each channel can still have different convolutions that read the source document using different kernel sizes, for instance, applying different context windows over words, pos-tags or shapes.</p>",
      "contentMarkdown": "*   Given a sequence of words w1:n\\=w1,…,wnw1:n\\=w1,…,wnw\\_{1:n} = w\\_{1}, \\\\ldots, w\\_{n}, where each is associated with an embedding vector of dimension ddd. A 1D convolution of width-kkk is the result of moving a sliding-window of size kkk over the sentence, and applying the same **convolution filter** or **kernel** to each window in the sequence, i.e., a dot-product between the concatenation of the embedding vectors in a given window and a weight vector uuu, which is then often followed by a non-linear activation function ggg.\n*   Considering a window of words wi,…,wi+kwi,…,wi+kw\\_{i}, \\\\ldots, w\\_{i+k} the concatenated vector of the iiith window is then:\n\nxi\\=\\[wi,wi+1,…,wi+k\\]∈R k × dxi\\=\\[wi,wi+1,…,wi+k\\]∈R k × d\n\n*   The **convolution filter** is applied to each window, resulting in scalar values ririr\\_{i}, each for the iiith window:\n\nri\\=g(xi⋅u)∈Rri\\=g(xi⋅u)∈R\n\n*   In practice one typically applies more filters, u1,…,ulu1,…,ulu\\_{1}, \\\\ldots, u\\_{l}, which can then be represented as a vector multiplied by a matrix UUU and with an addition of a bias term bbb:\n    \n    ri\\=g(xi⋅U+b)ri\\=g(xi⋅U+b)\n    \n    \\\\text{r}\\_{i} = g(x\\_{i} \\\\cdot U + b)\n    *   with ri∈Rl,   xi∈R k × d,   U∈R k ⋅ d ×l   and   b∈Rlri∈Rl,   xi∈R k × d,   U∈R k ⋅ d ×l   and   b∈Rl\\\\text{r}\\_{i} \\\\in R^{l},\\\\ \\\\ \\\\ x\\_{i} \\\\in R^{\\\\ k\\\\ \\\\times\\\\ d},\\\\ \\\\ \\\\ U \\\\in R^{\\\\ k\\\\ \\\\cdot\\\\ d \\\\ \\\\times l}\\\\ \\\\ \\\\ \\\\text{and}\\\\ \\\\ \\\\ b \\\\in R^{l}\n*   The following diagram from Yoav Goldberg’s book [“Neural Network Methods for NLP”](http://u.cs.biu.ac.il/~yogo/) shows an example of a sentence convolution in a vector-concatenation notation with kkk\\=2 and dimensional output lll\\=3.\n    \n\nIn practice one typically applies more filters, u1,…,ulu1,…,ulu\\_{1}, \\\\ldots, u\\_{l}, which can then be represented as a vector multiplied by a matrix UUU and with an addition of a bias term bbb:\n\n*   with ri∈Rl,   xi∈R k × d,   U∈R k ⋅ d ×l   and   b∈Rlri∈Rl,   xi∈R k × d,   U∈R k ⋅ d ×l   and   b∈Rl\\\\text{r}\\_{i} \\\\in R^{l},\\\\ \\\\ \\\\ x\\_{i} \\\\in R^{\\\\ k\\\\ \\\\times\\\\ d},\\\\ \\\\ \\\\ U \\\\in R^{\\\\ k\\\\ \\\\cdot\\\\ d \\\\ \\\\times l}\\\\ \\\\ \\\\ \\\\text{and}\\\\ \\\\ \\\\ b \\\\in R^{l}\n\nThe following diagram from Yoav Goldberg’s book [“Neural Network Methods for NLP”](http://u.cs.biu.ac.il/~yogo/) shows an example of a sentence convolution in a vector-concatenation notation with kkk\\=2 and dimensional output lll\\=3.\n\n![](/primers/ai/assets/cnns-for-text-classification/sentence_convolution-example.png)\n\n#### Channels\n\n*   In the introduction above we assumed we were processing a black and white image, and therefore we have one matrix representing the grayscale intensity of each pixel. With the [**RGB colour mode**](https://www.wikiwand.com/en/RGB_color_model) each pixel would be a combination of three intensity values instead, one for each of Red, Green and Blue components, and such representation would be stored in three different matrices, providing different characteristics or view of the image, referred to as a [**Channel**](https://www.wikiwand.com/en/Channel_\\(digital_image\\)). It’s common to apply a different set of filters to each channel, and then combine the three resulting vectors into a single vector.\n    \n*   We can also apply the multiple channels paradigm in text processing as well. For example, for a given phrase or window of text, one channel could be the sequence of words, a second channel could be the sequence of corresponding part-of-speech (POS) tags, and a third one could be the shape of the words:\n    \n\nIn the introduction above we assumed we were processing a black and white image, and therefore we have one matrix representing the grayscale intensity of each pixel. With the [**RGB colour mode**](https://www.wikiwand.com/en/RGB_color_model) each pixel would be a combination of three intensity values instead, one for each of Red, Green and Blue components, and such representation would be stored in three different matrices, providing different characteristics or view of the image, referred to as a [**Channel**](https://www.wikiwand.com/en/Channel_\\(digital_image\\)). It’s common to apply a different set of filters to each channel, and then combine the three resulting vectors into a single vector.\n\nWe can also apply the multiple channels paradigm in text processing as well. For example, for a given phrase or window of text, one channel could be the sequence of words, a second channel could be the sequence of corresponding part-of-speech (POS) tags, and a third one could be the shape of the words:\n\nWord:\n\nThe\n\nplane\n\nlands\n\nin\n\nLisbon\n\nPoS-tag:\n\nDET\n\nNOUN\n\nVERB\n\nPROP\n\nNOUN\n\nShape:\n\nXxx\n\nxxxx\n\nxxxx\n\nxx\n\nXxxxxx\n\nWord:\n\nThe\n\nplane\n\nlands\n\nin\n\nLisbon\n\nPoS-tag:\n\nDET\n\nNOUN\n\nVERB\n\nPROP\n\nNOUN\n\nShape:\n\nXxx\n\nxxxx\n\nxxxx\n\nxx\n\nXxxxxx\n\n*   Applying the convolution over the words will result in mmm vectors www, applying it over the PoS-tags will result also in mmm vectors, and the same for the shapes, again mmm vectors. These three different channels can then be combined either by summation:\n\npi\\=words1:m+pos1:m+shapes1:mpi\\=words1:m+pos1:m+shapes1:m\n\n*   or by concatenation:\n\npi\\=\\[words1:m;pos1:m;shapes1:m\\]pi\\=\\[words1:m;pos1:m;shapes1:m\\]\n\n**NOTE**: each channel can still have different convolutions that read the source document using different kernel sizes, for instance, applying different context windows over words, pos-tags or shapes.",
      "order": 4,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 4,
      "tags": [
        "miscellaneous",
        "neural network",
        "embedding",
        "convolution",
        "cnn",
        "nlp",
        "activation"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": true,
        "wordCount": 792,
        "contentLength": 103858
      },
      "nextCards": [
        "ai-cnns-for-text-classification-pooling-5",
        "ai-cnns-for-text-classification-convolutional-neural-networks-for-sentence-classif-6"
      ],
      "relatedCards": [
        "ai-gpu-architecture-operator-dispatch-and-kernel-mapping-14",
        "ai-ml-runtimes-supported-model-types-11",
        "ai-ml-runtimes-fbgemm-by-meta-server-cpus-64",
        "ai-top-30-papers-attention-is-all-you-need-13",
        "ai-ann-similarity-search-quantization-based-methods-5"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/cnns-for-text-classification/#1-d-convolutions-over-text",
      "scrapedAt": "2025-12-28T11:57:41.797Z",
      "siblings": [
        "ai-cnns-for-text-classification-convolutions-1",
        "ai-cnns-for-text-classification-pooling-2",
        "ai-cnns-for-text-classification-fully-connected-3",
        "ai-cnns-for-text-classification-pooling-5",
        "ai-cnns-for-text-classification-convolutional-neural-networks-for-sentence-classif-6"
      ]
    },
    {
      "id": "ai-cnns-for-text-classification-pooling-5",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Convolutional Neural Networks for Text Classification",
      "articleSlug": "cnns-for-text-classification",
      "chapter": "Convolutional Neural Networks for NLP",
      "title": "Pooling",
      "subtitle": "Convolutional Neural Networks for NLP",
      "contentHtml": "<ul>\n  <li>The pooling operation is used to combine the vectors resulting from different convolution windows into a single <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>l</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-349\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-350\"><span class=\"mi\" id=\"MathJax-Span-351\" style=\"font-family: STIXGeneral-Italic;\">l<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>l</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">l</script>-dimensional vector. This is done again by taking the <em>max</em> or the <em>average</em> value observed in resulting vector from the convolutions. Ideally this vector will capture the most relevant features of the sentence/document.</li>\n  <li>This vector is then fed further down in the network - hence, the idea that ConvNet itself is just a feature extractor - most probably to a full connected layer to perform prediction.</li>\n</ul>",
      "contentMarkdown": "*   The pooling operation is used to combine the vectors resulting from different convolution windows into a single lll\\-dimensional vector. This is done again by taking the _max_ or the _average_ value observed in resulting vector from the convolutions. Ideally this vector will capture the most relevant features of the sentence/document.\n*   This vector is then fed further down in the network - hence, the idea that ConvNet itself is just a feature extractor - most probably to a full connected layer to perform prediction.",
      "order": 5,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous",
        "convolution"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 85,
        "contentLength": 1832
      },
      "nextCards": [
        "ai-cnns-for-text-classification-convolutional-neural-networks-for-sentence-classif-6"
      ],
      "relatedCards": [
        "ai-gpu-architecture-model-definition-and-training-cpugpu-13",
        "ai-gpu-architecture-kernel-launch-and-execution-on-sms-16",
        "ai-ml-runtimes-common-architectural-layers-1",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/cnns-for-text-classification/#pooling",
      "scrapedAt": "2025-12-28T11:57:41.797Z",
      "siblings": [
        "ai-cnns-for-text-classification-convolutions-1",
        "ai-cnns-for-text-classification-pooling-2",
        "ai-cnns-for-text-classification-fully-connected-3",
        "ai-cnns-for-text-classification-1-d-convolutions-over-text-4",
        "ai-cnns-for-text-classification-convolutional-neural-networks-for-sentence-classif-6"
      ]
    },
    {
      "id": "ai-cnns-for-text-classification-convolutional-neural-networks-for-sentence-classif-6",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Convolutional Neural Networks for Text Classification",
      "articleSlug": "cnns-for-text-classification",
      "chapter": "Convolutional Neural Networks for NLP",
      "title": "Convolutional Neural Networks for Sentence Classification",
      "subtitle": "Convolutional Neural Networks for NLP",
      "contentHtml": "<ul>\n  <li>\n    <p>We did a quick experiment, based on the paper by <a href=\"http://www.aclweb.org/anthology/D14-1181\">Y. Kim</a>, implementing the four ConvNets models he used to perform sentence classification.</p>\n\n    <ul>\n      <li>\n        <p><strong>CNN-rand</strong>: all words are randomly initialized and then modified during training</p>\n      </li>\n      <li>\n        <p><strong>CNN-static</strong>: pre-trained vectors with all the words— including the unknown ones that are randomly initialized—kept static and only the other parameters of the model are learned</p>\n      </li>\n      <li>\n        <p><strong>CNN-non-static</strong>: same as CNN-static but word vectors are fine-tuned</p>\n      </li>\n      <li>\n        <p><strong>CNN-multichannel</strong>: model with two sets of word vectors. Each set of vectors is treated as a channel and each filter is applied</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>Let’s just first quickly look at how these different models look like in as a computational graph. The first three (i.e., CNN-rand, CNN-static and CNN-non-static) look pretty much the same:</p>\n  </li>\n</ul>\n<p>We did a quick experiment, based on the paper by <a href=\"http://www.aclweb.org/anthology/D14-1181\">Y. Kim</a>, implementing the four ConvNets models he used to perform sentence classification.</p>\n<ul>\n      <li>\n        <p><strong>CNN-rand</strong>: all words are randomly initialized and then modified during training</p>\n      </li>\n      <li>\n        <p><strong>CNN-static</strong>: pre-trained vectors with all the words— including the unknown ones that are randomly initialized—kept static and only the other parameters of the model are learned</p>\n      </li>\n      <li>\n        <p><strong>CNN-non-static</strong>: same as CNN-static but word vectors are fine-tuned</p>\n      </li>\n      <li>\n        <p><strong>CNN-multichannel</strong>: model with two sets of word vectors. Each set of vectors is treated as a channel and each filter is applied</p>\n      </li>\n    </ul>\n<p><strong>CNN-rand</strong>: all words are randomly initialized and then modified during training</p>\n<p><strong>CNN-static</strong>: pre-trained vectors with all the words— including the unknown ones that are randomly initialized—kept static and only the other parameters of the model are learned</p>\n<p><strong>CNN-non-static</strong>: same as CNN-static but word vectors are fine-tuned</p>\n<p><strong>CNN-multichannel</strong>: model with two sets of word vectors. Each set of vectors is treated as a channel and each filter is applied</p>\n<p>Let’s just first quickly look at how these different models look like in as a computational graph. The first three (i.e., CNN-rand, CNN-static and CNN-non-static) look pretty much the same:</p>\n<p><img src=\"/primers/ai/assets/cnns-for-text-classification/SentenceClassificationConvNets-no_multi_channel.svg\" alt=\"\"></p>\n<ul>\n  <li>The CNN-multichannel model uses two embedding layers, in one channel the embeddings are updated, in the second they remain static. It’s exactly the same network as above but duplicated and adding an extra layer do concatenate both results into a single vector:</li>\n</ul>\n<p><img src=\"/primers/ai/assets/cnns-for-text-classification/SentenceClassificationConvNets-multi_channel.svg\" alt=\"\"></p>",
      "contentMarkdown": "*   We did a quick experiment, based on the paper by [Y. Kim](http://www.aclweb.org/anthology/D14-1181), implementing the four ConvNets models he used to perform sentence classification.\n    \n    *   **CNN-rand**: all words are randomly initialized and then modified during training\n        \n    *   **CNN-static**: pre-trained vectors with all the words— including the unknown ones that are randomly initialized—kept static and only the other parameters of the model are learned\n        \n    *   **CNN-non-static**: same as CNN-static but word vectors are fine-tuned\n        \n    *   **CNN-multichannel**: model with two sets of word vectors. Each set of vectors is treated as a channel and each filter is applied\n        \n*   Let’s just first quickly look at how these different models look like in as a computational graph. The first three (i.e., CNN-rand, CNN-static and CNN-non-static) look pretty much the same:\n    \n\nWe did a quick experiment, based on the paper by [Y. Kim](http://www.aclweb.org/anthology/D14-1181), implementing the four ConvNets models he used to perform sentence classification.\n\n*   **CNN-rand**: all words are randomly initialized and then modified during training\n    \n*   **CNN-static**: pre-trained vectors with all the words— including the unknown ones that are randomly initialized—kept static and only the other parameters of the model are learned\n    \n*   **CNN-non-static**: same as CNN-static but word vectors are fine-tuned\n    \n*   **CNN-multichannel**: model with two sets of word vectors. Each set of vectors is treated as a channel and each filter is applied\n    \n\n**CNN-rand**: all words are randomly initialized and then modified during training\n\n**CNN-static**: pre-trained vectors with all the words— including the unknown ones that are randomly initialized—kept static and only the other parameters of the model are learned\n\n**CNN-non-static**: same as CNN-static but word vectors are fine-tuned\n\n**CNN-multichannel**: model with two sets of word vectors. Each set of vectors is treated as a channel and each filter is applied\n\nLet’s just first quickly look at how these different models look like in as a computational graph. The first three (i.e., CNN-rand, CNN-static and CNN-non-static) look pretty much the same:\n\n![](/primers/ai/assets/cnns-for-text-classification/SentenceClassificationConvNets-no_multi_channel.svg)\n\n*   The CNN-multichannel model uses two embedding layers, in one channel the embeddings are updated, in the second they remain static. It’s exactly the same network as above but duplicated and adding an extra layer do concatenate both results into a single vector:\n\n![](/primers/ai/assets/cnns-for-text-classification/SentenceClassificationConvNets-multi_channel.svg)",
      "order": 6,
      "orderInChapter": 3,
      "difficulty": 2,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "embedding",
        "cnn"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 365,
        "contentLength": 3291
      },
      "nextCards": [],
      "relatedCards": [
        "ai-ann-similarity-search-role-of-ann-in-recommendation-systems-3",
        "ai-ann-similarity-search-choosing-the-right-ann-algorithm-family-9",
        "ai-top-30-papers-scaling-laws-for-neural-language-models-22",
        "ai-ml-runtimes-architecture-27",
        "ai-ml-runtimes-example-pseudocode-flow-36"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/cnns-for-text-classification/#convolutional-neural-networks-for-sentence-classification",
      "scrapedAt": "2025-12-28T11:57:41.797Z",
      "siblings": [
        "ai-cnns-for-text-classification-convolutions-1",
        "ai-cnns-for-text-classification-pooling-2",
        "ai-cnns-for-text-classification-fully-connected-3",
        "ai-cnns-for-text-classification-1-d-convolutions-over-text-4",
        "ai-cnns-for-text-classification-pooling-5"
      ]
    },
    {
      "id": "ai-hmm-and-naive-bayes-training-1",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Relationship between Hidden Markov Models and Naive Bayes",
      "articleSlug": "hmm-and-naive-bayes",
      "chapter": "Naive Bayes Classifier",
      "title": "Training",
      "subtitle": "Naive Bayes Classifier",
      "contentHtml": "<ul>\n  <li>\n    <p>Training in Naive Bayes is mainly done by counting features and classes. Note that the procedure described below needs to be done for every class <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-386\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-387\"><span class=\"msubsup\" id=\"MathJax-Span-388\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-389\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-390\"><span class=\"mrow\" id=\"MathJax-Span-391\"><span class=\"mi\" id=\"MathJax-Span-392\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">y_{i}</script>.</p>\n  </li>\n  <li>\n    <p>To calculate the prior, we simple count how many samples in the training data fall into each class $y_{i}$ divided by the total number of samples:</p>\n  </li>\n</ul>\n<p>Training in Naive Bayes is mainly done by counting features and classes. Note that the procedure described below needs to be done for every class <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-386\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-387\"><span class=\"msubsup\" id=\"MathJax-Span-388\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-389\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-390\"><span class=\"mrow\" id=\"MathJax-Span-391\"><span class=\"mi\" id=\"MathJax-Span-392\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">y_{i}</script>.</p>\n<p>To calculate the prior, we simple count how many samples in the training data fall into each class $y_{i}$ divided by the total number of samples:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mfrac><msub><mi>N</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></mrow></msub><mi>N</mi></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-393\" style=\"width: 5.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.69em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.576em, 1004.69em, 3.023em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-394\"><span class=\"mi\" id=\"MathJax-Span-395\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-396\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-397\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-398\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-399\"><span class=\"mrow\" id=\"MathJax-Span-400\"><span class=\"mi\" id=\"MathJax-Span-401\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-402\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-403\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-404\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1001.25em, 4.482em, -999.997em); top: -4.789em; left: 50%; margin-left: -0.622em;\"><span class=\"msubsup\" id=\"MathJax-Span-405\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-406\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-407\"><span class=\"mrow\" id=\"MathJax-Span-408\"><span class=\"msubsup\" id=\"MathJax-Span-409\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-410\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-411\"><span class=\"mrow\" id=\"MathJax-Span-412\"><span class=\"mi\" id=\"MathJax-Span-413\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.73em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.362em;\"><span class=\"mi\" id=\"MathJax-Span-414\" style=\"font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1001.36em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 1.357em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left: 0px solid; width: 0px; height: 2.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><msub><mi>N</mi><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></mrow></msub><mi>N</mi></mfrac></math></span></span></div>\n<ul>\n  <li>To calculate the likelihood estimate, we count the number of times feature <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-415\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-416\"><span class=\"msubsup\" id=\"MathJax-Span-417\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-418\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-419\"><span class=\"mrow\" id=\"MathJax-Span-420\"><span class=\"mi\" id=\"MathJax-Span-421\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">w_{i}</script> appears among all features in all samples of class <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-422\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-423\"><span class=\"msubsup\" id=\"MathJax-Span-424\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-425\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-426\"><span class=\"mrow\" id=\"MathJax-Span-427\"><span class=\"mi\" id=\"MathJax-Span-428\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">y_{i}</script>:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2223;</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mfrac><mrow><mtext>count</mtext><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></mrow><mrow><munder><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2208;</mo><mi>X</mi></mrow></munder><mtext>count</mtext><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-429\" style=\"width: 13.753em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 11.461em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.732em, 1011.46em, 4.273em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-430\"><span class=\"mi\" id=\"MathJax-Span-431\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-432\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-433\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-434\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-435\"><span class=\"mrow\" id=\"MathJax-Span-436\"><span class=\"mi\" id=\"MathJax-Span-437\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-438\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-439\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-440\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-441\"><span class=\"mrow\" id=\"MathJax-Span-442\"><span class=\"mi\" id=\"MathJax-Span-443\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-444\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-445\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-446\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 6.565em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1004.74em, 4.378em, -999.997em); top: -4.685em; left: 50%; margin-left: -2.393em;\"><span class=\"mrow\" id=\"MathJax-Span-447\"><span class=\"mtext\" id=\"MathJax-Span-448\" style=\"font-family: STIXGeneral-Regular;\">count</span><span class=\"mo\" id=\"MathJax-Span-449\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-450\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-451\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-452\"><span class=\"mrow\" id=\"MathJax-Span-453\"><span class=\"mi\" id=\"MathJax-Span-454\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-455\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-456\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-457\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-458\"><span class=\"mrow\" id=\"MathJax-Span-459\"><span class=\"mi\" id=\"MathJax-Span-460\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-461\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.076em, 1006.41em, 5.315em, -999.997em); top: -3.279em; left: 50%; margin-left: -3.227em;\"><span class=\"mrow\" id=\"MathJax-Span-462\"><span class=\"munderover\" id=\"MathJax-Span-463\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0.263em;\"><span class=\"mo\" id=\"MathJax-Span-464\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.46em, 4.378em, -999.997em); top: -3.07em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-465\"><span class=\"mrow\" id=\"MathJax-Span-466\"><span class=\"msubsup\" id=\"MathJax-Span-467\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-468\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.904em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-469\"><span class=\"mrow\" id=\"MathJax-Span-470\"><span class=\"mi\" id=\"MathJax-Span-471\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-472\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-473\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-474\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">count</span><span class=\"mo\" id=\"MathJax-Span-475\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-476\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-477\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-478\"><span class=\"mrow\" id=\"MathJax-Span-479\"><span class=\"mi\" id=\"MathJax-Span-480\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-481\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-482\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-483\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-484\"><span class=\"mrow\" id=\"MathJax-Span-485\"><span class=\"mi\" id=\"MathJax-Span-486\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-487\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1006.57em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 6.565em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.309em; border-left: 0px solid; width: 0px; height: 4.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>∣</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mtext>count</mtext><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mrow><munder><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>∈</mo><mi>X</mi></mrow></munder><mtext>count</mtext><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mfrac></math></span></span></div>\n<ul>\n  <li>This will result in a big table of occurrences of features for all classes in the training data.</li>\n</ul>",
      "contentMarkdown": "*   Training in Naive Bayes is mainly done by counting features and classes. Note that the procedure described below needs to be done for every class yiyiy\\_{i}.\n    \n*   To calculate the prior, we simple count how many samples in the training data fall into each class $y\\_{i}$ divided by the total number of samples:\n    \n\nTraining in Naive Bayes is mainly done by counting features and classes. Note that the procedure described below needs to be done for every class yiyiy\\_{i}.\n\nTo calculate the prior, we simple count how many samples in the training data fall into each class $y\\_{i}$ divided by the total number of samples:\n\np(yi)\\=NyiNp(yi)\\=NyiN\n\n*   To calculate the likelihood estimate, we count the number of times feature wiwiw\\_{i} appears among all features in all samples of class yiyiy\\_{i}:\n\np(xi∣yi)\\=count(xi,yi)∑xi∈Xcount(xi,yi)p(xi∣yi)\\=count(xi,yi)∑xi∈Xcount(xi,yi)\n\n*   This will result in a big table of occurrences of features for all classes in the training data.",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 151,
        "contentLength": 26826
      },
      "nextCards": [
        "ai-hmm-and-naive-bayes-classification-2",
        "ai-hmm-and-naive-bayes-hmm-important-observations-3"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/hmm-and-naive-bayes/#training",
      "scrapedAt": "2025-12-28T11:57:47.028Z",
      "siblings": [
        "ai-hmm-and-naive-bayes-classification-2",
        "ai-hmm-and-naive-bayes-hmm-important-observations-3"
      ]
    },
    {
      "id": "ai-hmm-and-naive-bayes-classification-2",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Relationship between Hidden Markov Models and Naive Bayes",
      "articleSlug": "hmm-and-naive-bayes",
      "chapter": "Naive Bayes Classifier",
      "title": "Classification",
      "subtitle": "Naive Bayes Classifier",
      "contentHtml": "<ul>\n  <li>When given a new sample to classify, and assuming that it contains features <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>3</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>5</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-488\" style=\"width: 4.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.596em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1003.6em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-489\"><span class=\"msubsup\" id=\"MathJax-Span-490\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-491\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-492\"><span class=\"mrow\" id=\"MathJax-Span-493\"><span class=\"mn\" id=\"MathJax-Span-494\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-495\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-496\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-497\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-498\"><span class=\"mrow\" id=\"MathJax-Span-499\"><span class=\"mn\" id=\"MathJax-Span-500\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-501\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-502\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-503\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-504\"><span class=\"mrow\" id=\"MathJax-Span-505\"><span class=\"mn\" id=\"MathJax-Span-506\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>3</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>5</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">x_{1}, x_{3}, x_{5}</script>, we need to compute, for each class <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-507\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-508\"><span class=\"msubsup\" id=\"MathJax-Span-509\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-510\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-511\"><span class=\"mrow\" id=\"MathJax-Span-512\"><span class=\"mi\" id=\"MathJax-Span-513\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">y_{i}</script>:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2223;</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>3</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>5</mn></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-514\" style=\"width: 7.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.357em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.3em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-515\"><span class=\"mi\" id=\"MathJax-Span-516\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-517\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-518\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-519\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-520\"><span class=\"mrow\" id=\"MathJax-Span-521\"><span class=\"mi\" id=\"MathJax-Span-522\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-523\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-524\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-525\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-526\"><span class=\"mrow\" id=\"MathJax-Span-527\"><span class=\"mn\" id=\"MathJax-Span-528\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-529\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-530\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-531\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-532\"><span class=\"mrow\" id=\"MathJax-Span-533\"><span class=\"mn\" id=\"MathJax-Span-534\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-535\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-536\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-537\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-538\"><span class=\"mrow\" id=\"MathJax-Span-539\"><span class=\"mn\" id=\"MathJax-Span-540\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-541\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>∣</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>3</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>5</mn></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>This is decomposed into:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2223;</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>3</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>5</mn></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x22C5;</mo><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2223;</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x22C5;</mo><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2223;</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>3</mn></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x22C5;</mo><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2223;</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>5</mn></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-542\" style=\"width: 28.076em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 23.388em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1023.34em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-543\"><span class=\"mi\" id=\"MathJax-Span-544\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-545\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-546\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-547\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-548\"><span class=\"mrow\" id=\"MathJax-Span-549\"><span class=\"mi\" id=\"MathJax-Span-550\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-551\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-552\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-553\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-554\"><span class=\"mrow\" id=\"MathJax-Span-555\"><span class=\"mn\" id=\"MathJax-Span-556\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-557\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-558\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-559\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-560\"><span class=\"mrow\" id=\"MathJax-Span-561\"><span class=\"mn\" id=\"MathJax-Span-562\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-563\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-564\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-565\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-566\"><span class=\"mrow\" id=\"MathJax-Span-567\"><span class=\"mn\" id=\"MathJax-Span-568\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-569\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-570\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-571\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">p</span><span class=\"mo\" id=\"MathJax-Span-572\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-573\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-574\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-575\"><span class=\"mrow\" id=\"MathJax-Span-576\"><span class=\"mi\" id=\"MathJax-Span-577\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-578\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-579\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-580\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">p</span><span class=\"mo\" id=\"MathJax-Span-581\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-582\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-583\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-584\"><span class=\"mrow\" id=\"MathJax-Span-585\"><span class=\"mi\" id=\"MathJax-Span-586\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-587\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-588\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-589\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-590\"><span class=\"mrow\" id=\"MathJax-Span-591\"><span class=\"mn\" id=\"MathJax-Span-592\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-593\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-594\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-595\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">p</span><span class=\"mo\" id=\"MathJax-Span-596\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-597\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-598\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-599\"><span class=\"mrow\" id=\"MathJax-Span-600\"><span class=\"mi\" id=\"MathJax-Span-601\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-602\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-603\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-604\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-605\"><span class=\"mrow\" id=\"MathJax-Span-606\"><span class=\"mn\" id=\"MathJax-Span-607\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">3</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-608\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-609\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-610\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">p</span><span class=\"mo\" id=\"MathJax-Span-611\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-612\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-613\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-614\"><span class=\"mrow\" id=\"MathJax-Span-615\"><span class=\"mi\" id=\"MathJax-Span-616\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-617\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-618\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-619\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-620\"><span class=\"mrow\" id=\"MathJax-Span-621\"><span class=\"mn\" id=\"MathJax-Span-622\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">5</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-623\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>∣</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>3</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>5</mn></mrow></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>∣</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>∣</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>3</mn></mrow></msub><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>∣</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>5</mn></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>Again, this is calculated for each class <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-624\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-625\"><span class=\"msubsup\" id=\"MathJax-Span-626\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-627\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-628\"><span class=\"mrow\" id=\"MathJax-Span-629\"><span class=\"mi\" id=\"MathJax-Span-630\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">y_{i}</script>, and we assign to the new observed sample the class that has the highest score.</li>\n</ul>",
      "contentMarkdown": "*   When given a new sample to classify, and assuming that it contains features x1,x3,x5x1,x3,x5x\\_{1}, x\\_{3}, x\\_{5}, we need to compute, for each class yiyiy\\_{i}:\n\np(yi∣x1,x3,x5)p(yi∣x1,x3,x5)\n\n*   This is decomposed into:\n\np(yi∣x1,x3,x5)\\=p(yi)⋅p(yi∣x1)⋅p(yi∣x3)⋅p(yi∣x5)p(yi∣x1,x3,x5)\\=p(yi)⋅p(yi∣x1)⋅p(yi∣x3)⋅p(yi∣x5)\n\n*   Again, this is calculated for each class yiyiy\\_{i}, and we assign to the new observed sample the class that has the highest score.",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 56,
        "contentLength": 30161
      },
      "nextCards": [
        "ai-hmm-and-naive-bayes-hmm-important-observations-3"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/hmm-and-naive-bayes/#classification",
      "scrapedAt": "2025-12-28T11:57:47.028Z",
      "siblings": [
        "ai-hmm-and-naive-bayes-training-1",
        "ai-hmm-and-naive-bayes-hmm-important-observations-3"
      ]
    },
    {
      "id": "ai-hmm-and-naive-bayes-hmm-important-observations-3",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Relationship between Hidden Markov Models and Naive Bayes",
      "articleSlug": "hmm-and-naive-bayes",
      "chapter": "Hidden Markov Model",
      "title": "HMM Important Observations",
      "subtitle": "Hidden Markov Model",
      "contentHtml": "<ul>\n  <li>\n    <p>The main idea of this post was to see the connection between the Naive Bayes classifier and the HMM as a sequence classifier</p>\n  </li>\n  <li>\n    <p>If we make the hidden state of HMM fixed, we will have a Naive Bayes model.</p>\n  </li>\n  <li>\n    <p>There is only one feature at each word/observation in the sequence, namely the identity i.e., the value of the respective observation.</p>\n  </li>\n  <li>\n    <p>Each state depends only on its immediate predecessor, that is, each state <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-88-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1948\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.461em, 1000.58em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1949\"><span class=\"msubsup\" id=\"MathJax-Span-1950\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1951\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-1952\"><span class=\"mrow\" id=\"MathJax-Span-1953\"><span class=\"mi\" id=\"MathJax-Span-1954\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-88\">t_{i}</script> is independent of all its ancestors <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-89-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2212;</mo><mn>2</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1955\" style=\"width: 6.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1005.47em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1956\"><span class=\"msubsup\" id=\"MathJax-Span-1957\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1958\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-1959\"><span class=\"mrow\" id=\"MathJax-Span-1960\"><span class=\"mn\" id=\"MathJax-Span-1961\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1962\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1963\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1964\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-1965\"><span class=\"mrow\" id=\"MathJax-Span-1966\"><span class=\"mn\" id=\"MathJax-Span-1967\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1968\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-1969\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-1970\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1971\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1972\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-1973\"><span class=\"mrow\" id=\"MathJax-Span-1974\"><span class=\"mi\" id=\"MathJax-Span-1975\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1976\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1977\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>−</mo><mn>2</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-89\">t_{1}, t_{2}, \\dots, t_{i-2}</script> given its previous state <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-90-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1978\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.461em, 1001.41em, 2.555em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1979\"><span class=\"msubsup\" id=\"MathJax-Span-1980\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1981\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-1982\"><span class=\"mrow\" id=\"MathJax-Span-1983\"><span class=\"mi\" id=\"MathJax-Span-1984\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1985\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1986\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-90\">t_{i-1}</script>.</p>\n  </li>\n  <li>\n    <p>Each observation variable <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-91-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1987\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1988\"><span class=\"msubsup\" id=\"MathJax-Span-1989\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1990\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1991\"><span class=\"mrow\" id=\"MathJax-Span-1992\"><span class=\"mi\" id=\"MathJax-Span-1993\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-91\">w_{i}</script> depends only on the current state <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-92-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1994\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.461em, 1000.58em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1995\"><span class=\"msubsup\" id=\"MathJax-Span-1996\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1997\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-1998\"><span class=\"mrow\" id=\"MathJax-Span-1999\"><span class=\"mi\" id=\"MathJax-Span-2000\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-92\">t_{i}</script>.</p>\n  </li>\n</ul>\n<p>The main idea of this post was to see the connection between the Naive Bayes classifier and the HMM as a sequence classifier</p>\n<p>If we make the hidden state of HMM fixed, we will have a Naive Bayes model.</p>\n<p>There is only one feature at each word/observation in the sequence, namely the identity i.e., the value of the respective observation.</p>\n<p>Each state depends only on its immediate predecessor, that is, each state <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-88-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1948\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.461em, 1000.58em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1949\"><span class=\"msubsup\" id=\"MathJax-Span-1950\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1951\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-1952\"><span class=\"mrow\" id=\"MathJax-Span-1953\"><span class=\"mi\" id=\"MathJax-Span-1954\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-88\">t_{i}</script> is independent of all its ancestors <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-89-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msub><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><msub><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2212;</mo><mn>2</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1955\" style=\"width: 6.565em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.513em, 1005.47em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1956\"><span class=\"msubsup\" id=\"MathJax-Span-1957\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1958\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-1959\"><span class=\"mrow\" id=\"MathJax-Span-1960\"><span class=\"mn\" id=\"MathJax-Span-1961\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1962\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1963\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1964\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-1965\"><span class=\"mrow\" id=\"MathJax-Span-1966\"><span class=\"mn\" id=\"MathJax-Span-1967\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1968\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-1969\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-1970\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1971\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1972\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-1973\"><span class=\"mrow\" id=\"MathJax-Span-1974\"><span class=\"mi\" id=\"MathJax-Span-1975\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1976\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1977\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>−</mo><mn>2</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-89\">t_{1}, t_{2}, \\dots, t_{i-2}</script> given its previous state <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-90-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1978\" style=\"width: 1.721em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.461em, 1001.41em, 2.555em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1979\"><span class=\"msubsup\" id=\"MathJax-Span-1980\"><span style=\"display: inline-block; position: relative; width: 1.409em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1981\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-1982\"><span class=\"mrow\" id=\"MathJax-Span-1983\"><span class=\"mi\" id=\"MathJax-Span-1984\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1985\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1986\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-90\">t_{i-1}</script>.</p>\n<p>Each observation variable <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-91-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1987\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1988\"><span class=\"msubsup\" id=\"MathJax-Span-1989\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1990\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1991\"><span class=\"mrow\" id=\"MathJax-Span-1992\"><span class=\"mi\" id=\"MathJax-Span-1993\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-91\">w_{i}</script> depends only on the current state <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-92-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1994\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.461em, 1000.58em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1995\"><span class=\"msubsup\" id=\"MathJax-Span-1996\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.336em, 1000.32em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1997\" style=\"font-family: STIXGeneral-Italic;\">t<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-1998\"><span class=\"mrow\" id=\"MathJax-Span-1999\"><span class=\"mi\" id=\"MathJax-Span-2000\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>t</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-92\">t_{i}</script>.</p>",
      "contentMarkdown": "*   The main idea of this post was to see the connection between the Naive Bayes classifier and the HMM as a sequence classifier\n    \n*   If we make the hidden state of HMM fixed, we will have a Naive Bayes model.\n    \n*   There is only one feature at each word/observation in the sequence, namely the identity i.e., the value of the respective observation.\n    \n*   Each state depends only on its immediate predecessor, that is, each state titit\\_{i} is independent of all its ancestors t1,t2,…,ti−2t1,t2,…,ti−2t\\_{1}, t\\_{2}, \\\\dots, t\\_{i-2} given its previous state ti−1ti−1t\\_{i-1}.\n    \n*   Each observation variable wiwiw\\_{i} depends only on the current state titit\\_{i}.\n    \n\nThe main idea of this post was to see the connection between the Naive Bayes classifier and the HMM as a sequence classifier\n\nIf we make the hidden state of HMM fixed, we will have a Naive Bayes model.\n\nThere is only one feature at each word/observation in the sequence, namely the identity i.e., the value of the respective observation.\n\nEach state depends only on its immediate predecessor, that is, each state titit\\_{i} is independent of all its ancestors t1,t2,…,ti−2t1,t2,…,ti−2t\\_{1}, t\\_{2}, \\\\dots, t\\_{i-2} given its previous state ti−1ti−1t\\_{i-1}.\n\nEach observation variable wiwiw\\_{i} depends only on the current state titit\\_{i}.",
      "order": 3,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 203,
        "contentLength": 28359
      },
      "nextCards": [],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/hmm-and-naive-bayes/#hmm-important-observations",
      "scrapedAt": "2025-12-28T11:57:47.028Z",
      "siblings": [
        "ai-hmm-and-naive-bayes-training-1",
        "ai-hmm-and-naive-bayes-classification-2"
      ]
    },
    {
      "id": "ai-maximum-entropy-markov-models-and-logistic-reg-training-1",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Maximum Entropy Markov Models",
      "articleSlug": "maximum-entropy-markov-models-and-logistic-reg",
      "chapter": "Logistic Regression",
      "title": "Training",
      "subtitle": "Logistic Regression",
      "contentHtml": "<ul>\n  <li>\n    <p>By training the logistic regression classifier we want to find the ideal weights for each feature, that is, the weights that will make training examples fit best the classes to which they belong.</p>\n  </li>\n  <li>\n    <p>Logistic regression is trained with conditional maximum likelihood estimation. This means that we will choose the parameters <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-322\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-323\"><span class=\"mi\" id=\"MathJax-Span-324\" style=\"font-family: STIXGeneral-Italic;\">w</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">w</script> that maximize the probability of the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-325\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-326\"><span class=\"mi\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Italic;\">y</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">y</script> labels in the training data given the observations <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-328\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-329\"><span class=\"mi\" id=\"MathJax-Span-330\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">x</script>:</p>\n  </li>\n</ul>\n<p>By training the logistic regression classifier we want to find the ideal weights for each feature, that is, the weights that will make training examples fit best the classes to which they belong.</p>\n<p>Logistic regression is trained with conditional maximum likelihood estimation. This means that we will choose the parameters <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-322\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-323\"><span class=\"mi\" id=\"MathJax-Span-324\" style=\"font-family: STIXGeneral-Italic;\">w</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">w</script> that maximize the probability of the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-325\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-326\"><span class=\"mi\" id=\"MathJax-Span-327\" style=\"font-family: STIXGeneral-Italic;\">y</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">y</script> labels in the training data given the observations <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-328\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-329\"><span class=\"mi\" id=\"MathJax-Span-330\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>x</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">x</script>:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mo>=</mo><munder><mrow><mi>arg</mi><mo>&amp;#x2061;</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo></mrow><mi>w</mi></munder><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></munder><mi>log</mi><mo>&amp;#x2061;</mo><mtext>&amp;#xA0;</mtext><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msup><mo>&amp;#x2223;</mo><msup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-331\" style=\"width: 14.586em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 12.138em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1012.09em, 3.805em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-332\"><span class=\"texatom\" id=\"MathJax-Span-333\"><span class=\"mrow\" id=\"MathJax-Span-334\"><span class=\"munderover\" id=\"MathJax-Span-335\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-336\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.159em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-337\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-338\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munder\" id=\"MathJax-Span-339\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1003.18em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-340\"><span class=\"mi\" id=\"MathJax-Span-341\" style=\"font-family: STIXGeneral-Regular;\">arg</span><span class=\"mo\" id=\"MathJax-Span-342\"></span><span class=\"mo\" id=\"MathJax-Span-343\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">max</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.544em, 1000.47em, 4.273em, -999.997em); top: -3.122em; left: 1.357em;\"><span class=\"mi\" id=\"MathJax-Span-344\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-345\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-346\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.43em, -999.997em); top: -2.862em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-347\"><span class=\"mrow\" id=\"MathJax-Span-348\"><span class=\"mi\" id=\"MathJax-Span-349\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-350\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-351\"></span><span class=\"mtext\" id=\"MathJax-Span-352\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-353\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-354\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-355\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-356\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-357\"><span class=\"mrow\" id=\"MathJax-Span-358\"><span class=\"mi\" id=\"MathJax-Span-359\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-360\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-361\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-362\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-363\"><span class=\"mrow\" id=\"MathJax-Span-364\"><span class=\"mi\" id=\"MathJax-Span-365\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-366\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.747em; border-left: 0px solid; width: 0px; height: 3.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>=</mo><munder><mrow><mi>arg</mi><mo>⁡</mo><mo movablelimits=\"true\" form=\"prefix\">max</mo></mrow><mi>w</mi></munder><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></munder><mi>log</mi><mo>⁡</mo><mtext>&nbsp;</mtext><mi>P</mi><mo stretchy=\"false\">(</mo><msup><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msup><mo>∣</mo><msup><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msup><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>The objective function to maximize is:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>L</mi><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><munder><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></munder><mi>log</mi><mo>&amp;#x2061;</mo><mtext>&amp;#xA0;</mtext><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msup><mo>&amp;#x2223;</mo><msup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-367\" style=\"width: 12.034em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.003em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1009.95em, 3.805em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-368\"><span class=\"mi\" id=\"MathJax-Span-369\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-370\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-371\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-372\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-373\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-374\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-375\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.43em, -999.997em); top: -2.862em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-376\"><span class=\"mrow\" id=\"MathJax-Span-377\"><span class=\"mi\" id=\"MathJax-Span-378\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-379\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-380\"></span><span class=\"mtext\" id=\"MathJax-Span-381\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-382\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-383\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-384\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-385\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-386\"><span class=\"mrow\" id=\"MathJax-Span-387\"><span class=\"mi\" id=\"MathJax-Span-388\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-389\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-390\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-391\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-392\"><span class=\"mrow\" id=\"MathJax-Span-393\"><span class=\"mi\" id=\"MathJax-Span-394\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-395\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.747em; border-left: 0px solid; width: 0px; height: 3.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>L</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munder><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></munder><mi>log</mi><mo>⁡</mo><mtext>&nbsp;</mtext><mi>P</mi><mo stretchy=\"false\">(</mo><msup><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msup><mo>∣</mo><msup><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msup><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>… which by replacing with expanded form presented before and by applying the division log rules, takes the following form:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>L</mi><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><munder><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></munder><mi>log</mi><mo>&amp;#x2061;</mo><mi>exp</mi><mo>&amp;#x2061;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;2.047em&quot; minsize=&quot;2.047em&quot;>(</mo></mrow><munderover><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></munderover><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x22C5;</mo><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msup><mo>,</mo><msup><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msup><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;2.047em&quot; minsize=&quot;2.047em&quot;>)</mo></mrow><mo>&amp;#x2212;</mo><munder><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></munder><mi>log</mi><mo>&amp;#x2061;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><munder><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msup><mi>y</mi><mo>&amp;#x2032;</mo></msup><mo>&amp;#x2208;</mo><mi>Y</mi></mrow></munder><mi>exp</mi><mo>&amp;#x2061;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;2.047em&quot; minsize=&quot;2.047em&quot;>(</mo></mrow><munderover><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></munderover><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x22C5;</mo><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><msup><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msup><mo>,</mo><msup><mi>y</mi><mrow><mo class=&quot;MJX-variant&quot;>&amp;#x2032;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></mrow></msup><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;2.047em&quot; minsize=&quot;2.047em&quot;>)</mo></mrow></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-396\" style=\"width: 38.648em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 32.19em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.19em, 1032.03em, 5.732em, -999.997em); top: -4.112em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-397\"><span class=\"mi\" id=\"MathJax-Span-398\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-399\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-400\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-401\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-402\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-403\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-404\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.43em, -999.997em); top: -2.862em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-405\"><span class=\"mrow\" id=\"MathJax-Span-406\"><span class=\"mi\" id=\"MathJax-Span-407\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-408\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-409\"></span><span class=\"mi\" id=\"MathJax-Span-410\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">exp</span><span class=\"mo\" id=\"MathJax-Span-411\"></span><span class=\"texatom\" id=\"MathJax-Span-412\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-413\"><span class=\"mo\" id=\"MathJax-Span-414\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">(</span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-415\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-416\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-417\"><span class=\"mrow\" id=\"MathJax-Span-418\"><span class=\"mi\" id=\"MathJax-Span-419\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-420\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-421\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-422\"><span class=\"mrow\" id=\"MathJax-Span-423\"><span class=\"mi\" id=\"MathJax-Span-424\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-425\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-426\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-427\"><span class=\"mrow\" id=\"MathJax-Span-428\"><span class=\"mi\" id=\"MathJax-Span-429\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-430\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-431\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-432\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-433\"><span class=\"mrow\" id=\"MathJax-Span-434\"><span class=\"mi\" id=\"MathJax-Span-435\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-436\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-437\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-438\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-439\"><span class=\"mrow\" id=\"MathJax-Span-440\"><span class=\"mi\" id=\"MathJax-Span-441\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-442\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-443\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-444\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-445\"><span class=\"mrow\" id=\"MathJax-Span-446\"><span class=\"mi\" id=\"MathJax-Span-447\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-448\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"texatom\" id=\"MathJax-Span-449\" style=\"\"><span class=\"mrow\" id=\"MathJax-Span-450\"><span class=\"mo\" id=\"MathJax-Span-451\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">)</span></span></span></span><span class=\"mo\" id=\"MathJax-Span-452\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"munderover\" id=\"MathJax-Span-453\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-454\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.43em, -999.997em); top: -2.862em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-455\"><span class=\"mrow\" id=\"MathJax-Span-456\"><span class=\"mi\" id=\"MathJax-Span-457\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-458\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-459\"></span><span class=\"texatom\" id=\"MathJax-Span-460\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-461\"><span class=\"munderover\" id=\"MathJax-Span-462\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-463\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.336em, 1001.46em, 4.43em, -999.997em); top: -2.81em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-464\"><span class=\"mrow\" id=\"MathJax-Span-465\"><span class=\"msup\" id=\"MathJax-Span-466\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-467\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-468\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-469\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-470\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-471\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">exp</span><span class=\"mo\" id=\"MathJax-Span-472\"></span><span class=\"texatom\" id=\"MathJax-Span-473\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-474\"><span class=\"mo\" id=\"MathJax-Span-475\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">(</span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-476\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-477\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-478\"><span class=\"mrow\" id=\"MathJax-Span-479\"><span class=\"mi\" id=\"MathJax-Span-480\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-481\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-482\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-483\"><span class=\"mrow\" id=\"MathJax-Span-484\"><span class=\"mi\" id=\"MathJax-Span-485\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-486\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-487\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-488\"><span class=\"mrow\" id=\"MathJax-Span-489\"><span class=\"mi\" id=\"MathJax-Span-490\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-491\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-492\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-493\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-494\"><span class=\"mrow\" id=\"MathJax-Span-495\"><span class=\"mi\" id=\"MathJax-Span-496\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-497\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-498\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-499\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-500\"><span class=\"mrow\" id=\"MathJax-Span-501\"><span class=\"mi\" id=\"MathJax-Span-502\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-503\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-504\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-505\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.471em;\"><span class=\"mrow\" id=\"MathJax-Span-506\"><span class=\"mo\" id=\"MathJax-Span-507\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span class=\"texatom\" id=\"MathJax-Span-508\"><span class=\"mrow\" id=\"MathJax-Span-509\"><span class=\"mi\" id=\"MathJax-Span-510\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-511\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"texatom\" id=\"MathJax-Span-512\" style=\"\"><span class=\"mrow\" id=\"MathJax-Span-513\"><span class=\"mo\" id=\"MathJax-Span-514\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">)</span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.809em; border-left: 0px solid; width: 0px; height: 4.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>L</mi><mo stretchy=\"false\">(</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munder><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></munder><mi>log</mi><mo>⁡</mo><mi>exp</mi><mo>⁡</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo maxsize=\"2.047em\" minsize=\"2.047em\">(</mo></mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></munderover><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>⋅</mo><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msup><mo>,</mo><msup><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msup><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo maxsize=\"2.047em\" minsize=\"2.047em\">)</mo></mrow><mo>−</mo><munder><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></munder><mi>log</mi><mo>⁡</mo><mrow class=\"MJX-TeXAtom-ORD\"><munder><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><msup><mi>y</mi><mo>′</mo></msup><mo>∈</mo><mi>Y</mi></mrow></munder><mi>exp</mi><mo>⁡</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo maxsize=\"2.047em\" minsize=\"2.047em\">(</mo></mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></munderover><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>⋅</mo><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msup><mo>,</mo><msup><mi>y</mi><mrow><mo class=\"MJX-variant\">′</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></mrow></msup><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo maxsize=\"2.047em\" minsize=\"2.047em\">)</mo></mrow></mrow></math></span></span></div>\n<ul>\n  <li>Maximize this objective, i.e. finding the optimal weights, is typically solved by methods like stochastic gradient ascent, L-BFGS, or conjugate gradient.</li>\n</ul>",
      "contentMarkdown": "*   By training the logistic regression classifier we want to find the ideal weights for each feature, that is, the weights that will make training examples fit best the classes to which they belong.\n    \n*   Logistic regression is trained with conditional maximum likelihood estimation. This means that we will choose the parameters www that maximize the probability of the yyy labels in the training data given the observations xxx:\n    \n\nBy training the logistic regression classifier we want to find the ideal weights for each feature, that is, the weights that will make training examples fit best the classes to which they belong.\n\nLogistic regression is trained with conditional maximum likelihood estimation. This means that we will choose the parameters www that maximize the probability of the yyy labels in the training data given the observations xxx:\n\nŵ \\=argmaxw∑jlog P(yj∣yj)w^\\=arg⁡maxw∑jlog⁡ P(yj∣yj)\n\n*   The objective function to maximize is:\n\nL(w)\\=∑jlog P(yj∣yj)L(w)\\=∑jlog⁡ P(yj∣yj)\n\n*   … which by replacing with expanded form presented before and by applying the division log rules, takes the following form:\n\nL(w)\\=∑jlogexp(∑i\\=1Nwi⋅fi(xj,yj))−∑jlog∑y′∈Yexp(∑i\\=1Nwi⋅fi(xj,y′j))L(w)\\=∑jlog⁡exp⁡(∑i\\=1Nwi⋅fi(xj,yj))−∑jlog⁡∑y′∈Yexp⁡(∑i\\=1Nwi⋅fi(xj,y′j))\n\n*   Maximize this objective, i.e. finding the optimal weights, is typically solved by methods like stochastic gradient ascent, L-BFGS, or conjugate gradient.",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 194,
        "contentLength": 45245
      },
      "nextCards": [
        "ai-maximum-entropy-markov-models-and-logistic-reg-classification-2",
        "ai-maximum-entropy-markov-models-and-logistic-reg-features-functions-3"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/maximum-entropy-markov-models-and-logistic-reg/#training",
      "scrapedAt": "2025-12-28T11:57:52.135Z",
      "siblings": [
        "ai-maximum-entropy-markov-models-and-logistic-reg-classification-2",
        "ai-maximum-entropy-markov-models-and-logistic-reg-features-functions-3",
        "ai-maximum-entropy-markov-models-and-logistic-reg-training-and-decoding-4",
        "ai-maximum-entropy-markov-models-and-logistic-reg-memm-important-observations-5"
      ]
    },
    {
      "id": "ai-maximum-entropy-markov-models-and-logistic-reg-classification-2",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Maximum Entropy Markov Models",
      "articleSlug": "maximum-entropy-markov-models-and-logistic-reg",
      "chapter": "Logistic Regression",
      "title": "Classification",
      "subtitle": "Logistic Regression",
      "contentHtml": "<ul>\n  <li>In classification, logistic regression chooses a class by computing the probability of a given observation belonging to each of all the possible classes, then we can choose the one that yields the maximum probability.</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mo>=</mo><munder><mrow><mi>arg</mi><mo>&amp;#x2061;</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo></mrow><mrow><mi>y</mi><mo>&amp;#x2208;</mo><mi>Y</mi></mrow></munder><mtext>&amp;#xA0;</mtext><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>y</mi><mo>&amp;#x2223;</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-515\" style=\"width: 9.898em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 8.232em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1008.18em, 3.544em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-516\"><span class=\"texatom\" id=\"MathJax-Span-517\"><span class=\"mrow\" id=\"MathJax-Span-518\"><span class=\"munderover\" id=\"MathJax-Span-519\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-520\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.055em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-521\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-522\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munder\" id=\"MathJax-Span-523\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1003.18em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-524\"><span class=\"mi\" id=\"MathJax-Span-525\" style=\"font-family: STIXGeneral-Regular;\">arg</span><span class=\"mo\" id=\"MathJax-Span-526\"></span><span class=\"mo\" id=\"MathJax-Span-527\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">max</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.25em, 4.43em, -999.997em); top: -3.122em; left: 0.992em;\"><span class=\"mrow\" id=\"MathJax-Span-528\"><span class=\"mi\" id=\"MathJax-Span-529\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span class=\"mo\" id=\"MathJax-Span-530\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-531\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-532\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-533\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-534\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-535\" style=\"font-family: STIXGeneral-Italic;\">y</span><span class=\"mo\" id=\"MathJax-Span-536\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-537\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-538\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.434em; border-left: 0px solid; width: 0px; height: 2.378em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>=</mo><munder><mrow><mi>arg</mi><mo>⁡</mo><mo movablelimits=\"true\" form=\"prefix\">max</mo></mrow><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><mtext>&nbsp;</mtext><mi>P</mi><mo stretchy=\"false\">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo stretchy=\"false\">)</mo></math></span></span></div>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mo>=</mo><munder><mrow><mi>arg</mi><mo>&amp;#x2061;</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo></mrow><mrow><mi>y</mi><mo>&amp;#x2208;</mo><mi>Y</mi></mrow></munder><mfrac><mrow><mi>exp</mi><mo>&amp;#x2061;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;2.047em&quot; minsize=&quot;2.047em&quot;>(</mo></mrow><munderover><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></munderover><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x22C5;</mo><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;2.047em&quot; minsize=&quot;2.047em&quot;>)</mo></mrow></mrow><mrow><munder><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msup><mi>y</mi><mo>&amp;#x2032;</mo></msup><mo>&amp;#x2208;</mo><mi>Y</mi></mrow></munder><mi>exp</mi><mo>&amp;#x2061;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;2.047em&quot; minsize=&quot;2.047em&quot;>(</mo></mrow><munderover><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></munderover><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x22C5;</mo><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><msup><mi>y</mi><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;2.047em&quot; minsize=&quot;2.047em&quot;>)</mo></mrow></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-539\" style=\"width: 19.378em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 16.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(-1.039em, 1016.15em, 5.055em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-540\"><span class=\"texatom\" id=\"MathJax-Span-541\"><span class=\"mrow\" id=\"MathJax-Span-542\"><span class=\"munderover\" id=\"MathJax-Span-543\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-544\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.06em; left: 0.055em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-545\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-546\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munder\" id=\"MathJax-Span-547\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1003.18em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-548\"><span class=\"mi\" id=\"MathJax-Span-549\" style=\"font-family: STIXGeneral-Regular;\">arg</span><span class=\"mo\" id=\"MathJax-Span-550\"></span><span class=\"mo\" id=\"MathJax-Span-551\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">max</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.25em, 4.43em, -999.997em); top: -3.122em; left: 0.992em;\"><span class=\"mrow\" id=\"MathJax-Span-552\"><span class=\"mi\" id=\"MathJax-Span-553\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span class=\"mo\" id=\"MathJax-Span-554\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-555\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mfrac\" id=\"MathJax-Span-556\"><span style=\"display: inline-block; position: relative; width: 11.044em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(2.294em, 1008.75em, 5.211em, -999.997em); top: -5.518em; left: 50%; margin-left: -4.477em;\"><span class=\"mrow\" id=\"MathJax-Span-557\"><span class=\"mi\" id=\"MathJax-Span-558\" style=\"font-family: STIXGeneral-Regular;\">exp</span><span class=\"mo\" id=\"MathJax-Span-559\"></span><span class=\"texatom\" id=\"MathJax-Span-560\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-561\"><span class=\"mo\" id=\"MathJax-Span-562\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">(</span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-563\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-564\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -3.07em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-565\"><span class=\"mrow\" id=\"MathJax-Span-566\"><span class=\"mi\" id=\"MathJax-Span-567\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-568\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-569\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.52em, 4.169em, -999.997em); top: -4.997em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-570\"><span class=\"mrow\" id=\"MathJax-Span-571\"><span class=\"mi\" id=\"MathJax-Span-572\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-573\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-574\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-575\"><span class=\"mrow\" id=\"MathJax-Span-576\"><span class=\"mi\" id=\"MathJax-Span-577\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-578\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-579\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-580\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-581\"><span class=\"mrow\" id=\"MathJax-Span-582\"><span class=\"mi\" id=\"MathJax-Span-583\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-584\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-585\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-586\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-587\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">y</span><span class=\"mo\" id=\"MathJax-Span-588\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"texatom\" id=\"MathJax-Span-589\" style=\"\"><span class=\"mrow\" id=\"MathJax-Span-590\"><span class=\"mo\" id=\"MathJax-Span-591\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">)</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(2.294em, 1010.73em, 5.367em, -999.997em); top: -2.497em; left: 50%; margin-left: -5.466em;\"><span class=\"mrow\" id=\"MathJax-Span-592\"><span class=\"munderover\" id=\"MathJax-Span-593\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0.263em;\"><span class=\"mo\" id=\"MathJax-Span-594\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1001.46em, 4.43em, -999.997em); top: -3.07em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-595\"><span class=\"mrow\" id=\"MathJax-Span-596\"><span class=\"msup\" id=\"MathJax-Span-597\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-598\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.216em; left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-599\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-600\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-601\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-602\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">exp</span><span class=\"mo\" id=\"MathJax-Span-603\"></span><span class=\"texatom\" id=\"MathJax-Span-604\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-605\"><span class=\"mo\" id=\"MathJax-Span-606\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">(</span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-607\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-608\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -3.07em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-609\"><span class=\"mrow\" id=\"MathJax-Span-610\"><span class=\"mi\" id=\"MathJax-Span-611\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-612\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-613\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.52em, 4.169em, -999.997em); top: -4.997em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-614\"><span class=\"mrow\" id=\"MathJax-Span-615\"><span class=\"mi\" id=\"MathJax-Span-616\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-617\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-618\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-619\"><span class=\"mrow\" id=\"MathJax-Span-620\"><span class=\"mi\" id=\"MathJax-Span-621\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-622\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-623\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-624\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-625\"><span class=\"mrow\" id=\"MathJax-Span-626\"><span class=\"mi\" id=\"MathJax-Span-627\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-628\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-629\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-630\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msup\" id=\"MathJax-Span-631\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-632\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.471em;\"><span class=\"mo\" id=\"MathJax-Span-633\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-634\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"texatom\" id=\"MathJax-Span-635\" style=\"\"><span class=\"mrow\" id=\"MathJax-Span-636\"><span class=\"mo\" id=\"MathJax-Span-637\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">)</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1011.04em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 11.044em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -3.309em; border-left: 0px solid; width: 0px; height: 7.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">^</mo></mover></mrow><mo>=</mo><munder><mrow><mi>arg</mi><mo>⁡</mo><mo movablelimits=\"true\" form=\"prefix\">max</mo></mrow><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo maxsize=\"2.047em\" minsize=\"2.047em\">(</mo></mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></munderover><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>⋅</mo><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo maxsize=\"2.047em\" minsize=\"2.047em\">)</mo></mrow></mrow><mrow><munder><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><msup><mi>y</mi><mo>′</mo></msup><mo>∈</mo><mi>Y</mi></mrow></munder><mi>exp</mi><mo>⁡</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo maxsize=\"2.047em\" minsize=\"2.047em\">(</mo></mrow><munderover><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></munderover><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>⋅</mo><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msup><mi>y</mi><mo>′</mo></msup><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo maxsize=\"2.047em\" minsize=\"2.047em\">)</mo></mrow></mrow></mfrac></math></span></span></div>",
      "contentMarkdown": "*   In classification, logistic regression chooses a class by computing the probability of a given observation belonging to each of all the possible classes, then we can choose the one that yields the maximum probability.\n\nŷ \\=argmaxy∈Y P(y∣x)y^\\=arg⁡maxy∈Y P(y∣x)\n\nŷ \\=argmaxy∈Yexp(∑i\\=1Nwi⋅fi(x,y))∑y′∈Yexp(∑i\\=1Nwi⋅fi(x,y′))y^\\=arg⁡maxy∈Yexp⁡(∑i\\=1Nwi⋅fi(x,y))∑y′∈Yexp⁡(∑i\\=1Nwi⋅fi(x,y′))",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 41,
        "contentLength": 25258
      },
      "nextCards": [
        "ai-maximum-entropy-markov-models-and-logistic-reg-features-functions-3",
        "ai-maximum-entropy-markov-models-and-logistic-reg-training-and-decoding-4"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/maximum-entropy-markov-models-and-logistic-reg/#classification",
      "scrapedAt": "2025-12-28T11:57:52.135Z",
      "siblings": [
        "ai-maximum-entropy-markov-models-and-logistic-reg-training-1",
        "ai-maximum-entropy-markov-models-and-logistic-reg-features-functions-3",
        "ai-maximum-entropy-markov-models-and-logistic-reg-training-and-decoding-4",
        "ai-maximum-entropy-markov-models-and-logistic-reg-memm-important-observations-5"
      ]
    },
    {
      "id": "ai-maximum-entropy-markov-models-and-logistic-reg-features-functions-3",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Maximum Entropy Markov Models",
      "articleSlug": "maximum-entropy-markov-models-and-logistic-reg",
      "chapter": "Maximum Entropy Markov Models",
      "title": "Features Functions",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>\n    <p>The MEMM can condition on any useful feature of the input observation, in the HMM this wasn’t possible because the HMM is likelihood based, and hence we would have needed to compute the likelihood of each feature of the observation.</p>\n  </li>\n  <li>\n    <p>The use of state-observation transition functions, rather than the separate transition and observation functions as in HMMs, allows us to model transitions in terms of multiple, non-independent features of observations.</p>\n  </li>\n  <li>\n    <p>This is achieved by a multinomial logistic regression, to estimate the probability of each local tag given the previous tag (i.e., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>s</mi><mo>&amp;#x2032;</mo></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-741\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-742\"><span class=\"msup\" id=\"MathJax-Span-743\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-744\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.367em;\"><span class=\"mo\" id=\"MathJax-Span-745\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>s</mi><mo>′</mo></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">s'</script>), the observed word (i.e. <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>o</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-746\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-747\"><span class=\"mi\" id=\"MathJax-Span-748\" style=\"font-family: STIXGeneral-Italic;\">o</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>o</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">o</script>), and any other features (i.e., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><msup><mi>y</mi><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-749\" style=\"width: 3.596em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1002.92em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-750\"><span class=\"msubsup\" id=\"MathJax-Span-751\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-752\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-753\"><span class=\"mrow\" id=\"MathJax-Span-754\"><span class=\"mi\" id=\"MathJax-Span-755\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-756\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-757\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-758\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msup\" id=\"MathJax-Span-759\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-760\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"mo\" id=\"MathJax-Span-761\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-762\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msup><mi>y</mi><mo>′</mo></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-54\">f_{i}(x,y')</script>) we want to include:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>s</mi><mo>&amp;#x2223;</mo><msup><mi>s</mi><mo>&amp;#x2032;</mo></msup><mo>,</mo><mi>o</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Z</mi><mo stretchy=&quot;false&quot;>(</mo><mi>o</mi><mo>,</mo><msup><mi>s</mi><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo></mrow></mfrac><mtext>&amp;#xA0;</mtext><mi>exp</mi><mo>&amp;#x2061;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;2.047em&quot; minsize=&quot;2.047em&quot;>(</mo></mrow><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></munderover><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x22C5;</mo><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mi>o</mi><mo>,</mo><msup><mi>s</mi><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;2.047em&quot; minsize=&quot;2.047em&quot;>)</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-763\" style=\"width: 22.659em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 18.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.19em, 1018.7em, 5.523em, -999.997em); top: -4.112em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-764\"><span class=\"mi\" id=\"MathJax-Span-765\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-766\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-767\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-768\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msup\" id=\"MathJax-Span-769\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-770\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.367em;\"><span class=\"mo\" id=\"MathJax-Span-771\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-772\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-773\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">o</span><span class=\"mo\" id=\"MathJax-Span-774\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-775\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-776\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.076em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-777\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1002.87em, 4.326em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.456em;\"><span class=\"mrow\" id=\"MathJax-Span-778\"><span class=\"mi\" id=\"MathJax-Span-779\" style=\"font-family: STIXGeneral-Italic;\">Z<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-780\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-781\" style=\"font-family: STIXGeneral-Italic;\">o</span><span class=\"mo\" id=\"MathJax-Span-782\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msup\" id=\"MathJax-Span-783\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-784\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.367em;\"><span class=\"mo\" id=\"MathJax-Span-785\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-786\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.08em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.076em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-787\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-788\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">exp</span><span class=\"mo\" id=\"MathJax-Span-789\"></span><span class=\"texatom\" id=\"MathJax-Span-790\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-791\"><span class=\"mo\" id=\"MathJax-Span-792\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">(</span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-793\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-794\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-795\"><span class=\"mrow\" id=\"MathJax-Span-796\"><span class=\"mi\" id=\"MathJax-Span-797\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-798\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-799\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-800\"><span class=\"mrow\" id=\"MathJax-Span-801\"><span class=\"mi\" id=\"MathJax-Span-802\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-803\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-804\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-805\"><span class=\"mrow\" id=\"MathJax-Span-806\"><span class=\"mi\" id=\"MathJax-Span-807\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-808\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-809\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-810\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-811\"><span class=\"mrow\" id=\"MathJax-Span-812\"><span class=\"mi\" id=\"MathJax-Span-813\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-814\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-815\" style=\"font-family: STIXGeneral-Italic;\">o</span><span class=\"mo\" id=\"MathJax-Span-816\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msup\" id=\"MathJax-Span-817\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-818\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.367em;\"><span class=\"mo\" id=\"MathJax-Span-819\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-820\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"texatom\" id=\"MathJax-Span-821\" style=\"\"><span class=\"mrow\" id=\"MathJax-Span-822\"><span class=\"mo\" id=\"MathJax-Span-823\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">)</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.753em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>P</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo>∣</mo><msup><mi>s</mi><mo>′</mo></msup><mo>,</mo><mi>o</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Z</mi><mo stretchy=\"false\">(</mo><mi>o</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy=\"false\">)</mo></mrow></mfrac><mtext>&nbsp;</mtext><mi>exp</mi><mo>⁡</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo maxsize=\"2.047em\" minsize=\"2.047em\">(</mo></mrow><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></munderover><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>⋅</mo><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">(</mo><mi>o</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo maxsize=\"2.047em\" minsize=\"2.047em\">)</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-55\">P(s \\mid s',o) = \\frac{1}{Z(o,s')}\\  \\exp\\bigg( \\sum_{i=1}^{N} w_{i} \\cdot f_{i}(o,s') \\bigg)</script>\n\n    <ul>\n      <li>where, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-824\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-825\"><span class=\"msubsup\" id=\"MathJax-Span-826\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-827\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-828\"><span class=\"mrow\" id=\"MathJax-Span-829\"><span class=\"mi\" id=\"MathJax-Span-830\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">w_{i}</script> are the weights to be learned, associated to each feature <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mi>o</mi><mo>,</mo><msup><mi>s</mi><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-831\" style=\"width: 3.544em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1002.87em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-832\"><span class=\"msubsup\" id=\"MathJax-Span-833\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-834\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-835\"><span class=\"mrow\" id=\"MathJax-Span-836\"><span class=\"mi\" id=\"MathJax-Span-837\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-838\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-839\" style=\"font-family: STIXGeneral-Italic;\">o</span><span class=\"mo\" id=\"MathJax-Span-840\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msup\" id=\"MathJax-Span-841\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-842\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.367em;\"><span class=\"mo\" id=\"MathJax-Span-843\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-844\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">(</mo><mi>o</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">f_{i}(o,s')</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Z</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-845\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-846\"><span class=\"mi\" id=\"MathJax-Span-847\" style=\"font-family: STIXGeneral-Italic;\">Z<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Z</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-58\">Z</script> is the normalizing factor that makes the matrix sum\nto 1 across each row.</li>\n    </ul>\n  </li>\n  <li>\n    <p>The figure below (taken from “Speech and Language Processing” Daniel Jurafsky &amp; James H. Martin) shows feature functions taking into consideration the whole observation sequence.</p>\n  </li>\n</ul>\n<p>The MEMM can condition on any useful feature of the input observation, in the HMM this wasn’t possible because the HMM is likelihood based, and hence we would have needed to compute the likelihood of each feature of the observation.</p>\n<p>The use of state-observation transition functions, rather than the separate transition and observation functions as in HMMs, allows us to model transitions in terms of multiple, non-independent features of observations.</p>\n<p>This is achieved by a multinomial logistic regression, to estimate the probability of each local tag given the previous tag (i.e., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>s</mi><mo>&amp;#x2032;</mo></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-741\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1000.73em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-742\"><span class=\"msup\" id=\"MathJax-Span-743\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-744\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.367em;\"><span class=\"mo\" id=\"MathJax-Span-745\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>s</mi><mo>′</mo></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-52\">s'</script>), the observed word (i.e. <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>o</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-746\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.47em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-747\"><span class=\"mi\" id=\"MathJax-Span-748\" style=\"font-family: STIXGeneral-Italic;\">o</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>o</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">o</script>), and any other features (i.e., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><msup><mi>y</mi><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-749\" style=\"width: 3.596em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.971em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1002.92em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-750\"><span class=\"msubsup\" id=\"MathJax-Span-751\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-752\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-753\"><span class=\"mrow\" id=\"MathJax-Span-754\"><span class=\"mi\" id=\"MathJax-Span-755\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-756\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-757\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-758\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msup\" id=\"MathJax-Span-759\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-760\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"mo\" id=\"MathJax-Span-761\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-762\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msup><mi>y</mi><mo>′</mo></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-54\">f_{i}(x,y')</script>) we want to include:</p>\n<ul>\n      <li>where, <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-824\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-825\"><span class=\"msubsup\" id=\"MathJax-Span-826\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-827\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-828\"><span class=\"mrow\" id=\"MathJax-Span-829\"><span class=\"mi\" id=\"MathJax-Span-830\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 0.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-56\">w_{i}</script> are the weights to be learned, associated to each feature <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mi>o</mi><mo>,</mo><msup><mi>s</mi><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-831\" style=\"width: 3.544em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.919em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.253em, 1002.87em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-832\"><span class=\"msubsup\" id=\"MathJax-Span-833\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-834\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-835\"><span class=\"mrow\" id=\"MathJax-Span-836\"><span class=\"mi\" id=\"MathJax-Span-837\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-838\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-839\" style=\"font-family: STIXGeneral-Italic;\">o</span><span class=\"mo\" id=\"MathJax-Span-840\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msup\" id=\"MathJax-Span-841\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.37em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-842\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.367em;\"><span class=\"mo\" id=\"MathJax-Span-843\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-844\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">(</mo><mi>o</mi><mo>,</mo><msup><mi>s</mi><mo>′</mo></msup><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">f_{i}(o,s')</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Z</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-845\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-846\"><span class=\"mi\" id=\"MathJax-Span-847\" style=\"font-family: STIXGeneral-Italic;\">Z<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Z</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-58\">Z</script> is the normalizing factor that makes the matrix sum\nto 1 across each row.</li>\n    </ul>\n<p>The figure below (taken from “Speech and Language Processing” Daniel Jurafsky &amp; James H. Martin) shows feature functions taking into consideration the whole observation sequence.</p>\n<p><img src=\"/primers/ai/assets/maximum-entropy-markov-models-and-logistic-reg/MEMM_II.png\" alt=\"\"></p>",
      "contentMarkdown": "*   The MEMM can condition on any useful feature of the input observation, in the HMM this wasn’t possible because the HMM is likelihood based, and hence we would have needed to compute the likelihood of each feature of the observation.\n    \n*   The use of state-observation transition functions, rather than the separate transition and observation functions as in HMMs, allows us to model transitions in terms of multiple, non-independent features of observations.\n    \n*   This is achieved by a multinomial logistic regression, to estimate the probability of each local tag given the previous tag (i.e., s′s′s'), the observed word (i.e. ooo), and any other features (i.e., fi(x,y′)fi(x,y′)f\\_{i}(x,y')) we want to include:\n    \n    P(s∣s′,o)\\=1Z(o,s′) exp(∑i\\=1Nwi⋅fi(o,s′))P(s∣s′,o)\\=1Z(o,s′) exp⁡(∑i\\=1Nwi⋅fi(o,s′))\n    \n    P(s \\\\mid s',o) = \\\\frac{1}{Z(o,s')}\\\\ \\\\exp\\\\bigg( \\\\sum\\_{i=1}^{N} w\\_{i} \\\\cdot f\\_{i}(o,s') \\\\bigg)\n    *   where, wiwiw\\_{i} are the weights to be learned, associated to each feature fi(o,s′)fi(o,s′)f\\_{i}(o,s') and ZZZ is the normalizing factor that makes the matrix sum to 1 across each row.\n*   The figure below (taken from “Speech and Language Processing” Daniel Jurafsky & James H. Martin) shows feature functions taking into consideration the whole observation sequence.\n    \n\nThe MEMM can condition on any useful feature of the input observation, in the HMM this wasn’t possible because the HMM is likelihood based, and hence we would have needed to compute the likelihood of each feature of the observation.\n\nThe use of state-observation transition functions, rather than the separate transition and observation functions as in HMMs, allows us to model transitions in terms of multiple, non-independent features of observations.\n\nThis is achieved by a multinomial logistic regression, to estimate the probability of each local tag given the previous tag (i.e., s′s′s'), the observed word (i.e. ooo), and any other features (i.e., fi(x,y′)fi(x,y′)f\\_{i}(x,y')) we want to include:\n\n*   where, wiwiw\\_{i} are the weights to be learned, associated to each feature fi(o,s′)fi(o,s′)f\\_{i}(o,s') and ZZZ is the normalizing factor that makes the matrix sum to 1 across each row.\n\nThe figure below (taken from “Speech and Language Processing” Daniel Jurafsky & James H. Martin) shows feature functions taking into consideration the whole observation sequence.\n\n![](/primers/ai/assets/maximum-entropy-markov-models-and-logistic-reg/MEMM_II.png)",
      "order": 3,
      "orderInChapter": 1,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": true,
        "wordCount": 343,
        "contentLength": 40998
      },
      "nextCards": [
        "ai-maximum-entropy-markov-models-and-logistic-reg-training-and-decoding-4",
        "ai-maximum-entropy-markov-models-and-logistic-reg-memm-important-observations-5"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/maximum-entropy-markov-models-and-logistic-reg/#features-functions",
      "scrapedAt": "2025-12-28T11:57:52.135Z",
      "siblings": [
        "ai-maximum-entropy-markov-models-and-logistic-reg-training-1",
        "ai-maximum-entropy-markov-models-and-logistic-reg-classification-2",
        "ai-maximum-entropy-markov-models-and-logistic-reg-training-and-decoding-4",
        "ai-maximum-entropy-markov-models-and-logistic-reg-memm-important-observations-5"
      ]
    },
    {
      "id": "ai-maximum-entropy-markov-models-and-logistic-reg-training-and-decoding-4",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Maximum Entropy Markov Models",
      "articleSlug": "maximum-entropy-markov-models-and-logistic-reg",
      "chapter": "Maximum Entropy Markov Models",
      "title": "Training and Decoding",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>Taken from the original paper:</li>\n</ul>\n<blockquote>\n  <p>“In what follows, we will split <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-59-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>s</mi><mo>&amp;#x2223;</mo><msup><mi>s</mi><mo>&amp;#x2032;</mo></msup><mo>,</mo><mi>O</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-848\" style=\"width: 5.234em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.354em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.345em, 1004.31em, 2.549em, -999.998em); top: -2.22em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-849\"><span class=\"mi\" id=\"MathJax-Span-850\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-851\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-852\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-853\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.326em;\">∣</span><span class=\"msup\" id=\"MathJax-Span-854\" style=\"padding-left: 0.326em;\"><span style=\"display: inline-block; position: relative; width: 0.697em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.382em, 1000.37em, 4.123em, -999.998em); top: -3.979em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-855\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -4.35em; left: 0.373em;\"><span class=\"mo\" id=\"MathJax-Span-856\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-857\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-858\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.188em;\">O</span><span class=\"mo\" id=\"MathJax-Span-859\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.225em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 1.281em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo>∣</mo><msup><mi>s</mi><mo>′</mo></msup><mo>,</mo><mi>O</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-59\">P(s \\mid s', O)</script> into <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-60-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2223;</mo><mi>S</mi><mo stretchy=&quot;false&quot;>&amp;#x2223;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-860\" style=\"width: 1.299em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.067em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.391em, 1000.98em, 2.549em, -999.998em); top: -2.22em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-861\"><span class=\"mo\" id=\"MathJax-Span-862\" style=\"font-family: STIXGeneral-Regular;\">∣</span><span class=\"mi\" id=\"MathJax-Span-863\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.002em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-864\" style=\"font-family: STIXGeneral-Regular;\">∣</span></span><span style=\"display: inline-block; width: 0px; height: 2.225em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 1.169em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">∣</mo><mi>S</mi><mo stretchy=\"false\">∣</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-60\">\\mid S \\mid</script> separately trained transition functions \u0006<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msup><mi>s</mi><mo>&amp;#x2032;</mo></msup></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mi>S</mi><mo>&amp;#x2223;</mo><mi>o</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>s</mi><mo>&amp;#x2223;</mo><msup><mi>s</mi><mo>&amp;#x2032;</mo></msup><mo>,</mo><mi>O</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-865\" style=\"width: 11.299em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.4em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.345em, 1009.35em, 2.549em, -999.998em); top: -2.22em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-866\"><span class=\"msubsup\" id=\"MathJax-Span-867\"><span style=\"display: inline-block; position: relative; width: 1.206em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.197em, 1000.6em, 4.123em, -999.998em); top: -3.979em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-868\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.604em;\"><span class=\"texatom\" id=\"MathJax-Span-869\"><span class=\"mrow\" id=\"MathJax-Span-870\"><span class=\"msup\" id=\"MathJax-Span-871\"><span style=\"display: inline-block; position: relative; width: 0.512em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.521em, 1000.28em, 4.123em, -999.998em); top: -3.979em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-872\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -4.164em; left: 0.28em;\"><span class=\"mo\" id=\"MathJax-Span-873\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-874\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-875\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.002em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-876\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.326em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-877\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.326em;\">o</span><span class=\"mo\" id=\"MathJax-Span-878\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-879\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.326em;\">=</span><span class=\"mi\" id=\"MathJax-Span-880\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.326em;\">P</span><span class=\"mo\" id=\"MathJax-Span-881\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-882\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-883\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.326em;\">∣</span><span class=\"msup\" id=\"MathJax-Span-884\" style=\"padding-left: 0.326em;\"><span style=\"display: inline-block; position: relative; width: 0.697em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.382em, 1000.37em, 4.123em, -999.998em); top: -3.979em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-885\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -4.35em; left: 0.373em;\"><span class=\"mo\" id=\"MathJax-Span-886\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-887\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-888\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.188em;\">O</span><span class=\"mo\" id=\"MathJax-Span-889\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.225em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 1.281em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><msup><mi>s</mi><mo>′</mo></msup></mrow></msub><mo stretchy=\"false\">(</mo><mi>S</mi><mo>∣</mo><mi>o</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>P</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo>∣</mo><msup><mi>s</mi><mo>′</mo></msup><mo>,</mo><mi>O</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">P_{s'} ( S \\mid o) = P(s \\mid s', O)</script>. Each of these functions is given by an exponential model”</p>\n</blockquote>\n<p>“In what follows, we will split <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-59-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>s</mi><mo>&amp;#x2223;</mo><msup><mi>s</mi><mo>&amp;#x2032;</mo></msup><mo>,</mo><mi>O</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-848\" style=\"width: 5.234em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.354em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.345em, 1004.31em, 2.549em, -999.998em); top: -2.22em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-849\"><span class=\"mi\" id=\"MathJax-Span-850\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-851\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-852\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-853\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.326em;\">∣</span><span class=\"msup\" id=\"MathJax-Span-854\" style=\"padding-left: 0.326em;\"><span style=\"display: inline-block; position: relative; width: 0.697em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.382em, 1000.37em, 4.123em, -999.998em); top: -3.979em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-855\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -4.35em; left: 0.373em;\"><span class=\"mo\" id=\"MathJax-Span-856\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-857\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-858\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.188em;\">O</span><span class=\"mo\" id=\"MathJax-Span-859\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.225em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 1.281em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>P</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo>∣</mo><msup><mi>s</mi><mo>′</mo></msup><mo>,</mo><mi>O</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-59\">P(s \\mid s', O)</script> into <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-60-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>&amp;#x2223;</mo><mi>S</mi><mo stretchy=&quot;false&quot;>&amp;#x2223;</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-860\" style=\"width: 1.299em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.067em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.391em, 1000.98em, 2.549em, -999.998em); top: -2.22em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-861\"><span class=\"mo\" id=\"MathJax-Span-862\" style=\"font-family: STIXGeneral-Regular;\">∣</span><span class=\"mi\" id=\"MathJax-Span-863\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.002em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-864\" style=\"font-family: STIXGeneral-Regular;\">∣</span></span><span style=\"display: inline-block; width: 0px; height: 2.225em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 1.169em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">∣</mo><mi>S</mi><mo stretchy=\"false\">∣</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-60\">\\mid S \\mid</script> separately trained transition functions \u0006<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>P</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msup><mi>s</mi><mo>&amp;#x2032;</mo></msup></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mi>S</mi><mo>&amp;#x2223;</mo><mi>o</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mi>s</mi><mo>&amp;#x2223;</mo><msup><mi>s</mi><mo>&amp;#x2032;</mo></msup><mo>,</mo><mi>O</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-865\" style=\"width: 11.299em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.4em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.345em, 1009.35em, 2.549em, -999.998em); top: -2.22em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-866\"><span class=\"msubsup\" id=\"MathJax-Span-867\"><span style=\"display: inline-block; position: relative; width: 1.206em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.197em, 1000.6em, 4.123em, -999.998em); top: -3.979em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-868\" style=\"font-family: STIXGeneral-Italic;\">P</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -3.84em; left: 0.604em;\"><span class=\"texatom\" id=\"MathJax-Span-869\"><span class=\"mrow\" id=\"MathJax-Span-870\"><span class=\"msup\" id=\"MathJax-Span-871\"><span style=\"display: inline-block; position: relative; width: 0.512em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.521em, 1000.28em, 4.123em, -999.998em); top: -3.979em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-872\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -4.164em; left: 0.28em;\"><span class=\"mo\" id=\"MathJax-Span-873\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-874\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-875\" style=\"font-family: STIXGeneral-Italic;\">S<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.002em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-876\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.326em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-877\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.326em;\">o</span><span class=\"mo\" id=\"MathJax-Span-878\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-879\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.326em;\">=</span><span class=\"mi\" id=\"MathJax-Span-880\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.326em;\">P</span><span class=\"mo\" id=\"MathJax-Span-881\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-882\" style=\"font-family: STIXGeneral-Italic;\">s</span><span class=\"mo\" id=\"MathJax-Span-883\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.326em;\">∣</span><span class=\"msup\" id=\"MathJax-Span-884\" style=\"padding-left: 0.326em;\"><span style=\"display: inline-block; position: relative; width: 0.697em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.382em, 1000.37em, 4.123em, -999.998em); top: -3.979em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-885\" style=\"font-family: STIXGeneral-Italic;\">s</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span><span style=\"position: absolute; top: -4.35em; left: 0.373em;\"><span class=\"mo\" id=\"MathJax-Span-886\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 3.984em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-887\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-888\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.188em;\">O</span><span class=\"mo\" id=\"MathJax-Span-889\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.225em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.275em; border-left: 0px solid; width: 0px; height: 1.281em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>P</mi><mrow class=\"MJX-TeXAtom-ORD\"><msup><mi>s</mi><mo>′</mo></msup></mrow></msub><mo stretchy=\"false\">(</mo><mi>S</mi><mo>∣</mo><mi>o</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>P</mi><mo stretchy=\"false\">(</mo><mi>s</mi><mo>∣</mo><msup><mi>s</mi><mo>′</mo></msup><mo>,</mo><mi>O</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">P_{s'} ( S \\mid o) = P(s \\mid s', O)</script>. Each of these functions is given by an exponential model”</p>\n<ul>\n  <li>\n    <p>MEMMs train one logistic regression per state transition, normalised locally. The original MEMM paper, published in 2000, used a generalized iterative scaling (GIS) algorithm to fit the multinomial logistic regression, that is finding the perfect weights according to the training data. That algorithm has been largely surpassed by gradient-based methods such as L-BFGS.</p>\n  </li>\n  <li>\n    <p>For decoding, the same algorithm as in the HMM is used, the Viterbi, although just slightly adapted to accommodate the new method of estimating state transitions.</p>\n  </li>\n</ul>\n<p>MEMMs train one logistic regression per state transition, normalised locally. The original MEMM paper, published in 2000, used a generalized iterative scaling (GIS) algorithm to fit the multinomial logistic regression, that is finding the perfect weights according to the training data. That algorithm has been largely surpassed by gradient-based methods such as L-BFGS.</p>\n<p>For decoding, the same algorithm as in the HMM is used, the Viterbi, although just slightly adapted to accommodate the new method of estimating state transitions.</p>",
      "contentMarkdown": "*   Taken from the original paper:\n\n> “In what follows, we will split P(s∣s′,O)P(s∣s′,O)P(s \\\\mid s', O) into ∣S∣∣S∣\\\\mid S \\\\mid separately trained transition functions \u0006Ps′(S∣o)\\=P(s∣s′,O)Ps′(S∣o)\\=P(s∣s′,O)P\\_{s'} ( S \\\\mid o) = P(s \\\\mid s', O). Each of these functions is given by an exponential model”\n\n“In what follows, we will split P(s∣s′,O)P(s∣s′,O)P(s \\\\mid s', O) into ∣S∣∣S∣\\\\mid S \\\\mid separately trained transition functions \u0006Ps′(S∣o)\\=P(s∣s′,O)Ps′(S∣o)\\=P(s∣s′,O)P\\_{s'} ( S \\\\mid o) = P(s \\\\mid s', O). Each of these functions is given by an exponential model”\n\n*   MEMMs train one logistic regression per state transition, normalised locally. The original MEMM paper, published in 2000, used a generalized iterative scaling (GIS) algorithm to fit the multinomial logistic regression, that is finding the perfect weights according to the training data. That algorithm has been largely surpassed by gradient-based methods such as L-BFGS.\n    \n*   For decoding, the same algorithm as in the HMM is used, the Viterbi, although just slightly adapted to accommodate the new method of estimating state transitions.\n    \n\nMEMMs train one logistic regression per state transition, normalised locally. The original MEMM paper, published in 2000, used a generalized iterative scaling (GIS) algorithm to fit the multinomial logistic regression, that is finding the perfect weights according to the training data. That algorithm has been largely surpassed by gradient-based methods such as L-BFGS.\n\nFor decoding, the same algorithm as in the HMM is used, the Viterbi, although just slightly adapted to accommodate the new method of estimating state transitions.",
      "order": 4,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 243,
        "contentLength": 20804
      },
      "nextCards": [
        "ai-maximum-entropy-markov-models-and-logistic-reg-memm-important-observations-5"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/maximum-entropy-markov-models-and-logistic-reg/#training-and-decoding",
      "scrapedAt": "2025-12-28T11:57:52.135Z",
      "siblings": [
        "ai-maximum-entropy-markov-models-and-logistic-reg-training-1",
        "ai-maximum-entropy-markov-models-and-logistic-reg-classification-2",
        "ai-maximum-entropy-markov-models-and-logistic-reg-features-functions-3",
        "ai-maximum-entropy-markov-models-and-logistic-reg-memm-important-observations-5"
      ]
    },
    {
      "id": "ai-maximum-entropy-markov-models-and-logistic-reg-memm-important-observations-5",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Maximum Entropy Markov Models",
      "articleSlug": "maximum-entropy-markov-models-and-logistic-reg",
      "chapter": "Maximum Entropy Markov Models",
      "title": "MEMM Important Observations",
      "subtitle": null,
      "contentHtml": "<ul>\n  <li>\n    <p>The main advantage over the HMM is the use of feature vectors, making the transition probability sensitive to any word in the input sequence.</p>\n  </li>\n  <li>\n    <p>There is an exponential model associate to each (state, word) pair to calculate the conditional probability of the next state.</p>\n  </li>\n  <li>\n    <p>The exponential model allows the MEMMs to support long-distance interactions over the whole observation sequence together with the previous state, instead of two different probability distributions.</p>\n  </li>\n  <li>\n    <p>MEMM can be also augmented to include features involving additional past states, instead of just the previous one.</p>\n  </li>\n  <li>\n    <p>It also uses the Viterbi algorithm (slightly adapted) to perform decoding.</p>\n  </li>\n  <li>\n    <p>It suffers from the label bias problem, I will detailed in the next post about Conditional Random Fields.</p>\n  </li>\n</ul>\n<p>The main advantage over the HMM is the use of feature vectors, making the transition probability sensitive to any word in the input sequence.</p>\n<p>There is an exponential model associate to each (state, word) pair to calculate the conditional probability of the next state.</p>\n<p>The exponential model allows the MEMMs to support long-distance interactions over the whole observation sequence together with the previous state, instead of two different probability distributions.</p>\n<p>MEMM can be also augmented to include features involving additional past states, instead of just the previous one.</p>\n<p>It also uses the Viterbi algorithm (slightly adapted) to perform decoding.</p>\n<p>It suffers from the label bias problem, I will detailed in the next post about Conditional Random Fields.</p>",
      "contentMarkdown": "*   The main advantage over the HMM is the use of feature vectors, making the transition probability sensitive to any word in the input sequence.\n    \n*   There is an exponential model associate to each (state, word) pair to calculate the conditional probability of the next state.\n    \n*   The exponential model allows the MEMMs to support long-distance interactions over the whole observation sequence together with the previous state, instead of two different probability distributions.\n    \n*   MEMM can be also augmented to include features involving additional past states, instead of just the previous one.\n    \n*   It also uses the Viterbi algorithm (slightly adapted) to perform decoding.\n    \n*   It suffers from the label bias problem, I will detailed in the next post about Conditional Random Fields.\n    \n\nThe main advantage over the HMM is the use of feature vectors, making the transition probability sensitive to any word in the input sequence.\n\nThere is an exponential model associate to each (state, word) pair to calculate the conditional probability of the next state.\n\nThe exponential model allows the MEMMs to support long-distance interactions over the whole observation sequence together with the previous state, instead of two different probability distributions.\n\nMEMM can be also augmented to include features involving additional past states, instead of just the previous one.\n\nIt also uses the Viterbi algorithm (slightly adapted) to perform decoding.\n\nIt suffers from the label bias problem, I will detailed in the next post about Conditional Random Fields.",
      "order": 5,
      "orderInChapter": 3,
      "difficulty": 2,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 240,
        "contentLength": 1736
      },
      "nextCards": [],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/maximum-entropy-markov-models-and-logistic-reg/#memm-important-observations",
      "scrapedAt": "2025-12-28T11:57:52.135Z",
      "siblings": [
        "ai-maximum-entropy-markov-models-and-logistic-reg-training-1",
        "ai-maximum-entropy-markov-models-and-logistic-reg-classification-2",
        "ai-maximum-entropy-markov-models-and-logistic-reg-features-functions-3",
        "ai-maximum-entropy-markov-models-and-logistic-reg-training-and-decoding-4"
      ]
    },
    {
      "id": "ai-conditional-random-fields-label-bias-problem-in-memms-1",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Conditional Random Fields",
      "articleSlug": "conditional-random-fields",
      "chapter": "Introduction",
      "title": "Label Bias Problem in MEMMs",
      "subtitle": "Introduction",
      "contentHtml": "<ul>\n  <li>Recalling how the transition probabilities are computed in a MEMM model, from the previous post, we learned that the probability of the next state is only dependent on the observation (i.e., the sequence of words) and the previous state, that is, we have an exponential model for each state to tell us the conditional probability of the next states.</li>\n  <li>The figure below (taken from A. McCallum et al. 2000) shows the MEMM transition probability computation.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/conditional-random-fields/HMM.png\" alt=\"\"></p>\n<ul>\n  <li>This causes the so called <strong>Label Bias Problem</strong>, and Lafferty et al. 2001 demonstrate this through experiments and report it. We will not demonstrate it, but just give the basic intuition taken also from the paper. The figure below (taken from Lafferty et al. 2001) shows the label bias problem.</li>\n</ul>\n<p><img src=\"/primers/ai/assets/conditional-random-fields/Label_Bias_Problem.png\" alt=\"\"></p>\n<ul>\n  <li>Given the observation sequence: <strong><em>r</em> <em>i</em> <em>b</em></strong></li>\n</ul>\n<blockquote>\n  <p>In the first time step, r matches both transitions from the start state, so the probability mass gets distributed roughly equally among those two transitions. Next we observe i. Both states 1 and 4 have only one outgoing transition. State 1 has seen this observation often in training, state 4 has almost never seen this observation; but like state 1, state 4 has no choice but to pass all its mass to its single outgoing transition, since it is not generating the observation, only conditioning on it. Thus, states with a single outgoing transition effectively ignore their observations.</p>\n</blockquote>\n<p>In the first time step, r matches both transitions from the start state, so the probability mass gets distributed roughly equally among those two transitions. Next we observe i. Both states 1 and 4 have only one outgoing transition. State 1 has seen this observation often in training, state 4 has almost never seen this observation; but like state 1, state 4 has no choice but to pass all its mass to its single outgoing transition, since it is not generating the observation, only conditioning on it. Thus, states with a single outgoing transition effectively ignore their observations.</p>\n<blockquote>\n  <p>The top path and the bottom path will be about equally likely, independently of the observation sequence. If one of the two words is slightly more common in the training set, the transitions out of the start state will slightly prefer its corresponding transition, and that word’s state sequence will always win.</p>\n</blockquote>\n<p>The top path and the bottom path will be about equally likely, independently of the observation sequence. If one of the two words is slightly more common in the training set, the transitions out of the start state will slightly prefer its corresponding transition, and that word’s state sequence will always win.</p>\n<ul>\n  <li>\n    <p>Transitions from a given state are competing against each other only.</p>\n  </li>\n  <li>\n    <p>Per state normalization, i.e. sum of transition probability for any state has to sum to 1.</p>\n  </li>\n  <li>\n    <p>MEMM are normalized locally over each observation where the transitions going out from a state compete only against each other, as opposed to all the other transitions in the model.</p>\n  </li>\n  <li>\n    <p>States with a single outgoing transition effectively ignore their observations.</p>\n  </li>\n  <li>\n    <p>Causes bias: states with fewer arcs are preferred.</p>\n  </li>\n  <li>\n    <p>The idea of CRF is to drop this local per state normalization, and replace it by a global per sequence normalization.</p>\n  </li>\n  <li>\n    <p>So, how do we formalize this global normalization? I will try to explain it in the sections that follow.</p>\n  </li>\n</ul>\n<p>Transitions from a given state are competing against each other only.</p>\n<p>Per state normalization, i.e. sum of transition probability for any state has to sum to 1.</p>\n<p>MEMM are normalized locally over each observation where the transitions going out from a state compete only against each other, as opposed to all the other transitions in the model.</p>\n<p>States with a single outgoing transition effectively ignore their observations.</p>\n<p>Causes bias: states with fewer arcs are preferred.</p>\n<p>The idea of CRF is to drop this local per state normalization, and replace it by a global per sequence normalization.</p>\n<p>So, how do we formalize this global normalization? I will try to explain it in the sections that follow.</p>",
      "contentMarkdown": "*   Recalling how the transition probabilities are computed in a MEMM model, from the previous post, we learned that the probability of the next state is only dependent on the observation (i.e., the sequence of words) and the previous state, that is, we have an exponential model for each state to tell us the conditional probability of the next states.\n*   The figure below (taken from A. McCallum et al. 2000) shows the MEMM transition probability computation.\n\n![](/primers/ai/assets/conditional-random-fields/HMM.png)\n\n*   This causes the so called **Label Bias Problem**, and Lafferty et al. 2001 demonstrate this through experiments and report it. We will not demonstrate it, but just give the basic intuition taken also from the paper. The figure below (taken from Lafferty et al. 2001) shows the label bias problem.\n\n![](/primers/ai/assets/conditional-random-fields/Label_Bias_Problem.png)\n\n*   Given the observation sequence: **_r_ _i_ _b_**\n\n> In the first time step, r matches both transitions from the start state, so the probability mass gets distributed roughly equally among those two transitions. Next we observe i. Both states 1 and 4 have only one outgoing transition. State 1 has seen this observation often in training, state 4 has almost never seen this observation; but like state 1, state 4 has no choice but to pass all its mass to its single outgoing transition, since it is not generating the observation, only conditioning on it. Thus, states with a single outgoing transition effectively ignore their observations.\n\nIn the first time step, r matches both transitions from the start state, so the probability mass gets distributed roughly equally among those two transitions. Next we observe i. Both states 1 and 4 have only one outgoing transition. State 1 has seen this observation often in training, state 4 has almost never seen this observation; but like state 1, state 4 has no choice but to pass all its mass to its single outgoing transition, since it is not generating the observation, only conditioning on it. Thus, states with a single outgoing transition effectively ignore their observations.\n\n> The top path and the bottom path will be about equally likely, independently of the observation sequence. If one of the two words is slightly more common in the training set, the transitions out of the start state will slightly prefer its corresponding transition, and that word’s state sequence will always win.\n\nThe top path and the bottom path will be about equally likely, independently of the observation sequence. If one of the two words is slightly more common in the training set, the transitions out of the start state will slightly prefer its corresponding transition, and that word’s state sequence will always win.\n\n*   Transitions from a given state are competing against each other only.\n    \n*   Per state normalization, i.e. sum of transition probability for any state has to sum to 1.\n    \n*   MEMM are normalized locally over each observation where the transitions going out from a state compete only against each other, as opposed to all the other transitions in the model.\n    \n*   States with a single outgoing transition effectively ignore their observations.\n    \n*   Causes bias: states with fewer arcs are preferred.\n    \n*   The idea of CRF is to drop this local per state normalization, and replace it by a global per sequence normalization.\n    \n*   So, how do we formalize this global normalization? I will try to explain it in the sections that follow.\n    \n\nTransitions from a given state are competing against each other only.\n\nPer state normalization, i.e. sum of transition probability for any state has to sum to 1.\n\nMEMM are normalized locally over each observation where the transitions going out from a state compete only against each other, as opposed to all the other transitions in the model.\n\nStates with a single outgoing transition effectively ignore their observations.\n\nCauses bias: states with fewer arcs are preferred.\n\nThe idea of CRF is to drop this local per state normalization, and replace it by a global per sequence normalization.\n\nSo, how do we formalize this global normalization? I will try to explain it in the sections that follow.",
      "order": 1,
      "orderInChapter": 1,
      "difficulty": 2,
      "estimatedMinutes": 4,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": true,
        "wordCount": 677,
        "contentLength": 4622
      },
      "nextCards": [
        "ai-conditional-random-fields-undirected-graphical-models-2",
        "ai-conditional-random-fields-linear-chain-crfs-3"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/conditional-random-fields/#label-bias-problem-in-memms",
      "scrapedAt": "2025-12-28T11:57:57.127Z",
      "siblings": [
        "ai-conditional-random-fields-undirected-graphical-models-2",
        "ai-conditional-random-fields-linear-chain-crfs-3",
        "ai-conditional-random-fields-inference-4",
        "ai-conditional-random-fields-parameter-estimation-5",
        "ai-conditional-random-fields-wrapping-up-hmm-vs-memm-vs-crf-6"
      ]
    },
    {
      "id": "ai-conditional-random-fields-undirected-graphical-models-2",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Conditional Random Fields",
      "articleSlug": "conditional-random-fields",
      "chapter": "Introduction",
      "title": "Undirected Graphical Models",
      "subtitle": "Introduction",
      "contentHtml": "<ul>\n  <li>\n    <p>A Conditional Random Field can be seen as an undirected graphical model, or Markov Random Field, globally conditioned on <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">X</script>, the random variable representing observation sequence.</p>\n  </li>\n  <li>\n    <p><a href=\"http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&amp;context=cis_papers\">Lafferty et al. 2001</a> define a Conditional Random Field as:</p>\n\n    <ul>\n      <li>\n        <p><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-5\"><span class=\"mi\" id=\"MathJax-Span-6\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">X</script> is a random variable over data sequences to be labeled, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-7\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-8\"><span class=\"mi\" id=\"MathJax-Span-9\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">Y</script> is a random variable over corresponding label sequences.</p>\n      </li>\n      <li>\n        <p>The random variables <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-10\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-11\"><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">X</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-13\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">Y</script> are jointly distributed, but in a discriminative framework we construct a conditional model <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>Y</mi><mo>&amp;#x2223;</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-16\" style=\"width: 4.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.596em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.54em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-17\"><span class=\"mi\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-19\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-20\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-21\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-22\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-23\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>Y</mi><mo>∣</mo><mi>X</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">p(Y \\mid X)</script> from paired observation and label sequences:</p>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>Let <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mi>V</mi><mo>,</mo><mi>E</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-24\" style=\"width: 5.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.43em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.38em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-25\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-28\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-31\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-32\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>V</mi><mo>,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">G = (V , E)</script> be a graph such that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mi>v</mi><mo>&amp;#x2208;</mo><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-33\" style=\"width: 7.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.565em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.57em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-34\"><span class=\"mi\" id=\"MathJax-Span-35\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-38\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-39\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-40\"><span class=\"mrow\" id=\"MathJax-Span-41\"><span class=\"mi\" id=\"MathJax-Span-42\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-43\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mtext\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-45\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"mi\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi><mo>=</mo><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi></mrow></msub><mo stretchy=\"false\">)</mo><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mi>v</mi><mo>∈</mo><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">Y = (Y_{v})\\  \\ v \\in V</script>, so that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-49\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-50\"><span class=\"mi\" id=\"MathJax-Span-51\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">Y</script> is indexed by the vertices of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-52\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-53\"><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Italic;\">G</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">G</script>.</p>\n  </li>\n  <li>\n    <p><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-55\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.66em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-56\"><span class=\"mo\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-58\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">(X, Y)</script> is a conditional random field when each of the random variables <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-62\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-63\"><span class=\"msubsup\" id=\"MathJax-Span-64\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-66\"><span class=\"mrow\" id=\"MathJax-Span-67\"><span class=\"mi\" id=\"MathJax-Span-68\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>Y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">Y_{v}</script>, conditioned on <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-69\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-70\"><span class=\"mi\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">X</script>, obey the Markov property with respect to the graph:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-14-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi></mrow></msub><mo>&amp;#x2223;</mo><mi>X</mi><mo>,</mo><msub><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>w</mi></mrow></msub><mo>,</mo><mi>w</mi><mo>&amp;#x2260;</mo><mi>v</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi></mrow></msub><mo>&amp;#x2223;</mo><mi>X</mi><mo>,</mo><msub><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>w</mi></mrow></msub><mo>,</mo><mi>w</mi><mo>&amp;#x223C;</mo><mi>v</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-72\" style=\"width: 21.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 17.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1017.61em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-73\"><span class=\"mi\" id=\"MathJax-Span-74\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-75\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-76\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-77\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-78\"><span class=\"mrow\" id=\"MathJax-Span-79\"><span class=\"mi\" id=\"MathJax-Span-80\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-81\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-82\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-83\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-84\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-85\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-86\"><span class=\"mrow\" id=\"MathJax-Span-87\"><span class=\"mi\" id=\"MathJax-Span-88\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-89\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-90\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">w</span><span class=\"mo\" id=\"MathJax-Span-91\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">≠</span><span class=\"mi\" id=\"MathJax-Span-92\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">v</span><span class=\"mo\" id=\"MathJax-Span-93\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-94\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mi\" id=\"MathJax-Span-95\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">P</span><span class=\"mo\" id=\"MathJax-Span-96\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-97\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-98\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-99\"><span class=\"mrow\" id=\"MathJax-Span-100\"><span class=\"mi\" id=\"MathJax-Span-101\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-102\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-103\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-104\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-105\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-106\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-107\"><span class=\"mrow\" id=\"MathJax-Span-108\"><span class=\"mi\" id=\"MathJax-Span-109\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-110\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-111\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">w</span><span class=\"mo\" id=\"MathJax-Span-112\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∼</span><span class=\"mi\" id=\"MathJax-Span-113\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">v</span><span class=\"mo\" id=\"MathJax-Span-114\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi></mrow></msub><mo>∣</mo><mi>X</mi><mo>,</mo><msub><mi>Y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>w</mi></mrow></msub><mo>,</mo><mi>w</mi><mo>≠</mo><mi>v</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi></mrow></msub><mo>∣</mo><mi>X</mi><mo>,</mo><msub><mi>Y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>w</mi></mrow></msub><mo>,</mo><mi>w</mi><mo>∼</mo><mi>v</mi><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-14\">P(Y_{v} \\mid X, Y_{w}, w \\neq v) = P(Y_{v} \\mid X, Y_{w}, w \\sim v)</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi><mo>&amp;#x223C;</mo><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-115\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1002.4em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-116\"><span class=\"mi\" id=\"MathJax-Span-117\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-118\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∼</span><span class=\"mi\" id=\"MathJax-Span-119\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi><mo>∼</mo><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">w \\sim v</script> means that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-120\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-121\"><span class=\"mi\" id=\"MathJax-Span-122\" style=\"font-family: STIXGeneral-Italic;\">w</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">w</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-123\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-124\"><span class=\"mi\" id=\"MathJax-Span-125\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">v</script> are neighbors in G. Thus, a CRF is a random field globally conditioned on the observation <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-126\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-127\"><span class=\"mi\" id=\"MathJax-Span-128\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">X</script>. This goes already in the direction of what the MEMM doesn’t give us, states globally conditioned on the observation.</li>\n    </ul>\n  </li>\n  <li>\n    <p>This graph may have an arbitrary structure as long as it represents the label sequences being modeled, this is also called general Conditional Random Fields.</p>\n  </li>\n  <li>\n    <p>However the simplest and most common graph structured in NLP, which is the one used to model sequences is the one in which the nodes corresponding to elements of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-129\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-130\"><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">Y</script> form a simple first-order chain, as illustrated in the figure below:</p>\n  </li>\n</ul>\n<p>A Conditional Random Field can be seen as an undirected graphical model, or Markov Random Field, globally conditioned on <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"mi\" id=\"MathJax-Span-3\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\">X</script>, the random variable representing observation sequence.</p>\n<p><a href=\"http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&amp;context=cis_papers\">Lafferty et al. 2001</a> define a Conditional Random Field as:</p>\n<ul>\n      <li>\n        <p><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-5\"><span class=\"mi\" id=\"MathJax-Span-6\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">X</script> is a random variable over data sequences to be labeled, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-7\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-8\"><span class=\"mi\" id=\"MathJax-Span-9\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">Y</script> is a random variable over corresponding label sequences.</p>\n      </li>\n      <li>\n        <p>The random variables <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-10\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-11\"><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">X</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-13\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">Y</script> are jointly distributed, but in a discriminative framework we construct a conditional model <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>Y</mi><mo>&amp;#x2223;</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-16\" style=\"width: 4.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.596em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.54em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-17\"><span class=\"mi\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-19\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-20\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-21\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-22\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-23\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>Y</mi><mo>∣</mo><mi>X</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">p(Y \\mid X)</script> from paired observation and label sequences:</p>\n      </li>\n    </ul>\n<p><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-2-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-4\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-5\"><span class=\"mi\" id=\"MathJax-Span-6\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\">X</script> is a random variable over data sequences to be labeled, and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-3-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-7\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-8\"><span class=\"mi\" id=\"MathJax-Span-9\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\">Y</script> is a random variable over corresponding label sequences.</p>\n<p>The random variables <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-10\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-11\"><span class=\"mi\" id=\"MathJax-Span-12\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\">X</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-5-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-13\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-14\"><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\">Y</script> are jointly distributed, but in a discriminative framework we construct a conditional model <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-6-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>Y</mi><mo>&amp;#x2223;</mo><mi>X</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-16\" style=\"width: 4.326em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.596em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1003.54em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-17\"><span class=\"mi\" id=\"MathJax-Span-18\" style=\"font-family: STIXGeneral-Italic;\">p</span><span class=\"mo\" id=\"MathJax-Span-19\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-20\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-21\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"mi\" id=\"MathJax-Span-22\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-23\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>p</mi><mo stretchy=\"false\">(</mo><mi>Y</mi><mo>∣</mo><mi>X</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\">p(Y \\mid X)</script> from paired observation and label sequences:</p>\n<p>Let <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-7-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mi>V</mi><mo>,</mo><mi>E</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-24\" style=\"width: 5.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 4.43em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1004.38em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-25\"><span class=\"mi\" id=\"MathJax-Span-26\" style=\"font-family: STIXGeneral-Italic;\">G</span><span class=\"mo\" id=\"MathJax-Span-27\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-28\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"mi\" id=\"MathJax-Span-29\" style=\"font-family: STIXGeneral-Italic;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-30\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-31\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-32\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi><mo>=</mo><mo stretchy=\"false\">(</mo><mi>V</mi><mo>,</mo><mi>E</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\">G = (V , E)</script> be a graph such that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-8-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mtext>&amp;#xA0;</mtext><mi>v</mi><mo>&amp;#x2208;</mo><mi>V</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-33\" style=\"width: 7.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.565em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.409em, 1006.57em, 2.555em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-34\"><span class=\"mi\" id=\"MathJax-Span-35\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-36\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mo\" id=\"MathJax-Span-37\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-38\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-39\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-40\"><span class=\"mrow\" id=\"MathJax-Span-41\"><span class=\"mi\" id=\"MathJax-Span-42\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-43\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mtext\" id=\"MathJax-Span-44\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mtext\" id=\"MathJax-Span-45\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-46\" style=\"font-family: STIXGeneral-Italic;\">v</span><span class=\"mo\" id=\"MathJax-Span-47\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"mi\" id=\"MathJax-Span-48\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">V<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi><mo>=</mo><mo stretchy=\"false\">(</mo><msub><mi>Y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi></mrow></msub><mo stretchy=\"false\">)</mo><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mi>v</mi><mo>∈</mo><mi>V</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\">Y = (Y_{v})\\  \\ v \\in V</script>, so that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-9-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-49\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-50\"><span class=\"mi\" id=\"MathJax-Span-51\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\">Y</script> is indexed by the vertices of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-10-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-52\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-53\"><span class=\"mi\" id=\"MathJax-Span-54\" style=\"font-family: STIXGeneral-Italic;\">G</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>G</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\">G</script>.</p>\n<p><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-11-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-55\" style=\"width: 3.284em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.711em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.66em, 2.503em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-56\"><span class=\"mo\" id=\"MathJax-Span-57\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-58\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-59\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-61\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\">(X, Y)</script> is a conditional random field when each of the random variables <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-12-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>v</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-62\" style=\"width: 1.148em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.94em, 2.451em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-63\"><span class=\"msubsup\" id=\"MathJax-Span-64\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-65\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-66\"><span class=\"mrow\" id=\"MathJax-Span-67\"><span class=\"mi\" id=\"MathJax-Span-68\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">v</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>Y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>v</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\">Y_{v}</script>, conditioned on <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-13-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-69\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-70\"><span class=\"mi\" id=\"MathJax-Span-71\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\">X</script>, obey the Markov property with respect to the graph:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-15-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi><mo>&amp;#x223C;</mo><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-115\" style=\"width: 2.919em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.398em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.669em, 1002.4em, 2.398em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-116\"><span class=\"mi\" id=\"MathJax-Span-117\" style=\"font-family: STIXGeneral-Italic;\">w</span><span class=\"mo\" id=\"MathJax-Span-118\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∼</span><span class=\"mi\" id=\"MathJax-Span-119\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.315em;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi><mo>∼</mo><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\">w \\sim v</script> means that <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-16-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>w</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-120\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.73em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-121\"><span class=\"mi\" id=\"MathJax-Span-122\" style=\"font-family: STIXGeneral-Italic;\">w</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>w</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\">w</script> and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-17-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>v</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-123\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-124\"><span class=\"mi\" id=\"MathJax-Span-125\" style=\"font-family: STIXGeneral-Italic;\">v</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>v</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\">v</script> are neighbors in G. Thus, a CRF is a random field globally conditioned on the observation <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-18-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>X</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-126\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-127\"><span class=\"mi\" id=\"MathJax-Span-128\" style=\"font-family: STIXGeneral-Italic;\">X<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>X</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\">X</script>. This goes already in the direction of what the MEMM doesn’t give us, states globally conditioned on the observation.</li>\n    </ul>\n<p>This graph may have an arbitrary structure as long as it represents the label sequences being modeled, this is also called general Conditional Random Fields.</p>\n<p>However the simplest and most common graph structured in NLP, which is the one used to model sequences is the one in which the nodes corresponding to elements of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-19-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>Y</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-129\" style=\"width: 0.94em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.78em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-130\"><span class=\"mi\" id=\"MathJax-Span-131\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>Y</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\">Y</script> form a simple first-order chain, as illustrated in the figure below:</p>\n<p>The figure below (taken from Hanna Wallach 2004) shows chain-structured CRFs globally conditioned on X.</p>\n<p><img src=\"/primers/ai/assets/conditional-random-fields/Conditional_Random_Fields.png\" alt=\"\"></p>\n<ul>\n  <li>This is also called linear-chain conditional random fields, which is the type of CRF on which the rest of this post will focus.</li>\n</ul>",
      "contentMarkdown": "*   A Conditional Random Field can be seen as an undirected graphical model, or Markov Random Field, globally conditioned on XXX, the random variable representing observation sequence.\n    \n*   [Lafferty et al. 2001](http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers) define a Conditional Random Field as:\n    \n    *   XXX is a random variable over data sequences to be labeled, and YYY is a random variable over corresponding label sequences.\n        \n    *   The random variables XXX and YYY are jointly distributed, but in a discriminative framework we construct a conditional model p(Y∣X)p(Y∣X)p(Y \\\\mid X) from paired observation and label sequences:\n        \n*   Let G\\=(V,E)G\\=(V,E)G = (V , E) be a graph such that Y\\=(Yv)  v∈VY\\=(Yv)  v∈VY = (Y\\_{v})\\\\ \\\\ v \\\\in V, so that YYY is indexed by the vertices of GGG.\n    \n*   (X,Y)(X,Y)(X, Y) is a conditional random field when each of the random variables YvYvY\\_{v}, conditioned on XXX, obey the Markov property with respect to the graph:\n    \n    P(Yv∣X,Yw,w≠v)\\=P(Yv∣X,Yw,w∼v)P(Yv∣X,Yw,w≠v)\\=P(Yv∣X,Yw,w∼v)\n    \n    P(Y\\_{v} \\\\mid X, Y\\_{w}, w \\\\neq v) = P(Y\\_{v} \\\\mid X, Y\\_{w}, w \\\\sim v)\n    *   where w∼vw∼vw \\\\sim v means that www and vvv are neighbors in G. Thus, a CRF is a random field globally conditioned on the observation XXX. This goes already in the direction of what the MEMM doesn’t give us, states globally conditioned on the observation.\n*   This graph may have an arbitrary structure as long as it represents the label sequences being modeled, this is also called general Conditional Random Fields.\n    \n*   However the simplest and most common graph structured in NLP, which is the one used to model sequences is the one in which the nodes corresponding to elements of YYY form a simple first-order chain, as illustrated in the figure below:\n    \n\nA Conditional Random Field can be seen as an undirected graphical model, or Markov Random Field, globally conditioned on XXX, the random variable representing observation sequence.\n\n[Lafferty et al. 2001](http://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers) define a Conditional Random Field as:\n\n*   XXX is a random variable over data sequences to be labeled, and YYY is a random variable over corresponding label sequences.\n    \n*   The random variables XXX and YYY are jointly distributed, but in a discriminative framework we construct a conditional model p(Y∣X)p(Y∣X)p(Y \\\\mid X) from paired observation and label sequences:\n    \n\nXXX is a random variable over data sequences to be labeled, and YYY is a random variable over corresponding label sequences.\n\nThe random variables XXX and YYY are jointly distributed, but in a discriminative framework we construct a conditional model p(Y∣X)p(Y∣X)p(Y \\\\mid X) from paired observation and label sequences:\n\nLet G\\=(V,E)G\\=(V,E)G = (V , E) be a graph such that Y\\=(Yv)  v∈VY\\=(Yv)  v∈VY = (Y\\_{v})\\\\ \\\\ v \\\\in V, so that YYY is indexed by the vertices of GGG.\n\n(X,Y)(X,Y)(X, Y) is a conditional random field when each of the random variables YvYvY\\_{v}, conditioned on XXX, obey the Markov property with respect to the graph:\n\n*   where w∼vw∼vw \\\\sim v means that www and vvv are neighbors in G. Thus, a CRF is a random field globally conditioned on the observation XXX. This goes already in the direction of what the MEMM doesn’t give us, states globally conditioned on the observation.\n\nThis graph may have an arbitrary structure as long as it represents the label sequences being modeled, this is also called general Conditional Random Fields.\n\nHowever the simplest and most common graph structured in NLP, which is the one used to model sequences is the one in which the nodes corresponding to elements of YYY form a simple first-order chain, as illustrated in the figure below:\n\nThe figure below (taken from Hanna Wallach 2004) shows chain-structured CRFs globally conditioned on X.\n\n![](/primers/ai/assets/conditional-random-fields/Conditional_Random_Fields.png)\n\n*   This is also called linear-chain conditional random fields, which is the type of CRF on which the rest of this post will focus.",
      "order": 2,
      "orderInChapter": 2,
      "difficulty": 3,
      "estimatedMinutes": 4,
      "tags": [
        "miscellaneous",
        "nlp"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": true,
        "wordCount": 621,
        "contentLength": 75639
      },
      "nextCards": [
        "ai-conditional-random-fields-linear-chain-crfs-3",
        "ai-conditional-random-fields-inference-4"
      ],
      "relatedCards": [
        "ai-ml-runtimes-suitable-applications-8",
        "ai-ml-runtimes-overview-45",
        "ai-ml-runtimes-suitable-applications-44",
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/conditional-random-fields/#undirected-graphical-models",
      "scrapedAt": "2025-12-28T11:57:57.127Z",
      "siblings": [
        "ai-conditional-random-fields-label-bias-problem-in-memms-1",
        "ai-conditional-random-fields-linear-chain-crfs-3",
        "ai-conditional-random-fields-inference-4",
        "ai-conditional-random-fields-parameter-estimation-5",
        "ai-conditional-random-fields-wrapping-up-hmm-vs-memm-vs-crf-6"
      ]
    },
    {
      "id": "ai-conditional-random-fields-linear-chain-crfs-3",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Conditional Random Fields",
      "articleSlug": "conditional-random-fields",
      "chapter": "Introduction",
      "title": "Linear-chain CRFs",
      "subtitle": "Introduction",
      "contentHtml": "<ul>\n  <li>Let <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-20-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-132\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-133\"><span class=\"texatom\" id=\"MathJax-Span-134\"><span class=\"mrow\" id=\"MathJax-Span-135\"><span class=\"munderover\" id=\"MathJax-Span-136\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-137\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-138\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\">\\bar{x}</script> is a sequence of words and <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-21-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-139\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-140\"><span class=\"texatom\" id=\"MathJax-Span-141\"><span class=\"mrow\" id=\"MathJax-Span-142\"><span class=\"munderover\" id=\"MathJax-Span-143\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-144\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-145\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\">\\bar{y}</script> a corresponding sequence of <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-22-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-146\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-147\"><span class=\"mi\" id=\"MathJax-Span-148\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\">n</script> tags:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-23-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x2223;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x22C5;</mo><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></mrow><mrow><munder><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x2032;</mo></msup><mo>&amp;#x2208;</mo><mi>Y</mi></mrow></munder><mi>exp</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x22C5;</mo><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-149\" style=\"width: 16.826em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.013em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.732em, 1014.01em, 4.378em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-150\"><span class=\"mi\" id=\"MathJax-Span-151\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-152\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-153\"><span class=\"mrow\" id=\"MathJax-Span-154\"><span class=\"munderover\" id=\"MathJax-Span-155\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-156\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-157\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-158\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"texatom\" id=\"MathJax-Span-159\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-160\"><span class=\"munderover\" id=\"MathJax-Span-161\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-162\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-163\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-164\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"texatom\" id=\"MathJax-Span-165\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-166\"><span class=\"munderover\" id=\"MathJax-Span-167\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-168\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-169\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-170\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-171\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-172\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 8.388em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1006.2em, 4.378em, -999.997em); top: -4.685em; left: 50%; margin-left: -3.122em;\"><span class=\"mrow\" id=\"MathJax-Span-173\"><span class=\"mi\" id=\"MathJax-Span-174\" style=\"font-family: STIXGeneral-Regular;\">exp</span><span class=\"mo\" id=\"MathJax-Span-175\"></span><span class=\"mo\" id=\"MathJax-Span-176\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-177\"><span class=\"mrow\" id=\"MathJax-Span-178\"><span class=\"munderover\" id=\"MathJax-Span-179\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-180\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-181\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-182\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-183\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-184\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-185\"><span class=\"mrow\" id=\"MathJax-Span-186\"><span class=\"munderover\" id=\"MathJax-Span-187\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-188\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-189\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-190\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-191\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-192\"><span class=\"munderover\" id=\"MathJax-Span-193\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-194\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-195\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-196\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-197\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.076em, 1008.18em, 5.419em, -999.997em); top: -3.279em; left: 50%; margin-left: -4.112em;\"><span class=\"mrow\" id=\"MathJax-Span-198\"><span class=\"munderover\" id=\"MathJax-Span-199\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0.263em;\"><span class=\"mo\" id=\"MathJax-Span-200\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1001.46em, 4.43em, -999.997em); top: -2.966em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-201\"><span class=\"mrow\" id=\"MathJax-Span-202\"><span class=\"msup\" id=\"MathJax-Span-203\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-204\"><span class=\"mrow\" id=\"MathJax-Span-205\"><span class=\"munderover\" id=\"MathJax-Span-206\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-207\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.21em, 3.805em, -999.997em); top: -4.06em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-208\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-209\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-210\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-211\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-212\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">exp</span><span class=\"mo\" id=\"MathJax-Span-213\"></span><span class=\"mo\" id=\"MathJax-Span-214\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-215\"><span class=\"mrow\" id=\"MathJax-Span-216\"><span class=\"munderover\" id=\"MathJax-Span-217\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-218\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-219\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-220\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-221\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-222\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-223\"><span class=\"mrow\" id=\"MathJax-Span-224\"><span class=\"munderover\" id=\"MathJax-Span-225\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-226\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-227\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-228\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msup\" id=\"MathJax-Span-229\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-230\"><span class=\"mrow\" id=\"MathJax-Span-231\"><span class=\"munderover\" id=\"MathJax-Span-232\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-233\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-234\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"mo\" id=\"MathJax-Span-235\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-236\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-237\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1008.39em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 8.388em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.434em; border-left: 0px solid; width: 0px; height: 4.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>P</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>∣</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>;</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>⋅</mo><mi>F</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><mrow><munder><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>′</mo></msup><mo>∈</mo><mi>Y</mi></mrow></munder><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>⋅</mo><mi>F</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>′</mo></msup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow></mfrac></math></span></span></div>\n<ul>\n  <li>\n    <p>This can been seen as another log-linear model, but “giant” in the sense that:</p>\n\n    <ul>\n      <li>The space of possible values for <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-238\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-239\"><span class=\"texatom\" id=\"MathJax-Span-240\"><span class=\"mrow\" id=\"MathJax-Span-241\"><span class=\"munderover\" id=\"MathJax-Span-242\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-243\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">\\bar{y}</script>, i.e., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-245\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-246\"><span class=\"msubsup\" id=\"MathJax-Span-247\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-248\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-249\"><span class=\"mrow\" id=\"MathJax-Span-250\"><span class=\"mi\" id=\"MathJax-Span-251\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>Y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">Y^{n}</script>, is huge, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-252\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-253\"><span class=\"mi\" id=\"MathJax-Span-254\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">n</script> is the since of the sequence.</li>\n      <li>The normalization constant involves a sum over the set <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-255\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-256\"><span class=\"msubsup\" id=\"MathJax-Span-257\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-258\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-259\"><span class=\"mrow\" id=\"MathJax-Span-260\"><span class=\"mi\" id=\"MathJax-Span-261\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>Y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">Y^{n}</script>.</li>\n    </ul>\n  </li>\n  <li>\n    <p><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-262\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-263\"><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">F</script> will represent a global feature vector defined by a set of feature functions <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-265\" style=\"width: 4.534em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.75em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-266\"><span class=\"msubsup\" id=\"MathJax-Span-267\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-268\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-269\"><span class=\"mrow\" id=\"MathJax-Span-270\"><span class=\"mn\" id=\"MathJax-Span-271\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-272\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-273\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-274\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-275\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-276\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-277\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-278\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-279\"><span class=\"mrow\" id=\"MathJax-Span-280\"><span class=\"mi\" id=\"MathJax-Span-281\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">f_{1},...,f_{d}</script>, where each feature function <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-282\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.58em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-283\"><span class=\"msubsup\" id=\"MathJax-Span-284\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-285\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-286\"><span class=\"mrow\" id=\"MathJax-Span-287\"><span class=\"mi\" id=\"MathJax-Span-288\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">f_{j}</script> can analyse the whole <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-289\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-290\"><span class=\"texatom\" id=\"MathJax-Span-291\"><span class=\"mrow\" id=\"MathJax-Span-292\"><span class=\"munderover\" id=\"MathJax-Span-293\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-294\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-295\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">\\bar{x}</script> sequence, the current <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-296\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-297\"><span class=\"msubsup\" id=\"MathJax-Span-298\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-299\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-300\"><span class=\"mrow\" id=\"MathJax-Span-301\"><span class=\"mi\" id=\"MathJax-Span-302\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">y_{i}</script> and previous <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-303\" style=\"width: 1.878em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.57em, 2.555em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-304\"><span class=\"msubsup\" id=\"MathJax-Span-305\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-307\"><span class=\"mrow\" id=\"MathJax-Span-308\"><span class=\"mi\" id=\"MathJax-Span-309\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-310\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-311\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">y_{i-1}</script> positions in the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-312\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-313\"><span class=\"texatom\" id=\"MathJax-Span-314\"><span class=\"mrow\" id=\"MathJax-Span-315\"><span class=\"munderover\" id=\"MathJax-Span-316\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-317\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-318\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">\\bar{y}</script> labels sequence, and the current position <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-319\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-320\"><span class=\"mi\" id=\"MathJax-Span-321\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">i</script> in the sentence:</p>\n  </li>\n</ul>\n<p>This can been seen as another log-linear model, but “giant” in the sense that:</p>\n<ul>\n      <li>The space of possible values for <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-24-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-238\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-239\"><span class=\"texatom\" id=\"MathJax-Span-240\"><span class=\"mrow\" id=\"MathJax-Span-241\"><span class=\"munderover\" id=\"MathJax-Span-242\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-243\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-244\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\">\\bar{y}</script>, i.e., <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-25-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-245\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-246\"><span class=\"msubsup\" id=\"MathJax-Span-247\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-248\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-249\"><span class=\"mrow\" id=\"MathJax-Span-250\"><span class=\"mi\" id=\"MathJax-Span-251\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>Y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\">Y^{n}</script>, is huge, where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-26-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-252\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-253\"><span class=\"mi\" id=\"MathJax-Span-254\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\">n</script> is the since of the sequence.</li>\n      <li>The normalization constant involves a sum over the set <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-27-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>Y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-255\" style=\"width: 1.409em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.15em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-256\"><span class=\"msubsup\" id=\"MathJax-Span-257\"><span style=\"display: inline-block; position: relative; width: 1.148em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-258\" style=\"font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.732em;\"><span class=\"texatom\" id=\"MathJax-Span-259\"><span class=\"mrow\" id=\"MathJax-Span-260\"><span class=\"mi\" id=\"MathJax-Span-261\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">n</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mi>Y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>n</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">Y^{n}</script>.</li>\n    </ul>\n<p><span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-28-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-262\" style=\"width: 0.784em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.63em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-263\"><span class=\"mi\" id=\"MathJax-Span-264\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-28\">F</script> will represent a global feature vector defined by a set of feature functions <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-29-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-265\" style=\"width: 4.534em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 3.753em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1003.75em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-266\"><span class=\"msubsup\" id=\"MathJax-Span-267\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-268\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-269\"><span class=\"mrow\" id=\"MathJax-Span-270\"><span class=\"mn\" id=\"MathJax-Span-271\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-272\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-273\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-274\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-275\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">.</span><span class=\"mo\" id=\"MathJax-Span-276\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-277\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-278\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-279\"><span class=\"mrow\" id=\"MathJax-Span-280\"><span class=\"mi\" id=\"MathJax-Span-281\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>,</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-29\">f_{1},...,f_{d}</script>, where each feature function <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-30-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-282\" style=\"width: 0.732em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.58em, 2.607em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-283\"><span class=\"msubsup\" id=\"MathJax-Span-284\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-285\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-286\"><span class=\"mrow\" id=\"MathJax-Span-287\"><span class=\"mi\" id=\"MathJax-Span-288\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.434em; border-left: 0px solid; width: 0px; height: 1.316em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-30\">f_{j}</script> can analyse the whole <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-31-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-289\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-290\"><span class=\"texatom\" id=\"MathJax-Span-291\"><span class=\"mrow\" id=\"MathJax-Span-292\"><span class=\"munderover\" id=\"MathJax-Span-293\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-294\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-295\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-31\">\\bar{x}</script> sequence, the current <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-32-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-296\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-297\"><span class=\"msubsup\" id=\"MathJax-Span-298\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-299\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-300\"><span class=\"mrow\" id=\"MathJax-Span-301\"><span class=\"mi\" id=\"MathJax-Span-302\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-32\">y_{i}</script> and previous <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-33-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-303\" style=\"width: 1.878em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.57em, 2.555em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-304\"><span class=\"msubsup\" id=\"MathJax-Span-305\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-306\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-307\"><span class=\"mrow\" id=\"MathJax-Span-308\"><span class=\"mi\" id=\"MathJax-Span-309\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-310\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-311\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-33\">y_{i-1}</script> positions in the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-34-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-312\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-313\"><span class=\"texatom\" id=\"MathJax-Span-314\"><span class=\"mrow\" id=\"MathJax-Span-315\"><span class=\"munderover\" id=\"MathJax-Span-316\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-317\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-318\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-34\">\\bar{y}</script> labels sequence, and the current position <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-35-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>i</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-319\" style=\"width: 0.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.263em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.26em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-320\"><span class=\"mi\" id=\"MathJax-Span-321\" style=\"font-family: STIXGeneral-Italic;\">i</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>i</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-35\">i</script> in the sentence:</p>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-36-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><munder><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></munder><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><mi>i</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-322\" style=\"width: 13.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.992em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1010.94em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-323\"><span class=\"mi\" id=\"MathJax-Span-324\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-325\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-326\"><span class=\"mrow\" id=\"MathJax-Span-327\"><span class=\"munderover\" id=\"MathJax-Span-328\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-329\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-330\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-331\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-332\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-333\"><span class=\"munderover\" id=\"MathJax-Span-334\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-335\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-336\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-337\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-338\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-339\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-340\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.273em, -999.997em); top: -2.862em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-341\"><span class=\"mrow\" id=\"MathJax-Span-342\"><span class=\"mi\" id=\"MathJax-Span-343\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-344\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-345\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-346\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-347\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-348\"><span class=\"mrow\" id=\"MathJax-Span-349\"><span class=\"mi\" id=\"MathJax-Span-350\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-351\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-352\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-353\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-354\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-355\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-356\"><span class=\"mrow\" id=\"MathJax-Span-357\"><span class=\"mi\" id=\"MathJax-Span-358\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-359\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-360\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-361\"><span class=\"munderover\" id=\"MathJax-Span-362\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-363\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-364\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-365\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-366\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">i</span><span class=\"mo\" id=\"MathJax-Span-367\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 2.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>F</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo><mo>=</mo><munder><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></munder><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>\n    <p>We can define an arbitrary number of feature functions. The <em>k</em>’th global feature is then computed by summing the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-368\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.68em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-369\"><span class=\"msubsup\" id=\"MathJax-Span-370\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-371\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-372\"><span class=\"mrow\" id=\"MathJax-Span-373\"><span class=\"mi\" id=\"MathJax-Span-374\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">f_{k}</script> over all the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-375\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-376\"><span class=\"mi\" id=\"MathJax-Span-377\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">n</script> different state transitions <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-378\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-379\"><span class=\"texatom\" id=\"MathJax-Span-380\"><span class=\"mrow\" id=\"MathJax-Span-381\"><span class=\"munderover\" id=\"MathJax-Span-382\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-383\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-384\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">\\bar{y}</script>. In this way we have a “global” feature vector that maps the entire sequence: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;normal&quot;>I</mi><mspace width=&quot;negativethinmathspace&quot; /><mi mathvariant=&quot;normal&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-385\" style=\"width: 6.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.159em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1005.16em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-386\"><span class=\"mi\" id=\"MathJax-Span-387\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-388\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-389\"><span class=\"mrow\" id=\"MathJax-Span-390\"><span class=\"munderover\" id=\"MathJax-Span-391\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-392\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-393\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-394\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-395\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-396\"><span class=\"munderover\" id=\"MathJax-Span-397\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-398\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-399\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-400\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-401\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-402\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-403\"><span class=\"mrow\" id=\"MathJax-Span-404\"><span class=\"mi\" id=\"MathJax-Span-405\" style=\"font-family: STIXGeneral-Regular;\">I</span><span class=\"mspace\" id=\"MathJax-Span-406\" style=\"height: 0em; vertical-align: 0em; margin-left: -0.206em;\"></span><span class=\"mi\" id=\"MathJax-Span-407\" style=\"font-family: STIXGeneral-Regular;\">R</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-408\"><span class=\"mrow\" id=\"MathJax-Span-409\"><span class=\"mi\" id=\"MathJax-Span-410\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"normal\">I</mi><mspace width=\"negativethinmathspace\"></mspace><mi mathvariant=\"normal\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">F(\\bar{x}, \\bar{y}) \\in {\\rm I\\!R}^{d}</script>.</p>\n  </li>\n  <li>\n    <p>Thus, the full expanded linear-chain CRF equation is (figure taken from Sameer Maskey slides):</p>\n  </li>\n</ul>\n<p>We can define an arbitrary number of feature functions. The <em>k</em>’th global feature is then computed by summing the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-37-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>k</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-368\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1000.68em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-369\"><span class=\"msubsup\" id=\"MathJax-Span-370\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-371\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-372\"><span class=\"mrow\" id=\"MathJax-Span-373\"><span class=\"mi\" id=\"MathJax-Span-374\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">k<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>k</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-37\">f_{k}</script> over all the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-38-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>n</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-375\" style=\"width: 0.628em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.617em, 1000.52em, 2.346em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-376\"><span class=\"mi\" id=\"MathJax-Span-377\" style=\"font-family: STIXGeneral-Italic;\">n</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-38\">n</script> different state transitions <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-39-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-378\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-379\"><span class=\"texatom\" id=\"MathJax-Span-380\"><span class=\"mrow\" id=\"MathJax-Span-381\"><span class=\"munderover\" id=\"MathJax-Span-382\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-383\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-384\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-39\">\\bar{y}</script>. In this way we have a “global” feature vector that maps the entire sequence: <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-40-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;normal&quot;>I</mi><mspace width=&quot;negativethinmathspace&quot; /><mi mathvariant=&quot;normal&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-385\" style=\"width: 6.201em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 5.159em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.201em, 1005.16em, 2.607em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-386\"><span class=\"mi\" id=\"MathJax-Span-387\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-388\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-389\"><span class=\"mrow\" id=\"MathJax-Span-390\"><span class=\"munderover\" id=\"MathJax-Span-391\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-392\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-393\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-394\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-395\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-396\"><span class=\"munderover\" id=\"MathJax-Span-397\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-398\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-399\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-400\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-401\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-402\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.253em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.78em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-403\"><span class=\"mrow\" id=\"MathJax-Span-404\"><span class=\"mi\" id=\"MathJax-Span-405\" style=\"font-family: STIXGeneral-Regular;\">I</span><span class=\"mspace\" id=\"MathJax-Span-406\" style=\"height: 0em; vertical-align: 0em; margin-left: -0.206em;\"></span><span class=\"mi\" id=\"MathJax-Span-407\" style=\"font-family: STIXGeneral-Regular;\">R</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.836em;\"><span class=\"texatom\" id=\"MathJax-Span-408\"><span class=\"mrow\" id=\"MathJax-Span-409\"><span class=\"mi\" id=\"MathJax-Span-410\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.441em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"normal\">I</mi><mspace width=\"negativethinmathspace\"></mspace><mi mathvariant=\"normal\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-40\">F(\\bar{x}, \\bar{y}) \\in {\\rm I\\!R}^{d}</script>.</p>\n<p>Thus, the full expanded linear-chain CRF equation is (figure taken from Sameer Maskey slides):</p>\n<p><img src=\"/primers/ai/assets/conditional-random-fields/CRF_Equation.png\" alt=\"\"></p>\n<ul>\n  <li>Having the framework defined by the equation above we now analyze how to perform two operations: parameter estimation and sequence prediction.</li>\n</ul>",
      "contentMarkdown": "*   Let x¯x¯\\\\bar{x} is a sequence of words and y¯y¯\\\\bar{y} a corresponding sequence of nnn tags:\n\nP(y¯∣x¯;w¯)\\=exp(w¯⋅F(x¯,y¯))∑y¯′∈Yexp(w¯⋅F(x¯,y¯′))P(y¯∣x¯;w¯)\\=exp⁡(w¯⋅F(x¯,y¯))∑y¯′∈Yexp⁡(w¯⋅F(x¯,y¯′))\n\n*   This can been seen as another log-linear model, but “giant” in the sense that:\n    \n    *   The space of possible values for y¯y¯\\\\bar{y}, i.e., YnYnY^{n}, is huge, where nnn is the since of the sequence.\n    *   The normalization constant involves a sum over the set YnYnY^{n}.\n*   FFF will represent a global feature vector defined by a set of feature functions f1,...,fdf1,...,fdf\\_{1},...,f\\_{d}, where each feature function fjfjf\\_{j} can analyse the whole x¯x¯\\\\bar{x} sequence, the current yiyiy\\_{i} and previous yi−1yi−1y\\_{i-1} positions in the y¯y¯\\\\bar{y} labels sequence, and the current position iii in the sentence:\n    \n\nThis can been seen as another log-linear model, but “giant” in the sense that:\n\n*   The space of possible values for y¯y¯\\\\bar{y}, i.e., YnYnY^{n}, is huge, where nnn is the since of the sequence.\n*   The normalization constant involves a sum over the set YnYnY^{n}.\n\nFFF will represent a global feature vector defined by a set of feature functions f1,...,fdf1,...,fdf\\_{1},...,f\\_{d}, where each feature function fjfjf\\_{j} can analyse the whole x¯x¯\\\\bar{x} sequence, the current yiyiy\\_{i} and previous yi−1yi−1y\\_{i-1} positions in the y¯y¯\\\\bar{y} labels sequence, and the current position iii in the sentence:\n\nF(x¯,y¯)\\=∑if(yi−1,yi,x¯,i)F(x¯,y¯)\\=∑if(yi−1,yi,x¯,i)\n\n*   We can define an arbitrary number of feature functions. The _k_’th global feature is then computed by summing the fkfkf\\_{k} over all the nnn different state transitions y¯y¯\\\\bar{y}. In this way we have a “global” feature vector that maps the entire sequence: F(x¯,y¯)∈IRdF(x¯,y¯)∈IRdF(\\\\bar{x}, \\\\bar{y}) \\\\in {\\\\rm I\\\\!R}^{d}.\n    \n*   Thus, the full expanded linear-chain CRF equation is (figure taken from Sameer Maskey slides):\n    \n\nWe can define an arbitrary number of feature functions. The _k_’th global feature is then computed by summing the fkfkf\\_{k} over all the nnn different state transitions y¯y¯\\\\bar{y}. In this way we have a “global” feature vector that maps the entire sequence: F(x¯,y¯)∈IRdF(x¯,y¯)∈IRdF(\\\\bar{x}, \\\\bar{y}) \\\\in {\\\\rm I\\\\!R}^{d}.\n\nThus, the full expanded linear-chain CRF equation is (figure taken from Sameer Maskey slides):\n\n![](/primers/ai/assets/conditional-random-fields/CRF_Equation.png)\n\n*   Having the framework defined by the equation above we now analyze how to perform two operations: parameter estimation and sequence prediction.",
      "order": 3,
      "orderInChapter": 3,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": true,
        "wordCount": 349,
        "contentLength": 107211
      },
      "nextCards": [
        "ai-conditional-random-fields-inference-4",
        "ai-conditional-random-fields-parameter-estimation-5"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/conditional-random-fields/#linear-chain-crfs",
      "scrapedAt": "2025-12-28T11:57:57.127Z",
      "siblings": [
        "ai-conditional-random-fields-label-bias-problem-in-memms-1",
        "ai-conditional-random-fields-undirected-graphical-models-2",
        "ai-conditional-random-fields-inference-4",
        "ai-conditional-random-fields-parameter-estimation-5",
        "ai-conditional-random-fields-wrapping-up-hmm-vs-memm-vs-crf-6"
      ]
    },
    {
      "id": "ai-conditional-random-fields-inference-4",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Conditional Random Fields",
      "articleSlug": "conditional-random-fields",
      "chapter": "Introduction",
      "title": "Inference",
      "subtitle": "Introduction",
      "contentHtml": "<ul>\n  <li>Inference with a linear-chain CRF resolves to computing the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-41-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-411\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-412\"><span class=\"texatom\" id=\"MathJax-Span-413\"><span class=\"mrow\" id=\"MathJax-Span-414\"><span class=\"munderover\" id=\"MathJax-Span-415\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-416\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-417\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-41\">\\bar{y}</script> sequence that maximizes the following equation:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-42-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mo>=</mo><munder><mrow><mi>arg</mi><mo>&amp;#x2061;</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></munder><mtext>&amp;#xA0;</mtext><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x2223;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x22C5;</mo><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></mrow><mrow><munder><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x2032;</mo></msup><mo>&amp;#x2208;</mo><mi>Y</mi></mrow></munder><mi>exp</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x22C5;</mo><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-418\" style=\"width: 23.023em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 19.169em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.732em, 1019.17em, 4.378em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-419\"><span class=\"texatom\" id=\"MathJax-Span-420\"><span class=\"mrow\" id=\"MathJax-Span-421\"><span class=\"munderover\" id=\"MathJax-Span-422\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-423\"><span class=\"mrow\" id=\"MathJax-Span-424\"><span class=\"munderover\" id=\"MathJax-Span-425\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-426\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-427\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.055em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-428\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-429\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munder\" id=\"MathJax-Span-430\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1003.18em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-431\"><span class=\"mi\" id=\"MathJax-Span-432\" style=\"font-family: STIXGeneral-Regular;\">arg</span><span class=\"mo\" id=\"MathJax-Span-433\"></span><span class=\"mo\" id=\"MathJax-Span-434\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">max</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.43em, -999.997em); top: -3.122em; left: 1.461em;\"><span class=\"texatom\" id=\"MathJax-Span-435\"><span class=\"mrow\" id=\"MathJax-Span-436\"><span class=\"munderover\" id=\"MathJax-Span-437\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-438\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.21em, 3.805em, -999.997em); top: -4.06em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-439\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-440\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-441\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-442\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-443\"><span class=\"mrow\" id=\"MathJax-Span-444\"><span class=\"munderover\" id=\"MathJax-Span-445\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-446\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-447\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-448\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"texatom\" id=\"MathJax-Span-449\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-450\"><span class=\"munderover\" id=\"MathJax-Span-451\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-452\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-453\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-454\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"texatom\" id=\"MathJax-Span-455\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-456\"><span class=\"munderover\" id=\"MathJax-Span-457\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-458\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-459\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-460\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-461\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-462\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 8.388em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1006.2em, 4.378em, -999.997em); top: -4.685em; left: 50%; margin-left: -3.122em;\"><span class=\"mrow\" id=\"MathJax-Span-463\"><span class=\"mi\" id=\"MathJax-Span-464\" style=\"font-family: STIXGeneral-Regular;\">exp</span><span class=\"mo\" id=\"MathJax-Span-465\"></span><span class=\"mo\" id=\"MathJax-Span-466\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-467\"><span class=\"mrow\" id=\"MathJax-Span-468\"><span class=\"munderover\" id=\"MathJax-Span-469\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-470\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-471\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-472\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-473\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-474\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-475\"><span class=\"mrow\" id=\"MathJax-Span-476\"><span class=\"munderover\" id=\"MathJax-Span-477\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-478\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-479\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-480\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-481\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-482\"><span class=\"munderover\" id=\"MathJax-Span-483\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-484\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-485\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-486\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-487\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.076em, 1008.18em, 5.419em, -999.997em); top: -3.279em; left: 50%; margin-left: -4.112em;\"><span class=\"mrow\" id=\"MathJax-Span-488\"><span class=\"munderover\" id=\"MathJax-Span-489\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0.263em;\"><span class=\"mo\" id=\"MathJax-Span-490\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1001.46em, 4.43em, -999.997em); top: -2.966em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-491\"><span class=\"mrow\" id=\"MathJax-Span-492\"><span class=\"msup\" id=\"MathJax-Span-493\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-494\"><span class=\"mrow\" id=\"MathJax-Span-495\"><span class=\"munderover\" id=\"MathJax-Span-496\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-497\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.21em, 3.805em, -999.997em); top: -4.06em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-498\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-499\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-500\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-501\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-502\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">exp</span><span class=\"mo\" id=\"MathJax-Span-503\"></span><span class=\"mo\" id=\"MathJax-Span-504\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-505\"><span class=\"mrow\" id=\"MathJax-Span-506\"><span class=\"munderover\" id=\"MathJax-Span-507\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-508\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-509\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-510\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-511\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-512\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-513\"><span class=\"mrow\" id=\"MathJax-Span-514\"><span class=\"munderover\" id=\"MathJax-Span-515\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-516\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-517\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-518\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msup\" id=\"MathJax-Span-519\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-520\"><span class=\"mrow\" id=\"MathJax-Span-521\"><span class=\"munderover\" id=\"MathJax-Span-522\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-523\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-524\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"mo\" id=\"MathJax-Span-525\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-526\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-527\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1008.39em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 8.388em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.434em; border-left: 0px solid; width: 0px; height: 4.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">^</mo></mover></mrow><mo>=</mo><munder><mrow><mi>arg</mi><mo>⁡</mo><mo movablelimits=\"true\" form=\"prefix\">max</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow></munder><mtext>&nbsp;</mtext><mi>P</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>∣</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>;</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>⋅</mo><mi>F</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><mrow><munder><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>′</mo></msup><mo>∈</mo><mi>Y</mi></mrow></munder><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>⋅</mo><mi>F</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>′</mo></msup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow></mfrac></math></span></span></div>\n<ul>\n  <li>We want to try all possible <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-43-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-528\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-529\"><span class=\"texatom\" id=\"MathJax-Span-530\"><span class=\"mrow\" id=\"MathJax-Span-531\"><span class=\"munderover\" id=\"MathJax-Span-532\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-533\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-534\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-43\">\\bar{y}</script> sequences computing for each one the probability of “fitting” the observation <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-44-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-535\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-536\"><span class=\"texatom\" id=\"MathJax-Span-537\"><span class=\"mrow\" id=\"MathJax-Span-538\"><span class=\"munderover\" id=\"MathJax-Span-539\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-540\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-541\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-44\">\\bar{x}</script> with feature weights <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-45-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-542\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-543\"><span class=\"texatom\" id=\"MathJax-Span-544\"><span class=\"mrow\" id=\"MathJax-Span-545\"><span class=\"munderover\" id=\"MathJax-Span-546\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-547\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-548\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-45\">\\bar{w}</script>. If we just want the score for a particular labelling sequence <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-46-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-549\" style=\"width: 0.576em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.47em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-550\"><span class=\"texatom\" id=\"MathJax-Span-551\"><span class=\"mrow\" id=\"MathJax-Span-552\"><span class=\"munderover\" id=\"MathJax-Span-553\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-554\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-555\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-46\">\\bar{y}</script>, we can ignore the exponential inside the numerator, and the denominator:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-47-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mo>=</mo><munder><mrow><mi>arg</mi><mo>&amp;#x2061;</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></munder><mtext>&amp;#xA0;</mtext><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x2223;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>;</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><munder><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></munder><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mtext>&amp;#xA0;</mtext><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-556\" style=\"width: 18.909em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 15.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.096em, 1015.68em, 3.805em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-557\"><span class=\"texatom\" id=\"MathJax-Span-558\"><span class=\"mrow\" id=\"MathJax-Span-559\"><span class=\"munderover\" id=\"MathJax-Span-560\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-561\"><span class=\"mrow\" id=\"MathJax-Span-562\"><span class=\"munderover\" id=\"MathJax-Span-563\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-564\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-565\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.055em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-566\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-567\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munder\" id=\"MathJax-Span-568\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1003.18em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-569\"><span class=\"mi\" id=\"MathJax-Span-570\" style=\"font-family: STIXGeneral-Regular;\">arg</span><span class=\"mo\" id=\"MathJax-Span-571\"></span><span class=\"mo\" id=\"MathJax-Span-572\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">max</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.43em, -999.997em); top: -3.122em; left: 1.461em;\"><span class=\"texatom\" id=\"MathJax-Span-573\"><span class=\"mrow\" id=\"MathJax-Span-574\"><span class=\"munderover\" id=\"MathJax-Span-575\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-576\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.21em, 3.805em, -999.997em); top: -4.06em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-577\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-578\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-579\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-580\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-581\"><span class=\"mrow\" id=\"MathJax-Span-582\"><span class=\"munderover\" id=\"MathJax-Span-583\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-584\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-585\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-586\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"texatom\" id=\"MathJax-Span-587\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-588\"><span class=\"munderover\" id=\"MathJax-Span-589\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-590\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-591\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-592\" style=\"font-family: STIXGeneral-Regular;\">;</span><span class=\"mi\" id=\"MathJax-Span-593\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">w</span><span class=\"mo\" id=\"MathJax-Span-594\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-595\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-596\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-597\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.43em, -999.997em); top: -2.862em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-598\"><span class=\"mrow\" id=\"MathJax-Span-599\"><span class=\"mi\" id=\"MathJax-Span-600\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-601\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-602\"><span class=\"munderover\" id=\"MathJax-Span-603\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-604\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-605\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-606\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-607\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-608\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-609\"><span class=\"mrow\" id=\"MathJax-Span-610\"><span class=\"munderover\" id=\"MathJax-Span-611\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-612\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-613\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-614\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-615\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-616\"><span class=\"munderover\" id=\"MathJax-Span-617\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-618\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-619\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-620\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.747em; border-left: 0px solid; width: 0px; height: 3.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">^</mo></mover></mrow><mo>=</mo><munder><mrow><mi>arg</mi><mo>⁡</mo><mo movablelimits=\"true\" form=\"prefix\">max</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow></munder><mtext>&nbsp;</mtext><mi>P</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>∣</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>;</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>=</mo><munder><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></munder><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mtext>&nbsp;</mtext><mi>F</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>Then, we replace <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-48-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-621\" style=\"width: 3.232em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.659em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.61em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-622\"><span class=\"mi\" id=\"MathJax-Span-623\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-624\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-625\"><span class=\"mrow\" id=\"MathJax-Span-626\"><span class=\"munderover\" id=\"MathJax-Span-627\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-628\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-629\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-630\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-631\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-632\"><span class=\"munderover\" id=\"MathJax-Span-633\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-634\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-635\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-636\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 1.191em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>F</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-48\">F(\\bar{x},\\bar{y})</script> by it’s definition:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-49-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>&amp;#x005E;</mo></mover></mrow><mo>=</mo><munder><mrow><mi>arg</mi><mo>&amp;#x2061;</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></munder><mtext>&amp;#xA0;</mtext><munder><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></munder><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mtext>&amp;#xA0;</mtext><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><mi>i</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-637\" style=\"width: 16.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.388em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1013.34em, 3.596em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-638\"><span class=\"texatom\" id=\"MathJax-Span-639\"><span class=\"mrow\" id=\"MathJax-Span-640\"><span class=\"munderover\" id=\"MathJax-Span-641\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-642\"><span class=\"mrow\" id=\"MathJax-Span-643\"><span class=\"munderover\" id=\"MathJax-Span-644\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-645\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-646\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.055em;\"><span style=\"height: 0em; vertical-align: 0em; width: 0.367em; display: inline-block; overflow: hidden;\"></span><span class=\"mo\" id=\"MathJax-Span-647\" style=\"font-family: STIXGeneral-Regular;\">̂&nbsp;<span style=\"height: 0em; vertical-align: 0em; margin-left: -0.258em;\"></span></span><span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0em;\"></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-648\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munder\" id=\"MathJax-Span-649\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1003.18em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-650\"><span class=\"mi\" id=\"MathJax-Span-651\" style=\"font-family: STIXGeneral-Regular;\">arg</span><span class=\"mo\" id=\"MathJax-Span-652\"></span><span class=\"mo\" id=\"MathJax-Span-653\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">max</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.43em, -999.997em); top: -3.122em; left: 1.461em;\"><span class=\"texatom\" id=\"MathJax-Span-654\"><span class=\"mrow\" id=\"MathJax-Span-655\"><span class=\"munderover\" id=\"MathJax-Span-656\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-657\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.21em, 3.805em, -999.997em); top: -4.06em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-658\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-659\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"munderover\" id=\"MathJax-Span-660\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-661\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.273em, -999.997em); top: -2.862em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-662\"><span class=\"mrow\" id=\"MathJax-Span-663\"><span class=\"mi\" id=\"MathJax-Span-664\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-665\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-666\"><span class=\"munderover\" id=\"MathJax-Span-667\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-668\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-669\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-670\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-671\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-672\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-673\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-674\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-675\"><span class=\"mrow\" id=\"MathJax-Span-676\"><span class=\"mi\" id=\"MathJax-Span-677\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-678\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-679\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-680\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-681\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-682\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-683\"><span class=\"mrow\" id=\"MathJax-Span-684\"><span class=\"mi\" id=\"MathJax-Span-685\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-686\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-687\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-688\"><span class=\"munderover\" id=\"MathJax-Span-689\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-690\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-691\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-692\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-693\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">i</span><span class=\"mo\" id=\"MathJax-Span-694\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 2.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">^</mo></mover></mrow><mo>=</mo><munder><mrow><mi>arg</mi><mo>⁡</mo><mo movablelimits=\"true\" form=\"prefix\">max</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow></munder><mtext>&nbsp;</mtext><munder><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></munder><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mtext>&nbsp;</mtext><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>Each transition from state <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-50-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-695\" style=\"width: 1.878em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.57em, 2.555em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-696\"><span class=\"msubsup\" id=\"MathJax-Span-697\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-698\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-699\"><span class=\"mrow\" id=\"MathJax-Span-700\"><span class=\"mi\" id=\"MathJax-Span-701\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-702\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-703\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-50\">y_{i-1}</script> to state <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-51-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-704\" style=\"width: 0.888em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1000.73em, 2.503em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-705\"><span class=\"msubsup\" id=\"MathJax-Span-706\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-707\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-708\"><span class=\"mrow\" id=\"MathJax-Span-709\"><span class=\"mi\" id=\"MathJax-Span-710\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-51\">y_{i}</script> has an associated score:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-52-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mtext>&amp;#xA0;</mtext><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><mi>i</mi><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-711\" style=\"width: 7.815em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 6.513em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1006.46em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-712\"><span class=\"texatom\" id=\"MathJax-Span-713\"><span class=\"mrow\" id=\"MathJax-Span-714\"><span class=\"munderover\" id=\"MathJax-Span-715\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-716\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-717\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-718\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-719\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-720\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-721\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-722\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-723\"><span class=\"mrow\" id=\"MathJax-Span-724\"><span class=\"mi\" id=\"MathJax-Span-725\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-726\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-727\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-728\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-729\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-730\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-731\"><span class=\"mrow\" id=\"MathJax-Span-732\"><span class=\"mi\" id=\"MathJax-Span-733\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-734\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-735\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-736\"><span class=\"munderover\" id=\"MathJax-Span-737\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-738\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-739\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-740\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-741\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">i</span><span class=\"mo\" id=\"MathJax-Span-742\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mtext>&nbsp;</mtext><mi>f</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li>\n    <p>Since we took the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>exp</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-743\" style=\"width: 1.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.41em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-744\"><span class=\"mi\" id=\"MathJax-Span-745\" style=\"font-family: STIXGeneral-Regular;\">exp</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>exp</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">\\exp</script> out, this score could be positive or negative, intuitively, this score will be relatively high if the state transition is plausible, relatively low if this transition is implausible.</p>\n  </li>\n  <li>\n    <p>The decoding problem is then to find an entire sequence of states such that the sum of the transition scores is maximized. We can again solve this problem using a variant of the Viterbi algorithm, in a very similar way to the decoding algorithm for HMMs or MEMMs.</p>\n  </li>\n  <li>\n    <p>The denominator, also called the partition function:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-54-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>Z</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><munder><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x2032;</mo></msup><mo>&amp;#x2208;</mo><mi>Y</mi></mrow></munder><mi>exp</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><munder><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></munder><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msub><msub><mi>F</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-746\" style=\"width: 16.461em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.701em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.044em, 1013.65em, 3.805em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-747\"><span class=\"mi\" id=\"MathJax-Span-748\" style=\"font-family: STIXGeneral-Italic;\">Z<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-749\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-750\"><span class=\"mrow\" id=\"MathJax-Span-751\"><span class=\"munderover\" id=\"MathJax-Span-752\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-753\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-754\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-755\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mi\" id=\"MathJax-Span-756\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">w</span><span class=\"mo\" id=\"MathJax-Span-757\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-758\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"texatom\" id=\"MathJax-Span-759\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-760\"><span class=\"munderover\" id=\"MathJax-Span-761\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-762\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1001.46em, 4.43em, -999.997em); top: -2.758em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-763\"><span class=\"mrow\" id=\"MathJax-Span-764\"><span class=\"msup\" id=\"MathJax-Span-765\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-766\"><span class=\"mrow\" id=\"MathJax-Span-767\"><span class=\"munderover\" id=\"MathJax-Span-768\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-769\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.21em, 3.805em, -999.997em); top: -4.06em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-770\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-771\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-772\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-773\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-774\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">exp</span><span class=\"mo\" id=\"MathJax-Span-775\"></span><span class=\"mo\" id=\"MathJax-Span-776\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"munderover\" id=\"MathJax-Span-777\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-778\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.21em, 4.43em, -999.997em); top: -2.862em; left: 0.576em;\"><span class=\"texatom\" id=\"MathJax-Span-779\"><span class=\"mrow\" id=\"MathJax-Span-780\"><span class=\"mi\" id=\"MathJax-Span-781\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-782\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-783\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-784\"><span class=\"mrow\" id=\"MathJax-Span-785\"><span class=\"mi\" id=\"MathJax-Span-786\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-787\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-788\" style=\"font-family: STIXGeneral-Italic;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.628em;\"><span class=\"texatom\" id=\"MathJax-Span-789\"><span class=\"mrow\" id=\"MathJax-Span-790\"><span class=\"mi\" id=\"MathJax-Span-791\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-792\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-793\"><span class=\"mrow\" id=\"MathJax-Span-794\"><span class=\"munderover\" id=\"MathJax-Span-795\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-796\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-797\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-798\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msup\" id=\"MathJax-Span-799\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-800\"><span class=\"mrow\" id=\"MathJax-Span-801\"><span class=\"munderover\" id=\"MathJax-Span-802\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-803\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-804\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.471em;\"><span class=\"mo\" id=\"MathJax-Span-805\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-806\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-807\" style=\"font-family: STIXGeneral-Regular;\">)</span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.809em; border-left: 0px solid; width: 0px; height: 3.066em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>Z</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><mi>w</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mrow class=\"MJX-TeXAtom-ORD\"><munder><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>′</mo></msup><mo>∈</mo><mi>Y</mi></mrow></munder><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><munder><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></munder><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msub><msub><mi>F</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msub><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>′</mo></msup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-54\">Z(\\bar{x},w)= {\\sum\\limits_{\\bar{y}' \\in Y} \\exp(\\sum\\limits_{j} w_{j} F_{j}(\\bar{x},\\bar{y}'))}</script>\n\n    <ul>\n      <li>… is useful to compute a marginal probability. For example, this is useful for measuring the model’s confidence in it’s predicted labeling over a segment of input. This marginal probability can be computed efficiently using the forward-backward algorithm. See the references section for demonstrations on how\nthis is achieved.</li>\n    </ul>\n  </li>\n</ul>\n<p>Since we took the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-53-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>exp</mi></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-743\" style=\"width: 1.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.565em, 1001.41em, 2.555em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-744\"><span class=\"mi\" id=\"MathJax-Span-745\" style=\"font-family: STIXGeneral-Regular;\">exp</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.309em; border-left: 0px solid; width: 0px; height: 0.941em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>exp</mi></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-53\">\\exp</script> out, this score could be positive or negative, intuitively, this score will be relatively high if the state transition is plausible, relatively low if this transition is implausible.</p>\n<p>The decoding problem is then to find an entire sequence of states such that the sum of the transition scores is maximized. We can again solve this problem using a variant of the Viterbi algorithm, in a very similar way to the decoding algorithm for HMMs or MEMMs.</p>\n<p>The denominator, also called the partition function:</p>\n<ul>\n      <li>… is useful to compute a marginal probability. For example, this is useful for measuring the model’s confidence in it’s predicted labeling over a segment of input. This marginal probability can be computed efficiently using the forward-backward algorithm. See the references section for demonstrations on how\nthis is achieved.</li>\n    </ul>",
      "contentMarkdown": "*   Inference with a linear-chain CRF resolves to computing the y¯y¯\\\\bar{y} sequence that maximizes the following equation:\n\ny¯̂ \\=argmaxy¯ P(y¯∣x¯;w¯)\\=exp(w¯⋅F(x¯,y¯))∑y¯′∈Yexp(w¯⋅F(x¯,y¯′))y¯^\\=arg⁡maxy¯ P(y¯∣x¯;w¯)\\=exp⁡(w¯⋅F(x¯,y¯))∑y¯′∈Yexp⁡(w¯⋅F(x¯,y¯′))\n\n*   We want to try all possible y¯y¯\\\\bar{y} sequences computing for each one the probability of “fitting” the observation x¯x¯\\\\bar{x} with feature weights w¯w¯\\\\bar{w}. If we just want the score for a particular labelling sequence y¯y¯\\\\bar{y}, we can ignore the exponential inside the numerator, and the denominator:\n\ny¯̂ \\=argmaxy¯ P(y¯∣x¯;w)\\=∑jw¯ F(x¯,y¯)y¯^\\=arg⁡maxy¯ P(y¯∣x¯;w)\\=∑jw¯ F(x¯,y¯)\n\n*   Then, we replace F(x¯,y¯)F(x¯,y¯)F(\\\\bar{x},\\\\bar{y}) by it’s definition:\n\ny¯̂ \\=argmaxy¯ ∑iw¯ f(yi−1,yi,x¯,i)y¯^\\=arg⁡maxy¯ ∑iw¯ f(yi−1,yi,x¯,i)\n\n*   Each transition from state yi−1yi−1y\\_{i-1} to state yiyiy\\_{i} has an associated score:\n\nw¯ f(yi−1,yi,x¯,i)w¯ f(yi−1,yi,x¯,i)\n\n*   Since we took the expexp\\\\exp out, this score could be positive or negative, intuitively, this score will be relatively high if the state transition is plausible, relatively low if this transition is implausible.\n    \n*   The decoding problem is then to find an entire sequence of states such that the sum of the transition scores is maximized. We can again solve this problem using a variant of the Viterbi algorithm, in a very similar way to the decoding algorithm for HMMs or MEMMs.\n    \n*   The denominator, also called the partition function:\n    \n    Z(x¯,w)\\=∑y¯′∈Yexp(∑jwjFj(x¯,y¯′))Z(x¯,w)\\=∑y¯′∈Yexp⁡(∑jwjFj(x¯,y¯′))\n    \n    Z(\\\\bar{x},w)= {\\\\sum\\\\limits\\_{\\\\bar{y}' \\\\in Y} \\\\exp(\\\\sum\\\\limits\\_{j} w\\_{j} F\\_{j}(\\\\bar{x},\\\\bar{y}'))}\n    *   … is useful to compute a marginal probability. For example, this is useful for measuring the model’s confidence in it’s predicted labeling over a segment of input. This marginal probability can be computed efficiently using the forward-backward algorithm. See the references section for demonstrations on how this is achieved.\n\nSince we took the expexp\\\\exp out, this score could be positive or negative, intuitively, this score will be relatively high if the state transition is plausible, relatively low if this transition is implausible.\n\nThe decoding problem is then to find an entire sequence of states such that the sum of the transition scores is maximized. We can again solve this problem using a variant of the Viterbi algorithm, in a very similar way to the decoding algorithm for HMMs or MEMMs.\n\nThe denominator, also called the partition function:\n\n*   … is useful to compute a marginal probability. For example, this is useful for measuring the model’s confidence in it’s predicted labeling over a segment of input. This marginal probability can be computed efficiently using the forward-backward algorithm. See the references section for demonstrations on how this is achieved.",
      "order": 4,
      "orderInChapter": 4,
      "difficulty": 3,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 391,
        "contentLength": 93193
      },
      "nextCards": [
        "ai-conditional-random-fields-parameter-estimation-5",
        "ai-conditional-random-fields-wrapping-up-hmm-vs-memm-vs-crf-6"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/conditional-random-fields/#inference",
      "scrapedAt": "2025-12-28T11:57:57.127Z",
      "siblings": [
        "ai-conditional-random-fields-label-bias-problem-in-memms-1",
        "ai-conditional-random-fields-undirected-graphical-models-2",
        "ai-conditional-random-fields-linear-chain-crfs-3",
        "ai-conditional-random-fields-parameter-estimation-5",
        "ai-conditional-random-fields-wrapping-up-hmm-vs-memm-vs-crf-6"
      ]
    },
    {
      "id": "ai-conditional-random-fields-parameter-estimation-5",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Conditional Random Fields",
      "articleSlug": "conditional-random-fields",
      "chapter": "Introduction",
      "title": "Parameter Estimation",
      "subtitle": "Introduction",
      "contentHtml": "<ul>\n  <li>\n    <p>We also need to find the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-808\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-809\"><span class=\"texatom\" id=\"MathJax-Span-810\"><span class=\"mrow\" id=\"MathJax-Span-811\"><span class=\"munderover\" id=\"MathJax-Span-812\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-813\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-814\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-55\">\\bar{w}</script> parameters that best fit the training data, a given a set of labelled sentences:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-56-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>{</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>,</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>,</mo><mo>&amp;#x2026;</mo><mo>,</mo><mo stretchy=&quot;false&quot;>(</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>m</mi></mrow></msub><mo>,</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>m</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>}</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-815\" style=\"width: 11.096em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.221em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1009.12em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-816\"><span class=\"mo\" id=\"MathJax-Span-817\" style=\"font-family: STIXGeneral-Regular;\">{</span><span class=\"mo\" id=\"MathJax-Span-818\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-819\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-820\"><span class=\"mrow\" id=\"MathJax-Span-821\"><span class=\"munderover\" id=\"MathJax-Span-822\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-823\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-824\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-825\"><span class=\"mrow\" id=\"MathJax-Span-826\"><span class=\"mn\" id=\"MathJax-Span-827\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-828\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-829\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-830\"><span class=\"mrow\" id=\"MathJax-Span-831\"><span class=\"munderover\" id=\"MathJax-Span-832\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-833\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-834\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-835\"><span class=\"mrow\" id=\"MathJax-Span-836\"><span class=\"mn\" id=\"MathJax-Span-837\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-838\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-839\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"mo\" id=\"MathJax-Span-840\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">…</span><span class=\"mo\" id=\"MathJax-Span-841\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">,</span><span class=\"mo\" id=\"MathJax-Span-842\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-843\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-844\"><span class=\"mrow\" id=\"MathJax-Span-845\"><span class=\"munderover\" id=\"MathJax-Span-846\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-847\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-848\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-849\"><span class=\"mrow\" id=\"MathJax-Span-850\"><span class=\"mi\" id=\"MathJax-Span-851\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-852\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-853\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.044em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-854\"><span class=\"mrow\" id=\"MathJax-Span-855\"><span class=\"munderover\" id=\"MathJax-Span-856\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-857\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-858\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-859\"><span class=\"mrow\" id=\"MathJax-Span-860\"><span class=\"mi\" id=\"MathJax-Span-861\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-862\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-863\" style=\"font-family: STIXGeneral-Regular;\">}</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mo fence=\"false\" stretchy=\"false\">{</mo><mo stretchy=\"false\">(</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>,</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo><mo>,</mo><mo>…</mo><mo>,</mo><mo stretchy=\"false\">(</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>m</mi></mrow></msub><mo>,</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>m</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo fence=\"false\" stretchy=\"false\">}</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-56\">\\{(\\bar{x}_{1}, \\bar{y}_{1}), \\ldots , (\\bar{x}_{m}, \\bar{y}_{m})\\}</script>\n\n    <ul>\n      <li>where each pair <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>,</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-864\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.55em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-865\"><span class=\"mo\" id=\"MathJax-Span-866\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-867\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-868\"><span class=\"mrow\" id=\"MathJax-Span-869\"><span class=\"munderover\" id=\"MathJax-Span-870\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-871\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-872\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-873\"><span class=\"mrow\" id=\"MathJax-Span-874\"><span class=\"mi\" id=\"MathJax-Span-875\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-876\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-877\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-878\"><span class=\"mrow\" id=\"MathJax-Span-879\"><span class=\"munderover\" id=\"MathJax-Span-880\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-881\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-882\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-883\"><span class=\"mrow\" id=\"MathJax-Span-884\"><span class=\"mi\" id=\"MathJax-Span-885\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-886\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>,</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">(\\bar{x}_{i}, \\bar{y}_{i})</script> is a sentence with the corresponding word labels annotated. To find the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-887\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-888\"><span class=\"texatom\" id=\"MathJax-Span-889\"><span class=\"mrow\" id=\"MathJax-Span-890\"><span class=\"munderover\" id=\"MathJax-Span-891\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-892\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-893\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-58\">\\bar{w}</script> parameters that best fit the data we need to\nmaximize the conditional likelihood of the training data:</li>\n    </ul>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-59-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>L</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><munderover><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>m</mi></mrow></munderover><mi>log</mi><mo>&amp;#x2061;</mo><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow></msub><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-894\" style=\"width: 12.711em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 10.576em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.732em, 1010.52em, 3.909em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-895\"><span class=\"mi\" id=\"MathJax-Span-896\" style=\"font-family: STIXGeneral-Italic;\">L<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-897\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-898\"><span class=\"mrow\" id=\"MathJax-Span-899\"><span class=\"munderover\" id=\"MathJax-Span-900\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-901\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-902\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-903\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-904\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-905\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-906\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-907\"><span class=\"mrow\" id=\"MathJax-Span-908\"><span class=\"mi\" id=\"MathJax-Span-909\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-910\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-911\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.419em;\"><span class=\"texatom\" id=\"MathJax-Span-912\"><span class=\"mrow\" id=\"MathJax-Span-913\"><span class=\"mi\" id=\"MathJax-Span-914\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-915\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-916\"></span><span class=\"mi\" id=\"MathJax-Span-917\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">p</span><span class=\"mo\" id=\"MathJax-Span-918\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-919\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-920\"><span class=\"mrow\" id=\"MathJax-Span-921\"><span class=\"munderover\" id=\"MathJax-Span-922\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-923\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-924\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-925\"><span class=\"mrow\" id=\"MathJax-Span-926\"><span class=\"mn\" id=\"MathJax-Span-927\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-928\"><span class=\"mrow\" id=\"MathJax-Span-929\"><span class=\"mo\" id=\"MathJax-Span-930\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"msubsup\" id=\"MathJax-Span-931\"><span style=\"display: inline-block; position: relative; width: 0.888em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-932\"><span class=\"mrow\" id=\"MathJax-Span-933\"><span class=\"munderover\" id=\"MathJax-Span-934\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-935\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-936\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-937\"><span class=\"mrow\" id=\"MathJax-Span-938\"><span class=\"mn\" id=\"MathJax-Span-939\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-940\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-941\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-942\"><span class=\"munderover\" id=\"MathJax-Span-943\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-944\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-945\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-946\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>L</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo><mo>=</mo><munderover><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>m</mi></mrow></munderover><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mn>1</mn></mrow></msub><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-59\">L(\\bar{w}) = \\sum\\limits_{i=1}^{m} \\log p( \\bar{x}_{1} | \\bar{y}_{1}, \\bar{w} )</script>\n  </li>\n  <li>\n    <p>The parameter estimates are computed as:</p>\n\n<span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-60-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x2217;</mo></msup><mo>=</mo><munder><mrow><mi>arg</mi><mo>&amp;#x2061;</mo><mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;>max</mo></mrow><mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mtext>&amp;#xA0;</mtext><mo>&amp;#x2208;</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi mathvariant=&quot;normal&quot;>I</mi><mspace width=&quot;negativethinmathspace&quot; /><mi mathvariant=&quot;normal&quot;>R</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi></mrow></msup></mrow></munder><mtext>&amp;#xA0;</mtext><munderover><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>m</mi></mrow></munderover><mi>log</mi><mo>&amp;#x2061;</mo><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mfrac><mi>&amp;#x03BB;</mi><mn>2</mn></mfrac><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><msup><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msup></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-947\" style=\"width: 20.315em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 16.93em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.732em, 1016.93em, 3.909em, -999.997em); top: -2.497em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-948\"><span class=\"msubsup\" id=\"MathJax-Span-949\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-950\"><span class=\"mrow\" id=\"MathJax-Span-951\"><span class=\"munderover\" id=\"MathJax-Span-952\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-953\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-954\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.68em;\"><span class=\"mo\" id=\"MathJax-Span-955\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-956\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munder\" id=\"MathJax-Span-957\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 3.232em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1003.18em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-958\"><span class=\"mi\" id=\"MathJax-Span-959\" style=\"font-family: STIXGeneral-Regular;\">arg</span><span class=\"mo\" id=\"MathJax-Span-960\"></span><span class=\"mo\" id=\"MathJax-Span-961\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">max</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1002.14em, 4.273em, -999.997em); top: -2.966em; left: 0.523em;\"><span class=\"mrow\" id=\"MathJax-Span-962\"><span class=\"texatom\" id=\"MathJax-Span-963\"><span class=\"mrow\" id=\"MathJax-Span-964\"><span class=\"munderover\" id=\"MathJax-Span-965\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-966\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.21em, 3.805em, -999.997em); top: -4.06em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-967\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-968\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mo\" id=\"MathJax-Span-969\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"msubsup\" id=\"MathJax-Span-970\"><span style=\"display: inline-block; position: relative; width: 0.992em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.68em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-971\"><span class=\"mrow\" id=\"MathJax-Span-972\"><span class=\"mtext\" id=\"MathJax-Span-973\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-974\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">I</span><span class=\"mspace\" id=\"MathJax-Span-975\" style=\"height: 0em; vertical-align: 0em; margin-left: -0.206em;\"></span><span class=\"mi\" id=\"MathJax-Span-976\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">R</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.268em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-977\"><span class=\"mrow\" id=\"MathJax-Span-978\"><span class=\"mi\" id=\"MathJax-Span-979\" style=\"font-size: 50%; font-family: STIXGeneral-Italic;\">d<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-980\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"munderover\" id=\"MathJax-Span-981\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-982\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-983\"><span class=\"mrow\" id=\"MathJax-Span-984\"><span class=\"mi\" id=\"MathJax-Span-985\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-986\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-987\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.419em;\"><span class=\"texatom\" id=\"MathJax-Span-988\"><span class=\"mrow\" id=\"MathJax-Span-989\"><span class=\"mi\" id=\"MathJax-Span-990\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">m</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-991\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">log</span><span class=\"mo\" id=\"MathJax-Span-992\"></span><span class=\"mi\" id=\"MathJax-Span-993\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">p</span><span class=\"mo\" id=\"MathJax-Span-994\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-995\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-996\"><span class=\"mrow\" id=\"MathJax-Span-997\"><span class=\"munderover\" id=\"MathJax-Span-998\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-999\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-1000\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1001\"><span class=\"mrow\" id=\"MathJax-Span-1002\"><span class=\"mi\" id=\"MathJax-Span-1003\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-1004\"><span class=\"mrow\" id=\"MathJax-Span-1005\"><span class=\"mo\" id=\"MathJax-Span-1006\" style=\"font-family: STIXVariants;\">|</span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1007\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1008\"><span class=\"mrow\" id=\"MathJax-Span-1009\"><span class=\"munderover\" id=\"MathJax-Span-1010\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1011\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-1012\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1013\"><span class=\"mrow\" id=\"MathJax-Span-1014\"><span class=\"mi\" id=\"MathJax-Span-1015\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1016\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-1017\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-1018\"><span class=\"munderover\" id=\"MathJax-Span-1019\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1020\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-1021\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1022\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1023\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">−</span><span class=\"mfrac\" id=\"MathJax-Span-1024\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.628em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.206em;\"><span class=\"mi\" id=\"MathJax-Span-1025\" style=\"font-family: STIXGeneral-Italic;\">λ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1000.47em, 4.169em, -999.997em); top: -3.331em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-1026\" style=\"font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.63em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.628em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1027\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"texatom\" id=\"MathJax-Span-1028\"><span class=\"mrow\" id=\"MathJax-Span-1029\"><span class=\"munderover\" id=\"MathJax-Span-1030\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1031\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-1032\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1033\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1034\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.424em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1035\"><span class=\"mrow\" id=\"MathJax-Span-1036\"><span class=\"mn\" id=\"MathJax-Span-1037\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.503em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.566em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>∗</mo></msup><mo>=</mo><munder><mrow><mi>arg</mi><mo>⁡</mo><mo movablelimits=\"true\" form=\"prefix\">max</mo></mrow><mrow><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mtext>&nbsp;</mtext><mo>∈</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mtext>&nbsp;</mtext><mi mathvariant=\"normal\">I</mi><mspace width=\"negativethinmathspace\"></mspace><mi mathvariant=\"normal\">R</mi></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>d</mi></mrow></msup></mrow></munder><mtext>&nbsp;</mtext><munderover><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>m</mi></mrow></munderover><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo><mo>−</mo><mfrac><mi>λ</mi><mn>2</mn></mfrac><mo fence=\"false\" stretchy=\"false\">‖</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><msup><mo fence=\"false\" stretchy=\"false\">‖</mo><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msup></math></span></span></div><script type=\"math/tex; mode=display\" id=\"MathJax-Element-60\">\\bar{w}^* = \\underset{\\bar{w}\\ \\in {\\rm \\ I\\!R}^{d}} {\\arg\\max}\\ \\sum\\limits_{i=1}^{m} \\log p( \\bar{x}_{i} | \\bar{y}_{i}, \\bar{w}) - \\frac{\\lambda}{2} \\| \\bar{w} \\| ^{2}</script>\n\n    <ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mfrac><mi>&amp;#x03BB;</mi><mn>2</mn></mfrac><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><msup><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1038\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1002.61em, 2.711em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1039\"><span class=\"mfrac\" id=\"MathJax-Span-1040\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.154em;\"><span class=\"mi\" id=\"MathJax-Span-1041\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">λ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.154em;\"><span class=\"mn\" id=\"MathJax-Span-1042\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.47em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.471em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1043\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"texatom\" id=\"MathJax-Span-1044\"><span class=\"mrow\" id=\"MathJax-Span-1045\"><span class=\"munderover\" id=\"MathJax-Span-1046\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1047\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-1048\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1049\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1050\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1051\"><span class=\"mrow\" id=\"MathJax-Span-1052\"><span class=\"mn\" id=\"MathJax-Span-1053\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mfrac><mi>λ</mi><mn>2</mn></mfrac><mo fence=\"false\" stretchy=\"false\">‖</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><msup><mo fence=\"false\" stretchy=\"false\">‖</mo><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">\\frac{\\lambda}{2} \\| \\bar{w} \\| ^{2}</script> is an L2 regularization term.</li>\n    </ul>\n  </li>\n  <li>\n    <p>The standard approach to finding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-62-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x2217;</mo></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1054\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.1em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1055\"><span class=\"msubsup\" id=\"MathJax-Span-1056\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1057\"><span class=\"mrow\" id=\"MathJax-Span-1058\"><span class=\"munderover\" id=\"MathJax-Span-1059\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1060\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-1061\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"mo\" id=\"MathJax-Span-1062\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>∗</mo></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-62\">\\bar{w}^*</script> is to compute the gradient of the objective function, and use the gradient in an optimization algorithm like L-BFGS.</p>\n  </li>\n</ul>\n<p>We also need to find the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-55-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-808\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-809\"><span class=\"texatom\" id=\"MathJax-Span-810\"><span class=\"mrow\" id=\"MathJax-Span-811\"><span class=\"munderover\" id=\"MathJax-Span-812\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-813\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-814\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-55\">\\bar{w}</script> parameters that best fit the training data, a given a set of labelled sentences:</p>\n<ul>\n      <li>where each pair <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-57-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>,</mo><msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-864\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1002.55em, 2.607em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-865\"><span class=\"mo\" id=\"MathJax-Span-866\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-867\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-868\"><span class=\"mrow\" id=\"MathJax-Span-869\"><span class=\"munderover\" id=\"MathJax-Span-870\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-871\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-872\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-873\"><span class=\"mrow\" id=\"MathJax-Span-874\"><span class=\"mi\" id=\"MathJax-Span-875\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-876\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-877\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-878\"><span class=\"mrow\" id=\"MathJax-Span-879\"><span class=\"munderover\" id=\"MathJax-Span-880\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-881\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-882\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.747em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-883\"><span class=\"mrow\" id=\"MathJax-Span-884\"><span class=\"mi\" id=\"MathJax-Span-885\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-886\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.372em; border-left: 0px solid; width: 0px; height: 1.253em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mo stretchy=\"false\">(</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>,</mo><msub><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-57\">(\\bar{x}_{i}, \\bar{y}_{i})</script> is a sentence with the corresponding word labels annotated. To find the <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-58-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-887\" style=\"width: 0.836em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.357em, 1000.68em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-888\"><span class=\"texatom\" id=\"MathJax-Span-889\"><span class=\"mrow\" id=\"MathJax-Span-890\"><span class=\"munderover\" id=\"MathJax-Span-891\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-892\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-893\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 0.878em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-58\">\\bar{w}</script> parameters that best fit the data we need to\nmaximize the conditional likelihood of the training data:</li>\n    </ul>\n<p>The parameter estimates are computed as:</p>\n<ul>\n      <li>where <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-61-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mfrac><mi>&amp;#x03BB;</mi><mn>2</mn></mfrac><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><msup><mo fence=&quot;false&quot; stretchy=&quot;false&quot;>&amp;#x2016;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>2</mn></mrow></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1038\" style=\"width: 3.128em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 2.607em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.148em, 1002.61em, 2.711em, -999.997em); top: -2.185em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1039\"><span class=\"mfrac\" id=\"MathJax-Span-1040\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.169em, -999.997em); top: -4.424em; left: 50%; margin-left: -0.154em;\"><span class=\"mi\" id=\"MathJax-Span-1041\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">λ</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.169em, -999.997em); top: -3.643em; left: 50%; margin-left: -0.154em;\"><span class=\"mn\" id=\"MathJax-Span-1042\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1000.47em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 0.471em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1043\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span class=\"texatom\" id=\"MathJax-Span-1044\"><span class=\"mrow\" id=\"MathJax-Span-1045\"><span class=\"munderover\" id=\"MathJax-Span-1046\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1047\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-1048\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1049\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1050\" style=\"font-family: STIXGeneral-Regular;\">‖</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.523em;\"><span class=\"texatom\" id=\"MathJax-Span-1051\"><span class=\"mrow\" id=\"MathJax-Span-1052\"><span class=\"mn\" id=\"MathJax-Span-1053\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">2</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.19em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.497em; border-left: 0px solid; width: 0px; height: 1.691em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mfrac><mi>λ</mi><mn>2</mn></mfrac><mo fence=\"false\" stretchy=\"false\">‖</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><msup><mo fence=\"false\" stretchy=\"false\">‖</mo><mrow class=\"MJX-TeXAtom-ORD\"><mn>2</mn></mrow></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-61\">\\frac{\\lambda}{2} \\| \\bar{w} \\| ^{2}</script> is an L2 regularization term.</li>\n    </ul>\n<p>The standard approach to finding <span class=\"MathJax_Preview\" style=\"color: inherit; display: none;\"></span><span class=\"MathJax\" id=\"MathJax-Element-62-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x2217;</mo></msup></math>\" role=\"presentation\" style=\"position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1054\" style=\"width: 1.357em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(1.305em, 1001.1em, 2.294em, -999.997em); top: -2.133em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1055\"><span class=\"msubsup\" id=\"MathJax-Span-1056\"><span style=\"display: inline-block; position: relative; width: 1.096em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1057\"><span class=\"mrow\" id=\"MathJax-Span-1058\"><span class=\"munderover\" id=\"MathJax-Span-1059\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1060\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-1061\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.68em;\"><span class=\"mo\" id=\"MathJax-Span-1062\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∗</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.138em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -0.059em; border-left: 0px solid; width: 0px; height: 1.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>∗</mo></msup></math></span></span><script type=\"math/tex\" id=\"MathJax-Element-62\">\\bar{w}^*</script> is to compute the gradient of the objective function, and use the gradient in an optimization algorithm like L-BFGS.</p>",
      "contentMarkdown": "*   We also need to find the w¯w¯\\\\bar{w} parameters that best fit the training data, a given a set of labelled sentences:\n    \n    {(x¯1,y¯1),…,(x¯m,y¯m)}{(x¯1,y¯1),…,(x¯m,y¯m)}\n    \n    \\\\{(\\\\bar{x}\\_{1}, \\\\bar{y}\\_{1}), \\\\ldots , (\\\\bar{x}\\_{m}, \\\\bar{y}\\_{m})\\\\}\n    \n    *   where each pair (x¯i,y¯i)(x¯i,y¯i)(\\\\bar{x}\\_{i}, \\\\bar{y}\\_{i}) is a sentence with the corresponding word labels annotated. To find the w¯w¯\\\\bar{w} parameters that best fit the data we need to maximize the conditional likelihood of the training data:\n    \n    L(w¯)\\=∑i\\=1mlogp(x¯1|y¯1,w¯)L(w¯)\\=∑i\\=1mlog⁡p(x¯1|y¯1,w¯)\n    \n    L(\\\\bar{w}) = \\\\sum\\\\limits\\_{i=1}^{m} \\\\log p( \\\\bar{x}\\_{1} | \\\\bar{y}\\_{1}, \\\\bar{w} )\n*   The parameter estimates are computed as:\n    \n    w¯∗\\=argmaxw¯ ∈ IRd ∑i\\=1mlogp(x¯i|y¯i,w¯)−λ2‖w¯‖2w¯∗\\=arg⁡maxw¯ ∈ IRd ∑i\\=1mlog⁡p(x¯i|y¯i,w¯)−λ2‖w¯‖2\n    \n    \\\\bar{w}^\\* = \\\\underset{\\\\bar{w}\\\\ \\\\in {\\\\rm \\\\ I\\\\!R}^{d}} {\\\\arg\\\\max}\\\\ \\\\sum\\\\limits\\_{i=1}^{m} \\\\log p( \\\\bar{x}\\_{i} | \\\\bar{y}\\_{i}, \\\\bar{w}) - \\\\frac{\\\\lambda}{2} \\\\| \\\\bar{w} \\\\| ^{2}\n    *   where λ2‖w¯‖2λ2‖w¯‖2\\\\frac{\\\\lambda}{2} \\\\| \\\\bar{w} \\\\| ^{2} is an L2 regularization term.\n*   The standard approach to finding w¯∗w¯∗\\\\bar{w}^\\* is to compute the gradient of the objective function, and use the gradient in an optimization algorithm like L-BFGS.\n    \n\nWe also need to find the w¯w¯\\\\bar{w} parameters that best fit the training data, a given a set of labelled sentences:\n\n*   where each pair (x¯i,y¯i)(x¯i,y¯i)(\\\\bar{x}\\_{i}, \\\\bar{y}\\_{i}) is a sentence with the corresponding word labels annotated. To find the w¯w¯\\\\bar{w} parameters that best fit the data we need to maximize the conditional likelihood of the training data:\n\nThe parameter estimates are computed as:\n\n*   where λ2‖w¯‖2λ2‖w¯‖2\\\\frac{\\\\lambda}{2} \\\\| \\\\bar{w} \\\\| ^{2} is an L2 regularization term.\n\nThe standard approach to finding w¯∗w¯∗\\\\bar{w}^\\* is to compute the gradient of the objective function, and use the gradient in an optimization algorithm like L-BFGS.",
      "order": 5,
      "orderInChapter": 5,
      "difficulty": 4,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous",
        "optimization",
        "regularization"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": false,
        "wordCount": 249,
        "contentLength": 75424
      },
      "nextCards": [
        "ai-conditional-random-fields-wrapping-up-hmm-vs-memm-vs-crf-6",
        "ai-conditional-random-fields-crf-important-observations-7"
      ],
      "relatedCards": [
        "ai-gpu-architecture-streaming-multiprocessors-sm-evolution-21",
        "ai-ml-runtimes-architecture-4",
        "ai-ml-runtimes-pros-and-cons-6",
        "ai-ml-runtimes-implementation-details-22",
        "ai-ml-runtimes-pros-and-cons-23"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/conditional-random-fields/#parameter-estimation",
      "scrapedAt": "2025-12-28T11:57:57.127Z",
      "siblings": [
        "ai-conditional-random-fields-label-bias-problem-in-memms-1",
        "ai-conditional-random-fields-undirected-graphical-models-2",
        "ai-conditional-random-fields-linear-chain-crfs-3",
        "ai-conditional-random-fields-inference-4",
        "ai-conditional-random-fields-wrapping-up-hmm-vs-memm-vs-crf-6"
      ]
    },
    {
      "id": "ai-conditional-random-fields-wrapping-up-hmm-vs-memm-vs-crf-6",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Conditional Random Fields",
      "articleSlug": "conditional-random-fields",
      "chapter": "Introduction",
      "title": "Wrapping Up: HMM vs. MEMM vs. CRF",
      "subtitle": "Introduction",
      "contentHtml": "<ul>\n  <li>\n    <p>It is now helpful to look at the three sequence prediction models, and compared them. The figure bellow shows the graphical representation for the Hidden Markov Model, the Maximum Entropy Markov Model and the Conditional Random Fields.</p>\n  </li>\n  <li>\n    <p>The figure below (taken from Lafferty et al. 2001) shows the graph representation of HMM, MEMM and CRF:</p>\n  </li>\n</ul>\n<p>It is now helpful to look at the three sequence prediction models, and compared them. The figure bellow shows the graphical representation for the Hidden Markov Model, the Maximum Entropy Markov Model and the Conditional Random Fields.</p>\n<p>The figure below (taken from Lafferty et al. 2001) shows the graph representation of HMM, MEMM and CRF:</p>\n<p><img src=\"/primers/ai/assets/conditional-random-fields/HMM-MEMM-CRF.png\" alt=\"\"></p>\n<ul>\n  <li><strong>Hidden Markov Models</strong>:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-63-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><munderover><mo movablelimits=&quot;false&quot;>&amp;#x220F;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow></munderover><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2223;</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x22C5;</mo><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2223;</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1063\" style=\"width: 17.19em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 14.326em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.263em, 1014.27em, 3.648em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1064\"><span class=\"mi\" id=\"MathJax-Span-1065\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-1066\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-1067\"><span class=\"mrow\" id=\"MathJax-Span-1068\"><span class=\"munderover\" id=\"MathJax-Span-1069\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1070\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-1071\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1072\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-1073\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-1074\"><span class=\"munderover\" id=\"MathJax-Span-1075\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1076\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-1077\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1078\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1079\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-1080\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.3em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1081\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∏</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.159em;\"><span class=\"texatom\" id=\"MathJax-Span-1082\"><span class=\"mrow\" id=\"MathJax-Span-1083\"><span class=\"mi\" id=\"MathJax-Span-1084\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1085\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-1086\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.68em, 4.326em, -999.997em); top: -5.258em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-1087\"><span class=\"mrow\" id=\"MathJax-Span-1088\"><span class=\"texatom\" id=\"MathJax-Span-1089\"><span class=\"mrow\" id=\"MathJax-Span-1090\"><span class=\"mo\" id=\"MathJax-Span-1091\" style=\"font-size: 70.7%; font-family: STIXVariants;\">|</span></span></span><span class=\"texatom\" id=\"MathJax-Span-1092\"><span class=\"mrow\" id=\"MathJax-Span-1093\"><span class=\"munderover\" id=\"MathJax-Span-1094\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1095\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.21em, 3.805em, -999.997em); top: -4.06em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-1096\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-1097\"><span class=\"mrow\" id=\"MathJax-Span-1098\"><span class=\"mo\" id=\"MathJax-Span-1099\" style=\"font-size: 70.7%; font-family: STIXVariants;\">|</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-1100\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">P</span><span class=\"mo\" id=\"MathJax-Span-1101\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1102\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1103\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1104\"><span class=\"mrow\" id=\"MathJax-Span-1105\"><span class=\"mi\" id=\"MathJax-Span-1106\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1107\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-1108\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1109\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1110\"><span class=\"mrow\" id=\"MathJax-Span-1111\"><span class=\"mi\" id=\"MathJax-Span-1112\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1113\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1114\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1115\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1116\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-1117\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">P</span><span class=\"mo\" id=\"MathJax-Span-1118\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1119\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1120\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1121\"><span class=\"mrow\" id=\"MathJax-Span-1122\"><span class=\"mi\" id=\"MathJax-Span-1123\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1124\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-1125\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1126\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1127\"><span class=\"mrow\" id=\"MathJax-Span-1128\"><span class=\"mi\" id=\"MathJax-Span-1129\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1130\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.559em; border-left: 0px solid; width: 0px; height: 3.816em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>P</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo><mo>=</mo><munderover><mo movablelimits=\"false\">∏</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow></mrow></munderover><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>∣</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo><mo>⋅</mo><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>∣</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo></math></span></span></div>\n<ul>\n  <li><strong>Maximum Entropy Markov Models</strong>:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-64-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><munderover><mo movablelimits=&quot;false&quot;>&amp;#x220F;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow></munderover><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2223;</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><munderover><mo movablelimits=&quot;false&quot;>&amp;#x220F;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow></munderover><mfrac><mn>1</mn><mrow><mi>Z</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub><mo stretchy=&quot;false&quot;>)</mo></mrow></mfrac><mtext>&amp;#xA0;</mtext><mi>exp</mi><mo>&amp;#x2061;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;2.047em&quot; minsize=&quot;2.047em&quot;>(</mo></mrow><munderover><mo>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></munderover><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msub><mo>&amp;#x22C5;</mo><msub><mi>f</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>j</mi></mrow></msub><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub><mo stretchy=&quot;false&quot;>)</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo maxsize=&quot;2.047em&quot; minsize=&quot;2.047em&quot;>)</mo></mrow></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1131\" style=\"width: 34.638em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 28.857em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(2.138em, 1028.7em, 5.68em, -999.997em); top: -4.112em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1132\"><span class=\"mi\" id=\"MathJax-Span-1133\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-1134\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-1135\"><span class=\"mrow\" id=\"MathJax-Span-1136\"><span class=\"munderover\" id=\"MathJax-Span-1137\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1138\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-1139\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1140\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-1141\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-1142\"><span class=\"munderover\" id=\"MathJax-Span-1143\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1144\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-1145\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1146\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1147\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-1148\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.3em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1149\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∏</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.159em;\"><span class=\"texatom\" id=\"MathJax-Span-1150\"><span class=\"mrow\" id=\"MathJax-Span-1151\"><span class=\"mi\" id=\"MathJax-Span-1152\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1153\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-1154\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.68em, 4.326em, -999.997em); top: -5.258em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-1155\"><span class=\"mrow\" id=\"MathJax-Span-1156\"><span class=\"texatom\" id=\"MathJax-Span-1157\"><span class=\"mrow\" id=\"MathJax-Span-1158\"><span class=\"mo\" id=\"MathJax-Span-1159\" style=\"font-size: 70.7%; font-family: STIXVariants;\">|</span></span></span><span class=\"texatom\" id=\"MathJax-Span-1160\"><span class=\"mrow\" id=\"MathJax-Span-1161\"><span class=\"munderover\" id=\"MathJax-Span-1162\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1163\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.21em, 3.805em, -999.997em); top: -4.06em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-1164\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-1165\"><span class=\"mrow\" id=\"MathJax-Span-1166\"><span class=\"mo\" id=\"MathJax-Span-1167\" style=\"font-size: 70.7%; font-family: STIXVariants;\">|</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-1168\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.211em;\">P</span><span class=\"mo\" id=\"MathJax-Span-1169\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"msubsup\" id=\"MathJax-Span-1170\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1171\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1172\"><span class=\"mrow\" id=\"MathJax-Span-1173\"><span class=\"mi\" id=\"MathJax-Span-1174\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1175\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"msubsup\" id=\"MathJax-Span-1176\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1177\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1178\"><span class=\"mrow\" id=\"MathJax-Span-1179\"><span class=\"mi\" id=\"MathJax-Span-1180\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1181\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1182\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1183\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1184\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.732em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1185\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1186\"><span class=\"mrow\" id=\"MathJax-Span-1187\"><span class=\"mi\" id=\"MathJax-Span-1188\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1189\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1190\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"munderover\" id=\"MathJax-Span-1191\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 1.357em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.3em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1192\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∏</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.273em, -999.997em); top: -2.862em; left: 0.159em;\"><span class=\"texatom\" id=\"MathJax-Span-1193\"><span class=\"mrow\" id=\"MathJax-Span-1194\"><span class=\"mi\" id=\"MathJax-Span-1195\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1196\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-1197\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.68em, 4.326em, -999.997em); top: -5.258em; left: 0.315em;\"><span class=\"texatom\" id=\"MathJax-Span-1198\"><span class=\"mrow\" id=\"MathJax-Span-1199\"><span class=\"texatom\" id=\"MathJax-Span-1200\"><span class=\"mrow\" id=\"MathJax-Span-1201\"><span class=\"mo\" id=\"MathJax-Span-1202\" style=\"font-size: 70.7%; font-family: STIXVariants;\">|</span></span></span><span class=\"texatom\" id=\"MathJax-Span-1203\"><span class=\"mrow\" id=\"MathJax-Span-1204\"><span class=\"munderover\" id=\"MathJax-Span-1205\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1206\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.21em, 3.805em, -999.997em); top: -4.06em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-1207\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"texatom\" id=\"MathJax-Span-1208\"><span class=\"mrow\" id=\"MathJax-Span-1209\"><span class=\"mo\" id=\"MathJax-Span-1210\" style=\"font-size: 70.7%; font-family: STIXVariants;\">|</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mfrac\" id=\"MathJax-Span-1211\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 3.857em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.169em, -999.997em); top: -4.685em; left: 50%; margin-left: -0.258em;\"><span class=\"mn\" id=\"MathJax-Span-1212\" style=\"font-family: STIXGeneral-Regular;\">1</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.18em, 1003.7em, 4.43em, -999.997em); top: -3.331em; left: 50%; margin-left: -1.872em;\"><span class=\"mrow\" id=\"MathJax-Span-1213\"><span class=\"mi\" id=\"MathJax-Span-1214\" style=\"font-family: STIXGeneral-Italic;\">Z<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1215\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1216\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1217\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1218\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1219\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1220\"><span class=\"mrow\" id=\"MathJax-Span-1221\"><span class=\"mi\" id=\"MathJax-Span-1222\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1223\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1224\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1225\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1003.86em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.857em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span><span class=\"mtext\" id=\"MathJax-Span-1226\" style=\"font-family: STIXGeneral-Regular;\">&nbsp;</span><span class=\"mi\" id=\"MathJax-Span-1227\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">exp</span><span class=\"mo\" id=\"MathJax-Span-1228\"></span><span class=\"texatom\" id=\"MathJax-Span-1229\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-1230\"><span class=\"mo\" id=\"MathJax-Span-1231\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">(</span></span></span></span><span class=\"munderover\" id=\"MathJax-Span-1232\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.305em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.867em, 1001.2em, 4.638em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-1233\" style=\"font-family: STIXSizeOneSym; vertical-align: -0.518em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.388em, 1000.94em, 4.43em, -999.997em); top: -2.862em; left: 0.107em;\"><span class=\"texatom\" id=\"MathJax-Span-1234\"><span class=\"mrow\" id=\"MathJax-Span-1235\"><span class=\"mi\" id=\"MathJax-Span-1236\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1237\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">=</span><span class=\"mn\" id=\"MathJax-Span-1238\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1000.52em, 4.169em, -999.997em); top: -5.206em; left: 0.367em;\"><span class=\"texatom\" id=\"MathJax-Span-1239\"><span class=\"mrow\" id=\"MathJax-Span-1240\"><span class=\"mi\" id=\"MathJax-Span-1241\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-1242\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.94em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1243\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.68em;\"><span class=\"texatom\" id=\"MathJax-Span-1244\"><span class=\"mrow\" id=\"MathJax-Span-1245\"><span class=\"mi\" id=\"MathJax-Span-1246\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1247\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"msubsup\" id=\"MathJax-Span-1248\" style=\"padding-left: 0.263em;\"><span style=\"display: inline-block; position: relative; width: 0.576em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.18em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1249\" style=\"font-family: STIXGeneral-Italic;\">f<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.159em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.263em;\"><span class=\"texatom\" id=\"MathJax-Span-1250\"><span class=\"mrow\" id=\"MathJax-Span-1251\"><span class=\"mi\" id=\"MathJax-Span-1252\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">j<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1253\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"mi\" id=\"MathJax-Span-1254\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1255\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msubsup\" id=\"MathJax-Span-1256\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 1.565em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1257\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.471em;\"><span class=\"texatom\" id=\"MathJax-Span-1258\"><span class=\"mrow\" id=\"MathJax-Span-1259\"><span class=\"mi\" id=\"MathJax-Span-1260\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-1261\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">−</span><span class=\"mn\" id=\"MathJax-Span-1262\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">1</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1263\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"texatom\" id=\"MathJax-Span-1264\" style=\"\"><span class=\"mrow\" id=\"MathJax-Span-1265\"><span class=\"mo\" id=\"MathJax-Span-1266\" style=\"vertical-align: -0.57em;\"><span style=\"font-family: STIXSizeThreeSym;\">)</span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.117em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.747em; border-left: 0px solid; width: 0px; height: 4.003em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>P</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo><mo>=</mo><munderover><mo movablelimits=\"false\">∏</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow></mrow></munderover><mi>P</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo>∣</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo>=</mo><munderover><mo movablelimits=\"false\">∏</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mo stretchy=\"false\">|</mo></mrow></mrow></munderover><mfrac><mn>1</mn><mrow><mi>Z</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mfrac><mtext>&nbsp;</mtext><mi>exp</mi><mo>⁡</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo maxsize=\"2.047em\" minsize=\"2.047em\">(</mo></mrow><munderover><mo>∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow class=\"MJX-TeXAtom-ORD\"><mi>N</mi></mrow></munderover><msub><mi>w</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msub><mo>⋅</mo><msub><mi>f</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>j</mi></mrow></msub><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msub><mi>y</mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo><mrow class=\"MJX-TeXAtom-ORD\"><mo maxsize=\"2.047em\" minsize=\"2.047em\">)</mo></mrow></math></span></span></div>\n<ul>\n  <li><strong>Conditional Random Fields</strong>:</li>\n</ul>\n<div class=\"MathJax_Display\" style=\"text-align: center;\"><span class=\"MathJax\" id=\"MathJax-Element-65-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>P</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x2223;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x22C5;</mo><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></mrow><mrow><munder><mo movablelimits=&quot;false&quot;>&amp;#x2211;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x2032;</mo></msup><mo>&amp;#x2208;</mo><mi>Y</mi></mrow></munder><mi>exp</mi><mo>&amp;#x2061;</mo><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>w</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x22C5;</mo><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>x</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>,</mo><msup><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mover><mi>y</mi><mo stretchy=&quot;false&quot;>&amp;#x00AF;</mo></mover></mrow><mo>&amp;#x2032;</mo></msup><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></mrow></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1267\" style=\"width: 16.773em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 13.961em; height: 0px; font-size: 120%;\"><span style=\"position: absolute; clip: rect(0.732em, 1013.96em, 4.378em, -999.997em); top: -2.237em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-1268\"><span class=\"mi\" id=\"MathJax-Span-1269\" style=\"font-family: STIXGeneral-Italic;\">P</span><span class=\"mo\" id=\"MathJax-Span-1270\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-1271\"><span class=\"mrow\" id=\"MathJax-Span-1272\"><span class=\"munderover\" id=\"MathJax-Span-1273\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1274\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-1275\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1276\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">∣</span><span class=\"texatom\" id=\"MathJax-Span-1277\" style=\"padding-left: 0.315em;\"><span class=\"mrow\" id=\"MathJax-Span-1278\"><span class=\"munderover\" id=\"MathJax-Span-1279\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1280\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-1281\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1282\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-1283\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-1284\"><span class=\"munderover\" id=\"MathJax-Span-1285\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1286\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-1287\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1288\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1289\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.315em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-1290\" style=\"padding-left: 0.315em;\"><span style=\"display: inline-block; position: relative; width: 8.388em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;\"><span style=\"position: absolute; clip: rect(3.18em, 1006.2em, 4.378em, -999.997em); top: -4.685em; left: 50%; margin-left: -3.122em;\"><span class=\"mrow\" id=\"MathJax-Span-1291\"><span class=\"mi\" id=\"MathJax-Span-1292\" style=\"font-family: STIXGeneral-Regular;\">exp</span><span class=\"mo\" id=\"MathJax-Span-1293\"></span><span class=\"mo\" id=\"MathJax-Span-1294\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-1295\"><span class=\"mrow\" id=\"MathJax-Span-1296\"><span class=\"munderover\" id=\"MathJax-Span-1297\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1298\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-1299\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1300\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-1301\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1302\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-1303\"><span class=\"mrow\" id=\"MathJax-Span-1304\"><span class=\"munderover\" id=\"MathJax-Span-1305\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1306\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-1307\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1308\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"texatom\" id=\"MathJax-Span-1309\" style=\"padding-left: 0.211em;\"><span class=\"mrow\" id=\"MathJax-Span-1310\"><span class=\"munderover\" id=\"MathJax-Span-1311\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1312\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-1313\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1314\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1315\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.076em, 1008.18em, 5.419em, -999.997em); top: -3.279em; left: 50%; margin-left: -4.112em;\"><span class=\"mrow\" id=\"MathJax-Span-1316\"><span class=\"munderover\" id=\"MathJax-Span-1317\"><span style=\"display: inline-block; position: relative; width: 1.461em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.076em, 1000.84em, 4.43em, -999.997em); top: -4.008em; left: 0.263em;\"><span class=\"mo\" id=\"MathJax-Span-1318\" style=\"font-family: STIXGeneral-Regular; vertical-align: 0.003em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.284em, 1001.46em, 4.43em, -999.997em); top: -2.966em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1319\"><span class=\"mrow\" id=\"MathJax-Span-1320\"><span class=\"msup\" id=\"MathJax-Span-1321\"><span style=\"display: inline-block; position: relative; width: 0.523em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.388em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1322\"><span class=\"mrow\" id=\"MathJax-Span-1323\"><span class=\"munderover\" id=\"MathJax-Span-1324\"><span style=\"display: inline-block; position: relative; width: 0.315em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.544em, 1000.32em, 4.326em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1325\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.44em, 1000.21em, 3.805em, -999.997em); top: -4.06em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-1326\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.32em; left: 0.315em;\"><span class=\"mo\" id=\"MathJax-Span-1327\" style=\"font-size: 50%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1328\" style=\"font-size: 70.7%; font-family: STIXGeneral-Regular;\">∈</span><span class=\"mi\" id=\"MathJax-Span-1329\" style=\"font-size: 70.7%; font-family: STIXGeneral-Italic;\">Y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mi\" id=\"MathJax-Span-1330\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.211em;\">exp</span><span class=\"mo\" id=\"MathJax-Span-1331\"></span><span class=\"mo\" id=\"MathJax-Span-1332\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-1333\"><span class=\"mrow\" id=\"MathJax-Span-1334\"><span class=\"munderover\" id=\"MathJax-Span-1335\"><span style=\"display: inline-block; position: relative; width: 0.68em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.63em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1336\" style=\"font-family: STIXGeneral-Italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.159em;\"><span class=\"mo\" id=\"MathJax-Span-1337\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1338\" style=\"font-family: STIXGeneral-Regular; padding-left: 0.263em;\">⋅</span><span class=\"mi\" id=\"MathJax-Span-1339\" style=\"font-family: STIXGeneral-Italic; padding-left: 0.263em;\">F<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.055em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-1340\" style=\"font-family: STIXGeneral-Regular;\">(</span><span class=\"texatom\" id=\"MathJax-Span-1341\"><span class=\"mrow\" id=\"MathJax-Span-1342\"><span class=\"munderover\" id=\"MathJax-Span-1343\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.47em, 4.169em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1344\" style=\"font-family: STIXGeneral-Italic;\">x<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.107em;\"><span class=\"mo\" id=\"MathJax-Span-1345\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1346\" style=\"font-family: STIXGeneral-Regular;\">,</span><span class=\"msup\" id=\"MathJax-Span-1347\" style=\"padding-left: 0.211em;\"><span style=\"display: inline-block; position: relative; width: 0.784em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.232em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"texatom\" id=\"MathJax-Span-1348\"><span class=\"mrow\" id=\"MathJax-Span-1349\"><span class=\"munderover\" id=\"MathJax-Span-1350\"><span style=\"display: inline-block; position: relative; width: 0.471em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.44em, 1000.42em, 4.378em, -999.997em); top: -4.008em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-1351\" style=\"font-family: STIXGeneral-Italic;\">y</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(3.232em, 1000.32em, 3.596em, -999.997em); top: -4.008em; left: 0.055em;\"><span class=\"mo\" id=\"MathJax-Span-1352\" style=\"font-family: STIXGeneral-Regular;\">¯</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; top: -4.372em; left: 0.471em;\"><span class=\"mo\" id=\"MathJax-Span-1353\" style=\"font-size: 70.7%; font-family: STIXVariants;\">′</span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-1354\" style=\"font-family: STIXGeneral-Regular;\">)</span><span class=\"mo\" id=\"MathJax-Span-1355\" style=\"font-family: STIXGeneral-Regular;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.013em;\"></span></span><span style=\"position: absolute; clip: rect(0.836em, 1008.39em, 1.201em, -999.997em); top: -1.247em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 8.388em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.044em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.242em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -2.434em; border-left: 0px solid; width: 0px; height: 4.128em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>P</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>∣</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>⋅</mo><mi>F</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><mrow><munder><mo movablelimits=\"false\">∑</mo><mrow class=\"MJX-TeXAtom-ORD\"><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>′</mo></msup><mo>∈</mo><mi>Y</mi></mrow></munder><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>w</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>⋅</mo><mi>F</mi><mo stretchy=\"false\">(</mo><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>x</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>,</mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mover><mi>y</mi><mo stretchy=\"false\">¯</mo></mover></mrow><mo>′</mo></msup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow></mfrac></math></span></span></div>",
      "contentMarkdown": "*   It is now helpful to look at the three sequence prediction models, and compared them. The figure bellow shows the graphical representation for the Hidden Markov Model, the Maximum Entropy Markov Model and the Conditional Random Fields.\n    \n*   The figure below (taken from Lafferty et al. 2001) shows the graph representation of HMM, MEMM and CRF:\n    \n\nIt is now helpful to look at the three sequence prediction models, and compared them. The figure bellow shows the graphical representation for the Hidden Markov Model, the Maximum Entropy Markov Model and the Conditional Random Fields.\n\nThe figure below (taken from Lafferty et al. 2001) shows the graph representation of HMM, MEMM and CRF:\n\n![](/primers/ai/assets/conditional-random-fields/HMM-MEMM-CRF.png)\n\n*   **Hidden Markov Models**:\n\nP(y¯,x¯)\\=∏i\\=1|y¯|P(yi∣yi−1)⋅P(xi∣yi)P(y¯,x¯)\\=∏i\\=1|y¯|P(yi∣yi−1)⋅P(xi∣yi)\n\n*   **Maximum Entropy Markov Models**:\n\nP(y¯,x¯)\\=∏i\\=1|y¯|P(yi∣yi−1,xi)\\=∏i\\=1|y¯|1Z(x,yi−1) exp(∑j\\=1Nwj⋅fj(x,yi−1))P(y¯,x¯)\\=∏i\\=1|y¯|P(yi∣yi−1,xi)\\=∏i\\=1|y¯|1Z(x,yi−1) exp⁡(∑j\\=1Nwj⋅fj(x,yi−1))\n\n*   **Conditional Random Fields**:\n\nP(y¯∣x¯,w¯)\\=exp(w¯⋅F(x¯,y¯))∑y¯′∈Yexp(w¯⋅F(x¯,y¯′))P(y¯∣x¯,w¯)\\=exp⁡(w¯⋅F(x¯,y¯))∑y¯′∈Yexp⁡(w¯⋅F(x¯,y¯′))",
      "order": 6,
      "orderInChapter": 6,
      "difficulty": 3,
      "estimatedMinutes": 1,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": true,
        "hasImages": true,
        "wordCount": 131,
        "contentLength": 58565
      },
      "nextCards": [
        "ai-conditional-random-fields-crf-important-observations-7"
      ],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/conditional-random-fields/#wrapping-up:-hmm-vs.-memm-vs.-crf",
      "scrapedAt": "2025-12-28T11:57:57.127Z",
      "siblings": [
        "ai-conditional-random-fields-label-bias-problem-in-memms-1",
        "ai-conditional-random-fields-undirected-graphical-models-2",
        "ai-conditional-random-fields-linear-chain-crfs-3",
        "ai-conditional-random-fields-inference-4",
        "ai-conditional-random-fields-parameter-estimation-5"
      ]
    },
    {
      "id": "ai-conditional-random-fields-crf-important-observations-7",
      "domain": "ai_primers",
      "category": "Miscellaneous",
      "article": "Conditional Random Fields",
      "articleSlug": "conditional-random-fields",
      "chapter": "Introduction",
      "title": "CRF Important Observations",
      "subtitle": "Introduction",
      "contentHtml": "<ul>\n  <li>\n    <p>MEMMs are normalized locally over each observation, and hence suffer from the Label Bias problem, where the transitions going out from a state compete only against each other, as opposed to all the other transitions in the model.</p>\n  </li>\n  <li>\n    <p>CRFs avoid the label bias problem a weakness exhibited by Maximum Entropy Markov Models (MEMM). The big difference between MEMM and CRF is that MEMM is locally renormalized and suffers from the label bias problem, while CRFs are globally re-normalized.</p>\n  </li>\n  <li>\n    <p>The inference algorithm in CRF is again based on Viterbi algorithm.</p>\n  </li>\n  <li>\n    <p>Output transition and observation probabilities are not modelled separately.</p>\n  </li>\n  <li>\n    <p>Output transition dependent on the state and the observation as one conditional probability.</p>\n  </li>\n</ul>\n<p>MEMMs are normalized locally over each observation, and hence suffer from the Label Bias problem, where the transitions going out from a state compete only against each other, as opposed to all the other transitions in the model.</p>\n<p>CRFs avoid the label bias problem a weakness exhibited by Maximum Entropy Markov Models (MEMM). The big difference between MEMM and CRF is that MEMM is locally renormalized and suffers from the label bias problem, while CRFs are globally re-normalized.</p>\n<p>The inference algorithm in CRF is again based on Viterbi algorithm.</p>\n<p>Output transition and observation probabilities are not modelled separately.</p>\n<p>Output transition dependent on the state and the observation as one conditional probability.</p>",
      "contentMarkdown": "*   MEMMs are normalized locally over each observation, and hence suffer from the Label Bias problem, where the transitions going out from a state compete only against each other, as opposed to all the other transitions in the model.\n    \n*   CRFs avoid the label bias problem a weakness exhibited by Maximum Entropy Markov Models (MEMM). The big difference between MEMM and CRF is that MEMM is locally renormalized and suffers from the label bias problem, while CRFs are globally re-normalized.\n    \n*   The inference algorithm in CRF is again based on Viterbi algorithm.\n    \n*   Output transition and observation probabilities are not modelled separately.\n    \n*   Output transition dependent on the state and the observation as one conditional probability.\n    \n\nMEMMs are normalized locally over each observation, and hence suffer from the Label Bias problem, where the transitions going out from a state compete only against each other, as opposed to all the other transitions in the model.\n\nCRFs avoid the label bias problem a weakness exhibited by Maximum Entropy Markov Models (MEMM). The big difference between MEMM and CRF is that MEMM is locally renormalized and suffers from the label bias problem, while CRFs are globally re-normalized.\n\nThe inference algorithm in CRF is again based on Viterbi algorithm.\n\nOutput transition and observation probabilities are not modelled separately.\n\nOutput transition dependent on the state and the observation as one conditional probability.",
      "order": 7,
      "orderInChapter": 7,
      "difficulty": 2,
      "estimatedMinutes": 2,
      "tags": [
        "miscellaneous"
      ],
      "metadata": {
        "hasCode": false,
        "hasMath": false,
        "hasImages": false,
        "wordCount": 227,
        "contentLength": 1617
      },
      "nextCards": [],
      "relatedCards": [
        "ai-top-30-papers-the-first-law-of-complexodynamics-1",
        "ai-top-30-papers-quantifying-the-rise-and-fall-of-complexity-in-clo-19",
        "ai-top-30-papers-kolmogorov-complexity-and-algorithmic-randomness-25",
        "ai-gpu-architecture-cuda-cores-2",
        "ai-gpu-architecture-tensor-cores-3"
      ],
      "prerequisites": [],
      "sourceUrl": "https://aman.ai/primers/ai/conditional-random-fields/#crf-important-observations",
      "scrapedAt": "2025-12-28T11:57:57.127Z",
      "siblings": [
        "ai-conditional-random-fields-label-bias-problem-in-memms-1",
        "ai-conditional-random-fields-undirected-graphical-models-2",
        "ai-conditional-random-fields-linear-chain-crfs-3",
        "ai-conditional-random-fields-inference-4",
        "ai-conditional-random-fields-parameter-estimation-5"
      ]
    }
  ]
}